{"cells":[{"cell_type":"markdown","metadata":{"id":"96kvih9mXkNN"},"source":["> **Youtube Videos Transcription with OpenAI's Whisper**\n",">\n","> [![blog post shield](https://img.shields.io/static/v1?label=&message=Blog%20post&color=blue&style=for-the-badge&logo=openai&link=https://openai.com/blog/whisper)](https://openai.com/blog/whisper)\n","[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)](https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)\n","[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/openai/whisper)](https://github.com/openai/whisper)\n","[![paper shield](https://img.shields.io/static/v1?label=&message=Paper&color=blue&style=for-the-badge&link=https://cdn.openai.com/papers/whisper.pdf)](https://cdn.openai.com/papers/whisper.pdf)\n","[![model card shield](https://img.shields.io/static/v1?label=&message=Model%20card&color=blue&style=for-the-badge&link=https://github.com/openai/whisper/blob/main/model-card.md)](https://github.com/openai/whisper/blob/main/model-card.md)\n","> \n","> Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\n","> \n","> This Notebook will guide you through the transcription of a Youtube video using Whisper. You'll be able to explore most inference parameters or use the Notebook as-is to store the transcript and video audio in your Google Drive.\n","\n","# **Youtube Videos Transcription with m-bain's WhisperX**\n","\n","This Notebook is modified from ArthurFDLR's [whisper-youtube](https://github.com/ArthurFDLR/whisper-youtube) notebook.\n","\n","It was modified to utilize m-bain's [whisperX](https://github.com/m-bain/whisperX) instead for better accuracy in subtitle generation.\n","\n","Special thanks to [Barabazs's response](https://github.com/m-bain/whisperX/issues/173#issuecomment-1504768197) for fixing dependency problem."]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","id":"QshUbLqpX7L4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681308749349,"user_tz":-480,"elapsed":6,"user":{"displayName":"廖得凱","userId":"16578409113451863412"}},"outputId":"2655762f-62c2-45ca-c5ef-deefc1dd1cd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla T4 (UUID: GPU-e4cb19bc-b0a3-7c9d-1760-4427f58f4b40)\n","Wed Apr 12 14:12:29 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   56C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["#@markdown # **Check GPU type** 🕵️\n","\n","#@markdown The type of GPU you get assigned in your Colab session defined the speed at which the video will be transcribed.\n","#@markdown The higher the number of floating point operations per second (FLOPS), the faster the transcription.\n","#@markdown But even the least powerful GPU available in Colab is able to run any Whisper model.\n","#@markdown Make sure you've selected `GPU` as hardware accelerator for the Notebook (Runtime &rarr; Change runtime type &rarr; Hardware accelerator).\n","\n","#@markdown |  GPU   |  GPU RAM   | FP32 teraFLOPS |     Availability   |\n","#@markdown |:------:|:----------:|:--------------:|:------------------:|\n","#@markdown |  T4    |    16 GB   |       8.1      |         Free       |\n","#@markdown | P100   |    16 GB   |      10.6      |      Colab Pro     |\n","#@markdown | V100   |    16 GB   |      15.7      |  Colab Pro (Rare)  |\n","\n","#@markdown ---\n","#@markdown **Factory reset your Notebook's runtime if you want to get assigned a new GPU.**\n","\n","!nvidia-smi -L\n","\n","!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"cellView":"form","id":"IfG0E_WbRFI0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681308933242,"user_tz":-480,"elapsed":183895,"user":{"displayName":"廖得凱","userId":"16578409113451863412"}},"outputId":"512070bf-c775-4258-ddaa-759d07b60edf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-5emg069s\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-5emg069s\n","  Resolved https://github.com/openai/whisper.git to commit c09a7ae299c4c34c5839a76380ae407e7d785914\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (2.0.0+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (4.65.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (2.0.0)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (9.1.0)\n","Collecting tiktoken==0.3.1\n","  Downloading tiktoken-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpeg-python==0.2.0\n","  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (0.56.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (1.22.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from ffmpeg-python==0.2.0->openai-whisper==20230314) (0.18.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2022.10.31)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2.27.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (16.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.25.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.11.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba->openai-whisper==20230314) (67.6.1)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba->openai-whisper==20230314) (0.39.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->openai-whisper==20230314) (3.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->openai-whisper==20230314) (4.5.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->openai-whisper==20230314) (3.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->openai-whisper==20230314) (1.11.1)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2022.12.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->openai-whisper==20230314) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->openai-whisper==20230314) (1.3.0)\n","Building wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=798038 sha256=886fa5bc30ffc46e3f7eb256f4fe2bf38fded75c20760515cb0693f6eb971054\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-cd7_jn55/wheels/fe/03/29/e7919208d11b4ab32972cb448bb84a9a675d92cd52c9a48341\n","Successfully built openai-whisper\n","Installing collected packages: ffmpeg-python, tiktoken, openai-whisper\n","Successfully installed ffmpeg-python-0.2.0 openai-whisper-20230314 tiktoken-0.3.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytube\n","  Downloading pytube-12.1.3-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pytube\n","Successfully installed pytube-12.1.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/m-bain/whisperx.git\n","  Cloning https://github.com/m-bain/whisperx.git to /tmp/pip-req-build-08197ztc\n","  Running command git clone --filter=blob:none --quiet https://github.com/m-bain/whisperx.git /tmp/pip-req-build-08197ztc\n","  Resolved https://github.com/m-bain/whisperx.git to commit 6a72b61564dbea7242b8d4eab74af375df516759\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from whisperx==2.0) (1.22.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from whisperx==2.0) (1.5.3)\n","Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.9/dist-packages (from whisperx==2.0) (2.0.0+cu118)\n","Collecting torchaudio<1.0,>=0.10\n","  Downloading torchaudio-0.13.1-cp39-cp39-manylinux1_x86_64.whl (4.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from whisperx==2.0) (4.65.0)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.9/dist-packages (from whisperx==2.0) (9.1.0)\n","Collecting transformers>=4.19.0\n","  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ffmpeg-python==0.2.0 in /usr/local/lib/python3.9/dist-packages (from whisperx==2.0) (0.2.0)\n","Collecting pyannote.audio\n","  Downloading pyannote.audio-2.1.1-py2.py3-none-any.whl (390 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.7/390.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: openai-whisper==20230314 in /usr/local/lib/python3.9/dist-packages (from whisperx==2.0) (20230314)\n","Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from ffmpeg-python==0.2.0->whisperx==2.0) (0.18.3)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314->whisperx==2.0) (2.0.0)\n","Requirement already satisfied: tiktoken==0.3.1 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314->whisperx==2.0) (0.3.1)\n","Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314->whisperx==2.0) (0.56.4)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314->whisperx==2.0) (2.27.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314->whisperx==2.0) (2022.10.31)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314->whisperx==2.0) (16.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314->whisperx==2.0) (3.25.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314->whisperx==2.0) (3.11.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.9->whisperx==2.0) (3.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.9->whisperx==2.0) (1.11.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.9->whisperx==2.0) (3.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.9->whisperx==2.0) (4.5.0)\n","Collecting torch>=1.9\n","  Downloading torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9->whisperx==2.0) (67.6.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9->whisperx==2.0) (0.40.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisperx==2.0) (23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.19.0->whisperx==2.0) (6.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->whisperx==2.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->whisperx==2.0) (2022.7.1)\n","Collecting pyannote.pipeline<3.0,>=2.3\n","  Downloading pyannote.pipeline-2.3-py3-none-any.whl (30 kB)\n","Collecting networkx\n","  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-audiomentations>=0.11.0\n","  Downloading torch_audiomentations-0.11.0-py3-none-any.whl (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-metric-learning<2.0,>=1.0.0\n","  Downloading pytorch_metric_learning-1.7.3-py3-none-any.whl (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hmmlearn<0.3,>=0.2.7\n","  Downloading hmmlearn-0.2.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (217 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.2/217.2 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops<0.4.0,>=0.3\n","  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n","Collecting pyannote.database<5.0,>=4.1.1\n","  Downloading pyannote.database-4.1.3-py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-lightning<1.7,>=1.5.4\n","  Downloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting backports.cached-property\n","  Downloading backports.cached_property-1.0.2-py3-none-any.whl (6.1 kB)\n","Collecting soundfile<0.11,>=0.10.2\n","  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n","Collecting pyannote.core<5.0,>=4.4\n","  Downloading pyannote.core-4.5-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting semver<3.0,>=2.10.2\n","  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n","Collecting pyannote.metrics<4.0,>=3.2\n","  Downloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting asteroid-filterbanks<0.5,>=0.4\n","  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n","Collecting singledispatchmethod\n","  Downloading singledispatchmethod-1.0-py2.py3-none-any.whl (4.7 kB)\n","Collecting torchmetrics<1.0,>=0.6\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting omegaconf<3.0,>=2.1\n","  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting speechbrain<0.6,>=0.5.12\n","  Downloading speechbrain-0.5.14-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.0/519.0 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.9/dist-packages (from hmmlearn<0.3,>=0.2.7->pyannote.audio->whisperx==2.0) (1.10.1)\n","Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.9/dist-packages (from hmmlearn<0.3,>=0.2.7->pyannote.audio->whisperx==2.0) (1.2.2)\n","Collecting antlr4-python3-runtime==4.9.*\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting simplejson>=3.8.1\n","  Downloading simplejson-3.19.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.9/dist-packages (from pyannote.core<5.0,>=4.4->pyannote.audio->whisperx==2.0) (2.4.0)\n","Requirement already satisfied: typer[all]>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from pyannote.database<5.0,>=4.1.1->pyannote.audio->whisperx==2.0) (0.7.0)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==2.0) (3.7.1)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.9/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==2.0) (0.8.10)\n","Collecting docopt>=0.6.2\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting optuna>=1.4\n","  Downloading optuna-3.1.1-py3-none-any.whl (365 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->whisperx==2.0) (1.16.0)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (2023.3.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (2.12.1)\n","Collecting pyDeprecate>=0.3.1\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Collecting protobuf<=3.20.1\n","  Downloading protobuf-3.20.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314->whisperx==2.0) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314->whisperx==2.0) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314->whisperx==2.0) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314->whisperx==2.0) (2022.12.7)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile<0.11,>=0.10.2->pyannote.audio->whisperx==2.0) (1.15.1)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from speechbrain<0.6,>=0.5.12->pyannote.audio->whisperx==2.0) (1.2.0)\n","Collecting hyperpyyaml\n","  Downloading HyperPyYAML-1.2.0-py3-none-any.whl (16 kB)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.9->whisperx==2.0) (1.3.0)\n","Collecting julius<0.3,>=0.2.3\n","  Downloading julius-0.2.7.tar.gz (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-pitch-shift>=1.2.2\n","  Downloading torch_pitch_shift-1.2.3-py3-none-any.whl (4.9 kB)\n","Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio->whisperx==2.0) (0.10.0.post2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.9->whisperx==2.0) (2.1.2)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba->openai-whisper==20230314->whisperx==2.0) (0.39.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile<0.11,>=0.10.2->pyannote.audio->whisperx==2.0) (2.21)\n","Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting librosa>=0.6.0\n","  Downloading librosa-0.10.0.post1-py3-none-any.whl (252 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading librosa-0.10.0-py3-none-any.whl (252 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==2.0) (1.6.0)\n","Collecting resampy>=0.2.2\n","  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==2.0) (4.4.2)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==2.0) (3.0.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==2.0) (0.11.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==2.0) (1.0.7)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==2.0) (5.12.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==2.0) (4.39.3)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==2.0) (3.0.9)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==2.0) (8.4.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==2.0) (1.4.4)\n","Collecting colorlog\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Collecting cmaes>=0.9.1\n","  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna>=1.4->pyannote.pipeline<3.0,>=2.3->pyannote.audio->whisperx==2.0) (2.0.9)\n","Collecting alembic>=1.5.0\n","  Downloading alembic-1.10.3-py3-none-any.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.16->hmmlearn<0.3,>=0.2.7->pyannote.audio->whisperx==2.0) (3.1.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (1.53.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (1.4.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (2.2.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (2.17.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (3.4.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (0.7.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (1.0.0)\n","Collecting primePy>=1.3\n","  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer[all]>=0.2.1->pyannote.database<5.0,>=4.1.1->pyannote.audio->whisperx==2.0) (8.1.3)\n","Collecting shellingham<2.0.0,>=1.3.0\n","  Downloading shellingham-1.5.0.post1-py2.py3-none-any.whl (9.4 kB)\n","Collecting rich<13.0.0,>=10.11.0\n","  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colorama<0.5.0,>=0.4.3\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting ruamel.yaml>=0.17.8\n","  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (22.2.0)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Mako\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (0.2.8)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (5.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (1.3.1)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote.audio->whisperx==2.0) (3.15.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (6.2.0)\n","Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio->whisperx==2.0) (1.4.4)\n","Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich<13.0.0,>=10.11.0->typer[all]>=0.2.1->pyannote.database<5.0,>=4.1.1->pyannote.audio->whisperx==2.0) (2.14.0)\n","Collecting commonmark<0.10.0,>=0.9.0\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ruamel.yaml.clib>=0.2.6\n","  Downloading ruamel.yaml.clib-0.2.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.4/519.4 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna>=1.4->pyannote.pipeline<3.0,>=2.3->pyannote.audio->whisperx==2.0) (2.0.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning<1.7,>=1.5.4->pyannote.audio->whisperx==2.0) (3.2.2)\n","Building wheels for collected packages: whisperx, antlr4-python3-runtime, docopt, julius\n","  Building wheel for whisperx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for whisperx: filename=whisperx-2.0-py3-none-any.whl size=27352 sha256=f80504fb063f1f7c135709c008472bffae86f118ded51bcb68d3781425b8073d\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-4yjuphlm/wheels/c7/67/c8/d92ee7475af476abd5e4fb00fee58ac4f2a9e333668ea3c237\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144573 sha256=f62e37f387ffa2426e4c39c542598ba0cc3b51bea1ff5a911e5c2fdb0f220a24\n","  Stored in directory: /root/.cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=3d6ec8a32670a37a442c88e898e52b1491ea256e9bc8e380ada9c95482b0b5eb\n","  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n","  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21895 sha256=479b0afc63ddc2c6271eb8253c893f0a6654b894e230b057c8304603cfe74717\n","  Stored in directory: /root/.cache/pip/wheels/53/0a/a7/fc08f97438f4969d86afa7904336c2eb7eb422101359f3ad11\n","Successfully built whisperx antlr4-python3-runtime docopt julius\n","Installing collected packages: tokenizers, singledispatchmethod, sentencepiece, primePy, einops, docopt, commonmark, antlr4-python3-runtime, simplejson, shellingham, semver, ruamel.yaml.clib, rich, pyDeprecate, protobuf, omegaconf, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, networkx, multidict, Mako, frozenlist, colorlog, colorama, cmaes, backports.cached-property, async-timeout, yarl, soundfile, ruamel.yaml, resampy, pyannote.core, nvidia-cudnn-cu11, huggingface-hub, alembic, aiosignal, transformers, torch, pyannote.database, optuna, librosa, hyperpyyaml, hmmlearn, aiohttp, torchmetrics, torchaudio, pytorch-metric-learning, pyannote.pipeline, pyannote.metrics, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, torch-audiomentations, pyannote.audio, whisperx\n","  Attempting uninstall: rich\n","    Found existing installation: rich 13.3.3\n","    Uninstalling rich-13.3.3:\n","      Successfully uninstalled rich-13.3.3\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 3.1\n","    Uninstalling networkx-3.1:\n","      Successfully uninstalled networkx-3.1\n","  Attempting uninstall: soundfile\n","    Found existing installation: soundfile 0.12.1\n","    Uninstalling soundfile-0.12.1:\n","      Successfully uninstalled soundfile-0.12.1\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.0+cu118\n","    Uninstalling torch-2.0.0+cu118:\n","      Successfully uninstalled torch-2.0.0+cu118\n","  Attempting uninstall: librosa\n","    Found existing installation: librosa 0.10.0.post2\n","    Uninstalling librosa-0.10.0.post2:\n","      Successfully uninstalled librosa-0.10.0.post2\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 2.0.1+cu118\n","    Uninstalling torchaudio-2.0.1+cu118:\n","      Successfully uninstalled torchaudio-2.0.1+cu118\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n","torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n","torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n","tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n","googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-translate 3.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-firestore 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-datastore 2.15.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-bigquery 3.9.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-bigquery-storage 2.19.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Mako-1.2.4 aiohttp-3.8.4 aiosignal-1.3.1 alembic-1.10.3 antlr4-python3-runtime-4.9.3 asteroid-filterbanks-0.4.0 async-timeout-4.0.2 backports.cached-property-1.0.2 cmaes-0.9.1 colorama-0.4.6 colorlog-6.7.0 commonmark-0.9.1 docopt-0.6.2 einops-0.3.2 frozenlist-1.3.3 hmmlearn-0.2.8 huggingface-hub-0.13.4 hyperpyyaml-1.2.0 julius-0.2.7 librosa-0.9.2 multidict-6.0.4 networkx-2.8.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 omegaconf-2.3.0 optuna-3.1.1 primePy-1.3 protobuf-3.20.1 pyDeprecate-0.3.2 pyannote.audio-2.1.1 pyannote.core-4.5 pyannote.database-4.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-2.3 pytorch-lightning-1.6.5 pytorch-metric-learning-1.7.3 resampy-0.4.2 rich-12.6.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7 semver-2.13.0 sentencepiece-0.1.98 shellingham-1.5.0.post1 simplejson-3.19.1 singledispatchmethod-1.0 soundfile-0.10.3.post1 speechbrain-0.5.14 tokenizers-0.13.3 torch-1.13.1 torch-audiomentations-0.11.0 torch-pitch-shift-1.2.3 torchaudio-0.13.1 torchmetrics-0.11.4 transformers-4.27.4 whisperx-2.0 yarl-1.8.2\n","Found existing installation: torchtext 0.15.1\n","Uninstalling torchtext-0.15.1:\n","  Successfully uninstalled torchtext-0.15.1\n","Found existing installation: torchaudio 0.13.1\n","Uninstalling torchaudio-0.13.1:\n","  Successfully uninstalled torchaudio-0.13.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.9/dist-packages (1.13.1)\n","Collecting torchvision==0.14.1\n","  Downloading torchvision-0.14.1-cp39-cp39-manylinux1_x86_64.whl (24.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchaudio==0.13.1\n","  Using cached torchaudio-0.13.1-cp39-cp39-manylinux1_x86_64.whl (4.2 MB)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1) (8.5.0.96)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1) (4.5.0)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1) (11.10.3.66)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.14.1) (8.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision==0.14.1) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision==0.14.1) (2.27.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.6.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.40.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1) (1.26.15)\n","Installing collected packages: torchvision, torchaudio\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.15.1+cu118\n","    Uninstalling torchvision-0.15.1+cu118:\n","      Successfully uninstalled torchvision-0.15.1+cu118\n","Successfully installed torchaudio-0.13.1 torchvision-0.14.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting language-data\n","  Downloading language_data-1.1-py3-none-any.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting marisa-trie<0.8.0,>=0.7.7\n","  Downloading marisa_trie-0.7.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from marisa-trie<0.8.0,>=0.7.7->language-data) (67.6.1)\n","Installing collected packages: marisa-trie, language-data\n","Successfully installed language-data-1.1 marisa-trie-0.7.8\n"]},{"output_type":"stream","name":"stderr","text":["Using device: cuda:0\n"]}],"source":["#@markdown # **Install libraries** 🏗️\n","#@markdown This cell will take a little while to download several libraries, including Whisper.\n","\n","#@markdown ---\n","\n","! pip install git+https://github.com/openai/whisper.git\n","! pip install pytube\n","\n","# WhisperX\n","!pip install git+https://github.com/m-bain/whisperx.git\n","!pip uninstall torchtext torchaudio -y\n","!pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1\n","\n","!pip install language-data\n","import langcodes\n","\n","import sys\n","import warnings\n","import whisper\n","from pathlib import Path\n","import pytube\n","import subprocess\n","import torch\n","import shutil\n","import numpy as np\n","from IPython.display import display, Markdown, YouTubeVideo\n","\n","device = torch.device('cuda:0')\n","print('Using device:', device, file=sys.stderr)"]},{"cell_type":"code","execution_count":3,"metadata":{"cellView":"form","id":"1zwGAsr4sIgd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681308984798,"user_tz":-480,"elapsed":51562,"user":{"displayName":"廖得凱","userId":"16578409113451863412"}},"outputId":"5881de1a-358f-4fd0-ebf9-f00aa68bbb2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#@markdown # **Optional:** Save data in Google Drive 💾\n","#@markdown Enter a Google Drive path and run this cell if you want to store the results inside Google Drive.\n","\n","# Uncomment to copy generated images to drive, faster than downloading directly from colab in my experience.\n","from google.colab import drive\n","drive_mount_path = Path(\"/\") / \"content\" / \"drive\"\n","drive.mount(str(drive_mount_path))\n","#drive_mount_path /= \"My Drive\"\n","#@markdown ---\n","drive_path = \"MyDrive/IGDS/Whisper-Output\" #@param {type:\"string\"}\n","#@markdown ---\n","#@markdown **Run this cell again if you change your Google Drive path.**\n","\n","drive_whisper_path = drive_mount_path / Path(drive_path.lstrip(\"/\"))\n","drive_whisper_path.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":64},"id":"TMhrSq_GZ6kA","executionInfo":{"status":"ok","timestamp":1681309049904,"user_tz":-480,"elapsed":65111,"user":{"displayName":"廖得凱","userId":"16578409113451863412"}},"outputId":"432d86a6-93c7-4371-a2fc-276dc95132e1"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████████████████████████████████| 2.87G/2.87G [00:18<00:00, 171MiB/s]\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**large-v2 model is selected.**"},"metadata":{}}],"source":["#@markdown # **Model selection** 🧠\n","\n","#@markdown As of the first public release, there are 4 pre-trained options to play with:\n","\n","#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n","#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n","#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n","#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n","#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n","#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n","#@markdown | large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n","\n","#@markdown ---\n","Model = 'large-v2' #@param ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large', 'large-v2']\n","#@markdown ---\n","#@markdown **Run this cell again if you change the model.**\n","\n","whisper_model = whisper.load_model(Model)\n","\n","if Model in whisper.available_models():\n","    display(Markdown(\n","        f\"**{Model} model is selected.**\"\n","    ))\n","else:\n","    display(Markdown(\n","        f\"**{Model} model is no longer available.**<br /> Please select one of the following:<br /> - {'<br /> - '.join(whisper.available_models())}\"\n","    ))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"xYLPZQX9S7tU","outputId":"d2d88052-07f0-42a0-ccb3-068c0e5a1ad4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/10cD9xw5IjA.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/ZksvP9XqCPI.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/0Txx7A4aPnY.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/pQnU1gXF9e0.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/0Dk8MAxGcug.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/XV6z-1nnIyY.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/uhLAFukLc6s.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/Bdu4HdebuPs.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/cAUqDKwAZEs.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/lxW3u4MYt4E.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/g8g_CXVdYlo.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/QeI848Gv5tE.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/YAiNzqUYbYs.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/QizRoRLqdb8.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/DFRnz0xl6lI.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/Gbv4ggLIIzE.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/fcEqWob_K7s.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/uIBj0Xhp1SU.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/fxyNPsRLbVE.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/E8JyrxFVpsg.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/yw1TCbA2mDM.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/UMTeD9y3n-c.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/vhDvzkyPnks.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/Cx10-NhSRY0.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/6w9zryHyNAc.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/VWP7uxF7_Bc.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/XhCRSTYRWCc.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/nTpYyQOeuXk.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/e-wYZI72VfY.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/ZYYAqaRF6Rk.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/j3JHRQuG33E.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/A-vM_DqVU9E.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/kuRm-n1A0bo.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/qZ9Jt06JYhk.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/WRWibL95gns.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/V1vA_4qS8fY.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/a6Iye9pjKCc.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/IrYcm_rmRf4.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/u-9EIhLk5Mo.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/9ARfRyV1ou8.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/mkWKDYAJkxw.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/vkEOUaQIPVo.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/dLaQ0pzAm_s.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/l09KS5UKyr0.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/j_TYeIVXOkI.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/o9dWGzb9EfU.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/xRllzsUQz60.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/ZdVekUwr_SI.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/8HzJMoV3iQM.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/7cxkEC1qrls.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/DtBT1cK33sI.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/ILUjGmk7e6Q.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/qgclNVTtRkw.mp3 selected for transcription.**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**/content/drive/MyDrive/IGDS/IGDS_Audio/OPIPEcdMDvU.mp3 selected for transcription.**"},"metadata":{}}],"source":["#@markdown # **Video selection** 📺\n","\n","#@markdown Enter the URL of the Youtube video you want to transcribe, wether you want to save the audio file in your Google Drive, and run the cell.\n","\n","Type = \"Google Drive\" #@param ['Youtube video or playlist', 'Google Drive']\n","#@markdown ---\n","#@markdown #### **Youtube video or playlist**\n","URL = \"https://www.youtube.com/watch?v=7NpLxkkeh6Y\" #@param {type:\"string\"}\n","store_audio = True #@param {type:\"boolean\"}\n","#@markdown ---\n","#@markdown #### **Google Drive video, audio (mp4, wav), or folder containing video and/or audio files**\n","video_path = \"MyDrive/IGDS/IGDS_Audio\" #@param {type:\"string\"}\n","#@markdown ---\n","#@markdown **Run this cell again if you change the video.**\n","\n","video_path_local_list = []\n","\n","if Type == \"Youtube video or playlist\":\n","    \n","    try:\n","        list_video_yt = [pytube.YouTube(URL)]\n","    except Exception:\n","        try:\n","            list_video_yt = list(pytube.Playlist(URL).videos)\n","        except Exception:\n","            raise(RuntimeError(f\"{URL} isn't recgnized.\"))\n","    \n","    for video_yt in list_video_yt:\n","        try:\n","            video_yt.check_availability()\n","            display(\n","                YouTubeVideo(video_yt.video_id)\n","            )\n","        except pytube.exceptions.VideoUnavailable:\n","            display(\n","                Markdown(f\"**{URL} isn't available.**\"),\n","            )\n","            raise(RuntimeError(f\"{URL} isn't available.\"))\n","        video_path_local = Path(\".\").resolve() / (video_yt.video_id+\".mp4\")\n","        video_yt.streams.filter(\n","            type=\"audio\",\n","            mime_type=\"audio/mp4\",\n","            abr=\"48kbps\"\n","        ).first().download(\n","            output_path = video_path_local.parent,\n","            filename = video_path_local.name\n","        )\n","        if store_audio:\n","            shutil.copy(video_path_local, drive_whisper_path / video_path_local.name)\n","        video_path_local_list.append(video_path_local)\n","\n","elif Type == \"Google Drive\":\n","    # video_path_drive = drive_mount_path / Path(video_path.lstrip(\"/\"))\n","    video_path = drive_mount_path / Path(video_path.lstrip(\"/\"))\n","    if video_path.is_dir():\n","        for video_path_drive in video_path.glob(\"**/*\"):\n","            if video_path_drive.is_file():\n","                display(Markdown(f\"**{str(video_path_drive)} selected for transcription.**\"))\n","            elif video_path_drive.is_dir():\n","                display(Markdown(f\"**Subfolders not supported.**\"))\n","            else:\n","                display(Markdown(f\"**{str(video_path_drive)} does not exist, skipping.**\"))\n","            video_path_local = Path(\".\").resolve() / (video_path_drive.name)\n","            shutil.copy(video_path_drive, video_path_local)\n","            video_path_local_list.append(video_path_local)\n","    elif video_path.is_file():\n","        video_path_local = Path(\".\").resolve() / (video_path.name)\n","        shutil.copy(video_path, video_path_local)\n","        video_path_local_list.append(video_path_local)\n","        display(Markdown(f\"**{str(video_path)} selected for transcription.**\"))\n","    else:\n","        display(Markdown(f\"**{str(video_path)} does not exist.**\"))\n","\n","else:\n","    raise(TypeError(\"Please select supported input type.\"))\n","\n","for index, video_path_local in enumerate(video_path_local_list):\n","  if video_path_local.suffix == \".mp4\":\n","      video_path_local = video_path_local.with_suffix(\".wav\")\n","      video_path_local_list[index] = video_path_local # to make the element of array actually change.\n","      result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n","  elif video_path_local.suffix == \".mp3\":\n","      video_path_local = video_path_local.with_suffix(\".wav\")\n","      video_path_local_list[index] = video_path_local # to make the element of array actually change.\n","      result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp3\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","collapsed":true,"id":"-X0qB9JAzMLY"},"outputs":[],"source":["\n","#@markdown # **Run the model** 🚀\n","\n","#@markdown Run this cell to execute the transcription of the video. This can take a while and very based on the length of the video and the number of parameters of the model selected above.\n","\n","#@markdown ## **Parameters** ⚙️\n","\n","#@markdown ### **Behavior control**\n","#@markdown ---\n","language = \"Auto detection\" #@param ['Auto detection', 'Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Assamese', 'Azerbaijani', 'Bashkir', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Breton', 'Bulgarian', 'Burmese', 'Castilian', 'Catalan', 'Chinese', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian', 'Faroese', 'Finnish', 'Flemish', 'French', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Lao', 'Latin', 'Latvian', 'Letzeburgesch', 'Lingala', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Moldavian', 'Moldovan', 'Mongolian', 'Myanmar', 'Nepali', 'Norwegian', 'Nynorsk', 'Occitan', 'Panjabi', 'Pashto', 'Persian', 'Polish', 'Portuguese', 'Punjabi', 'Pushto', 'Romanian', 'Russian', 'Sanskrit', 'Serbian', 'Shona', 'Sindhi', 'Sinhala', 'Sinhalese', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog', 'Tajik', 'Tamil', 'Tatar', 'Telugu', 'Thai', 'Tibetan', 'Turkish', 'Turkmen', 'Ukrainian', 'Urdu', 'Uzbek', 'Valencian', 'Vietnamese', 'Welsh', 'Yiddish', 'Yoruba']\n","#@markdown > Language spoken in the audio, use `Auto detection` to let Whisper detect the language.\n","#@markdown ---\n","verbose = 'Live transcription' #@param ['Live transcription', 'Progress bar', 'None']\n","#@markdown > Whether to print out the progress and debug messages.\n","#@markdown ---\n","output_format = 'srt' #@param ['txt', 'vtt', 'srt', 'tsv', 'json', 'all']\n","#@markdown > Type of file to generate to record the transcription.\n","#@markdown ---\n","task = 'transcribe' #@param ['transcribe', 'translate']\n","#@markdown > Whether to perform X->X speech recognition (`transcribe`) or X->English translation (`translate`).\n","#@markdown ---\n","\n","#@markdown <br/>\n","\n","#@markdown ### **Optional: Fine tunning** \n","#@markdown ---\n","temperature = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.05}\n","#@markdown > Temperature to use for sampling.\n","#@markdown ---\n","temperature_increment_on_fallback = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n","#@markdown > Temperature to increase when falling back when the decoding fails to meet either of the thresholds below.\n","#@markdown ---\n","best_of = 5 #@param {type:\"integer\"}\n","#@markdown > Number of candidates when sampling with non-zero temperature.\n","#@markdown ---\n","beam_size = 8 #@param {type:\"integer\"}\n","#@markdown > Number of beams in beam search, only applicable when temperature is zero.\n","#@markdown ---\n","patience = 1.0 #@param {type:\"number\"}\n","#@markdown > Optional patience value to use in beam decoding, as in [*Beam Decoding with Controlled Patience*](https://arxiv.org/abs/2204.05424), the default (1.0) is equivalent to conventional beam search.\n","#@markdown ---\n","length_penalty = -0.05 #@param {type:\"slider\", min:-0.05, max:1, step:0.05}\n","#@markdown > Optional token length penalty coefficient (alpha) as in [*Google's Neural Machine Translation System*](https://arxiv.org/abs/1609.08144), set to negative value to uses simple length normalization.\n","#@markdown ---\n","suppress_tokens = \"-1\" #@param {type:\"string\"}\n","#@markdown > Comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations.\n","#@markdown ---\n","initial_prompt = \"\" #@param {type:\"string\"}\n","#@markdown > Optional text to provide as a prompt for the first window.\n","#@markdown ---\n","condition_on_previous_text = True #@param {type:\"boolean\"}\n","#@markdown > if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop.\n","#@markdown ---\n","fp16 = True #@param {type:\"boolean\"}\n","#@markdown > whether to perform inference in fp16.\n","#@markdown ---\n","compression_ratio_threshold = 2.4 #@param {type:\"number\"}\n","#@markdown > If the gzip compression ratio is higher than this value, treat the decoding as failed.\n","#@markdown ---\n","logprob_threshold = -1.0 #@param {type:\"number\"}\n","#@markdown > If the average log probability is lower than this value, treat the decoding as failed.\n","#@markdown ---\n","no_speech_threshold = 0.6 #@param {type:\"slider\", min:-0.0, max:1, step:0.05}\n","#@markdown > If the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence.\n","#@markdown ---\n","\n","verbose_lut = {\n","    'Live transcription': True,\n","    'Progress bar': False,\n","    'None': None\n","}\n","\n","args = dict(\n","    language = (None if language == \"Auto detection\" else language),\n","    verbose = verbose_lut[verbose],\n","    task = task,\n","    temperature = temperature,\n","    temperature_increment_on_fallback = temperature_increment_on_fallback,\n","    best_of = best_of,\n","    beam_size = beam_size,\n","    patience=patience,\n","    length_penalty=(length_penalty if length_penalty>=0.0 else None),\n","    suppress_tokens=suppress_tokens,\n","    initial_prompt=(None if not initial_prompt else initial_prompt),\n","    condition_on_previous_text=condition_on_previous_text,\n","    fp16=fp16,\n","    compression_ratio_threshold=compression_ratio_threshold,\n","    logprob_threshold=logprob_threshold,\n","    no_speech_threshold=no_speech_threshold\n",")\n","\n","temperature = args.pop(\"temperature\")\n","temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n","if temperature_increment_on_fallback is not None:\n","    temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n","else:\n","    temperature = [temperature]\n","\n","if Model.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n","    warnings.warn(f\"{Model} is an English-only model but receipted '{args['language']}'; using English instead.\")\n","    args[\"language\"] = \"en\""]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"s9gwytF6uRN-"},"outputs":[],"source":["#@markdown # **Whisper X**\n","\n","from whisperx import load_align_model, load_vad_model, transcribe_with_vad, align\n","\n","vad_filter = True #@param {type:\"boolean\"}\n","vad_onset = 0.5 #@param {type:\"number\"}\n","vad_offset = 0.363 #@param {type:\"number\"}\n","vad_model = load_vad_model(torch.device(device), vad_onset, vad_offset)\n","\n","align_model_name = None #@param {type: \"string\"}\n","\n","word_options = {\"highlight_words\": False, \"max_line_width\": None, \"max_line_count\": None}\n","\n","for video_path_local in video_path_local_list:\n","    display(Markdown(f\"### {video_path_local}\"))\n","    input_audio_path = str(video_path_local)\n","    # VAD+Whisper\n","    result = transcribe_with_vad(whisper_model, input_audio_path, vad_model, temperature=temperature, **args)\n","    # Force Alignement\n","    result_language_code = str(langcodes.find(result[\"language\"]))\n","    print(result_language_code)\n","    align_model, align_metadata = load_align_model(language_code=result_language_code, device=device, model_name=align_model_name)\n","    \n","    result_aligned = align(result[\"segments\"], align_model, align_metadata, input_audio_path, device)\n","\n","\n","    video_transcription = result_aligned\n","    # Save output\n","    whisper.utils.get_writer(\n","        output_format=output_format,\n","        output_dir=video_path_local.parent\n","      )(\n","          video_transcription, \n","          str(video_path_local.stem),\n","          word_options\n","      )\n","    try:\n","        if output_format==\"all\":\n","            for ext in ('txt', 'vtt', 'srt', 'tsv', 'json'):\n","                transcript_file_name = video_path_local.stem + \".\" + ext\n","                shutil.copy(\n","                    video_path_local.parent / transcript_file_name,\n","                    drive_whisper_path / transcript_file_name\n","                )\n","                display(Markdown(f\"**Transcript file created: {drive_whisper_path / transcript_file_name}**\"))\n","        else:\n","            transcript_file_name = video_path_local.stem + \".\" + output_format\n","            shutil.copy(\n","                video_path_local.parent / transcript_file_name,\n","                drive_whisper_path / transcript_file_name\n","            )\n","            display(Markdown(f\"**Transcript file created: {drive_whisper_path / transcript_file_name}**\"))\n","\n","    except:\n","        display(Markdown(f\"**Transcript file created: {transcript_local_path}**\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ad6n1m4deAHp"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}