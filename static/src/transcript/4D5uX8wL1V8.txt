Hello, my name is Colin Wyke.
I worked at Capybara Games working on a game called Below, and this is the rendering of Below, low complexity, high density detail.
So what is Below?
Below is a procedurally generated...
action adventure game. It is inspired by roguelikes of old.
And it's mixed in with handcrafted content. Here's some pictures that I find to be pretty cool to demonstrate the world of Below. The visual makeup of Below is low geometric complexity art in the form of these 3D geometry, high res hand-drawn backgrounds, and an intricate display of moving details. So this is a short outline of what we're going to talk about today. We're talking about making a dark game, dynamic lighting, and the layering transform or skewing method that we used in the game.
And there are zero Below-related puns in this presentation.
Things get dark.
Into darkness.
Below is an intentionally dark game.
We really wanted to have negative space as a prominent visual feature in the game.
And the thing that we're going to talk about today is the difficulties with making such a dark game.
some of the things that we encountered were contouring, banding, and being unable to distinguish from darkness, and the fact that users will adjust their gamma however they see fit when you eventually release the game.
So.
This is an example of contouring.
You can see that there's spaces of very clear areas where there's banding or segmenting of the visual space, which is bad.
We don't want that.
Here is a screenshot which is very, very dark.
But we want it to be very, very dark.
So we want to make sure that it is intentional.
The first thing you need to assume is that the end result will be on a poorly calibrated monitor or viewing condition.
users will destroy the white balance and gamma by cranking the gamma either on their television or within the game.
Therefore, you want to deliver the best quality image up until that point. So one of the first things that we did at the studio to try and level the playing field and make things better for I guess, development of the art was monitor calibration.
And it is hugely important.
It promotes consistent art creation.
And it helped, in our case, reducing false positive bugs because we knew that everyone was going to have a similarly calibrated monitor.
However, it doesn't fix time of day bias, which is the difference between your lighting conditions with outdoor lights.
the color temperature of outdoor light transforming to the indoor light later in the day.
So like indoor, outdoor lighting.
So what we proposed was a gamma correct pipeline.
Through development of the game, it was kind of difficult to figure out exactly how you make a gamma correct pipeline, because there aren't very many examples of exactly how you do it.
So I just wanted to sort of summarize how we did it, and hopefully other people can use that to do the same.
So the first thing was bringing in gamma textures, to your linear space rendering, where you do lighting, your post-processing, your color grading, your user gamma modification and noise, and then you compose that to the back buffer.
On top of that, you render your UI.
And the back buffer will be in gamma space, the same with the UI.
So sRGB is a color space which is perceptually uniform.
It's what most image editing tools, Photoshop, other image editing programs used to encode their color information. You're going to want to make sure to use your albedo textures or your base color textures for your 3D meshes, for your backgrounds. You want all of that to be in sRGB gamma space.
Also of note, the graphics APIs like DirectX and OpenGL cheaply convert between sRGB and linear, and that's something that you're gonna wanna use.
And then finally, the back buffers will also be in this sRGB gamma space.
linear space. So linear space, the reason that it's important is because gamma space is only valid between the values of zero and one. If you want to do any lighting outside of that range, you're going to be clamped to it and it's not going to go, it's not going to end well.
So you want to make sure that you maintain your intermediate results in your whole entire rendering pipeline up to the point where you put it to your back buffer in linear space.
And the types of render targets or texture buffers that you're going to be using to do that are RGBA, well.
Yeah, RGBA16F, R11, G11, B10F, or R10, G10, B10, A2.
The one that we used primarily in below was R11, G11, B10F, because we found that that had the best.
It had the best performance per cost ratio.
So linear space, the thing that's important is if you're gonna do any effects, post-processing effects, or lighting effects that are physically simulated, you definitely need to be working in linear space.
So anything such as depth of field, bloom, motion blur, color grading, tone mapping.
all of those you're going to need to be doing in linear space. So one of the problems that we had in gamma space was we realized that our color grading was introducing artifacts. And that was the contouring that I showed you before. So we decided that we needed to move our color grading into linear space. And what this did is it preserved the precision of your data in your render target to the final transformation.
So one of the ways that you can do color grading is by creating a lookup table in the form of a 32 by 32 by 32 color cube, which we see right here, which you can then splay into a 1024 by 32 strip.
which we see here, which you can put on top of a render from your game in Photoshop, do your color grading, export that, and then import it to your engine, and use that cube to basically the x, y, z-axis of the cube corresponds to your input color.
And what's in the actual pixel is your output color.
and that's what you can use for your color grading. What we found was that if you do this in Photoshop and you export it with 32 bit color or 24 bit color, 8 bit per channel, your input texture misses a lot of precision and will introduce a number of artifacts.
So what we decided to do was create an in-game color grading tool instead, which you see right here.
And it has all of the necessary functionality that we needed for doing the color grading from Photoshop.
And it automatically converts the color values from linear to, or from gamma space to linear space.
On top of that, this allowed us to create animations for interpolating between color grading scripts, which we use in game play to express a number of different in-game events.
And we're still using the color cube internally.
We just create the color tube and generate the transformation based on the color grading values that we have from our in-game editor.
And this helps a lot with the filmic look of Below.
And it became a pretty important feature for the development of Below.
So this is a picture of the color grading before the in-game editor and the better use of precision.
And this is after, which honestly, with these projectors, is probably hard to tell.
But that's a pretty good example of why you want to make sure that you're doing the best that you can before you actually release your game.
So another thing that was a pretty prominent or important effect that we used in the game was a noise or film grain, which we tried initially in the form of a shader, which has burns, scratches, and Perlin noise.
But we found it to be extremely expensive.
And sometimes, every once in a while, when the seed got to exactly the right spot, you would see a repeating pattern, which definitely didn't want.
We tried dithering and that also, it was just too video gamey. It makes it feel like it's an older, I mean, we're trying to make a game that is inspired by old roguelikes but not that much.
So what we ended up landing on was a noise generator called the WangHash, which uses binary operators.
You can see the code right here.
I'm not gonna explain all of it, but it uses binary operators, which makes it extremely fast.
The disassembly on it is actually pretty, pretty good.
And it had much higher quality than the other noise generation, and there was no distinguishable repeating pattern.
So this was before the noise and after, which you probably can't see too well.
And yeah, the noise, just a second before we move on, the noise was really, really important for breaking down that contouring even further to the point where it was really, really hard to tell any of it at all.
I think we're feeling pretty good about the way that the image quality is currently looking in the game.
So the final conversion to gamma space, you combine your lighting, your color grading, your user gamma, and your noise to the sRGB back buffer, and then you render your UI on top, which the reason you want to render your UI on top is to make sure that you're keeping your UI in sRGB space.
and if it's outputted directly to something that's in sRGB space, doesn't require any lighting, doesn't require your linear space, it's going to skip the conversion from sRGB to linear to sRGB and it's the best way to go.
So moving on, it's part two, let there be light, or making a lighting system from scratch. In the beginning with Below, we started out with a pseudo lighting system where there was a circle of light around the player and it was very, very rudimentary. This was back when the game was heavily 2D and the game was always envisioned to be dark and moody.
The requirements that we would need for creating this lighting system was Well, the reason that we needed to do it was that our game engine only supported a perspective projection and what we'll talk about later with the game is there's an orthographic camera that's required for the skewing method that we use to render the 3D meshes. And there are hand-drawn backgrounds and the combination of both of those things require us to use an ortho camera.
Basically, it's to make sure that the 3D meshes are rooted directly onto the hand-drawn backgrounds, and if there are any sprites, there is no perspective distortion around them.
So what if we added dynamic lighting to the game?
We started with a deferred lighting system, implemented that whole thing.
We found it to be pretty good, but then it was really slow on Xbox because it's really hard to fit your render targets into your ESRAM.
So we decided to try and create a tiled forward lighting system that would leverage the power of the Xbox.
So it starts with a compute shader that reduces the lights to a spatial subdivision of tiles.
In our game, it's 16 by 16 world units, which corresponds to approximately 16 by 16 screen space units.
and we divide it into those tiles, and then for each of those tiles, we cull a list of lights ahead of time before rendering the geometry.
We then pack the lights using a bit mask, and we were able to use Async Compute to do the light culling operation at the same time as we're doing other things, which I believe were the shadow rendering.
and a few other physics-based simulations.
So yeah, and it ended up being a big win.
So yes, when you're lighting, when you're actually evaluating the lighting is when you render your 3D meshes.
And at the time of the pixel shader of your 3D mesh, you unpack the tiles called light list.
And the way that we did that, and I'm always looking for neat optimizations and the best way to use, to leverage.
shader code or shader optimizations and what functionality is out there to best optimize stuff because that's what graphics programmers do often. And the interesting thing that we were able to do was use a count bits function and a first bit high to scan through the bit mask really, really quickly and unpack the lights. And you can see that in that code right there. But I should add that the binary arithmetic for doing these things is really, really fast on GPUs and it's something that was really, really helpful in optimizing the game. The other thing of note is that the tile forward lighting system works really, really well with MSAA. Originally we were trying to use a deferred lighting system that deferred all of your geometry information such as the normals, the albedo, any glow that you're going to do, any world space information to texture buffers, render targets. And if you're using MSAA, it's going to have as many samples as actual memory and that is extremely, it's disproportionately slow for MSAA.
Whereas, tiled forward lighting, your lighting evaluation only happens once per pixel and with the tiled forward lighting system, you're not required to defer as much information and it ended up working out really well with MSAA.
MSAA, we ended up, kind of needing to use because post-process anti-aliasing, we didn't find that FXAA was very good, and temporal anti-aliasing would become really, really difficult to do if you were doing any type of...
It would become very difficult to do if your geometry is really small and you need to, and the temporal aliasing of whether pixels render or don't between frames is going to be extraordinarily high.
So yeah, it helps with small geometry.
So point light shadows, one of the interesting things that we tried was dual paraboloid shadow maps. You can see one of the hemispheres of that right here. We tried doing that and there were difficult seam issues because you actually have two hemispheres that you're sampling between when you're trying to check for your shadows. And any time the geometry goes through the center of the light, it would warp.
the vertices and create a lot of interesting artifacts. So we decided to use cube maps. Cube maps, the downsides are you've got six faces to render and below a typical scene is hundreds of meshes and that's a lot of geometry to render.
Therefore, your CPU time is extremely valuable and gathering lists of geometry is something that you don't want to spend a whole lot of time doing. So here's a little animated gif of what one of those lights shadow cast would look like. And What we did to optimize this process was create a conservative volume of your lights and your camera.
And we multi-threaded the culling of the lists of that geometry to run simultaneously across many cores on the Xbox.
And it ended up working out really, really well.
And it was a good performance win as well.
So one of the other interesting point light shadow optimizations that we had was we used a geometry shader to replicate the geometry to all of the different faces of the cube map.
And we were able to do that in a single draw call.
So what was the impact of creating this lighting system in Below?
One of the things was, as a result, light became a resource in our game that it's craftable.
It's something that you can create.
It's something you get from defeating enemies, which allowed us to make really dark areas that, as you can see here, it's almost pitch black.
So.
It also allowed us to think about what happens if the player has this control over light.
Can we allow them to light up a room, essentially creating a breadcrumb path for them to follow?
And it gave us the confidence to make the game even darker than we initially anticipated.
And then...
Sorry, I'm running a little late.
So another issue, or another thing that we found by creating this lighting system was it allowed us to interact with, have enemies interact with light.
It allowed us to do things like create light volumes, which we then used to activate switches, and we even created this thing where lights will reveal hidden objects and that whole thing piggybacks on the lighting system and it extends all the way to lights that are actually hidden by other lights. So you can use a light to reveal this hidden world that's filled with lights that will then, for example, in these videos, illuminate the character whereas it would otherwise not be visible.
So the moral of the story is that if you're going to add the ability to create light in your game for gameplay, you better make sure that it scales as well as possible.
So here is part three, Things Get Skewed.
We're gonna talk about the skewing method of how we rendered the 3D meshes in the game.
And how it gives, sorry.
If you wanna know how it gives the authentic and original perspective of the game from an artistic perspective, go see my coworker Dan Cox's talk on Thursday, March 2nd.
room 201 in this building, I believe, at 3 p.m.
And this is a picture of a very, very shameless pug.
So the origin for our skewing in the game is that every time you die, you come back as a new character so we wanted to demonstrate that with randomized outfits and in order to do that, originally we had sprites, 2D sprites, and if you were to try and do all of those outfits in sprites, it would be an immense amount of data, it would just, not work out, it wasn't possible.
Therefore, we tried to figure out how we could render 3D meshes with the ortho camera because of the hand-drawn backgrounds.
And the first thing we tried was rotating everything along an axis, the one parallel or tangent to the camera, which was OK at first.
However, the shadows that we added, which came later, didn't match.
And that was a problem.
The other problem was that when geometry interacted with other geometry, it created a lot of issues.
Yeah, and then finally, there were issues with attachment where if you attach an object to another 3D object with the rotation, it became really difficult to make sure that that worked properly.
So, in our game, the skewing method that we used is the camera is top down on the y-axis and the screen space coordinates are x and z.
So what we do is we move the vertex position in the Z axis based on the Y position.
And then we push the vertex position in the Y axis towards the camera based on the Z world position.
And we do this for all 3D geometry that's been rendered.
And this is a demo of how it works.
That was the regular game view.
Now this is our 3D world view.
This is what it looks like without any of the skewing.
And then when we press the magic button to do the skew, you can see that it pushes it all towards the camera and aligns it in a ways so that it works.
almost as if it was rendering towards the camera.
But it allows us to create a depth buffer, which has all of the important information that we need to do effects such as screen space ambient occlusion, depth of the field, volumetric fog of war.
All of these things require a world space position and with a depth buffer that contained all of this information, we were able to do all of these things.
And here is a example of the game before the fog of war.
And then once it's been composited in, you can see the pillars are rising out of the fog. It looks pretty cool. And then we're able to, with the world space information, deduce or create the SSAO effect. In conclusion, indie game development is sometimes crazy. And A lot of the things that we talked about today weren't necessarily the fastest, most optimal way of creating something.
But sometimes you take the easy route because you're small teams, you don't have the most time in the world, and you're just trying to do the best that you can.
And lastly, with respect to the layering transform, Sometimes you get stuck with weird systems, and sometimes reinventing it would take too much time or take too much risk. But sometimes these weird things give your game its charm, and I think Below is a pretty charming game. Thanks for coming.
So we have some time for questions. Does anyone have any?
Cool. Um, well I will go over. Oh, right. Hello. Uh, so I noticed you do most of your lighting computation on the GPU.
But then you use that for gameplay mechanics.
Did that cause any challenges?
Because all of the final computations, like the volume, were on the GPU, correct?
And so did you have to move that data from the GPU to the CPU to do the gameplay based on it?
Yeah, sorry.
So to answer your question, we did all of the gameplay based lighting computations on the CPU.
All of the lighting information was coming in from our Lua light scripting system.
We brought all of that light information in, we put it through to C++, and then from there, sent the light information to the GPU.
But basically in between when you do the light culling, the culling is doing a bunch of those checks on CPU, so it seemed like a pretty good place to figure out those results.
Most of the time when we were referencing the gameplay-based light culling, checks, those were happening multi-threaded while we were doing the calling of the lights on CPU.
Thanks.
Any more questions?
I'm probably gonna go over to the other area, so if you have questions and you don't wanna ask here, I can answer them there.
Thank you again.
