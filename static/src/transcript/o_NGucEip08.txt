Thank you so much for coming.
It means a lot to us.
So this is a talk called Little Learning Machines, A Game About Training Neural Networks.
Fundamentally, this talk is about reinforcement learning in games and why training agents is actually fun.
And we made a small game about it.
And a lot of the second half of the talk is how to make training fast, which is a very important part of this.
But first off we need to kind of like recognize some prior work Black and white the 2001 classic.
I hope you all know about it Creatures 1996 which was even bigger influence.
I hope everyone knows what that is both of these games Use forms of reinforcement learning actually while you play and then more like more recently modern machine learning has been used in games such as Project Nero and While True Learn, ArtBot, which was made, or we found out just as we entered early access.
Incredible game.
And then LightBot, which doesn't use machine learning, but focused on teaching kind of like simplified models of programming.
But actually our main influence when going into making Little Learning Machines was a reinforcement learning short film we made called Agents.
And it was directed by Pietro Galliano, who is in the crowd and he's the best.
And both of these projects were created by transitional forms Transitional forms was founded with a goal of kind of like human machine empathy and getting machines to understand humans and humans to understand machines I'm Nick my background is in animation and illustration like primarily focusing on kind of like General science education and I helped co-direct this game and we're co-directing this talk.
So I am Dante, I was technical director on this game, and my background is in research and emerging technologies.
One of which, at the time, was reinforcement learning, specifically deep reinforcement learning.
Quick overview of reinforcement learning, don't want to get too far into it, this is the AI Summit after all, but you have an agent, you have an environment, they need to do things together.
Usually, in the form of the agent taking some sort of action that changes the environment, and then that environment changing, and those changes being reflected in what the agent can observe.
So the agent has some information about the environment that it observes.
And the way that the agent changes its own behavior is through reinforcement.
Reinforcement happens through the provision of rewards that can either be positive or negative.
You give an agent positive rewards for doing the things that you want it to do, and negative rewards for doing the things that you don't want it to do.
Over time, the agent optimizes and then develops a policy or a behavior that allows it to optimize this amount of reward.
Cool.
So one of the places I'd like to start is with this.
And this was one of the primary inspirations for our director, Pietro, when he was coming up with what we saw in RL as being inspirational.
This idea that within just these pixels, you can see that the blue and the red pixel are collecting these green apples.
And they're shooting yellow lasers at each other.
But even within this little 2D simulation, there's conflict.
And it's kind of interesting.
You can root for one.
Blue is my favorite, by far.
And this is a pattern that repeats during reinforcement learning.
You can see this is an example of a clean RL where it's trying to learn how to walk in the Mojoko environment.
And yeah, there's strife there.
The only reward function for this one was go right.
And RL doesn't have the intuitions we have.
just does what it can.
And we thought that this was very cute.
And then finally, we also saw RL at the time kind of exceed our expectations.
Again, this is an old paper, but OpenAI trained a team of five players to play Dota, which is If you've played Dota, you know that's really hard to collaborate at Dota.
So yeah, RL kind of created a model that played better than we did.
So that was amazing.
So all this kind of came to a head when in 2019, 2020, 2019, we released this film called Agents.
Yes, and so Agents was, like I said, the main inspiration going into Little Learning Machines.
It's a short film about reinforcement learning agents.
as well.
move these kind of creatures around this planet, try to balance it out.
And agents live on this place.
It's weird.
It's spinning.
It's kind of like being influenced by your interaction and the weight.
But actually, gravity goes down, which is weird.
But also, that means the agents can fall off.
So it's quite a dangerous place that they're trying to kind of coexist, cooperate, but also kind of create a bit of conflict around this.
And what's special about Agents is that the characters, the actors, were actually trained through reinforcement learning.
And this was back in 2019, like we just said.
They're just neural networks.
They're just neural networks.
And more importantly, users could actually interact with the pre-trained models in VR or literally on their smartphone.
And it was special for me at the time, because I had no idea what any of this was.
And it kind of inspired, I guess, the next six years after.
So the process was very classic.
In this video you can kind of see agents training.
There's a simplified environment, all running in parallel, and basically developers at the time, Dante and Alex, would set up these trainings and leave the agents training overnight and hoping for some sort of success.
Often that took 16, 18 hours plus with our small team, and for a long time we didn't see anything.
But one day we all came into the studio in the morning and we saw the runner.
He might not be that impressive to any, like, AI enthusiasts in here, but to us this was, like, a big deal.
And we love the runner.
He's kind of, like, turned into the meme of the office.
And it was really the first time that the team, like, really started feeling a connection and kind of, like, a love for this kind of, like, digital being.
If you can see what he just did, he tricked the other four agents to fall off by going to the other side.
and then just continue to run infinitely.
And the more behaviors we trained, the more we kind of like developed a lot of like love and pride and attachment for these like little beings.
But more than anything we found that watching intelligence and behavior kind of emerge out of the randomness they start with is in itself a uniquely rewarding experience and unlike anything we kind of like previously worked on or kind of experienced in games.
So we started to think about how we could use this in a project in the future.
Yeah, so we were a small team.
And once we started coming together and seeing these things happen, the mornings were just cool.
It was like, oh, what did they do last night?
One time, we gave them a reward to get as close to the middle of the planet as possible.
And they learned how to clip through the geometry, which was crazy.
But the problem we were having is, OK, we show up and we show the simulations.
And then the whole team has ideas.
Like Nick had ideas, Pietro had ideas, but everyone.
I think at one time our marketing manager, Michelle, came and she's like, oh, you want them to fight more?
Just increase the scarcity or have less flowers.
And then we did that, and it worked.
And the film became more interesting.
So the whole idea is Alex and I were gatekeeping anyone from running whatever simulations they wanted.
And that wouldn't work.
So we wanted to share it.
We wanted to make it so that they can run their own simulations.
But we had some problems.
So the first problem is that it was slow.
16 hours is a lot.
It's a long time.
I can't even wait for my crock pot to go that long.
I usually just eat raw meat.
And then the other thing was the hardware.
Alex and I had big thread rippers.
Back in the day those were beefy.
And we ran them on that and we had to run We had to run parallel simulations, and that was expensive.
So we either had to pay for cloud compute or pay for expensive hardware, and it wasn't something we could easily do or provision, at least with a small team.
The other thing is, this may not seem like a big deal, but coding is a big barrier of entry, especially for something that we saw as accessible as reinforcement learning.
So having to just code your own reward functions or change the physics of the world or whatever was a big barrier of entry.
It was also non-visual, which is difficult if you have disabilities that prevent you from engaging with textual formats.
It is sometimes difficult to work in a non-visual format.
And then finally, it was just very complex.
This is reinforcement learning, after all.
It's deep reinforcement learning.
There's a lot of Bellman equations and all that stuff.
So having an understanding of of kind of optimizability was a bit difficult to kind of convey well.
It was like, oh, that won't optimize.
It won't converge.
So just to give you an example of how complex things were when we wanted to help someone set things up, it's like, OK, well, you have to install Unity, install Python, set up your virtual environment.
OK, not that one, that other virtual environment.
Set up TensorFlow.
No, not that version of TensorFlow, the one that's not Make sure it supports CUDA, but not for that video card.
OK, get your environment running, do your reward functions, whatever.
Run the simulator.
Oh, you have a bug?
Ah, yeah, you might want to try running it again.
You know what?
I hope you like graphs, because that's what you're going to be looking at, just graphs.
And oh, and command lines, and more graphs, and more command lines.
Shout out to ML agents.
Their tooling was great.
But we wanted to make it way more accessible.
And by way more accessible, I mean really accessible.
So we did it.
We made this game.
We made a whole game out of it.
So yeah, the rest of the talk is going to tell you about Little Learning Machines, where we... Oh, sorry.
We made this game.
Sorry.
I'm looking at the wrong slides.
Yeah, we made this game called Little Learning Machines, which we're going to talk about.
And in this game, we implemented something called real-time reinforcement learning, which is not a new AI concept, but rather a game mechanic.
One that you can hopefully put into your game if you want your players to experience it.
So let me just quickly run you through a live demo.
It was supposed to be a live demo, but Nick doesn't trust me with the computer.
This is an agent that's been trained to water flowers.
You can see it's kind of walking around.
It's kind of walking around, looking at flowers that it wants to watch.
All right, Nick.
Nick moved it.
Put it up here.
We can see that it kind of navigates the area, looks for flowers to water.
But what we're going to do is we're going to reset it, just so you know that it's not pre-trained.
And that's what it looks like when you reset your brain, I know firsthand.
So this agent is now reset.
It's brand new.
Even though it previously was watering flowers, the same network is now completely randomized.
So the actions that it takes are kind of at random, anything.
And what we're going to do now is we're going to give it something completely new to do.
And I want to show you how quick and easy it is to kind of give it a new thing to focus on.
So in order to do that, we have to go to this space called the training cloud.
And in the training cloud, we can set up an environment for our agent to learn.
The first thing we need is obviously our agent.
This one is affectionately named Chungus.
So it gets put right in the middle.
And we're going to remove its old rewards.
It doesn't need them anymore.
And we're going to give it something to do.
So the easiest thing that I can show people in a talk is just collect crystals.
Let's put down, I don't know, 14 crystals and see, can it collect them?
And we don't have to code any sort of A star.
And this agent is not pre-programmed, OK?
We could have trained it to, I don't know, chop dogs with axes.
But instead we're telling it that it loves collecting crystals.
And that's put in there.
We set up the end conditions so that it doesn't go forever.
We saw what happens when you let agents go forever.
And boom, that's it.
We're running a training.
It's reinforcement learning happening on your computer.
Now, that doesn't sound super impressive, but to us, at the moment, this was like, oh yeah, this was, especially since in undergrad, I did a lot of this in labs, it's like, this is the smoothest it's gone in a long time.
And as it trains, you can see right now, it's starting to take random actions.
But little by little, you can see, right now it didn't want that crystal, but little by little, it develops a desire for crystals.
It can develop a desire for blood, too, if you train it that way.
But yeah, you can see that there's a little graph on the left.
There's still graphs.
We couldn't get rid of the graphs, at least not yet.
But it's a little bit more intuitive.
You can visually see the behavior that's happening.
You can get a sense of how the agent is improving.
And within the span of two and a half minutes, We have an agent.
Oh, well, the other thing is you can't really see the behavior that well develop in real time unless you kind of speed it up.
Collecting crystals is like a macro behavior.
So if you speed it up, you're going to get a better sense of kind of the macro decisions that your agent is doing, which is why we added that little speed up there.
But yeah, ta-da, a trained agent in a matter of minutes.
Now, obviously, this is a very simple monotonic problem.
But let's recap.
Was it slow?
No, it was pretty fast.
The setup is what took the most part.
It took like a minute and a half, but the whole training happened in a matter of like a minute or so.
Did we need a special hardware?
No, this was trained on an M1 Mac, but if you have a Surface Pro 3 from 2019, it'll work just fine.
Did we have to do any coding?
No.
All we did was select the reward functions visually, which clearly the whole process was a visual one.
And finally, was it complex?
Well, there we have a bit of a caveat.
And for the rest of the talk, we'll talk about some of the complexity that arises in trying to set up a world and behaviors like this.
So I'll let Nick introduce that.
Yeah, so first of all, our approach requires kind of like four steps.
The first being designing an actual world for real-time reinforcement learning.
The second being designing the agents.
The third, designing the training experience itself and what kind of like training loop the players will go through.
And then fourth, like packaging it all into a game.
But before we start that, I just need a few key learnings from agents that really kind of like influenced early decisions on how we kind of like set those systems up.
First off, there's in agents, there was a fundamental mismatch between how our team experienced training the actual reinforcement learning agents, seeing them learn and seeing their behaviors kind of emerge out of nothing versus audiences who like viewed these pre-trained models for the first time and may have not actually understood anything about machine learning.
So it was kind of like, it's difficult to understand machine learning in general, or what a pre-trained model is, what they see, what they're doing, and fundamental, how they're trained.
It's a lot easier to understand them when you're actually there in the room experiencing and watching them learn.
Second, agents had this really beautiful kind of story with a lot of different story states, which each story state kind of required different behaviors from the agents themselves.
So agents were expected to kind of like hit specific kind of behaviors at specific different story beats, which made it kind of really difficult to craft them.
And it didn't necessarily allow them to flourish on their own.
And on top of that, as you probably noticed, the environmental conditions we placed the agents in, in agents, were extremely difficult and very complicated.
Any action they took could easily, like, lead to their death.
On top of that, even their actual actions they could take to survive were quite limited.
They could move, They could consume the flower, and they could also headbutt each other, all which combined to making it quite difficult to find really beautiful behavior out of them.
And that led to the main thought, which is the agents can only be as smart as their environment lets them be.
So that's the main thing going into designing a world for little learning machines.
So when designing this world, like this.
we like in this design process were kind of like based on playfulness.
We want agents to interact with each other, whether it be through competition or cooperation.
And so because of that, the world itself needed to be able to support many agents running at the same time.
We ended up shipping with nine very diverse kind of brains in the experience.
But the system actually allows up to 100 agents operating in real time at all.
Like, inherently, they are pretty dumb.
In fact, they start off only taking random actions.
So the environment needed to be simple enough for a small brain to understand it, and diverse enough that it's not easily optimizable.
But most importantly, we want to encourage more generalized behavior over, like, over-optimized zombies.
So how do you encourage generalized behavior?
How do you create a dynamic world?
The same way you do it for your players.
You add the dynamism to kind of a component of the world and then you stretch that out.
So what we ended up doing is we stole a page from the book of Minecraft and we created items.
which are these kind of things that can exist on one of the blocks within the world and they allow the agent to perform dynamic behavior without the agent having to be dynamic itself.
You can have the same agent perform different types of behavior depending on the item that it's holding.
Yeah, here's a couple examples.
There's like a bat, an axe.
Whatever that is at the bottom, there's a dog.
These are all items within the world of little learning machines.
And then the world itself is supposed to be editable by the user.
The user journey is to create these environments in which your agents learn, and we wanted to make the environment as editable by the player as possible.
So we took that concept of voxels and items and stretched it to as much as we could.
But there was another reason we did this and this is Probably one of the trickiest parts of the talk is because making the world this way lets you train really fast The the world is actually not 3d.
It's a 2d type soco bound tile type grid Little modifications in that there's like one item per cell and then the agents kind of move around within those within those 2d cells There's height, but the height mostly is used for navigation And there's no tunnels allowed.
And the reason we did this is so that we could, A, discretize the space, and discrete observations for neural networks are much faster to train than analog ones.
And then the other thing is that, or continuous ones, the other thing is that it allowed us to to reduce the dimensionality of the space.
Instead of having to store 3D information and having to do a lot of the reasoning of the agent having to do with 3D information, it allowed the agent to make decisions on a 2D space, which is, again, trains a lot faster.
And there was no end problem to the user.
The user still sees a 3D world, but the agents and the AI train a lot faster because they're able to work on reduced dimensions.
The representation drove the design.
But not only did we discretize space, we also discretized time.
Having an agent make a decision every couple of frames was not possible for the kind of scale... Again, don't make an MMO.
the kind of scale that we were hoping to reach with this kind of game, and also having it run on the players' computers.
And the more we kind of discretized time, the faster it trained.
So what we ended up doing is we ended up dividing actions into turns.
So every time anything in the world has an effect on other parts of the world, any item has an effect on an item, or an agent has an effect on an item, these all happen within turns.
And that lets the evaluation of the simulation happen much faster.
Yeah, but not only that, but the key kind of focus on kind of making training fast was also the design of the agents themselves.
So the design of the agents, the agents in Little Learning Machines, we call them Animo.
And we wanted to create a goofy character that could demonstrate the transition from non-intelligence to intelligence, semi-intelligence.
The design is directly influenced by the networks themselves with their actions representing their body, which you can put some gloves, shoes on, or some eggshell.
Their network is their crystal brain.
And their observations are represented by their kind of like see-through bulbous head.
It was really important in the design that we like made sure you could see the brain at all times because it's kind of like the kind of like core consistency of them.
But the main goal was converting the discretized 2D space and the discretized kind of time space into a 3D representation that the audience could kind of like understand and learn from.
For a long time we didn't actually know how smart these agents could become, and that was a huge barrier and a huge struggle.
So we created a style that kind of showed that they're fallible machines and quite dumb at times.
They need to be cute, silly, and most of all, forgivable, because they're going to spend a lot of time running into walls.
and leaving the heavy lifting, basically portraying their intelligence through the animation system.
Basically filling the gaps of the 2D Soccuban style brains and their turn-based decisions, like filling in those gaps with kind of like animations that could communicate the intent behind those decisions so that players could actually observe what they're doing and learn from what they're doing.
So I'm going to break down some of the agent's architecture, just really quickly.
It's going to get a little technical, but hopefully we can get through it in some quickness.
The first thing is the vision.
As I showed you earlier, the agents can see an area around them.
Some agents can actually see infinitely far away, but they can only see a certain number of items.
And the way that gets processed is that each of these observations is this stack of specific features that they can observe.
So, for example, this agent that you see on the top left can observe a range of 5x5 around them, and within that range they can observe maybe the terrain, whether the medium that cell has, any items, any creatures, and anything that the creatures themselves are holding.
And then each one of these features turns into a set of Boolean values.
There are still some kind of floating point values that get passed in, but we found that the more we stuck to, again, Boolean values, the easier it was to train the networks.
Originally, we had a one-hot encoding for every single item in the game, but that quickly blew up as the number of observations needed to grow with everything that was in the game.
So eventually, we landed on this 20 questions model, which are these handcrafted embeddings where items that are similar can share similar embeddings.
And this also actually sped up training again, because it allowed an overlap in the types of behaviors that certain agents were able to develop.
So if you want an agent to not step on flowers and not step on fire, then they can generalize that behavior more easily through finding commonalities in the features.
So all of these bits get stacked into a big stack and they get passed into the network.
These diagrams are in the game.
We tried to explain this to the users to some degree to the point where they can be confidently curious about some of the agents that they train with, but we really don't think that this is that important.
Really, the point of adding these different kinds of learning to the game is so that the players get a sense of a different taste of the kind of representations of these different approaches at collecting observations for an agent.
And that's kind of how the whole game plays out.
Different agents have differences that just make them feel a little bit different.
As I said, all of these observations get tacked onto a network, and this network is just a standard neural network.
It's very small.
You'd be surprised at the level of intelligence that these kind of little networks are able to show.
Our smallest networks, for some of the first agents, we really want to prioritize training.
There's two metrics that we want to prioritize.
The first metric is time-to-first intelligence.
As you saw when the agent was collecting crystals, it happened really, really quickly.
And we need that to hit fast so that players are confident in the fact that they're actually learning.
But then the second metric that we try to balance against is maximum intelligence.
So smaller networks are not going to be able to do some of the more complex tasks in the game, for which you'll have to use some of the more complex networks.
So our agents that we shipped vary from 128, it's two fully connected layers of 128 neurons, and then some of the later agents just have 512 neurons in four layers, and again, fully connected.
There's nothing too special, but just with this alone they're able to do some fairly complex behavior if they're trained appropriately.
One interesting thing we do is that these agents are trained using proximal policy optimization.
We actually use the ML Agents Trainer.
We kind of stripped it out and reused it for our own purposes.
So one of the things that we do is we reset the critic every time we restart the training.
You would think that this would result in slower trainings, but actually as long as the policy is accurate enough, the samples kind of converge fast enough that you don't notice a drop that much.
and it allows you to kind of fine-tune your policy towards the direction which you wanted to take more easily.
A lot of tech words, but finally, after we take all of those observations, we put them into this kind of like network that you can either reset or train continuously.
They result in actions, which is what your character actually does.
So in kind of deep reinforcement learning, of the set of things that we're training for.
In agents, for example, we had multiple outputs, but here we're actually just looking for one.
And the way I like to explain this is you kind of have a controller, and you give it to your little sibling while you're playing with them, and instead of holding the controller with both hands, they press one button at a time with their finger.
That's basically what an agent is doing.
It's pressing one button at a time.
So every turn it's taking one of these seven actions.
That's what it wants to do.
And here's where another kind of trick in optimization happened that let us speed up the training, which was converging actions to have different meanings depending on semantics.
Originally we had more actions, but we found that that was really one of the big limiters on the kinds of behavior that we can train.
And we found that by having actions perform similar types of, have similar results, depending on contextual situations, it allowed us to put the weight of training not on the action decisions, but on the recognizing the different situations, which meant that we could reuse the latter parts of the network to optimize for specific actions.
Sorry, to reuse specific actions.
Like, for example, the same kind of part of the network could be used for, like, watering a flower than it is for chopping down a tree.
And it allows you to train an agent to do multiple things.
And the movement actions were also something where we found quite a bit of success by using these kind of tank controls instead of kind of like what's called cardinal directions.
It allowed us to exploit symmetries in a grid.
So you only need to train in one direction, and then all actions are kind of relative to that direction, which again was exploiting another symmetry.
It actually sped up training by a factor of three, which was pretty useful.
it let agents kind of develop egocentric views of the world.
It's a little bit harder to debug because when you're looking at kind of the agent's observations, you have to take into account which direction they're looking at, but it does result in better, like faster training times.
Yeah, so those were all kind of little tricks in designing your agent so they can train faster.
But now that we have this kind of world and we have this kind of agent, collecting crystals isn't going to do it.
What's kind of like some of the things you can expect out of an agent that's built like this?
And I think Nick can cover the first time we saw that.
So this is Sneaky.
He's my all-time favorite agent.
This was trained during the prototype.
I love him a lot, but he sadly no longer exists.
He's an agent that was trained originally to use multiple items to solve a puzzle.
As you can kind of see, what he's doing is his goal is to collect all of those flags.
And he has access to two tools at once.
He has a block maker to make bridges and an axe that can break down those blocks.
And if you notice it, I think he doesn't make a single mistake.
He does it in the most optimized way.
And this is why I love him.
But sadly, when I put Sneaky into a different environment, with the same puzzle, he does nothing, and he's basically overwhelmed with indecisiveness or something.
At the time, I had no idea what had gone wrong.
What did I do wrong?
But obviously, it became clear that he had over-optimized for that previous environment.
And so rather than understanding anything about the environment, understanding kind of like the IDs of the different kind of items and stuff.
Sneaky had purely basically remembered a set of actions in a row, basically kind of like a cheat code from a Sega game or something.
So obviously we would have to address this and make this clear in the design of the training experience itself.
So on to the designing of the training experience.
Our main goal was to kind of like faithfully recreate training of separate from basically the actual world they live in, which is the sandbox main island and the quest islands that you kind of put them in.
And this meant that we would have a training set versus an evaluation set.
And this was inspired directly by what we saw with Sneaky, is that I was training Sneaky in a very specific environment for like a long time, and at no point did I take Sneaky out of and just test him somewhere else.
At no point did I evaluate his behavior.
So we basically created a training loop from this.
We kind of like based it on a process of tweaking, and observing, and tweaking, and observing.
Starting with tweaking, which is creating a simulation space, setting up an environment, setting the rewards, setting the end conditions, starting the training, and then watching the training happen.
Like watch the animal, and I'm training becomes actually quite intuitive.
Fun!
We didn't know that this was the fun part of the game until very late into production.
Exactly, exactly.
So in this slide I can just talk about like basically setting rewards.
A lot of this experience and a lot of part of the tweaking is setting appropriate rewards for the appropriate environment.
So basically we created this kind of emoji-like system where it's kind of easy to kind of set different very specific rewards based on the kind they're going to break.
They're just going to get confused, obviously.
And that's not actually a big deal, because you can always play it, observe it, remove a reward.
Their networks keep their shape, so you can constantly change them while you're tweaking and observing.
And also, fundamentally, you can get some of the most complex behavior with a single reward in a really well-made environment.
And yeah, so that's setting rewards.
And while you're actually observing, we wanted to make it important that you could see the rewards being kind of accumulated at the right time.
In this case, this little guy is receiving a lot of rewards because he's receiving just one for holding onto the blockmaker to make sure he doesn't drop it, and then a lot when he's kind of using the blockmaker to make a block at the very specific height to gain crystals or to collect crystals.
So you can kind of see and debug the experience just by watching them and seeing them gain these rewards.
And it lets you kind of like figure out if you need to change anything and then so after that you gain these checkpoints So all the simulations are kind of being combined and kind of crushed in the background We got lots of running in the background And so we're averaging out kind of like all the different rewards and we're sending them back in so that you can see kind of like the average growth and progress of of your Anemo at the same time.
So in this video, you can kind of see me just scrolling over.
This is a lot of checkpoints collected.
And you can see the averages of the rewards being collected changing over time.
And most importantly, you can kind of see the ratio of the rewards changing over time, which actually became kind of like that's top tier players looking at the ratio change.
But on top of that, Just going over end conditions really quickly again, we really needed to allow players to control how the simulations were being reset.
Basically the first way to do that is how many turns in a single simulation you can run.
You can run it at 50 turns or up to 200 turns, and this really depends on the kind of behavior you want out of it.
And two, we wanted to end a simulation if a specific item of your choosing no longer exists.
And this is basically to waste no training time.
Training time is precious when we're doing this.
And we also didn't want you to have a chance or the animal to have a chance to collect straggler rewards when it already completed its task.
And that's a no-go as well.
That can lead to awful things.
and If you remember Sneaky and the optimization problem, that's still potentially a problem here.
Fundamentally, you can kind of start a training and move an axe a bit further or tweak the environments to add a bit of noise on your own, but by starting and observing and stopping and starting, that actual process becomes quite a lot quite quickly.
So we designed kind of the ability to kind of create a bit of noise in between resets Sadly, we call this wiggle and we can't ever turn back But basically players can control how much they want if they want their environment to change in between resets if not able to memorize that I need to walk forward, pick up this block maker, and then walk forward again and use it.
So just really trying to get them to operate in the training space, but then operate in the real world with actual integrity.
And just to kind of break down what's happening while you're seeing these kind of loops happen, even though you're seeing kind of the training happen in front of you, the training doesn't run in Unity itself.
We actually started by doing that, but there was a lot of problems, the least of which was we had to write our own trainer, and that's harder than you may think.
The other thing is like, sharing resources within the same process can often get in the way of the game itself, and we wanted to keep the game kind of running smooth while the training happened.
So we ditched the whole thing and sent it to another process.
So this, what happens when you hit play, it actually starts a Python process in the background, which actually does the training.
And originally we were, as I said, we were using the ML Agents Unity trainer, but turns out running Unity is really expensive.
So we got rid of Unity, too.
Well, we didn't get rid of the Unity that you're seeing when you're playing the game, but we got rid of the actually Unity for running the simulation.
What we ended up doing is we ended up taking all of the logic of the game and shipping it to Python so that the logic can run in Python completely uninterrupted.
making use of all of those optimizations we've been mentioning throughout the talk.
This made it so that the training runs in, like, blazingly fast.
When you're watching a single, like, animo do a motion, it's performing hundreds and hundreds of steps.
It takes, like, a hundred steps for every step it takes in-game.
And that varies by your computer, but even on the slowest computers we were able to get a significant speed up by doing this.
It's actually extremely laborious, and I go into detail later on in the paper you'll see at the end of the talk, but it took a lot of engineering to embed C Sharp into Python and be able to run the same logic that we use to run the game within the Python backend.
And the other thing that this does too is that if we ever wanted to ship to mobile, it wouldn't be too difficult to take that architecture and just send it to the cloud and then just you don't have a mobile game where the training happens in the actual cloud, not just the training cloud.
So yeah, this kind of architecture let us really speed up training.
And I should have put a metric slide, but I'm just going to tell you.
That sneaky behavior that you saw that Nick trained, it took 30 minutes to train that, to get someone to build a block maker and then use an axe to chop down either trees or obstacles.
And that wasn't even a generalized behavior.
In the launch version of Little Learning Machines, you can get that kind of behavior in like five minutes or less.
And all the time, you're kind of watching your agent kind of get stuck.
You have to change a couple things, and you have to run it again.
And it creates a lot more of an interactive experience, which actually makes it viable as a game mechanic, in our opinion.
So just to kind of summarize all of the tricks, the big bag of tricks on how to speed up your RL so you can put it in your game and have your players experience it while you're playing.
Number one, discretize your space.
It's much easier to make decisions about a discrete space.
For example, if this was a standing room, would you know where to sit or stand?
It's much easier if there's chairs that are discretized.
You can make that decision for yourself.
Number two, reduce your dimensions.
As much as possible, if you can flatten it, do so, because making choices on multiple dimensions increases the arity of the computation you have to do, which uses more neurons.
Discretize your time.
The less decisions you have to make, the better.
Discretize your actions.
Leaving some of the intelligence to the animation and navigation systems, whether you're using a nav mesh or a grid, it really reduces the work that your network has to do.
Exploit symmetries in your world, or design a world that has symmetries, so that your agents can reuse learning that they've done when facing north, and they don't have to relearn it when facing south.
This is especially useful if you have visual observations that use lighting data.
Next, exploit overlapping actions.
For example, this is contextual actions in video games.
Just like players, networks have a limited amount of verbs.
So if you can use those very wisely, it really reduces your training time.
And finally, just optimize your code.
That's not really something that I can give you advice for, but Steve here has a lot of good advice for you if you need that.
And then finally, a whole deck of cards.
But yeah, that's all the tips and tricks on how to make a game that has RL happening in real time in front of the players.
The last part of the talk is about making it into a game.
This is just a little design indulgence where we talk about some of the reasons we made it into the game and why.
We have this little training experience that's very packaged and novel, but what we were really Struggling with especially an early prototype is giving players a sense of well Why should I train or what should I train?
So we kind of decided to go both take two approaches on this and kind of we were divided at the beginning But we decided I'm doing both.
The first one is kind of like a player driven in Intrinsic motivation where we give the players a bunch of items and tools and we create interesting interactions between them and And we leave it up to the players to explore those interactions out of curiosity or a desire to see their machines grow.
But then also to kind of challenge the players and kind of give them a sense of, hey, you can do this with an agent.
We've tried.
We've been able to.
We give them these things called quests.
So we kind of divide the game into two.
So both of these are provided through islands.
And it kind of creates an extension to the training loop we mentioned earlier.
There's this idea of a player progression, where a player reaches a new island, they get new items, new animo maybe, new rewards to train with, and then they kind of get issued a challenge, whether that's intrinsic or extrinsic.
Intrinsically, they're like, oh, there's a skull here.
I wonder how this got here.
Or, hey, this thing makes fire.
What can I light on fire?
And then to be able to use that item, the player can't directly interact with the world, they have to train their agents.
This is similar to what Black and White did for some of their creatures, that they're the only ways in which you can achieve certain elements in the game.
But then also we kind of provided these quests, which is like, hey, do this thing, which we know you can do.
And that gives players kind of the motivation they need to kind of not get like, oh, I don't think the networks are smart enough to learn this, by guaranteeing basically that they are.
And then this kind of ties into the training.
So the whole thing is like the player goes into an island, figures something out.
They're like, oh, I want to do that.
And then they go into the training loop.
And then they're like, oh, I got to tweak, observe.
Is my agent doing the thing I want it to?
Tweak it more.
It's ready.
Bring it down to the island.
have do reinforcement learning in an academic setting, there's a set of things that you have to understand that aren't necessarily immediately intuitive from how you would, for example, train a dog.
You have to realize that, oh, your agent can kind of get stuck doing a certain thing, or you have to teach it to do something before you teach it to do something else.
Some of these are kind of commensurate with training an infant or a dog, but there are specific things that we think are unique to neural networks.
For example, This is the first quest, and we don't want to overwhelm players.
We don't want to introduce randomization right away.
So we introduce the dog, and the dog kind of has randomization built into it for free.
You pet the dog, and the dog runs away.
It's like, come chase me.
Come pet me over here.
And that's basically recreating the same problem for your agent, and it's doing the randomization for you.
So you don't have to modify the environment.
You don't get stuck in the same trap that Sneaky got stuck in.
And then the dog does that.
that skill for you, and you don't have to learn it as a player.
Later on, what's interesting is the second quest, you have to pet the dog in those cacti over there, and the first thing you notice is that your animal, that you have just trained to pet dogs, is starting to pet cacti.
Guess what?
It can't distinguish between either of them, because the cacti were not in the training data.
You didn't train an animal that can pet dogs, you trained an animal that can pet anything in front of it.
So it's gonna start petting the cacti and being like, I can't do anything with this.
So, in conclusion, Basically, what we've learned and what we've been trying to tell you is something you might already know, that watching something learn is an amazing experience, whether it's watching you guys learn or a family member or a pet rat or something, or an ML agent.
It's fun to watch something learn, especially when it's happening in front of you, and that by using careful design to train, you can actually train agents in real time as a gameplay mechanic.
And we believe audiences are capable of this.
It doesn't have to be a niche thing.
It can be understood like any other game mechanic in time.
It's kind of like what we always hope Pokemon kind of like promised us when we had children.
Or Tamagotchi.
Tamagotchi.
Something that could actually live and learn.
Watch your thing grow.
Watch your thing learn.
It actually learns.
It actually grows.
But we have a few reflections as well.
Like, what did we learn from making this game?
First off, learning should have not been just real time, but it should have been real time and continuous.
We wanted to mimic the experience generally used in reinforcement learning that we talked about, the simulation space and the real world space.
But in doing so, we limited them to kind of perform tasks of the user's desire, rather than having the agents actually potentially discover behaviors on their own.
In the future, it would be nice to aim towards letting the agents teach us as much as we were inflicting our agents on them.
And fundamentally, one of the problems that was in agents was that we talked about was you were just watching these pre-trained models.
And in fact, we kind of repeated this mistake in little learning machines to some extent, because all the action really happens when you're training them in the cloud.
And that's where you really find talk about These days when people talk about AI, and I'm sure lots of people are talking about AI here, they're often talking about large language models with billions, trillions of different neurons and parameters.
But we believe that understanding the foundations of AI can actually happen with a lot of different scales of models.
And we believe that there is a beauty to the small micro-examination of it.
and that you don't need to use the newest tools, you can actually just use the tool that works best for you and just go deep into it.
And then finally, while working on Little Living Machines, Dante heard me talk about this a lot, it was always the idea of the fruit fly.
And if any of you have taken a biology course in university or anything, you know that we study the biology of a fruit fly or of a small worm, and we kind of like look at the neurons and look at all the factors of it in order to kind of project into a larger scale and teach us kind of like the fundamentals of, you know, what's some pretty amazing cool science.
And in this way, I think that like playing with AI at these smaller scales actually creates a literacy and intuition about AI systems that are kind of like becoming ubiquitous, all-encompassing and somewhat scary.
Yeah, like last week I was talking to someone about embeddings, and it was so much easier to say, this is how an animal sees a flower, and this is how your large language models see your words.
They're just bits.
Exactly.
And using real-time learning as a game mechanic makes this sort of understanding and experience a lot more accessible.
And fundamentally, I think we believe that this base level of understanding is more important than ever.
Also, thanks to all the people that are listed here.
We couldn't have done it without any one of them.
And to the organizers of the AI Summit, a good first day, I would say.
Very interesting talk.
Yeah, great stuff.
I know everyone wants to just get out of here, but do we have time for questions?
Maybe, yeah, five minutes maybe.
Six, seven minutes.
That's what we're aiming for.
And if there are none, that's OK, too.
Go get a drink.
I think we have one over there.
Oh, awesome.
Sorry.
Thank you for the wonderful talk.
When you're doing your simulations, are you under the hood?
Are you doing many worlds in parallel?
Or are you just doing one single world?
So we started with doing many worlds in parallel.
And we tell the players we do many worlds in parallel.
It's much better for cache locality if you do one world at a time.
I'd recommend you check out the mini-world simulation paper at SIGGRAPH 2023 from my lab mate, Brendan Shacklett.
There's some work on high-performance GPU simulation of games using ECS systems.
Yes, so when we first designed the system, the reason why it's grid-based is because we wanted to do a GPU-based simulation, but man, engineering is hard, man.
Well, let's talk.
Let's talk.
Cool, cool, cool.
Thank you.
Yeah, let's alternate.
Sure.
Really wonderful talk.
I think this is wonderful work, and maybe one of the largest repositories of RL behavior or trained RL models in the world.
Well, I actually think it's true, and it's beautiful.
You mentioned old techniques.
Actually, about 20 years ago was a game called Nero.
Oh, you guys are aware.
Yeah, so.
Oh, it's on the slides.
It was on the first slide.
Oh, I missed the first slide.
Sorry about that.
Was that you guys?
No, no, no.
My advisor, Ken Stanley.
Julian's also done a lot of work in evolution.
I'm wondering if you tried something like that because 512 by 4, even fully connected layers that large, neuroevolution techniques kind of go from the ground up kind of approach where you take as few nodes as possible and grow them.
you know, many ways to skin a neural network.
So I'm going to try to get ahead of a couple questions here.
Often when I talk about this game, people ask, hey, why not tabular RL?
Why not neuroevolution?
And so one of the things that we really wanted to emphasize is not capturing the whole problem within the network itself.
And this allows for the network to develop more complex behavior, but more specialized behavior that is not tuned to the whole problem.
Tabular RL grows in dimensions really fast, and then neuroevolution has a problem where you kind of have to start from basic principles.
You have to teach it how to navigate first, whereas neural networks just kind of learn everything at once, which, well, at least in our experience, and it turned out to be the best implementation for us to kind of follow in this.
Hopefully that answers your question, but hopefully it matches your research as well.
Like Black and White, you've got RL as part of the gameplay.
Your QA team might want to talk to you.
Other people want to put learning in games.
Please check with your QA team about learning after ship, and make sure that it's the player that controls it and not the game, so your game doesn't go stupid or they accuse it of cheating because it got too smart because they figured them out.
Any comments on that at all?
I'm Which is a good thing for the technology, because then your QA team won't leave you in the parking lot one night when it goes stupid in the field otherwise.
Because a player did it.
And it's the player's problem if they teach it stupid or if they teach it too smart.
I see.
I see.
Do you want to talk about the problems QAing this game?
Yeah.
I think you're bang on.
And creating a game like this is extremely difficult to QA.
and we were QA-ing it a lot ourselves before we could even pass it on, but fundamentally that so much can go wrong, and it does go wrong, that you kind of can't catch it.
And I think there's a lot more work to do in this space in order to find where these behaviors actually break, and more than just QA-ing it, communicating properly why they broke, and at what time.
And I think we tried our best in some elements of this, but we could have spent five more years doing that.
One funny story is that one time, for a month, the agents could not see deep water.
So the agents kind of disabled when they fall into deep water, and they couldn't see where deep water was.
And they still worked, because they were able to kind of memorize a lot of the maps that you train them on.
So there was a moment where we were telling QA, like, oh, no, they should learn.
You're just training them wrong.
And then QA is like, no, they're really stupid.
And then we go in there, and we look at them, and they're like, oh, yeah, they're broken.
Sorry.
They can't see water.
They can't see water.
Our bad.
But yeah, good call.
I think one more question, and I think we've got that side next.
Hey, guys.
Great talk, great work.
First of all, there's lots of like, you know, Particular technical things I'd like to comment on from having done some very similar stuff myself but I'm gonna refrain from that so we can take a whole long discussion over the overfitting problems and so on and and and GPU paralyzation, but first point is like not a question I think it's awesome that this happens and it must happen because this is the kind of work that If we're ever going to use machine learning in runtime in games in an interesting manner, we need to design around it.
We need to leave old paradigms that are designed for the lack of AI.
And we need to leave the thinking that we can ever QA it.
We need to be able to step out into the bold new future and do things that are built on abilities, not on lack of abilities of AI.
So I think it's great work.
The question here is like, shouldn't this be much more of a social game?
And why should, and the question is, and the reasoning for this is that there's always going to be a tension between wanting things to learn fast and that learning something that is interesting is always going to take a long time, at least in interactions.
And isn't this something that should happen over kind of a social timescale between players?
I love that question, because I think that's just a great idea.
And why we want to give this talk, we framed a lot of this game over teaching fundamentals.
And maybe we did our best at teaching that.
And I think if there is more of a base level understanding of reinforcement learning, you can quickly get to games and experience social games that you're talking about.
Because I totally agree.
I think that is the best use scenario for this kind of project.
And I gotta end it, but thank you so much.
I totally agree with you, and thank you.
