Hello, thank you for coming to our talk today on evolving mixed reality.
I'm Jono.
I'm Samantha Gorman.
I'm Ron Gull.
I'm Brian Schwab.
So before we really launch into the talk, I just want to kind of frame the space that we're talking about a little bit.
So right now, a lot of AR that we see looks like this.
A lot of it is sort of single-plane, user-directed, tabletop, six-doff AR.
And people have done really, really cool, incredible, inspiring, exciting work there.
But I think that also everybody in this room probably has this sense of like, this is the first step towards this true mixed reality thing.
So to draw a couple examples what we mean when we say that, you know, that can mean everything from immersive games.
Sorry?
XR.
XR.
Everything from immersive games to industrial training to IoT controls.
I'll throw more examples out there, the kind of stuff you're thinking about.
I mean, the thing is, where we're going is just compute anywhere, pixels anywhere, with whatever device you use.
That's where we want to hit.
We're a long way from there.
Absolutely.
So let's meet each of our speakers.
Hi, everybody.
So my name is Ron Gow, and I work at the Interactive Media Group at Microsoft Research.
Just to make sure everybody understand, I'm not part of HoloLens.
So don't take anything I say as something that is going to happen with HoloLens.
We are working with them in Microsoft Research, but I know as much about them as I know about Magic Leap.
So we contribute ideas.
We have no idea what they are doing with them.
So in our group, we are working on several things that are relevant to this presentation and also on other things.
We started to focus on AR and VR and XR, sorry, about five to six years ago, and we did a couple of projects.
In our group we develop devices, we research ideas of how to design to these environments, and we have a lot of fun doing it.
It's pretty cool.
Well, that's it.
Hello, so I'm Samantha Gorman and I'm the co-founder of Tender Claws and we're about a 10 person studio and We're often we're artists directors designers that are often commissioned to think creatively around emerging tech Often at the same time as that tech is actually being developed.
So this is, I'm going to talk a little bit about Tendar, which was our collaboration with ARCore right when ARCore was being developed.
And as a case study, I was interested in going kind of away from tabletop, not because I don't think it's a really great area, but I want to do something more room-scale with AR.
And Tendar is essentially a long-form AR content app.
It's about three weeks of content, and it reacts meaningfully to the user's environment.
So it has modes that can address a user at home, a user in the outside world, multi-person and social use.
And at the core of Tendar's design is a strategic use of our device, of on-device rather, mobile vision, such as object recognition, sentiment analysis, to create a virtual friend and pet that can evolve over the three weeks by feasting on the player's emotions, the tears and joy of you and your loved ones, and responding to objects in the player's world.
So Tendar is actually, we're going to talk a little bit about machine learning later, but it's a fictitious company that ostensibly created this virtual pet guppy to improve its model for emotion recognition.
And by inviting users to build a relationship with this guppy, it teaches, as you play the game, a little bit about what machine learning is and how some of the object recognition in the game works itself.
Guppy in itself is a computer vision model that's inspired by various neurobiology research that attempts to emulate and automate how real vision systems operate and identify patterns.
It's kind of both a literal and figurative representation of machine learning since as you are playing the game, Guppy must eat and absorb the image, the data sets, the images of your emotions to survive.
And by eating those emotions, he's being trained on how to process and understand human emotions more accurately.
And you know, hilarity ensues as things kind of devolve over time.
Here's a brief teaser.
And pretend there's like, you know, fun instrumental music in the background.
So some of the things you're seeing in the video, there's various modes using both the front and back facing camera of obtaining emotion from a multiple, not just you as yourself, but multiple people in the space.
You can pull off the emotions from their face.
They turn into kind of these wonderful digestible blobs that Guppy can eat and sample and then give you a fortune of what you're really feeling.
So there's the sentiment analysis aspect.
You also see some of the object recognition, which is a context dependent on the various objects that can be in any of these spaces that the user will inhabit.
And there's hints of the game, like how to find them.
Some of the things that it also does is It thinks about, well, I guess to step back for a second, and we can talk more about this, but the challenge of why we did this project kind of early on into AR space was that our main interest in XR in general is that we see it as a rich space to prototype interfaces and models that kind of hint at or lead to a future of spatial computing.
That it's sort of the forerunner of what it will actually be in that environment.
So we value interaction design as a key.
A key element for making these spatial digital content that feels like truly present.
And there's actually a module that's not just world-facing, but that's self-facing, where you can decide to make the food you're feeding your fish by actually engaging with the fish and teaching it emotions.
And it's kind of this emotional dialogue you're having with the pet that then, like, results into this very, I guess, kind of like a little bit upsetting, you know, distillation of your emotions into food pellets that you feed.
And that's how, you know, the fish gets his image set data.
And then you get this wonderful little like fortune from Tendar that analyzes like, yes, this is accurate.
This is how you feel.
You know, you can choose what to do about that.
So one of the things that I think this panel in particular is relevant is that Tendr has to work with all kinds of player context and aspects of change to their world.
How do we design for a space where the content should be anchored to the user's world and have meaningful semantics with the user's world without feeling overly tethered to a surface?
It can happen on a bus, on a grocery store, on a toilet, you name it.
Act.
The other sort of unique thing about Tendar is it's thinking about AR for social play.
So there is a module that we often show at conferences and galleries that is for two players.
And one of the things that is happening there with the front-facing camera is the emotions of the two players are actually playing off each other and the audio is splitting to different ears.
So the different players are getting kind of live procedural updates on what the other player might be feeling.
which then changes how they're engaging with each other on screen, but it also changes how two users can engage physically in the space with the device and what that user intent means for moving an AR device around that space and how it changes the design of the experience or the application.
Hey guys, my name is Brian.
I work as a director on a thing called the Interaction Lab.
And so if you go to the next thing, it's basically a rapid prototyping group.
Over the last four and a half years, we've probably made 600 or 700 prototypes.
You can just pop through them, I don't care.
We did a bunch of things.
We did first turn on of all of the hardware and software and perception features as they came online.
We tried to give really fast feedback and guidance to those features as they got built.
We tried to think about how you would consume those features as a dev so we could actually iterate a little bit on the SDK before it got there.
We did a bunch of documentation and best known practices.
And then we finally did a lot of knowledge sharing.
And then slowly, as we walked up to launch, we kind of pushed a little bit further in that direction.
And the team, for about the last year or so, made a bunch of MR.
little moments called Magic Kit that was a bunch of stuff that we put out there specifically just to give some tiny little taste of spatialized computing and then all the source code in Unreal and Unity so that people could actually pull the dials apart and see what made it happen and when it fell apart and which perceptual cliffs were important and which one weren't and why everything was doing it.
So the source is all up there on our website.
Hey, I'm Jono. I'm on the labs team at Unity.
I'm leading UX dev for Mars, Mix and Augmented Reality Studio.
That's an upcoming set of tools for building XR experiences, M-R-A-R experiences, with a more visual, approachable workflow.
So, for example, we do simulation.
We'll talk kind of at length about simulation in a sec.
But this is an example of how this example game content would fit into this simulated living room.
We also have these visual tools for specifying the kinds of parameters of objects that you're seeking.
So here I'm saying I want my hero character to be on, it's gonna be on a surface, the surface is gonna be in some range of size, in some range of elevation.
You can see that here abstractly, and then also simulated, see how it actually matches.
Of course, face content is a big part of commercial AR right now.
So we have workflows for that.
And you can see here that I'm simulating live against the webcam.
Notice that we're not in play mode. This is at edit time.
I think this also kind of shows you the shape of things to come in terms of augmenting individual objects.
So I can say, like, I have this canonical version of an object that I can mark up and then, you know, see how that fits against, you know, real fed examples.
Here's that first scene that you saw running on device.
You see a little sample of some of our procedural tools and utilities here.
This has a rule in it that says, So any horizontal surface that's not the floor should fill with this terrain texture, this terrain material thing.
And then also those and the floor get nav-matched so our character can walk around.
And then also all the raised surfaces should build a ladder down to the floor.
So you can see all those elements.
And one of our goals with this project is to be able to open up XR experiences and XR development to people who are not programmers necessarily.
So to that end, everything you see here has no additional scripting.
This is all just kind of out of the box tooling.
And then also we're looking at tools on device.
This is a design mock-up of using a phone to capture a space and then to start to identify layout and rules and content descriptions that you want right there in the real space.
So ultimately you would pull this into the editor and use this room as a simulation that you can test against and also use.
The rules I'm starting to set up here as the beginning of your content scene.
So I'm saying like if I find a surface that looks kind of like this one, then put my hero on it.
And that would generate that rule that you saw earlier.
Okay, so that's who we are and kind of where we're all coming from.
So hopefully now you spent the setup to give you an idea of the kinds of things that we're interested in in the XR space and where we're going with all this.
So let's talk about why AR is complicated.
What are some of the many challenges of it?
Want to start us off?
In terms of us, we are trying to think about how to compose an AR experience that could be transferable to different environments of the user and still have semantic significance for those environments.
And part of one of the things that we were thinking about is, for instance, all that the objects that Guppy can recognize that he can respond to and that the user is kind of, there's Easter eggs and it's seeded throughout the game what these objects are, but users can go out and find these objects in their world.
And they started with like a thousand, we can talk about this later, and then worked our model down to about 200 of these.
And trying to figure out how.
Like a fire hydrant in one context may not look like the fire hydrant in another context.
So how do you design the AR experience to be able to recognize those and the interface that comes off of it and the visual design will be different depending on the different shape for what's on the screen and what the user encounters.
So thinking really, trying to think broader than just mapping a space to almost mapping a world.
So I actually had a question I wanted to ask you about that one.
Like, since you recognize all these objects that Guppy can respond to, how do you keep people from missing so much of the content that is there or that is possible?
Yeah, so the actual content is actually, it's like writing worked with eight writers over five months, and it's like a whole giant corpus of text messages that is partially generative and partially written that can come up in response to things the user does with Guppy in the AR world. So there's like weeks of content.
But parts of those peppered in within those messages is kind of hints about he's learning about the human world.
You're training him to like see and find objects of like things he may want to see.
So he's like, oh, you know, I heard about this type of device that, you know, does X, Y, and Z.
So that's a more explicit example.
But that can get players to like go out and bring their AR, you know, application into other spaces in the world.
If it's something that you can find at a grocery store, for instance, or you know, like a car wash.
There's also a module where Guppy sends you on excursions explicitly to like, for instance, a grocery store to talk about what he wants to see in the fish aisle.
So, you know, there's...
Element second, work with that.
I actually think that one of the biggest challenges of AR is not just that the experiences themselves are tough to make, it's that the player expectation is essentially nil.
Like they don't know what to think or do or if they can even move or if physics works or whatever.
And so like one of the challenges of making experiences for XR is to quickly and sort of transparently educate your users on what the experience is even going to do and what they can expect.
If I'm sitting in front of a screen, I know where all the pixels are going to come from.
I know where the controls are. And now that's all gone.
And so people don't even know where to look, unless you explicitly kind of make that contract with them very quickly. And they don't know what to do if you don't make that explicit contract with them fairly quickly. And that's one of the biggest challenges for now. That's sort of a moment in time challenge, right?
And in my case, we started to work on this area about six years ago when devices with an array of sensors started to appear.
And we faced more, you know, we were in uncharted territory.
And I'm coming from the background of geometry processing.
And then we found ourselves with a lot of data streaming fused to some extent into our application.
And OK, what?
Do I need to do in order to create a simple demo?
And then we start to find out that design tools are really missing.
So you get a lot of data, you get a lot of intelligent analysis of what's going on in the scene around you, but as a programmer, not that I'm an XR developer, but as a simple developer with tools like Unity or Unreal, I'm able to create stuff, but what stuff should I create and where should I put stuff?
in order to adapt to the environment around me.
And this is what led us to start thinking about design tools that will help developers, or will allow developers, to work in this environment without going through all the low-level geometry analysis and whatever.
So to that end, of course, one of the It may be the biggest challenge.
One of the biggest challenges is just the unknown nature of the real world, right?
We don't know where the user's gonna be.
Samantha's been talking about that, of are they on a bus, are they in a field, are they in a grocery store?
And how do you deal with that?
So, Ron, you wanna just kinda keep running with rules?
Yeah.
There was something I was going to say about the actually from the dev perspective, the most challenging thing and why it's important for the tools that these guys are building is getting everything to play nice together, especially we are trying to enhance the future of like XR by adding all these other like world sensing modules, just the combination of getting those things to play nice is difficult. And that's why what it's some of what these guys do is so important.
Absolutely. Yeah, we're going to talk about the challenging matrix of devices and capabilities and all that craziness.
So this, the nature of the thing that we're trying to get at, in a lot of ways you can think of it like CSS on the web, where we talk about, I want to specify some parameters and some rules that will let, in the CSS case, if my browser window is really small, or if it's really large, my content will intelligently adapt.
In this case we say, if the user's in that room, or that room, or the bus, or whatever, that their content adapts.
So to that end, I'm going to take this one.
So, continuing on what I said before, we started in a bunch of engineers trying to find a solution to a common problem.
We wanted to create amazing, magical VR and AR demos, and we had the mesh.
detected by some kind of depth camera.
And then we asked ourself, what do we need more?
So we started to throw balls in order to create magical physical simulations.
And then we found out about holes in the measures.
And then we thought about how to fix this problem and how to allow a more plausible kind of experiences.
And then...
We started to think about, okay, suppose I want to create an application, where should I put stuff?
Where should I hang my synthetic objects in the scene?
We started to think about consistency, adapting to different environments, adapting to different environments from the first frame, or asking the user to scan more data in order to adapt even better.
And the virtual objects needs to sit in the scene.
Nice locations, nice regarding color, regarding contrast.
I don't want to see, especially with additive devices, I don't want to put stuff on a window facing the sun outside.
All kind of rules that, when you think about them, seem simple, but.
We didn't want to go through this pipeline every new demo that we wanted to do, because we said to ourselves, this is a common set, this is the basic point.
I need to start, I need to have tools to start developing immediately.
And of course we took into account multiple users in the same environment that I want to put.
Even the simplest thing, I want to put something in the room that is visible or is viewed by two or more users, then it becomes a simple point that you can fix or you can solve theoretically very easily if you have all the data.
And this is what we wanted to say to the developer.
Don't take all the data.
We will try to help in this regard.
So and of course multiple application in the same area.
I want to open a new application, I want the previous open application to move gracefully and allow me or allow application not to fight over real estate of my screen and over my pixels.
So we developed a simple service called Flare.
This is before XR, so it's fast layout for augmented reality.
And we used declarative rules, which are very simple.
You just state the rules that you want your experience to follow, such as I want these TV screens to be hanged on a wall.
And this brings us to another point, what is a wall and how much semantic information do I have about the scene?
So we started with...
put it on any vertical, flat surface.
We don't care if it's a wall or something else, but the nice thing about rule-based systems is that the more information you have, the more intelligent your rule can become.
And then we started to...
figure out ways to solve such a system.
Because this is a kind of optimization problem, but a very non-convex optimization problem.
And we started to look for iterative methods, and iterative on purpose in order to be able to invest as much time as we have.
We wanted a solution, we wanted a good solution, but if we don't have 30 seconds to invest in it, we wanted a plausible or as good as possible solution in two seconds.
And we started to, we used a couple of methods of solving it and we achieved a very surprisingly good results on very low end devices.
And this is what motivate us to continue to look into the matter.
So this is a simple example.
And I wish I could play it.
Yeah, it's playing, yep.
Okay, so this is an example of building a racetrack in the room and the rules that were used to construct the racetrack is put a number of points in the room at a certain distance, range of distance between them and that are visible.
Each point can view the next point, view in the sense of.
There is nothing blocking the path.
And we construct the racetrack using a spline between these points.
And when we increase the number of points, we see that we adapt to the environment.
The racetrack is created larger and larger, the different, and of course, at some point, you might not be able to adhere to all the rules, but if your rules are valid in your environment, you'll get a solution which is surprisingly nice.
We were surprised, at least I personally was surprised.
And this is the same example, and the nice thing is that it's the same set of rules to design all the, to construct all these examples, and we found out that rules are very easy to use.
So, you can teach people, but it's not as clear as...
The problem is that you always, as a human, needs to decide, okay, I know the rules, but which one has more priorities?
And which one I want to follow no matter what, and which one I can allow to degrade from or not fulfill fully.
So it is a kind of art, but at least the basic idea is very simple to explain, and we actually had developers work with it.
It needs getting used to because you find out the difference between in the user's first room and visible to the user because you have stuff that can block your view.
But it's very easy to integrate multiple users, multiple applications, additional semantic data that will become available over time, object recognition, and what else.
So, and this is where the connection with Unity happened because Unity apparently is going more or less in the same direction and then.
We, yeah, we came across the Flare paper as we're working on our project and it was very exciting to us because we're like, yeah, like this is exactly, exactly the kind of approach that we're taking.
Just seeing the results that you had there was very encouraging on that route.
So yeah, let's see, did you want to talk about this one?
This is another example of graceful adaptation.
So we constructed a set of rules to put the creature in circle facing a middle creature.
We didn't define the radius of the circle.
And when the scene change, If obstacles appear, such as inanimate or a human, goes and disturb the previous arrangement, we simply added another rule to resolve, but be as close as possible to the previous solution.
So everything is very.
It's like Lego, everything fit together.
The only question is how much time do you have to invest in order to find a new solution.
And it's highly, the minute you have a strong GPU around, it's a matter of milliseconds.
On low end device, it becomes an issue, so.
Yeah.
So, yeah, like I was saying, we were really excited to find Flare.
I'd recommend everybody read it, by the way.
Just Google Flare, Microsoft Research paper, you'll get it.
Really good read.
And when we read it, we said, yes, this is exactly the kind of thing that we're talking about.
So we have very analogous systems to everything that Ranjit just described.
We talk about real-world objects, which is a scene object in your hierarchy that represents a real thing.
So I could be like, this game object in my scene represents a table in the real world.
The complexity of course is that table may or may not actually end up existing, so that's a whole thing.
Real-world objects are made out of conditions.
So you can see in this object, I'm defining.
So I have an object in the inspector there called floor.
I'm defining it with a tag condition, looking for the tag floor.
That's provided to us by Magic Leap and HoloLens.
And then on the other platforms that we support that don't give us that for free, we do some extra logic to fill that in so that we can just say every platform knows what a floor is.
That's actually enough for the floor.
In this case, I've added some extra conditions just to illustrate the case a little bit more.
I could also throw something on there saying, I'm looking for something that's horizontal.
I'm not looking for something that's vertical or something that's off-axis.
I also have a condition there for I want a surface that's of a particular range of size that's acceptable.
So we use those conditions, kind of stack them up to describe that real object in this sort of atomic way.
And then as Ron described, right, we then have this rules concept that sort of takes all that.
At a higher level, we say like, okay, when there is a floor, stick water on it.
When there is a raised surface, stick grass on it, et cetera.
You can see those rules kind of in the lower left of the screenshot there and see that they're all matching in this simulated kitchen environment here.
And then, yeah, Ron was also just describing degradation.
So we call those fallbacks, and we say, basically, you can describe the sort of ideal case where you're like, oh, I have this app that you need a two-story building and six different walls and three humans in it and have some crazy set of expectations.
But then say, if that fails, then here are some more, some simpler, simpler, simpler, simpler cases down to the point of like, oh, I don't even have tracking.
Now what?
Right now you're in a 3DOF experience or you're in a just 2D experience at that point.
Brian, you mentioned at one point the basketball thing.
Do you want to talk about that?
Yeah, so I was going to just mention, this isn't super new.
We have had very complicated game AIs in the past that use very similar systems.
I worked on A basketball game for Sony computer in the early 2000s.
And we had almost five megabytes of AI data.
It was just pure HTML of situations.
Everything from, oh, if you've got the ball in front of you.
And nobody in front of you can take a shot all the way till in the last three seconds of the game, if you're this particular player and the guy in front of you has a broken leg, do this thing.
Like all the way down to that level of specificity. And so these systems have been around for a while. I think that they're not typically needed for most games. It's more games that had a high degree of sort of knowledge base, which the real world represents the largest knowledge base that we need understanding that there is. And so that's why they're starting to make.
Kind of this resurgence.
The other thing that's nice about those rule-based systems is that they're human readable.
You can look at them as a human and you can say, I understand that in the last three seconds, if you're this guy and there's this blah blah blah, you can actually understand that very easily.
And so the reason why we had these huge AI systems on that particular game was that By being human readable, we could have a small army of hardcore basketball game designers who could crank stuff out year after year and add to that massive database of AI scenarios that it could respond to.
And so it was sort of a beautiful system so that non-technical staff, it was much more accessible to them.
So on the point of AI making a big comeback here, we promised we'd get into machine learning a little bit.
Do you wanna take us away there?
Yeah, I think when we were talking about this panel before, we had some interesting discussions and interplay about the use of machine learning.
In our case, I think it's actually, there's, it's such a wide topic, but Ways to sense the player's world and ways to use existing machine learning models and modules to incorporate that into an AR experience as a dev, as ways to sense the player's world and give you more data, I think is a really strong push towards the future of how we could engage with AR.
And for us in particular, no tools existed to really do that well at the time.
So it was learning a lot about how to operate within different systems and bring them into Unity.
And for, we actually used parts of TensorFlow as part of the object recognition and TensorFlow was not at that moment compatible with Unity so that we had to kind of build a wrapper around that.
And then essentially take the model, one of the models we were using in TensorFlow was one that was created at Stanford.
And it can recognize up to 1,000 plus objects.
But the image set it was trained on was mostly dog breeds.
So this is where kind of like the design thinking of being a dev working with like machine, you know, well.
So, this is a very interesting example of how machine learning models comes into place.
We knew that we wanted the fish to have an Easter egg where he could maybe identify all, take advantage of all those specific dog breeds.
So, we can very sensitively know the difference between a German Shepherd and a Schnauzer.
But very intensely, because we took that model and we binned it into objects that were recognizable and objects that could be identified.
make creative design decisions for what would be in the player's world.
And there were some very obvious things it couldn't recognize.
So very early on we had to make the decision that, okay, we're going to just bin X number of these objects into chickens.
So all types of, you know, like animals that walk in this particular way are been chickens, and narratively the fish is going to have a fixation with chickens.
And therefore that's why in this machine learning model, we're going to compensate that with narrative by like just, you know, talking about his chicken obsession.
Yeah, designing around modules is like one way to, you know, I think incorporate that into like the development of the game.
Anytime you have a massive soup of data and you're looking for temporal patterns, machine learning is an obvious choice. And because this is a new field, a lot of what we're talking about is machine learning, but don't be put off. A lot of what we're also talking about is stuff that essentially needs to be built at a very low level so that most people can just use it as a function, like where's the nearest table? It should be a function you can call, as opposed to let's ram through all of the geometry and run a learning blah blah blah on it so that we can find the flat surfaces and we'll declare certain types of flat surfaces to be tables.
That's the thing that the low level should do.
And a lot of what has evolved over the past four years is we've gone from you know, almost no semantic understanding of a geometric soup, to we're getting more and more and more through machine learning and other analytic methods, we're getting more and more semantic data that we can start to provide to developers.
Yeah, we were debating, when we were talking about this, we were debating should we tell everybody in the room, go learn machine learning?
And I think Brian just touched on that a lot, where in a lot of cases that's really on us, it's on the platform holders here, to provide that.
And I mentioned earlier that one of our goals on my team is to make this approachable to people who aren't programmers.
So we, like Brian was saying, We want to be able to just provide a model that's like, this one recognizes tables and chairs and household objects, and this one recognizes types of dogs.
You can just plug that in and use it.
That said.
I think we would all encourage everybody to at least get the shape of machine learning, just understand what is it, what are people talking about, and what are the general techniques used there.
So even if you're not a hardcore programmer, even if you're not going to make your own model, it is still just a very interesting thing to know about, and it will help a lot in this XR journey.
Yeah, from a dev perspective, I think that's true.
I think understanding a little bit about it helps underpin certain design decisions that you can make in terms of knowing what's available to you and what types of things you can do. 100%.
Who wants to talk about the user?
So the user is just as varied as the environments.
You know, even if you talk about just the quote-unquote human hand being this natural input, it's like, maybe.
Like, what if you have rheumatoid arthritis?
What if you really, really favor one hand over the other, even if you are left-handed?
You like using your right hand.
What if you just don't culturally think that this means yes?
You know what I mean?
Like, there's a number of things that have absolutely no bearing.
And so...
User input is in many ways almost the same problem as world understanding is user understanding.
And so a lot of what we end up doing on this side is the exact same problem with a different user, with a different data set. Like in exactly the same way that head pose is a fused input with a number of different things, hardware, software, and human understanding, right, that gives you head pose. Knowing where the hand is going is very much a hardware, software, and a human understanding problem.
You no longer just have a joystick that you're reading the value of an analog float coming out of it.
And so this is also a very tough problem, and again, is requiring the platforms to up their game as far as delivering solutions to this kind of understanding for the average dev.
For the sake of time, we're just going to plow through procedural content, learn up on it.
Same idea as machine learning, it'll do you well to check in on generating meshes.
But like you saw earlier, we're going to be providing some utilities for that, and there'll be more utilities, but it's important.
Does anybody want to touch on that before we roll on?
All right, cool.
So let's talk a bit about some specific tools and workflows that are there or that are coming online.
For one thing, simulation, I mentioned earlier.
This is the simulation view inside of Mars.
And here, I'm just going to kind of click through a couple different spaces, like how would this content work in this room versus that room versus that room versus anywhere else.
And here I'm swapping into a secondary simulation view where I can actually simulate as a device.
So here you can see that.
If you look in the left panel, you kind of see all the rays and the planes, the surfaces being generated by that device.
So this is a very coarse version of like, generally, this is how a phone or a headset would see the world.
So in this way, you can see like, as I move through the world, how would I expect my content to adapt?
And you can pick up a lot of issues there.
And another tool that I want to point out is debugging in the editor.
So here I see in my simulation that one of these trees didn't show up.
So I'm going to this compare mode where I'm like, why did it not show up on this surface that I'm expecting?
And sorry, the text is a little tiny there, but what's going on is in the inspector in those conditions that I was describing earlier, we're showing like this one is failing, this one, like the surface is too small.
So I say, okay, you know, adjust for that condition, include that also, and now you see it pop in there.
So I think that the idea of being able to ask your content, like, hey, why aren't you matching against this data that I expect you to work on, is very complicated in this context and very useful to have some tooling around.
Oh, so here there's one of the things was you can just fly around a little RC car and so I'm just flying around real quick then I push pause BAM and now what I can do is I can scrub backwards through all of that data right there in place standing in the same room. I can scrub backwards, I can scrub forwards I can realize there's way too much crap so I can like go over to the panel I can turn off a particular signal I can then Go back and forth and watch what the events are happening, when they're happening, and then right there in the place I can go, oh, this thing isn't firing because this particular collider is turned off because there's a ray of light that's coming into the room right here that's causing something.
This kind of like in-situ debugging is almost...
Crazy important for bugs that only happen in a particular room with particular lighting and a particular moment basically. And being able to just sort of pause right there. So we've made this module, this recording module, that you can just sort of drop into anything and like you've got a little recording window that you can kind of go backwards and forwards.
The awesome thing is, is hit pause there.
You can also like record what would, what would be seen through the device off to the side so that you can kind of like look at where you are right now and also put your finger over into the, into the stream and see the video from the, the device side view from that part of the stream.
So you can compare two streams right then and there.
And then lastly, this is all in world data, but you couldn't bake it out and just call it relative and then just take the dataset back to your desk.
and try and debug some things from your desk as well.
So it makes a nice little data set.
Instead of a Fraps video in your bug report, you can just include an entire huge data set and say the bug happens at this time.
And people back at their desk can try and debug it by importing that data set directly.
So let's talk about the ecosystem a little bit.
And one aspect of this, so Brian and I and our organizations obviously have been talking for a long time about, you know, I think we have very similar and inverted goals with each other, where we're trying to build tools that will let developers make experiences that will work on any platform, right?
That's kind of our whole thing, right?
And I don't want to speak too much for Magically, but generally the inverse.
Yeah, we're trying to allow creators to use any engine they want to make that content.
And so in the end, we have a particular set of hardware functionality.
And what Unity is trying to do is trying to basically make a set of authoring tools that are functionality agnostic.
So you not only have to abstract away the hardware, you have to abstract away the features themselves.
So that another human being might be an additional feature to an experience, which would lead you to a multi-user.
It's not just about hardware.
And so this sort of abstraction layer.
is not only going to allow different devices to run, but different devices with different functionality in different environments that afford different experiences all have to be abstracted out so that not only can the editor tell us what it needs to tell us, but that the devices can respond to the things that make sense.
Yeah, from a dev perspective, we worked with this project on AirCore and well, you know, it's out on Android, but You obviously want your game to be across as many platforms as possible, which is one of the reasons that you can use tools with Unity and create with Unity.
And from a...
You can say the other guys too, it's okay.
You know, well, we do, you know, we do work for...
But it's really actually the creating the AR across multiple platforms is not so hard, like there's no barrier really to us bringing this to iOS.
It's more that if you were doing, you're adding on these other modules of sensing, each platform can have its own kind of...
I guess like ways of doing world sensing that don't necessarily come across.
So if you're trying to advance AR in that way, then that is more the sticking point rather than bringing the AR content across platforms, at least in our experience.
Well, it's so interesting seeing your app that does all these things that we haven't seen before.
Many things we haven't seen in an app before, and many things we haven't seen simultaneously in the same app before.
And I think that seems like it speaks to the fact that you really dove deep into a platform versus trying to make something broad.
Yeah, and that's one of the advantages of diving deep into the platform is that you can push innovation in certain ways, but then you are tied to that vocabulary of working within that platform.
I just wanted to show this little table that we have on my team.
The y-axis there is platforms, whether that's hardware devices or software platforms, and the x-axis is functionality.
So it kind of gives you a sense of what it's like trying to support all these things, and we want all this to work nicely together.
So I just thought it was fun to see.
We were jokingly saying that in the 90s we saw this with 3D cards, and in the 80s we saw this with Sound Blaster equivalent cards.
So this is not new either.
Not a new problem, but an interesting space for sure.
Anything anybody else wanted to touch on on ecosystem?
I mean, I think that what you should be, the biggest part of what you should be taking away from this talk is that the evolution proved just how much data there was and how much we need tools to support the use of that data.
I think in exactly the same way that like in the early 90s, let's say when shaders became a thing, suddenly we realized, well not just shaders, but like, Bump maps and light maps and normal maps and like suddenly the tools to make good art became tenfold more important than it was when you could just make your textures and apply them to your.
You know, 200 poly model.
And so, likewise, now in order to sort of get the reality flavoring behind all of the things, like not just the art, but the interactions and the UI and the placement and all of the things, you have all of this extra data.
And so we're incorporating this.
explosion of tools in the ecosystem to try and get our heads wrapped around that and to get people to back to the productivity level that they've had for a long time now. Because we actually have pretty shockingly good tools nowadays to make screen-based entertainment or just regular applications. But those tools are largely, you know, the Microsoft paint of going forward is in that they are sufficient but they're somewhat basic for the level of data that we now need to push forward with.
So let's also touch on, we're talking all XR here, right?
Baran, I know you had some interesting other spaces.
Yeah, so in our case, we took the same rule-based system that we developed and knew how to solve fairly quickly and tried to see, and this is something that I really, really hopeful will happen when Unity will release their rule system, because rules are amazing.
It's stuff that you can't even imagine yet.
So we applied it, we said to ourself, okay, so layout of synthetic objects on the real world is one thing, but what if we want to create a layout of text over an image?
Then we have a set of rules, they are different.
Here we have color and contrast and font size and whatever.
But the idea is the same.
So here we took the metadata of the image and created a couple of examples.
Again, they are not perfect and they are not ready to be productized, but they show you the power of rules as a way of thinking about problems.
The way you have a solver, even if it's not a good solver, it gives you a starting point that will allow you to see things differently.
That's what we are hoping for.
And we, of course, we can't do stuff without machine learning.
So we are using the same set of rules to arrange for furniture arrangement and creation of environments because right now we are testing.
Machine learning models on completely synthetic rendering in order to achieve high accuracy object detection and these are a couple of examples that we are working on right now.
Some of the rules are created by a set of rules, not Feng Shui but similar walkable areas, facing TV in such basic rules.
And the object themselves in the scenes are placed using a set of rules to place them in a natural, as much as natural conditions as possible.
And we are checking to see how can photorealistic rendering right now, or physically based rendering, can help in easily create models such, applicable models, such as recognize a set of kitchen tools, recognize that in the future to the ecosystem.
All right.
I also found it really interesting, the upper right example there, where you're using the rules to then generate a layout of furniture, right?
Yes.
I found that example really compelling in particular, because we really want that functionality where we could just generate a billion rooms to go test your content against and say, oh, your content works in like 80% of rooms, according to our generator.
But then, Brian, I know you had some opinions about that.
I just, yeah, the thing I would watch out for is that...
Watch that your room generator isn't too simple, because it can make rooms that are nice and clean, and then your algorithms may only work in nice clean rooms, or it might make things that are, you know, who would ever put a television in the middle of the room? And it turns out that in some cultures that's where they always put the television. You just always have to watch out for implied biases in your generators at that point. And if you have algorithms that are learning off of those generators, you have to watch that the cart isn't pulling the horse and that sort of thing. And so...
We've used this a lot though, like he said, synthetic datasets end up being the thing that gets you a large corpus of your training, and then you try and layer it with real datasets as you can collect them.
But this is very well known in the magic, in the, what's it called?
machine learning community.
There's formulas for how much synthetic data is too much and that kind of thing already.
So again, this sort of stuff is, it's just hard problems and there's just so many of them.
So it's good stuff though.
One application of this stuff that is near and dear for me is applying this sort of rule-based thinking to strictly VR applications.
You know, right now in room-scale VR, I imagine probably many people in this room have probably done room-scale VR stuff, and kind of the Arguably the best you can do right now is sort of like fit a pre-built virtual room into the size that your guardian will allow. Like, oh, it turns out, you know, you're in a big empty room.
We can stick a really big virtual space in there. Or, you know, you don't have that much space so we're gonna stick a smaller version of that in.
Even that is pretty relatively rare.
So I'm really looking forward to being able to apply this sort of rule-based stuff to, you know, the system can take your guardian boundary shape, and on more advanced systems than that, it can take the actual mesh of the world for headsets that have cameras on them, right?
And actually give you a full-blown VR experience that also actually matches to your real space, which is just another flavor of the same problem, of course.
And there is so much that we could not jam into this talk.
Yeah, when you think about, think about like what I said earlier was, you have to quickly determine what the rules of engagement are for this particular experience.
Well, imagine if there's four experiences all running in the same room at once.
Like how do you tell people that?
Or do you limit it?
Or do you not?
Or whatever.
The thing I would say is that this is a very wide open space.
XR is huge and it's very infantile.
And I would say that if this sort of stuff interests you, there's a huge amount of companies that would love, you know, if you love building tools and you love solving problems that nobody has solved yet, I would say please come and talk to people because there's a lot more tools to make and there's a lot more rules to figure out.
And...
We need to get to the point where we have those high-powered tools that allow everybody to start making this space because this is where human-computer interaction is going.
Soon, there will not be screens up on this wall.
We'll all be sitting and we'll all be talking and we'll all be interacting with something that's floating around and we'll all have a little copy here and we'll all be talking and...
I'll be collecting a bunch of stuff over to the side, and I'll get flagged eventually that one of your guys' questions is pretty big, and so I'll start to formulate an answer, and then I'll push it out to the crowd.
We'll be doing stuff like this very, very frequently, very, very quickly.
I don't think it's even 10 years out, personally.
And so, like, this is where we're heading.
This is why we want to do the things that we're doing, because...
We're sick and tired of watching people walk around with their little screen in their hand, and they're not connecting with each other.
And what we really want to do is share compute with each other and share pixels with each other, and this is really the only way we can do it.
But it's hard.
Hell yeah.
There's definitely things I could, sorry, touch on about privacy and ethics.
I did want to just back up a little bit to the procedural and the fact that something that may be useful to discuss about how The Tendar was created in terms of like, if you are creating a character that's not necessarily tethered to a surface, how do you get it to move and act in a room that it feels more natural?
And that there's a very complex algorithm behind how the fish swims and moves around that room.
And at the point when we were making the app, this semantic definition of what a wall is was not available to us.
So one of the ways to get around that is to use the point cloud data.
And then we were able to ray cast to figure out the boundaries of the room and generate this model so that the character can move within that space.
So it's a mix of generating from the procedural.
That was helpful.
I'm happy to also talk about privacy and ethics, too.
There's Tendar starts with a disclaimer that all the data is on your phone, and that is true.
It's all models that are just saved to your phone, even though the company is a kind of fictional, speculative fiction that talks about gathering emotion.
I always like having permissions, privacy, and ethics on any talk that we do in this space when we're not directly talking about that, just because it's obviously such an aspect of all of this, and it's becoming more and more a part of the cultural conversation, which is wonderful.
But I just kind of want to bookmark that in there, of like, make sure when we're having these conversations that we're also having those conversations too.
But yeah, like Brian was saying, there's so, so much in this space.
There are so many unsolved problems and any of us who are cracking into a particular part of it, we're doing that knowing that there's all, this is like infinite amount of stuff on the sides that we're gonna have to address in time as well.
So like, I'm sure everybody in this room is doing, is working on a particular piece of the puzzle and I'm really excited to see this all coalesce in the next coming years.
That's what we got.
Thank you very much.
And if you have any questions, please remember to use the microphones in the aisles there.
Thank you.
Thank you.
Thank you.
Thank you.
Thanks again.
Thanks again.
Cool.
Total understanding.
I have a question.
Is this on?
OK.
In regards to understanding the diverse worlds that are out there and your diverse users, have your companies found good ways to understand how to make your experiences inclusive?
Like, how do you understand the variety of living room setups that are around the world, the variety of gestures that people understand and how those relate to their cultures at scale?
I mean, quite honestly, it's just a massive data collection.
Program that we're undergoing right now.
Like, you know, every region that we're going to go into as a hardware developer, we have to go in and actually find that information out.
And in exactly the same way that, like, when I worked at Blizzard and we put Hearthstone out on 19 different languages, we had to, like, get all of the text translated to those 19 languages using native speakers.
It's sort of the same thing. You have to go out.
You have to do the hard work in order to do that.
Cool. Thank you.
Yeah, totally agree with that.
I mean, I also want to put it out there.
I mean, I know this is, you know, always said and always understood, but I really want to underscore it here that like, I think it's super important that you work with people from different backgrounds to be asking those kinds of questions and to be calling things out as they see them of like, oh, hey, that is an assumption right there.
That's a bias.
So, you know, plus one, always gather your group.
Thanks guys.
Thank you.
Hey guys, got a quick question.
I noticed if the device that you're using is stable, then the animation plays smoothly.
But if there's any type of motion, it seems very glitchy.
Or is there better stabilization or frame-to-frame blending?
Because if you're moving around, any augmented reality that I've seen so far just is kind of jerky.
Is that a?
Software, hardware, is that something moving into the future that you guys see being fixed, or do you guys don't think it'll be fixed?
No, I mean you probably have just seen fairly early examples.
I mean I see stuff every day that doesn't feel very jerky and so like the thing is is that especially with You know, head mounted systems, the frame rate, frame rate is super important because you're, you're dealing with a physics prediction problem and the smaller, the Delta T on that physics prediction problem, the better the prediction.
And so like the faster the system runs, the better, like, like exponentially better the, the, the head post gets.
And so if you've seen an early system, that's running at a rock solid 30 frames per second, it's going to be jerky just because when you move the predicting module can't really predict.
With a third of a millisecond, you know, with 300 milliseconds where it's going to be.
Where as if it's running at 90 or 120 Hertz, suddenly that becomes silky smooth.
Okay.
And for devs the things we most care about in shipping is frame rate and smoothness, so.
Awesome, thank you guys.
Can we switch this side?
Hi, I'm doing a graduate project on AR and the environment.
And I was wondering, can you recommend some solid resources for us non-engineers who are trying to reverse engineer a lot of the stuff we're finding in tutorials?
Then they're too old.
And so we're basically trying to do planar target image and GPS tagged 3D objects.
Come talk afterwards and we can toss you some stuff.
It's difficult on camera to give you recommendations.
Yeah, I was just working through, like, I can name a few people, but we're not supposed to, like, pick sides.
I wanted to touch on something you guys had mentioned early on about how to train people to interact with these spaces.
Everybody knows how to swipe to unlock your phone, but we had to be trained to understand how to interact with that screen space.
So what do you guys see as the future of a design language that we can all adopt that is easy for people to understand and we don't have to build that into our experience?
Yeah, I mean, that is very, very much what is being built right now.
That is the bastion level that is forming under our feet as we run down the hallway, right?
And so, I would say that if you have interest in that work, please get involved in it, because we're building it right now, and that is a moment-in-time problem.
I see little kids now walk up to stores and touch the window because they think it's going to do something.
And so they have an expectation right off the bat that a shiny, glossy surface means I can do stuff with it. And we're getting to that point now where I can put a device on people's head and they stick their hand up into the field of sensing right off the bat, which you didn't have even two or three years ago. And so we're getting in that direction, but there's so much work to do. I mean, it took 50 years to get Swype. And so it's going to take us a chunk of time to get a good set.
So it's a chicken and egg problem as we develop the tools.
Whatever sticks best is going to be what we adopt.
And there's also a stepping stone problem.
People currently have their expectations here based on desktop and mobile.
Here's what we could even potentially offer them, but they wouldn't even think to try it.
They have to do this and then you have to give them this and you kind of have to step them up there.
So a lot of times, we end up having tutorial levels that have input schemes that are more based off mobile and then slowly you introduce them to the fact that they don't have to use that input scheme.
Just over the course of one tutorial, you translate them from nothing, to a touch-based thing, to a gesture-based thing, and then they spend the rest of their time in the app in gestures and they never even use the touch stuff.
Like it's important.
And I think it's a function of both how these platforms be adopted by the public, because Magic Leap just came out, but you can see in the HoloLens history, the...
initialization, the realization that it's not a good practice, and then the changing of the mind.
So it's an iterative process.
And because the adaptation is not, you know, it's not millions of users sending data.
We don't know what will work.
And we are learning as we go.
So as Brian said, get involved.
Yeah, certainly.
Well, thank you for the answer.
And I think it's like in Tender actually, it is a question also of user interface and design on this on particular applications.
Like one of the things we were doing was using like thumbprints in certain ways and indicating graphically that the phone could swivel.
But I think that with more head-mounted displays now, it will be more apparent, like the way that people will be trained to look around the world.
But when you're making a mobile application, they're going to still look at it as if it's a 2D monitor.
So in order to collect the emotions, we have to kind of train them a little bit to actually look around.
And it is part of the actual graphics on screen.
That's what I put out there.
Every day on Unity's internal Slack, and I'm sure this is the same at Epic and Microsoft and Magic Leap and everywhere else, people are just posting all day, like look at the cool things this user's doing, look at what this person made, look at what this person made.
So yeah, please keep making that stuff and share it and tweet it and do all that because we see it and it does work directly back into the tools that we're making and the conversations we're having and all of that.
Very exciting.
Take a nap?
One more?
We should probably get going.
I'll be quick. I had five questions, but I'll just ask one.
Okay, please.
Device or server-based ML solution for spatial recognition.
5G, is that a solution or we should still rely on the device itself?
Sorry, repeat the question one more time.
Server-based or device-based spatial recognition.
No.
So you can walk and determine the space without staying in one place.
So what is the question?
Server-based or client-based?
Yeah, what do you think is going to take off better?
Server-based or client-based?
Both, yeah, yeah, yeah.
So the heavy-duty stuff will always be in the cloud, but you need the responsiveness of on-device.
Yeah, I mean, even, like, let's, you know, let's say we did jump to 6G.
I would say that we still have a latency issue, especially when I was talking earlier about the fact that you need, in some cases, to have really good head pose, 120 hertz, you know, sort of frame rate, and nothing on the cloud is going to be at that rate.
So let's say Google does give you the data of 3D space around you, they do the Google 3D map, and you can pull the data from Google. Will that help?
Oh sure, I mean, don't get me wrong, there will be a ton of applications where stuff on the cloud makes it down to the device in time or in a way that makes it super usable. It's just that...
Because of the latency, if I'm going to do completely recognizing stuff on my hand, I don't want to be sending that to some cloud and then bringing it back down when I'm specifically trying to do very, very hard, fast, precise work off of my hand, let's say.
So the question of, for instance, analyzing a room can be done on the cloud because it's nothing to, it's not very urgent to, I can stand like three seconds or four seconds until you get a result, but identifying your dog running into the room needs to be immediate.
You can't tolerate latency there, otherwise things will go very wrong.
So, yeah, both.
Let's cut it there and take it to the hallway.
Thank you very much for coming.
