Hello, I'm Richard Katz, and I'll be speaking today about the character pipeline I worked on at Zenimax Online Studios for The Elder Scrolls Online.
I'm going to talk in detail about the character rigging pipeline, the rigging tools, the animation tools, and I'll wrap up the talk with a short post-mortem from my perspective as a senior technical artist on a team of well over 100 developers.
I'm going to compress six years of development into about 60 minutes, so it might go a little quick.
And, yeah, in contrast to the quick development cycles that Louise and Brian talked about earlier today, this is quite a long project that I've been on.
First, let me tell you a little about myself.
I received a bachelor's degree in fine arts from the College of New Jersey.
I started in the games industry in 1997, over 17 years ago.
Back then, it wasn't enough just to be a competent artist.
You also had to be fairly competent with some of the technical aspects of game development as well.
My first industry job was at Sierra Online, working on the MMO Middle Earth Online.
I worked at the O'Kers Studio for about a year and a half until it was shut down.
I was a 3D generalist.
I touched every stage of a character from concept to game.
I was doing concept, model, UVs, texture, skinning, animation, and technical art.
We were working in 3D Studio Max version 2.0 and the first release of MaxScript.
And I built a layer system that Max did not include back then.
Max didn't have layers in version 2.0.
And a sprite rendering system that cut times.
for getting a character from max into the game from eight hours to thirty minutes.
My second job was at the 3DO company in Redwood City, California, where I worked on three Might and Magic titles.
Crusaders of Might and Magic, Warriors of Might and Magic, and Shifters.
With a parenthetical of Might and Magic, since it was a sequel to Warriors.
I started at 3DO doing more of the same thing I was doing at Sierra.
Concept model UVs, texture skinning.
animation and tech art.
Within my first few months there, I was promoted to lead artist, where I ended up doing a lot less character work and a lot more of everything else.
UI design and scripting, particle effects and scripting, and I was managing a team of between four to eight artists plus outsourcers.
Most of what I've done since 3DO has been character technical art.
Over the past 11 years, I've worked at Z-Axis, Visual Concepts, Secret Level, and ZeniMax Online.
Of those 11 years, five of those years working in Maya, and six years working in 3D Studio Max.
Some of the titles I shipped over that time, X-Men, The Official Game, Fantastic Four, Rise of the Silver Surfer, and Iron Man.
But I'm here to talk to you about my work on The Elder Scrolls Online.
Elder Scrolls Online, or ESO, is a massively multiplayer online role-playing game.
MMORPG, or just MMO for short.
ESO is a progression to online from the popular Elder Scrolls single-player RPG series from Bethesda Softworks, or Bethesda Game Studio.
Elder Scrolls III Morrowind, Elder Scrolls IV Oblivion, and Elder Scrolls V Skyrim.
Skyrim came out during ESO production.
I started working on ESO in 2008 and the game shipped in 2014 after numerous style and design revamps. ESO is an MMO.
Massive is the operative word here.
We had 10 playable races multiplied by two genders, mostly variations on human or elf, but we also had two beast races, the Khajiit, or cat-headed humanoids, and the Argonians, or lizard-headed humanoids.
So we didn't have a huge variation in playable race anatomy, but we did need to expose to the players a system for customizing their playable characters' physiques and faces.
We had a very ambitious 90, yes 9-0, unique monster rigs.
Some of those rigs had as many as four or five mesh variations.
These humanoid monster rigs were built on a custom rigging system and tool set.
We had been established as a 3DS Max studio, but most of the animators came from a Maya background.
Early on, we wanted to avoid using BiPED as an animation framework.
So we built proprietary animation rigs and tools, which evolved over the first few years of the project.
There were several major limitations from tech.
We needed to render up to 200 players on screen in our player versus player battles.
So the characters needed to be a single draw call each.
For most of the project, DirectX 9 was our minimum spec, so every conceivable, potentially movable part on a humanoid character needed to be influenced by 77 bones or less, and there would be no LODs, either geometry or skeleton.
Every deformable part had to be accounted for in the 77 bone limit.
hair, loincloths, tail, shoulder armor, eyeballs, weapons, tassets, etc.
Originally, monsters and NPCs were going to be limited to 56 bones, but that constraint was removed pretty early on, and everything was allowed up to the 77 bone limit.
We were tied to 3ds Max 2010 for the entirety of production.
3ds Max has no useful referencing for animation.
There are two referencing systems in Max, but neither one is suitable for character animation.
Xrefs either replace your animation controllers with an Xref controller, or merge in copies of source controllers with no updates and no hierarchy changes possible.
Containers still seem like a work in progress, and in 2010, I don't think they've been updated much since.
They were pretty awkward to use with animated characters.
So we needed a system that embedded rigs in each max scene.
And that could be updated via batch.
There's a couple of major disadvantages with this.
Custom animation scene setups get blown away when a rig is updated.
And embedded rigs and scenes easily become out of sync with the latest master rig for a character.
Oops.
One animator, Dave Sanita, authored a rough animation rig before I was hired, which we used as a starting point.
And that's what I just said.
The 3ds Max scene that contained the rig already had acquired tons of cruft.
Orphan custom attributes, unused materials and animation controllers.
One thing Max is good at is accumulating this cruft, but it's bad at allowing you to clean it out once contaminated.
So from the start, we knew we needed a way to build brand new rigs and fresh uncontaminated scenes.
I started by just disassembling and scriptifying the rig.
I broke down the animation rig into modules.
Base, torso, legs, arms, hands, head, robe, and cape and tail.
At the same time, I created a template skeleton guide rig, a skeleton with freely movable joint locations.
The guide rig could be saved out to an XML file and rebuilt in the scene from the XML file.
Each module in the animation rig then built along the guide rig locations.
Once the initial generic humanoid template rig was set up, Rigging any humanoid was as simple as duplicating the template skeleton, adjusting joint locations, clicking the build rig button, and then skinning it.
About half the animations in the game, about 5,000 out of about 10,000, were authored on the standard humanoid rig.
For player characters, NPCs, and some humanoid monsters.
On the surface, it's pretty easy to sit down and start animating with, but it has a deep array of options baked into the basic rig.
Our spine only had two bones because of our low bone limit.
It was FK only and had options for rotational space switching to the parent body or root.
You rotate the abdomen and the chest could optionally follow it or maintain its orientation relative to the hips or the world.
We also had switches on many controls in the rig, so the animators could animate using Max's TCB Quaternion controllers instead of the rig's default Euler XYZ controllers.
The head defaults to an aim system, but it can be switched to animate in FK.
Or it can be switched to animate in FK.
In aim mode, we have an auto-neck that inherits about half of the head's orientation.
The Autodeck has an X-Y switch for forward-facing necks, in the case of humanoids with extremely hunched shoulders or quadrupeds.
The eyes have independent look-at target that can follow the head or world, and an FK override.
The head aim and head and neck FK controls have space switching and Euler-Quaternion switching like the torso.
Our legs are IK only, but that generally wasn't a problem.
The foot control could be switched from root space to body space in the case the character needed to leave the ground.
We have a hip fudge control to tweak the femur and hip joint.
The foot has a bunch of driven attributes.
Roll at ball, heel, and toe tip, a twist with a sliding pivot, and a few different tilts.
The arms were built with fake FK.
The bones thought they were in IK at all times.
But the FK hand control was an IK control that was a child of the forearm control, as was the elbow pull vector control.
When we switched to IK, the bones constraints would blend between different sets of controls.
There's an IK-FK snap built-in custom attribute on the wrist config node.
Like the other modules, the arm controls have space switching and Euler-Quaternion switching.
The hands have two layers of finger attributes, a set of meta properties, and a set of finer per-digit properties.
All the keyable channels lived on the config bar, but had some two-way wires so they could be posed in the viewport as well.
We used Paul Neill's pen attribute holder on the config bar.
It made it easy to bake save poses into the rig at build time.
For most of the project, our player characters were going to be able to equip capes.
so our rigs had a cape module.
The tails for the beast races were originally going to just follow the cape animations.
Then we added mutually exclusive tails, so a character could either have something skinned to the cape bones or the tail bones, but not at the same time.
With the capes unlocked from the tails, we still couldn't move the capes very much because we still had to support weapon docking on the character's backs.
Eventually we cut capes to reclaim some texture atlas space for higher resolution weapons and first-person camera view.
But our tails are still driven by phantom cape controls in the rig.
They have some space switching to follow the back or pelvis depending on whether it's a cape or a tail.
There's also an IK mode for the cape, but I don't think it ever got any use in the player animations.
One feature that did get a lot of use is the procedural wave settings to drive a looping sine wave-like motion with a couple of sliders.
As any self-respecting RPG does, we have robes and dresses.
Unfortunately, we have no game-side simulation.
So, and we only had four bones allocated to the robe.
The animators basically just wanted it to stay out of their way.
So a large majority of the robe animation was driven automatically by the leg motion.
Then the animators could clean up any outstanding issues with the FK controls that sat on top of the automation.
This is how we ended up driving the robe.
One bone in the front between the shins, one bone in the back between the calves, and one on each side of the lower legs.
The front and back bones maintained a position between the lower legs, oriented to the plane between the lower legs, and forced outward when necessary to preserve volume.
The sidebones then slid around the calf centers to maintain volume there.
The actual lower legs themselves weren't weighted to the robe geometry.
The weapon controls can be docked at several locations on the characters' back and hips, and there's an IK system built in to plant the weapons on the ground.
But humanoids weren't the only characters in our game.
The Watcher is a take on the classic fantasy beholder character, a giant flying eyeball.
But the Watcher's most obvious feature are its 15 arms, three main arms with leaf-like mittens, four mid-sized tentacles, and eight smaller minor tentacles.
The main arms had independent controls for FK, IK, and could also be driven procedurally with the same system we used on the humanoid tails.
The middle tentacles were driven mainly by the ambient procedural motion with an FK layer on top to refine the motion and fix collisions.
The eight minor tentacles were all driven by a single set of procedural settings with a phase setting to keep them all different shapes at any given frame.
And of course, like the mid-sized tentacle, the minor ones had an FK tweak layer on top as well.
We had a snake module that we used for various creatures.
A plain old snake critter, a giant snake monster, like pictured, that had a torso section that always stayed vertical, an ice wraith that looked like a flying snake skeleton that sort of swam through the air, and two different naga-like monsters, the two-armed lamia and the four-armed harvester.
It took a lot of iteration to iron out the kinks in the snake rig.
There's a spline IK underneath everything, but Max's spline IK just has no consistency for up vectors, so I only used the position from the spline IK to drive the, and drove the orientation with another layer of helper nodes.
But it was easy to flip those as well when the look at targets turn 90 degrees on any axis.
So we ended up subdividing the chain by adding more point helpers constrained between the bones on the spline IK, so that we could twist the controls at least 360 degrees before the bones started flipping.
When our second winged demon type character came up, the animator working on it showed me a God of War III behind-the-scenes video on YouTube.
The rig featured in the video was for a manticore.
The wings on the creature had a single master billowing control that drove the shape of the wing membrane.
I built something similar for our wing rig, but the wings on our gargoyle and grievous twilight characters only had about 40 polygons, so the results weren't as awesome as they could have been.
We ended up with two different kinds of quadruped legs.
Predators, like bears, wolves, and cats, have forelegs with pretty short hand segments.
The majority of the limb is humerus and forearm.
The module we used for those had an IK chain from shoulder to wrist, similar to a biped.
Hooved animals tend to have much longer carpal or tarsal segments, and their knees sometimes don't even come below the bottom of their torsos.
Three or four bone IK chains are unpredictable, in Max at least, so the solution I used for these kinds of limbs incorporated two bone IK chains into a larger whole.
The module for those limbs, and some predator hind limbs, had an IK chain that started at the knee and ended between the carpals and phalanges.
The wrist or ankle was in the middle of the chain.
The thigh then had a toggleable aim toward the IK control on the ground.
There was a knee tweak control to get some specific shapes and stretch the leg a little bit, which the animators tended to abuse.
Quadruped torsos could switch between IK and FK depending on the shapes the animators wanted to achieve in an animation.
The body root defaulted to a position near the center of mass but could slide up and down the length of the torso so it could pivot anywhere from the neck to the tail.
The rigs themselves are the products of a suite of tools authored to create and edit the rigs.
The pipeline looks something like this.
A master XML metadata file that contains the locations of each character's specific data relative to itself.
The master file is called rigbuild.xml.
Each character had about 10 XML files and one clean mesh scene, just the mesh, no cruft.
When we build a rig, it looks up the character in the rig build metadata file and finds the character's rig config file, which tells us what modules to build for that character.
For instance, we'll build the guide skeleton first.
The config file will just say, yes, build a guide skeleton.
The scripts look up the location of the guide skeleton XML in the rig build meta file and then runs the guide skeleton XML through some scripts that build the skeleton in the scene.
Then we go back to the config file.
to see what other modules to build for this rig.
Look up the location of the XML data for each relevant module and build the module from the XML data.
After initially setting up and building a rig once, a rig could be constructed from these few components in just a few seconds.
This is the tool we use to build the rigs.
The character list is built from...
from the rigbuild.xml file, and when you select a character, it loads character-specific rigconfig file that can describe what to build for this character, which modules to build.
The left pane is a list of rig modules in our library, and the right pane is a view of selected characters, rigconfig.xml, telling the tool which modules to build.
When we click build, the tool collects a list of modules for this character, to build for this character, and sends the list to a MaxRip function that builds each module in order along the guide skeleton.
And when it's done, it deletes the guide skeleton.
This is a simple MaxUI floater for editing the guide skeleton, also known as the proportion skeleton.
It builds and saves the guide skeleton and has tools for mirroring locator positions, aligning locators, and globally scaling the rig via translation.
It has some editing functionality, but we always just added joints in the text editor, position the joints in max, and re-save the XML.
It also builds and saves the mesh skeleton to verify hierarchy and for adding additional export data.
So we lay out the skeleton, set up the rig, code additional bits if needed, and build the rig.
Some nodes get automatically placed in layers and sets, but many don't.
So we sort all the nodes into nicely named layers, sort controls into various selection sets, and adjust the size and location of any controls that are hard to see.
Now we want to save all that sorting and cleanup so we don't have to do it ever again.
That's where this tool comes in, the Unbuild Rig tool.
It shows another view of the character's XML files from the rigbuild.xml meta file.
It has Perforce integration so we can check out the files, click save, and resubmit them.
Now the rig is built and we need to skin the mesh to follow it.
Some monsters only have a single mesh and that's it.
Some have multiple mesh variants.
And our dynamic characters, our playable races, and other humanoids, they're a whole other level of crazy.
The main interface for the skin weights here shows visible geometry in the scene, and will tell you if each mesh has a skin modifier and shows you each weight file's perforce status.
After we've skinned a mesh, this panel lets us save the skin out to an XML file with weights per vertex.
It has perforce integration, and we can sync edit, add, submit, and revert the weights and XML files right from the panel.
When we rebuild the rig, assuming the mesh doesn't get updated, the saved weights file will get applied to the mesh.
And that only works if the mesh's vertex order or vertex count don't change.
However, the weight file also contains enough vertex and face info to rebuild a facsimile of the original mesh.
When the mesh topology has to change, we use another mode in this tool to get the old weights onto the new geometry.
The tool rebuilds the original mesh from the saved.xml.skin.weights file and loads the per-vertex weights onto the new temporary mesh.
It applies Max's skin wrap modifier to the target mesh, interpolates the weights from the source to the temporary mesh, and then converts the skin wrap to a skin modifier.
Then it deletes the temporary mesh.
Now we can clean up anything that didn't transfer from the skin wrap well and resave the XML with the correct per-vertex weights.
That was what that slide was.
Like most games, we have a 4 influence per vertex limit on skin weights.
I don't like to work with the skin's preset to a 4 influence limit.
I leave it at the 20 influence default because I don't want Max to automatically remove influences while I'm working.
So I have a weight cleaner utility which removes weights below a threshold and removes the smallest influences above the limit of 4.
Our dynamic character parts system allows players to wear a huge assortment of mixed armor sets.
So the geometry is broken up into body parts.
Eight for the basic body, that's torso, heads, upper arms, lower arms, hands, upper legs, lower legs, and feet.
Another half dozen deformable clothing bits.
Each of those had about a dozen variations per gender at launch.
But we've had a mandate since launch to add more silhouette variation in the armor, so we're up to about 30 variants on some body sections now.
On top of that, we have a metric ton of hair and adornment variations and dozens of rigid armor attachments.
For all of this to work, we needed to synchronize the seams between all of these parts.
Skin weights and vertex normals.
The normals needed to be contiguous for shading purposes, and Max has poor support for vertex normals in general.
The base editable poly doesn't have any support to view or edit vertex normals.
Max traditionally derived vertex normals from its face smoothing groups.
The editable poly does support vertex normals though, but you need to edit them in an edit normal modifier, and then collapse it down.
So our copy normals tool needed to apply edit normal modifiers to the source and target meshes, copy the normals, and then collapse them down.
Back to weights, the dynamic character meshes actually each have two sets of weights.
The animation skeleton bone weights, and a second set of weights for customizing characters' body shape and facial features.
Both sets had to have their seams match up exactly.
Every forearm had to have the same exact weights where it meets the upper arms.
And all the upper arms had to have the same exact weights at the forearms at the same seam.
We created a...
Discman.
templates of seam locations to apply weights to multiple targets at once.
Every studio I work at, I end up creating a variation of this tool to do batches in Max or Maya scenes.
It takes a list of files, a chunk of Max script code to run on each file, with an optional resave after the script executes.
It runs everything through a try-catch construct to avoid batches ending prematurely, and saves out a log file with success or fail results to review after long batches.
We ended up doing a lot with Max's custom attributes, but Max doesn't provide much for viewing or editing the contents of a custom attribute's parameters.
This tool recurses through each object and modifiers and lets us view or edit the data.
We can directly enter values for simple types or elements of complex types like vectors or matrices and pick objects for Max object tabs, and it automatically inserts node transform monitors for weak referencing.
It can grow or shrink parameter tabs, too.
We also have a suite of tools that face the artists and animators.
The pipeline started simple, and there was never time or reason to expand it much further later.
The users would just add a startup script path in 3D Studio Max Preferences.
It was a single startup script in the target path, and it built a tools menu in Max at startup, and that was about all there was to it.
The tools all ran from a network path.
I would develop them in a source-controlled local folder and push them to a user-run network path when they were ready.
The tool set didn't have any DLLs or other binary components, so we never had any network access conflicts.
The coolest thing was that I could fix bugs and update scripts on the network, and then the artist could rerun the tools from the menu and see the updates without even needing to restart Max.
The animators use this tool daily, the Animation Transfer Panel.
It allows the animators to update rigs in single scenes or multiple scenes since our rigs have to live in each Max scene.
And it wraps up...
it wraps the functionality of Max's XAF or XML animation format but with additional custom data that this tool packed into the XAF files.
Perforce status is integrated into the UI so animators can see what's up to date or who has what file checked out at a glance and they can sync and check out Max and XAF files.
The rigs have data-driven display panels for toggling visibility of controls in the rig.
They were designed to take up as little screen real estate as possible.
It's authored as an XML file, and that file is encoded and stored in the rig when the rig is built.
The panel interprets the encoded data and it constructs the panel on demand.
The same tech is used for other panels for visibility of mesh body parts and for trajectories.
The animators use a pose tool to save, load, and mirror single poses.
The poses are saved in an XML file.
The tool has options for working in world space or local space, whole character selection, and has per-force integration to show the status of the pose files in sync, edit, and submit them into the depot.
3D Studio Max has animation layers.
They were introduced around Max 2009.
But for my first six months on the project, we were working in Max release nine, which predated the animation layers.
We used Max's freeze transform system to create zero default values on controls.
It's a macro script included with Max for many years.
It creates a list controller on the position and rotation tracks and stuffs the current transform into controllers at the first index of the list labeled frozen and then creates new controllers at the second index labeled zero and sets it as default.
In Maya, I would just build up a hierarchy of empty transforms.
However, our tools made assumptions about this frozen transform list controller paradigm.
When animators needed to use Max's animation layers, it would collapse everything, blow away the frozen transform list controllers, and break our rigs and tools.
So we brewed up our own version of the animation layers that respected the freeze transform list controllers.
The animation layer system kept track of additional list controllers and layer membership via an XML file embedded in a single string property on a custom attribute.
Collapsing the layers worked per-key for keyable tracks and distributed the values as one would expect, but since they were just more list controllers, animators could replace default layer controllers with non-keyable controllers, a noise controller for jitter or position constraint to fake a hand holding a character's head.
Then the animator could bake the results down into keys if they wished.
The animation transfer panel needed to peek into XAF files during load, read some custom data from the file to see if there were animation layers in the saved XAF.
If there were, it would need to build out the additional animation layer controllers before loading the animation data into the rig.
The Dynamics panel was originally created with the intent of breaking characters with 3ds Max's Havok Reactor Dynamics system.
It allowed the animators to group rig bones into separately definable sets of animatable controls to keyframe independent of hierarchy, and then blend to and from the normal rig behavior.
Ultimately, it did see some creative uses, but mainly for keyframing instead of reactor dynamics.
Our tools weren't tied to specific object names, but they were tied to specific set and layer names, which were shared across all rigs.
If you merge in objects from another file in 3DS Max, layers with the same names get combined, and selection sets don't merge at all.
The namespace tool let animators define a Maya-like namespace for up to four characters in a single scene.
It switches a character out of namespace mode so that it can perform tool operations like load or save animation or export.
It was a little clumsy, but it was a quick solution around systemic problems in the tools and rigs that would take too long to fix.
3ds Max's built-in ghosting only ghosts the currently selected mesh.
And the options are only accessible in a tab in the preferences.
The ghosting tool allows for color and alpha settings, interval settings, forward or backward ghosting, and of course, any specified mesh.
It uses 3DS Max's point cache modifier, which is surprisingly responsive with 5 to 10 meshes.
I mentioned earlier about our built-in parallel Euler and TCB rotation tracks.
on many of our rig controls.
This is a tool to convert the Euler to Quaternion and vice versa.
Either 1-to-1 key conversion or bake out a key per frame.
If an animator found weird, wobbly Euler gimbling in something they've already animated, it's pretty easy to convert a time segment to TCB keys and smooth out their arcs.
Our character team had a subgroup devoted to character customization.
One of the character artists gave me some specifications for a rig to allow the other customization artists to sculpt body and face shapes.
I worked the rig into our pipeline.
It gets built just like an animation rig, even though it has a different set of requirements and a different user base.
Our character artists are good at what they do, but on average they're a little more tech-phobic than the animators.
So their interface for the customization rig tries to support them with a ton of callbacks to make sure they're working in the correct pose in the rig and make sure they're not editing poses with auto key off.
The customization artists work on the rig in local Mac scenes and save out single poses to be merged into a master customization rig.
The UI synced scene control selection with labeled buttons and shape buttons.
Similar to the animation rig display panel, the layout is authored in an XML file.
encoded into the rig at build time and then rendered by the panel when it's opened.
We made extensive use of 3ds Max's custom attributes in the animation rig.
At its simplest, they're Maya-like, animatable, user-defined properties stored in a parameter block.
Max then lets you embed user interfaces to interact with these user-defined properties.
But it's all Mac script, so you can embed some pretty complicated functionality into these custom attributes.
And they live in the rig, so that makes them extremely portable.
I told you this was going to be a postmortem.
We made a number of decisions during the course of the production that worked out pretty well.
The rigging system was, for the most part, data-driven.
We used XML as our data format.
3ds Max's integration of .NET made this easy.
Everyone has system.xml on their computers.
JSON might have been a better format for data, but it would have meant distributing additional DLLs to the artists.
It's easier to do Perforce merges on text data files over binaries or even over code.
As I mentioned earlier, we used one big metadata file called rigbuild.xml, which is a huge file containing a flat list of characters with subelement pointers to character-specific XML files.
Everything in the rig was broken down into text-based data descriptions.
XML files describe their skeletons, layers, sets, skinning, poses, animations, and where to find all that data.
Looking forward, it might be beneficial to abstract this data a little further and have the file point to character-specific metafiles containing the sub-element file locations for each character.
It would make it easier to find what you're looking for in a smaller file.
I need that. I'm still on this one.
Including vertex and face data in our skin files was pure win, though there is enough information to recreate original meshes and skin wrap to updated topologies.
One of our tech artists recently used that data to allow customization artists to preview part meshes in Max while they construct new armor parts.
In the future, we might add more mesh data, like UVs or vertex normals, to totally remove the dependency on a clean mesh living in a 3DS Max scene.
but I think we'll probably end up moving the data over to FBX to make a future switch to Maya easier.
There are two types of artists.
The ones that give way more feedback than possible to respond to, and those that bang their heads against the tools in silent frustration.
The artists that provide feedback are the ones that help make the tools and rigs better for everyone.
No tool or rig is perfect, and some requests are ridiculous, but listening, responding, and prioritizing is important.
We can't fix tools if we don't know how frustrating they are to use.
In a perfect world, the technical artists would be using those tools too, but schedule constraints cause this to happen far less often than desirable.
There's also two kinds of meetings, good meetings and bad meetings.
We had a lot of meetings and most of them were good ones, solution focused, don't veer off topic and make sure everyone has all the information they need to make informed decisions in the meeting and on their own.
We had daily check-ins and bottleneck checks.
I think most studios do this nowadays, but it wasn't always as regular earlier in my career.
At Zoss, we generically refer to it as Scrum, but it's not really capital-S Scrum.
Producers tend to focus on checklists of tasks.
In the art pipeline, there are just too many nested dependencies to be too flexible or agile with the schedule.
Modular rigging made our timelines feasible.
We can prototype new monsters on existing modules and branch them later if we need something more unique.
Rigging speed was king when creating 90-plus different monster rigs.
For the animator, everything is familiar and consistent between rigs.
Further along into production, fixes and modifications into behavior on one rig can propagate to older stuff.
For instance, we made significant fixes to the snake tail rig to correct flipping issues, and we were able to apply that fix to the other monsters that used that module.
I sat in the animator bullpen.
I could peek over my shoulder, over my monitor, or over the cubicle wall to communicate with the animators who were using the rigs that I was building.
they could do the same to ask me questions.
It required a minimum of energy to hop five feet to help someone at their desk.
Finally, we had a ton of experience on our team.
Two out of six of our senior animators were former lead animators at their previous studios.
Across the rest of the art team, we have so many 10-year game dev veterans.
When you're talking to people with this level of experience, it makes explanations much easier.
Veteran artists often have good technical chops.
They had to, to be pretty technical, to have been a game artist back in the day.
Sorry for the cliché, but experienced developers are the most valuable asset a game studio has.
The politically correct version of what went wrong, but I believe that every mistake is a learning experience.
We should have pushed harder to adopt Maya as our DCC.
Working around 3ds Max's limitations is a full-time job.
For example, 3ds Max has various UI features that are just not scriptable.
Also, certain features like skin are only accessible in script if it's selected in the UI.
And for speed, complex rigs such as ours can be downright sluggish in Max.
referenced rigs would have made rig updates infinitely easier and may have made some things possible to update that we were just too afraid to do because of the time, labor, and potential for broken animations after a batch update.
Ahem.
We should have pushed harder for better game tech, such as cloth, dangly bits, hair, foot and arm IK, and state machines.
All we had was export the entire skeleton as an FK animation, and a programmer tied a lot of it together behind a wall of code.
We used RAD Game Tools' Granny as an animation platform, but we weren't even using all the features offered in that system.
Even though we fought against it until the end, eventually, pretty much every humanoid— and player character used a single animation set.
The set grew and grew and had infinite refinement passes, but at the cost of variation among races and genders.
The physical and skeletal variation between races and genders had to be reduced to almost nothing because our retargeting couldn't handle it.
We had poor man's arm IK for females using two-handed weapons, but it was difficult to set up or edit.
There was a word I saw players use to describe our animation in beta.
Janky.
In my opinion, our animations looked pretty consistently fluid and believable in Max, but the animations became extremely compressed after export.
We experimented with flatter hierarchies, but that caused issues with blending upper body and lower body motion.
and it was during our flirtation with impacts driven by code, which didn't understand a non-standard hierarchy.
We did have a processor written that was supposed to isolate and compress separate branches of that skeleton hierarchy differently, based on the motion in each animation, but I don't think it ever made it into our pipeline.
Compression was worst where it mattered the most.
Feet, which were supposed to be planted on the ground, jittered visibly.
Weapon arcs at the end of a long chain from the root up the spine down the arm terminating in a weapon bone became zigzag W shapes instead of smooth arcs.
Our exporter, in conjunction with 3ds Max, sampled subframes, making single-frame mode switches problematic.
Similarly, we had fake parent space switches for sheathing a weapon temporarily to free a hand to cast a spell.
And that looked fine in Max, but never worked perfectly in the game, because the sheathed weapon was still a child of the hand.
The sheathed weapon was still a child of the hand.
For a game of this caliber, being limited to so few bones was harsh.
At launch, we had no skeletal LODs.
We made a lot of sacrifices and compromises for the bone limitations.
We really could have used more than three bones for a cape.
four bones for a robe or two bones for hair, or even nine bones for a hand instead of a full 16.
Full voiceover and first person view were tacked on late in development and saw their scope creep wide open without getting the tech or manpower to support it.
We released the game at launch with only a single bone flappy jaw.
We finally adopted face effects for NPC conversation post-launch, but really should have planned better and shipped with it in the first place.
Something I've been interested in exploring lately is some studios moving away from a rig that does everything at all times, and using a lighter base rig with extendable control construction tools.
Our animation rig was a monstrosity of confusing panels and options.
It was heavy and slow, and the built-in configuration options made overhauling modules impossible without breaking existing animations.
A lighter, simple rig would basically just be an FK rig.
You want IK arms? Push the button to construct an IK control overlay on the fly.
And then, possibly collapse it back to FK when you're done.
Especially if we move to adopt motion capture in the future, this could be the way to go.
As mentioned several times, 3ds Max does not have usable referencing.
So our rigs were duplicated in every animation scene.
We should have implemented frequent automatic batch rig updates to ensure all of our animation scenes were always on the up-to-date rigs.
When things broke, we would have been able to find the offending systems as soon as they failed rather than sometimes months later.
Always listen to your instincts, otherwise known as don't believe the artists.
Animators swore they didn't want a control picker UI for three years, until they did.
Part of that was different animators coming onto the team with different methods and styles of working.
Always assume animators need more than one character in a scene.
Our namespace tool had too many limitations and most tools were written with the assumption that there would only be a single rig in a Max scene.
Again, the animators said that they would never need more than one rig in a scene because there was no tech to support paired animations.
Later on, we realized we needed to sync multiple characters in cutscenes and mount rider animations.
The hacky namespace system I put together for one-off cutscenes ended up being used for entire mount rider suites which then made it difficult to update the rigs in those scenes with tools that didn't support the namespaces.
It was like an animator tool's arms race.
I started out with good intentions in keeping our MaxScript tools strictly layered between library functions, tools functions, and UI functionality.
But eventually random functions in the tools layer ended up being cross-used in other tools.
Encapsulation of functions into structs wasn't a thing that was obvious in 2008, but became standard across the tech art forums by 2011.
Some of our tools, especially UIs, were updated to this format, but many old tools and libraries remain as big clumps of global functions.
The iteration cycle on our customization rig was pretty bad.
It took 20 to 30 minutes per iteration from a max side fix to seeing it in game, especially if I had to rebuild the rig.
Building the template skeleton, building deformation rig, building the animation skeleton, and merging in the raw meshes only took 30 seconds.
The slowest part of the build was loading the skin weights onto dozens of meshes.
It took 10 minutes.
I think part of that huge limitation is...
part of that was the huge limitation in Max.
The skin modifier requires that it is active in the modifier panel to do any scripted modifications.
Even with redraw off, it's slow.
Then the export took five minutes.
And there is no hot loading of the customization rig in-game, so we needed to fully quit and restart the game, which took another five to ten minutes on our dev server.
Finally, for most of the project, there was no technical art team.
And what did exist was not seen as being at the same level of the other art sub-disciplines.
We needed more tech artists earlier.
For three years, it was just me.
And I was assigned to report to the lead animator.
For the next two years after that, it was just me and a junior tech artist, both under the lead animator.
We were not seen as equal as other disciplines, and therefore had little voice or advocation in the studio.
Finally, just a couple months before launch, we hired a lead technical artist, merged one tools programmer and one technical modeler onto the team, and hired another tech artist with a programming background.
But it was too little, too late.
By the time this happened, we were in code and art lock for launch.
The Elder Scrolls Online has been in release for 11 months, but as a live MMO, we've released tons of new content and features since launch.
ZeniMax recently announced that we're going to transition to buy-to-play, which means there will no longer be a monthly subscription required to play.
And we're looking forward to our imminent launch for PlayStation 4 and Xbox One.
I wanted to give a shout out to these fine folks who contributed to the game.
And what time is it?
10 minutes left?
Anybody have questions?
OK.
Thanks.
Thanks for coming.
And hope I didn't put anybody to sleep.
Hello.
Hey, got one for you over here.
Oh, hi.
How are you doing?
Good talk.
Thank you very much.
Yeah, so you were talking about trying to push for Maya and adopting that.
Would you see that as a realistic goal this far into the project?
I'm assuming it's a live game that's going to have continual support.
Right, well we are looking forward to a future product and you know nowadays with FBX it's fairly easy to get stuff from Max to Maya as long as the rig on the other end kind of supports the input.
So we're talking about doing some of that, just like rebuilding our system in Maya and then loading all the animations into it and seeing what happens.
So further on in that, I've worked in environments where 3ds Max and Maya are both used in the same environment, and it can be a little tricky.
So if that was adopted for the animation pipeline, would you also push for that being adopted by the environment and the rest of the studio?
Probably not.
It would mostly be for the animation team.
So they keep that separate.
Yeah.
OK.
Yeah.
Well, thank you very much.
I have a question.
You mentioned about IK, so which method do you use to solve IK problem?
Which what?
I mean, you use which method to solve a universal kinematic problem?
You only use the built-in IK handler, or you have your own?
No, there's a built-in solver, whatever the.
Okay.
The hierarchy independent or whatever it's called.
And another question is I heard a saying that if you want to import your animation into game, you need to remove the IK before you do the import.
Well, it just samples the skeleton as if the whole thing was just an FK.
So, I mean, I know you need to bake all the animations into frame and then import.
Do you think it is necessary to remove the IK before you import the...
No, the exporter just samples whatever the skeleton is doing at any particular frame.
It doesn't matter how the skeleton is driven.
I have that one tool, the dynamics tool, where I'm changing the constraint on the skeleton to go from the regular rig behavior to a whole separate set of controls that have no hierarchy whatsoever so that you can break skeletons up or do some crazy stuff with golems that are made of rocks and stuff, make the rocks all fall apart and tumble around.
hierarchically in Max, that's still a hierarchy, but controlled entirely by a separate set of controls.
The thing is, I think if the animation is down, whether IK is existing doesn't affect the animation itself, but it also takes some CPU time, so I think to the efficiency part, maybe...
I heard a thing that if you want to, like, improve your efficiency, you need to remove the IK, but I don't know.
Yeah, we don't have any IK in game side in the client.
So it's all FK, yeah.
Anybody else?
Hi.
I'm not sure this is a relevant question, but on the Elder Scrolls line, there's a lot of dialogue for NPC.
And I know they have a lip sync on it.
And that animation was manually created by an animator, or there was kind of a technical thing going on.
Yeah, well at launch, all we had was the jaw flapping, like one bone, and that was driven by just the audio waveforms.
After launch, we implemented face effects, and that was all generated procedurally, automatically, by the face effects system.
