I'm really glad I can be here.
Actually, I thought this time I might not make it through immigration because of my Pakistani name.
But I did, and well, this is the first time for me here at GDC.
It's been really enjoyable so far.
And yeah, I hope you'll enjoy this talk as much as I've enjoyed all the talks that I've been to.
But before I start to talk about data-driven granular synthesis, I need to, oh yeah, first of all, I need to remind you to turn your cell phones on after the talk.
And before I start to talk about data-driven granular synthesis, I just want to introduce myself really shortly.
My name is Sajjad Siddique.
I work as an audio researcher at Square Enix in beautiful Tokyo in Japan.
And yeah, so audio researcher.
I've been asked by lots of people, what does it actually mean?
And well, to tell you the truth.
I'm really bad at sound design.
And I think that's not only because my ears are really bad, but also because I think it's more because of my bad taste.
And I'm also really bad at programming.
So people who see my code laugh at me.
People who want to use my code rewrite it.
So calling myself an audio researcher just turns this, how should I say, deficiencies into.
maybe well into charming qualities at least that's what I like to believe.
Past projects that I've been working on as a researcher are many projects where I do audio synthesis for video games.
For example, I've been working on the car engine sound for Final Fantasy 15.
I've been also working on the sound of destruction for some other video games, given some presentation about that at the Computer Entertainment Developers Conference.
in Yokohama in Japan.
I've been also working on morphing of sounds and also morphing of granular sounds.
The method that I'm going to talk about today, and just to be sure that we're all on the same page, because I know the title can be a bit cryptical.
So data-driven granular synthesis is, well, to explain it really shortly, it's a sound synthesis method that can be used.
For video games and special small way from some of those small bay from so-called grains and that's why we call it from less than this is right. I'm going to explain that in more detail later of course.
But before I do so I also want to let you know what sounds you can actually use it for so that the talk maybe gets a bit more meaning for everybody.
My favorite sound and my favorite example is the son of rain.
I think the method can really like explain very easily when you when you think about rain so that's why I'm going to use rain also in this presentation.
But you can also use it for the sound of running water like like a small stream or something or like running water into a basin or something like that.
It can also be used for different sounds like flooding clothes or paper or something like that for cracking fire and lots of other sounds.
You'll hear some examples later on.
Now if you talk about the sun of rain.
Then imagine you have a video game where you you were where you want to have the sun of rain and now of course you know that there are lots of different rain sounds and the thing that you probably have to do in the beginning is to decide what kind of rain you want to have in your video game.
And if you use the traditional approach to implement the rain sound then you would go with samples that you play in a loop and those samples will have to have a certain length.
Because you don't want the player to notice that they're in the loop, right?
So, like, I don't know.
We're talking maybe 20 seconds or something like that.
And, yeah, even if you decide that you're only going to do, only going to use two different sounds, you still need those two long samples for your video game.
And that means if you use high quality and samples that are like about 20 seconds long, you end up spending around seven megabytes per sample.
Now, of course, we as audio people know that this is nothing in comparison to what graphics people are using.
But still, at my company, I've been told, Sajat, this is too much, we need to reduce this.
So, yeah, this is one of the problems that I think it's not a problem because I think we should be giving them to memory.
Oh, sorry.
But there's another thing that I think is a problem with this approach, and that is the transitions, for example, between different sounds of rain.
Or it's really hard to apply some variation to those samples.
You can of course do fading, but if you just apply fading...
you are not going to be able to have very natural sounding transitions or for example, if you want to start the sound of rain during the game, how would you make it just fade in the sound?
Well, maybe that will do, but it doesn't sound very natural, right?
So, to summarize the challenges or the motivation behind the project that I've been working on, it's because we looked into ways to save some memory and I wanted to have something where we can like have lots of variation and play around with the sounds and yeah, change the way it sounds during the game.
So the solution that I'm proposing is data-driven granular synthesis.
And yeah, well, first of all I want to explain granular synthesis in a bit more detail.
I guess lots of you are familiar with granular synthesis.
Like, raise your hand who knows the term, who's been working with it.
Cool, okay.
Yeah.
Well, definitions of granular synthesis, it's a very flexible method, a very powerful method, and that's why definitions vary a lot, like between different people and between different implementations or ways to use it.
And, um...
Yeah, it's a bit difficult to find a definition that fits all of the usages of granular synthesis, but the definition that I came up with is granular synthesis is a synthesis method used to synthesize granular sounds.
I do have to admit that this definition is absolutely useless, because if you don't understand granular synthesis, then of course you don't understand granular sound, right?
So let's talk about granular sounds.
Think about the sound of rain.
The sound of rain is actually the sum of a lot of raindrops falling onto a surface.
And we're talking about 100,000 of raindrops or something like that, right?
And those raindrops converge into a single sound.
And the reason why I call this single sound or I call this sound of rain a granular sound is because those single sound events, like in the case of rain, that would be the drops hitting the surface.
can be called in the context of granular synthesis, they can be called grains.
and a granular sound consists of grains.
So granular sound is the sound that consists of a lot of single sound events.
In the case of water, it would be air bubbles trapped in the water that are resonating at the resonance frequencies.
This may be an example that's less straightforward to understand, but also in the case of water, it's actually like the sound of water is, or consists of a lot of single sound events that are also quite similar to each other.
To synthesize such sounds, we use granular synthesis.
And the method is actually really quite simple, because all that we do is we need recordings, or we need waveforms of those single sound events.
And we just take a lot of them, and we mix them together.
In practice, of course, because we have limited memory and we cannot record an unlimited number of sounds, we limit the number of grains to, say, for example, a few hundreds.
And to increase the variation during synthesis, what we're doing is we are applying random attenuation to them.
So every grain would be like, mm.
Once it's played like a full intensity and then the next time is going to be used maybe half of the intensity and yes, on the very 8 the intensity randomly and then we place them like the attenuated rains replace him into the final mix at random points in time.
And we do that for certain number of grains per second and like for example for heavy rain. We would use lots of friends for seconds a what like 100,000 and for very light rain which is I don't know only like 100,000 grains a second or something like that.
The random attenuation that we are applying to the sound is very important in giving the sound a certain character.
And for example, in the case of rain, we would use, or our mix would look like this.
We have, like here on the horizontal axis, you have time.
And on the vertical axis, you have the amplitude of the grains.
So that's sort of the random attenuation that we are applying to the grains.
You have.
a lot of grains with low amplitude.
And you have only a few grains with a high amplitude.
The reason for that is, or the reason why this gives us a natural sound in the case of rain, is because if you think of the sound of rain, there are only a few raindrops hitting the surface close to you.
But the number of raindrops hitting the surface far away from you, of course the surface is a larger area, so that number is huge.
And this is why there are more grains, or we need more grains with a low amplitude.
And one interesting thing about sounds like this, sounds of rain and other natural sounds, is the fact that actually those grains with a low amplitude all converge to noise.
So we can update our granular synthesis.
And instead of taking a huge number of grains per second that we have to place into the mix, we can also just reduce the number of grains per second that we put into the mix and add some noise in the end.
That gives us an optimized version of granular synthesis, which runs much faster.
And the sound quality is still the same.
So yeah, for video games, that's, I guess, the most intelligent approach.
Yeah, but now, of course, the question, where do we get those grains from?
Where do we get those single sound events that we need for granular synthesis from?
Like for rain, where do we get the sound of raindrops hitting the surface from?
Of course, what we can do, what we can try to do, is to record those sounds.
I've tried it and I've failed.
But I have to admit, I'm not really good at recording.
So I'm sure that if you do it the proper way, maybe you're able to catch the sound of those raindrops if you have a nice studio and you do good setup and stuff like that.
But also, if you go for recording, you should keep in mind that different, like, rainfalls on different materials. So you have to do, like, recordings for different surface materials and stuff like that. And maybe drop sizes can be different and stuff like that.
So you have to, well, I guess recording is actually a lot of work.
And also, if you want to go with the recording approach, then you still need to know what your noise looks like if you want to use the optimized granular synthesis.
What kind of noise do you put in the output to simulate all those raindrops that fall on the surface very far away from you?
And the most difficult thing maybe is to find a good random attenuation distribution.
So.
I told you before that lots of raindrops need to have a small amplitude and only a few raindrops need to have a high amplitude.
But what exactly is the ratio between those two?
So you still need to, I guess, just experiment and figure it out by playing around with the parameters.
Yeah, I've tried that and I found it very... well, it did work and I also got...
I love sound synthesis, so what I did, I managed to synthesize the sound of raindrops falling onto the surface.
And with that I actually got quite nice results, like I got sound that was sounding, well, decent, I guess.
quite natural.
But the problem was for the synthesis of the grains and also for all the other parameters of the granular synthesis, I had so many parameters that when I showed it to the sound designers in my company, they were just like, what is this parameter for?
And I tried to explain it to them, but they were so abstract that it was actually really hard to explain.
And I guess if you didn't build the system yourself, then it was really hard to use.
So I realized that's not a good way to go.
And then I thought, we already have all these beautiful samples of rain sounds lying around in our service, and we should make use of them.
So what I tried to do is extract all this data that we need for granular synthesis from existing recordings.
And that actually worked quite well.
And this is why, well, this is what the presentation today is about, data-driven granular synthesis. So we take data, and that data is in the form of a recording of existing rain sound, for example.
And we extract all the data that we need for granular synthesis from that sound.
The method that we're using to extract the grains and the noise and all the other parameters, well, I'd love to talk about it in detail.
Unfortunately, I won't have much time for that.
So just to give you a quick summary of how it's working.
First of all, we estimate the noise in the recording of the rain sound.
We do that by cutting the sound in the frames, then we convert all the frames to the frequency domain, so we get the spectra.
Then we look at the spectra, and of course the loudness will be different between all the different frames, and we estimate the noise by looking at, say, just the 15 or those frames with the lowest amplitude, with the lowest energy, because we just say that, OK, maybe in these frames we just have the noise.
There are no, like, loud drops in those frames.
This is what gives us the estimated noise spectrum.
And then we subtract.
that noise from the original sound.
This is a technique called spectral subtraction.
That's, I guess, very commonly known in signal processing.
And after doing so, we are left with the clean signal.
It's not really clean, but let's say the signal with reduced noise.
And...
We cut out the grains just from the loudest frames that we have in the signal.
This again is actually quite a simplistic approach, but I found it to work quite well, actually.
So we just, we do it in a greedy manner, like we just look for the loudest point in the waveform or in the spectrogram, and this is our first grain. We cut that out.
And then we continue to look for the second loudest, and so on.
And, well, the problem is that, of course, if there are a lot of grains per second, then, of course, those grains are overlapping, right?
So, if you cut out part of a grain, then the tail of the grain is buried under the next grains.
So, it's not possible, or it's not easy to get the tail of the grain out of the recording.
So what we are doing is we estimate the tail of the extracted grain.
If you want to know more details about the methods that we're using, then, yeah, as I said, I don't have much time today, but please have a look at these two.
Well, the first one is not published yet.
I'm going to do a presentation at the ACE Convention Berlin about the technical details of the method.
But there's already paper which is, well, it does not contain the estimation of the tail, for example, so it's, well, it's not the most up-to-date version of the algorithm, but still, if you look at it, you get an idea of how it's working.
And also, if you want to know more, you can drop me an email.
I'll show you my email address later, and maybe I can also send you a version of the new paper.
You can find the second paper on the website of my company.
So yeah, feel free to have a look at it.
Okay, now of course we want to know what it sounds like.
So, well I've some sounds here, but actually I think I'm going to jump straight to the demo.
So here I've already loaded the sound of rain.
This is the recording.
And now we're going to do an analysis.
So in this analysis, first of all, we estimate the noise, and now we're extracting the grains.
that takes some time because like we have to convert all the frames to the frequency domain, then we have to estimate the tails and stuff like that.
We need some processing time for that.
And the next thing that we are or that this program is trying to do, it's trying to find an optimal configuration for the loudness parameters of the grains that are placed randomly into the mix.
And the optimal configuration is supposed to be a configuration that gives us a sound that sounds as close to the original as possible.
So it's still running, but with the parameters that we've already found.
Sounds like this.
And it's still trying to optimize it further.
So these values are going to change.
But I don't want to wait for that.
So I think...
we're just going to play around with the values ourselves.
For example, I can increase the number of frames per second.
Or the amount of noise in the sound.
Or the parameters of the loudness.
And then yeah, I was talking about transitions before.
So for transitions, what you can do, you can create different configurations of these parameters here, and then place them onto this map.
And then you can sort of morph between different configurations.
Let's try that.
Okay, maybe less noise.
Yeah.
Yeah, and all this like different variations of the sound can be created from just a few seconds of recording that we're using.
Like, I think here I used like five seconds or something like that.
Okay, let's try a different sound.
For example, oh, I like the paper one.
Let's start the analysis, this is what the recording sounds like.
And now, oh yeah, well, the sounds that I'm using, that I'm analyzing to get the grains and everything, they're mono sounds.
But since we placed the grains into the mix randomly, we can actually also apply random panning to the sounds, which means that even though the source sound is a mono sound, we can create sounds with an arbitrary number of channels.
by just applying some random panning to the grains that we place into the mix.
Okay, let's listen to this sound.
The paper.
**paper rustling** **paper rustling** **paper rustling** **paper rustling** **paper rustling** **paper rustling** **paper rustling** And now, my idea was that, for example, here you have a slider that goes from zero to one.
You could attach that to a parameter in the game where I don't know, you have, for example, some movement of some thing with lots of paper on it or something, and then you would just go from movement is zero or very fast. Stuff like that, right?
Okay, let me go back to the presentation in PowerPoint.
There are, well, I've already, of course, I've talked a lot about the advantages of the technique.
And well, there are the flexibility.
I've already told you can use an arbitrary number of channels.
We also need less memory.
So 2.5 megabytes is really the maximum that we need, but actually we can, without an audible loss in quality, it depends on the sound that we're going to synthesize, but we can reduce it to one megabyte or even less, because the size of the grain, or the size of the grains, like length of the grains, well, for some sounds they can be shorter, and for some sounds they have to be a bit longer, but for example, for the sound of rain, it doesn't matter if they're really short.
So for the sound of rain, I think I could shrink it down to like, 500 gig kilobytes or something like that.
And unfortunately, there's also some disadvantages.
Probably you can guess that, because we are synthesizing the sound in real time.
So we need some processing power.
I think it's not too slow.
It takes us about 19 milliseconds to generate one second of high quality stereo audio.
So yeah, it does run in real time easily.
You've seen it in the demo.
Yeah.
This nearly concludes my talk, but yeah, future plans.
The first one, I have to warn you, is a bit embarrassing.
I actually want to use it in video games.
I think everybody here at GDC has lots of experience to use stuff like in video games.
And I'm talking about a technique that's not in any ship title yet.
But we are working on it.
I cannot tell you any more details, but I hope that you're going to see some stuff soon.
You've seen the analysis process take some time.
Of course, I want to speed, I want to accelerate that.
And there's also some scope of improvement for the grain extraction algorithm.
But yeah, those I leave for the future.
So yeah, this concludes my talk.
Thank you so much for your attention.
If you, thank you.
If you want to get in touch, here's my email address.
Drop me a line any time.
Yeah, well, I think we have some time for questions, so if there's anybody who wants to ask something, yeah, please, there's a mic here.
Step up to the mic.
Thanks for the talk.
My main question was, you parametrize gain and variance for the gain for your grains.
So are you just applying a gain to your recordings or do you actually pick from the data loud and quiet grains and select them based on a sorting or something like that?
No, I'm applying the gain only to the grains.
So I'm not sorting the grains.
But actually, you make a very good point.
Because by applying a high gain to grains that had a low gain in the original sound, you sort of distorted it in some ways.
Maybe some of you heard that.
And that is actually a very intelligent idea, like to sort the grains in terms of loudness, and then pick the loud grains for where you want to have loud grains, and the grains with small amplitude, where you want to have grains with a low amplitude.
That will, I guess, make it sound even more natural.
Yeah, cool.
So I was going to recommend this thing by IRCAM called CutArt.
Oh, okay, yeah, I know CutArt.
It's like a 2D plane system that lets you sort grains.
It's wonderful, I love CutArt.
All right, thanks.
Thank you.
Hi, thank you for your talk.
Thank you.
I wanted to ask, how long did it take you to develop this, and how big was the team that was working on it?
The team working on it is, they're all here, because it's just me.
And it took me not too long, actually.
So you've seen that I published one paper about it in 2015.
But yeah, don't think that I've been working only on this since 2000 and whatever 15 until now.
Like there's lots of different projects ongoing like going on at the same time.
I think if I add up all the work that I did for this, it would be like 3 months or something like that.
But I was really also playing around a lot with like different parameters for extraction and stuff like that.
Cool. Thank you.
Thank you.
Thank you so much for the talk.
Thank you.
So when you're implementing these into a game, are you writing your own tools for that?
Or is it working as a plug-in?
The plan is that I develop it as a library.
And that library is sort of in some ways, no, it's not really a plug-in.
So yeah, it's more like writing on tools for that.
OK, thank you.
Thank you.
Any more questions?
Please step up to the mic.
Hi.
Hi.
Terrific demo and explanation.
Thank you.
You touched upon different materials, rain falling on different materials.
Yes.
Obviously, starting with small samples on the different materials would allow you to localize, you know, what...
a pond and you're under an awning and pavement, etc.
Do you foresee in the future being able to somehow model or use some other type of synthesis or convolution to create that in real time as well?
I actually did something like that because I think I mentioned before I was actually first doing synthesis of the sound of, for the sound of the raindrops, right?
And when I was doing that I was doing convolution like with the sort of impulse that hits the surface and there could be different surfaces and different impulse responses for different materials and stuff like that.
I did play around with that but the problem is most of the time we actually, I think it's a terrific idea, I think it's a terrific technique that we should use.
Yeah.
Also, we need to start things slowly, right?
So first of all, try this, and then go on.
But I think it's a terrific idea.
Thanks.
Thank you.
Yeah, so if there's no more questions, or if you're too embarrassed to ask, you can always drop me a line.
And yeah, anyway, time's up, I guess.
If you want to talk to me, I'll be around.
And yeah, thanks again for coming.
