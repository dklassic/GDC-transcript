Hi there, and welcome to The Sound of Anthem.
We're very excited to be here and have an opportunity to show you some of the things that we accomplished on this game.
Today, we're gonna walk you through some of the high-level concepts of the core audio experience but then also dive deep into some of the more interesting sound challenges we faced while working on this project.
We at BioWare take great pride in building unique, engaging audio experiences.
That pride is further bolstered by seeing all of you here today with an interest in what we do.
Our studio is full of passionate and dedicated developers trying not only to make the best experience for the players, but for ourselves as well.
I mean, why else would anyone move to the Canadian North or the deserts of Texas, if not to work at Bioware?
With that, let's start off by telling you a little bit about who we are.
Hey everyone.
I'm Cody. I'm an audio director here at Bioware.
I've been working in the industry for a little over six years now.
I got my start doing cinematics for Dragon Age Inquisition after graduating from Vancouver Film School for sound design.
On Anthem, I had a large chunk of product ownership for the game, including overseeing the gameplay audio crew and managing technical system design for audio.
As for me, I've been working in the games industry since 2006.
Initially, I studied out of the Art Institute of Vancouver, and then like Cody and many of the others at BioWare, I took the audio program at the Vancouver Film School.
More relevant to our talk, I've been at BioWare for the last six years, where among other things, I am the director of audio.
Up next, I want to introduce to you the core audio team.
This is our work family.
This is, or at least was, the Anthem shipping team at Bioware.
What Cody and I will be presenting today is a result of many months and years of hard work by these fine people, and we couldn't be prouder to be doing so.
Now I'm going to walk you through some of the core tenets and understandings about the audio vision and how we set ourselves up to execute upon those.
Anthem was a project that's been in the works for a number of years at Bioware.
Starting small, but even in its infancy, it had been a noticeable departure from the titles and play styles that we're typically known for.
Where previously we lent into fantasy of the Dragon Age or sci-fi of Mass Effect, Anthem was more of a combination of those, a science fantasy reality.
While you can see elements of our past franchises, with some of Anthem's focus on character development or exploration of exotic locales, it was primarily fixated from day one to bringing super heroic, rewarding and engaging gameplay, complex and deadly enemy encounters, all while playing together with your friends.
So with that, the audio vision we set for ourselves followed suit.
Our drive was to produce the heroic action and technology of the Marvel universe, set in the living, foreign, hostile world of Avatar.
In addition to that, we focused heavily on immersion and believability.
While the overarching idea of the game and direction of our sounds had them lean into the otherworldly and fantastical, our systems within the engine focused on treating them as if they had originated within the world as we know it.
These were systems of dynamic reverbs to be the glue that pulled all the sounds together.
Interior versus exterior mixing based on your position or sources and defined areas.
Early and late reflection calculations providing dynamic feedback from the area around you to inform slap off walls, et cetera.
Switching content tails on area type.
This is for weapons and explosions, having unique assets whenever they originate for wherever they originated on the map.
roll-offs based on the speed of sound calculations for accuracy in proper delays on big explosions and thunderclaps, and then in relative orientation to focus the mix on what the player is looking at.
The result being an immersive, but out-of-this-world audio experience.
Next I'm going to walk you through some of the tools that we utilized to achieve these goals.
To begin, our toolset, for those of you not aware, BioWare and nearly EA-wide, we use Frostbite as our development game engine.
Created initially by DICE in Sweden for the Battlefield games, BioWare was a pioneer in bringing a tool that was built for first-person shooters primarily, and then have it serve a wider variety of titles.
Our first game using it was Dragon Age Inquisition, where both Cody and I started with the company.
It's a very powerful tool, its main benefit being that it's set up in a way to allow teams to access all other portions of the game in one way or another.
So essentially, whatever you see in a level, or in a character blueprint, or in an animation timeline, you can access back to its root, then pull events and parameters off to inform your sounds. As you can imagine, it can be very empowering.
Here are just a couple screenshots of three of our audio patches to show some of the level's complexity on that back end after we've received events.
First, you can see it takes a modular approach to building up patches using nodes of various functions, moving from inputs on the left to sample playback and manipulation in the middle, to outputs on the right.
You can see that there are vast possibilities and complexity within as well.
This last patch here is our master patch for our player weapons, the primarily yellow stack in the middle representing many elements firing together to essentially build a sound at playback.
rather than just playing something preconceived.
Very similar to stacking up audio assets in a DAW.
I'm not going to go into much more detail about the tool set itself, as there are more interesting things to see and hear, but I just wanted to give you a brief peek behind the scenes to show that many of our sounds do follow this same approach.
Source building.
So a major foundation and tool for our work is finding unique and interesting source materials.
Over the project cycle, our internal libraries grow exponentially as we put heavy importance on gathering content that will help us create an experience unique to us.
With regards to recording, here are some pictures of a handful of sessions that our team has done in the last couple of years.
covering everything from dropping and smashing cars in a junkyard, handling antique knobs and switches, didgeridooing a didgeridoo, taking part in cross-team weapon recordings, we did big walla sessions, lots of ambiences, electro-sluice or electromagnetic captures of everything, plunging yogurt, blacksmith shops, a bumblebee in a water bottle, singing bowls, and squeaky floors, and on and on and on.
So in addition to that...
Modular synthesis was a big contributor for this project.
As you can see, our previous sound effects supervisor, Mike Kent, had caught the modular bug pretty bad over the years, leading to ship.
This condition, fortunately and unfortunately, spread within our team, seeing some people building their own racks and others experimenting more with our multiple hardware synths that we pass around the studio.
But from this rack, Mike had developed and defined many of our sound palettes and iconic earth-shattering content.
It inspired and often functioned as a communal collaboration spot within the halls of BioWare Edmonton, but always seemed to deliver something new and amazing.
Here are a couple clips of the team catching some modular source and some of the in-game content where the results were worked into layers.
Moving off of our tools, we're saying into some game content.
First of all, we'll talk about music.
Music is often seen as the most important element in a game, or any media for that matter.
It provides the emotion, the suspense, the drive to an experience that in many cases would have been void of it.
It's an understatement that in any of our projects, when the music is implemented, it changes from essentially being a tech demo to a living, breathing experience.
It always acts as a great unifier with the team at large, something for everybody to get behind.
With the music for Anthem, we got a mix of otherworldly sounds, driving, action-filled beats, and a mysterious suspense all in the right amounts.
To get this, something we do for all our games of Bioware is develop a series of palettes for our content.
We do this for defining various factions of enemies, suit variants in the case of Anthem, weapon types, areas within the world, or in this case, the emotional pillars of music.
So working with our project's creative director, And Lead Writers, we build a compilation of descriptors for the overall core emotions of the game that we then work with the composer to bring across in the score.
As you can see, the common themes mentioned before and the audio vision are represented here, creating a cohesive language for all to follow.
Now let's talk about our composer.
For Anthem, we selected an amazing talent in Sarah Shachner.
Before joining her team, she had brought music to several excellent franchises and films, from Assassin's Creed to Call of Duty.
She's been able to show great breadth in her content and that ability was needed with the direction we were going.
She truly is one of the best for creating a seamless combination of classical orchestra and choir, modular synthesis, vocoded vocalizations, and then often using non-traditional source in traditional ways.
With this, she provided a musical landscape that was not only full of emotion, but one that walked the line between a classic action game and this otherworldly sound.
So here's a little bit of that. ♪♪ At this point, I'm going to turn it over to Cody for a bit.
He's going to run you through some of the more in-depth implementations of the core gameplay experience with some examples of each.
Should be a great showcase of the complexity that goes into some things you might not have really thought about as you blast around the map.
Cody.
Cool. Thanks, Eric.
So since the large exosuits referred to as javelins are one of the biggest and most unique components of Anthem's action gameplay, we're going to do a deeper dive into how we went about constructing the audio for them.
As Eric mentioned, one of the core tenets of the initial audio vision was this idea of having everything feel grounded in reality within the game's environments.
This needed to extend to the suit design to deliver physicality and mechanical accuracy to the technology of the world.
This vision is most obviously represented by the Ranger and the Colossus suits, but we retain those ideas in every possible way for the more extravagant Storm and Interceptor suits as well.
Let's take a quick look at the ideas that we used for each of the four suits, as well as some short videos of the full suite of actions that each suit was capable of performing.
So starting off with the Ranger suit, this was intended to be our most accessible Javelin.
It's a basic soldier class, and from an audio point of view, we wanted this class to be a simple expression of the game's audio vision.
creating an obvious physical correlation to all the sounds used on the suit.
It's what we described as medium tech, so as not to go too far into a high tech or a sci-fi vibe.
It's also kept us away from the low tech and janky feeling.
Its thrusters are explosive and punchy with high frequency detail layers for the ignition triggers.
Its locomotion uses realistic servos and detailed mechanical movements to create a really clean but grounded feel.
So here's the Ranger.
So, then from a production and a technology basis, building the Ranger suit gave us the template for all the suits that came after it.
And while there were minor deviations from the design for that template, it pretty much held true throughout production.
Next up is the Colossus suit.
This was in every sense of the word our heavy class.
It's the tank.
The audio team affectionately referred to it as a garbage truck.
We wanted it to be the most aggressive and lowest tech.
It needed to sound dirty and weighty, but not messy, if possible.
And we pushed it the furthest with distortion effects.
in the thruster sounds to bring out that realistic rocket launch and crackle.
Its servos were really grinding and its mechanics were thick and chunky.
So here's what the Colossus sounds like.
Up next is the Storm Suit.
This was our Spellcaster class, and this really provided us with an opportunity to break away from the very grounded and realistic aesthetic of the Colossus and the Ranger Suit.
The suit wields the power of ember to create magical spells and propel itself around the environment.
So for the audio, we wanted to have a consistent feel to all the things the suit was capable of doing with that specific power.
So you'll hear a lot of granular and glassy, liquidy elements, as well as some rolling thunder to give the abilities a really grand, explosive feel in the environment.
Its locomotion contains more airy servos and then more resonant, plastic-sounding mechanics.
And then of course, we included additive layers for the cape flaps for the storm, because that's an iconic part of its look.
So here's what the storm sounds like.
Next up, the Interceptor Suit is our melee and gadget suit.
It's gameplay is speedy and agile and it was a really great opportunity for us to explore that high-tech vibe.
The concept of high-tech agility played in the sound design really nicely with a lot of tight, punchy thrusters, more sci-fi laser elements.
Another interesting part of the sound design were the servos.
One random idea that was thrown out was to reference Bruce Lee's iconic screams as a tonal basis for the movements.
In real, the principal sound designer for the suit actually did design a sound based on that idea.
So remember when you hear this little squirrely synth sound layered into the movements?
It was actually inspired by this.
Okay, here's the interceptor.
So, as I said, when we built the Ranger, it provided a good template for us to base the remaining suits on.
So let's go over what the shared components of that template were, and then I'll dive a bit deeper into some of the specifics of how we implemented this content.
So for the template, we had basic locomotion, so this is all footsteps and movement.
Thrusters, these were any actions that the suit uses its jet boosters for.
So this included all jumping, sprinting, evading, hovering, and flying.
The hovering and the flying had more specific logic for the looping components of those states to get them to react to the character's flight velocity, pitch, and lean.
Then we had swimming, which was essentially a simplified version of the flight loop system.
We had falling and landing.
We had sliding on steep surfaces.
We had melee both on the ground and from the air.
And then all the additional animation coverage for things like mantling, reloads, emotes, arrivals, victory poses.
And I'll expand on that system a little bit more shortly.
So first off with locomotion, like much of our content, our locomotion was built in modular layers.
The modular design gives us a lot of flexibility to better react to the animations by using real-time game parameters to control the sound's playback.
The layers built for the Javelin Locomotion were a Bass layer, which is a simple initial transient layer.
Then there's the Mech layer, which is the second transient and the mechanical detail.
Then what we do is we use the character's speed to scale a delay between the triggering of the bass and the mech layer, and then that kind of treats them like a heel and a toe respectively, for a more pronounced difference between walking and running.
That sounds like this.
And then we have the servo layer.
This is for syncing with our movement and filling the gaps between the steps.
We adjust the pitch of the servo layer based on the character's speed.
Interestingly, going against our initial instinct to pitch it up when the player ran or sprinted, we found that pitching it down had the effect of making the suit sound like it was under greater strain to propel itself forward.
Next, we have the Material layer. This is just a generic material sound with different content based on the javelin size.
So for this we used the current ground material's softness rating, and controlled which layer of these layers held the initial transient for the step.
So on hard surfaces, the base layer would be the base transient of the footstep, but on soft surfaces we switched and the Material layer held the initial transient.
And this helped prevent the stacking of multiple transients, which would have caused the sound to get muddied.
And then finally, the weight layer was just a simple low-end bump to give a consistent weight to the suit.
Our flight sounds for the game were built into three layers.
So we had the body, which is the main thruster sound, typically an engine afterburner distortion, with the wind to sell the movement through the open air, and then a noise layer to represent the environment the player was flying through.
The body and the wind layer had two blended intensities that were constantly crossfading between each other based on physics parameters.
Noise would transition between interior and exterior.
So controlling all three of these layers is something called Thruster Load.
We use the pitch and the lean of the character as well as the user's up-down stick deflection to generate a single value representing the Thruster Load.
This controlled the intensity of the sound as well as the intensity of the Thruster VFX, so flying straight down gave you more wind and less thruster, and flying straight up or banking brought more intensity to the boosters.
So once we had the base flight sounds established, we thought it would be interesting to try and get a little bit more reactivity from the physical environment built into the flying sound.
So for this we repurposed our Raycast system.
This is the same system we used to simulate early reflections in the game.
Doing these ray checks, we would know when you were flying past transient objects, as well as skimming a contiguous surface.
And we had one-shot whooshes as well as loops for skimming on the left and the right, as well as below the character.
This actually evolved into a really cool multi-department collaboration, whereby the system that the audio team had built for Flybys was able to be fully integrated with VFX and actually design to do things like the cooling of the javelin thrusters when skimming the water.
One of the challenges we had with the audio design for the suits was getting them to feel really heavy and complex without going overboard and layering too much audio onto them.
This issue was especially prevalent with lands.
We started the sounds for the suits landing state, of course, and had even conditionally swapped them based on the fall velocity, but we still weren't properly really capturing the full weight and energy of the one-ton mech suits falling through the air, impacting the ground.
So, to demonstrate this, here's what the lands sound like just by themselves in-game.
So, our first thought, our solution was to add a system for playing audio on the falling state as it would lead into the landing state.
So, at first we just had a simple wind loop that played based on the fall velocity of the character.
And that was kind of interesting, but we weren't really capturing the kind of dynamics that would make it feel dramatic.
So we need to be able to play one-shots before the landing impact, so we could get a lead-in woosh that would capture the energy better.
Luckily, animation had already built a way to generate the predicted time to land while falling through the air.
And then using that, we could hand-design one-shots of a fixed length and trigger them at the exact right moment.
For this we had three intensities of landing possible, depending on how far you had fallen.
So I mirrored that with the incoming one shots.
And then we had a hard land state, it was a specific state where you go impact the ground and have a short recovery time to get up.
So I made that incoming sound much longer to build up that energy.
We also had a second loop sweetener that was more chaotic.
And then with a consistent timing, I then could apply that to all of the suits.
It didn't take too long to get everything covered and the results were pretty good.
So for nearly all the javelin movements, we knew it'd be important to give ourselves the tools to hit all the animation content that the team would be making without having to do hand edited and bespoke audio for each animation.
So this process is pretty much identical to what we've developed on previous projects for taking cloth and movement fully.
Here we just needed to abstract the content to fit with the javelin sounds of servos and mech movements.
To do this, we basically identify a short list of event types.
so that we could design a bucket of Javelin-specific content that could be reused for all the animations.
The content for the event sounds like this.
So as a baseline, this would give us decent coverage, and then anything super specific we could layer on top.
These events then could be called on the timelines for the animations.
The event calls are all generic because this timeline can play on any javelin.
So the specific javelin that they were played on is passed to the sound patch on the backend to do the correct asset selection.
So here's an example of one emote tagged entirely with generic calls playing on four different javelin types.
All right, so now moving on from the javelins, the creatures of Anthem were always conceptualized to be that thing that brings forth a sense of danger and discovery to the world.
In order to help us deliver on this vision, we partnered with Boom Audio to get a couple brand new sound libraries made featuring some really incredibly unique creature source and animal source for our games.
In addition to all of our own recording that Eric mentioned, collaborating with Boom is a huge help in delivering a lot of these aggressive, visceral, and foreign sounding creatures.
Their source sounds can be heard throughout the game's creature sound design.
So here's some examples of some of the creatures in Anthem.
Interesting tidbit is that one of the source sounds for the arachnid queen at the end there was actually a really squeaky dishwasher that was in our old building here in Edmonton.
So moving away from the wild beasts of Anthem, we wanted to also talk about how we approach building a creature language through enemy vocal sounds.
It's a really difficult task that requires building a small collaborative group, working across several departments, including writing, art, and design.
Additionally, at BioWare, our VO is handled largely by our dedicated VO and performance department.
So it's really imperative that all these stakeholders work closely together in order to deliver the highest quality end results.
For the scars specifically, the direction for the pallet was that these are actually swarms of insect-like creatures and they'd actually just began mimicking humans in their appearance and their technology and ultimately in their speech, but it wouldn't be something that they actually were speaking any intelligible language.
So the challenge here was to create a characteristic for this faction that preserved the relatability and emotion of a language without containing any significantly recognizable speech.
Our first step with this was to engage with our writing department and start brainstorming ideas.
One of our writers wrote up a script for the Scar that she loosely based off some Hungarian that she had learned when she was young.
So she wrote up this cue sheet, and then we recorded a scratch session locally to test out how the language worked.
So here are some examples of raw audio from that scratch session.
Nesmir!
Zordik!
Unicorns!
Lockstodon!
Unicorns!
Feekehazor!
So we were able to take that and then start working through some of our ideas for processing for the scars.
So, as you can hear, having the language scripted was only half the battle.
Our challenge would be in providing a solid and reproducible direction for how we wanted actors to actually perform the lines.
The idea being that we need to be able to scale up to several different actors performing these scripts to get variations.
but that meant we really needed to put a lot of effort into capturing our gold standard example before we were able to trigger a pipeline to expand on the content.
For this, we hired some outside help.
We have a very talented voice actor named Olaf Jansson.
And Olaf actually specializes in creature vocalizations.
So he took a shot at our script and this is what he came back with.
See that shot?
Lucky for us, that performance did a great job of capturing the aggression and the menace we were needed from the palette.
Then after doing some extensive work on the processing change, we finally landed on the vision of a swarm of insects speaking a broken non-language through a janky radio.
Here's Olaf's performance after being processed.
So now of course, we had this fantastic example content.
It was much easier to get other actors to perform that script, provide consistent high quality performances than we could reliably batch process using our SCAR processing.
And then here, so here are final versions from other performances.
Okay, and that's all for me, so I'm going to throw it back over to Eric to talk about some of the systems we use to build Anthem's dynamic world audio.
Thank you, Cody.
So to wrap up our more detailed looks into the audio, I wanted to quickly walk you through a couple of our processes for getting the world of Anthem to stand up to a benchmark of realism in its function, but fantastical in its presentation.
The approach to Anthem's world audio was essentially having all elements work together to inform the whole.
Over the previous two projects, we started to build a core set of level audio tools that have enabled us to cover our vast worlds with great accuracy.
So we're achieving this by systems that deal with automatic mixing and switching of content, interior versus exterior prioritization systems, dynamic day-night cycles with advanced timelines determining density and frequency of wildlife to be heard throughout the days, supportive weather, sitting atop dynamic real-world based weather systems.
and then a set of tools that lean into procedural placement and data collection.
To that last point, essentially we have a way to tag up all the objects in our worlds, giving them unique identifiers to then feed back to a centralized collation point.
Those tags are then weighed against each other.
prioritized and eventually serve as drivers for which core ambiences to play, as placements for point source sounds, or as locations for contextual audio content to feed back from. So you can see in this little animation, the player essentially acts as a polling point for the data around you at any given moment.
That data then drives the underlying systems to do the things mentioned just before. You can imagine setting all this up was quite a task, but the result was a more or less automatic coverage of our large regions.
This then allowed our teams to focus on custom sounds and situations to override that base layer.
So let's take a quick look at that in action.
Just a little bit of context before we start here.
What you're going to see as we get close is the debug start appearing on screen.
These points it will initialize as blue, then they'll turn green as they're active and transmitting back to the system the type and size of content that they represent.
So because in this example, these are mainly foliage and it is daytime, the core ambience will switch to our day jungle palette with the corresponding reverb, area type settings, as well as that daytime point source content.
Next, peripheral to that but still a core level system that we lean on heavily is what we call Big World Sounds.
While we cover the base of the ambience with a quad of representative air winds, in some cases through that system we just spoke about, we use the Big World system to populate the world with all types of dynamic environmental content.
The system in its most simplistic form essentially picks sounds from a set of defined buckets and throws them randomly 360 into the world around the player.
Nothing new there.
This being at definable distances and timings between its triggerings.
These could be anything from the obvious like distant construction work, creature calls, and all the way to the more area-defining and mood-setting ambience swells that we lean into pretty heavily.
Essentially these are sounds of unseen source.
In its more complex form, we can then associate its placements to not just pick randomly, but to look for the points that I demonstrated in the previous section.
This then allows us to do things like having birds only trigger from placements in tree canopies, or random conversations playing off of actual NPC groupings, etc.
So in the end, you have a level of density and accuracy in the soundscape that is again more or less built procedurally as you move within it.
So let's take a little peek at what I'm talking about.
The first part of the video will be showing debug of cueing sounds looking to play back in the BigWorld system at that given time and location.
What follows is a mix of level ambiences with one-shot BigWorlds triggering from across the game.
The mix was a time where our adherence to organization and pre-planning really paid off.
This was compounded when put in combination with the game's more procedurally-driven mixing attributes.
With things like making sure our audio patches were as lean and approachable as possible, so the team could focus on making great sounds versus struggling with implementation.
We had RMS-driven EQs to give us control of different bus interactions on the fly, setting up our sound content to be prioritized properly against the many thousand other sounds trying to find a spot.
Having specific mix states and dynamically driven mixers set up for pulling out elements when we want to focus your ear.
HDR mixing based on loudness values we associate to given sounds or groups of sounds.
We centralize our reverb controls to have uniformity across all levels.
And then having implementation, asset prep and design standards set up for the team for content going into the game.
These and many other things allowed us to get a handle on what is a very busy mix environment.
Additionally, and lucky for us, this past year in live service has given us the ability to continue to evolve our mix in ways we haven't been able to in past projects.
So let's take a little look at what I'm talking about with some action and some gameplay.
This is fun! It's like finding lost treasure.
Lost treasure guarded by Scars.
Scars? What Scars?
Sorry, Freenancer. Looks like they're only at your location.
How is that fair?
Just lucky, I guess.
Thanks for watching!
And that's it.
On behalf of both of us, the Core Audio team back in Edmonton and Austin, and all the others that you can see scrolling by that have helped us and our team deliver another great sounding experience at Bioware.
We thank you so much.
Thanks for having us here today.
