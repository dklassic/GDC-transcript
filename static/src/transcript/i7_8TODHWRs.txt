Hello, everyone.
Before we start today, I'm supposed to remind you that you're all seasoned pros at this point, to please just turn off all noise-making devices, and also that a survey will be sent out at the end of this talk.
Just please fill that out.
It'll be super useful for me and super useful to the organizers of this awesome conference to figure out how to make the conference better for everyone.
So, hello, my name is Karen Tsai. I'm a senior engineering manager at Duolingo.
And thanks for joining me today on this final stretch of an amazing conference.
I'm here today to present lessons from Duolingo. How to make learning hard things easy.
Now, how many people have used Duolingo before?
Oh my gosh, that's so awesome to see. So many of you.
Well, if you don't know what Duolingo is, we're an app that tries to teach you foreign languages for free.
We have over 300 million users now, and we are frequently at the top of the charts for top-grossing and top-free education apps on both Google Play and iOS.
So you've probably heard of Duolingo, but you surely haven't heard of me, so let me give you a little bit of background.
In undergrad, I majored in computer science with a minor in neuroscience, focusing on reinforcement learning.
So the intersection of technology and of neuroscience and learning has always been a passion of mine.
I'd like to think that I also had an unofficial minor in League of Legends, because I probably spent more time on League than on my neuroscience minor.
And in the fall of 2012, I joined Duolingo as an engineer about three months after it launched.
So I was able to see the app grow from under 1 million users to now over 300 million, and all the lovely bumps along the way.
Nowadays, I'm the engineering manager for two teams at Duolingo.
There's the courses team, which tries to make sure we're teaching the right content.
and the core learning team, which aims to ensure that we're teaching that content to our learners without lowering engagement.
These teams are constantly having to think about how to teach foreign languages to people without driving users away.
So you might be wondering, what's an education app like Dolingo doing at a place like GDC?
Well, it's simple, really.
We want games, I mean games are fun.
We wanted learning a language to be fun.
So naturally we wanted Duolingo to feel like a game.
When we talk about product, when we talk about design, about monetization, we talk about games, not other education apps at Duolingo.
Many of the mechanics you see in the Duolingo app were inspired by games that you've created.
But the main reason we're here is that the challenges we face are very similar to that of any mobile game.
We all live in a world of really engaging apps and activities that all compete for our time.
As creators of a mobile education app, we had to think really carefully about how we can compete for users' time when the nature of what we do is traditionally thought of boring, confusing, or at best, really difficult.
We try to teach foreign languages, and we need to convince users that they should be learning instead of doing something else.
People are not usually super motivated to work hard at something voluntarily, especially when they aren't forced to.
When there's a teacher in the classroom, they can enforce learning or help students get unstuck when they're stuck.
And school is mandatory, so you are guaranteed to have students for a certain period of time and can employ really effective learning methods in helping them learn what they needed to.
But on an online platform where players are sacrificing opportunity costs of playing something else, you have to make sure the user is willing to get through on their own.
Even within apps, there may be parts that you need a user to get through that are just less exciting or harder to get through than the rest of your game.
And if it's too hard or too boring, you'll likely lose that player forever.
So the gaming industry right now is incredibly saturated, and it's definitely not easy to make a successful and appealing game that stands out among the crowd.
If the rise of hyper-casual games are any indication, lowering the barrier to entry on games is becoming more and more lucrative.
When designing an experience that appeals to all types of players, we found that we have to think along a lot of the same dimensions as games do.
We have to accommodate differing rates of skill acquisition, for example.
Some users will be really quick to pick things up, and some will just need a lot more time.
And the same goes for learning a language.
Our games should try to be flexible in allowing slower players to go at their own pace.
There's also play context.
Players might only have a small window of time to play your game, or they might be ready to play your game for hours on end.
If you offer something in your game that offers a satisfying experience for both short and long moments of play, it opens up when your game can be played.
There's also prior skill transfer. If a player is already familiar with your core genre or familiar with games at all, they'll likely need less onboarding than someone who has never played a similar game before.
And we need to make an experience that accommodates both of these types of users.
And finally, there's motivation.
We have users learning a language for travel, to connect with old loved ones, or in many cases, looking to better their socioeconomic statuses.
We have users who also don't actually care about learning a language at all, but just want to feel productive while doing something fun.
You likely have players playing your games for differing reasons, whether it be to achieve mastery, to be social with friends, or just to unwind.
And you'll cast a wider net if you create a good experience for all of these types of motivations.
So Duolingo's at GDC to share how we've tackled the challenge of teaching something hard to all types of users while keeping them around.
For the last six years, we've learned a lot through both successes and failures about what works well and what doesn't when it comes to teaching things to online users.
I mean, in the end, every game has a learning curve.
And every game is going to benefit from having a learning curve that doesn't drive users away.
Let's make sure that that learning curve isn't holding your game back.
So for the next 45 minutes or so, we're gonna talk about how to teach stuff while keeping things engaging.
First, I'm going to spend a chunk of time talking about metrics, everyone's favorite thing.
I'll share a bunch of A-B test results with you of times that our intuition was wrong, while talking about how we measure things, what things we measure, and why we A-B test everything.
Then I'll go over five principles on how to create learnable content.
We have a lot of really talented learning experts in the company that help inform product and engineering on how to create a good learning experience and good learning curriculum.
Then we'll dive into how to go about presenting that content in a way that drives engagement and is still good for learning outcomes.
I'll share a few tricks for what we use to make the whole experience motivating.
And then finally, we'll recap with some actionable takeaways and leave a few minutes for Q&A.
So first we'll talk about metrics, which is likely something all of you have a lot of familiarity with.
This presentation will be full of metrics to demonstrate the impact that applying certain principles had on our numbers.
Many of you already constantly probably look at user retention, LTV, CPI, DAUs, and all sorts of three-letter acronyms.
And many of you have at least heard of A-B testing, which is a core part of Duolingo.
So really quickly, here's a quick overview of what A-B testing is.
If you want to test a new experience, you take your users, you split them into two buckets.
On one bucket, which we call control, users get the old experience.
They continue doing exactly what they've been doing.
In the other bucket, they're in the experiment condition, and they get the new experience you're trying to test.
You take a look at the metrics for each group and see which one's doing better to decide whether or not your new experience is worth launching.
So Duolingo is obsessed with A-B testing every change to the product, and we believe it's the main reason we've kept growing.
To date, we've run over 1,800 tests.
And through them, we've steadily and incrementally increased D1 retention from 13% in 2012 to 55% now.
And I've picked up a bunch of these results to share with you today, simply because they help illustrate the problems we're trying to solve.
So here's a quick tip also for A-B testing, if you're already doing this on your platforms.
Not every user is affected by an experience you're trying to test.
For example, if the change you're testing is on level three of a dungeon, you really only want to take a look at the users who reach level three, not the users who are still on level two and not the users who are way past level three.
Everyone else is just noise that lowers the statistical power of your A-B test.
And if you don't have many users to start with, it will be really hard to run an A-B test that gets statistical significance if you don't filter out this noise.
So if you plan on A-B testing things, which I highly recommend, you could take your analytical tools further by only measuring users that would actually be impacted by your change.
To reduce noise, only count players who would have been exposed or are exposed to the changes you're testing, and you'll get a lot more out of your metrics that way.
So as a disclaimer, the numbers in this presentation are based on relative increase between control and experiment buckets, and not absolute numbers unless otherwise indicated.
Cool, so the first A-B test I'm gonna present to you is what I call the capitalization crutch, and those who use Duolingo are probably very familiar with this.
So here's an exercise from Duolingo with a word bank to help you out, and it's asking you to translate this English sentence into Spanish using the word bank.
Now, can you guess what the first word of the right answer is?
Yeah, you actually don't need to know any Spanish to guess.
It's the one with the capital letter, right?
It's VAS.
And users said this made the exercise way too easy.
And honestly, it was a bug that it was capitalized in the first place.
So, we wanted to fix it, but since we A-B test everything, we tested removing that lower casing instead of just doing it.
So now VAS is not capitalized in experiment condition.
And we were hopeful that this would be fine and we don't want to give away the first word anymore.
So this test actually revealed an absolute D1 retention drop of .5% when we did this.
So unfortunately, to this day, we still capitalize the first word in these word banks until we can figure out something that works better.
The lesson learned here is that even the tiniest change in difficulty can have really large and significant consequences on retention.
This is a problem because learning a language well and efficiently is pretty hard.
We knew if we made the app way too easy, people might just not learn anything at all.
So here's another one.
We often ask learners to translate the language they're learning into their native language.
So if I'm learning Spanish, I might be asked to translate a Spanish sentence into English.
This is a lot easier than, for example, trying to translate English into Spanish, because I wouldn't have to come up with the Spanish words myself, I just need to recognize those words and know what they mean in English.
However, it's a lot less useful for me when I'm trying to have a conversation in Spanish and I don't get to practice creating the language I'm learning enough.
So learners asked us to give them more translations into the languages they're learning because it would let them practice more.
This was a top request on our user run forum and app reviews, et cetera.
And we thought it was a really great idea.
So we ran a test just flipping one out of about every 20 exercises to the harder translation type, translating from one direction to the other.
And on iOS, this test yielded a decrease of 1.5 daily active users when we did this.
One out of 20 challenges was enough to have these results.
So this goes along with the earlier lesson that by making things harder, it's probably bad for DAUs and retention.
But what was harder to swallow about this test was that what the community asked for and what their behavior showed wasn't always aligned.
We encounter this frequently after this experiment as well on other experiments we've run.
It's always important to listen to your players, but it's also good to remember that certain channels like the forum or app reviews might skew towards vocal minorities.
Players have a tendency to propose solutions that they think will solve an underlying problem, but might end up hurting instead.
So try to find the problem instead of taking a solution at face value, and don't feel pressured to implement the solutions your users demand if there might be a better one.
Users will often speak through their behavior as well as words.
So far we've seen that making things harder, even in an effort to teach things better, often leads to hits in engagement metrics like DAUs and retention.
But what about just encouraging users to do a little bit more, right?
If you do more in the app, you're gonna learn more language and it's gonna be awesome.
So in an effort to let our users pick their goals, we offer different tiers based on predicted time per day we want you to spend on Duolingo.
More casual learners could commit to just doing Duolingo for five minutes a day, whereas more serious learners could strive for 20 minutes a day.
However, what we found was interesting, using the casual goal as a baseline, is D14 retention for all other goals were just way worse.
Learners who chose the most aggressive goal had 23% lower retention than learners who chose the most casual one.
Now, of course, a lot of factors could account for this.
It's likely that some of it has to do with our platform being better for casual learners than for more advanced learners at the time.
But other experiments have also shown us that when learners try to do too much at once, they quickly burn out and leave.
So teaching harder stuff doesn't seem successful.
And asking them to do a lot also seems to do poorly sometimes.
So what things actually help?
This is one of my favorite experiments.
This happened the first year I was at Duolingo, hence the retro art style here.
On Duolingo, we present little bite-sized chunks of knowledge in the form of what we call lessons.
And several lessons make up one skill.
So here you see a skill with four lessons.
If you finish all the lessons in the skill, you got this trophy icon to light up at the end.
So in one of our courses, we accidentally added another lesson as part of an A-B test.
Nothing else changed on this page.
There was just an extra box there.
However, oddly, this increased click rate on the very first lesson by 3%.
Extremely significant result.
And just to emphasize how weird this was, that means when a user got to this page before they clicked on anything, 3% more users clicked on that first lesson than in the control condition, just because we don't even know.
This could have been because the trophy is now closer to it.
Maybe they misread it and thought they'd get the trophy if they did less than one.
We still aren't sure.
But what this did tell us is that the weirdest things help sometimes.
And we learned early on that user behavior is just really hard to predict without observation.
And that's why user testing and A-B testing are so vital to what we do day to day.
All right, one more.
So in the spirit of tests that didn't make sense to us, I want to share one more experiment.
For the first time, we decided to allow users to opt in to a change we were making to the inner case that we call crown levels, which I'll talk about a little later.
We have this cute little modal that pops up and asks you, hey, do you want to update now and try out this new experiment?
Because of our A-B testing culture, which our users quickly picked up on, users in the past had complained about not being offered a choice whenever we changed major things.
So we tried offering them a choice, of course, as an A-B test.
The results were pretty good.
Though we had no gains in DA user retention, sessions were up by 14% for users who had a choice between updating to the new experience and not.
However, When we looked at the branch of the experiment that didn't give users a choice and just forced them into the condition, the metrics were actually way better.
We believe that it might have been a combination of the new experience actually just being much better, but also suspect that sometimes choice might cause regret or delay acceptance.
We had spent so much effort on this opt-in experience, but it turned out to actually not be helpful at all.
You could see that the D1 increase is huge there in that condition.
So what all of these experiments tell us is that what's intuitive isn't necessarily right all the time, and metrics really help us keep accountable by telling us whether user behavior actually agrees when we make intuitive changes.
When it comes to making learning things in your game better, you have to really question if what you think helps actually helps.
So to do this, you need to measure the right things as well.
You'll gain a lot of insight by measuring everything you can within reason.
For example, we're able to identify lessons that are particularly hard on Duolingo and try to fix them because we measure things like time taken, quit rates, exercise failure rates, everything we can possibly measure about how users are doing their lessons.
We're able to fix things on a very precise level and target what we need to improve.
For onboarding or tutorials, it'll be critical to see the funnel of how many players drop off along each step.
Maybe there's a step in there that everyone is getting hung up on that you could make better.
Also, regularly question which metrics should be used to make decisions.
Sometimes it's really easy to cheat the metrics so they go in your favor.
For example, if we wanted to maximize how many lessons get done on Duolingo, all we have to do is make the lessons way shorter, right?
The number would go up drastically.
When investigating the success of onboarding or a tutorial in your game, don't just look at how many players got through it.
You also have to make sure that they actually gained the information you were trying to deliver in that experience.
If you want to make sure your learning experience is both effective and engaging, you have to be able to measure both things.
So the bottom line is measure results, not just activity.
It's great if you run a test that increases onboarding completion, and maybe you removed a step to do it, but it's likely that users actually might understand less because they no longer have that step that you removed.
At Duolingo, we try to hold ourselves accountable by measuring learning and not just completion.
Recently, we created exercises aligned to the CEFR language standard, which is a standard language level framework.
The questions that we created are independent of our own content curriculum.
And we designed an experience called the learning quiz, which looks like a normal experience in Duolingo, but is actually there to measure how well our users are learning.
It helped to tell us not just if we were successful in teaching the things we wanted to teach, but even if we were teaching the right things at all.
We can detect if we're really bad at teaching certain grammar or have holes in vocabulary we're teaching because of this standard.
For games, you'll probably need to create your own internal measurements of whether or not your users are picking up the right things.
Try to bake in learning indicators to see if they learned what they need to.
All right, so metrics are important.
Let's assume you have a good analytics framework to detect where improvements can be made and you can keep yourself accountable for it.
The first step of making hard things easy to learn is to make the content itself more learnable.
We'll go over five principles that we apply at Duolingo based on insights from our learning experts.
So the first one is to quickly deliver satisfaction.
And what I mean by this is to teach something immediately satisfying first.
It's important for us that within a user's first few minutes on Duolingo, they'll learn a full functional sentence in the language they want to learn.
We don't start by asking users to memorize vocab flashcards or read grammar notes with the promise of eventually being able to use the language.
we make sure they learn the language immediately.
We want to give them the satisfaction to say, I can do this now, and it didn't even take that much effort.
So find a small but complete and satisfying piece of your gameplay, and make players gain that experience as early as possible.
You probably already do this to an extent.
I mean, I don't know any non-tabletop game that requires you to learn all of the rules and mechanics right away before you can do anything.
But it's likely that there are some things that you can still put off teaching until a little later just to deliver a fun experience a little faster.
You only get a small window to prove to your users that their time spent will be fruitful and satisfying and fun, and if they can't be convinced of this in the first few minutes, they might leave and never come back.
Number two on how to produce learnable content is to remember that though instruction is useful at times, it does not replace practice.
If you want really efficient learning, often it's tempting to give very detailed instructions, especially when it comes to grammar.
So here's an example from our Klingon course, and yes, we have Klingon on Duolingo.
Here's a paragraph describing the grammar of verb prefixes in the third person.
It says if the subject is third person, he, she, it, they, and has either no object or a third person object, et cetera, it describes in great detail when to use a prefix on verbs.
And this is actually incredibly useful and many learners love this.
But there are also a lot of users who won't bother to read all of this text and wouldn't understand it even if they did manage to muster up the motivation to read it.
What we lean on instead is providing a lot of practice for a topic at hand as a learner does exercises in our app with the goal of leading a learner to form their own generalizations.
So I'm about to try to pronounce Klingon, so I hope there are no native Klingon speakers in the audience, because I'm going to butcher it.
So for example, we might present the sentence, Yasha, Mara, I'm supposed to say Klingon very aggressively is what I'm told so that's what I'll do.
It means does Mara understand. But Mara, Raj, yeah, means Mara, do you understand?
Here you can start to suspect what adding bl to the verb does here. Here are two more examples.
Yasha, Tor, does Tor understand?
And then torg, blah, yah.
Torg, do you understand?
So now just based on these four examples, you can probably infer that the BL is needed on a verb when talking directly to the person, but not when talking about them.
By having tons of practice, learners not only have a less intimidating experience, but also start to learn things implicitly and create their own rules for how things work.
The brain is an amazing thing, and it can learn complex things really intuitively.
In fact, it's actually better for long-term retention of knowledge to learn implicitly rather than explicitly.
The best thing to do is provide enough exposure to encourage this type of learning.
So in Breath of the Wild, I thought the game did an excellent job at giving really lightweight instruction and tutorials for battling.
They presented new mechanics and fun ways, and shrines you needed to complete to progress.
One such mechanic was how to parry with your shield.
The instructions say press A to deflect an attack with your shield right before it lands for a perfect guard.
All right, cool.
But the game didn't try to describe to you in excruciating detail precisely how to time it or when to use it or what kind of attacks you could parry.
You just need to practice to get good at it.
And I don't remember how many times I got set on fire trying to practice how to deflect a guardian laser, but that's something I just really needed to get a lot of practice on, even with perfect instructions.
I learned implicitly about which animations lined up with the proper timing and what things could be parried.
And that was not only more useful, it was a lot more fun.
So third piece of advice on creating learnable content is to cleverly disguise difficult or boring concepts.
It's a well-known trick to try to pair less exciting things with fun things if you want the less exciting things to get done.
So in the early days of Duolingo, on all of our units, we had, well, all of our units, which we call skills, were named things like present tense, determiners, adverbs three, prepositions.
We used to have what we call a grammar wall on Duolingo, a group of skills that explicitly tried to teach you difficult grammar.
And we found that many users dropped off at this point once they hit this grammar wall.
Now, in newer versions of our curriculum, we sprinkle that grammar into skills like recreation, restaurant, travel.
We use a theme like recreation to teach you verbs like run, swim, and play.
And it's a clever way that we can teach you how to use the present tense when you're just learning about how to describe your hobbies and what you like to do.
We can disguise a grammatical concept like how to ask questions in a skill about how to ask for directions.
The learner thinks that they're learning about travel skills and having fun doing it, but what they're really learning is how questions are asked.
In the context of games, you can make particularly challenging things into competitions or events.
You can try to bake more mundane but useful things, like how to set certain settings, into the game progression itself.
The opportunities are boundless, but your users are going to have a lot more fun if they feel like they're working towards something that they want to.
Advice number four when it comes to making content learnable is to offer a lot of exposure of that content in different contexts.
On Duolingo, we have a lot of exercise types to add variety to the experience and also present many ways to learn something.
So at one point, you might be asked to select a picture of a fish, then recall how to say fish, and then translate, I have a pet fish, all in the same lesson. You know, there are many different ways that you can practice the word fish, and by the end of it, we want learners to be very comfortable with it. Going back to the shield pair example from Breath of the Wild, I learned what was the same and what was different between pairing a Lynel, a Moblin, or a Guardian.
Change up the environment.
and allow users to apply what they know to new situations.
You see examples of this in games everywhere, like having different stages and items in Smash Ultimate, or using the same items across many different champions on League of Legends.
This will allow users to create correct generalizations about what they've learned and how it applies in new situations.
It's also really exhilarating to discover you already know how to do something in a new situation.
and exposure in different contexts is key in solidifying cognitive learning.
And finally, making things memorable also makes them more learnable.
This seems super obvious, but it's easy to forget to be intentional about it.
So we actually have a Twitter account dedicated to really weird sentences found on Duolingo, and some examples include, there are 1,500 cat photos in my cell phone.
Where can we hide the bodies?
And you do not exist.
These are real screenshots from real users.
And actually, some users criticize that these are not pragmatic and useful.
You wouldn't normally say these things when traveling, et cetera.
But they actually serve a real purpose.
For one, going against expectation makes things more salient or noticeable.
You have to think about what the sentence really means.
Maybe second guess yourself, did they actually say that?
Instead of making assumptions, in your games, find times to make things you want the user to learn surprising, funny, or emotional in some way.
But just a quick word of warning, for users, doing something wrong is also memorable.
We all know the saying, learn from your mistakes, because mistakes are really easy to remember and learn from.
But make sure it's not a negative experience if you plan to rely on this for learning.
If you plan to lead the user to do something incorrect, to teach them something, provide an environment where the user isn't afraid to make mistakes, or it could be really frustrating.
All right, so now that we have learnable content, there needs to be a good framework to present that content.
So the framework we'll talk about today is difficulty scaffolding.
On Duolingo, it used to be that once you finished a unit of learning called a skill, it was just done.
It would turn gold, and then over time, it would fade away to remind you to practice it.
But once you were done, you had finished that skill.
About a year ago, we changed the experience to something that we called crown levels.
It made it so that there were now five levels on every skill instead of just one.
For each level you gained, you earned a shiny crown.
And we gradually increased the difficulty of exercises you got as you leveled up a skill.
You could then choose whether you wanted to level up a skill you had already learned or move on to learn new content.
This feature was critical to us for improving learning.
It provided a clever way to gate harder material for those who were serious enough to level up.
And we could finally inject harder and more effective learning material without killing engagement by just putting it at higher skill levels.
This allowed users to self-select.
It satisfied both casual learners who wanted cursory knowledge of the language to very serious learners who wanted to master everything and practice as much as they could.
So, Crown Levels is probably our biggest experience redesign ever and one of our most successful.
For new users, we saw an increase 5% in DA use, 7% in sessions completed.
We also got a 2% increase in D1 retention, 4% in D7, and 6% in D14 retention.
Now, if you wonder what I mean by new users, we almost always run both new user and existing user experiments for every significant change.
New user experiments are for users created after a certain point in time.
We decide what experience they'll get before they even start Duolingo.
So they start with that experience and have a consistent one.
This removes novelty effect of certain features.
On the other hand, existing user experiments or old user experiments test the effect of giving the new experience to users who have already seen the old experience and changed to the new one.
It would include novelty effect.
We saw huge gains in the old user experience as well.
Sessions increased by 23%, likely due to novelty effect.
But D1 and D7 both also increased by three and 4% respectively.
We don't have D14 retention numbers because we launched this experience to users before we could get them, since we didn't want to wait any longer.
The metrics were just too good.
For both new and old users, the experiment was a huge win for both engagement and for learning.
Learners were gradually exposed to harder exercise types as their account age increased, and we were able to expose many more learners to harder, effective exercises overall, and only when they were ready for them.
So the lesson here and our advice to games is to find ways to scaffold difficulty.
It's certainly helped our numbers tremendously.
Provide paths for both breadth and depth so that users can self-select into the experience they want naturally.
Let players who love a feature go deep into it.
And let casual players stay more surface level until they're ready.
You'll likely be able to convert casual players to higher difficulty over time if you can keep them for longer.
And what's more is by having a framework like this, it helps with the scalability of your game design.
If you can create a framework that opens up a lot of possibilities in the future, it lowers the barrier of entry for new ideas and lets you iterate quickly at lower risk.
We were finally able to introduce more exercise types with high learning value using this new crown system that we've always wanted to, but never introduced in fear of scaring away newer users.
Now we just introduce them at later crown levels.
Likewise, if you have certain opportunities, open up at later player levels or skills.
You can make those experiences more intense without fear of scaring away the newbies.
How to present learning content all boils down to balancing cognitive load, the amount of brain effort needed at any given time.
Too much of it is exhausting and too little of it is boring.
Our crowns change let users self-select into how intense they wanted their learning experience to be.
So now that we have a framework, we should also try to make sure we don't overwhelm our users within that framework.
One key thing is to resist the urge to get to the good stuff right away.
Make sure that users are prepared for it first.
The boss battle might be the most exciting thing in your game and you can't wait for users to get to it, but you'll set the player up for failure if you drive them to that experience too soon.
Also, take into account the learning curve of using your game, not just what you're trying to teach.
We wanted to start by teaching really important vocab and grammar right away at Duolingo.
But what led to much better engagement was first teaching trivially easy vocabulary.
For example, at the beginning of the experience, we used a lot of names or other words that are similar between the native language and the language being learned, even if it's not the most immediately useful.
This allowed learners to focus first on learning how our app works in the first place, the types of exercises we use, using hints throughout, the lesson and the skill system, etc., before also trying to learn a new language on top of that. Learning it all at once would be too much.
A lot of games do this well by having an onboarding experience that targets learning how things work, like the menu, without overloading the user with lore or new mechanics off the bat.
But in an attempt to not underwhelm the user, you may run the risk of, I mean overwhelm the user, you might run the risk of underwhelming them with things that are too easy or don't take into account prior skill.
Remember to keep the experience interesting for those advanced users too.
You can get around this by trying to manage difficulty by user cohort.
So for example, we track the failure rate of every exercise on Duolingo.
Every time someone has to translate I have a dog to Spanish, we see how many learners get it right, how many get it wrong. And we have a threshold where we throw out exercises that are too hard.
But we separate these based on crowd level. So users at the first level, more beginner users, all get the same threshold as other beginners, whereas more advanced users probably can handle more difficult sentences and get them wrong less often.
In games, leagues and leaderboards are a prime example of this, where you're constantly in the same grouping as players with similar activity levels.
You either get promoted or demoted regularly to stay calibrated, and ELO systems are the same way.
Also in games like StarCraft II or in Civ VI, you might be asked at the beginning to self-report your prior experience so that the onboarding can be accelerated if you're already experienced in a genre.
Provide natural ways for your players to speed up their progression when needed.
All right, the last step after having great learnable content and a framework to make going through that framework a good experience.
Sorry, that sentence didn't grammar.
The last step in creating a good experience is to design a motivating UX for that experience.
And actually a surprisingly large part of making an experience delightful is to just avoid things that are not delightful.
Which brings me to this, something that you hear over and over again but cannot be understated, don't make users read.
Because users don't read.
So here's an example where Duo the Owl helpfully pops up on the website, excited for your first lesson, and tells learners they can hover over words to get a hint whenever they don't know what a word means.
We even highlight in orange new words to try to encourage people to hover over them and use these hints we provide.
And this is actually a key part of how Duolingo works.
We don't give you upfront instruction on how language works.
We just kind of throw exercises at you and convince you that you can do it with just a little bit of help.
So we found that fewer than 40% of our monthly active users use hints at all.
And we have seen many user testing videos where learners are really confused about how they're supposed to know the meaning of words without being taught them.
Many say that they had no idea they could hover on or tap words.
This single sentence was not being read by many people, or if it was, it was being quickly forgotten.
Here's another example where in big red letters we tell learners why they got something wrong.
Here, the learner entered in women instead of woman, and we point out that the singular form woman should have been used here instead.
However, when looking at the forms around this exercise, we'll still often see learners confused about why their answer wasn't correct.
This user says, I wrote women instead of woman and feel it should be counted correct.
It's likely that when users see longish text, they continue on without reading it, or skim it and forget it.
And you'll note here that these are just single sentences that we found our users were just not reading.
So what I found helpful in the past is performing the utterly screwed test.
Ask yourself, is there a way for the player to recover if they were suddenly plopped into your game at any point?
Will there be anything they missed out on at the beginning that would prevent them from moving on?
If your game involves reading a one-time wall of text at the beginning, it'll be very hard for a user to keep going if they come back after a hiatus.
But if your game is intuitive to play without much explanation, it's much easier to get back into.
For Duolingo, resurrected users, or users who come back after a long break, make up a large chunk of our daily active users.
So making sure that they can get back into the experience without a ton of additional onboarding is really important for our metrics.
Besides players not reading, this is also a huge reason to avoid text.
At Duolingo, we see lots of text as a possible indicator of lazy design.
Text is often a crutch for solving difficult instructional problems that often have a harder, but more user-friendly design alternative.
The more intuitive your design, the simpler your design, the less text you'll need.
This is largely why we didn't have explicit grammar instruction in the app for so long, why we keep all of our exercises really short, and why we use lots of colors, icons, animations, and game mechanics to convey instruction instead.
Something we've also found works well is to offer help as close to when it's needed as possible.
For example, we provide in-lesson hints instead of asking learners to memorize all of the words ahead of time.
Because even if you manage to have a player read the text, they're still not likely to remember it.
So offering help at the moment that it's needed is really useful for your players.
Second component of creating a good learning experience is to prevent the player from self-destructing on your game.
If you have a game with a really steep learning curve, it's easy for learners to burn out or get frustrated.
So pacing mechanics helps with this.
It helps make sure that you aren't burning through or introducing your content too quickly.
On Duolingo, we introduced a controversial feature called the health mechanic in our iOS app, where health segments refill after a period of time.
If a user makes a mistake, they lose a health segment.
They can re-earn this health by practicing material that they've already learned.
And this means users naturally have to slow down if they're moving too quickly.
However, this also led to increases in D1 and D7 retention when we ran this, despite a lot of outrage from our users.
And it has the plus side of also slowing down learners to really try to think about and get the right answer on exercises and think harder about the learning aspect.
Cramming in general is really bad for long-term learning and retention if you want players to learn a concept and grasp it well.
Users will quickly burn out, and you'll have high user churn if you fail to pace them correctly.
If you want to build a habit of your users playing your games every day, it's good to encourage them to come back later sometimes.
Timers and pacing also give motivation to come back and push back against binging.
And they give a great reason for notifications that also helps bring users back.
Another form of self-destruction we saw was users getting frustrated with themselves if they got several exercises wrong in a row.
We saw this in user testing.
So this might seem really silly, but we started adding a friendly motivational message reminding learners that they're still learning even when they get things wrong.
So Duo, our helpful little owl, pops up and says, even when you're wrong, you're still learning.
And this led to huge engagement wins, just this simple tiny thing.
So we saw nearly a 2% increase in DAUs.
Retention increased 3% for D1, 6% for D7, 7% for D14 retention, this little bubble message here.
And that's an insane gain for such a simple, simple feature.
If you have a way to detect if your player is getting frustrated, setting up just little ways to intervene and encourage them can do so much for retention.
And finally, though it seems rather obvious, remember to not only discourage destructive behavior, but to also encourage desired behavior.
Achievements or quests are a really classic way to do this, and you see them in games all the time.
When we added achievements on Duolingo, they gave us wins in DAUs, sessions completed, and retention.
But one of their main benefits is also providing clever ways to increase discovery behavior in your game.
For Duolingo, some fewer used features, like looking at your profile tab or adding friends, doubled in engagement over 100 percent increase in clicks with the addition of achievements.
And finally, if we're talking about one of our best performing mechanisms for motivation, it's definitely the streak.
The streak is how many days in a row you've met your goal on Duolingo.
There is nothing that unlocks when your streak is high.
There's very little social proof or ability to show off your streak.
There is no currency attached to your streak.
We don't gate any learning content on your streak.
It's simply a number that people are proud of, and it doesn't actually gain you anything in our app.
But our users care so deeply about it and find it incredibly motivating, so much in fact that they get devastated when they lose a long streak.
In response, we created the idea of a streak repair, the ability to repair your streak if you accidentally missed a day.
And to illustrate how much this simple little flame motivator mattered, we tested offering one free monthly streak repair as part of our subscription perk.
And when we did this, subscription revenue increased almost 60%.
60% subscription revenue, because users really care about this little flame icon that got them nothing in the app.
So never underestimate the power of a simple UI element, whether it's a motivational message, an awesome animation, a color change, or a tiny flame in the app that you wanna keep burning for hundreds of days.
You might be surprised how motivating these things really can be.
So let me take a few minutes to recap lessons today about how to create a good learning experience within your game.
So here are things you should probably do yesterday, but at least do them as soon as possible.
First, identify holes in what you're currently measuring, making sure that you have good indicators for results and not just activity.
Create learning indicators to see if players are actually learning what you need them to.
Invest in a good A-B testing framework.
Like I said, we went from 13% D1 retention to 55%, just from incremental changes in A-B testing everything.
Remember to cut out the noise from players who aren't exposed to the experience you want to test, and you'll get a lot more statistical power that way.
Use A-B tests to see how much new features help and hurt to keep yourself accountable.
Improve your intuitions with data.
And make all early stuff way easier.
I can't recall a single experiment where making things easier hurt engagement within our app.
However, the caveat is make sure that those learning indicators are in place so you ensure you aren't sacrificing too much for the sake of engagement.
And then here are some actions that you can look into in the near future for improving the learning curve in your games.
Remember to identify and deliver the smallest unit of satisfaction as early as possible to your players.
Give them the confidence that they can do this, they can conquer and get good at your game.
Get them interested in the first few minutes of play and provide that baby step needed to start climbing that learning curve.
Disguise boring concepts by replacing or pairing them with interesting ones.
If you can come up with a good way to make volume adjustment fun, do it.
Look at what steps seem to have higher user drop-off and target those first as areas of improvement.
Scaffold difficulty in your game if you want to appeal to a wider range of players.
Provide paths for both breadth and depth so that more beginner gamers can enjoy your game and advanced players can have a blast mastering it.
Perform the utterly screwed test on your game.
Are there parts of it where the player would be totally lost if they just picked it back up at that point?
Try to create ways to recover at those points.
Winning back users who want to give your game a second chance can be critical to improving your daily active user numbers.
Remove text.
It's likely you could probably remove half the text in your game.
And finally, add delight.
Find ways to intervene when the player needs help and to encourage the player when they're crushing it, even if it's by really simple UI elements.
So if you want to talk about specific ideas for your game, have questions about Duolingo or learning, or just want to say hi, feel free to reach me at this email.
I'm also happy to take a few questions now and after the talk in the wrap-up area.
So thank you all so much for coming out today.
Safe travels home, and I'm looking forward to continuing to be inspired by all the games you create in the years to come.
Thank you.
Oh boy, hello.
Hi Karen.
Hi.
Thanks, that was a great talk.
Thank you.
Regarding streaks.
Yes.
Did you guys test anything, like rewards or anything related to streaks?
Yeah, absolutely. So we are looking into a few things. One thing that happens is we have an in-game currency that we call Lingits right now.
And as you get streaks of multiple 10, you actually get a bunch of Lingits with it. So that has helped.
One thing that we're also testing is how many days to sort of show in regards to your streak.
Like if we could get users to just a three-day streak, maybe they're more likely to do a seven-day streak, things like that.
So we're always playing around with streak because it's such an important motivator.
Thank you.
Thank you.
Hi.
Hi.
How do you process feedback from your users?
For example, that one was talking about one word that was wrong.
How do you process that inside?
How is the team working on this feedback to upgrade Duolingo?
Yeah, that's a great question.
So there are a lot of channels for user feedback.
One thing that I mentioned in the talk, too, is sometimes words aren't always as indicative as actions.
So we try to rely on our metrics as much as possible.
Like, are users getting frustrated and quitting right after they get this thing wrong, et cetera?
But in terms of qualitative feedback, and not just quantitative, we do have a customer support that processes reports and error reports from our users.
And a lot of us that work in product also constantly check our user forums and our app rating reviews to see if there's any good qualitative feedback we can gain.
It is a lot of work to process all of that qualitative feedback, but it's also really important.
Thank you.
Thank you.
Hello, and thank you for the talk.
I want to ask something, but first, because I'm not so familiar with your app, what sort of monetization scheme do you have?
Yeah, that's a great question.
So we make revenue from ads, and we also do a subscription model.
So for a while we did in-app purchases, ads, as well as a subscription model.
One of the key tenets of Duolingo's mission is we want all learning content to be free for anyone, because not everyone has the means to pay for education.
So our subscription model allows users to do things like download lessons offline, get a free streak repair, and things like that.
And we have found that subscriptions are probably going to be our main source of revenue.
I see. So it seems that you're very centric about looking at the metrics to see what works and what doesn't.
Now, you also mentioned at some point that sometimes you can skew the data or look at it in a weird way.
So you artificially create the results you want.
What sort of filters or checks do you have in place to confirm whether a certain metric that seems to be pointing in the right direction really does so?
Yeah, that's a fantastic question. And honestly, it's sort of hard to have something in place because the people who are prone to those tendencies will need to also be the ones that put checks and balances in place.
So, for example, on the teams I'm on with learning, it's our goal to not decrease things like retention and D1, D1 retention and DAUs.
And we have this other metric that we call the learning metric, based on those sort of quiz scores on how users are doing and how they're learning, that tries to keep us accountable, right?
If we increase engagement a ton by just making things way easier, but people are learning less, we see that other metric go down.
So what I would suggest is just going through the thought process and exercise of seeing what things you can cheat on and then making metrics on top of those things to go the other way to keep yourself in checks and balances and sometimes you just need really honest co-workers to help you with that too.
Thank you.
Hi. So we make kids a learning product for much younger age range.
And one of the things we always run into is this balance between learning and fun.
Oh, yeah.
Yeah. So I'm interested in how you guys define like either boundaries or, you know, guidelines to make those kind of trade-offs.
Yeah, I mean, this is the problem that we've had ever since the beginning of Duolingo, and it's definitely not one that we've actually solved. We're constantly improving it.
But what has really helped is, between learning engagement, like I said, if you can measure both, you can see what changes actually improve both.
And then you can try to avoid situations where you have to choose between one or the other.
So for example, the crown levels experiment I presented was one where we got really great wins in both.
Whenever we revamp our curriculum, we also see gains in both.
So we're sort of procrastinating on how we can do that trade-off by just trying to first target features that would actually increase both of these things at the same time.
Hello.
Hi.
Thank you so much for the talk. I wrote my question down. So, A-B experiments that feature large changes, such as revamping the entire fundamental curriculum, take time to deliver quantifiable results. This makes measuring the results in a timely manner much more difficult.
In this kind of situation, how do you know if these types of changes are the correct changes?
And as a side question, when is it right to disregard all metrics and go with your gut feeling?
Yeah, oh gosh, that's so hard. So, Duolingo actually has another app called Tiny Cards that I worked on for a while. And this is an example of an app that didn't have that many users, so we couldn't rely on just sheer numbers to deliver really quick metrics results.
And I'm sure a lot of games are in this situation.
So sometimes what you have to do is sort of rely on the best practices and measurements and the tests that other larger companies have already run in order to just make a best guess for what might work for your game.
I will say that usually it's very rare that we disregard all metrics and go with gut.
But we do actually, a mission and core values really helps you keep true to at least things that you don't want to compromise on.
So for example, if we started charging for learning content, I bet we could make a ton more money.
But that is something that we just encode in our values that we're never going to try.
So, it's hard, right, but I think if you have really fundamental core values in place and build teams around it, if you're willing to take risks sometimes for larger features and just see how they do, you'll still be able to move quickly even if you don't have huge user numbers to get you results within a few days.
Thank you.
And just a note, I think we're going to be kicked out in the next 30 seconds or so, so I'll take one more question and then I'm happy to just head on over to the wrap-up room right after this.
Go for it.
So when you're running multiple A-B tests, are there any measures or strategies you use to make sure one test is not biasing the results of the other test?
Yeah, so the beauty of A-B tests is because it's a completely random sample, It's supposed to be the case that whenever you have a test that splits up users into two buckets, any test also takes a random sampling of those two buckets.
So you can do sort of cohort analysis and see, hey, users in these two experiments, those two really don't interact very well.
or you can see bonuses there.
So we have sort of things in place to try to prevent us from doing that.
But the also key is all of the experiments should be independent of each other.
So if you have experiment A running, a user's just as likely to be in experiment B or not in experiment B if they're in experiment A.
Got it.
Thanks.
All right.
I'm very happy to go over to the wrap-up after.
Thank you so much, everyone.
