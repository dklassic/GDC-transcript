Okay, so these are two Assassin's Creed games, Syndicate and Origins, the most recent ones.
So Syndicate came out in 2015, and Origins came out in 2017.
And if we look at the size of the maps in this game, we'll see that it's grown incredibly.
So it's grown about 250 times larger in the span of a couple of years.
So this is like the kind of, increase in scope which players expect from Ubisoft games.
But this talks about animation, so does anyone want to hazard a guess how many animations were in Assassin's Creed Origins?
So, there was roughly 15,000 animations in this game.
So, it had like a 3 year development cycle.
So that means on average there was about 20 animations added a day.
So that's 20 animations that need to be recorded in the motion capture studio, cleaned up by motion capture technicians.
They need to be processed by animators, edited, tweaked.
And then they need to be put in game by a designer.
And they need to be tested and QC'd by testers.
So that's an incredible, incredible amount of work.
And the question is, what are people going to expect in a few years time from Ubisoft games in regards to animations?
Are they going to expect 100,000 animations or a million animations?
What is the sort of scale we're talking about?
So I work for La Forge, which is Ubisoft's main R&D department in Montreal.
And one of the initiatives I'm working on is basically trying to tackle this question.
So how can we prepare for this scaling up, which is bound to happen?
And I've been working with a bunch of people there, in particular Simon Clavet, who did some previous work on motion matching.
And so today, what I really want to show to you is our philosophy and how we've been thinking about how we might be able to scale these systems up.
Okay, so let's have a look at the background.
The most high-level overview of an animation system looks something like this.
We have some player input.
This goes into this black box animation system, and as output we get a pose.
We don't actually send the button presses directly.
What we do is we convert them into some kind of high-level representation of what we want, like where we want the player to go, which direction we want them to be facing, these sorts of things.
So this we're going to call gameplay.
And one nice thing is that we can also use inputs from NPCs or other sorts of controllers.
So what we actually do with this animation system, typically, or at least the way it's been done in many of the large Ubisoft games, is that basically here we have a state machine.
And we have a bunch of different nodes in the state machine, and each of them represent a state the character can be in, and that roughly means what animation the player is playing.
So here we might have a state, which means the character is in locomotion.
And we can click on this state, and what happens is it pops open, and inside are many, many more states.
And we can click on another one of these states, so maybe we click on a walking state.
And inside, there's a whole bunch more states.
So it's like hierarchical, this state machine.
Inside are more states and more transitions and more logic for how to move between states.
So maybe we click on another one, like a.
Maybe this is like the turn right mode of the walking state of locomotion.
And it pops up another state machine.
And we can click again.
So maybe here we have the turning right to idle state of the walking state machine of locomotion, et cetera.
And we can click on this one.
And what it pops open is something different.
So inside here we have a blend tree.
And a blend tree is basically saying how can we blend a bunch of different animations and what conditions are we using to blend them to produce the final pose we give as output.
So we can click on one of these nodes in the blend tree as well.
Oh, and it pops up another blend tree.
So this thing is like hierarchical as well.
And we can go down again.
And now maybe finally we get to the data.
Okay, so now we actually have inside this final node all the way down, we have a clip which we've recorded in the motion capture studio.
maybe been touched up by animators, and the file name looks something crazy like this.
So it's some sort of crazy string describing exactly what's happening in this clip.
And so our data flow looks like this.
We get our input from gameplay, we're going all the way through this crazy state machine, through the blend tree, all the way to get to the data here, and then we're outputting the pose of the character based on this.
And you may think I'm exaggerating for comic effect, but actually it's much, much worse than you could ever imagine.
So like I said, there's 15,000 animations.
This is for the latest Assassin's Creed.
I asked them to run some stats for me.
About 15,000 animations, about 5,000 states in this state machine.
And it's about 12 levels deep.
So that's the kind of magnitude we're talking about.
And OK, so you may think, OK, so that's how it is.
But it shipped many, many successful games and made lots and lots of money.
So what's the problem?
So the problem is this, right?
we have a director.
And the director comes to you one day, and you're maintaining the state machine.
And he says, we want the player character to be able to be injured.
So we want him to be walking around injured.
And so you're maintaining the state machine, and you think, well, what could I do to add this to the state machine?
OK, so one thing you could do is you could add an injured node at every single leaf state and just have a kind of transition which switches between being injured and not.
But some states don't make sense when injured.
So maybe some whole parts of this hierarchical state machine don't really make sense when you're injured.
So you need to do something like go through this whole state machine and work out when it makes sense to be injured and when it doesn't.
Another option is we could kind of duplicate this whole graph and replace all the data in it with injured versions of the same data.
But we sort of have the same issue, which is that Some states or some leafs we might not have recorded any injured data or injured data might not make sense.
So we might have to kind of jump back to the original tree at random points.
And finally, we could just try and hack something together for this case.
But of course, if you do this and you have too many hacks, then the technical debt builds up and you start to get in a lot of trouble.
But anyway, your boss wants you to do this, so you have to do something.
So you do something.
and then sometime later the director comes back to you again and he says great so the next scene the character is injured he's also tired and he gets stabbed in the eye halfway through so we need locomotion for all of these different things so then your action is something like this so the dream the dream would be a setup more like this okay so day one the director comes to you we want the player character to be injured Day two, we go to the motion capture studio.
We capture a bunch of injured motion.
We'd limp around or whatever.
Day three, we grab all these files from the motion capture studio.
We drag and drop them into our system.
And day four, everything works.
Day five, director says, okay, now we want the character to be injured, tired, and he gets stabbed in the eye.
So we do this at the motion capture studio.
We go and stab ourselves in the eye.
drag and drop, everything works, great.
Okay, so what this really is talking about is like scalability.
So how can we have a process for building animation systems which is scalable and it doesn't give us a headache every time we wanna add something new?
So that's what this talk is all about.
And these are the three kind of ideas in this talk.
And really these are ideas that come from machine learning in some way.
So what we ideally want is a generalized solution, which can work for many different cases.
And to get this, what we need to do is specify exactly what variables we want in the system.
And to get this working well, we need to think more carefully about how we manage our data.
So these are kind of the three stages I'm going to talk about.
So first, we'll talk about data separation.
So if we have a look again at our data.
set up which we had before, there's kind of an awkward thing which is that conceptually all the data is living inside this state machine.
So the first kind of conceptual step is to have a separate database and to pull your data out and have it live in the database and have your state machine, rather than outputting a pose, actually output a kind of a pointer of some kind or a file name with a time.
So of course you're thinking, yeah, obviously this is how we actually do it in practice.
Like we don't actually have the data living inside the state machine.
But there's like an important conceptual difference here.
And if we want to blend, well, we can have the blending kind of happen after this data retrieval stage.
We ask for multiple different animations, and we get the different weights as well.
And the kind of important conceptual reason why we want to separate out this database is that the first thing we can do, which is really nice, is get rid of this craziness with the file name.
So we have our file names in our database, and we have our file name or some sort of pointer coming from the state machine, and the first kind of step is to basically replace.
all of these things which were in this description, the file name, with tags.
So now what the state machine outputs is basically a list of tags for what it desires the motion to be like.
And we can have similar tags in our data.
So for example, this is like a little prototype tool we have for tagging our data.
So here we have a really long take.
This is about 15 minutes of raw animation data.
And what we've done is we've gone and tagged all the different sections for what they represent.
And in a sense this is the same as editing or cutting up this clip into small sections.
And your tags can be as detailed or as simple as you like.
So, for example, if you have a cut scene, you could have a unique tag just for this cut scene.
And your state machine could output this tag.
Or you can have very general tags, like just walking or locomotion or turning, these sorts of things.
So you have a lot of flexibility when you use these tags instead of the file names.
And you can also have multiple ranges inside these files, so that's pretty nice.
So already, there's a lot of nice things.
So one thing is that your state machine development doesn't depend on your database.
So you can update your state machine and update your database separately.
You can also swap out your database for new characters.
So you can keep your state machine logic exactly the same, and just swap out the database if you have a different style of character.
Or you can have some sort of fallback database.
So if the state machine requests a set of tags which are not in your database, you can fall back to some more general database.
And the overall kind of idea is that let's move away from thinking about assets and start thinking about databases as a whole.
And move away from kind of file names and start thinking about tags.
and have a separate process for motion retrieval.
So what we've basically done is kind of got this classic game development set up with file names and assets and tags and editing, these sorts of things.
And we've moved and put it into a database, which is really how machine learning people think about data.
So that's the first stage done.
So now I'm going to talk about specifying the desired variables.
OK.
let's have a look at our setup again.
So, okay, we've improved some things, but we still have this really huge state machine here.
And this state machine is kind of a complex thing because it's a mix of gameplay and animation.
So some states are purely aesthetic, so maybe like a turn 25 degrees or something like this.
It doesn't really have any meaning in gameplay.
It's just there so that we can play a slightly different animation.
And we have some states which are important for game play.
So maybe if the character is falling over, or if the character is doing a roll, that's actually a different thing in game play.
And you can perform different actions depending on these states.
So one thing which would be really nice is if we could separate out this big state machine into purely game play related states and purely aesthetic ones.
So one thing we can do is this.
We get our state machine, and we get just the kind of gameplay, simplified gameplay version.
So only states which have some meaningful thing to do with gameplay that we keep, and all the aesthetic states we remove.
And we pull in all the other data related to the aesthetics from outside.
So for example, the fact that the character is male, this is kind of like a global variable or something we can get in from outside.
Or things from gameplay like where we want the character to be going, what speed they want to be going at, where they want to be looking.
Lots of these things have nothing to do with gameplay.
They're not managed, they're not, they don't change the gameplay at all, they're purely aesthetic.
So we can pull these right through and kind of bypass this state machine.
And we can tag exactly the same variables and exactly the same properties in our data.
So these are actually numerical values.
So that's kind of the main difference now.
They're not just tags.
Some of these are like numerical values, which we also have labeled in our database.
So we need to kind of change a little bit how we look up which clip to play next.
And the basic idea you can use is just filter out the clips where the tags don't match and return the nearest numerical match for the rest of the numerical inputs.
OK, so that's how we do this.
And we're going to call this matching because it's not really like querying a database anymore.
It's more like trying to find the best match for the desired inputs.
And something kind of magical happens now, which is that this state machine sort of disappears into gameplay.
So now maintaining the state machine is really like the role of the gameplay programmers.
And as animation programmers, what we're really doing is the stuff on the left-hand side of this.
So now...
if we think that the game play is the state machines purely a game play construct now animation system looks more like this all we have is this matching component where we try and get the user input and we match it to something in the data there's one kind of extra step which is that we don't want gameplay to specify timing animation so we want them to say what kind of sort of animation they want but we don't want to say okay we want them we want an animation which is kind of halfway through playing or something like this And one thing we can do is we can use the previous pose which was output by our system to describe the timing animation.
So maybe the previous pose was the character with the right foot down.
So now we know when we match our next frame we want to output or our next clip we want to play that it should start with the right foot down.
And this setup is basically extremely similar conceptually to what was shipped in For Honor and what was called motion matching at the time.
So there was a gameplay state machine, and there was tags and numerical variables coming in, and there was this kind of matching happening with the database.
So this is a little clip from For Honor where you can see a bit how it's working.
So here we see all the different potential clips in the database which could be played next.
And essentially, the system is picking the one which most closely matches the desired user input.
So here, the desired user input is the red trajectory along the ground.
And it's picking the clip which best matches that.
And one of the really nice things about this setup is that, unlike this big state machine, new variables are really easy to add.
So for example, if we had the injured, if we wanted to add an injured state to the character, we just have an additional input saying whether the character should be injured, and we add this as an additional tag in our database.
So the idea here is that instead of states, we want to describe the animation we want by variables.
Instead of querying this database, we want to have some sort of fuzzy matching where we just try and roughly get the best clip that matches.
And we need to annotate these variables in the data.
So we need to annotate these tags on these numerical variables.
So that kind of gets us to the point of motion matching and something that's been shipped in For Honor and something which has been proven pretty effective.
So what's next, right?
What's the next step?
How far can we push this?
So that's the third point here, which is talking about generalizing the solution.
So I have a minor warning, which is that When you talk about generalization, often the way we generalize is by using math.
So there is a couple of equations.
I hope that's OK.
OK, so let's have a look at our setup again.
The first thing we can do is we can move the pose over to the right-hand side.
It doesn't really matter the fact that it's looping around.
We can actually just consider it as part of the input.
And we can think of this matching process as a mathematical function, which takes a big list of numbers, takes a vector and just produces another vector, so it produces a big list of numbers.
And the way we can think about it is something like this.
Let's say we have an input on the right here.
The way we can represent it as a big list of numbers is something like this.
So for example, we can have this sort of enumeration where we just give a one-hot vector to say whether the character is male or female.
Similarly, for whichever kind of general state they're in, like locomotion, idle, falling, this can also be an enumeration.
Similarly for the style and the numerical values we can just give directly.
So this can be the position the character wants to be in in the future, the speed they want to be going at, where they're looking.
We can also have the pose.
So for example, if we had the pose as output as our Y here, we can represent it by using the position of each of the joints, position and rotation of each of the joints.
So here we have the first joint position, first joint rotation, second joint position, second joint rotation.
and then the final joint. So whatever we give as input and whatever we give as output we can represent them as two huge big lists of numbers and we can see our function, our matching as a mathematical function which maps from one to the other.
When we think about our database in this regard, what we actually have are pairs of X's and Y's, pairs of X's and corresponding Y's.
So for each pose in our database, each Y, we have all the associated variables and tags.
So this is our kind of associated X.
So our database now is really just pairs of X's and corresponding Y's.
And our matching function is just a function that uses this database to map from X's to Y's.
And this is exactly what we call supervised learning in the machine learning community.
So it's using a database of X's and Y's to learn a mapping from X to Y.
And we don't call this matching in the machine learning community, we call it a regression.
So motion matching is a special case of this regression called nearest neighbor regression.
So we can think about all these things in terms of existing machine learning concepts.
So I'll explain how that works now.
So basically in nearest neighbor regression what we're doing is we're taking our x, which we get as input, and we're calculating the distance to all the x's in our database and we're returning the y with the smallest distance to our x.
So for example here we can calculate the distance to all these x's.
And we see that this x1 has the smallest distance, so we return y1.
We return the pose y1.
So when we're doing this matching and finding the nearest numerical match, this is exactly the same as doing nearest neighbor regression.
All right, so all of this has been a little bit conceptual, so let's actually see a real example.
So here's some training data we use, or some of the training data.
So it's really just a kind of really long, 15 minute take of unstructured locomotion.
And we got about 200 megabytes of this training data with various different characters, all different things.
And our input x, what we want to give is we're going to give the previous frame joint positions, so positions of the joints in the previous frame, their velocities, how fast they're moving.
And also our target is going to be where we want the character to be in one second's time, what velocity we want them to be going at, and which direction we want them to be facing.
And our output Y is actually going to be a block of animation.
So we're going to output a one-second block of animation, all the joint positions and all the joint rotations for one second.
And our function F, what we're going to do is we're going to call it every one second.
So every time we need a new block, we're going to call it, get that new block and put it in.
Or if the user input changes, they specify a new desired position or direction, we're going to call it straight away and get a new block straight away.
So let's see what happens if we use nearest neighbor regression.
So what we get is we get a system like this.
So we have the character, and he's following the desired user input.
And the most noticeable thing is that there's a click.
So every time the nearest neighbor regression chooses a different clip to play from, you get this instantaneous jump where it's switching clips.
But what we can do is we can add some blending.
Okay, so now we have a little bit of blending between clips to make sure they kind of blend smoothly between each other.
And we get quite a nice system, so it's pretty responsive.
And most importantly, we get lots of kind of diverse, interesting locomotion.
It doesn't look particularly formulaic.
And we can get lots of different variety as well.
So this result looks pretty much like most motion matching you've seen, because that's basically what it is.
It's essentially doing motion matching, but under a more general framework.
And the memory and runtime are both fairly reasonable.
So we have about 200 megabytes of data, one millisecond.
So it's obviously not fast.
But for the amount of variety and the amount of different motions you see, and for the simplicity of it, it's pretty good.
So there's kind of one interesting thing, if we frame this more generally, as we have, which is that there's not just kind of a few types of regression, there's actually an insane amount.
So this is the contents page from a supervised learning library called scikit-learn.
And we can see nearest neighbor regression is down here, so it's not some obscure edge case, it's actually quite a popular machine learning algorithm.
And we can see there are some other ones, so for example, here's an algorithm called Gaussian processes.
So these were pretty popular for a long time before neural networks.
And basically, they do a smooth interpolation of the data.
So let's see if we could just replace nearest neighbor regression with Gaussian processes and see what happens.
So here we get our database, we train a Gaussian process, and we plug this in as our function f.
So what happens is something like this.
So it basically doesn't work, right?
It looks pretty bizarre, and we're not really sure what's going on.
And Gaussian processes, they're kind of limited in scalability.
So they scale really poorly with the amount of data you train them on.
And I could only use about 1,000 samples for training them, so maybe we just didn't use enough data.
So let's look again at our contents page.
Oh, so there's neural networks.
Great.
the hot thing at the moment.
So we can try these and see what happens.
So one nice thing about these is we've got virtually unlimited data capacity.
And we can throw away the data once it's trained.
And they're fast to evaluate with low memory usage.
So as far as our goal of scalability is concerned, they seem like the ideal thing to use.
OK, so how many of you know how a neural network works?
Okay, a fair amount, that's good.
So I'm gonna give my little five minute rundown of neural networks.
So don't worry, it's just gonna be quick.
So a neural network, like all machine learning algorithms, is just a function.
So for example, this is a simple function you kind of recognize from high school.
It takes some input and produces some output.
And as we've seen, these are represented basically by vectors, big lists of numbers.
And a single layer of a neural network is described by a function that looks like this.
So these variables, w and b, are the weights of the neural network.
So we have the input, x and y, the input and output, x and y, on either side.
And the first operation we do is we multiply this big vector x by this weight matrix w.
And we add another vector which is the bias b which is another set of weights for the network.
And then we pass this through an activation function.
And the activation function is basically a simple function which produces some sort of bend or non-linearity in the output.
So it's actually super, super simple and super familiar.
It looks exactly like this really basic function we can think of from high school.
And when we stack, we have a deep neural network.
We're basically just nesting this equation in upon itself.
So this equation is going to be exactly the equation we use for our f in this machine learning problem.
So it should be kind of immediately obvious how simple and how small and compact using this sort of function is.
So it's really nothing complicated and the whole thing is encoded by these weights, these w's and these b's.
So this is why we can throw away our database once it's trained.
And the way we train it is fairly simple as well.
So what we do is we put all of our x's in our database through this function, and we see what numbers they produce, and we compare them to the y's in our databases.
And then we basically use this to update the network weights.
So there's a bunch of different algorithms for doing this.
And we repeat it thousands and thousands of times.
And eventually we have the weights and biases which work for this particular function.
I think it's kind of, you can see why it's exciting from a scalability standpoint to use neural networks because they're very, very small, very, very compact, and we can understand the size and the computational complexity of how they're working, and it's completely independent of the size of our training data.
So we can do this.
We train a neural network.
and then we throw away our data and it looks something like this.
So it's still basically not really working.
It's doing kind of something, but it's not doing what we want.
So why is this?
The reason is that machine learning, it's not this magic black box.
You can't just train this neural network with no regard for anything.
And the results you're going to get are going to depend so much on how you represent your input, how you represent your output, and how and when you use this function f.
Okay, so it's not as simple as, it's not as nice as just looking at this big contents page for supervised learning and just picking an algorithm and trying it out.
And additionally there's something really bad about the problem we're trying to solve, which is this function f isn't well defined.
So for example, if we have the character standing still and we say, go forward, so we just give him a target in the future to go forward, there's two different choices.
So he could either lead with his left foot or he could lead with his right foot.
And both of these choices will get him to the goal.
But the neural network or whatever machine learning algorithm you're using, it doesn't know which one to choose.
So what it tends to do is it just produces an average of them both.
And this is what you see when you see the character kind of floating along the ground.
It's an average of using your left foot and using your right foot.
So the question is, can we resolve this ambiguity?
Irregardless of tweaking our input and our output representation, it looks like we need to solve this problem first if we're going to have any chance of getting it working.
So the answer is obviously yes, we can.
And one way we can do it is by...
specifying the timing directly. So we can use this concept of the phase to tell us exactly the timing of our left foot and our right foot and how we're cycling through our animation.
So for example, the phase we can say it's a variable where when your left foot first goes down it's zero, when your right foot goes down it's pi, and when your left foot goes down again it's two pi. So it's this kind of cyclic variable which is cycling between zero and two pi.
And then one idea is we could use a separate F depending on what the current phase of the character is.
So assuming the character always has some value for his phase representing where he is in this cycle, we can try and use a different F.
So let's try and do this in the kind of most simple way possible.
What we can do is we can separate all our x's and y's in our database into different bins depending on the phase.
So all of our data where the character's got his right foot down is going to go in a different bin to all the data where a character has his left foot down, for example.
And then at runtime, given the phase, we basically select which bin we want to use.
And we use the function we've trained for this particular bin to generate our y from our x.
So let's see how this works.
So let's say we've binned our data like this.
So we have six different functions along this phase space, which we've binned our data into.
And we get a new phase at runtime.
So now we see which bin this is.
We see which data is there, which function we've trained f.
And we use this to produce our Y from our X.
OK, so that's how we're going to do it.
So let's try this again with a bunch of different machine learning algorithms and see what happens.
So we're going to set it up a little bit differently.
Our input will be exactly the same, the previous pose of the character and where we want to go in one second.
But our output now is just going to be one frame.
So we're not going to output a block of animation.
We're just going to output one frame at a time.
And what we're going to do is we're going to select our function f depending on our phase, call it to get our next pose of the character, and then update our phase value.
So we also measure how much the phase changes at each frame.
So for each frame in the database, we have the change in the phase, and we update this.
So let's try it with nearest neighbor regression and see what happens.
So here we kind of get something that looks pretty similar to our original nearest neighbor regression where we were outputting blocks of animation that was like motion matching.
So that's kind of a good sign that things are roughly working as intended.
And one thing you'll notice is that the cycle of the locomotion is kind of much more strongly preserved in this setup. So in the other one maybe there was more kind of diverse.
movement where the phase could change fast or slow and different stepping patterns. Here we really kind of see the cycle going on and on. And we have the same kind of jumping issue where when it jumps to a new clip it kind of clicks. We could also add blending like before and we get a nice result. So let's see how the Gaussian process fares in this case.
It basically doesn't work again.
So I don't know what this means.
So probably it means that having lots of data is very important, right?
And we can sort of see that the phase is still cycling.
It just looks a bit bizarre.
So we kind of have some feeling that our phase thing is working.
We're not just getting this gliding motion, but the quality of the output is not great.
And there are some other kind of questions we can ask about this approach.
Like, what if the phase is in between two different bins?
And what about when it suddenly switches to a new bin?
There maybe we'll get some sort of discontinuity or some sort of jump.
And it seems like a waste to train multiple functions F.
So in each of these bins we have a different F, but lots of them are gonna be extremely similar because they're only just kind of a little bit in time different from the previous one.
and how can we use a neural network to attempt this same problem.
This is basically where we were at for some previous research I did called face function neural networks and this is exactly what we did now.
The basic idea is that we have a neural network where the weights are generated from the phase directly.
So the weights are generated as a function of the phase and they change continuously and smoothly along with the phase.
So as a diagram it looks something like this.
What we have as input first is our phase variable and the phase variable it loops around in this circle.
And we have kind of four sets of different neural network weights.
and we have our phase function here. So what this does is it's basically interpolating these four sets of different neural network weights depending on what the phase value is.
So for each different phase value we get a slightly different mix of these four different neural network weights.
And these interpolated weights, they get given as the main weights to our normal neural network which maps from X's to Y's.
So here we have our neural network, our equation which I showed before which takes an X and produces a Y.
So if we do this in exactly our same setup as before, we get something that looks like this.
So it's still not perfect.
It's not completely...
extremely responsive, but the quality of the generated motion is actually quite nice.
And it's smooth and it varies continuously.
So if we continue tweaking these x and these y's, and we continue tweaking this f, and we try and incorporate all our best practices for how we train a neural network, then we get something that looks more like this.
So what we have in the end is we have a character which can follow a trajectory nicely, produce kind of nice, natural movement.
And to show how scalable this is, we also trained it on a bunch of data where the character is going over rough terrain.
So here we had about a gigabyte of data where the character was going over different rough terrain.
So it was kind of roughly, I think an hour and a half of data we captured walking over different rough terrain.
So we can see that the scalability is really there.
We can train on absolutely huge amounts of data and it works and it adapts to all these different varied situations.
And we can also give these kind of tags, which I described in the beginning.
So here we tag whether the character should be crouching or not crouching.
And we can give a continuous tag here based on the height of the ceiling.
And the character will kind of naturally and somewhat smoothly adapt.
Or we can give somewhat more discrete tags.
So here we give a tag saying that the character should be jumping at this point in the future.
And that's what the character does.
So it sees that in the database, the only place where this tag was present was in jumping motions.
So this is what it plays.
And because we're also giving as input the height of the terrain under the character, he can somewhat adapt his style of jump based on what's below him.
And the really nice thing is that once we have this neural network working, the performance is pretty good for the amount of data it was trained on.
So in the kind of most compact way, what we can get is just 10 megabytes of neural network weights with a runtime of about 1 millisecond.
So 10 megabytes is pretty incredible if you think that it was trained on literally gigabytes of motion data.
And we've thrown all of this away, and we just keep this 10 megabytes.
Or in this particular phase function neural network approach, we can do some sort of pre-computation, which uses more memory but can evaluate a bit faster.
So it's definitely delivering what it promises in regards to scalability when it comes to data.
So that's how you can kind of generalize this solution, frame it in a more classic machine learning way, and try out a bunch of different experiments to see what works.
So in conclusion, there are a couple of sacrifices we have to make for this approach.
So you have to give up precise control.
So if we think about those 15,000 animations that were made for Assassin's Creed Origins, at some point we're just not going to be able to hand author all these animations.
So we have to think about ways which can scale, and lots of these ways we can scale are going to require giving up precise control in many contexts.
It also requires learning a whole new skill set.
So doing machine learning programming is a completely different beast to normal programming.
And it's very difficult and requires lots of different skills which you may not have learned in school or may not have learned over your career.
And finally, it doesn't deal with a large number of special cases.
So it's not like we can just throw out everything we've got already and just use this thing to replace everything.
There's going to have to be some period of overlap between old systems and new systems.
But for scalability.
Animation quality is kind of losing the battle against complexity.
So every time someone wants to push with the current approach of the state machine to add a new way of doing things, it's kind of suffering against the complexity of the system.
And we can use machine learning to try and balance this fight, or at least some ideas from machine learning.
And these ideas might be one of the best ways we can try and make progress in this direction.
So some things we're looking at in the future.
So how can we remove this phase variable, deal with non-cyclic motions?
How can we scale to these hundreds of different styles?
So we talked about all these different tags.
What if we had tags?
What if we had a huge database with hundreds of different styles, and all of these were tagged in detail?
Would this work, and how would it work?
And how can we continue to improve the quality?
So in some sense, you can't get better quality than just playing back the motion capture data directly.
But how can we throw out the motion capture data and still retain a really high quality solution?
So I have a couple of bloopers so you can see what it's really like to do machine learning every day.
So you have some nice artifacts like this.
or one like this.
And this is like a nice moonwalk he was doing.
Okay, thank you very much.
Applause So if you have any questions you can come up to the mics.
Hi, hello.
Great talk, great work.
Thanks for sharing two short ones.
I didn't get how do you generate the clips for the training data?
How are you controlling the character?
And the second one, how do you get any research paper that you can mention?
We can get the details.
Thank you.
The clips for the training data are taken from this big, long database of long captures we did.
So if we're talking about having one-second clips, we basically have overlapping one-second clips all the way through the database, unless it was kind of tagged as junk.
And for the paper, if you Google face-functioned neural networks, it will pop up.
Hey, great talk.
In your diagram of the phase neural network, it looked like you had a separate network that was feeding weights into the existing neural network.
Is there a reason why you couldn't just use the phase as an input feature into the set?
Did that yield errors or something?
Yeah, so you can use the phase as an additional input variable, and it works somewhat, but the quality is not as high.
So the kind of details for why that is is a bit complicated to explain.
But roughly, you can think about it as when you give the phase in at the side, it's really much more similar to binning the data into really separate bins.
Whereas where you give it in at the top, it's more like giving another variable saying how to blend between the whole database at once.
So giving it in the side is really like taking a slice of this database with just the phase values you want.
Hey Daniel, great talk.
I wanted to ask about the tagging process.
And are you looking towards automating that kind of thing?
It's a very manual process.
And who would do that kind of work?
Because as the data scales, obviously that's a lot of work to tag all that data.
Yeah.
So I guess one of the great things about doing this tagging is that you can also use it to train classifiers, animation classifiers.
So you'd hope that you could get to a point where you have a large enough database where from then on you can do the tagging automatically with a classifier.
Thank you.
Hello, I was wondering how much iteration was done for representing the data going into the neural network because I could see the rotation as part of the pose being very hard to be consumed by a neural network and for a neural network to make sense of rotational data.
Yeah, so there's a lot of iteration.
That's kind of the black magic of machine learning.
And that's where you need to have a good intuition and do lots of different experiments.
So you can see exactly how we got the results we did in our paper if you look it up.
But yeah, that's one thing you need to play around a lot with to get good results.
All right, thank you.
Um, excellent talk, very interesting.
So a lot of the examples that I've, I've seen for both this talk and motion matching were were largely for character locomotion, navigation animation.
If you're, say, working in a space where maybe you have a very sophisticated combat simulation in your game, have you found any success in this approach for driving additive animations or layered animations through something like this?
Maybe something where your animation data doesn't necessarily have variables like speed or character intent, it's maybe just something like I got shot and...
Yeah.
Something like that.
So you can use the motion matching stuff to drive events.
So in For Honor, the attacks are actually matched too.
So you'll have an input which says, please play this style of attack in this amount of time.
And it will try and match the best one for the current pose and other factors.
So probably you should go to the UFC talk tomorrow.
I'm sure they're going to be talking a lot more about that sort of stuff.
And I'm sure it will be super, super insightful.
Thanks for the tip.
Hi, great talk.
Is it possible to incrementally train these sorts of neural networks?
So if I add a couple of new clips of a new kind of turn or something, I don't have to necessarily retrain the whole thing for tens of hours of computation time?
So, in theory, yeah, there is some research showing how you can incrementally train neural networks.
So, I don't have any personal experience doing this, but the research is there at least.
It just needs to be tried out, I think.
Not to my knowledge. I guess we'll see.
One nice thing about this approach for quadrupeds is that, well, it's very hard to get a quadruped to act, but you can get a whole bunch of raw animation data from a quadruped by just putting it in a motion capture studio.
Thank you.
Hi, great talk.
Question, did you have to overlay a lot of IK to lock the hands and feet?
And did you try to augment the rotation data with the absolute position of the joint?
So there is a little so in the Examples I showed there was no IK or anything All the joints were represented in the character space rather than the local space In the previous paper with the guy walking over the terrain. There was a little bit of IK not that much It's not that strongly required. It again depends on your Your representation and these sorts of things. Thank you How many supervised training data do you use?
Sorry?
How many training data do you use?
So in the stuff I was showing here, it was about 200 megabytes, which I think is probably about kind of.
half an hour to an hour of data.
So you saw in the clip, there was kind of three of us in the motion capture studio doing a whole bunch of random locomotion moves.
And I think we did about half an hour of that, something like that.
Thank you.
Yeah, great work. I was curious for fine, you know, you mentioned that you lose this fine detail.
Do you switch to another system when you have to like animate like cut scene sort of thing where somebody has to pick up a bottle or, you know, pick something specific up? Can you use this for that? Or is it not?
Yeah, you can just switch to the different system.
There's no reason why not.
But you could also potentially do it in the same system.
Like I was saying, you could actually tag the cut scene with one specific tag and say, this is a super high priority tag, so please make sure you start this cut scene when it goes.
But when you want that much control, it's probably easier to blend out to existing systems.
Thanks, great talk.
Thanks for making it very approachable.
Two questions.
How do you...
I don't like this mic.
How do you approach fine tuning?
Where does that fall now between the animation lead...
and the programmer on the neural network side of things with this approach?
And then have you also taken, the second question is, have you taken this and applied it in a layered fashion, like run and gun, where you're kind of breaking apart into different subcomponents?
So as far as fine-tuning is concerned, it's still quite early and we need probably dedicated tools to make the tuning a viable option.
So if your data is relatively small and you're doing something like motion matching, you can actually edit the data and that works pretty well.
If you're training a neural network, it's much harder to guarantee what the results are going to be like and probably will need dedicated tools.
And your second question was?
Layering.
So running on, things like that.
The kind of philosophy of machine learning is not to separate out things into layers.
And rather, you hope that it learns, in some sense, what is a layer and what isn't a layer.
For example, the crouching you saw.
Although we never captured any crouching on rough terrain.
If you crouch and you go over the rough terrain in our little demo, it does actually somewhat adapt the feet positions and these sorts of things.
So it's somehow learned that crouching is kind of independent of what the other actions you do on rough terrain are.
So the hope is that if you provide it with enough data, it will learn to do all these sorts of layering operations for you, and you can keep everything in one system.
That's the idea.
Thank you.
Hello, thank you, very exciting talk.
So I want to ask you about the neural network training.
So how do you decide how many layers you use, how many nodes in each nodes, and then the activation function you use, and then either you normalize the input or not.
So do you find everything optimized, or do you think there is still work to do to find the optimized solution?
Uh, yeah, so there is some kind of.
rules of thumb and intuitions you pick up if you do machine learning and you can also read the previous literature to see what researchers doing similar applications have done to get a kind of rough idea of how you might want to structure your neural network.
Yeah, there's always going to be improvements, and actually that's one of the best things about using neural networks is that you can import all the future improvements people come up with.
So maybe tomorrow someone will come up with a new activation function which improves everything.
We can implement that pretty much then and there, tomorrow morning, try it and see what happens.
And if it solves all our problems, that's amazing.
So one huge benefit of generalizing and...
making it more like a general machine learning framework is to borrow from these other people who are developing these things.
Nicole Azarro from Zio and Follow the White Rabbit.
I was curious, really, really great talk and definitely very approachable.
I was curious about taking this approach to other, you know, aspects of animation and like if you had like an Errol Flynn, you know, kind of style of motion like maybe in the attacks or something like that.
Would that approach, locomotion is, how the character moves is important and you want to have different kinds of motion for different types of characters, but if you have your star character with, you know, different special kind of moves, will that, will this approach work or what changes?
Yeah, so it's a bit difficult if you really want handcrafted animation because you need such vast quantities of data to get good results.
So probably my advice would be to start with a database of locomotion data which is motion captured and large and to try and come up with procedural tweaks which automatically stylize this motion.
in the way you want, or something like this, so that you can easily produce large amounts of stylized data.
And if you can do that, then yeah, it works in exactly the same way.
It can work for stylized motion, too.
Thank you.
Okay, I think that's everyone.
Thank you very much.
