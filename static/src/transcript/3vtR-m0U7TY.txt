So welcome, everyone.
This is a movie game called Avatar Frontiers of Pandora.
So naturally, this talk is a sequel to the talk that I did in 2020, which was Finding Space for Sound Environmental Acoustics of Tom Clancy's Division 2.
I have a couple of caveats to point out.
One of them is I will use the term emitter a lot.
And what that is, in Snowdrop Engine World is a virtual render context that has a position and an orientation that may or may not be used by the sound designers.
And all of the videos I've made are in stereo fold down, so unfortunately you won't get the full benefit of object mixing.
So just imagine in your minds when you're hearing the stereo how great it would be if it was in Atmos, but it isn't.
Other spatial audio systems are available.
My name is Robert Banton.
My pronouns are they, them.
And my job title is Snowdrop Audio Architect.
What does that mean?
I'm an audio programmer by trade.
I steer the roadmap for the audio tech within the Snowdrop pipeline.
And Snowdrop, of course, is an engine within Ubisoft that we use for making lots of different games.
My background is in MPEG codecs originally, music tech, and cinema audio.
And this year is my 10th year in the games industry.
So to summarize, what we're going to be talking about today are two things, BubbleSpace 2.0.
In the last talk, we talked about BubbleSpace.
I just want to show you some things that have happened in the last five years.
It might be interesting.
And Slapback 4.0.
So to be clear, what we shipped with Division 2 was Slapback 3.0.
Two other versions before that never got shipped or saw the light of day.
So I'm keeping track.
In case you're wondering, I don't remember there being a 3 or a 2.
You heard 3.
Okay, let's get started.
So, Bubble Space 2.0.
What is it?
So what we're doing is, is that we are devising a system for finding the free air around the player.
So in an analogy like this with a Division player, it's finding the best fit cylinder in a 3D space.
And that's the version we shipped in 1.0, we shipped in Division 2.
The tightest fit cylinder is about the azimuth plane, and the height of it is from above the player's head.
And it uses raycasts, it uses them radially and above the player's position.
at That way you can do effectively 12, but you're doing it over four frames, which aids in mitigating the CPU cost.
And then, whilst it does these measurements, by the way, these dots, the green dots are a raycast that hit nothing, and the red dot is a raycast that hit something.
So that gives you an idea, and the outer circle is the maximum range of the raycast.
And this is a plan view of typical American city, like Washington, D.C., for instance.
And, of course, that means that when the player moves around the world, The green circle is shrinking and expanding constantly, and from that we can do some intelligent mixing.
So here's a video.
This is like pre-shipping, I guess it was a year before we shipped Division 2, of it working.
death death So there's some examples there.
What you were seeing there was controlling weapon tails.
So each bullet has the initial hit and then spliced onto it.
The tail has some reverb baked into it and it was switching sizes of tails depending on the size of the environment and also mixing in some kind of like local ping pong delays, maybe when you're in alleyway, mixing out large ambiences that were in the street, that kind of thing.
So what did we do with Bubble Space 2.0?
So, the thing that we did before, we still do, it's an option, is called the minima azimuth estimation, and I call that the blue ring.
So if you look here, the red dots are a raycast hit.
And the green one is where it's hit nothing.
And the yellow ones is where they both hit, but it's gonna ignore one of them.
And the reason why is, is that whenever you have three hits, it's sorting and showing the two shortest distances.
So the furthest one at the end of that sort gets ignored.
So it's only averaging the two shortest ones.
And then the overall effect is this blue circle.
So that would have been our azimuth estimate for bubble space back then.
And it works great, except that sometimes when you're getting into cover or going up against the wall, it shrinks very, very small.
And sometimes it shrinks very, very, very large.
In some cases, you want that.
In some cases, you don't.
So I had a look into it, like, can we have a less aggressive Azimuth estimates?
So we came up with this thing called the Maxima one.
So what's the difference?
The difference is the sorting order is reversed.
So it's exactly the same calculations, but now we're looking at the two largest points and averaging them.
So in that case, the two red ones are being used, and the green one is being ignored.
In the yellow case, the yellow dots, one of those is being used, and the red one's being used.
If you look there, the actual circle is larger, but it's actually more like the average between, in a situation where you're in a large rectangular space, if you look at the total distances you average between these two, it's more close to that rectangle than the other one was.
The other one's more like, closer to the minimum width.
This is more like the average width.
So it turns out this is preferable for outdoor environments where you're coming out of a cave, you're out in the forest and it's less extreme and it means obviously that problem where you got into cover doesn't occur.
So here's an example.
I forgot to mention also, a lot of these videos look kind of, ugh, because I often put the graphic settings right down so I don't get GPU bound.
So if you're wondering, why does the game look so ugly?
That's why.
But you should be losing your ears anyway, so who cares?
So that's the azimuth estimation done.
So we have two modes.
And it's basically the same calculation.
So depending on how you sort, oh, hi, Alyssa.
Depending on how you sort them, you can get the two largest or the two smallest.
And that's it.
For the Zenith estimation, what we were doing originally was we were taking three raycasts going straight up, and we were taking the ones that agree the most.
The reason why you need to do that is you might be standing under a satellite dish, lampposts or something.
If you had one raycast, you might get a really weird edge case where suddenly it feels like the whole world has come down, but it's just because you're standing under a lamppost.
So the idea of doing three is to make it a bit more democratic.
One of the things about it is, is that minimally the two heights have to marginally agree for it to affect change.
So once it takes a measurement, two raycasts have to agree again for it to change.
So you don't get sudden changes.
Basically, everything has to agree before something happens.
So intelligent mix makes it a bit more stable.
And the tolerance is based on the sample area above the player.
So that actual radius distance is what gives you the tolerance.
The wider the tolerance, sorry, the wider the radius, the less tolerant it is to things that it measures above it.
So imagine in this room here, everything would agree because it's a flat ceiling, but maybe if we're in a cave or in a forest canopy, they'd be constantly changing.
So if you make a smaller radius, they're more likely to agree.
In a larger radius, they're less likely to agree.
So that was how we controlled the tolerance.
And of course, edge cases are easily dealt with, as you can see here, with this hypothetical block hovering in the air.
So, what do we change?
So, it kind of got interesting because the Zenith estimation now has a variable tolerance.
This is something that Dave Ostenacker asked me about.
He said, look, I want to be able to fly one of those flying lizards called Ikran under like a stone archway, these huge bridges.
But I want it to kick in and out the sparkly sweeteners of the rain immediately if you fly under the bridge, but if you're 50 meters below, don't kick in immediately.
And in fact, at some point, maybe, for the perspective of the rain, the bridge is kind of out of focus, so maybe don't change anything at all.
So the sample area can get smaller or larger with height.
That's how we deal with that.
Therefore, the intolerance can taper off with increased height.
So I have a diagram here.
So imagine you start ray casting with a radius like that, and then you taper them in.
The further you go up, the more they tend to agree.
So eventually, you get to a situation where they will always agree.
And that's how you deal with that.
If you want to have a tolerance, be very, very tolerant of high distances.
But that's not actually what he wanted.
He wanted the opposite of that, which is fine because we can taper the other direction.
We can go from out, wider and wider.
In this situation, they're more likely to agree closer to the player's head and less likely to agree further away.
And of course, therefore, you can then have the idea of the rain, not the main rain, but like the sparkly high-frequency stuff, mid-frequency stuff maybe, cut in and out just as you clip under that bridge, like when you're driving a car on the motorway when it's raining.
But if it's really, really high up, don't really notice.
So here's a video showing that.
So hopefully you noticed the fact that when you were just clipping under the bridge, it kicks in quicker, and then lower down, hardly makes any difference.
So the summary conclusion for this is, with the bubble space, is that having both minimum and maximum estimates makes it more useful.
And the variable zenith tolerance is very cheap to do, because all you're doing is really tapering those raycasts, right?
You're not actually adding any extra calculations.
And it's very easy to do with a coned radius, which is much more applicable for a planetary scale type stuff, large organic landscapes and so on, and the weird things you find on Pandora.
There we go.
So let's get into Slapback 4.0.
What is slapback?
So picture of Elvis Presley.
Slapback was originally this delay for a single short repeat echo effect.
And it was simulating essentially the immediate echo from a hard wall near a loudspeaker.
So when you had, we were playing in a speakeasy, you might have had this effect.
And then when you went to the recording studio, it's gone.
And you were used to hearing that.
So they simulated it.
and it was popularized in the 1950s with singers such as Elvis and so on.
So the slapback system in Snowdrop started out doing something vaguely similar, so that's the name stuck, basically, and we're still calling it that.
And in fact, there's other variations like mountain slapback and stuff like that, which is sometimes used in movie sound design, so we'll have some of that in a moment.
So in the inception, the version 3.0, as I said, was the one that was development, came out with Division 2.
It was enabled a few months into the initial launch of the game because we had some problems with memory fragmentation, with delay line cannibalization, actually.
And this is what AIMS to do.
I stole this from a textbook.
I didn't make this up.
So what you're doing is you're imagining the source position.
And then you're essentially creating a phantom image of where the mirror image would be.
and then you can re-render the sound at that distance and direction, add the propagation delay, flip the phase, and then effectively you have a very simple idea of an effect.
Additionally, of course, if you know the material of that wall, you can add some filtering to the delayed version, if it's rock or concrete or wood or earth, and then it starts to sound like the material that it's bouncing from.
In Division 2, what we were doing was we were raycasting in six directions.
It's data-driven, but you can choose six.
And then what it was doing is the raycast hits, which are the red ones, the green ones is where it hits nothing.
They have a surface normal, and then from that surface normal you can calculate where the phantom image would be.
So it wasn't, the raycast themselves are kind of like feelers, working out where the, because you don't really know where the materials or surfaces are, so you have to kind of guess and then you can extrapolate from there.
And then what it did was it placed emitters around the player, remember the word emitter I used earlier?
And the placement, the raycasting was using this phantom image technique.
And what we did was, in order to avoid having to do piercing raycasts that are very expensive, we had our own physics world.
So this image here, which was taken from the last talk, is very small, but it's because it's not that important.
The left side is basically the render view, what the player sees, and the right-hand side is what the audio raycast would see, which is, as you can see, a much more simpler geometry.
and cheaper because you're just doing a single hit raycast, not having to pierce.
You only see the materials you care about.
So what you would do is you'd mark up these materials as acoustically opaque.
And then when they're streamed into the world, they'll get added into this physical world.
And it would also shut down when the player was indoors because we had another system for indoors with our procedural reverb system.
And also because the echoes are very, very, very close.
You get sort of a flamming effect when they're very close.
It's accurate, but it's not cinematic.
So what we did was select audio from the player and route them to a fake device driver.
And then what we do is we feed back from the device driver back into the mix matrix.
So it's like a loopback system essentially.
At that point you can add on the delay and you can spatialize, you can move the direction.
And you can also compute the attenuations of distance and all that good stuff.
And the transit delay was added with a bespoke DSP effect because we wanted to have a delay that you could change as it's playing.
So here's a little demo of that.
Imagine how great that would sound in Atmos.
So the overview of SatBack 4.0 was we wanted to extend the system.
So a more versatile system, we wanted to be able to tackle landscapes and natural phenomena and weird stuff that you get in the world of Pandora.
It works in two functional layers with distinct behavioral differences, which I'll explain what they are in a moment.
And it allows for both early and late reflection effects.
What we were doing before was just doing early reflections first order.
Now we're having kind of that plus some pseudo late reflection effects.
And we also implemented material IDs for filtering.
The delay effect always had filters in it, but they weren't doing anything.
Now they are.
So here's a demo.
I'm going to show the system off, firing the gun randomly in the forest, system on, and then a more realistic thing where you're running in and out of a building, firing at stuff at random, and you'll hear the echoes between the early and late reflection balance change over time.
That's off.
Okay.
Fun fact, what you saw there was an exploding barrel.
Why was that part of the slapback system?
The reason was it's being mixed into it.
Does that make any sense?
Well, think about it like this.
The further away it is, the quieter it is, so it will pick up less in the system.
The closer it is, the closer it is to the player position, and therefore more accurate the echoes.
So it's kind of a cheat, but I think it works great.
That's why I put that in there.
So, in order to achieve this, you need several things.
First of all, you need to have a raycast emitter manager that places reflection points on designer set rules.
You want an audio signal router for capturing source sounds and playing them back through multiple emitter positions.
You need some DSP, digital signal processing, in case you forgot what that means, at each emitter destination that handles flexible delay times and filter EQ.
And for the late reflection effects, the emitter destination needs to be routed through an environmentally defined reverb generator with no pre-delay.
I'll explain how that works in a moment.
So let's look at reverb.
As you might have noticed, I stole that from the Audio Kinetic website.
But it is a good diagram of what reverb can look like.
So the red spike is initial source excitation.
The green ones are the early reflections, the ones you get from the immediate surfaces first, second, third order.
And then the blue stuff is after it starts bouncing repeatedly and starts becoming this late reflection diffuse reverb effect.
So what is early reflections?
Let's just define it.
in a room space, designing a room space like this, they talk about things like the direct field and the reverberant field and figuring out that critical distance to make sure that people are sitting in the reverberant field because then it's easier to predict what they're going to hear.
But when you do those calculations with the formula you got from college, you might get the wrong distance.
And the reason is, is because you're not taking into account the early reflections being like their own sources.
So not just these speakers, but imagine if these walls were hard.
There would also be additional sources, and that pushes back the critical distance.
So therefore, I will say that since we're not ever in games in an ideal acoustic space, that the early reflections, especially first order ones, are really part of the direct field.
The late reflections, nth order echoes, echo, echo, echo, echo stuff.
They're diffuse, largely phase invariant, kind of like noise, really.
And they have very predictable loudness attenuation over distance, so quite different behavior.
And therefore they're part of what we call the reverberant field.
So everything beyond the critical distance, reverberant field.
So for instance, right now, probably Guy there is in the direct field.
and the GDC staff are in the reverberant field for sure.
Hey.
So let's look at it like this.
If you were to capture that, if you made that impulse response yourself with an omni mic, what are you getting?
You're actually getting a bunch of different reflections coming at different angles and they're being recorded equally and then they're all summing together.
So if you were to place that in So you're componentizing parts of that reverb.
So rather than going with the final impulse, you're breaking it up into smaller chunks, and then when they re-sum, as you move around the world, they do change.
And in fact, even if you make a lower quality reverb than a very expensive impulse response that you've licensed from Penguin Audio or somebody, the fact that it's moving around, it actually makes it much more satisfying, and in many ways probably is superior because it's reactive.
So, I'm gonna define reverberation time, T60, okay?
Defined as the time it takes the reverberant sound pressure level from the impulse to fall by 60 decibels.
Interestingly enough, a tile bathroom can have the same T60 as a Gothic church.
They're quite different in size, so what's happening there?
Clearly the church is a much larger space, so why is the T60 the same?
Well, distance reflections, how much energy is being dissipated.
So, how your brain estimates the scale of reverberant space is from processing the times of the early reflections.
You learn this as a baby growing up.
You start to recognize the early reflections are super important.
That's how you know you're in a bathroom, if you're blindfolded, compared to being in a gothic church or in a train station concourse, for example.
Therefore, the delayed times in the direct field are critical to immersion.
That's my point.
That's why it's worth mentioning.
So the light reflections are essential for colored noise that indicate what the surfaces are made from and that kind of thing, so they know the difference between marble and wood and that kind of thing.
So stone or concrete versus wood or carpet, as I said here, they're clearly very different materials.
So the raycast and emitter management.
We have two raycast functionalized working independently.
The raycasts have a minimum distance radius.
So rather than going straight from the, oops, sorry.
rather than going straight from the center of the player position, they can be a little bit further out.
This is needed for a thing that I call the ring of local origins.
I will have a diagram for this in a moment.
Reflection points are rendered via emitters, of course, and the layers have different emitter cannibalization behavior.
So what you're doing is you're budgeting your emitters for these two layers and how they cannibalize and decide to go to a new reflection point changes because for what you want for early reflections and what you want for late reflections can be quite different.
So the early reflections layer was optimized around some response enough to oncoming geometry.
So what's happening is, because you're moving around, they're moving quickly, quickly, quickly.
And whatever they were playing a moment ago, they keep playing in the new position.
The late reflection stuff is optimized around long tail fidelity.
So what will happen instead is, a new point will start forming, the other one will start fading out, because it still has reverb in it, so you don't want it to cut out immediately.
And if you moved it to the new position, it might sound weird.
So that's how we get around that.
You can actually change the fade out times.
And this is how you tackle the direct field versus the reverberant field stuff, okay?
So, I'm gonna use a division character again, because there was no clip art available for Avatar.
It's player-centric, so it only happens to the actual player and not to any of the other characters yet.
Mathematically, it still only performs first-order reflections.
So that thing I showed you before, hitting off the walls, it still does that.
So for the azimuth raycast, if you imagine you raycast out radially, what it's doing is it's hitting like an imaginary surface, and then you can work out where the phantom image is, and if it duplicates, then you know, well, they're duplicates.
So in this case, from three hits, we go, okay, well, two of them are the same as the other, so don't just kill them off.
And then if you end up with one, And also we can do things like we can have an azimuth height offset which we can then correct for When we resolve the position there might be handy for not accidentally hitting things that are Just outside of your minimum distance, but they're still too close.
Maybe a wall cover you know Video games, right?
And then upwards or downwards.
So depending on the layer, for the early reflections, we raycast upwards because we want to hear ceiling stuff.
And when we're in the air flying, we want to hear the late reflections from the ground.
So we raycast down for the late reflection layer.
So here's some maths, but visualize graphically.
Don't panic.
Reflection points are derived from the raycast hit positions and the surface normals.
So reflected images can be positioned either by the full 3D phantom image.
So you start off with the white circle, you ray cast down, you get your ray cast hit, it's red, and from there you have a position and a normal.
And what you do is the reflected distance scale is two times the dot product of the direction vector from P to Q, that ray cast, okay?
Then the reflection vector, I've got to move my mouse somewhere, where's my mouse pointer gone?
reflection vector is then the surface normal multiplied by the reflected distance scale, and then you can then shift across.
So from the extruded position, if you subtract that from the original source position, you get the blue dot.
That's your phantom image position.
You can do it fully 3D.
You can lock it in the horizontal plane if you want to do that.
is the same in terms of like looking overhead.
And interestingly enough, there was another mode that Alex asked for, which is like, what if we don't bother with that at all and we just double the distance?
So we call this distance only, that's another mode you can do.
Interestingly enough, it's less correct, but it's more stable usually, actually, because you think about it, if you're running on the side of a mountain wall, those reflections are gonna be constantly shifting around, where if you want something stable that's always at the right kind of distance, maybe that's better to just do distance only.
Super cheap, right?
So here we go.
This is what the ring of local origins is about.
This is how we got our minimum distance.
So we have the player position.
We have a radius around it, which is the minimum distance.
We ray cast from a local origin around that radius.
When we get our hit, we then calculate the rest of the stuff you just saw as if it came from the center.
Record the hits, and then we compute the reflection points accordingly.
And so you get the same calculation, but in this example, you would have completely missed that little wall on the right there, which is what you want for something that is further out.
So without having to do any kind of additional calculations, by literally just moving the origin away, you're getting later reflection points.
And of course, if you do the distance only, you get that effect.
So let's focus on the early reflection layer.
As old reflection points invalidate, emitters move immediately to the new position.
Reflection DSP affects play uninterrupted.
So if you happen to be firing a gun at that time, whatever gunshot's still playing out, just move to the new position.
Delay time and filter settings are updated to the reflection point on the fly.
This is why it's important to have a delay time that can move as you're rendering, and filters that can move stably whilst you're rendering, so you don't hear any weird pops or glitches.
Okay, let's take it through step by step.
So imagine we are in some kind of gully.
This is a plan view.
The player position is the black dot.
It'll raycast out to, say, the right there, find those two hits, realize it's the same position, so it'll cannibalize one of them, and then you end up with that blue dot.
That is our ER position, okay?
If the player moves forward, oh, sorry, I did the wrong thing.
Player moves forward, what it'll do is, is it'll make a new position, but it realizes it's close enough to the old one to just shift the point.
So it just keeps playing, moves immediately, like that.
Keep moving again, it's going to find a new position, which is not strictly correct, because as you can see the wall isn't there anymore.
You could actually sanity check that.
If you re-raycast that position and realize you're not getting the same information, you could kill it completely.
But, video games.
And then, when you go up to here, you get another position over there, but it's so far away from the last one that it decides it's going to kill off The older one now, rather than moving it, it's going to fade that out and start a new one over there.
And that's how we deal with early reflections.
Late reverberation.
So here we're going to do something a bit different.
As old reflection points invalidate, they fade out, and new reflection points can begin in new positions.
There's always a kind of a fade happening.
It allows for long reverb tails to play out.
So if you've already committed to a rather than cutting that off, that would sound disturbing, right?
Let it play out and then fade.
So it uses the ring of local origins quite extensively, because you really want to have that minimum raycast radius quite a far bit out.
To prevent hitting valid materials too soon, that's really what it's for.
It's much more efficient than using a piercing raycast.
So what you could have done is decide to do a piercing raycast, and then the number of hits you got back, just keep shimmying down that array until you found one that was a favorable distance.
But super expensive.
This, it's just one raycast and you're done.
And then you add reverb to the delay line.
So the reverb engine is working normally, but it's getting a super delayed copy.
And therefore, when you hear it back, you're getting it spatialized at that distance with the reverb that's defined by the environment.
And therefore, the shift, you see here, the yellow line is varying with the delay time.
That's what you're shifting.
So it's the pre-delay on that reverb is what's affecting.
And it's happening in all these different positions around your player's head.
Let us look at the same gully situation, but with this wider ring of local origins.
Let us start here.
The player stood there, and you have bottom left, top right, bottom right, three raycasts, and therefore we have these phantom image positions.
We move forwards, they invalidate, and they start to fade out because they are invalid, and new ones are found.
Momentarily, you actually have more emitters playing than you have actually budgeted for.
That is what you have to double your budget for this.
and then you move again, and then the other ones that were just started fade out again, and the new ones start further on.
And that's it.
So in terms of like filtering, I did some research and got some real acoustic data back about what happens in different environments.
I have a chart here from somewhere in the Pacific Northwest, and it's different plants that someone's found the acoustic absorption spectrum for.
And I use this information for the cloud forest.
Don't panic, baby tears is a plant, not actual baby tears.
Let me clarify that, because when I showed this to somebody, they panicked.
They went, no, it's... So what you do then is, is that this is the absorption spectrum.
So to make the EQ, you just basically flip that.
It is power-based.
And then once you add the logarithms, you get a kind of decibel shape.
So that's all the EQ settings I based upon, based on that.
And I don't think they ever change.
I think Alex was happy with them, so cool.
So every value reflection point has a material ID, and we take the material ID, whatever that material is, and then we have a list of known EQ settings, and we apply it.
Materials are mapped to filter parameters, so we use the raw filter parameters.
And then we send those in to Wwise through real-time parameter controls, and that's what the reflection effect is getting for every single emitter.
And of course it can be different for every emitter, right?
So this is a little bit of a window screenshot from Snowdrop.
So it's kind of small, but basically there's a couple of materials there, and basically you can decide to have, there's two types of filter in the delay.
You can have one of them on, both of them on, whatever.
If one of them's off, it just disables and it's not part of that, it doesn't take part in the processing.
So you have this idea of center frequency and gain and some kind of balance, and this is because the filters have different profiles.
One of them is a peaking filter, So therefore, we can change the cutoff frequency and the gain.
I set the Q factor to four, just so I don't have the Q factor changing constantly.
The other one is a high-low shelf kind of filter.
We can move the center frequency like that.
So you create a list of materials that you want to target.
And if the material is not on that list, it just ignores it, filter turns off.
So therefore, it's just completely flat frequency response.
For each material, it links material ID used in the game.
So it's for game-specific now, right?
enable each parametric band crossover filter as required and these values are posted to the slapback emitter as it moves or is reactivated.
Any unlisted materials disable the filters when they are encountered.
So and one other thing I want to bring up.
We're sending the propagation distance, but the sound designers choose the delay.
So here is an example of one of the curves they used.
If you think about it, 450 meters should be about two seconds, and it's not.
It's about one second.
That's because it sounds cool that way.
Entertainment.
The signal rerouting stuff, the guts of that is essentially that we have this broadcaster system, very similar to the one in the Red Engine, funnily enough.
where you're taking each time that the Wwise engine is doing its render pass, you get a callback.
There's two buffers and they're switching constantly.
So you imagine after the second callback, the next one will look like the top image.
You either have one buffer recording and the other one playing, but they never meet.
We can actually have up to eight of these.
So there are broadcast channels and inside of those, they have up to eight audio channels.
So think of it like a digital tape loop.
If you ever use one of those, you have a record head and a playback head.
And the loop gets moved on from a callback, we register within a rendering chain, so we know when it's happened.
It does add latency though.
So by doing this, you're adding an extra buffer of latency.
So if you're rendering 1,024 samples, by rerouting it this way, you're adding 1,024 samples, which is 21.33 milliseconds at 48 kilohertz sample rate.
However, we're gonna delay even more, so it's fine.
So the recording side, we have basically the sender sync plugin in Wwise, which is a fake audio device.
Wwise doesn't know that it's not an audio device.
It's actually writing to a common memory.
And it sends the audio always to the record head.
It doesn't know which one.
Every time it writes, it gets whichever one the broadcaster tells it to, remember, because they're switching.
And it controls what the broadcast channel it's using.
and how many audio channels is allocated.
So you might, if you're deciding to make a game like X-Defiant where it's only stereo, then maybe reduce the allocation to two channels.
Called once at the end of each render pass, as I said.
So every time it renders, call back, switch.
From the playback side, we have a receiver source plugin, and that just takes whatever's recorded in the last pass, right?
Because that will be now in the playback buffer.
And that's triggered by a play event.
So the emitter needs to know there's a play action.
And then it will start reading from that buffer.
It's faded out by the stop event.
So we always have a stop event, say, to fade it out, as I said before.
But depending on whether it's early or late reflection, that fade out might be very short or quite long.
And it controls what the broadcast channel it reads from.
So you can actually have a one-to-many relationship.
So if you record once to one channel, you can read many, many times from that same broadcast channel.
It gets its actual audio channel layout, specifically whether it's 7.1 or 5.1 or whatever it is, it gets that from the sender sync plugin.
So that's defined by the actual fake audio device, which it actually gets from the properties set up by the playback engine.
And it is called once at the beginning of each render pass.
The filtering part Each reflection point receives a receiver instance, okay?
Then we add the delay.
So Wwise is playing a source plugin with a single parameter, the broadcast channel, and then the audio signal from the source plugin needs delaying and filtering, right?
We need to get that propagation delay in there.
We need to filter according to materials.
So how it works is it's kind of like this.
You have a circular buffer, and the record pointer is spinning around constantly.
Imagine it's going to the right, and when it gets to the end, it goes back to the start again.
The output side is going to be behind it, and the difference between those two positions is the actual delay that you hear.
Now if you shift that audio position suddenly, it'll hear a click or a pop or it'll not sound good.
So what we do is we actually create a nice power complemented crossfade between the old delay and the new delay.
This is not actually the high fidelity delay with Doppler you would expect.
What's actually happening is it's just editing.
And that way, you can get around unnecessary Doppler.
It depends how fast you're running, I guess.
But if you're in a car, you would definitely hear that in a racing game, right?
But this is it.
We're now jumping to the new part of the delay buffer and doing it whilst removing any kind of weird artifacts.
And then we do phase control, which is just basically flipping the phase.
And then we do the filter EQ.
And that's the output sample.
So, do I need to, yeah, basically I've just said all this stuff, haven't I?
There we go.
So, let's have...
Some kind of like technical points in case it comes up.
You need specialized memory management to do this because you're constantly needing these delay lines and they're half a meg in size, right?
Why half a meg?
Well, four bytes per sample is 131, 72 samples or 2.73 seconds at 48 kilohertz sample rate.
Why does it need to be up to two seconds?
Because that's the size of the world you're dealing with.
Those are the kind of delays you might have to deal with.
Speed of sound and air is typically 340 meters per second.
In Pandora, they never told us.
We never have found out what the speed of sound in that atmosphere is.
But let's say it's like air.
Therefore, the maximum transit distance could be 928 meters.
You need to have multi-threaded access because this is being rendered in multiple threads.
And you want to be able to prevent memory fragmentation.
So therefore, we need specialized memory management, right?
So the first filter looks like this.
If you're a DSP programmer, you should recognize this.
This is called the biquadratic filter direct form two.
And what happens is you change five multiplies, and you can get lots of different filter effects.
In this case, we're just using a purple formula for those coefficients.
We're not doing any kind of musical modulation type stuff, so this is actually not a very stable filter.
Don't use this if you're trying to do something, you know, DJing.
Don't.
But, as I said, the Q factor is fixed to four.
That makes it simpler to do the calculations, the coefficients, actually.
And then you can then do things like you can attenuate, or you can boost, or you can move the frequency left to right, and this is the kind of the overall range that that filter works at, okay?
That's our Bode plot.
For the other filter, this is a little bit more unexpected for some of you, I guess.
The crossover filter part looks like this.
So if you look at the inner core of that, that is a low-pass filter modeled on a capacitor-resistor time constant that discharges exponentially.
And the K value, that multiplier, is one minus e to the minus omega t. That big T is actually the sample rate inverted.
It's a time of one sample.
The e to the minus omega t, if you calculate it exactly like that, it's kind of expensive, so you don't do that, you do a polynomial fit, because that's obviously a very good candidate for a SIMD optimization then.
Anyway, the outer line is the same input that's subtracted from this output, and then you get like a high pass characteristic, kind of.
And then at the end here, we have a voltage divider, and if it's exactly in the middle, they will perfectly cancel each other out and be completely phase linear.
If you don't believe me, There you go.
So the inner core part, as I said, that time constant low-pass filter is the blue line.
And then when you subtract that from the input, you get the orange line with a bit of a bump, but don't worry about that.
And then when you sum them together, completely phase linear.
So you can have this on and not hear it.
But of course, you can then change the offset between the two.
And so you can get things like this.
So you can boost it.
You can boost the other way.
So therefore, you get your high-low shelving in a nice, efficient way that sounds very natural.
Are you all ready for a white noise test?
All right, here we go.
Here are the filters in action, just with some white noise.
You can hear them working.
Here we go.
How are your ears?
Hopefully that tells you how the filters are working, right?
So this is when we're taking the echoed sound and we're filtering it, that's how it's being applied.
So the combined results, I put this all into one picture.
Let's go back to that gully.
So what we're actually getting, if we're in that position, we're getting two early reflections.
One of them is quieter than the other, because it's further away.
And we're getting four late reflections with the reverb applied.
It's defined by the environment with different filtering.
And because of their angles, they're spatialized at different angles around the head.
And different distances mean they have different pre-delays.
There we go.
Does that picture make sense?
This is the decomponentized version of that nice impulse response you saw at the start.
And then, I want to show an example where it's realistically in the game.
The one other thing that's missing from this is obstruction occlusion tests, because sometimes you'll have phantom emitters that are behind a wall, and you want them to be filtered that way, right?
So here's an example in-game, very chaotic, but because of the propagation system, you can therefore control what you hear, depending on the geometry around you.
All soldiers be ready There we go.
So, let's summarize then.
The reflection point cannibalization needs to be different.
You need to have different modes for that to optimize around the requirements of early or late reflections, right?
That direct field, reverberant field.
Ray casting with a ring of local origins, really useful.
It just incentivizes doing something where you're maximizing the amount of information and minimizing the CPU cost.
And filtering materials is a big win, obviously, because it sounds, you get lots of variations, much more natural sounding.
Okay?
And you need robust memory management.
I did a more elaborate description of that in the 2020 talk, so if you're curious, you can go back to that.
It's on YouTube.
And one last thing.
Some people will think, we should hire an audio program to do this.
So I want to explain to you how big the team is that's working at Snowdrop Reference Branch right now.
In Sweden, we have me, Martin Wallin, who's the lead.
We have Martin Löfgren, who did the emitter manager.
Kasper Zedekunas, who did the amazing propagation system that works on the GPU.
We have Hang Zhang.
We also have Dave Driggers in Stockholm.
In Germany, we have Hannah Kriegler, who's working on rapid prototyping stuff.
And we also have in Canada, we have Oliver Snade and Andrew Smith, neither of whom are Canadian, but for some reason they live in Toronto.
So, what I'm trying to say with this picture is, as well as credit where credit's due, you don't need an audio programmer, you need a platoon.
Thanks for listening.
So I guess some of you have some questions.
Yeah, thanks for that.
That was a buffet of delicious ideas for an audio program.
I had a question about the ring of local emitters.
Is that based on bubble space at all, or is that just a completely separate thing?
So the ring of local origins is just like another circle that I just visualized in my head.
To be specifically, the bubble space thing, the reason why I call it that is because when I originally drew it out on paper, it looked like lots of bubbles on the page.
It's not really.
bubbles, it's cylinders, but anyway.
The ring of local origins is just a imaginary radius you've created in your system where you then create these origins you may cast out from.
And then you use the original position for doing the phantom image technique, that's all.
So it's just a mathematical concept to avoid hitting things too soon.
Okay, cool.
Can I ask a quick follow-up question on that?
Do you do any slewing over time for the bubble space calculations as well?
There's lots of smoothing going on.
Again, there's a better description of that in the last talk, 2020, so I won't repeat it here.
That's why I didn't put it in this talk, because it would seem like money for old rope.
But yeah, so there's lots of smoothing going on, and that's why you can avoid weird glitches, like walking past a bunch of pillars in a train station, because it will pick them up, but there won't be enough coherency for them to take effect to the overall calculation, which happens actually over 16 frames.
Hey, thanks for the great presentation.
You mentioned that it was always player- or listener-based, all these systems?
So, you could do it listener-based.
You could do that.
What I found was, is that because of the way we make games, listener is usually the camera position, or something close to it.
Maybe you interpolate between player and camera position.
going to But what you can do is that you have a threshold for when you start making the calculations happen again.
Because you're not doing it every frame.
If you're standing still, you do the calculations once and you've stopped.
And then you have a threshold.
And you can set that in data.
But let's say it's half a meter.
You move half a meter, and then it does the calculations again.
If you were to do that with the listener position, you'd probably be doing lots and lots of extra calculations and just be heavier on the CPU.
So that's the trade-off.
So it wasn't actually my question, sorry.
It was, did you ever find a situation where you would have liked to have an emitter, so another emitter than the player position, to also have its calculations?
And are you considering that for the future?
We have lots of, OK, 10 minutes to go.
We have tried a lot of different things.
And some of it sounds really cool, but it's just very expensive.
And if you remember, we were originally targeting Xbox One and PS4.
So this is why it ended up the way that it is now.
However, now we're in a next generation and we're using the GPU and stuff like that.
Lots of possibilities.
So we'll see.
Maybe in a couple of years I'll do another talk and I'll remind you of this question and go, hey look, guess what we did?
So we'll see.
I look forward to it.
I have a question about geometries and I was wondering who is simplifying the geometry for audio and how hard?
How is the geometry simplified?
That's actually done by technical artists.
So what they do is they start off with a random mesh.
And if it turns out to be too complicated, they simplify it.
So if you get like a fallen down tree trunk, they might turn that into a box.
That's what ends up in that physics world.
And that's what we see with the raycasts.
Does that make sense?
Is it hard for them to do?
It's just in the data.
I had the same question, so great talk.
I have another question.
and You could use FDN reverb or something else, some algorithm.
That's the part that doesn't matter.
That's the modular part of it, so that's up to you to decide.
And maybe it's, you could have more reflections, more and more LRO type late reflections with a cheaper algorithm perhaps, if you wanted to do that.
But that's, I purposely avoided talking about that because that's not part of the system that is decided further down the, it's actually the environment element manager in Snowdrop that actually defines that.
With having the reflections be driven by this kind of like ring buffer kind of thing, was that sync plug-in where you were collecting the delay line, was that, did you have just like one of those or did you put them on multiple different buses?
I'm wondering about like... So the sync plug-in part is one per exit destination.
If you come to the Audio Kinetic booth at 115, I will show you that slide.
to didn't end up in the game.
But it was a nice idea.
I think it just didn't quite work.
But yes, that would have been a separate channel.
So therefore, you have a different device and then a different auxiliary send.
And therefore, you read it off a different channel on the way back again and then do something else with it.
Thank you for your talk.
I think in your system, all the reflection and the light reverberation is fully 3D positioning, but isn't the light reverberation should be more omnidirectional?
Sorry, what was the last part of the question?
Late reverberation should be omnidirectional.
It's non-directional.
Yeah, so it's a good point.
So the question is, if you're positioning these emitters and they are being spatialized, when you're adding reverb to them, how does that affect it?
Well, in actual fact, the reverb in and of itself is working already to render out the channels.
And so what you're sending to that has already been spatialized in some way.
So you've already worked out the distribution based on the emitter position.
the delay.
And then however that reverb chooses to do its work, that's what ends up going to the channels.
So they're actually completely decoupled.
Okay, thank you.
Did I exhaust all the curiosity in the room?
Should we go get a coffee?
Alright, thanks very much.
