we go. All right. Thank you all for coming. Remember the usual things. Turn off anything that might make noise. Keep arms and hands inside the vehicle. And if you've taken AP statistics, you've seen all this before. So let's see some other talk.
AP statistics, big topic, complicated, lots of math. Not getting into that today. Today I am going to give you the TLDR version. I'm going to give you immediate results in 30 minutes like Bob Ross. A few basic moves you can make to bang out some happy trees and little mountains in the space of this talk.
Actual statisticians will probably be looking down their noses at me at the things I'm glossing over and simplifying.
But it takes a year to be Rembrandt and 30 minutes to be Bob Ross.
Let's do it.
Today I'm going to show you four different ways to come to decisions about A-B tests using science instead of yelling about opinions.
All of the stuff you can do in Excel, a couple of them are built into Excel.
The others are macros that I will give you.
And I'll close out with some words of caution.
Now, to conserve time here, I'm going to gloss over a little bit about the actual how-to in Excel.
I'll show you the dialogues, but I'm going to blitz through them really quickly.
So, all of that information will be up at my support page, along with the slides, so you don't have to take pictures of every slide.
You can just sit back and enjoy the show such as it is.
Why do we need statistical tests? If you were to walk into a forest and measure every tree and get a graph something like this, you would not need fancy math to know that spruces are taller than maples.
But if you could measure only 16 trees in a forest, and you got a graph like this, well, do happy trees grow taller than unhappy ones? Who can say?
To avoid getting me into NDA trouble, let us say that all of my examples here come from Hypothetical Games, Incorporated. And here at Hypothetical Games, Incorporated, our build times kind of suck. A full rebuild of the whole game, including assets, takes two hours.
And let's say that I'm a programmer at Hypothetical Games and I'm tired of looking at progress bars. So I go to my boss and I say, you know, if we just bought everybody a solid state drive, build times would get faster and I'd save a lot of time.
My boss, he's kind of a curmudgeon, and he makes the good point that SSDs are really expensive, and would it be worth it to buy one for everybody in the office.
All right, well, I'll come in over a weekend, clean out my disk, do a full rebuild from scratch, two and a half hours, fantastic.
Now I'm going to take my SSD from home, come in, plug it into my work computer, build on that thing, two hours and 20 minutes.
All right, 10 minute improvement, a little bit.
Just to be sure though, let me run this test again back on the hard drive, and it's 2 hours and 10 minutes, so what's faster than what?
Alright, I'm going to spend the entire weekend building over and over and over again, and I get numbers that are all over the place.
On average, the solid state drive builds about 10 minutes faster, but here's the problem with averages.
With a small sample, and a random distribution, you can get completely different averages, even if the underlying distribution is the same, so long as your sample size is small.
Here's another way of looking at it.
Let's say that I went to two schools and I gave every kid at each school a puzzle and I just measured how long it took them to solve the puzzle.
You might look at this and say that kids from New Jersey are smarter than kids from the Bronx, of course.
But that depends on which particular students you're testing.
If I had grabbed only three of the students from every school and tested them, well, the averages might have looked completely different.
Or they might have looked completely different in the opposite direction.
In fact, we can enumerate every possible three student combination.
And every average that comes out of every one of those three student combinations, and those averages of all the combos will itself follow a distribution kind of like this.
So, based on the sample size we have, we have a large uncertainty about whether the mean of the sample corresponds to the mean of the actual population.
As our sample size gets bigger, this estimate becomes more and more precise.
And as it gets smaller, we have more and more uncertainty.
So the tests I'm showing you today are all about managing this particular uncertainty of having only an estimate of this population mean from your sample mean.
And in case you're wondering where this data comes from, it's the exact same distribution.
So any differences were purely from chance.
All right, back to build times. We're scientists. Scientists build a hypothesis and then test it. What's my hypothesis?
Build times are faster in a solid state drive. Actually, it's easier to disprove something than it is to prove it. So let's disprove Fred. Fred's hypothesis is that SSDs don't make a difference. And if we can show that Fred is wrong, then I'm right. Probably.
The test for this is called Student's Tea Test.
It's actually named after this guy, William Gossett, who had to publish under the pseudonym Student because he was under NDA to Guinness at the time that he invented it.
Go figure.
He was concerned with quality control on beer.
So let's say that this distribution represents like the specific gravity of bottles of Guinness that roll off the line.
And it's kind of clustered.
It's a little bit of variation.
And for every batch, he has to grab 10 bottles off the next batch, look at those 10 bottles, kind of guess at what the real distribution is that they represent, and then say, Do those bottles match Guinness' quality control standards, or are they different enough from the specs that we have that we need to kill the batch?
Now, student's boss, apparently an ancestor of Fred, not a big fan of killing the entire batch and a bunch of profits.
And he says, well, you know, you've only got 10 bottles here.
With a sample size that small, maybe your sample mean is only a little bit different from the population mean just by chance.
I can prove you're wrong, says student, probably.
There's a couple ways. One is if the mean of our sample was way different from the from the spec that we're expecting, we can know like there is no possible way these ten bottles could represent a good batch of beer.
Or, if student had measured every single bottle that came off the line, then we would have a really good estimate.
of the mean.
But if the sample size is small, and the means are close to each other, well, you know, we can't really disprove Fred.
So this is what the student's t-test built into Excel does.
It's one of the most useful tests in statistics.
It takes the data values that you have, and it takes the size of the sample, and it spits out a number p, which for the purposes of this talk, is basically the chance that Fred is right.
We can make a bet with Fred before we run the numbers to say if there is less than say a 5% chance that you're right, then we'll agree that you're probably wrong and the two samples are in fact different.
So that's what P is.
The P value that you've seen in like all the scientific studies, it's basically the chance of a false positive on saying that the two things are different.
So if Fred's wrong, we can say we rejected the null hypothesis, we've rejected the Fred hypothesis, and the two groups are statistically significantly different.
But you gotta pick this number before you do any tests.
Like, don't shoot an arrow at a hay bale and then paint the target around it and call it bullseye and say, well it's a 15% chance of being a false positive, we must be fine.
Also, any actual statisticians in the audience are now totally cringing because I've completely oversimplified everything.
P-values are hard, good enough for our purposes.
Back to the build times.
Here they are in minutes.
So here's how you do a t-test in Excel.
It's built in. You go to the data menu.
You pop open this tool pack.
You pick t-test.
In this case we're doing two samples.
pops up a wizard, you specify your acceptable false positive threshold there, gives you a table.
That's the number you care about.
0.043, 4.3% chance of Fred being right.
There's this one-tail number, ignore it.
That number is for when you are certain what direction the difference may be in.
You would only use that number if you were absolutely positive, without doubt that SSDs could only build faster and not slower.
But you never know that ahead of time.
If you did, you wouldn't need to be doing the statistics.
So always use the two-tail value.
Now, should we actually buy the SSDs?
All we've shown is that the two populations are different, but not how different, and not whether it's worth spending all the money on those drives.
So what we actually care about is the actual difference in means, how much of a change is it.
This difference in means has a margin of error just like the actual sample mean.
And we call that the confidence interval in the difference in means, which is basically, we know that the difference is 11 minutes, give or take a few minutes.
We calculate the give or take using this macro that I'm giving you at that slide, at that URL that I showed you earlier, and we'll show you again.
In this case, it pops out a number, 8.14.
That means that the actual difference in build times is, with 95% certainty, somewhere between 11 minutes minus or plus 8 minutes, which is to say somewhere between about 3 and 19 minutes.
So should we buy the drives?
Let's do the arithmetic.
We've saved about 11 minutes per build.
Let's say a dev costs the company $0.50 a minute.
So we've saved $5.50 a build.
SSD costs about $300.
So every dev would need to be doing 55 builds from scratch in order for the SSD to pay for itself.
Except we've got that uncertainty.
So if we did this test like 1,000 times, the average would be somewhere between 3 minutes and 19 minutes difference so we would have to do somewhere between 210 or 31 full rebuilds per dev for the drive to pay for itself so just because the two things are statistically significantly different it doesn't mean that the difference is actually practical you have to look at how big the difference in means is you can't just say well the two things are different then we should buy all the drives Incidentally, if the confidence interval straddled zero, if I said the difference in build times is like five minutes plus or minus ten minutes, then it's like the improvement is negative five to fifteen minutes, which is to say I've definitely proven that things are either better or worse or the same, which is to say it proved absolutely nothing. So if your confidence interval straddles zero, you don't have a result.
You use a t-test for continuous outcomes, anything you can measure with like an integer or floating point number.
And you use it for things that are sort of normal-ishly distributed, kind of bell-shaped.
It doesn't have to be exact, the t-test is pretty robust, you can get away with a lot of lopsidedness and a lot of lumpiness, but it has to be something that has a real mean like that, where the mean is meaningful.
If you've got something like the one on the right, the average value there, like there aren't even any members of the population that are at that average.
Like saying the average human being has 1.9 eyes.
So, it doesn't really matter.
So you can't use a t-test for that.
Okay, let's say that Hypothetical Games is working on an online puzzle title, Angry Confectioners.
And this is one of those games where we can actually push updates up to our players and measure them live.
Fantastic.
So we've got our new cupcake mode and cupcake mode isn't doing great.
And Fred says, you know, I think cupcake mode just isn't any fun and we should probably cut it.
I'm thinking, you know, I think, like I've been talking to my friends and they're just complaining that cupcake mode is just a little too hard.
And I think if we make cupcake mode easier, people would play the puzzles more because they'd get less frustrated.
Here's the problem. Play times for this game are all over the place. Some people play one hour a week, some people play ten hour a week.
So, how can we, like, measure a difference in play time when we've got such a huge diversity in player, in player, uh, play times?
So, how about this?
How about we run an experiment, I'll show some people the easy version, I'll show some people the hard version, and I'll switch them, and then we'll compare every person's playtime during the week where they saw the hard version to that same person's playtime when they played the easy version.
We can filter out individual differences between people in this way.
When we do this, it's still a t-test, it's just called a paired t-test, as opposed to a two-sample, also built into Excel.
What we're asking is if I make this puzzle easier, more people spend time playing, we've got two versions of the puzzle, and my hypothesis that I'm trying to prove or disprove is that people play the hard version either more or less than they play the easy one.
And Fred's saying, no, cupcake mode just is no fun at all, we should cut it, I don't think how hard it's going to be is gonna make a difference.
Let's run the experiment.
We've got a population, cut them in half, half of them get the easy version, half of them get the hard version.
After a week, give them a switcheroo, record how long they all played the game.
The advantage of doing a crossover this way is it filters out things like everybody playing the game less week to week, or if there's a holiday or something, like it kind of washes out because you've split the population in half.
We've got data from every player for every version.
Let's add 40 more of these.
Now we calculate the difference for each person from how much they played the hard version from how much they played the easy version.
Looks kind of like that.
That's where the zero number is.
Looks a little bit lopsided.
Let's do the math.
Go back to that data analysis thing in Excel.
You pick paired two sample t-test.
Out pops this number.
1% chance of Fred being right.
So we can say that yes, people do play the hard version about negative .18 hours less per week, about 11 minutes less per week.
So yes, we should probably make cupcake mode a little bit easier.
And maybe change the other puzzles too.
What about binary outcomes?
Questions like, do people turn left or right?
Do they do the thing or not?
Do they even buy something or not?
Let's say that Hypothetical Games is working on Generic MOBA.
And Generic MOBA is a free-to-play game that makes its money by selling garments of some description.
Pants, let's say.
So we have a nice pants shop.
The problem is, nobody goes to the pants shop.
And Fred's kind of getting a little bit peevish with me.
Well, so our problem is that nobody even goes to the store.
Do they know the store is there?
Maybe there's like an activation energy.
Like maybe they just have to get over that hump.
So what can we do that makes them at least go to the store at least once and buy or just do anything there, see that they have an inventory?
Maybe they'll notice something's on sale and want to buy pants.
So how about?
I'm like this, how about we give everybody a free coupon for one free monocle.
And you go to the store and you cash that in, and then they'll have gone through the experience of knowing that the store is there and doing all that.
And Fred's like, wait, we're not selling any pants, and now you want to lose the revenue on monocles as well, what difference is this going to make?
Here's our question.
Are people who get a monocle more likely to buy pants?
Alright, let's do a scientific study once again.
We've got our players, let's cut them into a control group and an experimental group.
experimental group gets monocles, control group gets no monocle coupons.
We'll wait, we'll wait a couple weeks, and we'll see how many people from each group went on to buy pants at some point in the next couple weeks.
What's my hypothesis? I'm hypothesizing that people who got the monocles are more likely to buy pants that week.
And Fred's like, hang on a second, you're making an assumption here, and your assumption is that people who get monocles are more likely to buy pants.
But you've got no data for that, right? I could easily be making people less likely to buy pants, maybe I have filled their sartorial needs for the week.
So...
Whenever you're doing this kind of test, and you're saying, have I caused more of something?
Consider that you might have caused less of it.
That's exactly what the two-tailed test is about.
All right, so our hypothesis is that people in the experimental group buy pants at a different rate from the control group people.
And Fred's hypothesis is that it doesn't make a difference, and they're going to buy pants at the same rate.
All right.
Also, if we did this test 100 times, you'd get slightly different results every time.
So for this test, we also have the confidence intervals things and a little bit of uncertainty.
We'll work those out.
Got two options for this scenario.
One is the so-called relative risk test.
This is used in epidemiology.
It's the thing where they say, like, people who smoke are four times as likely to spontaneously combust.
The other is the difference of proportions.
And it's what you say when you say our market share has increased by 5 percentage points.
And you really mean it's gone from 20 to 25, as opposed to like 20 to 22.
Let's do the relative risk first.
Here's our data.
Eight people in the control group bought pants after a couple weeks.
19 people in the experimental group bought pants in a couple weeks.
Chart looks kind of like that.
Well, that's the number of people from the experimental group who bought pants.
It's about two and a half times, a little bit less, than the number of people in the control group who bought pants.
So the relative risk of somebody in the experimental group buying pants is about 2.4.
The confidence interval, once again from the macros that I've given you at the URL, comes out to about 1.08 to 5.25, so we know that monocle recipients are about 2.4 times more likely, with an uncertainty.
of somewhere between 1.1 and 5.25.
But Fred's definitely wrong, or at least he's only 3% likely to be right.
Let's do the difference of proportions.
So that's much simpler, we've got a 5% conversion ratio on the control group, 12% on the experimental group, 12 minus five is seven.
That's our number.
Once again, there's a confidence interval here that we have to calculate.
Macro for that is up on the page as well.
If you do the math this way, the interval comes out to about 0.8%, to about 13%.
Small sample size, huge uncertainty.
Which test do we use in this scenario?
Well, what question are we trying to answer?
In this case, we're trying to answer how much more cash are we getting out of people by giving them the coupons?
In this case, the absolute test is useful, right?
12 per 100 people in the experimental group buy pants, five in 100 people in the control group buy pants, so that's about 59 bucks per 100 players in the experimental, 25 bucks per 100 players in the control, so about a $34 difference.
or depending on that uncertainty somewhere between four and sixty four dollars.
You know if I went to Fred and said we could probably make thirty four bucks more per hundred players he'd be like, eh. But if I went to him and I said this idea makes people more than twice as likely to buy pants, sounds a little bit better. So my conclusion is you should use the relative risk test to convince your boss, your marketing department, your VP, and the absolute test to convince yourself.
All right, what if our distribution is kind of lopsided, like this?
What if we've got like an app store rating like this, right?
And rating people one to five, and you know app store ratings are always kind of bimodal because human beings are nonlinear and the difference in our heads between five stars and four stars is not the same as between four and three.
Let's think of an example.
All right, hypothetical games, working on a new puzzle.
We built the puzzle. We send it home with our friends and family for a weekend to play test it.
We've got some metrics in the app.
Data comes back after the weekend.
Here's how long everybody played our game before they got bored.
Most people didn't play it very much. A couple people played it a lot.
Look at this thing and say, I think I can probably do better than that.
Alright.
So, spend a week coding up a new version of the puzzle.
Fix a few things. Send it home with a totally different group of testers.
Data comes back like that.
I think it's an improvement.
But if you look at the actual averages of these two groups, the average time spent playing the orange puzzle is less.
It's 6, or 5.9.
And the average time spent playing the blue puzzle is higher, 6.8.
Why?
Because averages are very sensitive to outliers.
And I'm like, but Fred, 78% of the people who played the blue puzzle played it for less, a lower than average amount of time. And like, this, who is this? Is this your parents?
So the number we care about is the median, which is just the number where half the population is less than that and half the population is more than that.
In this case, the median time spent playing for the blue puzzle is two, which is like half the people played for less than two hours.
But in the case of the orange puzzle, the median is six, so half of the people played as much as six hours.
Now Fred, always the sourpuss, is telling me, well this is because we only tested it in like a dozen people.
And if you did this with a hundred players, the two puzzle playtimes would look more or less the same.
I'm like, no, we can math our way out of this one.
The test for this one is the Mann-Whitney test.
It is, once again, an Excel macro that I've linked to.
This one is not built into Excel, but there's this awesome guy who made a whole bunch of wizards that includes this one.
Here is our data.
On the left we've got the number of hours that people played Fred's version of the puzzle.
On the right we've got the number of hours that people played my version of the puzzle.
Pull the data into Excel.
Pop open the thing, this is the wizard that I'm giving you.
Pop open the non-parametric test, up comes the man Whitney.
Data comes out like this.
Chance of Fred being right is 3%.
Orange version of the puzzle is better.
Some cautions.
All this time we have been using 5% as the threshold for a false positive.
But you know what? 5% happens a lot, as you all probably noticed.
So you've always got to define your hypothesis before you do the test.
Make your bet with Fred before you run into it.
And don't go data mining.
Don't do the thing where you say we've got like a hundred different metrics in our players and let's run a hundred different t-tests to see if any of them correlate with console brand.
Do people on PlayStation play longer than Xbox? No. Okay.
Do people on PlayStation buy more than Xbox? No.
Do they kill more dragons? Whatever.
If you have a 5% chance of any one of these tests giving you a false positive, you are almost certain, if you've done that many, to be off the charts.
I mean, to definitely get a false positive.
especially do not do the thing where you've got a hundred variables and you say for every one of these hundreds Do they correlate with any of the other 99 variables because then you are absolutely Guaranteed to get a false positive and you are not doing science. You are running a novelty website Also, even if you have a real association between two things It doesn't necessarily mean that one thing is causing the other.
Right? We've all heard the thing, correlation is not causation.
It's not just that two things can be correlated without being linked.
It's that they can truly be linked, but not have a causal linkage.
For example, on correlated.org, 22 people who took the survey have a Pinterest account.
39% of them have a, sorry, 22% of them have a Pinterest account.
39% of the people who own cardigans have a Pinterest account.
Does cardigans cause having Pinterest accounts? Does Pinterest cause buying cardigans? Does the same people like both cardigans and Pinterest? Can't tell from this data. Let's do a lightning round. Put it all together.
Working on a VR game. Got a new camera scheme. Does it make our playtesters less nauseous than the other scheme? What are we measuring? If we are measuring whether testers were able to finish the game or not.
It's a binary outcome and a relative risk test, or difference of proportions.
What if we're measuring how long they're able to play before they need a break?
It's a continuous outcome, it's kind of bell-shaped, probably, so you can use a t-test for this one.
What if we've asked them to rate their nausea from one to five?
Well, if the ratings come in in the bell curve, we can use a t-test.
If the ratings come out like this, it's not a normal outcome, so we have to use the Mann-Whitney test.
What if we're making an educational game?
We've let kids play our game.
Can they math better after playing our game?
Well, if we are comparing before and after test scores on the same kids, like every kid after every kid before, It is a paired t-test.
If you were to compare like a control group and an experimental group, you would calculate the difference between before and after for every student from the control, the difference between before and after from every student for the experimental, and you would do an independent t-test on the differences.
We're making a game with a lot of PVP character classes.
Are they balanced?
Seek adult supervision.
Can't do it with the math I've shown you.
But you can do some further reading.
So as I mentioned, I've given you some tests that are good in certain specific scenarios, but with this moderate power comes moderate responsibility. Do not push your tests beyond what they are able to do. Here is a chart from a real scientific study I found. Basically some experimental psychologists asked people a bunch of questions like, do you enjoy bitter foods and drinks? And do you tend to be cynical? And then they correlated every one of these questions against every other one of these questions. So people who tend to be cynical tend to enjoy bitter foods and drinks.
headline the next week. That's good stuff. Use your skills for good and not stupid. Please. All right. Thanks to everybody who helped me out with this talk. Mark Cerini, of course. My long-suffering sponsor, Haldor. My boss, Mike Acton, who was the opposite of Fred, I just want to say right here.
and everybody who came to my rehearsals.
And extra special, special thanks to Dr. Garrison at the University of Washington, who literally taught me everything I know about statistics and without whom this talk would not have been even remotely possible.
And to my surprise, I even have time for Q&A.
Hi, I got a question.
Oh, so there you are.
So, when discussing rating nausea from 1 to 5, you said if the results look like a bell curve, use t-test, otherwise use that other one.
Yep.
It seems like it would be tempting to choose the test that gives better results.
That is an excellent point.
Yes, you should probably just use the Mann-Whitney test.
So scales like that are called Likert scales.
And depending on which statistician you ask, they'll say, if it's at least five categories, if it's at least 10 categories, you can treat it as continuous.
But if it's less than that, you have to treat it as if it were what's called ordinal, where you can't actually assign them to a continuous number.
5 is right at the cusp, so it's kind of a judgment call.
Now that you've mentioned that, I would say, yeah, you should probably just use the Mann-Whitney.
It's almost as good as the t-test in all scenarios anyway.
All right, thank you.
Any other questions?
All right, I'm gonna skip ahead into the Q&A slide.
So, what if...
your outcome is something that can't be measured as a continuous number.
What if it's something you can't put a score on, but you can just say, this thing is better than that thing.
Like, what if you can just rank things from best to worst?
Right? Now, you can't look at these things and say that the difference between undiscovered country and first contact is exactly the same as the difference between search for Spock and the motion picture.
You can just say, which of these things is better than which of the other things.
This test is what the Man Whitney was actually designed to do.
It's called an ordinal outcome because you can only order things.
You can't measure them continuously.
So in this case, our predictor is the movie even-numbered or odd-numbered.
And the outcome is, is it good or not?
And we can do the math, do the thing, pop up the numbers, do the thing.
Yeah, and it turns out actually the even-numbered movies aren't better.
Or at least to prove it, we'd need like 30 of them.
Yeah, it's time to bring in Riker.
All right, if there's no other questions, let's all get comfy.
