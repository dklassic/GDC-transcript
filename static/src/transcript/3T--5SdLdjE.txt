Hello, my name is Alina Hill.
This talk is called Make Your Game Run on the Quest and Look Pretty Too.
We'll be talking about art optimization on the Oculus Quest VR headset and how to make art assets which will look good and perform well.
We'll cover the basic do's and don'ts for asset creation and setup and how visual choices can impact performance.
So a bit about me and why I'm giving this talk.
I'm a 3D artist and art manager at the indie studio Phenomena, here in San Francisco.
I spend most of my time creating art assets, breaking down art assets, and setting up assets in game.
I worked on the title Luna, shown here, which was my first VR title.
Since then, I've worked on a few more AR VR projects, including two VR projects developed exclusively for the Quest.
Our first Quest project was in development before the device was released, and the latest project is releasing in about a month or two.
Unfortunately I can't show stills from either of these projects today, so I have to bear with some of my art tests.
So why am I giving this talk?
Well, even though I had already shipped a VR game, when we first got our Quest devices, I was honestly pretty mystified.
I'm not a performance engineer, so there's a lot I don't know about how rendering works, but I still needed to make a lot of decisions about how to make art assets for this device.
It was hard for me to know what art assets were going to be a problem and what was not.
Should I treat it like a mobile phone or like a VR game?
Or a VR game running on min-spec?
And how would all of this affect the overall look of the game?
I wasn't sure.
There were some guides and documentation that got me started.
And there are fortunately more now.
But the guides and the documentation were pretty technical.
What I needed was more practical, less here's how the hardware works, guidelines geared towards art production and direction that covered a wide range of asset types, tools, techniques, and their limitations.
So that's what I'm trying to put together here, a practical guide for those of you who are new to the Quest, but who really want to make your game look beautiful on this cool new device.
Speaking of the Quest, let's start there. How cool is it that we have a mobile VR headset now?
Honestly, I had a lot of fun after the Quest launched sharing games with people in unexpected places.
Did you know it still works on a train?
But what I love about this device is that it really changes who can access VR.
Most people don't get sick from it, since the 6th off is pretty good, but also you don't need a gaming PC to run a VR app.
Those two things alone just blow open the doors on who can access VR.
In a lot of the talks I've seen, this is normally where the speaker would explain the Quest's cool new tile renderer and how it's able to do what it does because it changes the buffer fragment meatmorp or whatever, but I make art, so if that interests you, please learn about it from an engineer.
What you need to know, or what I needed to know is, what are its restrictions as a mobile device when it comes to rendering?
What are its VR restrictions?
And what are the special Quest-only restrictions?
Because restrictions aren't bad, they just change the artistic decisions we make, and so we need to know what they are so we can learn how to work with them.
I'll go over a variety of these restrictions or performance guidelines, whatever you want to call them, but the first one I want to discuss is the one I had the least familiarity with when I started my first Quest project.
Let's talk about draw calls.
I sometimes go to schools and tell classrooms full of kids what my job is and how cool and creative it is.
They always seem to ask, it's art, but you use computers.
So do you do a lot of math?
I usually lie and say no, because while I do some math every day, it's usually pretty simple, and anyone who can learn to use a 3D modeling program can probably learn to type 360 divided by whatever into a calculator.
So now we have to do some math, but again, it's simple.
Draw calls are how many times in every frame the computer needs to draw an object.
These drawings stack up into layers and create the final frame.
A PC can have several hundred draw calls per frame and not slow down, but the Quest really needs you to keep that number under about 120.
And artists actually have the biggest impact on this number, so we need to make sure we don't add unnecessary draw calls while we make art for the game, because 120 is a very easy number to hit.
Okay, time for the math.
First, each separate object is a draw call.
So if you have, for example, a character on a plane with a skybox, then you have three draw calls, one for each character, plane, skybox.
Okay, that doesn't seem too bad.
Maybe make sure you merge environment pieces as much as possible, right?
Well.
Second, each separate material on an object is a draw call.
Okay, that character actually has four different materials applied.
Well, that's a draw call per material, so we're at 6.
4 on the character, plus 1 skybox, plus 1 ground plane.
OK, the math is easy, but as a 3D artist, I need to use different materials and objects a lot.
Counting draw calls isn't something we're used to needing to take into consideration.
So just by keeping this math in mind when setting up models and textures, you can keep this number down.
Try to keep each separate object, whether static or skin, to a single material, if possible.
Atlas textures and merge meshes in environments where you can.
Cool, so we can make better choices when setting up materials and models, but we can't keep track of the whole game in our head.
So we need to be able to look at what the actual number of draw calls is during runtime to make sure we're on target.
So let's talk tools.
There are a couple tools I use a lot to keep an eye on performance or try to figure out what's affecting performance.
The first of these is RenderDoc.
RenderDoc connects with your quest and allows you to capture single frames from your game and break them down into calls and other information.
You can use it to find out if something is taking more draw calls than you anticipated, or something is rendering on the wrong layer, or one time I was able to find there was a non-optimized light that was casting shadows everywhere and I had overlooked it.
It's just a really great insight into the nuts and bolts of the images being drawn.
Especially because draw calls can be a big performance hit on this device, it's very useful to be able to look over that information, even as an artist.
So here's a screenshot from RenderDoc.
I've highlighted here where the draw calls are listed, and you can actually tab through each draw call and see what's rendering.
You can look at the textures being used, and the basic mesh that's being drawn as well.
there's a ton more information you can get from RenderDoc, but just being able to inspect the draw calls and see what's being drawn has been a lifesaver for me.
Second of all, frame rate.
This talk could also be called how to make your game run at frame rate on the Quest, because frame rate or frames per second is probably the most important performance indicator we care about.
In VR, you need at least 60, preferably 72 frames per second.
A little frame lag on your phone or console game, and it's annoying, but frame lag in VR can make your player feel sick or even fall over.
If you make the most beautiful VR game, but it runs at 30 frames per second, no one will play it and everyone who tries will get sick.
So you can use any FPS counter that you want, but Oculus puts out a tool called OVRmetrics, which basically outputs performance information on the screen.
Most of these numbers don't mean much to me, but if that frames per second counter is at 72, I know I'm good. If it's below, say, 65, I should probably fix something. And sure, it might be something non-art related, which is tanking your frame rate, but let's be honest, it's probably the art. In this image, every option is displayed, which looks insane.
I usually just leave on the frames per second graph and counter, maybe a few other things.
I couldn't get a screenshot of how I set up mine though, because it doesn't seem to show up in regularly captured photos or in RenderDoc.
Additionally, if you're a total nerd, you can just leave OVR metrics on on your quest and see what the framerate of your favorite games is managing to achieve.
Sorry developers.
Okay, really quickly let's go over some basics for setting up mobile VR rendering.
I don't want to dedicate a lot of time to these, but I think it's worth mentioning to get started.
There are forward rendering, depth pass rendering, single pass stereo, anti-aliasing, and fixed foveated rendering.
Forward rendering.
You'll need to use forward rendering.
The Quest is a mobile device, and that's the rule for mobile devices right now.
Make sure you find out what forward rendering entails and means as far as art assets.
The biggest bummer here is that decals don't work in forward rendering mode, so you'll actually have to hide your texture scenes properly.
In Unity, you can use the lightweight render pipeline, which is part of their new Scriptable Render Pipeline.
It's set up for forward rendering and a few other things which are really helpful in VR.
I found this to be really cool and useful without even using the full power of the new Scriptable Render Pipeline.
And in real, if you're not doing some magic like pulling a different branch of the engine, the settings for forward rendering are here in the project settings.
Depth rendering, there's no depth rendering.
The hardware just doesn't do this, I think.
So, no fun shaders that use depth info.
This includes soft particles.
But don't worry, this doesn't mean you can't have some kind of fog.
It's just not as fancy. We'll revisit fog though later.
Single pass stereo is a technique which saves you performance by basically drawing both eyes as one image rather than drawing one eye and then drawing the other eye.
This is on by default in Unity's lightweight render pipeline.
And in Unreal, it's here in these settings.
You need it, you want it. It's anti-aliasing.
Without anti-aliasing, things look ugly really fast.
In forward rendering mode, you have multi-sample anti-aliasing.
And four times is good if your performance can handle it.
Eight times is a bit of overkill, depending on your game's style, I think.
But, use what you can get away with.
It's in the project settings in Unreal.
And in your lightweight render pipeline asset in Unity.
Alright, one last thing for general rendering settings is fixed foveated rendering.
This can come in handy, especially when you're stretching your rendering budget as much as possible.
What this does is it lowers the pixel resolution on the edges of the screen so they don't take as much time to render that part of the image.
This works because the output image in VR is a little warped or fish-eyed to accommodate the distortion of the lens.
So there's a little resolution wiggle room in those areas that are being squished at the edges.
And this isn't necessarily a trade-off. We perceive less detailed information in our periphery, so the player may not even notice it too much, and you could potentially save quite a bit of performance.
Although, if done somewhat aggressively, fixed-foveated rendering can creep further into the middle of the render, to the point where you can start to see the drop-off in resolution pretty obviously.
These images show what part of the screen is affected and how much.
There's a left and right eye.
Low fixed bovine rendering only affects a little part of the screen and doesn't drop the resolution that much.
But turned up high, more of the screen is affected and the drop off in resolution is a lot stronger.
You can see this a little bit more obviously in this render of the Oculus Home.
Here I've turned it on low and high for comparison.
You can also turn it off altogether, of course.
Particularly on the couch and on the dome cross beams, you can see the pixelation that occurs in the higher contrast areas as the resolution drops.
So I've played some games where it was obviously set up pretty high, and I really only noticed it when standing still.
Anytime I actively played and was looking around, it became less noticeable.
But in others I found it could be really distracting, even if it was on a more moderate setting, if there was a really high contrast values in that part of the screen.
Things such as stars become really noisy when the resolution drops.
That said, if you use fixed-boviated rendering, start subtle so there's not a hard line of resolution drop in the player's view, and be aware of the noise it might make in high-contrast areas.
Now that we've covered the basics of performance, let's really jump into the artistic side of all this.
First, lights and shadows.
Lights are already a known source of performance issues, so there's no surprise there.
There's a reason we have things like lightmap baking, light and shadow distance culling, etc.
Lights are expensive.
How to light your game is also one of your biggest artistic choices, so how do you balance what you want to do with what you have time for?
Because with enough time and effort, you can have a pretty nicely lit game, in the traditional PBR sense.
Not a next-gen console or anything, but pretty darn good.
Some games have set up really great lighting, but the catch is it's probably all baked down.
If you can bake it, you can probably have it.
Which sounds like an easy answer, and there's absolutely nothing wrong with doing that.
It's a very old and well-used process, and it can yield really great results.
But it takes extra time and effort to make baked lighting look good.
And it's important to think about whether your project's timeline can afford the amount of time it takes to do light mapping.
If you only have one scene to light, probably not a big deal to light map it.
But if you have 12, well, you'd better have a pretty big team then.
So can you have non-baked lights?
Yes, you can have one dynamic or stationary light.
They're much more expensive at runtime, because they need to update every frame in order to change how they affect moving objects, such as the player's hands.
Most mobile devices can't handle too many dynamic lights, but one is enough for your needs if you're careful, especially if you're baking down more complex lighting.
Also, why waste computation?
Even if you can get away with one directional light in the whole scene and still have it look okay, still think about baking out the static areas so you have wiggle room for other things.
What about dynamic shadows?
Yes, but dynamic shadows probably have the worst performance impact out of anything light and shadow related.
So really think about how much you want them and where.
You can only have one shadow casting light at a time, so if it would look odd or you'd have to spend time you don't have orchestrating which light is casting shadows when, then think about whether they're worth having at all.
I would personally recommend just leaving them out, unless everything else is running really light.
Then you can get away with some object casting hard shadows.
No soft shadows.
For example, here Bonfire and Half and Half both use them, but both these games are only rendering a few characters at a time, and have a pretty limited environment.
If your game looks like one of these, then sure.
The Half and Half example is a closeup of the player's shadow.
They look fine as hard shadows, and they're actually a lot of fun to play with.
If you're going for something more realistic, or have a lot more things to render, then save yourself the headache and just try and do without them.
The quest is probably not the best platform for your shadow puppet puzzler.
What about unlit looks, or looks with simplified lighting?
If these kinds of styles could work for your game's aesthetic, then I'd recommend using them.
For example, Virtual Virtual Reality looks great on the quest.
It's a go-to example for me because I've played it on both the Rift and the quest, and I really don't notice a drop in the art quality.
On the quest, it seems there aren't any dynamic shadows in the game.
The characters and the player-moved objects don't cast shadows, but static meshes have shadows, so I've assumed they've been baked in.
It almost fools you into thinking that more things have shadows than they do, especially since the objects themselves are cell-shaded.
so the illusion is pretty complete.
Another example is Nog, which technically isn't a game that's out on the quest, but it is out on the go, and I've always thought it was a great use of the unlit style.
It's bright and interesting all on its own, and since this game wasn't developed for VR initially, I assume it's just a style choice.
So if a bold graphic style works, then use it.
Also, styles that use faked lighting or light and shadows that are painted in can look really good and save a lot on performance.
Totally Unlit is going to be the easiest to make performant, and PBR is going to be the most difficult and time consuming.
Bear in mind your project's timeline and resources and try to find a balance of what you can do versus what you really need for the aesthetic goal of your game.
Next up is shaders, materials, and textures.
So here, you mostly need to remember that mobile devices need things to run a little lighter.
I mentioned the lightweight render pipeline earlier, and things like that will get you far, but you also have to keep account of what's going into your materials.
Textures.
Don't go crazy on texture resolutions. This isn't one of those next-gen consoles running 8 4k textures per character. It's a mobile device and crunching all those pixels has a cost.
In addition to texture size, be conservative with the number of textures needed for each material.
Use what you really need and what can really contributes to the look, because they'll add up over time.
What this means artistically is that some cool PBR looks that require a ton of textures at high resolutions aren't going to perform well.
Maybe if your game is limited in the number of environments and props needing textures, you can push for that look, but generally you want a look that requires less maps overall.
You can still texture everything in your game while keeping things performant.
But be mindful of what you're doing.
Use some tiling textures, try to reuse textures as much as possible.
Also, don't forget to pay attention to your MIP levels and filtering, or your beautiful textures will look like pixel-y garbage.
Shaders.
Obviously, shaders can have some impact.
Shader complexity is measured in the number of instructions, and you can preview shader complexity in some engines to kind of keep an eye on how complex things are getting overall.
On the quest, it's good to ask if you can get away with a lighter shader and test complex ones that you make.
It's also important to keep an eye on the number of textures needed in a shader and how much of the screen space the texture will fill.
On my first quest project, we spent a couple days trying to figure out what was impacting performance on one scene before we discovered it was the water shader.
The shader didn't seem that complex, but since it occupied a lot of the screen, and extended way into the distance, it ended up being really expensive to draw all the textures over and over.
A quick redesign of its look really increased performance in that scene a lot.
That said, don't be afraid of shaders at all.
Some instructions are really lightweight, and shaders are a kind of beautiful math magic that you can use to make really interesting effects and styles in your game.
On my most recent Quest project, I opted to use a style of texturing which uses gradient swatches instead of unique textures.
In this style, you add a light and shadow gradient to the color swatch, and then adjust the model's UV placement so that it sort of looks lit.
Initially, I was going to do the gradients in texture and bake them down in Photoshop, but I experimented with setting that up in the shader so that I could adjust the gradients and what their color was room to room and create a slightly different lighting look.
So while the shader is more complex than a non-lit shader, it still has less performance impact than a lit shader, and it gives me a lot more control over the look I'm aiming for.
Materials.
One thing, which is quest specific. There's a slight performance hit every time a draw calls switches, materials, or switches shaders.
So if it's drawing an object using one material and then goes to the next object it's using a different material or different shader, there's a performance hit. So there's a big incentive to atlas as much as you can and try to reduce the number of unique materials needed.
So maybe you have four animating props in a scene, for example.
which are similar looking, but all different rigs, and therefore, different draw calls.
You still get better performance if those four skin meshes are all using the same material, even though they're still separate draw calls.
It might also be Advent Changes to limit the number of shaders being used in a scene for that same reason.
Splitting out functionality that you don't always need in one shader to two sounds smart, but if you're using both quite often, maybe just see how it performs if you just use the more complex one for everything.
Again, it really depends on how many objects and how complex of a shader.
As a 3D artist, my first question when starting to develop for the Quest was, of course, what's my poly count?
The initial numbers I heard from Oculus were pretty understandable, given that it's a mobile device.
I've made mobile games before, but 3D artists, we always want more polys, so I couldn't just accept the number. I had to be sure. So the poly limit given by Oculus is 50 to 100k ferts, which is okay. I've made do with less, but I did some tests. Okay, so just because some static mesh tests...
with no physics or lights or gameplay, can handle half a million polys, doesn't mean that's true of the final game scenario.
But I have consistently overshot Oculus' recommended poly count in almost every scene I've made, and so long as the other systems are kept light, I can almost double their recommended numbers.
In fact, on my most recent project, I opted for a style which demands more mid-poly models.
but has really lightweight shaders and textures, and uses no lights at all.
So I can get away with needing the extra polys in that style.
It did some tests and found out that most of my scenes in that project averaged about 150k.
I did start hitting some ceilings, but only when I was being really lazy with my poly count.
And again, everything adds up.
So if you really want a PBR look with lights and some dynamic shadows, maybe you don't have the wiggle room in your poly count and you should aim for the target that Oculus gives.
I think that I was only able to really push this number due to the lack of lighting and textures in my project style.
So what about the classic methods of lowering poly count?
They're still there and they still work, so that's good.
But they have a couple of caveats as well.
Instancing.
Yeah, it can really help with poly count.
But remember our draw calls?
Only certain kinds of instances actually draw on one call.
And some types still render each instance on a different call.
So you might save polys, but add draw calls.
And since draw calls are more of an impact hit, be cautious with instancing, and make sure that it's also batching the draw calls together.
We'll discuss batching more in a moment.
LODs are also a great way to keep your polycount in check.
I would particularly recommend that you use LODs for foliage and other things which repeat a lot.
Depending on which engine you're using, certain instances will also work with LODs.
So you can kind of get away with two for one, combining them to one draw call and still using LODs.
Okay, so what can you do artistically when working with lower polycount?
First of all, remember that all games involve some amount of poly monitoring.
Even next-gen games don't just throw ZBrush sculpts into the engine.
So even if you don't want a low-poly aesthetic, you're already working within a normal pipeline, you just need to be a little more cautious than on a traditional platform and keep an eye on how your attempts to reduce poly count may affect draw calls.
Second, think about embracing a low-poly look.
I actually really love low-poly styles and generally model low poly stuff, it's not everyone's cup of tea and certainly doesn't work for every game, but it can work for every type of game. And even if a low poly style is not what you want for your game, looking at how visual problems are solved with less polys can help you overcome poly count problems that you're dealing with. This is a difficult thing to show examples of because it's one of the places where artistic creativity and technical implementation really intersect without many rules.
But as an example, foliage can be difficult to make performant, especially if you're trying to use a lot of cards.
So looking at foliage in low poly games might help you find a way to create foliage which still suits your game style but doesn't rely on all those cards.
You can still hit the visual target you want by learning to construct an asset in a different way.
Just look at these games.
Regardless of the final poly count, which I don't know because I didn't make these, These games lean into a certain low poly aesthetic, such as faceting on the enemies in Super Hot, or the chunky shapes in Job Simulator, or the simplified graphic shapes in Virtual Virtual Reality. They all look great and distinct from one another. So even if you're a little cautious, poly count is probably not going to be where you hit performance walls. I really only see this being an issue for pretty large scale games with lots of large environments and lots of foliage.
But that said, looking to low poly styles to solve various visual problems may help your game look more polished than simply trying to use traditional high to low pipeline and just forcing it even lower.
All right, back to batching.
Speaking of LODs and instances and poly counts, I'm gonna jump back into our draw call math and discuss matching.
I found batching to be really useful and I feel it's important to cover the batching options.
So.
Batching is when the engine takes normally separate objects and draws them all in the same call.
As a quick example, this is what happens to all the quads that are generated in a dust mode particle system, so that each particle is in a draw call.
So that's batching, but you can also batch with regular static meshes in the game.
Which sounds great, but batching isn't going to magically solve all your draw call needs.
They come with some strict rules too, but knowing what's going on with batching can help you set things up a little better.
I don't want to get too into the weeds because there are different methods and rules depending on which engine you use, but what's important is figuring out whether batching is costing more performance to compute than just adding some draw calls and merging meshes.
It's something that which may be on a case-by-case basis, depending on what kind of lighting you're using or other things which may affect performance.
But at least it's good to know what tools you have.
There are two types of batching in Unity, dynamic batching and static batching.
Unity will dynamically batch objects under 300 verts, which use the same mesh and material.
There are a few more rules to this, but the point is that it largely happens without you having to set much up.
The drawback to dynamic batching is that it comes with some computation per object batched.
So if a ton of objects are being batched together, it might actually not save you much performance.
But for a smaller number of objects, it's actually pretty great.
Then there's static batching.
Marking an object as static in Unity tells Unity that the object doesn't move, and therefore can be batched.
This static batching works with higher poly objects than the dynamic batching.
Basically what it does is it merges meshes together into one object and then draws that using separate calls for different materials.
This means it comes with a higher memory overhead since it's saving out that merged mesh, so again you don't get it for free, but it is a great way to consolidate your scenes after set dressing.
There are a couple methods of batching in Unreal as well.
The main one is instant static meshes.
Unreal already deals with instancing meshes pretty well, but converting some static meshes into a proper instant static mesh will combine them into one call.
However, you can end up with this kind of situation, where I have really combined all my draw calls, but I've also far exceeded my poly count, and nothing is getting called, the LODs aren't working, so my advice is to go straight to a different type of instance when working on the quest from Unreal.
and that is the hierarchical instance static mesh.
These are my favorite.
You kind of get everything here, LODs, batching meshes into one call, instance calling.
They're kind of amazing.
The only drawback to hierarchical instance static meshes is just working with them since you can't comfortably move the instances around.
However, it is pretty easy to create a tool which runs through all the instances in the scene and converts them to a hierarchical instance static mesh.
If you really need a lot of instances of something, hierarchical and synthetic meshes are really the way to go in Unreal.
I've used these a lot, especially with natural elements like rocks, which get duplicated out quite a bit.
And of course, not all polys are the same. Skin meshes are more expensive because they're being deformed, and I don't think there's really any benchmark for how many skin meshes you can have per scene, or how many polys can be skinned versus static.
It's going to depend on your game's overall performance, as well as the rig itself.
Higher poly meshes, or skin meshes with a higher number of influences per vertex, are going to be heavier.
The number of vertex influences can really hit performance.
This isn't true for just the quest, it's true for most platforms, particularly mobile platforms.
So when painting weights on skin meshes, I try to stick to 2-3 joints influencing each vert as much as possible.
I will do three for things that need to be really smooth, but I would recommend not going above four.
And balance the number of joint influences you have in your rigs against how many animating meshes you need at any given point.
This is why the quality levels can adjust how many bones influence a vertex.
It's just a big impact on performance.
If you're still unsure about how many animating meshes you can use in a scene, run some tests.
Use a scene with a static polycount that's a good average for your game and throw some rough rigs in.
I just add multiples of 5 to 10 until I start to see performance drops.
Too many animating characters can be a quick way to tank your framerate, but at the same time you want to use as much as you can because animation really brings games to life.
Bear in mind though that animation controllers with a lot of layers can also impact performance, so if you're planning some really intense animation controllers, be sure to test those as well.
Issues around transparency is one of the aspects of the quest that's not obvious.
Transparency isn't a big issue on most platforms, probably because of the differences in the hardware.
But transparent objects need to be handled with a lot of care when developing for the quest.
The problem seems to stem from overdraw, which is when multiple objects or multiple polys are being drawn on top of one another.
This already affects performance because you're taking time to draw things which maybe aren't even seen.
But overdraw is basically unavoidable to some extent in all games.
What becomes a problem on the quest is when one object is transparent, and so both objects will affect the final pixel in the image.
In this example here, I've put the wall in front of several other objects, and you can see that those objects are still being drawn, but you can't actually see them.
That's an example of overdraw.
In transparency, you're drawing all those objects, but multiple of those objects are affecting the final outcome.
Small objects like dust motes or text don't have a big impact because they don't overlap each other a lot and don't occupy a lot of screen space.
But larger transparent objects like large cloud or fog cards, effect sprites, or even head bubble transition fades can be a real performance issue, especially if you're struggling to keep your frame rate up.
My best advice to you is to avoid using transparency techniques that can occupy a large part of the screen, especially if they're going to layer a lot.
You can use transparency here and there, but you need to make sure to keep it as small and infrequent as possible, and test it on the quest whenever you're going to add something like this.
So because transparency is not normally a big issue for game art, it gets a little tricky to work around not using it, so here are a few quick tips.
Don't use big soft cloud or fog cards.
For soft particles you need depth pass rendering anyway, which we don't have on mobile, so they'll just clip through hard meshes and they won't look great anyway.
I find sometimes that really soft alpha cards don't also look as good in build as they do in engine, so be sure to test your effects on the headset.
The old style of fog, linear fog or vertex fog, performs really well on the Quest.
I really pushed this and made it do some really interesting things.
So as long as you have enough vertices to support it, it can work really well.
Try to avoid using plant cards or foliage cards.
There are other ways to make plants than just cards.
Explore some styles that don't use as many cards and maybe rely on polys for the plant shapes.
For sprites, use meshes fitted to the image to reduce as much overdraw as possible.
Engines and sprite support software already makes this really easy to do, so there's no reason to render everything on big quads, especially since you're not as confined on the polys.
Also, you can pick a more graphic look for effects and use meshes instead of transparent cards. Here on the right I've made some soap bubbles, and rather than using an alpha for transparency, I've kept the style really graphic.
These are mesh renders. They still read clearly, but I don't have to worry about performance as much for these.
Also, there's a setting in Particle Systems called Max Particle Size, which is really helpful.
It'll prevent particles from taking up too much screen space, so that way your frame rate won't drop just because some dust moat got really close to the camera.
And using the full screen mesh fade for transitions is easy, but can also affect the frame rate.
And if you're already pushing performance, maybe try some different transition styles.
Or build it into the shader.
And if you do use a mesh for transitions, make sure to disable the object when it's fully transparent.
It should only be visible during the transition.
So if you've developed for VR at all, you know that post-processing is already somewhat problematic, and not just for performance reasons.
Some post-effects just don't translate well into VR.
largely due to the differences between the images shown to each eye.
However, there are plenty of things like bloom, color correction, or color grading, which are really helpful. On the Quest, however, the tile renderer makes any traditional post processing really slow. Like, really slow.
Because that post means after all the draw calls are done, and sending the image back through to get processed again is really a bad idea on this hardware.
So while it's one of the biggest performance disappointments for art on this platform, it largely just means that the traditional or built-in post-processing tools are not going to cut it. You have to implement what you need in other ways. Color correction, for example, can be done before the image is complete. You can add color correction into the shader and adjust it from there, even scene to scene using global parameters.
Cards can be used in places where you really, really want some bloom, since you can't get a whole image-style bloom pass.
But the thing to keep in mind with post-processing is that it's just a tool to achieve a visual goal.
If you can't use the tool you know, it doesn't mean that you give up on the work that you needed it to do.
Assess the effect you wanted, and figure out other ways to get those results visually.
So I've talked a lot about testing things to figure out what could be affecting your frame rate, or before committing yourself to a new technique or asset style all over your game.
But what makes a good test?
To find out what's affecting performance in a scene, I start by eliminating or changing the big offenders, and making a build with those changes.
For example, I might disable or change the lighting scenario, or try removing a complex shader.
Sometimes I'll replace high poly meshes with lower poly ones just to see if reducing the poly count helps.
I ran this test one time on a very frustrating scene where I replaced every mesh with a cube just to make sure that the poly count wasn't the problem.
Version control is very important here because you can revert all this.
This is the same garden scene that I've been using through the whole talk, but I've just selected every mesh filter and replaced it with a cube.
Obviously the polycount is going to be a lot lower, so if I still had problems at this point, I can eliminate a couple things.
The best thing you can do to keep performance in check on the quest is to test often during development.
That way, if the framerate does drop suddenly, you'll kind of know what's new in the scene and therefore most likely the cause.
There are things to be cautious about while testing, though, or you'll end up with misleading information.
So I'm going to do this example.
Here I've put some fog cards into a really lightweight scene, mainly to show how bad the clipping looks, but there's no lights, hardly any textures, hardly any geo or other effects in the scene, so of course it's still running at 72 frames per second, nothing else is going on.
The scene isn't representative of a finished scene in the game.
Unless it is, well then yeah, it is this.
The important thing is that you try to make your test scenes have the same complexity, so that you're using a reasonably similar number of draw calls, or using the same lighting complexity.
If your test scene is really lightweight to render, then anything you test in there may behave differently when added into a more completed scene in your game.
So, how do you translate all this information into decisions about how your game looks from the beginning?
Well, most importantly, know your budget.
How big is your game?
How many people are making this game?
What is their expertise?
What does your timeline look like?
If the answer to these questions is you have a lot of people and plenty of time, then try to make your game look like whatever you want.
You can afford to spend time to explore and finesse the performance.
If the answer sounds like most games, you have a few people and a less than ideal amount of time.
Then aim for a style or make choices within your desired style, which eliminate problems and overhead.
On my most recent quest project, we had a pretty short production timeline.
I had done this piece, which you've seen throughout the talk, as a test of using the gradient swatches instead of traditional texture maps, before I even had the project get started.
Look, I didn't even use the whole texture map.
But for this really short project, I opted to use this method along with an unlit shader.
And I saved a lot of time not having to worry about creating textures, managing texel density, and adjusting lights.
And I used that time in other places like effects and animation and shaders.
So, don't feel like this.
Form an art style around your time and resources.
and then you can focus your time on making your art look good instead of focusing on making it perform.
And while the two are never inseparable, spending time making your game look polished is always better than trying to squeeze in new techniques that you don't have time to optimize.
So in summary, test often, and keep an eye on your draw calls.
Be cautious with lighting and shadows, and try to set those up smart in the beginning so that you can save yourself trouble down the line.
You'll need to get creative with some things like post-processing and transparency, since a lot of the traditional methods can affect performance if you're not careful.
But generally, try to have fun and make what you want, because you can achieve some really great art on the quest by just applying a little knowledge of how it works.
Thank you for listening to this talk.
If you would like to contact me or check out more of my art, you can see my website or catch me on Twitter.
I'm Linus Teapot or Alina Hale on most platforms, so I'm pretty easy to find.
Thank you again for listening.
I hope this was really helpful, and I hope this helps you make some more great VR games.
