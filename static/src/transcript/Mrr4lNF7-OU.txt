Okay, looks like it's time for me to start. Just a reminder, if you could shut off your cell phones and remember to fill out the evaluation forms after we're over. Can everybody hear me in the back? Good, good. Thank you. All right. My name is Paul Tozer. I'm here to talk about the Game Outcomes Project and what it taught us about the science of creating effective teams and how teamwork, leadership and culture drive outcomes.
Okay. My surface pro 4 has hung. Apparently. Oh, there we go.
Got something. Okay. So, I'm going to go ahead and do this.
Quick introduction, I'm the owner and general manager of Mothership Entertainment in Austin, a small studio building an unannounced science fiction strategy game.
I also earned an MSC in technology management from University of Pennsylvania and the Wharton School of Business.
And the only reason that's relevant is because the Wharton School of Business inspired the Game Outcomes Project.
Not a consultant.
I'm not trying to sell you anything.
I'm a 22-year industry veteran motivated by intellectual curiosity.
I'd like to take the opportunity to introduce the Game Outcomes Project team.
We are an independent, industry-academic partnership.
In the upper left, Dr. Karen Vureau of McEwen University was our enormous help with statistics.
Also, Juliana Pillimer of the Wharton School of Business.
She is a PhD student.
She helped us a lot with our question design.
Our industry members included Eric Byron, David Wegbright, Jinghua Yang, also known as Z, Ben Weber, Lucian Parsons, and Endark Tang, who translated all of our articles into Chinese.
I'd also like to thank those of you in the audience who participated in the study.
It would not have been possible without you.
A couple of quick notes, I'm going to be doing the starting with the survey design and notes on how we design the survey.
Going to talk about non-cultural factors.
Going to talk a little bit about crunch.
Going to talk about culture.
Going to very briefly discuss non-correlated factors, and then draw some conclusions from that.
So in the fall of 2014.
our team designed a survey.
We decided to ask game developers a bunch of questions about their most recently finished game project.
This included 116 questions about culture, followed by four questions about the project's outcome.
We thought, OK, there have to be at least some factors in this list about team culture that are going to correlate with success or failure.
And if we can get a couple hundred responses and we can correlate them and see which of the responses are positively and negatively correlated with different kinds of outcomes, we can learn a lot from that.
So this was a first of its kind correlational study that correlated a development team's internal culture with their actual results.
We received 273 responses from game projects that had neither been canceled nor abandoned during development.
And based on that, we did a lot of data analysis and wrote up a series of five articles on GammaCitra.
And for those of you who have read the GammaCitra articles, I want to assure you that a lot of the data that you're going to see today is different in its form, if not the conclusions, from what was in the five GammaCitra articles.
Our hypothesis was that different teams have different cultures.
And.
We also hypothesized that chance always plays a role in game development, but we felt certain that outcomes have to be strongly influenced by their team culture in the aggregate.
And we certainly found that the first one is undeniably true, and we believe strongly the second one is true as well.
And we'll talk about why we feel that way a little later on.
So we wanted to know, is this true?
And if so, which cultural factors most strongly influence the odds of success, and what can we learn from that?
We had an optional question at the end asking game developers what project they were actually referring to.
I'm not going to read through this whole list.
Only fewer than 5% of respondents actually told us which game they were referring to.
But based on that, we know that our 270 responses included, at the very least, these games.
Survey design.
A few disclaimers before we start.
This study was not an academic paper, so it was not peer reviewed.
It's an ex post facto analysis.
In other words, it's based on a survey asking people about projects they recently completed.
In an ideal world, of course, we'd have a time machine.
And we could do a little A-B testing, go back in time, find the same development team, and say, OK, same people, same time, working on the exact same game project.
Now I want you to make a slightly different decision.
during development.
I want you to use Scrum instead of Agile, or Waterfall instead of Scrum, or I want you to decide not to crunch instead of crunching.
How does that affect your outcome?
The reality is, of course, we have no way of doing that.
There is no way to get exactly the same team to do the same thing twice, because they will have learned the first time going through.
So in the real world, there's no way to run that experiment.
So it's subject to cognitive biases.
I might look back on a project with a disastrous ending and be bitter about that, and that might affect my survey response.
Finally, correlation isn't causation.
Our study gave us a ton of correlations we can look at, but we can't technically prove that any of these correlations are causal factors by themselves.
And I'll discuss correlation and causation a little bit later in the talk.
A few quick notes about our survey design and the steps we took to minimize bias.
We randomized the order of the cultural questions in SurveyMonkey to remove any influence that a single ordering might have had on the way the questions were answered.
We had a lot of redundant questions where we'd ask the same question twice with redundant tones.
We would ask, did the team do X?
In addition to asking, did the team avoid doing X?
And it was very interesting because we found there was an almost perfect inverse correlation between those answers.
So...
The fact of the matter is those questions were kind of redundant.
We could have cut down on the number of questions just by choosing one or the other.
But it was a nice reinforcement that our questions were being answered honestly and accurately.
Finally, we asked our four questions about outcomes at the very end of the study because we didn't want to bias anyone about what we were actually looking for when asking the culture questions.
Let's talk about what we mean by outcomes.
Obviously, there's no single definition of what makes a good outcome.
Different game projects have different goals and different definitions of success.
If you're working on a AAA project, you probably care more than anything else about your return on investment.
If you're working on an indie project, maybe you care more about critical acclaim.
If it's your first time making a video game, maybe you just care about getting it done.
Maybe you're trying to make a political statement with your game.
Maybe you're trying to kill fascists like Woody Guthrie's guitar or something like that, and that's fine.
So in that case, maybe you care more about your team's internal satisfaction with meeting your project goals.
So our team came up with a list of four factors that we felt most developers would agree generally describe positive outcomes.
Our feeling was that most game developers would say, these are the factors that generally describe success.
So we asked the following four questions.
I'm not going to read all these in the interest of time.
So we asked things like, to the best of your knowledge, what was the game's financial return on investment or ROI?
In other words, what kind of profit or loss did the company developing the game take as a result of publication?
And we'd ask this on a seven point scale.
And all four of these questions were on a six or seven point scale.
So we were able to take those four questions, convert them into normalized values between 0 and 1.
Then multiply each of those by 20, well, I'll get to that a little later.
So we felt it made sense to combine all four of those into an aggregate outcome score.
Our view was that a truly successful project was one that had a good ROI, was completed on time, received high critical acclaim, and that the team felt achieved its goals.
We tried a bunch of different ways of building that aggregate outcome score.
We tried one based on probability theory, multiplying the outcome scores together.
We tried one based on weighted sums and tuning the optimal weights.
In the end, we found that the thing that worked best and correlated the best with everything else in the study was simply adding those four scores together.
So we've got four values between 0 and 1.
Just add them together.
And then that number between 0 and 4.
correlated better than anything else we came up with.
So we stuck with that.
And then we multiplied that value by 25 to get a number between 0 and 100.
So it's like a letter grade.
So we have a bunch of cultural factors as represented by over 110 survey questions.
And we can correlate any individual culture question or group of questions with any one of those outcomes and the aggregate outcome score.
But what does that correlation mean?
For example, maybe we find that teams that used more outsourced labor instead of internal labor influences the aggregate outcome in some way.
Maybe it makes it higher or lower.
What can we read into that?
Well, because the outcomes happened after the game shipped, there's really no way the outcomes could have caused the cultural factors.
So if there's any causal relationship at all, it has to be the cultural factors causing the outcomes.
Now, sure, it's a survey, and it's possible somebody's memory of the teen culture while taking the survey was influenced by the outcome.
That's possible, and maybe it biased the survey a little bit.
But assuming bias didn't sway the survey too much, either there is no causal relationship, or it's the cultural factors or some other factors behind them causing the outcome.
A quick note on the statistics used in our analysis of the survey results.
Not going to go into this in too much detail.
Most of you are probably familiar with the concept of a correlation.
Just to be sure, here's a quick refresher.
The correlation between two variables is the degree to which those variables show a tendency to vary together.
So you see the little boxes.
The one on the far upper right, there's no correlation.
X and Y have nothing to do with each other if I know the value of X.
I cannot say anything about the value of Y.
In the upper left hand corner, that's a perfect, almost perfect inverse correlation.
As X gets bigger, Y gets smaller.
And so I can say a lot about the value of Y if I know the value of X.
Similarly, in the lower right corner, we have a perfect positive correlation.
X and Y both get bigger or smaller together.
So if I know the value of Y, I can perfectly state the value of, if I know the value of X, I can perfectly state the value of Y.
We used a Spearman correlation with the outcome score for most questions.
I'm not gonna go into details on that.
Look it up on Google if you're interested.
I'm sure Google can explain it much better than I can.
And the reason we used Spearman instead of the more popular and commonly used Pearson correlation was because many of our questions were based on a Likert scale.
For questions.
that had two mutually exclusive answers, we used a Wilcoxon rank-sum test to detect significant differences.
For questions with more than two mutually exclusive answers, we used the Kruskal-Wallis one-way analysis of variance.
But again, if you're interested, Google it.
Look it up in the articles.
There's a lot more detail there that I can't get into.
So let's assume we have some cultural factor like teams that drink coffee.
Obviously, we didn't actually ask about coffee drinking in the study, but let's roll with it.
So we find the answers to that question on our survey correlate with higher values on the outcome scale for project delays, higher meaning better.
The graph might look something like this.
The gray dots form a rising diagonal line, and there's a strong positive correlation between coffee drinking and teams that shift the games on time.
What claims can we make about this?
If we have a correlation like this, can we say that drinking more coffee improves your odds of shifting a game on time?
Well, no, not exactly.
But we can say that one of three things must be happening.
First, there might be no causal relationship.
Depending on the statistical p-value of the correlation, we can rule that out as highly unlikely in some cases.
And we talk about p-values in the articles.
I don't really have time to get into it here.
Again, I strongly recommend googling that if you're interested.
Briefly, the p-value is a measure of how likely the results are to have occurred by chance.
The second possibility is that maybe A really does cause B and maybe drinking more coffee really does help you ship on time.
The third possibility is maybe there's some other hidden causal factor involved.
For example, maybe we've just hired people who work harder and harder working people happen to drink more coffee.
That's totally a possibility.
But it does let us pretty much rule out the opposite.
If someone says, hey, drinking coffee actually hurts your chances of shipping a game on time, we can show them this chart.
and say, OK, if that were the case, it should have shown up in this chart.
We should see an inverse correlation, but we see a positive correlation instead.
And you can say, OK, for any correlations where we find the statistical p-values are low enough to pretty much rule out coincidence, we can say that there could have been some other hidden input c, which accounts for the results, some hidden causal factor like just hiring better people.
But we asked over 110 questions over a very broad range of cultural factors.
And it's very hard to imagine that there really is very much we left out of it.
So we can't 100% rule out the idea that maybe more successful teams just hire better people.
And the correlations of all the culture questions were just coincidence.
And there's some magical thing those better people are doing that isn't accounted for in our survey questions.
That's totally a possibility.
We can't rule that out.
And you're welcome to believe there are other factors involved.
But what I would ask you to think about is this.
If you find there are things that other teams are doing that are strongly correlated with success or failure, it might not hurt to learn about them and see how that applies to your own teams.
Maybe there's some other factor involved, and maybe not.
But if there are simple changes you can make that might impact your outcomes, why not make them?
I'm going to start off the discussion of the results with what I'll call non-cultural factors.
This is a graph of the average team size plotted against the aggregate outcome. The horizontal axis is the team size based on logarithmic scale with larger teams toward the right. The vertical axis is the outcome with better outcomes at the top. We've also colored the dots based on outcomes so green is better, red is bad. So as you can see the trend line goes upward toward the right indicating the very large teams seem to have slightly better outcomes although the correlation here is not statistically significant.
The total project duration had a negative correlation, and it was statistically significant.
You'll notice the black trend line here goes downward.
And the horizontal axis is the total project duration.
But that's not really surprising.
Projects that are in trouble are naturally going to take longer and experience more project delays.
So nobody should really be surprised by this.
We asked five yes or no questions about financial incentives.
If you're spending money, or at least offering money to try to motivate your team.
it's interesting to ask whether it really makes a difference.
And for four out of five of those, we found there was absolutely no significant correlation.
From left to right, we're looking at team-based incentives, incentives based on royalties, incentives based on Metacritic review scores, and other royalties.
And in each case, the no answer is on the left, and the yes answer is on the right.
and we used a Wilcoxon rank sum test to detect significance in results here.
The one place where financial incentives did make a difference was with individually tailored financial incentives.
In other words, pay for performance plans and things like that.
The mean outcomes for teams that did use individually tailored incentives was 63.2 versus 56.5 for teams that did not.
Again, we used Wilcoxon rank sum test for statistical significance and we found a p-value of .17 for this.
We also found that this effect seemed to drop off for large teams, that is, teams of size greater than 50.
And we're not sure why this would be the case.
However, it does suggest that if you're going to offer financial incentives to your team, it's probably a better idea to offer them to specific individuals, to specific tasks within a limited time frame along the lines of a pay for performance plan rather than offering royalties or anything like that.
We asked a question about what production methodology a team used, whether it was waterfall, Agile, Agile using scrum or other. We also had an option for don't know. And we found that there was essentially no statistically significant difference. Waterfall seems slightly lower in this image but it's not statistically significant. So this is a very surprising result. Advocates for Agile and scrum in particular seem to often treat it as a holy grail and we spend a ton of time talking about methodologies at GDC.
But our results really make you scratch your head.
If Agile and Scrum really are such holy grails, why don't we see a difference in outcomes?
Here's another way of looking at the same data.
Each of the colored dots is a single team in our survey.
The orange teams used Waterfall, the green teams used Agile, the red dots used Agile with Scrum, the purple dots used Other, and the blue dots answered I don't know.
So do you see the pattern here?
No, because there is no pattern, right?
Obviously trick question.
So you'll notice that all of the lines, the orange line, the purple line, the red line, the green line, are almost coincidence.
And that is Waterfall, Scrum, Agile, and Other.
And they are almost identical in terms of their outcomes.
You'll notice that the blue line, which is the people who answered, I don't know what our team's production methodology is, which clearly indicates that there were serious production issues on that team.
that plummets pretty hard after a team size of five.
The horizontal axis here is the size of the team.
So I want to be clear that I'm not saying that production is pointless.
I'm not saying that it's important, only that it probably has the effectiveness of production probably has a lot more to do with how you do your production, how you fit it to your team, rather than whether you're using Agile, Waterfall, or anything else.
We also looked at team experience levels.
We asked a question early in the survey about the average level of experience on the team in terms of the number of years, and then measured statistical significance using a Kruskal-Wallis test.
And as you can see, there's a real progression here, especially as you move from four to five years, which is the middle column, to six or seven years, which is the fourth column, outcomes start to get a lot higher.
Notice how in particular for teams with eight or more years of experience, not only is the average much higher, but none of the aggregate outcome scores are below 45.
That lower right-hand corner is almost completely empty.
Teams with an average of eight or more years of experience.
clearly don't fail nearly as much, right?
That doesn't necessarily prove that the experience caused the better outcomes.
It could simply mean that to a large extent, more experienced developers are attracted to teams that have historically been more successful, and their experience makes them more likely to be hired by those teams.
But either way, experience clearly does matter.
We asked a question about what technology solution the team used to build its game.
From top to bottom, the rows are top row, new engine or technology solution.
Second row, internal or proprietary engine or technology, such as EA Frostbite.
The third row was external or licensed engine or technology, such as Unity or Unreal or Crytek.
The fourth row was a technology or engine from previous version of the same game or a similar game in the series.
And the bottom row was other.
And what we found was that none of these really had any statistically significant correlation with the project outcome, except for the fourth column, which is the engine for a sequel.
based on the previous game's engine.
And when you think about it, that shouldn't be even a little bit surprising, because if you are making a sequel to a game, obviously, first of all, the first game was successful, you know what the game is, you know what your market is, you know how to identify your market, you already have a technology base that you used to build the engine the first time, you probably have the same team that worked with the engine and they already know how to use it for this type of game. So there are a whole lot of advantages.
in that particular case.
So it's really no surprising that those teams showed much higher aggregate and statistically significantly higher aggregate outcomes.
Let's talk about crunch.
There's an attitude among many developers that crunch is a necessary part of any good game development project.
There's a quote from an interview with a certain veteran game developer that sums up this attitude.
Crunch will always exist in studios that strive for quality.
And a large percentage of our industry shares this view.
And I want to point out that the implicit assumption here is that extraordinary results can only come from extraordinary effort, and that extraordinary effort necessarily implies extraordinary overtime.
Now, I think most of us are aware of the data of the harmful effects of extended overtime on individuals.
There's tons of data clearly illustrating that the abuse of overtime not only leads to lower total productivity, not only drives a lot of talented people out of our industry, but it leads to a higher risk of relationship failure, mental illness, alcohol abuse, depression, and a host of other problems.
I think many of us are also probably aware of the recent findings that pulling all-nighters causes permanent brain damage.
And outside the game industry, it's generally accepted that overtime abuse is a bad idea.
And the practice of crunch is relatively rare.
But there's an implicit assumption here, first of all, that crunch actually improves the quality of the game.
But how do we know that's true?
And sure, I know a lot of you are probably looking at me like I'm crazy and thinking, hey, wait a minute.
On my last game, we crunched 80 hours a week.
We never would have finished it if we hadn't crunched.
And yeah, maybe that's true.
And sure, there have been quite a few undeniably great games made with crunch.
But I also know a lady who chain-smoked and lived to 95.
Can we say that because she was chain-smoking, the chain-smoking caused her enhanced longevity?
Or should we look at the broader data and say, OK, putting it in perspective, that lady is an extreme outlier.
It actually takes seven years off your life on average.
And how can we know for sure that that lady wouldn't have lived to 112 if she hadn't been a chain-smoker?
And similarly, if you ship a great game and you crunch to make it.
Do you really want to give crunch all the credit for that?
Can you really say our game was great because of the crunch?
How can you be sure it wouldn't have been even better if you hadn't?
You don't have a time machine any more than I do.
You can't go back in time and rerun that experiment to see how it would have turned out if you'd made slightly different decisions.
So I want to ask, is there really a solid link in the aggregate between more crunch and better outcomes?
Or are we just assuming the crunch works?
without any real aggregate data to back it up.
We asked a total of five questions relating to crunch in our survey.
Two of those questions related to the overall amount of crunch the teams engaged in.
The three bars in this graph show all of our 273 samples grouped into three categories.
The bar on the left shows the 25% of teams that engaged in the least amount of crunch.
The bar on the right shows the 25% of teams that engaged in the greatest amount of crunch.
And the bar in the middle is the central 50% of teams that did a medium amount of crunch.
And the vertical axis is aggregate outcome score, with the top of the scale, as before, representing teams with the most strongly positive outcomes.
So that makes you wonder.
If crunch is a necessary precondition for quality, how do you explain the green dots on the top left?
We're looking at a whole lot of successful teams there, and they achieved their success with the least amount of crunch.
In fact, the group with the least crunch had the greatest number of project with positive outcomes of all three groups.
I'm sure the three data points inside that box all strive for quality.
And yet, they didn't crunch very much, if at all.
So how did they manage to achieve success?
And similarly, take a look at the lower right.
We see a whole lot of red dots toward the bottom showing teams that experienced very poor outcomes despite doing more crunch than anybody else. In fact, most of the teams that experienced poor outcomes used the greatest amount of crunch.
Let's dive in a little deeper. Among the teams that did the most crunch on the right column, there were 71 data points with an average outcome score of 48.
The middle 50% of boxes in this category, that is the dots inside the gray boxes in the vertical box and whisker plot, had outcome scores between 34 and 63.
Now let's look at the bar on the left.
These are the 25% of teams that did the least amount of crunch.
Here we have 64 data points with an average outcome score of 62.
The middle 50% of those samples, again the ones inside the gray boxes, had outcome scores between 51 and 74.
So you may be looking at that and saying, okay, well, that's nice, but what about when crunch is voluntary? I mean, hey, look at me, I'm doing overtime because I'm passionate about the product, not because I have a boss holding the gun to my head, right? So now let's look at solely voluntary crunch. We had two questions on our survey asking whether any crunch done on the project was voluntary or mandatory, regardless of how much crunch actually occurred. The left column is the mandatory crunch group.
The right column is the voluntary crunch group.
And just as before, the middle bar is the middle 50% of teams.
Here we see a very strong and statistically significant correlation.
The teams that reported that crunch was mandatory were far more likely to report low outcomes.
And the teams that reported that any crunch was voluntary were far more likely to report positive outcomes.
So in the 25% most mandatory crunch group.
60 data points in that bar, the average outcome score was 49.
50% of them were between 34 and 63.
In the group with the 25% most voluntary, there was 71 data points with an average outcome score of 67.
50% of those were between 54 and 80.
Here's a graph, a little table, showing all the data sorted by how much crunch they did and whether that crunch was voluntary or mandatory.
So you can see the horizontal axis, left side is voluntary, right side is mandatory.
Top row is the most crunch, the bottom row is the least amount of crunch.
Now look at the upper right corner.
The teams that were in the 25 percent quartile of saying the crunch was the most mandatory and that also did the most amount of crunch.
had a median outcome score of 39.84. In the lower left, you notice the little green box with a value of 70.62. This is the teams that were simultaneously reporting that they were in the most voluntary quartile and the least amount of crunch quartile.
Now, let's say you're coming into the industry for the first time. You don't have any preconceived notions about crunch at all. Maybe you don't even know what crunch even is, right? Which of these groups would you want to be in?
Now, you may be looking at this and saying, wait, hold on a second, Paul, this is all BS. You're picking up a spurious correlation. All you're really picking up is whether teams are experiencing problems. Because everyone knows the teams that are experiencing problems are more likely to try to use crunch to solve them. And that's true. I've certainly known teams that were just dedicated to crunch no matter what because they thought it worked. But, yeah, there are definitely a lot of teams that say, hey, we're now having problems, let's crunch. And that's how we will try and dig ourselves out of this hole.
So we're going to refer to this as the crunch salvage hypothesis, which is crunch is more likely to occur on projects that are already in trouble.
And these results are just picking up that underlying correlation.
So here's what we did to try to tease out whether that was actually what was going on.
We built a predictive model based on a linear regression for all of the cultural factors in our entire study.
And we found that there were 30 of them that we could use.
put them into a linear regression, and build a linear regression that had a correlation of 0.82 with the aggregate outcome score.
That is an incredibly high correlation.
And the p-value here was incredibly close to zero.
So we could predict with a very high degree of accuracy what a team's aggregate outcome was going to be.
based on those 30 cultural factors.
Then what we did was we took that same linear regression and we removed all the questions related to crunch.
This lowered the correlation from 0.82 to 0.81.
So you'll notice here in the upper left side, culture, teamwork, and production factors excluding crunch.
That was what went into the crunch-free model.
So based on that, we were able to look at every team and say, OK, what would?
the crunch-free model predict the aggregate outcome score would be.
And then we could ask, how much did they crunch, and what is the difference between their actual outcome and the model that our linear regression reported?
In other words, how much lift did crunch give them beyond what this very powerful linear regression model reported?
And here's what we got.
Here's the same chart with the same categories.
Instead of showing the actual outcome score, we're just showing the lift that these teams seem to experience from added crunch.
And again, note that this is a 0 to 100 scale for outcome scores, right?
So a value of 3 or 4 is tiny. It's almost insignificant here.
You'll notice in the upper right, the teams that were in the most mandatory quartile and the most crunch quartile...
had scores that were 3.91 at outcome points lower than the crunch free model would have predicted. And in the lower left corner, the teams that were both in the most voluntary quartile and the least amount of crunch quartile had aggregate outcome scores 3.17 points higher than you would have predicted. So this suggests that maybe crunch actually has a minimal effect on the outcome of your project.
Another way of putting it is your studio's culture probably already dictates most of the outcome.
And crunching really isn't going to change that.
I want to be clear that we didn't set out with any preconceived notions or trying to prove anything.
We just followed where the data led us with an open mind.
But the data spoke very strongly on this.
Now you might be saying, hey, my team crunched and our game turned out great.
And that's true.
Some projects do a lot of crunch and they turn out just fine.
But I have to ask, do we crunch because it works, or do we do it because we believe it works?
A single data point is meaningless on its own.
Remember the 95-year-old chain smoker.
There's no way to tell unless you do a thorough statistical analysis on a large data set.
And all of us in this room have different opinions on crunch.
All of us have different experiences.
And if we go by who's an expert or who's got the most experience, we'll still just end up with a room full of arguing.
Because any position you can take on an issue like this, I can find an equally qualified person on the other end of the spectrum to argue the other side.
There are a couple of potential weaknesses with our findings on crunch. One is that it doesn't precisely define what crunch is.
We were not able to nail down, here's how many hours a week constitute crunch.
It doesn't nail down the point of diminishing returns for overtime.
In this next version of the study later this year, we're going to try to quantify that.
But my challenge to you, if you disagree, is to prove me wrong, but prove me wrong with data.
Nothing else is going to convince me.
Let's talk about culture.
And this is where we saw the really strong correlations in our study.
So in order to be able to measure culture and analyze it systematically and quantitatively, we need some sort of conceptual model for it.
And we throw around a lot of words in the industry, like teamwork and culture.
But those are all very big and wishy-washy words.
We need to nail down exactly what we mean by good teamwork or bad teamwork, or how one studio's culture differs from another studio's culture on a very precise level.
And this is important.
You can't debug computer code unless you understand the programming language it's written in.
And similarly, you can't debug your team unless you really understand the language of good teams.
Just to give you an example of a conceptual model for effective teams, I know some managers who try to use Freudian psychoanalysis as a management tool.
And that is not the right tool for the job.
If you're playing armchair psychiatrist with your employees, you're not going to fix anything.
You're not going to understand your employees, and you're really just going to tick them off.
And you're probably going to make a dysfunctional team even worse.
So it's very important to have the right conceptual model of effective teams.
Back in 2009 through 2012, I got an MSE degree in technology management from University of Pennsylvania in the Wharton School.
And as part of that, I studied organizational behavior and design under Adam Grant, the top-rated professor at the Wharton School.
And in that class, they taught a scientifically validated model for team effectiveness that you can use to analyze and diagnose any team in any industry.
And I found that really fascinating because I'd never realized there was such a thing before.
or that there was such a thing as a model of team effectiveness, or they could be scientifically validated.
And it put all of my own experiences in the game industry into an incredible perspective.
And I started to look back on my own past experiences and see how so many of the teams that I had worked with had different cultures that correlated dramatically with the results they actually achieved.
The Wharton effectiveness model is based on the book Leading Teams, Setting the Stage for Great Performances by J. Richard Hackman.
So we decided to use Hackman's model as one of the foundations of our survey.
Here's Hackman's model in a nutshell.
Number one, real team.
This means there have to be clear team boundaries, who's in the team, who isn't.
Team members have to have clear authority over their own tasks.
They have to be able to build those tasks themselves, not have them handed to them from on high.
The team composition has to be...
based on diverse skills.
There has to be stable membership.
If people are constantly coming and going, it's not a real team.
Number two, there has to be a compelling direction, a clear motivating goal that guides and motivates the team's efforts.
Number three, enabling structure.
Tasks, roles, and responsibilities have to be clearly specified and designed for individual members.
And there has to be a clear definition of who is and who isn't on the team.
Number four, supportive context.
This means there should be incentives encouraging the desired behaviors and discouraging the unwanted ones.
There should be tools and affordances to get the job done.
And team members need to have a level of psychological safety that they can raise the red flag, call out when the ship is about to hit an iceberg, and know that they're going to be listened to and not politically punished for that.
Finally, they have to have access to expert coaching.
This means motivators, consultants, and educators outside the boundaries of the team who can help them raise their game.
In addition to that, we also looked at the model described in the book The Five Dysfunctions of a Team by Patrick Lencioni.
This book describes a very different model, and it describes how things typically go wrong with the team's culture.
But rather than explaining it this way, I'm going to use Yoda to help me out.
It starts with a lack of trust between team members, which leads to a fear of conflict because nobody trusts anybody else and nobody feels like they can speak out safely.
This leads to a lack of commitment because nobody feels engaged with the project.
The lack of commitment leads to an avoidance of accountability, this leads to an inattention to results, and a disconnect between the team's perception of itself and the results they're actually getting.
And that leads to games that don't live up to their potential.
And that leads to hate, hate leads to anger, anger leads to the dark side.
You know how it all goes.
So what's nice about these two models is they're essentially orthogonal to each other.
For the most part, the Wharton School model, that is the Hackman model, describes external factors, structural factors, context, how to set up the right environment for a team to really excel.
Whereas the five dysfunctions model, the Lencioni model, describes the internal dynamics of a team and how those internal relationships can go awry and what to do to fix them when that happens.
I'm going to refer to these as the Hackman model on the left and the Lencioni model on the right.
So here's what we found.
And yes, there's a lot of numbers on here.
But before you freak out, I promise I'm not actually going to show the numbers this way.
These are all in the articles on GammaCetera if you want to read them there.
I'm going to give you a very different view of the data.
Really quickly, on the left side, we have five of the questions that we asked with this particular aspect of the Hackman model.
Along the top, we show the correlations with project delays, ROI, Metacritic, internal goals, all the various outcome factors, as well as the aggregate outcome score.
But numbers are boring.
And rather than showing a whole bunch of correlation values, what I'd like to do is show you the actual responses from the top teams and the bottom teams.
On this slide we have three of those questions from that section, I'm sorry, two of those questions from that section.
I've taken just the top 25% of teams from our survey and the bottom 25% of teams as ranked by their aggregate outcome score.
So the blue teams represent, the blue area represents the top teams, the red represents the bottom teams.
We've omitted the middle 50% of teams completely for these slides that I'm going to show you.
And across the horizontal axis, we have the level of agreement or disagreement with each question, where agree completely is all the way on the right, to neutral in the middle, to disagree completely in the lower left.
And the vertical axis represents the total count of responses in that category for either group.
So look at what happens.
The question is, the team composition didn't change during the course of development other than growing when needed.
Among the top teams in blue, 16 people responded agree completely, and 17 people said strongly agree.
Among the bottom 25% of teams, only six people said agree completely to this question, and 10 said strongly agree.
So the curve has shifted way out to the right.
And you'll see this with almost all of these questions as we go forward, where the top teams agree much more strongly with positive statements.
On the left, the organizational structure and membership of the team were clear from the outset. 17 people from top teams said agree completely, while only four people from the bottom 25% of teams said agree completely. And on that same chart, 11 or more people from the bottom teams said either disagree completely or strongly disagree that the organizational structure and membership were clear, while among the top teams, exactly five people gave each of those responses.
On the right side, the team composition didn't change during the course of development other than growing when needed.
So second aspect of the Hackman model, compelling direction.
Does the team have a clear motivating goal that helps focus and guide their efforts?
On the left side, the development plan for the game was clear and well-communicated to the team.
On the right side, the vision for the final version of the game was clear and well-communicated to the team.
You can see the blue teams agree much more strongly.
The third factor, enabling structure.
Are tasks, roles, and responsibilities clearly specified and designed for individual members and properly matched to their individual skills?
Team members' tasks were well-defined and clearly specified.
On the right side, team members' responsibilities and job roles were carefully matched with their particular skills and abilities.
You can see the blue teams in both cases agree much more strongly with the red teams.
Supportive context.
Does the team have the psychological safety they need to speak openly about problems?
And do they have faith that if they speak up, you'll take appropriate action and they won't be politically punished for that?
Do they have incentives encouraging the desired behaviors and disincentivizing the undesirable behaviors?
And do they have the tools and affordances they need to get their game, to get their jobs done?
So on the left side, we have the question, mistakes were treated as learning opportunities or a chance to improve, not a nail in your coffin.
On the right side, we asked people, personnel issues within the team, teamwork problems and HR issues, were dealt with professionally and appropriately.
Expert coaching.
Does the team have access to experts outside the team's boundaries, including motivators to help push them forward, consultants to help guide them, and educators to help improve their skills?
On the left side, you see the team explicitly attempted to develop team members' skills or otherwise assist them in their skill development outside of their normal day-to-day work.
You see the blue teams agree much more strongly with this.
On the right side, we received some form of coaching or guidance to enhance our efforts.
or improve our effectiveness as team members. Now this is a little interesting because we can see in this case both the red teams and the blue teams generally disagree with this. So this is an industry wide pattern. We don't do a lot of training, we don't do a lot of coaching. It tends to be pretty sink or swim. But still in this case there is a lot more agreement from the blue teams than the red teams.
We also asked a bunch of questions related to Lencioni's 5 dysfunctions model.
We got very similar correlations here to the strong correlations we got with the Hackman model.
We asked around 20 questions in this category, but in the interest of time, I'm only going to show you three of those.
The first one, it was safe to take a risk on this team and stick your neck out to say something that needed to be said.
In the middle, most team members bought into the decisions that were made.
Now on the right side, we've got a very interesting outlier because here we've got a question that was asked in a negative tone.
We're not asking, did the team do something good?
We're asking about something bad that happened on the team.
And as you would expect, here we see red and blue have flipped.
The blue teams are on the left, indicating much stronger disagreement with this statement.
The red teams are on the right.
So the question is, some team members put their ego or their own need for recognition above the collective goals of the game projects.
The blue teams, the top 25% of teams, strongly disagreed with this in the aggregate.
And the red teams strongly agreed overall.
We also used a third book, which is 12, The Elements of Great Managing.
This book is based on a team effectiveness model built by Gallup, which is based on thousands of interviews with teams around the world.
So like the Hackman model used by the Wharton School, this is a scientifically validated model for team effectiveness.
Now for the most part, this book is the least useful of the three and I'm not going to spend anything more than this one slide on it.
because most of its factors are already covered by the Hackman model and the Lencioni model.
But there are five factors I wanted to call out that are unique to this book.
A connection with the company's values had a correlation of 0.39.
That's a pretty strong correlation.
Receiving regular, powerful, insightful feedback.
Having peers who are committed to doing quality work.
My opinion matters.
And recognition and praise.
We also asked over two dozen questions regarding other factors, specifically related to game development.
Having a shared vision for the game had a correlation of 0.5.
That's an incredibly strong correlation for anything in the social sciences.
And that is the single strongest correlation of any of the questions that we asked in our entire study.
Resolving disagreements about the game design, also very strong, 0.45.
Justifying and communicating design changes, 0.45.
Celebrating novel ideas, even if they don't achieve their intended result, 0.38.
A respectful relationship between the management and the team, 0.36.
So here I want to quickly discuss factors that were not correlated with outcomes.
And as far as we can tell, these factors had no global correlations.
We've already discussed at the beginning team size on any absolute scale, production methodology, whether team used Agile, Scrum, Waterfall, anything else, and financial incentives other than individually tailored financial incentives, none of those had any global correlations with outcomes.
We had a couple questions about cross-functional versus per-discipline teams.
Here, we saw no global correlation.
We asked a question about game genre.
And unfortunately, I like to think that there are hidden correlations here, but we provided about 25 different genre categories.
And once you take 273 answers and split them up into 25 different game genre categories, shooter, tower defense, city builder, all these other things, your sample sizes are now too small to really compare them to each other of any statistical significance.
Having a friend on the team was not correlated.
Sharing team members with other projects.
Having a preference for face-to-face communication versus email.
Or a reliance on temp workers or contractors, or the use of outsourced labor.
So it's interesting toward the end that external labor had no global correlation, or the use of contractors or temp workers.
And what that probably just implies is that the real key here is not whether you use external talent.
It's what external talent you work with, and how you build and manage that relationship, and how carefully and diligently you work with an outsourced team.
OK, let's talk about some of the conclusions we can draw from this study.
And here's the part where I get up on my soapbox a little bit if I'm not there already.
Number one, let's start demanding evidence.
So the next time somebody tells you that Waterfall is the devil, and you have to use Agile or Scrum, and your game is totally doomed if you don't use their particular production methodology, ask them for evidence.
Ask them to prove that it works. Our industry is full of untested assumptions, and many practices persist because we don't question them.
And similarly, if someone tells you the only way to make your game truly great is to work 80-hour weeks, and your game is going to be total crap if you don't do that, Ask for evidence for that, too. Ask for numbers. Ask for data.
Conclusion number two. The real opportunity for improving our odds of better outcomes are mostly cultural. Back in 2012, Bethesda Game Studios executive producer Todd Howard gave a keynote speech at DICE and he showed this slide.
And he said the following, and I'm going to quote him verbatim here because I love this quote.
This is one of the big rules that we have, which is the plan that you have is not as important as your culture.
So you see a lot of game makers will say, so here's the big schedule, here's everything we're going to do.
You know, if they're really trying, they're going to run into problems.
And those problems are solved by the culture you have on your team.
I have my own way of putting it, which is this.
I believe every game is a reflection of the team that made it. If you want to make a better game, you need to build a better team and a better culture on your team. In other words, your game comes from your team and your team is a product of your culture. So if you want to make a better game, build a better culture.
So the real opportunities for improving our odds of better outcomes are mostly cultural.
Leadership, teamwork and cultural factors accounted for more than 85% of the difference between the best teams and the rest.
And before you resort to crunch, have you tried debugging your culture?
Have you tried debugging your development process?
And have you tried making sure the first 40 or 50 hours every week that you're actually working are actually being used efficiently?
Why scale something that's broken?
Number three, there is an established science to building effective teams.
Let's start using it.
Our findings mirror those of management science exactly.
Be aware of the limitations of your own experience and instincts.
recognize that our industry is not a unique and special snowflake. We are still humans working in organizations and management research is equally relevant to us.
Number four, if you want to get the most out of your culture, you have to design it. It's not going to happen by itself.
Write or update your mission and value statement.
It matters.
It really does.
Hire people who will embrace your values.
It's really not enough to say, hey, my philosophy is to hire great people and get out of the way.
I've heard a lot of people in the game industry say that.
I just hire great people and get out of the way.
And if you really have that attitude, if that's really how you go into leadership and game development, you're missing about 95% of your job.
Because you can hire the best people in the world.
they're still going to run into a million problems over the course of development.
And if you are just letting those problems fester, you're going to run into big problems.
And your job as a leader is to proactively manage risk.
Designing your culture intentionally means it's your job to incentivize the behaviors that align with your values and disincentivize those that do not.
Actively shape your team's internal discourse to promote healthy feedback and creative conflict.
in the creative process and minimize politics and interpersonal conflict.
And keep the millions of small problems and values misalignments from turning into big ones.
And that's the end of my talk.
Our Gamma Citra articles are at that link if you're interested in reading through those some more.
If you have questions or feedback, the Game Outcomes Project has a Twitter account.
We are just Game Outcomes on Twitter.
Those are the three books used in our study.
And again, I definitely recommend the two on the left, Hackman and Lencioni.
We're going to do a new survey toward the end of this year.
And our plan is to keep evolving this every year or two and refining it and finding better and better questions.
We're also doing another project on leader interviews.
One of our team, Eric Byron, is interviewing various leaders within the game industry.
So if you're interested in participating, In that, there is Eric's email and I strongly recommend emailing him.
And finally, if you want to contact me, there's my contact info at the bottom.
And we've got some time for questions, if anybody has any.
Thank you.
I'm curious if you check to see if other factors could account to explain crunch time in a linear regression.
Yeah, so the question was whether we check to see if other factors could explain crunch.
We looked at cross correlations of the quantity of crunch and the outcomes with pretty much every other factor that we could think of, including, you know, project duration, team size and.
There's a note in our article, it does actually, it's the fourth article in the series, if you go to Gamma Citra, there are actually pretty strong correlations with especially things like lack of organization and a lack of respect for team members, right?
It actually has very, the quantity of crunch, people are far more likely to experience crunch on a project if there is not a respectful relationship between the management and the team, which is exactly what you would expect.
So, next question.
Sure. How exactly did you define the different methodologies on the survey or did you just ask people if they were doing agile without any kind of framework to that?
Yeah, essentially, if you go to the GammaCytra articles, that has a link to our methodology page and you can download the actual questions directly from that.
But what we did was we asked...
you know, what production methodology a team was generally using along with a description of what that meant.
So are you using waterfall and here's a brief description of waterfall as a staged planning process versus a brief description of Agile and so on. So next question.
great talk, enjoyed it. I had a question about the years of experience you used. It was kind of like almost two, three, four, up to eight plus. You started off by mentioning you've been in the industry for over 20 years. I thought that eight plus, I'd be interested in knowing how that continues beyond, right?
What about 10 and 15?
And I really wish I had more data there, and certainly in the future we're going to refine those categories.
We'll probably ask, you know, we'll probably branch them into like average of 10, 15 or more and so on.
And we may also add a question asking the actual number of years.
And again, I'm sorry that we didn't do that, but in the next version of the study we'll do that for sure.
Next question.
Excellent talk, congrats.
In future surveys, will you be able to ask teams pre-release so that a positive outcome doesn't influence their recollections?
Right. The question was, in the future, will we be able to ask teams about their culture pre-release so that the outcome does not bias them?
We would definitely like to do that. We just have to make sure that we...
actually are able to match people up properly and it's going to require that people essentially fill out the survey twice.
Or at least two different versions of the same survey, the culture and then later on ask the outcome questions.
So, I think that would be a great way to help reduce some of the possible bias. Yes?
For experience, was that team experience or experience as individuals?
Yeah, so the question was how we asked about the team experience and...
what we asked was the average years of experience for people on the team. So we asked people to tell us do people on your team have an average of one to three years of experience, four to six, seven to eight and so on.
their length of time together, working together.
Right.
There was nothing in there about their length of time working together as a team.
And that's actually a very powerful factor, because teams that have just been put together for the first time, certainly all the management research they talked about at Wharton School was that that has a very, very powerful effect.
So we should ask that in the future.
And we'll include that on the next version of the study.
Yes.
So in one of your slides you show the graph that essentially shows how fellow game developers help one another outside of work.
Do you think a mentorship mentality like Shigeru Miyamoto's while he's developing a game, do you think that's also beneficial to a healthy game development culture?
I'm sorry, the question was, I couldn't quite make out, you were asking about mentorship?
Was that correct?
Yes, do you think it's...
Yeah, I think mentorship is absolutely, you know, extremely important and being able to learn on the job, grow your skills continually.
And it's something that I think in the game industry, even the top teams tend to take an attitude of people should be taking that on themselves rather than studios and companies actually building development plans intentionally for their developers, for their employees.
And I think that would help.
Thank you very much.
Yeah, yeah.
Yes.
Hi.
I was wondering if you could expound a little bit more on communication.
You had a little point about how email versus face-to-face didn't have a high correlation to outcome.
I was wondering if you studied, I may have missed this, but virtual teams or grouping by discipline versus grouping by team, stuff like that in an office.
Yeah.
So the question was about communication and what question we asked about that.
that was pretty much all we had as far as overall how the format in which the communication occurred.
We did not ask any questions about virtual teams, but we did ask about, let me go back here.
Wait, actually it's closer to the end.
Oh, come on now.
Sorry.
All right, let's go to the end here.
Yeah, we did ask two separate questions about, here we go, reliance on temp workers and contractors and use of outsourced labor.
We did not ask about virtual teams.
But we do want to get more into this in the future and find out, are there actual correlations?
So, yes.
Did you notice any correlation between methodologies and duration specifically, as opposed to the aggregate?
Yes, so was there any correlation between methodologies and things like team size and duration?
And yes, there were.
One of the things was we found that larger teams are somewhat more likely to use waterfall and smaller teams were somewhat more likely to use agile, which kind of makes sense because if you're building some massive game that's like the next...
game in the call of duty franchise and you've got 200 developers on your team, you've got a huge burn rate and you really, really need to plan things out very, very carefully.
You can't afford to be agile and say, hey, we've suddenly come up with this great idea for our game design, let's just pivot this entire 200-person team.
It's a lot harder to pivot.
So you'll notice this one here, the orange line is waterfall.
It sort of goes up toward the right, indicating that larger teams.
more likely to use waterfall and the smaller teams are more likely to use agile and scrum because they are small enough and versatile and agile enough.
to use those methodologies. We also occasionally post, do additional analysis of our findings, find correlations between some of the different factors and not just with outcomes. And if you follow us at Game Outcomes on Twitter, we occasionally post them there. So we've also got some historical stuff on that. If you scroll through our Twitter feed, you should find them. Next question?
Yeah, did you ask about either individual or team level of experience with any of the individual?
Like, did you ask about how experienced is your team with Agile versus Waterfall?
How experienced are you in this industry?
Great, great question.
So the question was, did we ask how experienced is the team with Agile or Scrum, or how long has the team worked together?
What's the team's overall level of experience?
We did not ask that.
My instincts tell me, and I think what I learned about management science, would tell me that both of those would be huge factors.
So we will try to include those on the next version of our talk, if at all possible, of our survey, if at all possible.
Yes.
Hi. Great talk. It was awesome.
And I apologize if this is a question that I believe you addressed in the talk, but I didn't quite catch.
In terms of how the outcomes were viewed, I got that a lot of it was provided in terms of the team's perception of outcome.
Was there any information on actual outcome, so money made or by the game, etc.?
Yes, so that's a good point.
The outcome score is a lot of it was based on the team's perception of outcome.
And the question of how do we know the team wasn't just saying, oh, we did great, when actually they did terrible, or vice versa, right?
So one of the things we did, and this isn't necessarily 100% scientific here, but we had an optional question at the end.
you know, asking what project people actually worked on. And so based on that, we were able to analyze, you know, we had the outcome question of what was the game's Metacritic score, what was the overall critical reception and people would answer, oh, my Metacritic was between 70 and 75, between 76 and 80, between 81, 85 and so on, answered based on those categories. So we were actually able to cross correlate those and say, hey, this, you know, this person worked on shadows of Mordor, Donkey Kong Country returns, whatever game And they answered that this was their critical reception.
Let's actually look up the Metacritic score.
And even though there were about 20 of them, we found that they were all answered honestly.
Every single one of them, the category that they had filled in for the Metacritic score was what the actual Metacritic score for that game was.
And again, it's too bad that we only knew what 5% of the games were.
and that we can't tell you who answered what, right, because that's confidential.
But that was a nice reinforcement that people did seem to be answering these questions honestly.
So thank you.
Regarding survey methodology, for the questions that you pulled from Linceone and Hackman, were those questions, did they have an established ChromeVox alpha level for them?
An established what?
Like an alpha level, like that they were reliable and valid questions, or were they just questions from the book?
Oh, oh, yeah, so how did we design the questions from the Hackman and Lencioni model?
Yeah, essentially, it was the whole Game Outcomes Project team going through these models and just trying to find the questions that we thought correlated most closely with what the Hackman and Lencioni model represented.
So, yeah.
There's no, I don't think there are any official like surveys of the Hackman and Lensini models.
I didn't think so.
That's why I wanted to ask.
I think there is one for the Gallup model, but I don't know, I don't have access to the actual questions.
Okay, I think so.
Yep, last question.
Yep.
By the way, if anybody, I think we're running out of time, but if anybody has any more questions, ping us anytime at gameoutcomes on Twitter.
Let me put up that link again.
All right.
Whoops.
Thank you.
Thank you.
