Hello, and welcome to the talk about how to encourage social and emotional learning in-game in a data and research-driven manner.
So let's start.
A meme always says more than a thousand words.
And today we will prove that at least one of these do exist, if the right circumstances are there.
My name is Jane Scullman, and I'm executive producer for Star Stable, and also the platform for the pilot that we are going to talk about.
Star Stable is one of three products within Star Stable Entertainment, and the other two being Wild Song and Curry.
Besides being the executive producer for Star Stable, or at the time, technical producer, I was for this pilot also the driver and initiative taker.
On stage, I have two friends with me, and without them, this pilot wouldn't exist.
Hi, my name's Alex.
I'm the founder of Otterloo.
We build AI technology to create and maintain positive communities.
I previously worked at Google within their trust and safety teams for a number of years.
And hi, my name is Paulina and I'm the co-founder and CEO of Pepe Agency.
We are a Swedish based agency specialized in developing social and emotional learning experiences through play.
And today I'll share a little bit about how we went about when we crafted the in-game message that could encourage a more positive and kind behavior online.
So before we go into the pilot, let's talk a bit about the platform that we use, the Star Stable game.
We are an MMO and our target audience is girls. You come for the horses and the equestrian dream, but you stay for the friends and the community. On screens we see four of the main characters, the Soul Riders. The Soul Riders are a symbol for friendship, kindness, and a bit of magic.
They all have their own horses.
just as the player, and the players can choose between 100 different horse breeds, and still counting.
Some other facts about the games, we have been around for 10 years, and we have 650,000 monthly active users.
We are translated into 14 different languages, and are played in 180 countries.
We do weekly releases, so we promise our players something new every week, and we have never failed, actually.
And we also are cross-platform today.
So if you are on a mobile iOS and I'm on desktop PC or Mac, we can play in the same PlayStation.
And it's also very proud to say that we are 54% women within the studio.
And let's visit our world, the magical Jorvik.
You So let's go to the pilot.
We had a challenge and we asked ourselves, are we the police or are we the paths to our players?
Being an online game and community for girls between eight and above, there are some rules and regulations to follow when it comes to the social elements of the game.
Social elements like the chat and other in-game communications.
And of course we also want to keep our players safe and be able to play in an environment without being bullied or treated badly.
And at the same time not compromising the fun of connecting to new friends.
So we asked ourselves if there was some other way to create a good and caring environment than banning the players that behave badly and acting like the police.
Almost two years ago, we stopped using the more traditional method for in-game chats that filter out words and ban players automatically when abusing these words.
We first had a blacklist approach, which means a list of words that players can't use, they get hashed out in the chat.
That was not enough for creating a good environment, so we switched to white listening, which means a list with allowed words. And those are the only words that the player can't use, which of course creates some kind of limitation. So this let us, both approaches created dissatisfied players due to the limitation and also not understanding why you got banned, as you can see on the screen here.
So this led us to take a more modern attitude and we started to explore what AI moderation could do for us.
Today, all our in-game communication channels are moderated based on AI with support for our human moderation team.
But we were still not 100% satisfied with the player environment and we started to ask what more could be done.
At the same time, I got in contact with Alex through LinkedIn and we had a first conversation about AI in relation to online communities and games.
And we soon realized we had the same future vision for in-game communication.
The vision we discussed was something that I believe was the solution to our challenge.
Our players seek to exchange ideas about issues like mental health, identity, family, school, and life's big questions.
Policing created a lot of frustration for our player community, because it didn't let them to talk about things they wanted to talk about.
We of course comply with all regulations there are, but beyond that, who are we to stop players from talking about what matters to them?
And just like in the schoolyard, they will talk about these topics.
Our job is to protect them, not to cut them off from important conversations with friends.
So with the joint vision, we started to become more concrete.
And I invited some other key people from Star Stable.
Our director of customer experience, Cecilia Munte, and director of community, Jane Billett.
They got on board with the vision and the three of us created space in our current roadmaps in order to enable this pilot.
The pilot also included three tech teams. It was the game server team, backend and also the client.
It also was supported by marketing and our communication and community team, of course.
Each team brought their knowledge and together we started to create a foundation for the pilot.
So with a joint vision and a great pilot team, we were ready to go.
And the aim of the pilot was to effectively address and decrease negative social behavior in-game without sacrificing our players' social engagement.
We decided to run the pilot in UK, using three servers during six weeks period.
In order to verify the pilot, we would use two groups of players, a treatment group and a control group.
So when a player in the treatment group was detected to send harmful messages, they got a pop-up with a message to stop, think, and act before sending the next message.
No one was blocked from sending more harmful messages, but instead encouraged to take a better decision.
And in order to get the right message on the pop-up, we added Pepe Agency to the pilot.
Yes, so some of you might be thinking, why social and emotional learning?
What is that and how can that be used to reduce toxicity?
Well, to start off, when we're using traditional methods like banning players, we're risking to leave them feeling perhaps even more frustrated or unaware of their hurtful behavior.
To give you a comparison, I'm sure many of us in here have found ourselves in moments when we've lashed out either at our partner, our kids, or maybe a friend.
And in the moment after, we thought, oh gosh, why did I do that?
And so what if you would had a little person on your shoulder that told you, hey Paulina, why don't you stop and think for a moment?
I'm sure that our behaviour would have been completely different.
And that was the exact feeling that we wanted to provide players on Star Stable.
To give them guidance to change their behaviour so that they could form positive relationships.
And we saw that moment as a skill-building opportunity, but it was important that we did not judge their current behaviour.
And the skills that we wanted to install are not only about being able to label your emotions and the feelings that you're feeling in that heated moment, it's also about understanding other people's emotions and how you can use that information to form positive and healthy relationships.
Now, when we design, we have a principle to design with empathy.
And in this case, we needed to go to the root of why people bully.
In many cases, it's because you feel unhappy, scared, or unheard.
It might be because of something that happened at home, or it could be something that happened in the game.
It might also be that I lack the skills to control my impulses or to label my emotions.
And in the case of Star Stable, which I think is true for many online games, we could see that players were acting out of rage when they lost a race.
Or perhaps they spotted bullying in a chat, but they were too afraid to act, and they might even continue and tag along with the friends in the behavior that they could see.
We could also see that players felt disappointed about a recent release, and they acted out in that frustration.
So with this knowledge and the awareness of what situation a player might find him or herself in when they're acting in their anger, we developed a three-step approach that is based on a research-based method called non-violent communication. And we divided it into three steps where the first one is to help the player identify his or her feelings and the situation that we would like that person to change.
So we talk about self-awareness and self-management.
The second step is to help the player understand how the hurtful behavior is causing others to feel.
So social awareness.
And then we wanted to encourage a more positive behavior by giving an example of what that player could do.
So talking about relationship building.
And in the case of Star Stable, this is the message that we ended up using.
So first you see, hey, it seems you sent an unkind message.
We're helping the player identify the behavior that we would like to change, but without judging.
Secondly, stop and think, how would you feel if someone said that to you?
So giving the player the opportunity to experience empathy and put themselves in the shoes of another player's feet.
And then third, remember, being a good friend can also get you lots of new friends.
So why would I change my behavior?
Well, if I can get new friends, that sounds pretty good.
Now the message would not be effective if we couldn't identify the right moments to deliver this.
And in order to do so, we needed the powerful technology that Autoloo could contribute with.
So, as Polina mentioned, we now have the message, right?
So we needed to know when to trigger it, and also how do we actually measure this?
How do we measure that it's effective at all?
So what we used was something called transformer models, and to give a very simple explanation of this, imagine an AI model that ingests the entire Wikipedia.
and then through that learns language and for example, cultural contexts and things like this.
And that's something we use as a base.
But I'll also show you how this can go wrong with this cutting edge AI technology.
So I've put this sentence here, which you can see, Winfield, today you suck, can see you're only 10, right?
And all of you have read this here and probably made up your mind in terms of what this actually means.
Now.
if we used an out of the box transformer model that would be coming from uh say it read the entire Wikipedia, it might interpret this as being, Winful being a person who is ten years old. Now in the context of star stable, this would be very wrong. Winful is actually a horse breed and ten is most likely referring to the level of the horse in this case.
Now, the way we kind of made sure that our AI model was adapted to Star Stable was that we let it ingest data from Star Stable, from the community.
So just like, say, a community manager builds up this intuition of how people communicate in the game, the AI model was learning those different associations and connotations.
So when we get to the last layer where we're actually saying, what is hurtful behavior towards another user, the AI model that was fine-tuned to Star Stable would know that, okay, here you're actually writing to your own horse in this case, not another user, so maybe we shouldn't be triggering the message.
However, if we'd used a kind of out-of-the-box AI model, it probably would have triggered the message in this case.
So you can see there, even with this cutting edge AI technology, we have to be wary of what can go wrong.
And this is still very effective compared to, say, keyword filters, but it has its caveats as well.
Now, we had this so we knew when to trigger the message, and as Jane was mentioning, we needed a way to kind of measure this.
And I'm sure with COVID being around, we're all somewhat familiar with kind of clinical trials and how they work.
And you could say we used a similar approach here.
So we had a treatment and a control group.
Now, there were some considerations we had to make here.
So the treatment group would get the message triggered when they were writing something hurtful to another user.
The control group would not get the message.
However, StarStable already had some built-in safety features so that if you were writing something hurtful to another user, it would kind of get blocked out.
And we didn't remove that feature for the control group.
So both groups would actually still have their messages removed.
And the reason why we did that was to make sure that we're not exposing other users to more kind of a negative experience that they wouldn't have had otherwise.
And those are some of the considerations you need to kind of make when you're trying out these type of things.
Another thing that we did was also making sure that we could measure this in real time as the data was flowing in.
Because our hypothesis was that this would have a positive impact.
However, once out in the wild, things can change pretty dramatically and we might experience a really adverse reaction to this.
So we wanted to know that we could kind of hit a kill switch if something went really wrong or if things look really positive, we could actually roll this out even quicker and stop the actual test.
Now, we did this in conjunction with the community team, so we were looking at the quantitative data that was coming in to us in a live manner, but we were also listening to the community team that was getting the qualitative data coming in from the actual community, so seeing if there was any feedback coming in there. And I think it's very important to look at both those aspects as you're trying to do something like this, and you also have to be very wary of, you know, what potentially could go wrong when you're...
running this type of test.
But through doing this, we could actually measure how effective this message was.
And I'm gonna hand it back over to Jane now to kind of talk about the results that we saw.
So did we succeed?
And what was the conclusion after six weeks?
The first week, we did not see any significant differences between the two user groups.
But as the pilot proceeded, that would change.
Already after four weeks, we could see a significant difference between the two groups, which made us 99% confident that the message we displayed for the players was reducing the number of harmful messages in the control group.
Interesting as well, no players contacted our player support during the course of the pilot with other questions or complaints, so they accepted the pilot.
But we did see some interesting behavior in the chat with our players' first discovery in the pop-up.
Not all, but some will try to trigger it again, most likely as a challenge to see where the boundary goes before getting a ban or some other punishment.
So it could look like this, as we see on the screen.
In the first session, same player, user A, first say something harmful to another player, hope you get hit by the bus.
And then, oh, I get some warnings.
And then self-reflecting, how many warnings do you need before you get a chat ban?
and session two, I want another one, ah noob, and then got it. So, uh, quite interesting behavior from, from our players.
So with the two different groups, treatment and control, the length of the pilot, and the amount of chat messages sent during this period, we can with 98% confidence say that we did reduce 5% of harmful messages among our players in the UK.
Maybe it doesn't sound that much, but 5% translated into actual messages are more than 800 negative interactions not being sent.
And that done without policing, no banning, and instead encourage our players to stop, think, and do what a nice friend does. So I would say, yes, we did succeed. So what's next? In the game, we will see the Dark Riders take entrance.
But with the pilot, where do we want to go from here?
After the pilot was done, we're quite sure that this is one of the missing pieces in our strategy to create a friendly environment in our game.
In our overall vision, there are three parts that we call the moderation puzzle.
There are the rules and regulation element that ensure things that absolutely should not be in the chat are not part of the chat.
There's the self-moderation piece.
which players alert us when there's something going on in the chat.
And then there's the third part about encourage positive behavior in-game, which the pilots showed was possible.
So right now we are preparing to set up a long-term structure for this to be a permanent part of our in-game communication.
We have the vision and we do have the knowledge now.
So Coming to the end, some key takeaways, and I would say have a strong vision, makes sort of the key, and also a strong team.
And then I include both the internal team and, of course, good partners that can enable where you don't have the resources.
Yes, and think about how you can support and build skills on the players' terms.
So you created the game, but your players created their community.
So how can you, in your game, create guidance and help players behave in a more positive way without judging?
And last but not least...
If you have the right circumstances the right environment, nice people in online game do exist.
So that was the end, so I think we are in the Q and A.
Barely. I don't know if the. It's on.
Hello.
Hello.
I love the three-point research kind of framework that you had.
I noticed the phrasing, the first sentence.
I noticed you sent in a harmful message.
I thought of the first point was more of like self-awareness of an emotion.
So I know it's kind of hard to identify what they're going through.
Any kind of thought about what that first sentence should be, kind of like to fully get at that self-awareness framework?
Yes, so we had a couple of ideas on how we can use that research-based method and there are A-B testings that we hope we can do.
And one way could have been to say, hey, it seems that you've sent an unkind message.
Take a moment to stop and think how are you feeling in this moment and then go on and say, okay, well.
built the empathy skills, how would you feel if someone said this to you? What can be a little bit risky when talking about very strong emotions or big emotions is that we couldn't in that moment offer also a skill building to how to deal with that emotion. So we decided to focus on the change in behavior.
and focus a little bit more on the social awareness and the relationship skill building, although I'm sure there are moments where we could have had a stronger focus on the self-awareness instead.
Cool, that makes sense. Thanks.
Thank you.
Hello, can you talk a little bit about whether or not that 5% reduction was specific to certain players and you saw an actual reform in some of those player behaviors over time or was this just generically across the board regardless of specific player behavior?
We actually looked at this on aggregate across the two different groups.
What is interesting, what we saw was the most, so you would have a small, majority of players, you kind of have this one humped camel.
Majority of players might send a couple of harmful messages, one harmful message, and then you would have kind of like, a few players that were sending a lot more.
What we saw was that this group actually did not reform.
did not change behavior overall.
You kind of had the same kind of camel shape afterwards.
Those kind of insights kind of tell you that, okay, maybe we need another approach with this group.
You know, okay, on aggregate, we are actually able to change some of the behavior, but for this group, we might need some other type of intervention there as well, which I think is important since not every player is going to be the same and respond the same to this type of messaging.
And then as a follow-up, were there any long-term changes that you observed in player behavior that could be linked to this type of model?
Like, for example, were players who were consistently seeing this warning less likely to engage in the future, or was there no change in behavior?
So we ran this over a six week period and what we saw during that period was that you would have, are you referring now to the specific like harmful messaging or?
I mean more in terms of like retention of players.
Oh we didn't look at the actual retention of players in this case, we were more specifically looking at the harmful behavior towards other players.
Thank you.
Hello, first I want to say that I really like the methodology you guys use and this is the perfect population to use it on like a young audience that they're already learning social skills. How did you guys measure success of this methodology?
I guess that's where we wanted to have this more quantitative measure, to actually see that there is a reduction.
And I'm speaking a bit from my personal experience, coming from Google and trust and safety.
And it's a lot of the time you're trying to mitigate risk and kind of reduce harmful behavior.
And it's been difficult to measure.
I think one area here, which we can all agree on is like, how do we see that we have some success?
And that's why we kind of did this more.
had this more clinical approach in terms of A-B testing to actually be able to quantitatively measure that okay messages from this specific group were going down in terms of the harmful messages.
Yeah and I can add to that as a I have a very strong vision that as long as one child or teen or player is not being presented with this hurtful message that's a success for me personally.
And then obviously there are things that could be tested in terms of optimizing things and maybe, as we talked about, having another message for certain players, etc.
But I think for our pilot, the success was to actually see a decrease in harmful messages.
Thank you.
I can continue on that as well.
So yes, that was the goal in itself.
We could have done a lot of other sort of...
metrics to look up on, but we'd like to keep it simple.
We started to talk about a lot of messaging trees and it started to become complex, but we said let's try it, rather than to get it perfect.
I think one last thing to add to this is that the positive impact, I think, from this is that what we present here today, we hope that many of you in here, if you are a developer or you're working with community management, you can go back to your teams and you can show, hey, this actually works.
There's hard numbers here that can show that this actually can have a positive impact on your players.
Thank you.
I think there's the last question.
OK. Thank you. Very interesting pilot.
So, a lot of the neuroscience studies show that it's extremely difficult to change behavior without awareness first.
And also...
it's much more difficult to get awareness when you are in the heat of the emotion and the action. So have you been thinking of any like preventive or like reinforcing positive experiences even before like they deteriorate or any way to enhance like the trust?
you know, relationships, strong trusting relationship also helps to reduce this kind of behavior.
So it's like you almost try to tackle like the most difficult aspect, but like what could you do to build up to kind of, you know, reduce in advance?
What we do have in the game as well is ambassadors that are in the game and that do talk to our players and they also try to be nice friends and role models.
So I think that is some type of awareness and we do have guidelines.
And the game in itself stands for kindness.
We have one of our values is kindness is a superpower.
So we talk a lot about sort of friendship, kindness and what that can enable.
It's not just being sort of kind, it is a superpower and what does it entail.
So I think that is spring to some awareness, if that was what you meant.
Yeah, partly. It's also like...
You don't want to suppress negative emotions neither.
So it's more like learning how to deal with them, also when they arise.
But learning to do that in the heat of the emotion is super hard.
So it's, yeah, that's kind of what I meant.
Like, how do they learn to deal with negative emotions before they lash out?
So I think your question is around emotional regulation and what can we do to promote that in a game.
And of course there are many ways such as if it's losing a race for instance, you could have encouraging friends that say, oh next time we'll do it or like you still did a great race and just as we would in a real life setting with a lack of a better word.
And I think in terms of lashing out and behaving perhaps in a hurtful way, as I said in this pilot, we decided to focus on sort of changing a behavior and how to build the relationships, but I'm very curious to see what we can do also with emotional regulation.
And I think that question will be more around...
not minimizing the freedom of a player and not becoming too educational or being almost their psychologist in that sense.
So it's a tough question and I hope that it's something that we can work with in the future.
Great, thank you.
And thank you so much for listening.
Thank you.
