So my name is Dan Somali.
I'm a tools programmer at Guerrilla Games.
And together with my colleague, Sander van der Steen, I'm going to talk about how we rebuilt our tools pipeline during the development of Horizon Zero Dawn.
So for those of you who might be unfamiliar with the game, let's take a quick look at a trailer.
All right, so now everybody knows exactly what the game is about.
Let's continue.
So prior to creating Horizon, we worked on Killzone, which is a series of first-person shooters.
And when we were going to make the jump from building shooters to creating the vast open world of Horizon, it was going to be a big challenge for all disciplines within our company, and the tools team was no exception.
So we'll cover a few different areas of interest, but I guess the main takeaways of this talk are that you'll get...
bit of insight into why we decided to rebuild our tools and learn a little bit about how we approached this while we were already in production.
And we'll show a few ways in which we think that it paid off.
So if you're somebody who is responsible for maintaining a legacy toolset, or you're just somebody who uses one on a daily basis, this will hopefully inspire you to do something about that situation.
So yeah, to begin with, I'll talk a little bit about the tools that we use on Killzone Shadowfall, followed by a bit of background on the process of actually rebuilding our toolchain.
And then we'll go into some detail about our level editor, our 3D editor, and how actual data editing works.
And if there's time, I'll cover a few other tools that we have and finally share a couple of learnings from our project and a bit about our future plans.
So at the start of Horizon, we had a pretty fragmented tool set.
It was built upon a mixture of different languages and frameworks.
We had very long iteration times.
I mean, in a really bad case, it could be like, between a designer making a change and actually being able to see it in-game was like 20 or 30 minutes.
And to be frank, we had a lot of pretty unhappy users.
I mean, at this time, Unity and Unreal were becoming much more popular and visible.
So we had some pretty strong dissatisfaction with our users when they were comparing their workflows and features in Guerrilla Tools and comparing it to Unreal and Unity.
So just a quick technical detail.
At Gorilla we use an intermediate storage format which is called Cortext and we use that for all of our game data.
Almost everything except for textures and audio.
And before the data is loaded in the game, we transform it through a... into an optimized binary representation through a process that we call conversion.
So the primary editor that we had for Cortext was the Cortext editor.
Here's an image of the thing in all of its glory.
It's pretty old school.
Cortext editor displays the contents of a Cortext file as a 2D graph.
So...
just within follow resources with the file represented as notes on the on the cameras and references between those objects are represented as wires.
So uses to create and figure a lot of our game data but in terms of actual data editing it's pretty low level it's it's basically one step removed from just editing your cortex files directly in a text editor which incidentally is how a lot of the configuration for earlier kills and game was actually done.
So much of the user's energy was spent on the busy work of kind of creating and understanding these complicated resource structures instead of just being free to focus on the actual creative process.
This tool was written in C Sharp and WinForms and a Sony framework called ATF, which was recently open sourced.
And additionally, the code base for this tool was fantastically complicated.
There was, it required really a lot of effort to make even trivial changes in this tool.
So there was only a handful of programmers in the company who could actually even work on it, and there was basically nobody who enjoyed working on it at all.
And the next major piece of a pipeline was Maya, specifically our in-house Maya plugin.
So the plugin allows us to edit our game environments inside of Maya and then export them into core text.
And the plugin uses Decima's renderer in the viewport, so you get an accurate representation of how your assets are going to look in the game.
However, it doesn't run the entire game loop, so and also it leaves a lot of the game systems in an uninitialized state, which means that you miss a lot of the things that actually bring life to the game world.
This was built in C++ and Python and also uses the Maya API very heavily.
And much like the Cortex editor, there was only a handful of people who actually were able to work on it and also willing to work on it.
So the last piece was a state machine editor called SMED.
And SMED is a tool that generates Lua script, which is then executed in game.
And this was used by designers for much of the level scripting logic within Killzone.
So SMED began life as a prototype actually. It was created by a technical designer to see if it would be possible to produce a visual script editor for our game.
The deal was that if the prototype worked out and was proven to be okay, then the tools team would take ownership of this thing and make it into a real tool and then...
Yeah, we would go forward.
But even though the prototype was successful, that transfer of ownership thing never really happened.
And so at some point, this tool gained critical mass and the technical designer that made the original prototype was now responsible for maintaining an important part of the tool chain.
And this tool was written in Python.
So with the set up a designer who's aiming to accomplish a simple task is is forced to navigate between the sort of like Bermuda Triangle of lost productivity which involves like flipping between these 3 different applications manually copying string identifiers file names and values between them and the tools also didn't talk to each other so it meant that mistakes are only indicated way after the fact and usually by some kind of impenetrable error message when you try to look at the game just not not ideal.
There were also many other tools and utilities that were part of the daily guerrilla workflow, but there isn't really time to go into them here.
So one kind of tangential thing is that during shadow fall possibly out of just sheer frustration with the way that the tools pipeline was set up, the debug UI was created.
And debug UI was an in-game user interface framework.
It's a pretty feature-rich framework, but most importantly for what I'm talking about right now, it was something that programmers found very easy to use.
And because of that, we ended up with a large number of very useful in-game tools.
So I'm going to show one of those quickly, just so you can get an idea of what I'm talking about.
So this is our in-game GPU profiler and you can see it displaying timing values for various draw calls during a frame and it allows you to jump to different points in the frame to examine the render targets or the back buffer.
You can also see the inputs for the different samplers and so on.
And it has an X-ray view which you can move over the scene and get an idea of which intermediate buffers are contributing to the final frame and what they actually look like.
Yeah.
So that was a little bit about the tools and some of the technology, but what about the game itself.
So when starting out, we knew that Horizon was going to be very, very different to Killzone.
We hadn't made an RPG before.
We hadn't made an open-world game before.
And aside from that, this game was going to include new types of content which we just hadn't built before, like quests and interactive conversations.
And additionally, the tools team at Guerrilla is relatively small.
So depending on how you count it, we had like four or six full-time tools programmers for a team size of about 220 people.
So we had to be really efficient about how we used our time and our resources.
All things considered, it was pretty clear that our extant tools pipeline would not easily scale to meet the demands of Horizon, so we decided to start again.
So at this point, we were ready to define some initial goals for the new toolchain.
And we decided to start by building a tools framework, which was going to act as the basis of all of our future tools development.
And this was intended mainly to mitigate the arbitrary linguistic and technological differences between the tools and just enforce some consistency between all of them.
Decima, which I'll cover a little bit later on, is our in-house engine, and we wanted to leverage as much of it as possible while building the new tools.
And lastly, the debug UI had shown us that if it's easy to do so, then programmers will happily make tools for their game systems.
So by using the engine libraries and keeping the others code simple, we would increase the chances of getting buy-in from other teams and, most importantly, help from other teams.
And, yeah.
So just because for an editing environment.
We want to make sure that we build tools that would fit our content creators needs so that and we decided to build like specific editors for each discipline and we call these editing context and we built from usable components that we could then create new ones for other teams as they needed them and so these editing context would sit inside of it a single application where the content creator would be able to do all the work and this would become the integrated game development environment which we now call the decimator.
So before continuing, I want to provide a little bit of background on Decima itself.
Decima is, as I said, our in-house game engine, and most recently it was used for Killzone Shadowfall, RIGS, Mechanized Combat League, Horizon Zero Dawn, and is being used for the upcoming Kojima productions titled Death Stranding.
And like most engines, it's separated into layers.
So at the base, we have platform-independent game systems just above the OS, and this insulates the layers above from the operating system, which makes it just easier to port between different platforms.
It's also important to note that our engine has always had a full PC build for development purposes.
Next is core.
This adds rendering and mesh primitives, lights, particles, and so on.
And then game code on top of that, which adds more specific game features such as entities and crowds and cinematics.
So we wanted to put the tools framework on top of that stack so that we would have access to everything below.
Decima is a very mature code base, and it contained many libraries that would be useful for tools development.
So here you can see the kind of things that we were interested in taking advantage of.
One of the most important features was the engine's super optimized RTTI system, or runtime type introspection system.
which is exactly what we need to be able to manipulate data in an Editor. So aside from getting a lot of well-written code for free, using the Decima Engine libraries would also mean that the Editor's code would present a lower barrier to entry for the other programmers in the company when compared to the Cortex Editor and SMED and the Maya plugin.
So to fast forward a little bit, here's the Decima Editor as it exists today.
And this is our level Editor context, which is a 3D viewport onto the game and an Asset Browser at the bottom, a hierarchical outliner on the left, and an Attributed Editor on the right.
Pretty standard stuff that you would expect to see.
This is a shot of our visual script editor, which is used for almost all of our in-game designer-driven logic.
It has a 2D diagram renderer, which displays the logic graph of the script.
On the right, you can see it uses the same attribute editor as the previous slide.
And finally, here's a shot of our conversation editor or dialogue editor.
So it uses the same diagram render as the last view, but to show the logic graph of the conversation.
And this time, there's two new components.
So on the bottom left, there is a timeline for positioning stuff inside of a sequencer and a screenplay editor for writers who work on the in-game dialogue, which is on the bottom right.
So we...
We'd established some initial goals, and we actually needed to start moving at this point.
But the thing is, Killzone was already finished, and aside from a few people who were remaining on the project to support multiplayer and DLC, everybody was moving onto Horizon.
So production was already starting.
And despite the fact there was many issues with the Killzone toolchain, it did work.
You could make a game with it, and the content creators were familiar with those tools, so they didn't necessarily even want anything to change, which seems illogical, but that's just how it was.
And we also had no way to gauge how successful we'd be in this endeavor.
So we were going to have to start really small, minimize risks.
We couldn't disrupt the production of the game, and therefore we would need to continue supporting the old tools while we did this drive for new stuff.
So with that in mind, we decided to focus on a single team, which was our audio team.
And it represents a pretty small group of users.
At this time, it was around four people, and they have a very specific set of needs for their workflow.
So by essentially porting this workflow from the core text editor into the fledgling Decima editor, we would have an isolated test case for the viability of the framework as a whole.
So our audio team works primarily with NodeGraphs, and NodeGraph is a high-performance visual scripting language that we developed in-house.
And this is a technology that we, it generates C++ by the way, which runs natively on the target.
And this was something we used very heavily in Shadowfall, and it was being proposed as a replacement for SMED and Lua.
So this meant that if we were successful in porting this audio team's workflow, then later it would have benefits for other teams in production.
If you're interested in learning more about this technology, there's actually a talk from GDC 2014 by Andreas Varga and Anton Waltek, and they go into some detail about how our audio system works.
So as a reminder, this is the thing that we were trying to replace.
And this is our first prototype, which we built in C++ and Qt.
It used Qt's QPainter class for the diagram, displayed the nodes in a similar visual style to the old editor.
At this point, it wasn't very usable.
It was basically just like a read-only display for node graphs that already existed.
We also had some performance issues with Qt, so we decided that now we had access to Decima's renderer, we could just rewrite the diagram rendering using that.
So that allowed us to write the diagram renderer as a single, it draws a single quad and then it has some pixel shader magic to draw the entire thing.
It was super fast.
It can handle huge amounts of data.
Wasn't very useful though, because you still couldn't edit or save or anything, but it looked pretty good.
And this is what we have today, kind of skipping forward a few steps.
So once we added an attribute editor and a way to browse for files and save them and edit them and so on, we managed to convince one or two members of the audio team to just try it out.
And this was a really, really painful process, actually.
It required many iterations, and every time they came up against something that wasn't possible in the new editor, they would just immediately go back to using the old tools.
But we focused a lot of our energy on implementing features that they were missing and eventually achieve feature parity with the old editor, and these guys started working in the Decima editor every day.
So it was our initial test case and it worked pretty well.
We reached feature parity with the old editor much more quickly than we anticipated.
And because we shed this like onerous complexity of the old editor, it meant that we could implement new features very easily.
And most importantly, programmers from other teams were comfortable enough with the code that they were able to contribute some pretty awesome features.
But I'll come back to that later.
For now, I'll hand over to Sandra.
OK, thank you, Dan.
So the Node Graph editor gave the tools team the necessary confidence to pick up the next larger project.
And that would be creating a level editor, or 3D editor, as we call it.
So in the next section, I'll explain some of the early design decisions that led to this new editor.
So why create a 3D editor at this stage, you might wonder.
As Dan mentioned, we were already in production, and so this is fairly ambitious.
Why did we not just simply invest more into our Maya plugin?
So first of all, the first reason was that Horizon Zero Dawn needed a lot of Quest content, which is a somewhat new content type coming from the Killzone series.
So the game design workflow that we had in place in Maya was really the worst part of our Maya plug-in in terms of usability.
Maya is simply not designed to do any level editing.
So secondly.
the 3D editor, a 3D editor like Unreal Unity would eventually benefit other workflows as well.
It is a formidable task, and if we hadn't started it back then, it would have never been able to finish it in time for Horizon in the first place.
And finally, the game design team offered a very achievable starting point for us to work on a 3D editor.
If we could facilitate only object placement in the game, we would already provide a huge workflow improvement for the game design team.
So now that we've established why we wanted this 3D editor, let's have a look at how we can actually achieve this.
Looking at what we have available, a tools framework in its infancy, and a game executable, we are immediately presented with the first choice.
Do we integrate the desired game subsystems into our editor, as illustrated on the left?
Or do we actually merge the full game into our editor environment?
So let's zoom into using only game subsystems.
So this is the advantage of being able to tailor make your game loop based specifically on your editor needs.
This leads to a very controllable viewport, skipping unwanted game subsystems.
However, doing this will introduce a second code path where these systems will be used, leading to code complexity and potentially disparity between what you'll see in your editor versus what you see in the final game.
Because more code is required, it will also take longer to develop.
So the alternative then is integrating the full game in the editor.
Game systems would truly function as they would in game, and as an added bonus, we can use the earlier discussed debug UI for free to give us a head start.
We would also be able to play in the editor just like we would be able to do in game.
As a disadvantage, though, it does introduce more overhead, as a lot of game subsystems are usually not required for certain editing operations.
An example of this would be a weather system.
It's typically not very useful for the game to be raining when you're actually just placing an object.
It's both a hindrance for performance and usability.
So given the time considerations that we had at hand, remember that we were already in production, we really felt that integrating the full game would be our best option.
So now that we want the full game running in the editor, how do we actually achieve that?
We're faced with another decision.
Do we run the game in the same process as the editor, or as we depict on the left here?
Or do we attach the editor to the game executable and use an out-of-process solution?
Again, we examined our options.
An in-process game is faster to set up and easier to debug because all data changes are direct and synchronous.
A crash in the game is problematic, though, as it will inevitably take down the editor, which will cause loss of work and loss of frustration.
It also makes it easy to modify in-game data directly, as you can access game objects via pointers.
Over time, you could end up with a situation where many different systems in the editor would access different systems in the game.
We felt that this would lead to an unmanageable situation, but enforcing this separation, we figured that would be hard, because due to time constraints, it's ever so tempting to just directly access a game system.
An out-of-process solution, on the other hand, offers more stability, primarily because you would be able to recover from a crashing game.
Because the game is a separate process, coders are forced to work in a cleaner way.
as the editor cannot directly modify game objects via pointers.
Finally, UI updates are not tied to the frame updates in the game and so your UI will be more responsive, even though the game might not be.
So the downside of this is that you have to write a communication layer, resulting in a longer initial development time.
This communication layer would also be asynchronous, which make it quite tricky for debugging.
So similar to the decision on integrating the full game, we went for an in-process solution because of the time constraints and pressure from production.
We did decide that an out-of-process solution should always remain an option later on, and so wanted to ensure code separation.
To enforce this code separation, we developed the 3D editor as if the game was running in a separate process.
No data was to be shared between the editor and the game.
Because of this separation that we quite strictly enforced, I'll continue to use the word process to describe, like editor process, meaning code executing in the editor, and game process for code executed in the game.
Anyway.
Luckily for us, making these early quick gains paid off quite well.
So I have a video I'm about to show you here from roughly a year after starting in 3D Editor.
And it demonstrates how a game designer uses the ability to play in the editor for his workflow.
So to start off, the designer explores some section of the world in play mode, decides this is a nice spot for an encounter, and so goes into edit mode.
Now there are some trees in the middle here.
So what we're going to do is we're going to paint them out to clear the area a bit.
And we also want to paint some more trees in the surroundings.
So we're just going to do that.
And then in the center of the screen where you want to place this encounter, you want to flatten the terrain out a bit.
So we use some terrain sculpting magic for that.
And so this looks like now we can actually place our encounter.
So we use the content hierarchy on the left to create an encounter, give it a name, create it in the middle, use the asset browser also on the left to create some robots.
So now we have two robots and we want to add a rock in the middle, provide some player cover perhaps.
I'm not a game designer, so I wouldn't really know.
But yeah, so we just use the tools to drag drop the rock in, place it.
Save it, going to try it out.
So now we're immediately back into edit mode.
And yeah, Aloy can shoot some robots and see if this actually works out well.
Pretty sweet.
So with our high-level design now discussed, let's look into some subsystems that we required for this 3D editor to actually work.
So in this section, we'll also see how we deal with the separation between editor code and game code.
So a problem that you encounter quite early on when then diving into this solution is that the data that is in the game is not necessarily the same as the data that is on disk.
This is caused by the process called conversion that Dan explained earlier.
Examples of what can happen in conversion are shader compilation, and mesh optimization, et cetera.
So having this optimized game data is a fairly common situation in games, but it provides a problem for editing as information is lost.
Saving out cortex based objects that are read by the editor is not always possible once you're in the binary game data.
So luckily, we already stated that we did not want the editor to operate directly on the data in the game.
As a result, the editor works on the cortex counterpart, if you will, from the loaded binary data in the game.
Each process, therefore, has its own copy of the data, and it cannot access the copy of the other process.
So data in the editor is not identical nor shared between editor and game.
A central concept in dealing with this problem is something that we call the editor node.
So editor nodes provide all editing functionality for an object in the game.
An editor node knows about a game object, but this is a one-directional relationship.
So this is a convenient way of keeping the game code clean.
The editor node lives in a separate project and code file from the game, and so the game code stays very clean.
There's no hash ifdef editor in our code base.
Second problem caused by the game operating on the converted data is hierarchy.
For performance reasons, the game has little hierarchy of objects left remaining, resulting in a long, flat list of objects that are all in world space.
This is very fast and efficient for rendering in the game, but not very good for editing, as often we want to edit in local space.
So in order to restore some hierarchy of objects for editing in game, we built a hierarchical tree structure of these editor nodes.
So this is done once the game is loaded and the user enters the edit mode.
The tree is constructed top down, starting at the content root, and reflects the content that is currently loaded in the game.
So having a single editor node tree, as shown on the image here, would violate our earlier stated goal that the editor cannot directly modify game data.
The tree structure is built up in game, and so it should only be used by the code living in the game.
So in order to facilitate editing hierarchical data, we actually have two editor node trees.
So this allows both the editor and the game to reason about content hierarchy and do world or local space conversions, et cetera.
So these two trees are structurally identical, but reference different data.
Changes made in the editor are mapped onto the node tree and communicated to the game.
So what we actually communicate in our editor is the next part of this presentation, and for which I'll hand over back to Dan.
All right, so making changes to objects.
When defining goals or requirements, it's sometimes important to list things that you know you don't want.
And in our case, after working with the Quotext editor and to a lesser extent the other legacy tools, we had a pretty good idea of what we didn't want.
So we didn't want to have any kind of proxy representations for game objects or string-based accessors or editor-specific boilerplate in the code or any kind of code generation steps in the build pipeline that would make the editor work.
And that helped us to define some goals when it came to how we wanted to actually write the editor code.
So...
In the editor code, we wanted to just work directly with the concrete objects that the C++ classes and structs that were defined in the engine.
And data modification by the code should just be performed with direct member access or via accessor functions.
And mostly as a corollary of these two goals, we should be able to reuse any kind of existing data transformation utilities that already existed within the engine code and within our conversion pipeline.
So doing these things would mean that our game and engine programmers would also understand the editor code and would find it easy to work with.
So being able to make changes is good, but what is best in life?
In any kind of data editing environment, it's critical to be able to undo changes that have been made, and this allows users to experiment, to recover from their mistakes, and generally feel like they're in control over what's actually going on.
So, looking at this slide, like the first goal seems almost redundant to state, but it is actually important.
The code for the undo system had to be as simple and reliable as possible, and in the interest of accomplishing these goals, we decided to avoid the kind of common...
uh, command pattern which involves a virtual do and undo function.
Um, because, well, there's two reasons for this.
Like, the first is we didn't think the undo system or anything related to it should need to change in order to support new editor features.
And we wanted to remove the possibility of having asymmetric data transformations.
That is to say, an undo implementation that isn't the exact inverse of its do implementation.
So given that we're approaching this whole thing from a completely clean slate, we may as well consider a totally ideal scenario.
And in our case, that would be a system that just sees all the changes that are being made in the editor, knows how to undo those changes automatically.
So we investigated this a little bit, but quickly found that such a system involved way too much overhead.
And it was also pretty difficult to delineate when complicated changes were starting and ending.
So the compromise that we settled for is that before modifying data, a programmer will provide some hints to the undo system about which objects are going to be modified, and then perform some modifications, and later indicate when those modifications are complete.
So it's actually described changes in the editor.
We use a transaction system, and the transactions consist of commands.
And you can see a pretty simplistic example of such a thing here.
But each command describes a simple individual change, such as the addition or removal of an object or an individual change to an attribute value.
And these changes are annotated with RTTI so that we have enough information to actually use these transactions to transform the editor data.
And the transaction system also has a subscriber mechanism, which allows you to receive notifications for all data changes that are happening in the editor.
And this system is mostly used to update the user interface inside of the tool, have the visual representation of the data remain in sync with the backing data.
So when a transaction is created or applied against the data, It is sent around to all the registered listeners in the editor.
So in this case, that would be the individual components that have subscribed for notifications.
And each listener is free to respond to the data change however it likes.
But typically, a controller will inspect the transaction to see if any of the data which is being changed is represented in the view, and then it will just update its visual representation.
For our views, we follow the Model-View-Controller pattern quite strictly, which has a few advantages in such a system.
Mostly, code separation in that our components don't need to know about each other or even...
communicate with each other.
They're only interested in representing data, modifying data, and responding to modifications in data.
It also means that the components support changes from anywhere, so they can be representing a change that's made by themselves or by other components, or even framework-level operations such as reloading of files and what have you.
And it also means that because of this, there's no notion of directionality in any of the editor code, so views aren't concerned if this operation is a do or an undo or a redo.
They just respond to the data changes.
So one of the things we tried in the early days when we were playing around with the system is to actually build these transactions by hand and then apply them against the data in order to effect the change.
And this is possible, but it actually proved very quickly to be way too cumbersome to be viable and it didn't fit with our ease of use goals in coding terms.
So instead, we built a utility class which would just analyze changes after they'd already happened and generate a transaction that described all the changes that it had observed.
So, before making a change, the programmer will use this diffutil to take a snapshot of the object state.
And a snapshot is just a shallow, attribute-wise copy of the original object.
And after that, the code is free to make any kind of modification to that object.
And once those changes are complete, the programmer will tell the diffutil that he or she is finished with the changes, and the diffutil will compare the original snapshot against the current state.
And...
This works by using our RTTI system, so it reads the attribute values, and therefore we only see changes on data that is actually exposed to the RTTI system.
So it's more fine-grained than just doing direct memory comparisons.
Yeah, so how does this actually look in code?
As you can see here, we have a super handy function that's gonna give us a pointer to a spotlight that we intend to modify.
So we call that function, and then we create a diffutil on the stack, and we use it to take a snapshot of the spotlight's current state.
So then we just perform some kind of modification.
In this case, we use an accessor function to set the cone angle to 30 degrees.
And then afterwards, we tell the diffutil that we're done by calling commitChanges.
And we also supply a human-readable description of what we just did, which can be shown to the user in the user interface, for example.
And then the diffutil will compare the snapshot against the current state, and it will produce a transaction that describes that change.
And that's the only interaction that a programmer who's writing editor code actually has to do with the undo...
has to perform to work with the undo system.
So once we have a system that describes changes in this super granular fashion, it means that writing code to undo those changes is actually trivial.
So to revert a change, we take our original transaction and we reverse the order of the commands and then we replace each command with its inverse.
So, for example, an add object command becomes a remove object command, a set value command will use the old value instead of the new value, and once we have this transaction, we just apply it against the data and the data is now back in its original state.
Easy.
So in the same way as before, when we have an undo transaction, we just send a notification about that transaction to all the subscribed listeners, and the whole system just works without any kind of code changes.
So what went well with the system?
Well, it was very, very reliable, and it's a transparent system, at least to the programmers who are using it, and it can represent any kind of complicated changes.
Because our in-editor views are only ever responding to transactions, there is no distinction between do and undo in any of our editor view code.
And I guess what's most telling about this code is the fact that it hasn't really changed all that much in the last couple of years.
I mean, we wrote it. Of course, there were some things that we didn't figure out in the beginning, so we had to tweak it a little bit.
But at some point, it worked, and it just has continued working for, like, two and a half years or three years or something.
That said, there are a couple of things that are not super great.
So...
The system relies very heavily on having transactions which describe all data changes that are occurring.
However, as a programmer, it's pretty easy to forget to surround your changes in a diffutil before you actually start directly modifying the object.
And this can lead to some very subtle irregularities in the timeline of the history stack.
We perform some validation on transactions and debug bills to try and check the changes and make sure that everything makes sense, but of course we can't catch everything.
And lastly the system doesn't support any file system changes which means that if a user moves files around using our refactoring tools the system doesn't support undoing those changes.
But this is something that we're actually looking into fixing right now and we'll try and get it working for our next title.
But actually now I'm going to hand back to Sander who will describe how this works with the 3D editor.
So let's map this on to everything I explained about the 3D editor before.
So when we make a data change in the editor, it results in the transaction being broadcast.
So the editor node tree living in the editor listens for these changes and picks up this change.
It determines which node in the tree is affected by the change and sends this information to the game along with the data change itself.
So the corresponding editor node in the game then receives the change and modifies the game object, and thereby completing the edit.
So with the information I've given before about having 2 editor node trees in our process and the data differences between the editor and the game, the previous diagram is obviously a bit of a simplification.
For starters, data in the editor is in local space, and objects in the game are in world space.
Let's have a look at how the node trees allow us to deal with these changes.
So let's say we modified the local space of the spotlight object, my spotlight, in this little hierarchy you see here.
The spotlight has a parent object, myPrefab, which for this example will be the root of the tree.
Each node in the tree can act as an adapter for transactions flowing through it.
To facilitate this, all transactions are applied at the root of the tree.
In this case, when the local space changes passes through the prefab, it appends its own local matrix to the transaction.
So the transaction then being passed to the next object, which is the final object in the spotlight.
It can do its modification.
And because the data has flown through its parent, the data transaction will now be in real space.
So because these steps are recursive, we return back to the prefab after successful application of the change in the spotlight.
Should an ancestor node need to respond to changes of its children, this is the place where you could do it.
More advanced objects might trigger updates to vegetation placement or terrain, as we'll see in the following video.
So in this video that I borrowed from my colleague, we're actually moving a road node.
And road nodes are part of the content hierarchy.
And its parent node is a road system.
And that actually knows how to modify the terrain.
So moving these roads, as you see, actually updates the vegetation and the terrain systems in the game.
This is a good example of how the EditorNodeTree actually facilitates this.
So to the observer viewer, there is a piece of information missing here.
Tools like the ones we've just shown actually draw a gizmo in the game and allow the user to interactively operate on it.
Plenty of tools work this way, such as, for example, moving, but also painting, et cetera.
So how did we implement this in the framework?
Because we already know how to describe a data change using transactions and a diffutil, in an Eve implementation, we would simply reverse the order of the transaction and support the following data flow as well.
So reading the diagram from right to left, the tool sends a transaction to the node tree in the game, which then transmits its change back to the editor, where we do the same modification on the cortex data that we have just done in game.
So we did not do this, because this is problematic for various reasons.
First of all, it becomes unclear which process has authority over the data.
Secondly, because of data loss in conversion, it might be impossible to have this code path in the first place.
And thirdly, and lastly, you would need some system to prevent a transaction being generated in the editor to flow back to the game again and then create a cycle.
So because of these problems, we implemented tool support or gizmo support somewhat differently.
The key here is that the tools do not actually modify the data.
They simply broadcast an intention to the editor, telling the editor what they'd like to change.
It's up to the editor to actually make the data modification itself.
This data modification is then processed in exactly the same way as any other transaction.
And that way, the editor keeps authority over the data at all times.
So it also automatically works with undo, as the input for the editor data modification on the left is then the undo system instead of the tool.
It is still the editor node in the game that modifies the game object as a result of a transaction in the editor.
The game itself has no notion of the undo system at all.
So there's one part in updating a running game that we have not looked at at all.
And that is structural changes to the node tree itself, adding and removing objects.
So this is done by a separate system that we call GameSync.
So GameSync is a continuation of a system that we had in place before we started working on the 3D editor and was written for hot-loading particles into a running game.
The system has been heavily expanded during the development of Horizon, and it works by tracking all changes in the editor, creating a patch file, and then inserting that into the game.
Since we like working with spotlights, let's copy the spotlight in this hierarchy and see how GameSync works.
So first, the incoming change is analyzed.
The analysis phase determines which objects are modified and what other dependencies are required for successful application of the patch.
So these dependencies are extra objects that are required for the conversion process or extra objects that might be required for the targeted game subsystem.
The analysis phase writes out a single cortex file containing all required objects.
So in this example, it contains the prefab as it will contain a reference to this new spotlight.
And obviously, the file will contain a new spotlight itself.
So this cortex file is then converted into a binary file and saved to disk, ready to be loaded and inserted into the game.
The game is then paused at a sync point, meaning no streaming is happening and all threads are paused effectively.
So applying the patch means copying the objects in the patch file to the affected game systems.
The target objects in the game are overwritten, meaning that any cached pointers in the game actually will remain valid.
Finally, after applying the patch, the game and all of its subsystems are resumed.
Recapping on how GameSync worked, from a tools team we have mixed feelings.
On the one hand, we have a solution that accepts all changes throughout the editor, and even changes on disk can be tracked by monitoring the file system.
It can also sync patch files to a dev kit, allowing a user to see edits on the target without having to restart the game.
On the other hand, getting GameSync fully reliable proved impossible throughout the project.
Because game systems predate game sync, game sync tried to respect these systems as much as possible and thereby avoiding work for other code teams.
This led to special case code creeping into these systems and very complex analysis code.
These complexities further then complicated or generated performance problems where worst case applying a relatively simple change could take as much as five to 10 seconds, breaking a user's workflow.
All of these complexities also led to inconsistencies in the results as well.
Game sync could crash or produce incorrect results, such as objects being not placed incorrectly placed in the world.
And this then led to a very low level of confidence from the users, making it high on our wish list to rewrite for a next project.
But let's not end here.
For the final part of this presentation, I would like to give the word back to Dan so he can give an overview of other tools that we have developed.
Those out.
Okay as I mentioned earlier we managed to get some help from another team while working working on the note graph editor and the most significant contribution came in the form of a fully featured debugger which was able to show debug information in real time from active instances of a graph which was running within the editor or just within a connected instance of the game running on target.
And so the debugger had all the kind of usual features you'd expect to see such as variable inspection execution visualization and.
It was also gathering this information over time, so this meant that the editor could then facilitate historical debugging and it would highlight previous executions on a timeline where the execution flow had changed.
So it meant that it was very easy for users to kind of scrub back in time and figure out when some particular condition was met or some condition had changed.
So this debug functionality turned out to be super useful during the development of Horizon Zero Dawn, and it spawned some similar debuggers for other systems, and all of those debuggers and visualizations were made with very minimal involvement from the tools team, which for us was a pretty good indicator that we were successful in making the tools framework something that could be used by the rest of the programmers in the company.
So here's a video of the note graph debugger I was talking about.
You can see how the graph is currently executing.
This is actually the graph sound for the main character's breathing.
You can hover over inputs on the graph and see what is actually going on, how the value is changing over time.
and you can see the execution path being visualized, but you can also pause the execution of the graph in game, and then start to look at previous executions, and you can see how long they were taking in microseconds and so on, and you can actually then step through those executions and see when the control flow changed and flip back and forth and see how the values are actually working in this graph.
Something else that was completely new for Horizon was interactive conversations, and With this feature, we were not beholden to any kind of existing workflow.
So it meant that we were free to just build a new editing context from scratch to meet the specialized needs of the teams that were going to be using this tool.
So that would be the writers and the quest designers and cinematic artists who would actually make the conversations within Horizon.
So here's an overview of the editor itself.
It's kind of hard to read, to be honest, but we have a graph that just controls the flow of the conversation.
So the main character is speaking to somebody and they are presented with dialogue choices and the graph can also check for conditions like have you accomplished a particular quest objective and so on.
Let's take a quick look at a conversation in Horizon.
You gave them the best work you'd ever do.
And look at it now.
They defaced it because they hated what it stood for.
They defaced a thing slaves like him lived and died for.
Slaves they've already forgotten.
Short-sighted bastards, the lot.
If they thought their son set off this bridge, they'd jump after it.
Ah...
I thank you for honoring my wish.
I hope you can find peace.
Okay, so that was actually from quite a while ago in production.
But what's interesting about this conversation is that it's completely auto-generated.
So, I mean aside from the audio, all of the animations and camera cuts and gesture beats and so on were selected by a data-driven rule-based system.
And the...
Auto generation system will also generate a robo voice and facial animations where required.
So basically it means that a designer could just produce game ready content by typing in a few sentences for a conversation and then immediately see it in the game or even just game sync it into an active instance of the game and get quick feedback on how their actual flow of the conversation is actually going to be.
The system was pretty heavily inspired by the work of CD Project Red and BioWare who've both delivered talks in the last several years about how their conversation systems work.
If you have any interest in the subject, I highly recommend you check out the talk from last year's GDC, which is shown here about the Witcher 3's dialogue systems.
But this is something that we were only able to do because we'd built this new framework that meant it was easy for us to experiment and just try out new ideas.
So something else is the profiler framework.
So at the beginning of the talk, I showed a video of our GPU profiler, and this is a video of our global profiler, which is for profiling on larger segments of time, more along the lines of milliseconds.
But we had this profile framework already running in-game and we were able to expose it to the editors that we were able to start getting profile data back from any tools basically.
So we can see like how much time is being spent in certain areas, we can drill in really quite to quite small time scales and get an overview on what's actually being done in the editor or the game.
So we expose this profiler framework to the editor and this allowed us to build a front end for the profiler, which can show profile data in real time from any tool.
And this of course includes the game, but it also applies to the executables that we use to run our conversion process and any of our kind of demons that are running to facilitate our other tools, framework stuff.
And it's interesting to note that the editor also runs the profiler framework, which means that it can profile itself.
in real time which is super handy for being able to investigate performance issues quickly or just figure out why the editors running slowly on like one guys machine.
Something else we worked on you is a world map and one of things about working on an open world game is it can be pretty hard to get an overview over where things are happening so to that and we started working on a 2 dimensional map representation of our game data.
displaying game data on the world map it's used in game is a pretty logical progression and thanks to a plugable overlay system. It's possible to create visualizations for whatever we want basically so right now we use it for showing stuff like the locations of quests or the location of Jira bugs and it's pretty handy to get a quick quick eye on what's actually happening in the game.
But unfortunately it came on pretty online very late in the project and it wasn't really widely used but it's something we'll probably continue to work on pretty heavily for the next project.
There's a quick video of that.
This is just a view that shows some world encounters.
So you're able to kind of filter and see where you might run into particular types of robots within the world.
And again, there's filtering, so you can get a quick overview of what you actually want to see.
There's another overlay that we have which shows our performance stats for each tile, which is gathered on a daily basis.
And this shows whether our tiles are in memory and within CPU budgets and GPU budgets and so on.
It shows tiles that are in budget as green and tiles that are out of budget as red.
And unfortunately, this screenshot is from very late in production, I think right before Gold Master, so everything is green because we were in budget.
But we can drill into that tile and then get an overview on the different parts of performance stats that have been happening over time, over very long periods of time, because we gather these stats always.
So the last thing that actually came up was...
Now that all of our tools are written in C++, it meant that a few of the programs at Guerrilla had the idea of creating a C API for the tools framework, so we could use this to expose certain areas of the tools framework's functionality to other languages.
And so, with that in mind, we exposed stuff like file loading and saving and object manipulations, and then we wrote a wrapper in Python.
for the decimal API which allows tech artists and designers and anybody who knows most Python basically to create scripts that will like create game content or modify existing game content and they use these the same tools framework functionality that we actually used to build the editor.
So revisiting the C++ code example from earlier we can look at the same code which is written in Python.
And yeah, we retrieve the spotlight to edit.
And in Python, we create a transaction so that we can actually start modifying data.
And then we just use some accessors to modify the cone angle once again.
And then we just commit the transaction.
And this is how somebody would do the same things that we do in the editor with Python.
There's also a lot of other really cool stuff that I would like to show today, but there just isn't enough time.
So I'll move on to conclusions.
So now that the game is finished, we have some time to kind of tick stock.
And we realized that we were much more successful in this whole endeavor than we anticipated, but there's still a lot of work left to do.
We'd like to bring more of our team's workflows into the new editor.
A lot of our environment, our set dressing is still being done in Maya.
Some of our VFX work involving particles is still performed in a standalone particle editor tool.
And so...
We'll start building you at a contest for these different workflows and continue to move everybody in the company into our editor.
And it's under mentioned we'd like to rewrite the game sync system.
So it's a source of a lot of mind boggling debugging problems throughout the project and.
we want to 3 is out of process that we can stabilize the editor because even though we we had a really quick start with the 3 editor and a lot of progress it meant that when we came towards the end of the game and and the project story in the game was being optimized very heavily that had a really big impact on the stability of the editor. So we'd like to separate this process so that we don't lose start this use his work from crashes in the in the game.
And although we made a lot of progress on this project for the tools user and space building was done in like let's say an organic fashion which means that a lot of tools and concepts of the editor as user interface are kind of confusing or irritating and users find it pretty hard to discover new features and so we want to work on the stuff quite a lot in the future.
Let's say thanks to everyone at the gorilla who helped out with the stuff and some special thanks to the people whose work you've you've seen here today.
And.
Also, we are we're hiring so if you're interested in working with What is frankly just a bunch of really awesome people with a lot of talent?
Then please click that link and send us your resume and we'll take a look But yeah, that's it. So does anybody have any questions?
Thank you.
So after seeing some video content, like just mass market content, on how they were changing the workflow of Decima Engine in previous titles to fit the new Death Stranding title, I was wondering, is there anything in particular that you learned from that experience or that was integrated as a result of that experience with the Kojima Productions team once they came on?
Yeah, I mean, those guys were working for many years on their engine, and Fox Engine had a lot of cool capabilities.
So we learned a lot of technical stuff from those guys, but in terms of workflow changes, they're much more focused on cinematics than we were.
So our cinematics pipeline was something that we analyzed quite heavily to understand how we could improve it to fit their workflows where they wanted to have very long sequences of a lot of complicated interaction between many characters.
So that was a pretty big part of it.
But yeah, that's about it.
So a couple of questions.
One would be.
How did you get buy-in from production and management, or were there any problems with it?
And also, in the early days when you were working on stuff that wasn't yet usable, were you having to support two systems for people editing the game?
Yes.
So, the question is, like, how do we get buy-in from other teams, and how do we actually, like, approach the process of dealing with two separate parts of editing?
So getting buy-in from management wasn't that difficult because we had the backing of our technical director to just push forward with this that we thought, in a way that we thought made sense.
Getting buy-in from our users was much harder because the tools were really kind of crappy for like a very long time.
And that meant that nobody wanted to use them.
So we had to invest a lot of energy in just making sure that we met all the demands of like reaching feature parity for that stuff.
In terms of editing, having two different systems of editing, that was kind of a pretty.
tricky tricky thing is very hard to summarize shortly because there's like many different angles that was always different systems and we have in the game and moving on to new data formats in the new editor would make anything much easier. But I can give you some specific examples in more detail but it's it's very very hard thing to to summarize.
mainly like all the old tools pretty much had a feature stop once this got rolling.
So we did support them.
I mean, as Dan ended his presentation, not all teams have moved over to the new editor.
But yeah, the new features for the old tools is zero or very little since we've started this.
Hey.
Thanks for the talk.
do you have any um like uh data messaging uh steps like you know like a pipeline step or something like that and how does that work with the like you know the diff patch file that you do or is that actually what you mean in the uh the game sync stuff where it's actually done on the game side where you know the the the the editor data has to be kind of messaged like you know strings need to be hashed and.
all the things need to be generated for the run time to work.
Okay, so you're talking specifically about how the editor data is actually transferred into the game and how that transformation process works?
Yes.
Yeah.
Okay.
So we have a conversion pipeline that is basically just lazily converting the graph of content that is going to be loaded into the game.
And that conversion process can involve basically anything.
I mean, in the simplest form, it's just that we take some pre-converted data and write it to a binary stream in accordance with the RTTI schema.
But in complicated cases, it can be things like where we do a lot of baking operations and then write out Stream in a certain way or whatever, but this is something which Game systems and game objects are free to implement. However, they want so there's a lot of different things happening there basically, but so that Runs in the game process, right? It technically does but not not inside the game instance So we use the same executable But it's it's a different instance like we rename the executable file and then we run the conversion process so that will basically just eat up the core text files and spit out core binary files.
And those core binary files are then actually loaded by the game.
So this is essentially an offline process, but it's done on demand.
So as you're working in the editor or whatever, and you load up the game, the game will make a request to a convert worker for this operation.
It will convert the data, and then that will get loaded into the game.
Cool.
Thanks.
I had a question about a little less technical, I guess, logistics of making this happen.
And so you guys started out with one prototype.
Was that pushed from like engineers to higher levels?
Like did you pitch it or was it like an order from up there somewhere?
Like how did you guys go about that?
We don't really have like a sort of...
strong hierarchy or something and the real so it was nobody saying like hey you need to make this or something it was more like as a tool seen we discussed it quite quite extensively and we look to the options we're faced with and it was kind of like the new game was so big and complicated that while it would be technically possible to make it with all tools it would just require so much effort that we decided we should at least explore making a new editor and trying that out so that was something that we just sort of did and it was something we a conclusion that we reached as consensus as a team basically.
I had a quick question about the basic stuff like the menus and the docking panels and the trees. They were all rewritten using this That's the framework that that was started. There was nothing reused from OS code or you know I'm trying to say Well for all of the kind of like lower level stuff like File handles and what have you that was already present inside of the decimal engine For the user interface, we have sort of two different parts.
We have our own custom control framework, we call it, which is just simple things like buttons or any kind of two-dimensional rendering.
And then for the most of the bread and butter controls of the user interface, such as buttons and menus and whatever, we use a framework called Qt, or Q-T, depending on how you want to pronounce it.
But yeah, that's it.
All right.
OK.
One more.
Quick question.
So at what point of the process did you decide to create the Decima API?
And how much of an undertaking was it to create the Python wrappers for your tech artists?
That was something that happened very recently, actually.
I don't recall the exact date, but I think it was within the last two or three months or something.
And in terms of getting it online, it was a very easy thing to make, in a way.
It took much more time to really drill down and understand exactly what this thing was going to do and how each API call was going to look and whatever.
And then one guy just sat and wrote it, and it took him, I think, a couple of weeks to make the thing.
Cool.
Thanks.
All right, well, I think there's no more questions, so.
Thank you.
Thank you very much.
