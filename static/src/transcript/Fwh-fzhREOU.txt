Welcome to this presentation.
My name is Timo HÃ¤nepurula, and I'm going to be talking about modernizing rendering at Supercell.
So, first of all, I want to remind everyone to please turn off their phones if they're potentially making any sounds, and remember to fill in the review forms after the presentation.
So I'm going to start off the presentation with a short history tour on technology development at the company.
After that, I'm going to be talking about the actual process of rewriting our core renderer and porting all of our games on top of the new system.
Then, I'm going to be talking about the new system itself, going to give an overview of its architecture and talk a bit about the feature set.
And finally, I'm going to go through some of the learnings from along the way.
But first off, a little bit of introductions.
So the name was already mentioned, Timo Hennepurla.
I've been a Supercellian since March 2020, and ever since I've been focusing on building our shared rendering capabilities.
So my career actually started in the IT industry.
I did that for about five or so years, but I always wanted to be in the game development industry.
So I decided to join Bugbear Entertainment and work primarily on the game Wreckfest.
After Bugbear, I joined Next Games as the lead programmer on The Walking Dead No Man's Land.
Before Supercell, I was at a small startup called Reforge Studios.
So Supercell was founded in 2010.
We have about 420 or 430 people, give or take, globally.
And we're best known for games such as Hay Day, Clash of Clans, Boom Beach, Clash Royale, and Brawl Stars.
So now let's talk a bit about the history of technology development here.
So we actually started developing our games in Flash.
Our first game, Gunshine.net, was made on Flash for the Facebook platform.
And Facebook was the first component in our multi-platform strategy back then.
But we soon realized that Gunshine.net was not going to be the huge success that we had hoped it to be, so we decided to kill the game and pivoted to tablet-oriented mobile, which was really on the rise back then.
So we needed something native to run on iOS and Android, because Flash just wouldn't cut it and the options back then were quite a lot more limited than they are nowadays.
We also wanted to continue authoring in Flash because we had a lot of experience in the pipeline, and we also had a lot of content coming up in the pipeline as well, so it made sense.
So this was the birth of what we call Titan.
So we developed this pipeline for converting Flash content into our in-house format, and a rendering system for rendering this content, and we call this Stage.
Stage is closely modeled off of the Flash API.
Stage is also part of Titan, but Titan is quite a lot more than just rendering, or stage in this case.
Titan has a lot of components that are utilized by the game teams to implement the game functionality, components that they might not necessarily want to, or it doesn't make sense for them, to develop on their own.
Historically, Titan has been something of a toolbox of components from which game teams can pick and choose these components, but it's becoming more and more of an actual engine.
Titan is also maintained by the tools and technology team.
So even though every game team does have the capacity and can commit directly into Titan, the primary responsibility for maintaining the engine still falls on the tools and technology team.
And I'm also personally part of that team.
So we've been seeing this growing need for 3D.
Historically, our early games have been isometric, pre-rendered, or hand-painted 3D.
So if you look at Hay Day or Clash of Clans, it looks 3D-ish, but it's actually not real-time rendered 3D.
But games such as Everdale, which is one of our beta games, and Brawl Stars really highlighted the potential of 3D in our games and how it could make for more vivid environments.
It's worth mentioning here that even though Everdale was killed, it's found a new home under MetaCore.
So we're definitely looking forward to what they can come up with the idea with the concept.
So then some new wind started blowing for 3D.
And one of the big things that happened was that Apple decided to drop a bomb when they deprecated OpenGL ES.
This was a huge problem for us because we heavily relied on OpenGL ES.
Pretty much all of our rendering was built directly on top of it.
We had some lightweight layers on top of OpenGL ES, but a lot of the games were also directly calling into the underlying API.
So if we look at a bird's eye view of our legacy rendering architecture, then at the bottom we have the OpenGL and OpenGL ES layer directly, no abstractions.
On top of this, we have this lightweight helper layer with shader loading on a render class that help with some vertex buffer binding things and stuff like that.
GL image for OpenGL texture creation and texture content loading.
A very kind of simple and lightweight stuff.
On top of this, we had 3D rendering functionality for meshes, materials, models, scenes, et cetera.
And this layer also kind of contains stage.
And this was using the underlying layer in addition to directly calling into OpenGL ES as well.
And then finally, we have a game-specific rendering layer which used all the layers underneath, including directly calling into OpenGL.
So we were heavily reliant on OpenGL.
So we wanted to future-proof our rendering.
We really saw this as an opportunity.
So we didn't want to just work around the issue of not having metal by just implementing a side-by-side metal path for rendering.
We wanted to do this properly.
Our legacy rendering path was really holding us back.
So it's becoming more and more complicated to create anything more complex, 3D effects.
So it was really holding us back.
And we had these situations where we had 2D rendering states leaking into 3D rendering states and it was really hard to maintain a separation between these systems.
So we needed a proper abstraction layer that would allow our game rendering engineers to focus on the actual game visuals and not think too much about the underlying API.
So this could be considered a risky move to rewrite the render of billion dollar games, but we believe that changes like these are often necessary to keep us competitive.
So we keep moving our games to new Titan features because it helps us in many ways.
It helps us improve our existing games with every update.
So if we, for instance, develop a new content delivery system, then these games are able to deliver better and more content to our players.
It also allows us to keep the tools and technology team smaller because we can focus on this single stack and we don't have multiple branches of our technology stack to maintain.
Also, it makes it a lot easier to share learnings and technology between games because we have this common language that we're speaking, common platform that we're using.
So now it's time to introduce Thor, or Titan Hardware Oriented Slash Optimized Renderer.
In fact, this is actually just a name that I came up after watching a Marvel Thor movie, but the term was nice and it had a nice abbreviation, so that stuck.
So Thor is this abstraction layer.
Currently it abstracts Metal, OpenGL, OpenGL ES, and Vulkan.
Vulkan is still an experimental support, but we're pushing hard to get that into live as soon as possible.
When I joined the company, we had like a skeleton of what was to become Thor.
So it was very simple, but we had the basic architecture in place already, and we had basic support for OpenGL, OpenGL ES, and Metal.
Vulkan was still missing at this point.
And actually quite a lot of like core features, you know, were still missing and a lot of this has been rewritten partially because of multi-threaded support added to OpenGL.
But it was a good start.
So during the spring of 2020, I worked on Thor pretty much in isolation.
So I didn't have any...
game project to actually work on. I just had this separate test project for different rendering tests for creating textures, creating compiling shaders, and rendering primitives.
But right before summer holidays, which in Finland, the game industry typically has in roughly around July, so this was by the end of June or so, I got Hay Day running in a very limited manner.
So as you can see, the screenshot is quite buggy, but it is the first screenshot of the game actually running on Thor.
So it was a huge milestone because after this, we could just start improving Thor, making it better, and making it production ready.
Hay Day was chosen due to its limited rendering feature set.
It basically only relied on stage.
We didn't have to implement any of the 3D rendering functionality.
Of course, we had to prepare for the worst.
So we set up automated testing from the get-go, even before we were considering merging anything in.
So we had these predefined scenarios for smoke testing.
Tests that would scour through the game, go through different menus, press different buttons, and basically kind of automatically play the game.
We also have video recording for recording the play session and storing that on disk for later analysis.
We had log capturing so that if we saw some issues in the videos, we could go into the logs and see if there's any potential reasons for why that might be happening.
And we could execute these tests in Google Firebase.
So we could take hundreds of devices and combinations with different OS versions and try out the tests on there.
Get really good coverage.
This helped us find multiple issues in rendering before launch.
And towards the end of the presentation, I have a couple of interesting examples of these.
And it allowed QA to focus on the more tricky issues.
So, during the fall of 2020, I focused on improving and stabilizing Thor, so basically making it production-ready, fixing small issues, doing performance optimization, and stuff like that.
Working with the game team was super easy.
So whenever I had any questions about where certain things were implemented, how the rendering was done, or how to make certain kinds of builds, I could just directly go to the team and get answers really quickly.
And no one really questioned my ability to work on this.
And I was able to do really big decisions autonomously.
So in the end, the update of Hay Day using Thor was released on the 23rd of November in 2020.
And this was the second huge milestone for Thor, because after this point, we had a game in the wild and we could start collecting performance metrics, crash reports, and overall player feedback.
Luckily, we didn't run into a lot of issues in the live, so I was able to move to the next project, which was that I was jumping from one game team to another, porting the game rendering over to Thor and improving Thor while doing so.
So the next game in the line was Clash of Clans.
And this was actually the first game to be ported that had 3D elements in it.
So at this point, I needed to port meshes, render queues, materials, all that stuff.
But it was a relatively straightforward process, partially because stage was already ported and most of the rendering in Clash relied on stage with just a couple of 3D elements there.
This update went out pretty quickly after heyday on the 12th of April in 2021.
Next up was Clash Royale, and Clash Royale was actually started in parallel with Clash of Clans.
And there weren't a lot of huge surprises again here, mainly some shader variant filtering systems that needed to be implemented. But the biggest issue really was matching the timelines of the project, so their release schedule. And it took quite a lot of time to bring this update out, which came out on the 25th of October in 2021.
The fourth game out was Brawl Stars.
And Brawl Stars was probably the most complicated.
Most of the functionality itself was already there, but because of the sheer amount of content that we had and the variations of the different shaders that we had, we ran into a number of smaller issues that needed to be fixed.
Also, QA had their hands full on multiple releases of the game, so it just took a while to go through QA.
Eventually this update went out on 16th of December in 2021.
The last game to be ported was Boom Beach.
And this took the longest, because, again, the release schedules of the game.
And Boom has its own, a bit different way of doing 3D rendering.
So there was somewhat more work to be done there.
But the update eventually went out on the 2nd of November in 2022.
So at this point, all the live games had been ported, and we could do a surgical operation to remove the old render path.
So this was the third major milestone for Thor development.
And we could consider the project done at this point.
You've probably noticed a pattern here with me mentioning that the biggest problem was really the timeline of the projects.
And that is true, because it was complicated to match the work being done on Thor to the timelines of these individual projects, because they still wanted to develop the games forward, develop all the new cool features that they wanted for their players, while I was still working on the core rendering.
So that was probably the most challenging part on here.
But that was just the beginning.
So the development of Thor has continued nonstop.
We've now shifted focus to building on top of Thor.
So now we're able to build new rendering techniques, which we couldn't before.
It also has made sharing a lot easier.
It's easier to talk if you have a common code base and common language to speak with.
And it has allowed us to build better tooling and use a lot of better tooling because of the pipeline, the new pipeline underneath.
So now let's talk a bit about Thor itself.
So we had three major design goals for Thor.
First of all, it needed to support multiple rendering APIs.
We wanted to add metal support, but we had to also keep the OpenGL and OpenGL ES support.
So naturally, we had at least two APIs.
But we also wanted to add support for Vulkan, because that's the direction that Android has been pushing really hard.
We also wanted it to be powerful and efficient, but easy to use.
So we wanted to be able to create these modern, impressive visuals with the system efficiently.
But we also wanted the game engineers and the game rendering engineers to be able to do this without a lot of hassle.
And we wanted to make this future proof.
So we didn't want to be rewriting the system anytime soon.
It was going to be our rendering platform for the future.
So Thor in a nutshell.
Currently Thor supports these APIs, which I already mentioned earlier.
It is architectured around the concepts of Metal, because we found that Metal is probably the best compromise between performance and usability.
And also, Metal was kind of our primary target initially, so it made sense to architecture things around it.
Thor is also multi-threaded, so this was a really important point for us, because modern mobile devices tend to have a lot of cores, and different types of cores also.
So we wanted to be able to utilize that to create multi-threaded rendering algorithms, which we couldn't before.
So we had a number of design challenges also.
First of all, it needed to support all the legacy devices the render path also supported.
This partially comes from our mission, which is to create great games that as many people as possible play for years and that are remembered forever.
So we didn't want to be dropping these players that have been enjoying our games for years just because we were rewriting this core rendering, which was more or less Future-proofing our rendering, but it wasn't immediately providing any huge benefits to our players.
It was going to provide those benefits in the future.
So the added abstractions should also be very efficient.
We had a very efficient use of OpenGL ES originally because we had built all of our rendering directly on top of OpenGL ES.
And abstractions tend to add overhead.
So we wanted to keep this to the minimum.
This was probably our biggest challenge.
We wanted the new render path to coexist with the old render path.
So we didn't want to put all of our eggs in a single basket.
And we wanted to update our games one after another to make sure that the game before worked before we moved to the next game.
So let's look at a bird's eye view of the core architecture of Thor.
So we have the bottom layer, which is a platform specific layer.
And this is different for Metal, Vulkan, OpenGL and OpenGL ES.
For OpenGL and OpenGL ES, it shares quite a lot of code, actually.
So technically, it's kind of the same platform with some variations.
On top of this, we have the actual abstraction layer, which contains resources such as buffers, textures, different render states such as the render pipeline, depth stencil state, shader function definitions, and all that.
And this is what we call Thor.
So it is the hardware or the underlying API abstraction layer.
On top of this, we have the same layer that we had before for 3D rendering, mainly also including stage.
And we didn't re-architecture this layer in any significant way.
We ported it over to Thor, but we didn't make any big changes to its structure.
Then we have the game-specific rendering layer, which now depends only on Thor and the scene layer here.
So as you can see, it's no longer dependent on the platform API, which was one of our biggest goals here.
So let's quickly look at the composition of a single frame here.
So we start off with command buffer encoding.
This is a concept that we took from Metal.
So you create a command buffer, and then you create a render encoder, which is responsible for encoding these draw calls and state sets into the command buffer.
This is emulated on OpenGL because OpenGL does not have an explicit concept of command buffers.
Metal and Vulkan has this, so for them it's basically just a native operation.
But for OpenGL, we have a host memory buffer where we encode these state sets and draw calls, et cetera.
This command buffer is then pushed into the command queue.
And on Metal and Vulkan, this basically goes through the driver and to GPU execution, et cetera.
But for OpenGL, we actually have a dedicated working thread that unwraps the command buffer and executes the OpenGL commands.
So this has allowed us to create these command buffers on any threads, but the actual OpenGL commands are executed on a single thread.
We can of course have multiple command buffers being encoded and in execution in parallel.
There is only a limit for how many command buffers can exist at any given time.
But if you encode these command buffers, commit them and wait for some of them to be freed again, you can encode any amount of command buffers during a single frame.
And this hard limit of command buffers executing actually comes from Metal.
Then, of course, the second command buffer here gets executed.
And finally, when we're complete with the frame, we schedule a present.
That concludes the frame.
So let's talk a bit about shaders.
So we author our shaders in GLSL.
This is then compiled into Sperby modules.
And we convert the SPIR-V module to the target-specific languages using SPIR-V cross.
For OpenGL, it's like different variations of GLSL, and for Metal, that's MSL.
These are then loaded at runtime and converted into the runtime concepts, shader functions, or compiled and linked into shader programs.
And on OpenGL, we also have a shader program cache that we can use to optimize the load times significantly.
And of course, on Vulkan, we directly load the Spurvy module.
But it's important to note that this Spurvy module might not be the same for every single platform because we might be doing some optimizations for them, depending on the platform.
We have both a load time and a build time pipeline here.
So we can do the SPIR-V cross compilation either at load time or at build time.
The load time approach was something that we originally had where we shipped the SPIR-V modules and then at load time, we did the cross compilation to get the final target language, which was then compiled.
But now we've since moved to a build time pipeline where we actually built the different variations of these shaders offline during the game build.
In addition to this, we also have a variant system.
So every single shader can define a set of variant flags.
And the pipeline then builds different variations of these.
But as we all know from a lot of other engines, this can cost quite a lot of time to build all these variations.
So we also support specialization constants.
So if you request a shader function from Thor, you can specify a set of specialization constants which will further branch out the shader implementation. On Metal, this uses function constants, and on Vulkan it's specialization constants, and on OpenGL we actually utilize the way that Spurvy cross kind of emulates the specialization constants, but we're able to define these values at compilation time.
So naturally, not all features of Thor are available on all platforms.
Games can check for the availability of certain features using feature flag tests.
But some of these features are also emulated.
And this allows for a uniform base implementation.
So the game rendering engineers can expect a certain uniform implementation or base implementation to be there to build on top of, and then they can branch out for specific type of features that they might want to be using.
An example of these emulated features on OpenGL ES 2 backend is uniform buffers.
So in OpenGL ES2, we don't have support for native uniform buffers, so we just have a host memory buffer where we store the uniform values based on the shader uniform block layout.
And then when we want to actually render some primitives, we resolve these by reading from the memory buffer and using glUniform calls to set the values of the uniforms.
So looking into the future, focus has now shifted to building on top of Thor, as already mentioned.
I have a couple of examples here of things that we're currently working on.
So of course we're adding features to Thor. Whenever we realize that there's some new feature in Metal or Vulcan, we consider adding that to the core of Thor, if it is something that really benefits the game teams.
In addition to this, we have a couple of new systems that I wanted to mention here that we're working on.
First of all, a render graph implementation, and something what we call Odin.
And this is not a coincidence that it's called Odin.
So, now let's quickly talk about the render graph.
The render graph is basically a task, render task management system.
Each task implements a piece of frame rendering, and we have two types of tasks on here.
First of all, we have generic tasks, which do not output pixels.
And a good example of these is CPU skinning.
And this task is basically responsible for taking the model transforms, the untransformed vertex data, processing that, and outputting that into a temporary vertex buffer for rendering.
Then we have render passes, and render passes actually output pixels, either off-screen or on-screen framebuffers.
These tasks are then queued for execution on worker queues, and we can have basically any number of worker queues, depending on how many different cores the platform has.
It really makes sense to use different amount of queues.
Synchronization is based on the inputs and outputs of these nodes.
So as you can see, there is a dependency between model skinning and the different render passes here.
So the model skinning needs to complete before the render passes can start.
Also, there is a dependency between the shadow pass and the color pass, but you can see there's quite a lot of overlap in the execution between these tasks.
This is because the color pass only meets the frame buffer output of the shadow pass, but only on the GPU.
So we can overlap most of the execution of these tasks, because that execution is preparation of the command buffers.
So the only important thing is to make sure that the command buffer for the shadow pass is committed before the color pass.
And this is something that the system also takes into account.
So I'm not going to go too much into Odin at this point, but I wanted to mention that because this project with Thor did not really touch the middle layer that much.
But the project called Odin is this.
So the purpose is to modernize this middle layer.
So to be able to be better equipped to fulfill the needs of our new games and our current games, to have different kinds of materials, make that more data-driven and easier to use.
So now let's dig into some learnings from along the way.
The first thing that I wanted to talk about here is quality.
So our games are played on a wide range of devices.
We have about 250 million monthly average users who have quite a large variety of different kinds of devices.
So testing everything manually is just impossible.
And automated testing, smoke testing, really took the pressure off of QA and allowed us to have really good coverage here.
Another very important topic, at least for myself, is proper self-review.
So making sure that the commit that you're making or the code that you're merging, that you understand it and that it is up to your own quality standards.
So this basically helps you feel sure that you've made the best decisions with the available time and information.
Also, it builds confidence in your coworkers that you are not making decisions at random.
And finally, that you're also better equipped to fix any bugs that are surely going to pop up in your code in the future. And finally, respect your QA. They really help me sleep my nights in relative comfort. Something that's very important with mobile development overall is energy efficiency.
So batteries and heat dissipation tend to be very limiting factors on mobile.
You don't see a lot of devices with active cooling, like fans blowing.
And these devices are targeted towards not sustained performance, but peak performance.
So if you try to sustain the full 100% usage of the device, then you're going to run through battery and it's going to be overheating and the device will be throttling.
Rendering needs to pay extra attention to this because a lot of the rendering code tends to be on the hot path.
And this is really important to fulfill our mission because it allows us to, you know, we need to be efficient even on these lower end devices and we need to allow our players to play our games as long as possible.
So taking into account the batteries and basically gaming comfort as well.
So a couple of examples of how to take into account energy efficiency.
First of all, I would recommend to use cache efficient data layouts, especially on hot memory.
I dug up some information in preparation to this presentation, indicating that the difference between a cache miss and a cache hit is about 200.
times. So it's 200 times less efficient in terms of energy consumption if you get a cache miss.
So you should definitely aim to design your data layouts in such a way that they are efficient for the for the cache. Also favor using multiple cores. So if you're able to split your rendering algorithm into multiple threads.
then you're able to put less pressure on individual cores, allowing them to run at a lower voltage, saving a lot of energy.
Also, aim to use low-power cores.
Most modern devices nowadays have low-power cores.
They might have mid-power cores, high-power cores, different sets of different kind of cores.
And using the low-power cores or the mid-power cores really saves a lot of energy.
If we look at Clash of Clans, for instance, it mostly runs on the low-power cores, especially on the even slightly higher-end devices.
And finally, I would recommend everyone to write sensible code from the get-go, because you will probably not have the time to fix your code if you have not allocated time to do that.
So make sure your code, the code that you commit in, is sensible enough.
It doesn't use a lot of brute forcing, and it is just something that can go into production if you do not have the time to optimize it further.
Also I wanted to put this picture here which showed the difference in energy consumption before and after moving to Thor.
So this was the comparison between using OpenGL ES on the old, the legacy render pipeline compared to the new render pipeline using Metal.
Of course, sometimes devices don't work as expected.
So many Android devices, especially, ship with buggy drivers or hardware.
The situation isn't as bad as we had expected it to be, but it is still significant.
So I have a couple of examples here for the inquisitive mind of certain situations that we ran into during this project.
So first of all, there's this case of missing code blocks, nested code blocks.
Then we have using outputs as temporaries.
And finally, broken uniform structures.
So let's talk about the missing code blocks first.
So when we were analyzing the recorded video from our test cases, we noticed that on some devices, the hero characters were darker than on other devices.
Digging into this, I pinpointed the issue to a function roughly like this.
So we were doing srdb decoding if a constant was set to true.
And I thought that, well, maybe it's the constant that just has the wrong value, so I'll just hack it in and just try, you know, checking if true is true.
But this did not fix the issue.
So at this point, I was like, well, maybe it's actually a driver issue, so I'll just remove the if, and at least now it should execute that code.
Turns out it didn't.
So the result was that the driver just didn't like code blocks within functions, which was pretty ugly because we had to move this type of code to the main function.
Putting any code like this in a nested function or a function that was called from main just did not work.
And this was a relatively new OpenGL ES3 level device.
Then we have this case of using fragment output as temporaries.
This is actually something where we did wrong, but the result was quite interesting, I think.
So in the recording, we saw that on some devices, the characters were drawn in grayscale.
And I pinpointed it to code like this.
So we were writing into the fragment output.
And then we were doing the linear to sRGB encoding here by reading from the output value and writing it back.
Doing it from the temporary value basically fixed the issue.
So we're not reading from fracColor anymore.
This is something that the OpenTL specification actually states should not be done.
So you should never read from the fragment output.
You should only write into that.
But nevertheless, it was really interesting how it worked on some devices and the compiler didn't really complain about it.
And the output was a very interesting case.
Finally, there's the case of broken uniform structures.
So on some devices, we just had a black screen.
So this is like the worst nightmare of any rendering engineers, looking at a black screen.
You have no idea if it's rendering anything.
But look, I opened the build on the machine, a device where it was broken, and I was looking at it for quite a long time, thinking what could be wrong, and then I noticed some small flickering at the lower corner of the screen.
So I thought that, well, definitely, it is rendering something, but what's wrong with this?
So I looked at it using GAPID, which is an Android graphics debugging tool, and it showed some inconsistencies between two different views.
These views displayed the uniform data of the shader program.
So it was just showing different data there.
So this is how we had defined the uniform block in the source shader.
And this was converted for the OpenGL ES2 backend, which did not have uniform buffer support, into code like this.
So it generated a structure with a couple of matrices and then a uniform based on that structure.
Turns out, the driver didn't like uniforms defined in this way.
And when we were updating the uniform values, some of the columns in uView got values from uProjectionView.
So it was messing up the locations of the uniforms in some weird way.
The resolution to this issue was to use something in SpervyX that basically flattens out the uniform block into a vec4 array.
So we could just update this whole uniform using a single uniform, glUniform call.
The only problem was that this does not allow interleaving floating point values with integer values in the uniform block. But for the games that had this issue and needed to use the OpenGL ES2 backend, this was not really an issue.
So finally, I wanted to talk about something very important that's kind of been glanced over during this presentation, but I wanted to highlight at this point, which is the whole getting it done mentality that we have at the company.
So our culture is really about empowering people to do their best work.
We have very little red tape.
So whenever you need to do something and you need to start off a project to improve a certain system, you don't have to go through a lot of different kinds of organizational hoops.
Don't have to go through different levels of management to make designs and get approvals for them.
You can just kind of become your own project manager and start moving it forward.
Of course, there are cases where this doesn't work for big structural changes, some strategically important things.
But for most of the system development that we're doing, it works really well.
People also tend to take responsibility and ownership of the code that they're developing and the systems that they're working on.
So I usually say that it's not really a problem if you screw up.
That's totally okay.
That's, that's human, but it's really what happens after that, what matters.
So if you really take, um, take up the responsibility and you help people fix your, your issues, or you fix them yourselves, that is really the important thing.
And this whole mentality of autonomous development, basically, being able to do that, allowed me to work efficiently and move between different game teams and talk directly with the game teams and get things done really quickly and make big decisions without going through different levels of management.
Of course, it's not all roses.
Like, you know, some people occasionally make decisions without making the necessary preparations and talking to people who actually, like, know about the system that this person will be working on.
But overall, the benefits really outweigh the downsides, and it's been working really well.
So this actually brings me to my conclusions.
So first of all, we built technology for a need.
So for Thor, we really needed to add mental support, so that was a given.
But we also wanted to make it possible for game rendering engineers to build better visuals in the future.
So we were not just rebuilding something just because it was a fun thing to do.
We also had to proceed really carefully because we were rewriting the render of billion dollar games.
So we wanted to make sure that we're not, um, we're not putting all of our eggs in a single basket and then dropping it.
And finally, Thor is our platform for the future for rendering.
So we were able to build this common language that now all the different game teams use, and we can rely on, and now we can build on top of this.
And finally, there is a very important thing that I wanted to mention here, which is that we are hiring.
So if you want to know more about the company, you can come talk to me or any one of the supercellians roaming around here, of which there are quite many this year, actually.
But apart from that, I think it's time for questions now.
I thank you very much.
So earlier you mentioned that you tried to put your game on the slower cores on mobile.
I'd like to know how you do that on iOS, which doesn't provide an API for Thread Affinity.
Basically really about like how much you actually push the CPUs.
So just try to like be as efficient as possible and it'll automatically move it there.
So in Clash's case, it is relatively lightweight on the CPUs so it kind of naturally happens.
But yes, it is true that there's like no way of defining that.
But then again, if you do have that interface and then you force it onto the low power course and it's not enough, then you're not gonna have the frame rate that you want.
So the best way is to just like make sure that you have as efficient, use as efficient algorithms as possible to just naturally make it happen.
Yeah, we're running into that problem. Thank you.
Okay, yeah, no problem. Thanks.
Yeah, I have mostly the same question, but don't you run into the issue that when you run on the low power cores that you're not getting your deadlines, for example?
It's pretty smart at actually scaling.
Like, you know, there are cases where if you try to do something really smart, like automatically scale your, like especially in Metal, if you try to scale your presentation times based on like the actual time that you're executing on the GPU, then you get this kind of rubber band effect where it's like spending more time and then you scale it down, then spending less and you scale it up.
But yeah, so it can be, you know, it's a difficult problem in some cases to solve.
And it's really about like trial and error and looking at your metrics using the tools that you have and what you can do there.
All right. Thank you.
Hi, how are you finding the driver stability on like Android Vulkan drivers?
Are you finding bugs with all the drivers or are you having to limit your devices to like a known set of devices?
Yes, so we've been seeing multiple issues, like some very weird cases, even on high-end devices, things like the pipeline cache is not working at all for a large amount of specialization constants.
So it is a kind of a challenging world.
It's getting better.
The problem really is that you have the baseline profiles that you can rely on, but that doesn't really tell anything about the quality of the driver.
So it is just a thing that's improving over time and we just need to maybe try it out on those devices and then blacklist them if it just doesn't work there and move on there incrementally.
But yeah, definitely we're running into those issues.
Okay, thank you.
Thanks.
Hey, thanks for the talk.
When you selected Spire V, did you analyze other tools like BGFX or other kind of backends like that?
Excuse me, selected what?
When you selected Spire V, did you analyze other tools like BGFX or others?
Spervy basically was like a, it was a pretty good natural selection in the sense that it worked directly for Vulkan as well.
And there were pretty good tools for that, like Spervy cross, which we could use.
The first Spervy cross is not.
Perfect. And Spurvy itself is not perfect.
So we run into issues with, for instance, floating point 16 support, which Metal doesn't support the relax position attributes on there.
So it gets a bit tricky to add like floating point 16 support.
So definitely it's not perfect.
But.
We didn't really, really, think we didn't really do really in-depth analysis on that, to be honest, but it was just a tool that looked to fit our requirements really well overall, and the pipeline was there.
We could cover all of the different use cases like Metal and OpenGL and Vulkan for us with that.
But we have been thinking about potentially either switching a solution or potentially even building our own pipeline on there because of some of the issues that we're having with it.
But I hope that answers your question.
Thanks.
Hi.
What tool were you using to measure wattage on the iPhone 7?
This was like GameBench, I think.
Yeah.
When you find problems with the drivers, are you just doing workarounds and moving on, or are you contacting the driver manufacturers? And if so, how long does it take till that gets resolved?
Well, both, but the problem really is that it's not a practical route to expect the driver update, because many of the drivers actually never get updated.
And you might have like even slightly older devices which are updated like maybe for two years or so, and then after that they never get any updates.
or like the drivers might even never be updated.
So we usually have to just work around the issue in some way and then, you know, just hope that it's going to be fixed at some point.
Thanks.
Thanks.
So if there are no other questions, then I thank you very much.
I hope you enjoyed the video.
Thank you for watching.
See you soon.
Bye.
Bye.
Bye.
Bye.
Bye .
