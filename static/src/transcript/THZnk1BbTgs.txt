My name is Aviv Stern, and I'm the director of analytics at Social Points.
And I've been a gamer since the 80s.
I started with D&D, the gateway game, and then I matured into XCOM, unlike today, where I play D&D and XCOM.
I've been doing analytics since the 90s.
We had many names.
We called it business intelligence.
We called it data science, data mining.
At its core, it's always about the same thing.
We're trying to answer questions, and we're trying to find insights that can help the business.
I'm currently answering questions and finding insights at SocialPoint, which is a take-two company in Barcelona.
All right.
Why are we here?
Less of a philosophical question, more of a specifical question.
We're here to equip you with analytics examples, things that we've done, mainly at SocialPoint, and things that you can use.
in your games.
Now, these examples that we chose for the presentation are examples you can use in any size studio.
They can work with a single game studio, whether you're indie or not.
Multi-game studios, I've done this at SocialPoint.
Some of these things I've also implemented at Plarium, which is another gaming company I've worked at.
And I've even implemented some of these best practices in companies that do mobile apps and didn't do games.
We are here to learn from some of our mistakes.
Hopefully it can save you guys some headache and some money.
And we'll talk about some of the things that we've done at SocialPoint that didn't work too well, or things that we are doing differently.
And we are not here to make you analytics experts.
I'm sorry, but that might take a bit more than an hour.
All right.
Our story begins, as many stories begin, with a company.
Take two acquired SocialPoint.
at 2017, then SocialPoint began a strong focus on growth.
They brought in heads of product from Rovio, from King, the roster of heads of product, which is basically our equivalent of a studio head, has grown a lot.
I joined the company on March, about a year ago.
This is me on my first day, so happy.
And the events that we will be focusing on for this presentation are the events that transpired between my joining and this here presentation.
All right?
Okay.
What will we talk about?
We will be talking about the dark side of analytics.
Analytics can be very valuable.
I'm a data person, of course I believe in analytics, but there also can be a very negative side if you do it wrong.
And it has a cost.
For example, one of the discussions we're currently having in SocialPoint is about, should we be A-B testing?
A-B testing is a practice, for those who do not know, is a practice of, for example, taking a game, implementing changes, and then releasing the game with the unchanged version and the changed version, and randomly sampling people into one of the two versions.
This lets you compare side by side what is the real impact of the changes.
Are people in the new version?
buying more, retaining better, converting better.
That's an A-B test.
Why not A-B test everything?
Because it costs money and time.
When I put out a new version, I need to wait.
I need to wait for the results.
I need to wait for enough users to come in for my analysis to be significant.
And that means that I'm slowing down the game.
If the game wants to run very quickly, if the game wants to release versions and move on to the next thing, sometimes more data is not the answer.
So that is the dark side of analytics.
We will talk mainly about mistakes we've done and things you can avoid.
All right, tracking user actions.
We will talk about tracking user action.
By tracking, I mean collecting information about logins, purchases, battles, level ups, all those events that happen in the game, and we collect that information so that we can analyze what happens.
Now, this can be less painful if we do some of the things that I will show you today.
Playing levels without players. It can be done.
You can have a level played without users. It can actually be very beneficial.
And I will show you how we did it at SocialPoint and why we had to do it.
Basically we had a problem we needed to solve.
And I will talk about testing, specifically A-B testing.
I will talk about testing with small audiences.
It can be a new game that you're releasing and doesn't have three million downloads, or it can be a new feature that you're doing for players at level 24 on the third dragon.
So you might have a small audience and statistics doesn't like small audiences.
So I'll show you some hacks that you can implement so you can still use A-B tests in those scenarios.
Okay, about the cases that I will discuss.
These are cases from my personal experience.
They apply, like I said, to many studios and many games.
I will try to speak mainly English.
I'll try to avoid any terms that are too technical, too deep, but I will be touching on many frameworks from business and from data, and analytics is a very quickly evolving field.
When I do introduce a concept, I will try to explain what it is.
I will not go too deeply into it.
My presentation will have a deep reference.
When we upload it onto the website, you can go there and you can find descriptions of most of the things that we'll discuss today.
All right?
Lastly, there may be some assembly required.
What I want to do today is to equip you with things that you can do in your games, and I want you to be able to go to your analytics teams, whether they're in-house or vendor, and say, build me this.
but you will still need to work with analytics people to get this done.
Or in the immortal words of Ikea, screwdriver, screwdriver, hammer.
All right, let's get cooking.
This is, by the way, thanks to the Tasty Town, our team, that's one of our games, and they made sure to splash some color into the presentation.
We'll also have some dragons.
Okay, SocialPoint, why did we have to start this?
Why did we have to go into this adventure and find ways to use data differently?
SocialPoint was using data long before I came.
SocialPoint has been around for about 10 years, even more now.
So data was there, why did we have to change things?
Where did all these challenges come from?
SocialPoint during 2019, after the acquisition with the new roster of heads of product, faced several dramatic changes.
That really had to...
We had to redefine a lot of the ways we were doing things.
First thing that happened was we had five games going into 2018.
And during 2018, we had 23 games launched.
Some were in development.
Some were launching and dying.
Some were being replaced.
But we had pretty much five times the amount of games that we were ready to handle.
And I'll give you a hint.
Analytics did not have five times the headcount or the budget.
So we had to find a way to make it work.
Another thing that happened is we switched from having a company that's very well optimized on live games.
Social Point had games, they had millions of users, they had money, and we were trying to do something that was very different, which is release a lot of games with very small teams, two people teams.
Sometimes those two people were engineers, they didn't even have an artist or a designer, and do things very quickly.
Have an assembly line of games that keep launching and learning and improving.
The other thing we needed to do was really change how quickly we answer questions.
We had to be able to answer questions.
When you have a game that has three major versions in a week and launches, and next week launches globally, and then goes from iOS to Android, you have to be able to keep up with the game.
You cannot be demanding the game to slow down so that you can give them answers.
You have to find a way to work within that constraint, which was something that was quite new to us.
We had games that were much bigger and we had much more data.
It was usually a different case.
The last thing we had to do was, like I said, the team didn't expand as the portfolio expanded.
So we had a lot more games.
Before that, before 2018, every game had an analyst.
Which by the way, I strongly recommend if you can do it, always have someone in your game team that is doing data.
If that person goes to bed at night thinking about the questions in the game, living the game, playing the game, playing the competitor games, that will bring you a lot of value.
We data people work best when we're part of the product team.
That's when we understand the real problems that need to be solved.
But when you have 23 games, some of them have two people teams, and some of them run and have multiple versions every week, you don't have an analyst per game.
We had to change the structure.
We had to create a pool of analysts that work like internal consultants just to be able to keep up.
And that's something that was a compromise. We had a very hard time with that. We really wanted to leave the games.
We wanted to be living in the games going to everything that the game was doing and being forced to work on multiple games at the same time was something that was a bit of a compromise. But it was also something that was very agile and made a lot of sense for the company. All right. So why? Why did all this happen? Why did the games So many games come out, why do we have to work in this way?
The reason behind it is that SocialPoint adopted a lean startup approach to games.
Those of you who are not familiar with the lean startup, the lean startup is a methodology.
It's a lot of things.
It's probably one of the most impactful movements.
in how we do startups or how we build businesses with big variants.
But in game terms, it's this.
I joined SocialPoint on my first month there.
I met my colleagues from Take-Two.
And we had this conversation.
They were talking about launching Red Dead Redemption.
The way you create a Red Dead Redemption, a lot of the decisions about design, about difficulty, about content, those decisions are made before the game is shipped.
All of my games, I make the decisions after I hit the market.
Theme, difficulty, levels, balancing, content, all of these things are things that I can change and I should change and I do change after the game launches.
Lean Startup is a methodology that really helps when you're creating a product that you're gonna learn by having it being used.
That's a very good framework for that.
Another thing that it does is really helps you minimize the cost of failure.
If you don't know about the Lean Startup, I strongly recommend you read these books.
If you have the time, I think it's gonna be worth your while.
These are the two books I buy, every one on my team has them.
Every team that I joined, these are the first two books that I buy everyone.
The idea is, in its core, to have a lot of focus.
There's gonna be links to this in the reference as well.
Sadly, I don't have any equity in the books, but they have impacted a lot how I do my job and how we work as a company.
I'm gonna do a very short explanation of what Lean Startup is in our industry.
The most prominent voice of doing lean startup in the games industry is Supercell.
Supercell have a lot of speeches about this, a lot of public talks.
This is a strategy that they have adopted and they know better than me to say how, so this is a link for what they do.
The idea is when you're doing a lean startup approach to product design, you have a loop of stages that you keep doing very quickly.
You build something.
like a new maze, then you release it, you measure how people are playing that maze, or a new level, or a new game, and then you learn.
You see if people are enjoying this, if people are not enjoying this, where it works.
Then you build something else, and you keep that going.
Where analytics comes into this is, analytics really helps you find out what doesn't work.
The key thing about being lean, is not having the secret to success.
It doesn't necessarily mean all your games will be amazing.
What it does mean is, you'll find out very quickly what doesn't work.
And this is really, really important.
You can take a game that runs for two years development with a 20 people team, and we had this happen in SocialPoint, is released, and then we found some of the problems we had, or you can find that out with two people in three months.
And then if that doesn't work, try something else.
We're trying new genres, we're trying match threes, we're trying different themes.
That doesn't work, you wanna switch to a different genre, you wanna do word puzzles, take the same theme and do that.
But find out very quickly, if you're not reaching that, whatever your goal is, day one 60% retention, conversion of 2%, whatever you're aiming for, the lean startup is a way to find out very quickly, are you getting there, are you getting closer, or not?
And if not, then you kill it and you move to something else.
So the idea is to fail fast.
Saves you time and saves you money.
All right, Lean Analytics.
Lean Analytics is a school within the Lean startup.
And this is what we're gonna talk about today.
I'm going to say a few very trivial things now.
What I'm gonna say now in this part of the talk is going to be things that are very common sense.
None of this is rocket science.
All of these things that I'm saying will sound very reasonable, and I'm not telling you anything new.
But this is the part where I'm talking about things we did wrong.
All of these things are based on things that we did not implement correctly, things that we failed to do at SocialPoint and other companies that I've worked at.
So it's really worthwhile to take these things and adopt them.
This is a discipline that you have to keep.
And I'll show you in a minute how we keep ourselves very disciplined.
Measure what matters.
This is a very nice philosophical saying, but what's behind it is this.
SocialPoint at the beginning of 2018 had more than a thousand metrics.
We were using more than a thousand metrics to measure if the game was doing well.
We had too much data.
We had so much data, when you asked a game designer what they look at, every game designer was looking at different things.
What we did was, we really narrowed it down.
We asked what really is important?
Of all these things, what makes the cut?
What are the super KPIs?
The core KPIs that we have to know.
When you're driving a car, You only know what you need to know.
The dashboard doesn't give you everything you need to know.
Air pressure, the heat of the ground.
You don't need to know all these things when you're doing that.
If there's a problem, we drill down.
But we've been able to narrow it down to retention, conversion, LTV.
We've been able to talk about revenues.
And when we talk about a very specific cost, by the way, some things that didn't make the cut, second session retention.
We actually don't have that on the first view of a game.
When we talk about is the game good, we talk about day one retention, day three retention, but we don't talk about a session granularity.
Sometimes we don't even talk about a daily granularity.
We look at it on a weekly basis.
This was a system that was designed for the executive team and really created a different conversation.
And it changed the mode from bombarded with million reports and million metrics to really focusing on what's important.
The other thing is, Don't develop what you can buy.
Now this is really true for small companies and it's also true for big companies.
We have a very strong data engineering team and we do do a lot of our tools in-house.
We also have scientists.
When I do A-B tests, it's my scientists who create the tools for that and my analysts benefit from that.
But even with that, there's things that I don't develop.
One of the things, one of the problems that we had at SocialPoint, we had KPIs that were misbehaving.
We had a process that was getting data, for example, AppSlayer data about user attribution.
This was data that was telling us which user came into the game through which marketing campaign, which was really key because we spend a lot of money in marketing, so we want to know.
AppSlayer had a bug.
Our process was the same process.
We didn't get any notice about the process failing.
But AppSlayer had a bug, I think, about four months ago.
And they just gave us double the data.
So we ran the same process, nothing failed, no red flags, and we got double the installations.
So either we were doing our job really, really good at marketing, or something was wrong.
This is called a business anomaly.
By the way, business anomalies can happen for a good reason.
It could be that a new market opens up for you and users are really liking what you do, but it means that the KPI is behaving not in the way you'd expect.
You can develop this in-house or you can use a tool that does that and we're using Anodot for that.
By the way, we're working with King, we have a center in Barcelona, which is the Business Incident Management Center, I believe it's called, and we're working with the people there to learn how they're doing it.
Now this is king and social point.
These are big companies and we're still benefiting from buying things and not developing them in-house.
So if you're a small studio and you wanna have analytics, but you don't have an analytics team and it doesn't make sense for you to have analytics team, use something like Delta DNA.
You can still have analytics.
You don't need to create everything in-house.
There's enough services.
Use a cloud, you don't need to develop a cloud.
The world's come a long way.
All right.
Focus on value, BMF, product market fit.
Product market fit is a very fancy way to say in lean startup terms that work on things that people care about.
I'll give you an example.
One of the things, one of the first researches that I saw from the data science team, I came into Social Point, what are we working on?
And I talked to the scientists.
One of them presented a model that was very clever.
He was analyzing sentiment analysis.
So how people were reacting to our games, to our versions.
He took data from story views, social media, customer center.
He built a very robust system that shows if people have a positive or negative response to versions, to changes that we're making.
Amazing, super interesting.
No one ever used it for anything.
He did that analysis.
It took a lot of time.
It took a lot of data.
It was very clever.
No one in the games was waiting for that.
No one in the games was using that as part of the decision making, it just sat there and became a very good article for the university. But that's it. So when you work on things, make sure there's a value attached to that. There is a caveat to that a lot of the things you do in analytics are a leap of faith. I'm not sure this will move the money needle. I'm not sure whether this will be something that is going to be major impact on the game.
But even though you choose sometimes things that are a bit of a risk, make sure that you ask yourself, is this something, is this the best thing I could be doing now?
This is true when you have 26 people like we have, and it's more true when you have one.
What is the most important answer?
I should be working on right now.
Because if I'm working on the impact of the new maze, or the new island, or the new dragon, I'm not working on how to monetize existing users.
So always make sure that you're very disciplined, and I'll show you how we do that in social points.
One last thing, another trivial bit.
Remember the big picture.
When you're working on problems, we have a very strong tendency as people to focus on that problem.
We were doing a morality analysis for one of our games.
We had to intervene and make sure that analysis applies to other games.
We were doing a new game with match threes.
We had a game, a version that we launched for a game that we killed.
The only reason we did that was not because that game would benefit from that, but we would get some learning for things we do in the future.
Even if you only have one game in your life, I really hope you'll be making more games.
Always remember, take something from it.
Always try to do things in a way that benefits.
other games. When you answer a question, who buys? Try to answer that question in a way that you can rewrite or rerun it in other games. This is really important when you're running analytics in a company. All right. So how do we at SocialPoint make sure that we don't lose focus? How do we make sure that when we're working on something, It's something interesting.
Now I will say this, due diligence, I had two startups that were amazing and would have changed the world.
I created an AI that plays games and does everything for you and was remarkable and no one bought it.
Both of my startups ended as being very, very good experiences, but no one used them.
Just because you think something is interesting, just because you think someone is going to use this, that's not the way.
You need to make sure that you measure if someone is using this.
And this is how we do that at SocialPoint.
The chart that you're seeing here, remember I told you we took the super KPIs?
And we created a dashboard for the super KPIs.
It was originally intended for 20 people, just in the executive team and a bit more.
And it was very widely adopted in the company.
Just to keep in mind, the whole company is about 300 people.
So we had a very wide adoption of the tool.
Now this is an internal product.
I don't really have monetization other than my salary, but I am looking all the time at how many active users are there.
And when I release a new version for something that I develop internally, I make sure that people use it.
And if they don't, I ask why.
What did I miss?
The other thing we do is, and this is specifically for analysis, this is actually the same classification tree that we use.
I know it's a bit small, but basically it starts with, is it actionable?
First thing you want to do when you start this, is this actionable?
Is someone going to use this?
Is this going to tie into a process?
Sentiment analysis is brilliant.
Is it actionable?
No, don't do it.
If you know someone is going to use it, if customer service is going to use it, if marketing is going to use it, community managers, yes, but first validate that it's actionable.
Then ask, does this affect a core KPI 25%?
The focus that we got because we defined a very close group of KPIs is a focus that serves us for everything.
Now, whenever I have a conversation with anyone in the company, the first thing we will ask is this, what are you aiming to do?
Is this something that's gonna affect a core KPI?
If it doesn't affect conversion, LTV, retention, revenues, why are we having this conversation?
CPI.
So this is basically how we do this.
We go through these stages and we classify all our work in analytics across the teams, engineering, analysts, data scientists, everything we do, we work around this and we always try to maximize how much of what we're doing is high impact.
And it's a lot harder than it sounds.
It would sound trivial, just put people in the game teams and let the game teams work.
The game teams sometimes ask questions that are very not interesting.
Some game designer will ask, is the Halloween dragon better because it's orange or not?
Say I find an answer for that, what are you gonna change?
Next year you're gonna do a different dragon, let's talk about it when we get to the next Halloween.
Right now it's not applicable.
So always make sure that there's something behind what you're doing.
A lot harder than it sounds.
Okay, let's talk a bit about things that we're doing.
Things that were high impact, and things that did qualify as relevant, and change how we work in the company.
So, take you back a few slides, we had a big problem.
The company was basically going 5X on the number of games.
Things had to happen very quickly.
And one of the things that happened was we had a big problem with how we do tracking.
The company was already doing tracking.
It was doing it very well.
The technology was there, architecture was there.
It was not a problem of architecture.
We had to change how we collect user data.
I'm going to give you two things that we did to help that.
The first one is what we did at social point.
The second one, one core pack to rule them all is based on something we did at Plarium.
All right.
Tracking.
So how does tracking work?
And this is true for most games.
It's a bit of a simplification, but bear with me.
The game has a new feature.
Or there's a new game.
Someone wants to release something, they want to do something seasonal, Halloween special.
So we design it, we say this is design two.
Bring in new users through virality.
This is designed to convert or convert to a second purchase.
Users who are buying.
We align on a primary goal, and based on that, we define what we're gonna track.
This is really important.
What you track is a result of what you ask.
If you don't know what you're trying to achieve, don't start talking about what you wanna track, because you will fall back into collecting everything.
If someone tells you collect everything, we'll sort it out later, don't.
All right.
I just angered half of my data people community.
All right.
I hope none of them are here.
Okay, game implements the tracking.
So we had a feature, we had a definition, we defined the events, then the game devs, every time the user buys this seasonal dragon, they write a row and they put that row somewhere.
There's a file that gets sent.
Specifically for us, it was some folder in the cloud.
Then the data engineering team takes that group of files and make it into a table so the people can actually use it.
There's a lot of ways to do this.
That's not the goal of this conversation.
But this was a process that we were doing.
The problem with this process is, everything that the game does has to go through data engineering.
Every time the game team wants to have a new feature, a new measure, a new event, they have to go through data engineering, which is a small team and works.
for design for five games.
So that created a major bottleneck.
We were actually delaying the launches of game.
There were games that were coming out and not knowing what's happening.
They had features that were being used and it wasn't written anywhere how many times it was being used because the tracking wasn't there.
And my engineers were super frustrated because all they were doing was this, putting wires together.
They had nothing that was interesting, no projects that had high impact.
They were just there to configure wires.
So we changed it.
What we did was we implemented a change that I'm gonna mention in a minute, and I'm gonna use some tech terms, but what we did was we just let the games run.
Anything that the game reports, any data, any event that the game collects is automatically exposed as a table.
And that was very big.
That let the games run very, very quickly, and it freed up the engineers.
What we did behind the scenes, and if you want to hear more about this, it will be in the references, or you can catch up to me, but what we did was we implemented a game-specific data lake.
We just created a database where they can get data, and something that parses from raw events into tables, which is really not hard.
It took us about two weeks.
Okay.
What did we do, what did we achieve?
We eliminated a major bottleneck.
This is super simple.
This is not something that's, we didn't implement a new patent, we didn't change how the cloud works.
We just took that process, added some automation at the end, and made that very less painful.
Games still need to develop code.
Games still need to code tracking.
If they want something to be recorded, they still need to add that record to the action, but they don't longer depend on anyone else parsing that for them.
Data engineering became very, very happy because they were able to do dashboards like the one I just showed you.
For the executive team, they were able to create new dashboards for video ads to help our marketing teams and the games, and they were working on projects that had a much higher impact.
One thing to remember, game independence is really good.
And I have, as we speak here, I have multiple games in Barcelona, creating tables that I know nothing about.
As the data guy in the company, I have no idea what they're doing.
They could create anything they want.
They could be doing it for word puzzles, match threes, they could be creating it for strategy games.
I don't need to know.
I will need to know when they have traction.
When they get to a point where they were launched and they have enough users, that's when we have to bring them back into the fold and make sure that we do everything in an organized manner.
Then I can guarantee they have quality, that their data is secured, that I'm logging what they're doing, that I can create things for them.
Until that point, they have independence.
Now this democracy is really good.
But you have to be, it's not my problem anymore.
It's still my problem, it's still my responsibility.
I don't go out of the picture for that.
Last thing, some things you don't do with yourself.
Don't DIY everything.
It's really good that the games can create their own tracking and collect user actions.
This is fantastic.
This is a very good example for something you wanna have the game do.
Because they can run as fast as they can.
The thing that you don't want them to do is you don't want them to set up their own architecture to collect information in the cloud for offline events.
So if their game can be played offline, you don't want them to have to solve that.
It's completely okay if I have to build an IKEA chair.
If you ask me to build a Golden Gate Bridge, I'm probably gonna consult someone.
All right, you have to, that independence is not absolute.
All right, based on a true story, we had a game that wanted to implement a bidding system for the programmatic bidding.
We had a team in the company, it wasn't a game, but it was a team in marketing that wanted to develop our own bidding system.
Then this is another example for don't develop what you can buy.
This is not the core competency for the company.
We are game developers.
All right.
One core pack to rule them all.
The idea behind this is we had a problem.
I was working at Clarion.
The main problem we had at Clarion had nothing to do with this.
Clarion was switching from Facebook to mobile.
Switching from Facebook to mobile basically forced us to rewrite all the data architecture.
We had to bring in some very clever people with very fancy titles, and we had to build a big data system.
But we saw an opportunity.
to organize and bring some order into how we do tracking.
And the thing that we did at Plarium was we sat with the studios.
And we had studios in the Ukraine, in the US, in Tel Aviv.
We even had two studios in the Ukraine within hours of each other, one in Simferopol and one in Kharkov.
And they weren't talking to each other.
Each of them had their own data collecting system, their own database, two game-specific.
And then the GM asked, how much money did I make today?
And that was a bit of a problem.
All the pieces of the puzzle were basically scattered.
So when we recreated the system at Clarion, what we did was we sat down with every head of studio and we said, okay, what happens all the time?
What are the things that you always repeat?
Now keep in mind, some of these games were strategies, some of these games were racing.
They have very different, very distinct styles, but there are things that they all do.
So some events we found are always re-implemented.
Like login, like purchase.
Some of the things that we saw were they were not using the same definitions.
So there could be someone tracking a login and writing a timestamp that is local.
And someone would be writing a timestamp that is CET.
And then when you need to try to understand who did what and how many active users, I had a very basic question.
You get lost.
A problem that we had at SocialPoint with this was those new games that I mentioned, all those new games that keep launching every week and making a mess of my life, A lot of them were launching without events.
Even when they had their own personal ability to do it, they were missing some data from the stores about the purchases.
Still, some of the configurations, some of the wires weren't always there.
And there's a set of things that every game needs, and there's a set of configurations you can do for everyone.
So what we did was we just decided to create one universal component that every game can plug in.
and just have all the tracking.
Instead of each game developing their own approach to what do I track, how do I track, we said, okay, here's something, all you need to do is call this component in your code.
It's an SDK.
If you don't know what it is, that's perfectly okay.
If you do, fantastic.
We created a component in our system that everyone used in the same way and we standardized how we collect events.
This was Quite a challenge from a business perspective, because once you have that distributed freedom and everyone does what they want, you want them to change what they do just so everyone can be aligned.
It's not always easy, but there's a lot of benefits.
One benefit was that the number of problems we had with games launching and things not working, like for example, data from Apple Store, it was gone.
Almost every game now when they launch is just a lot simpler.
They know they call this component, they know how they're supposed to call it.
They don't need to think too much or reinvent the wheel every time there's a new game.
Also, it saves some time.
And my analysts have an easier time when they need to do an analysis on several games, and they can know that an event for login always has the user ID, and this is how you calculate a user ID.
It always has a date, and it has a location, and that location is a coordinates, or it's a country code.
If you standardize those things, which is very simple and very common sense, I'm not saying anything here that is complicated, but if you get to that point, you'll make your life a lot easier.
By the way, if you are using an external tool like Delta DNA, you are using an SDK.
All right.
Playing levels without having players.
How do you go about getting play data for your game without having players?
Again, we had a problem.
We had a game that was a match three game, Dogs Home.
Very nice game, developed by a guy who came in for King and he was trying to understand what the hell was going on.
We were creating versions.
And those versions had a specific learning curve in mind.
We were trying to create a learning curve that goes along certain pathways.
This should be a harder point.
This should be a monetization trigger.
This should be a relaxed area.
And what happened was most of the time, it was just too easy.
Just users were breezing through it.
We also had another game, Legends at War, that users just made it to level 20 without any challenge.
Like the first time they had any problems would be level 10.
And we would tweak it, and we would re-release the game, and this time it would be too hard.
And user would churn.
So we were trying to aim for a certain playing experience, but to get there every time we did it, we released a version, we had users play it, we lost those users, which cost us money in marketing spend, and we lost a lot of money.
The idea was, can we solve this problem?
Can we have a level played as we design it?
While we're designing the game, get data for what this level will be played, how this level will be played.
So this is Sisyphus, by the way, with a rock.
Very simple.
We just got a machine to do it.
What you're seeing here is actually a screenshot from the Mac of one of my guys in my team.
This is actually a Unity environment.
That's the game.
and you're seeing the AI that we created play the game.
You're literally seeing the game being played as you're designing it.
So the game designer, and this goes back to what I said about fit.
We were trying to solve a problem and we knew exactly how this would work.
It was actionable, we knew who's gonna use it, we knew what the value is, and we knew which process we would be helping.
We sat down with the team and we created for the game designer with a click of a button, he can test any level that he had.
He'd have a certain seed for the level and he would play it and he will get the results.
If we played 100 times, about take 10 seconds.
By the way, the version we just saw, this version with the graphics takes about three hours because of all the graphics.
We have a headless version where you don't see anything and you just get the results, that take 10 seconds.
So the idea was, Create an AI, which may sound like a super complex NASA project, but it took us a few weeks with one person.
None of the things that I'm using today is something that is insurmountable for a small team.
You do need a data scientist, but you need one and a Mac.
None of this was done with Pentagon Computing.
We created an AI.
This AI that we're talking about here was specifically for match threes.
We had AIs for other types of games in the company, RPGs, for example.
It does 100 plays in a matter of seconds, and we're improving that even more.
So the game designer would have a level.
He would say, okay, play this as if you're really good.
We call that explosive mode.
Explosive mode is the mode that goes for the maximum explosions.
Always looks for four of a kind or something that really is gonna bright up the screen.
And we can also say play this as a normal user.
So you can simulate user playing it, and you can get a lot of information for that.
One thing to keep in mind, this AI that we created doesn't tell you what's fun.
It also doesn't predict retention, it doesn't tell you if the users will buy, what they will buy, yet.
We're only simulating the playing.
But that was very beneficial for what we were trying to do.
All right, and it can be done by very many teams.
You do need a scientist, it is a prerequisite for this, but.
Okay.
Testing for small audiences.
My last example that I want to share with you is something that for us was a main point of pain.
This was something that was driving us crazy because the analytics people, we want to test everything.
And when you ask me, did a version work?
I don't want to say yes or no.
I want to say, well.
The theme worked.
That feature generated a lot of conversion, but at the cost of churn.
And that feature was completely useless.
No one tried it.
That's what I want to say when you ask me about a version.
But to do that, I need to collect very detailed information.
I need to do a lot of focus on learning.
I want to learn as much as I can.
The problem is, if that makes the game run very, very slow, and everything that they do, I want to have a statistical test, and I want to analyze it, I want it to be significant, so I need a lot of users in my samples, that becomes a big problem for the business.
That was something that we had to solve.
Now this also was exacerbated by the fact we had so many new games.
But I can tell you honestly, between us, we have this problem with the big games too, where I had millions of DAOs, even there I had this problem.
Even there would take me sometimes a month to give results on a feature.
That feature is for level 27 people of specific country who played a specific thing on an alliance, on a specific live ops.
That will also take a very long time.
So, we wanted to have A-B testing run more quickly.
We had a lot of new games.
Time was very critical.
And we wanted to stop slowing them down.
Now, Social Point is so smart about data that when we had these discussions, they would still do this.
Some companies that I worked at, if A-B testing is slowing you down, you would just not be doing A-B testing.
You just drop that.
You do something else.
But we did.
And what we did was we changed how we A-B test.
Now, I'm going to explain this in a very simple way, and this is something that everyone can do.
I will mention a bit the math behind what we did.
But in one sentence, what we did was we just applied what we know.
When we did testing, instead of coming to A-B test, completely naive and say, okay, let's see, what is the likelihood of someone buying this dragon, of someone playing this new type of battle?
What we did was we took what we know about users.
We had a conversion percent that we know.
We said, okay, given that we know users tend to convert, for example, on the first session, and users tend to buy the first dragon by level seven.
Given that, What can we do?
And this is in statistical terms, this is a switch, I apologize in advance for the Chinese.
It's a switch from a frequentist approach to a Bayesian approach.
It just means given what I know, what is the likelihood of something happening?
In business terms, what it meant was we took something that took a month and a million users and we made it take 1,000 users in a week.
It also means we took a lot of assumptions.
It means that when I have a new game and I say, well, I know my conversion is usually mm and happens on the first session, I assume that game is like my other game, which may or may not be the case.
So when you do these things, please consult an expert.
Okay, please make sure that you work with someone who can verify or validate if this applies.
But if you can take your learnings and take what you know, you can do A-B testing much more quickly with a lot less users, significantly less.
So we use what we know.
Another thing that I really recommend that you do is run what's called an AA test.
Take the same version, don't change anything, randomly sample two groups, 50% go here, 50% go here, and check what is the difference.
You will be surprised.
We found 5 to 10% difference with nothing different between them, just because of volatility in the market.
just because some whale came into one group and made some crazy noise.
So one thing you wanna know about your games if you are going to be testing is what's the natural occurring variance that you can expect.
And if you know, by the way, that five to 10% occurs naturally, this is what you would do differently.
You would try something, you would put something in a store.
You'd put a new offer in the store, and you would say, okay, this offer, I only showed it to some users, but the ones that I showed it to are 5% more likely to buy it.
If you know you already have a 5% or 10% variance, that means nothing.
Because then what will happen, and this happened to us in social point, we released it and nothing happened.
And the test said 5%, so what the hell?
So be careful with what you assume.
The idea of an A-B test is, again, to try to explain what will work and how much effect you can work, how much effect you can expect.
By the way, one of the biggest debates we have right now in social point is.
What good is an A-B test that shows you something would not work?
We have a big argument about this.
I have a lot of tests that I'm doing, and what I'm seeing is, forget the ones that I'm seeing damage, that I'm seeing it's gonna destroy a user base, that's easy.
I do a defensive test, I see that it's gonna be badly received by the community, easy.
The other thing is, I have some tests where someone has a great idea, they change something, we test it, and we see that there's no impact.
If you ask me, I will tell you, I think that's very useful.
Because you can avoid releasing that, if you know there's not gonna be an impact.
And most of the time, they listen to us and they don't.
Sometimes, game designers still release it, and guess what?
No impact.
Just more noise in the versions.
Just more distractions.
It's not always a bad thing, but innovation is also good.
Okay, so AA, and be very careful about what you see.
Consider the variance.
Another thing you can do, and this is just a basic business side of testing, try to use things that are zero or one.
You can have a test and say, okay, what will be the impact on user spend?
And then you have anywhere between, they will spend a dollar more a day, $2, $2.5, $10 more.
That's a continuous spectrum.
Statistically, that makes it harder to classify a group.
If you say, what is the likelihood that someone would buy?
Just conversion, one or zero, it's a lot easier to design a test for that with less users.
Retention, will they stay in the game the next day or not?
That's something that's a lot easier to do with smaller user bases.
So I strongly recommend binary variables.
What we achieved with this was, like I said, we just cut down by magnitudes how many users we need and how much time we take.
Now, one thing I'm gonna leave you with, this is an amazing success, we were very happy.
It made the new games able to use A-B testing as part of their work.
Do we do A-B testing in all the new games?
Nope.
This is actually an argument that I had and I lost, and I think it was good that I lost it.
Sometimes what you wanna do is you just wanna measure a big impact.
Like I said, we have a new game, that new game is aiming for 3X.
If that new version doesn't achieve 3x on its retention, it doesn't matter.
It just doesn't matter which parts of the version worked or what of it worked or not.
You can just throw the kitchen sink at it.
Did it work?
No?
Okay, try something else, but try something drastically different.
Don't just change, tweak.
For optimization, for tweaks, A-B testing, I think, are a must.
But when you try to make huge changes or measure things, very quickly, like in a new game.
Maybe more data is not the right answer.
All right, so what did we talk about?
We talked about the dark side of analytics.
I told you a bit about some of the problems we had or some of the things we did that we could have done differently or we're doing differently now.
Specifically, just keep focused.
Be very disciplined in how you apply data.
When you apply data, make sure that there's an answer that someone is waiting for.
Make sure that you're solving something or you're gonna change a process in how the games are made.
Otherwise, maybe this is not the most important thing to do.
We talked about tracking user actions and the benefits of letting the games do it on their own.
Do it yourself tracking.
If you empower the games, if you let the product teams run at the pace that they wanna run, you can change a lot of things.
You can remove a lot of bottlenecks.
Okay, playing levels with our players.
I showed you what we did to solve a very big design question that we had and some tips about how you can take A-B testing and still apply it to small studios, small audiences, small features.
All right, thank you.
Questions?
Yeah, all right.
Thanks for the talk, that was great.
I understand the concept of you automatically throwing something into your data lake once you implement it in one of your games, et cetera.
But somebody has to consume that eventually.
Somebody has to create a report that's going to show that to them, right?
How do you diminish the bottleneck between you showing a new feature to somebody when you have already probably some humongous backlog of more important things?
So the question is, I just, so for everyone to hear, I have this data, like I have this personal game-specific database that they can dump things into and analyze, but someone needs to visualize that data, or someone needs to do an analysis about it.
And that usually happens as a prioritization in the game.
So we have an analyst for the game, or they have a set amount of resources from my pool that they can use, and we work with them.
We usually do that with a self-service tool as well.
So the analyst doesn't work with engineering in the main dashboards that I do.
They use something like Tableau or Charter.
But then again, I have 10 versions of the same dashboard right now hanging around that I need to wrangle.
But you have to do the same for the visualization.
Otherwise, it doesn't work.
That's right.
Thanks.
All right. Yes.
Hello. In our studio, we have a really passionate player base that are quite closely knit, and we were against doing A-B testing for a while because we're concerned that players are going to compare the data and they have in the past with some things. So do you have any tips against that or for us how to work with that issue?
Yeah, so the question is about A-B testing not as a statistical experiment, but you're affecting the community, and some of the community is exposed to things.
Did I get it right?
Yeah, so before I came to the studio, when I suggested if we could do A-B testing, I was recommended it might not be worth doing because the players will compare the data and go, oh, why does this person have something that I don't in my game?
So you have to be very careful about how we do it.
We're very limited.
Some of the tests I want to do in my long-running games, there's an implicit contract with the players.
There's things like price sensitivity or economy that are really hard for me to change.
It's a lot easier for me to do in a new game.
But one of the things we do when we do roll out something, and we know some users will see it, especially when you do it with whales or heavily paying users, We usually communicate with them before.
We have our community managers involve them in the process.
And we say, we're going to choose, we're going to test a new type of warchief or a new type of legendary item.
And they know it's coming.
They're usually quite excited to it.
And they're like, who's going to get it?
But you really have to manage that.
If you don't manage the expectations, it can become.
It's quite easy with new games.
New games where the community is changing, you can pretty much change anything, and most of the cohorts, there's still not a community established yet.
But it's true, it limits what A-B tests we do.
Thank you.
All right.
Hi.
First, very, very good, very good presentation.
Pretty insightful.
I've got a question about A-B testing.
When you A-B testing and you've got many things to A-B test, do you A-B test them separately or do A-B test multiple changes at the same time?
And if so, how do you manage the, you know?
So if I understand correctly, the question is do we do A-B test and A-B test or A-B-C-D test?
Yeah, or if you have multiple.
Ah, multiple A-B tests happening at the same time.
Exactly.
Yeah.
Because they interact with each other and how do you manage this?
Yeah, well we have invested a lot into having a system that samples and really isolates users.
And we always have a group of users that is very big that we don't touch.
For a few versions even, that group would not even see new versions that we rolled out and we're not testing.
Because you, In small user bases, we just run multiple A, B tests, and we just make sure that users are not into groups.
But when you have a long running game, and you're testing something for advanced level, like level 10, and you're testing something for new users, some of the new users might end up level 10, and we make...
very strong efforts to make sure that technically that doesn't happen.
It almost never does.
Sorry, one more.
But do you allow one user to be involved in two A-B tests?
No, usually we don't.
Usually we don't.
The fact that we have tests now that conclude within a week or two usually, that's a lot easier.
Because they can just block them for a while.
Okay, cool, thanks.
Yes.
I was confused at one point where you said the answer was not to save everything.
Yes.
And then you said the answer was to save everything.
The question there was your bottleneck.
So you sort of said two different, it sounded like two different.
It depends.
When I say don't save everything, I think when you're approaching a situation where you're deciding to implement tracking.
be very sure that you know why you're tracking this.
I've seen too many cases where someone takes a database, just fills it up with data, what is called a data swamp instead of a data lake, and they just, it sits there, no one uses it because someone once asked the question, so they implemented it, and now when they come to use it a year later, it's unusable.
It's missing user IDs, the dates are not there.
So when I say don't save everything, I'm trying to refer to When you approach a question and you're not sure, try to refine the question.
Don't just say, yeah, you know what, I'll take that, and that, and that, because every one of those fields is gonna be a pain in the ass when you have to do data quality later.
If you start adding country codes to something that is global, you'll need to do the country codes.
And when you work with marketing and they have specific regions for campaigns, that becomes a bit of a hassle.
So be...
specific and surgical about what you're tracking.
That's what I mean when I say don't track everything.
The other thing is let the game track whatever they need.
So if a game needs something or they think they need something, I usually don't get involved in that decision.
I just let them run.
And sometimes it makes sense, sometimes not so much.
But they know not to come to me when there's a problem.
Right?
That make sense?
That answer your question?
All right.
First of all, thank you for the great talk.
I felt the same pain and same ideas because we provide more than 20 PCs and mobile games at the same time and I'm doing the analytics there.
I basically have two questions.
First one is, in the decision-making process, the first one was, is it actionable?
And I was wondering, if you take the time into account, for example, is it actionable in a week or a month?
So if something that takes more than three months, but that benefit can be really high, but also there must be some kind of probability of making a success could be a little bit low.
So I just wonder how you take those aspects into account when you're making a decision.
So the question is, How do I know if something is actionable when I decide whether to work on it or not?
It's a bit of a philosophical question because you can define a lot of things, but usually my goal in having that is avoiding things that I don't have a sponsor.
My simple, my litmus test is, if there is no sponsor from the business, there's no game designer behind this, there's no VP involved, why am I doing this?
Like, is it an architecture thing that I'm doing because I know we need to?
and I just do it, then it's okay.
But otherwise, if I'm trying to answer a question for the business, why isn't business in this meeting?
When I go into a meeting and there's no one from product there, that's what I mean by actionable.
I usually try to make sure there's a sponsor I know will use at least one game will use this.
My second question is about the lean startup approach.
We have games that have dozens of programmers and dozens of artists and loads of people involved.
And I really like the idea of fast, iterative approach, but sometimes you analytics guys would have to wait until the game is hitting the market with massive marketing budget.
So usually I see a lot of people in the first week, but then it would diminish.
So once it is released, then there are not many things I can do in terms of like analyzing sometimes.
Yeah.
So I just wonder like how you deal with that kind of...
We really try to make a case with it as a product and we had, September we had a roadshow.
And look, SocialPoint was very data driven, so it was very easy, but still I had, I was sitting down with directors of products and VPs, and I was trying to explain to them why before they even start developing, not the A-B test, when they think about a feature, I want to be there and do an estimate of the impact, because I want to make sure that as soon as I can, I can cancel something that I don't think will have an impact.
And it's a process, it's a process you need to go through.
Thanks.
It's education, but when you do it and you say this pricing would not work and they don't listen, and the pricing doesn't work, next time they listen.
Thank you very much.
All right.
Yes.
Thank you.
In talking about the low number of users and testing them early on, you had mentioned as the first bullet point using something that you already knew.
Yeah.
I wanna make sure I understand correctly.
Are you talking about basically verifying an assumption you already have or measuring the effect of it?
How exactly is that helping you?
user data, what I mean using something you know, when you come to a test, if this is the first game you've done, and you have no user data, it's not very applicable. But, you know, if you have already a baseline for the conversion rates, you have a baseline for what people spend their money on.
or where your best users are coming from, you can integrate that into the model and it would really lower the number of users you need to collect to have a statistical conclusion.
So it's just taking the numbers you have, the KPIs, into the model as a factor.
You change the model from being naive and every case is its own case, to taking answers you already have.
That's what I mean.
Make sense?
Okay.
I had a second question too.
In getting your company to adopt the lean startup model, was that something that came from a top-down approach or did you meet a lot of resistance in getting your company to adopt it?
So this is a funny story because I was sitting with the CEOs and the founders and I was trying to pitch them on this.
They've been doing this for a year.
I was just trying to say, and they were very polite because they're Spanish, and they were like, yeah, yeah, they're listening to me, but the company was already doing Lean Startup.
What I did was really change how analytics can become the champion of this.
I think a big part of lean startup is the experiments and the measuring.
And the biggest problem you have if you're trying to be lean is when data doesn't run as fast as the business.
This is why so much of what we did was just make it a lot faster, make it a lot more lean.
But it was coming from the top down.
They were already doing lean before I started.
Hi.
Hi.
Question on the topic that you talked about, the AA variants, instead of doing AB.
And you said serving the two different AA models to two different populations to differ variants.
I don't know if I missed it, but how random are those two populations?
So for example, do you control for something, at least for those two?
Like, for example.
does one, like A1, for example, is a population of whales, and then A2 is maybe a population of players that have never spent?
Like, do you control for anything at all, or is it just completely random?
So, excellent question.
I'll just repeat.
So, the question is, when I...
say you go to that group, you go to that group, how much thought is behind that?
Am I just randomly assigning people to groups or do I take what I know about them before I assign?
I don't assign usually by whales.
When I analyze, I look at how many whales were there to gain context for my analysis, but I don't control for whales.
If one group, it's, when you do it with a big enough audience, statistically, you should get somewhat balanced, unless you do it with a very small group.
The other thing is I do, factor that into my analysis, I don't say, do it by demographics, or I make sure that we have a good representation of all the countries. Unless I think country is super specific for that feature, if I think this is going to be something that's highly paid, and it's less meaningful to test in the Philippines than in Canada and the US, I might control for that. But most of the time, I don't, I can if I want to, most of the time I don't, when I do the analysis, it's one of the main things I look at.
All right.
Thank you.
Okay, cool.
I think we have time for one more question.
Yes, one more question there.
Thank you for your presentation.
Actually, my question pretty similar to the previous person.
When you do the analysis, well, before you do the analysis, you've got to understand how do you view the audience for variation A and variation B so that they would be similar in terms of geo, in terms of source.
But as I understood, you were talking about testing on old users.
So how do you build audience among old users?
Because, well, usually there are not enough users to make a representative analysis.
It depends on you.
So the question is, if I understand correctly, when I do this with old users?
How do you build audiences among old users when you run A-B tests?
So for me at SocialPoint, my life is pretty good in that regard because I have users who have been playing the game since 2012 and they're still paying.
So actually my active user base is very high.
I really do things differently.
And one of the things we're doing right now with the team is to try to distinguish when you do a test for new users, and when you do a test for existing users, what are the things you need to pay attention to?
This is actually something that we're trying to expand on right now.
But at SocialPoint, that problem of not having enough existing users is usually not the case.
The problem is that we'll be trying something that is level 25.
and you have had to play an alliance in the last live ops, and that really narrows it down.
So those who might run longer, those are the ones who actually take a lot longer to run.
Yeah.
Okay?
Thank you.
All right, thanks everyone.
