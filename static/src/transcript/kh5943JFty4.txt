I'm Alex, I'm one of the co-founders and the CTO at Servius.
Today I'll be talking about just generally our tech stack, how it developed.
I'm trying to give advice for how we learned to generally make it flexible and extensible for cross-platform VR game development across multiple games and genres.
Lots of it is based on the mistakes that we made that we learned from, so hopefully you guys can avoid some of those mistakes.
And just some examples of some of the technologies that we've developed over the last few years and how they helped us get where we are today.
So some of our previous titles that you may have heard of, Raw Data, it's a FPS sci-fi shooter.
Those videos are playing, right?
There, cool.
Sci-fi shooter, hero-based combat.
That was our first title.
Then we made Spring Vector, which is sort of a Mario Kart racer that has this locomotion system we call fluid locomotion that has a whole bunch of different moves.
Electronauts, EDM, Remix, kind of experience application.
Creed is a boxing game based on the Creed movie IP.
So in general, we've always been interested in motion control, driven mechanics, and see how we can create unique experiences using VR hardware.
And the last two titles that we recently released this year is Battle Wake, which is a Pirate Ship Combat Game.
So vehicular locomotion on the ocean with other ships that you're shooting at and using different abilities against those ships.
And also Westworld Awakening, which is based on the Westworld IP and it's set in the actual Westworld world.
And it was a collaboration with HBO.
So, raw data, this is the first game that we released.
There was a long kind of development cycle with a lot of iteration.
A lot of the stuff that went into it, we actually started working in Unity.
So this is going to be a pretty Unreal-heavy talk, because as we transitioned to Unreal, a lot of our existing tech has moved to Unreal.
We started working in Unity originally, so...
A lot of our knowledge kind of transitioned to Unreal and gave us a chance to rewrite that and improve on it.
And some of the most foundational pieces of tech that we developed even back in Unity days is this interaction system.
I mean that's pretty key to making a VR experience with motion controller mechanics, using your hands to interact with the world.
The interaction system that I'm referring to here is really how we set up the I guess the association between the player interacting with something in the world, like the starting of the interaction and the stopping of the interaction.
There's a lot of different components and movable pieces that are in the system.
Particularly, one set of objects live on the character side, the other set of objects live on the interaction side.
On the character side, we have obviously the avatar that has these avatar hands that have these interactor components.
Within the interactor components, they have a slew of what are called initiators.
So these are objects that are responsible for setting up...
Essentially, the paradigm of how an interaction starts.
So most simple example is pressing a button.
But you can create, it's a base abstract class, so you can create any kind of implementation you want.
So for example, even something based on touch.
So a typical example is like a hand scanner.
If I put my hand on a hand scanner, then interaction is just going to automatically start.
And then the initiators themselves, sorry if my programming names aren't great, I keep trying to rename these things, but they're they have these surveyors, they essentially survey the world for interactions and range so they're responsible for setting up how you find an attraction that can be initiated.
So a simple example is something close to the hand or potentially shooting a ray from your hand.
But this is once again a very abstract kind of interface up to the game to decide how we want to implement these and what initiators are set up, connected to what surveyors.
On the interaction side, we have the root interaction actor that has these interaction controllers.
So the controllers serve the purpose of grouping hand grips together.
And these are the hand grips that an interactor actually interacts with.
So typical interaction, you will actually grab a hand grip, essentially, in the world.
And the controllers also define what initiators they respond to and what kind of surveyables they have, which let them be discovered by the surveyors.
So this is a fairly flexible architecture that has let us do all interactions from all our games going forward.
I mean, we've been continuously improving it, obviously, but just breaking it up into these pieces has helped us generally keep it very flexible.
So the other part of this is the interaction positioning system.
So once the interaction starts, the question is what happens.
So a typical example of interaction will be you holding an object.
So this is kind of a tree-based hierarchical architecture that we built, where you essentially, the positionable nodes are connected together.
There's constraints, or a list of constraints that you can assign between these connections.
So, and this allows us to do things like a shotgun, where it has just a, you know, a trigger grip, but it also has a pump controller.
The pump is attached to essentially the pump controller, and there's a linear constraint that the designers can set how much it moves.
And there's a grip that's attached to the pump controller.
Now our constraints also fire events, like when they hit different limits, so it's very easy to then use this to set up logic in the shotgun to respond to the events of it hitting the different limits to basically like cock the shotgun.
But this is also reusable for like say a slide wheel switch where we have the...
It's a slightly more, I guess, complicated example where the grip itself also has a constraint to the controller.
And we can set up a constraint that lets the hand go parallel to the grip and even rotate around the grip so that you can get a much more natural rotation and position around the grip.
So all these elements, all these components are kind of reusable and just tunable in the editor.
And you can put these pieces together however you want.
And another system that we started developing back from those days is the marionette.
This is basically a procedural IK animation solution.
So, driving the entire characters animation based on the head and hands.
And I would say the foot placement particularly in raw data was still pretty rough.
That was actually...
The marionette was mainly written by our director of research who came on.
The foot placement, I'm going to have to admit, was written by me back then.
He has now polished it up and made it much better in Battlewake.
So if you play Battlewake, you'll see yourself standing around the table, messing around with other people, and you'll see a much more polished foot placement.
Weapon system is another core system that we developed in raw data that we have basically continued to use to this day in all our games.
It's flexible enough to even use for you know the in-spam vector when you shoot things out of your hands.
That's still the weapon system, and it's still also driving all the weapons in our battle wake, and it's also fairly modular.
We broke it up into different components so that once again game can The first thing that we're going to look at is the fire mode.
So fire mode is basically a fire mode that defines completely different systems as long as it implements interface.
So one core part of it is a fire mode component that defines essentially the behavior of when it tries to do damage.
So the most typical one is automatic, you hold the trigger down or just keep firing at a constant rate.
You can implement burst fire that way.
You can also implement spooling.
You can implement charge shot.
Charge shot entailing you holding the trigger down, charges up shot, and you shoot it.
And nice thing here is in raw data, like our guns could actually go between different modes because of your different powers.
So once again, encapsulating that in a separate component allowed the weapon to just completely transform its behavior by just switching what fire mode component it was using.
And the FireMirror component talks to our Damager component, which is, once again, just a base interface that just responds to some simple API, like tryDamage, and it's up to it to determine what to do, how to detect something that can be damaged.
RayTraced is a very simple implementation of that.
but that it can be further extended for, you know, we implemented penetrating damage.
So it wasn't just a simple hit scan, but it actually did a multi-trace and decreased damage as it would go through different materials.
And then there's also projectile implementation, which is spawned projectile actor that's simulated through the world.
Volume based is also a simple notation where if you want to do like a flamethrower, etc.
Then there's an ammo system that's also decoupled that hooks into this and a firing effect system.
So all these are fairly decoupled and by breaking it up into these different pieces, we're able to reuse this for our AI and even just autonomous turrets.
Or even in Spring Vector, there's like traps that in the world that You try to dodge.
So on the flip side, this is when we started developing our damage systems.
One part of it is a damageable component.
So the damageable component allowed you to author, essentially, the damageable attributes on a per-body region basis.
There's a damage asset that's associated, essentially, and it can be reused by different characters.
And it also is had a – the damageable attribute is a fairly abstract class that you can inherit that can essentially override how damage is dealt.
So you could make, like, an armor attribute or – potentially something that responds differently to different kind of elemental damage.
And this allowed us to support dismemberment headshots because now we can track essentially health on a per-damageable body basis.
Additionally, this is when we started developing a hit reaction system.
So this allows you to essentially associate the damage events to an appropriate response.
Most basic is plain animation.
So now our animators can make, for example, an animation for you getting shot with a shotgun in the chest versus getting shot in your arm with a pistol.
And this allows the designers to author how these are connected.
Other properties that you can trigger is ragdoll physicalization, partial ragdoll or in what the impulses are, or potentially whether it causes a dismemberment event or a gib event even.
And another example is it allows you to, for example, rotate according to damage direction so that if we have an animation of you getting knocked back, for those cases we actually want you to rotate the hit entity to face towards the hit direction so that the animation can properly push them in that direction.
So this is a system we are actually currently heavily refactoring for our next title because it just became just one giant mess and we are modularizing it much more so that it's made out of these hit reaction modules that are edit inline new objects, if you're familiar with Unreal.
So that we can, like one big part of this is selecting what the hit reaction is and the other one part is triggering it.
So by breaking these up into modules, we are, instead of having this one giant object that every game was trying to stick new things, it just became giant up to six things.
We're breaking it up so then you can have like a ragdoll module.
You're going to have an animation module.
And that thing can also tell you what the waiting is to determine whether that hit reaction should be picked.
So this is our first title, and it was also multiplayer.
So we've built most of our system with multiplayer in mind.
Multiplayer is not something you want to add on mid-development.
You're going to end up realizing some of your design decisions just don't make sense for multiplayer, don't work.
You're going to end up rewriting your systems.
So it really helps that we build our systems with multiplier in mind because now we have this whole family of systems that we can plug in and create new kind of multiplier experiences.
This is when we also start working on some of our backend services. We have our own dedicated service solution hosted on AWS.
This does allow us to do PC cross platform between Oculus and and Vive stores and just today actually was shipped the Vive port.
Battlewake update, so that we can do it with the Vive port as well.
PSVR is something we really want to support and obviously quest in the future, something we're looking at something you have to keep in mind though.
Not only are you dealing with just just even getting it to work, but you have to keep in mind of TRCs the platforms have different rules around.
cross-platform and on top of that now you're really locked into updating both versions at the same time which can be pretty challenging.
So especially if you're a smaller developer just having updating every single SKU at the same time can be pretty challenging.
We also develop our own leaderboard solution so that we can do cross-platform leaderboards.
So raw data was in early access when PSVR shipped.
We weren't originally going to port it to PSVR, or we weren't originally going to release it on PSVR.
But...
I'm just working on it was a great new platform. Massive new players are pouring into VR so we couldn't really pass that opportunity. So, we had to port raw data in early access. We did learn a lot about what it takes to develop for a more constrained system. Now we have PSVR as a leads queue and our quest. One of the biggest things we had to. We've.
We had to figure out pretty quick is spawning actors dynamically at runtime is just way too expensive.
I mean you're getting at least like sometimes 10 milliseconds, 20 milliseconds spikes.
And you know you only have like a 16 millisecond budget.
So we modified the engine to have a...
To make it much more easy for us to spawn a whole bunch of actor instances and have them essentially in a dormant state, and when they leave the dormancy and enter the dormancy automatically enable and disable systems like collision and simulation by taking the physics actor in and out of the physics scene, also taking it out of the rendering scene so the thing wouldn't render.
And also, of course, callbacks to inside of the actors and components that the thing was coming in and out of the pool.
So this makes it much, instead of you having to manually every time, like, go through every single actor's components and reset them, this makes it much easier for us to just add, set up whatever actors we want for pooling.
And on top of that, once again, all our systems are now built with pooling in mind.
So that really helps as we tackle new games, because we know we just can't be spawning all these actor instances.
There are some exciting and fun elements to get tried and tested by developers.
A lot of tools for performance there and pulling is something we developed because particularly we you know a lot of the UIs end up just being kind of stacks essentially of you click on one button and your screen comes on.
Well now we can just dynamically crawl through our screens based on the buttons because they know which screens they're going to and just automatically pre-spawn all those instances.
So you don't have to do as much manual labor to set up like what is being pulled.
Async overlap system, so we discovered that overlaps in Unreal, and that refers to essentially you have one trigger box, as a continuity of one shape overlapping with another shape.
So it's helpful for detecting when a character steps into a fire or something, or a character reaches a new checkpoint.
Now the Unreal implementation is pretty expensive, because essentially if you enable an overlap every time you move a component, it will do a physics scene query right there on the game thread.
If you have a lot of things running around, each time a thing moves, it will cost 0.2, 0.3 milliseconds.
Those things really stack up.
So we built our own really simple async overlap system that is completely homegrown, but it's also much more simple.
You essentially assign objects in different sets.
And you say, this set of objects overlaps with this set of objects.
The nice thing is you're not limited by the number of physics channels. You know, with physics, you have like 32 or 16 channels or something. We don't really have to deal with that. And it's much easier to debug because it's all our code and it's just really it's just box and box overlaps. It's a simple calculation.
And button remapping is an issue we had in porting raw data in mid-development.
We had to redesign some mechanics that just didn't work.
And we had to explicitly check for platform to determine button behavior.
So this is pretty messy and this is something we've been dealing with.
And only now I think we're coming out with a much cleaner design for dealing with this in Unreal.
So moving on from raw data, we wanted to really do something different.
We were tired of the gritty kind of environment, realistic graphics and shooting.
We wanted to just really kind of pivot.
And so we prototyped this platforming game called Spring Vector.
And this is kind of built almost like one of those, I guess there's a TV show or something.
Essentially it's an obstacle course.
And we found that climbing was surprisingly a comfortable experience.
So we were really excited about that, but the joystick movement between the obstacles definitely was uncomfortable for people and broke up the sense, the fluidity of the climbing.
But we just couldn't see how teleporting would work with an obstacle course, where you can just teleport around obstacles.
So we realized since climbing was so comfortable, just using your hands to move around was...
Definitely a way to go.
So, we ended up prototyping and developing a lot of different mechanics that use your hands to move around.
A lot of it was built around the concept of the stride, just as we climbing where you grab a node and you.
Through this, you release the grab button with the movement.
You essentially, every time you press the button, you are making contact with the ground.
And then as you move your hand back, you're essentially pushing yourself in the ground.
And as you release, it's as if your foot is taking off the ground.
So you're basically creating impulses that push you forward.
And that kind of movement system is surprising and comfortable because I think it really helps trick your brain that it's the environment that's moving, not you.
And then we also built jumping that's based on the same kind of concept, we added flying that allowed you to put your hands in front of you, and even added steering by steering your hands, which was also turned surprising comfortable.
This is, there was a lot of user testing at the office to especially with the most.
Not sure if you can hear me well.
I'm going to go ahead and get started.
Okay.
So we built this for nausea prone users to make sure this is as comfortable as possible.
We would make these systems kind of tunable at runtime with like buttons or whatever and basically run people through it and ask them whether different configurations would get them comfortable.
We took it to GDC, too, and had a lot of great feedback there.
But the funny thing is, once we took it to GDC, we realized that climbing was now the hardest part of the system, even though that's where it started.
So, and the other thing we realized is, you know, it's one thing to take your game to show floor and have someone standing there and make sure that the users understand what they're doing.
But teaching players how to use your system is, is really complicated, even raw data we had a lot of issues with players understand like we put in a bunch of tutorials but teaching players, especially new to VR is really tough.
We brought in focus testers and To try to run through our tutorials and polish that up.
And we revamped climbing.
I think we made great...
Sorry, I didn't realize it was...
Can you guys hear that?
Is it loud?
I don't know how to control this.
This is just a really cool video I'd like to show.
I can't even see the video.
I don't know why the screen doesn't show you.
Okay.
Yeah, so obviously some people figure it out and like we're total badasses at this.
We're even better than our players.
But at the same time, a lot of people are still like too confused with the whole pressing, releasing button.
So we knew we had to polish that further.
And at the same time, we also, a new project was spinning up and wanted to tackle another genre, and that was Creed, boxing.
We did some melee in raw data, but it was very simple. Your hands were kinematic, they would just go through essentially the enemies.
And, you know, one of the characters even had a sword, so she could slice enemies, but some people said it really felt like you were using a baseball bat rather than a laser katana.
So we really wanted to polish that experience up, and particularly the sense of the impact of your punch.
So that's the physical collision detection of the hands.
So we played around with physical simulation for your weapons and hands in the past, essentially using rigid body constraints.
But in raw data, during our early pre-production, We decided to go away from that.
The weapons would get stuck in the environment sometimes, but we'd get really confused, especially new users.
And it really didn't serve any purpose in the game, really.
And then here, we decided to try to tackle this problem again, but in a much more controlled environment in a boxing ring.
And here, because we had PTSD from all the physics, we decided we're just going to sweep the spheres ourselves and have more control over, essentially, what the collisions were doing.
I think that had pretty good results.
We were doing the same for the enemy's AI, so that if you block, and even in multiplayer, so if you blocked and the enemy tried to punch you, the sphere would hit and the AK would offset the hand, and that really added some sense of physicality to the combat.
We also wanted to not just make the act of punching impactful, we wanted to make it also the act of getting punched more real, more immersive.
So in raw data we had these giant robots that would run at you and charge you and hit you and you'd fly back.
So flying back is typically relatively comfortable.
I mean, moving forward is the most comfortable, but we realize that flying back is also not too bad.
But this is a boxing game.
We didn't want to make you actually fly out of the court.
So we first developed this kind of knockback mechanic, where if you get punched hard enough, you get knocked out of your body, which is a surprisingly comfortable experience and also.
Pretty fun mini game trying to match up what your character is doing, forcing you to kind of experience what's happening, but took it taking a step further, getting completely knocked out your body. And that's also a mini game, it gets consecutively harder the more you get knocked out, you have to like run faster and this is where we.
Put back the fluid locomotion, but minus the whole stride concept.
Realize just moving your hands fast to move forward is relatively comfortable.
We ended up adding it to also as an option inside of the actual matches.
But once again, we simplified it.
Just hold two buttons and move your hands.
And that was still pretty comfortable.
So after these two games, some common patterns of issues started emerging.
Rather foolishly, I would say, we put most of our code in this one core service plugin, thinking, oh, it's a plugin, so we're going to share it with all our games, and this is how all the games should work. Well, the thing is, VR was relatively new, so things were developing really fast. A lot of our assumptions for how the hardware was going to work just didn't work. And on top of that, we're tackling all these different genres, which was really breaking our assumption.
And on top of that, just having everything in one place just made things way too coupled, made it difficult to refactor systems in isolation, or even just debug systems in isolation, it was kind of unclear what was connected to what.
So our solution was to get rid of the base classes, we had these. So, we prefix all our core systems with SVR just to make it easier to immediately identify where everything is. Our game code also has its own prefixes based on on the Based on the code name.
So we got rid of all our base classes, like game mode, game state, pawn, and player state.
It was convenient to like, when you start a new project, I'm just gonna subclass these classes, and all of a sudden, technically everything works.
Except for we realized that all these assumptions were just constantly getting broken, and teams were just kinda arguing about what was the right way to do things at the base level.
So instead we just decoupled things into independent plugins with abstraction layers.
And to make the process of starting new projects easier, we have this template that has configurations of these really important base classes.
But really, these serve as shells as a way to put together our systems with loose connections.
And they really serve as an example.
So once a project starts, it's a complete branch of these classes.
And people are free to hack together these classes.
From that from then on.
So this is when we basically started this great pluginification. You can see a screenshot of our old serious plugin which has a whole bunch of folders that shouldn't even be there anyway.
And then on the right you have 37 current plugins that we have, and with more coming, and just even without within these plugins themselves we're trying to instill more of a So we want to make sure that we get a sense of modularity and breaking things up so there's actually multiple modules within these things so that it's easier to swap out subsystems within these systems if game decides to do so.
Like I mentioned, for example, in a damage we have the hit reaction we have the damageable in the weapon system. We have the fire modes, the damagers and the firing effects. All those have base abstract classes and living in, and then they all live in their own separate module so theoretically, those subsystems can be completely separate.
And so we want to make sure that we get a sense of modularity and breaking things up so that it's easier to swap out subsystems within these systems if game decides to do so.
Completely swapped out in the game, they need to.
So BattleWake.
BattleWake is the last game we shipped.
It's a, like I said, it's a pirate combat simulator.
This builds up on a lot of our existing tech.
You know, we wouldn't have been able to get to that point as fast as we could have.
One of its main pillars is obviously vehicle locomotion.
There was a lot of work put in here to make it a comfortable experience.
Some of it was learned from fluid locomotion, a lot of the comfort effects that we developed.
Obviously the water tech was another big element and that had to actually be tuned so that you get the impression of a rocky ocean but actually there's a lot of fakery going on so then when it gets under your ship the ocean actually comes down.
I don't know when we're going to tackle a big killer combo game next, but one system that's for sure we're going to probably start using a lot in the future is this ECS projectile system that we built.
So this directly hooks into our current weapon system. It's essentially a new type of damager, but this damager talks to this new projectile system because our old projectile system would spawn a projectile actor. Now in this game we're trying to spawn You know, thousands of projectiles flying through the world.
It would just be far too inefficient to actually simulate each one of these actors individually.
So this is a much more data-driven approach.
Essentially, the projectile definition is we have a data asset.
Within the data asset, there's a bunch of edit inline new objects.
Inline new objects, you can think of them as almost like components or whatever.
So, there's four of them.
Damage.axisBase, projectileMovementBase, projectileCollision and projectileFX.
So these are abstract.
You can declare different types.
Some movement.
You can just, the standard one, it basically just simulates gravity and travels in a straight trajectory.
But if you want.
If the player wants to, they can create a crazy one that does a spiral collision.
SVR Damage Access Base, it's actually what's used in all our damage systems.
Once again, you can make any kinds of damage calculators and attach it to anything, to the weapon itself.
changes the damage according to the distance from a shot, or how many things it got hit in its path.
And projectile effects, there's actually a list of them that are attached to this data asset.
Like one of them is an instant static mesh projectile effects, which essentially there's an instant static mesh manager which renders all the cannonballs in one draw call.
There's a projectile manager, and essentially it maintains a projectile collection array.
It's an array for each one of these data assets.
And inside of this collection, there's essentially a list of projectile instances with all that data.
So this is just kind of standard ECS data-driven approach, so that all the data is laid out in one flat buffer, and it's all in a pool, so that cycling through all these things is just really great for the cache.
Another thing that we developed that we're already actually using in our future titles is damage decal composition.
Essentially we wanted to give great feedback to the player about the locational damage on the ships.
The feedback exactly how much damage you're also dealing to the ship now the simple approach would have been to just attach decal components To the ship now the problem is here you're on the on the rendering side. You're now incurring you know charcoal per decal And on the CPU side, our ships are moving around.
So now you're updating each one of these decals on the CPU whenever your ship moves.
So, and on top of that, like, we have weapons that are just, like, raised and just sliced through the ship, like, every frame.
Like, that would have just spawned so many decals.
So instead, we essentially project.
We have a projection of the ship from four different angles.
And we essentially have a damage map for how the damage map looks.
And the decals are just essentially, we have a render target.
And decals are rendered into the render target as masks.
And we can use the four channels to display a different kind of damage.
And it's locational, too.
And the nice thing is here is the damage itself can be animated, too, because it's a damage map.
So whatever the artists want to do, They can and it's nice because it's coherent throughout the entire ship instead of you spawning little spots per ship.
So nice thing about this is we're now going to be using it on our characters, because in the past we didn't really have a clear, a clean way of displaying damage on a character's body.
So future developments.
a player movement refactor if I had to outline another big problem that or big mistake that we did.
And this actually started during essentially Westworld's development.
So it's, I guess, a problem of just sharing too much and I guess trying to put everything inside a service plugin also is an example of that I think sometimes engineers get really excited about the concept of over sharing everything instead of trying to make We're trying to branch out sometimes, so we really tried to reuse the character movement component in Unreal as much as possible.
But that was really a mistake because that thing was really built for just traditional joystick movement or AI movement and just trying to stick.
player VR movement in there has been a nightmare, especially trying to put in completely different movement mechanics like fluid locomotion and climbing.
And it just became this one giant file of lots of things happening.
Then you weren't really sure how things were connected.
And then on top of that, we're now trying to support different locomotion styles like joystick movement, fluid locomotion, and teleport.
And now you have even more complexity there, trying to go between these different things.
So we decided to throw all that out and basically started building everything kind of from scratch.
We based on this state machine approach.
So we have this state machine that maintains the current state.
So it's a base class, movement state, SVR movement state.
You can subclass that and really it's up to you to calculate the desired delta.
And that can be subclassed for different locomotion types.
So fluid, et cetera, or joystick, teleport.
And now it's very easy for you to, A, switch between these different modes, because you just say this is the state.
It's up to you to determine when you're switching between different states, because you still could have a climbing state that makes sense that would transition from there.
And there's also support for additive movement state, because turning in place using, essentially, the joystick to do a quick turn, that's an additive state.
So that can be added on top.
And on top of that, the handling itself is also further decoupled in a separate object.
There's a base movement input manager.
And you can declare one that works with a specific movement state.
You can switch these things around dynamically.
So you could make one, for example, for different platforms if you needed to.
Or you could decide in our game, we do like the joystick movement implementation, but we just want to completely throw out how we're talking to the movement state.
So that's given us a lot of flexibility and really helped clean that up a lot.
And just generally having full control now of the character collision logic, just outside of any assumptions of the character movement, just really helped us really polish that.
And then on the topic of input, that's another like, it has been another big pain point of ours.
Unreal comes with this input system that's INI driven.
So it's nice because, you know, anytime you have INI, Data driven methods of modifying your system, you know, you can't really target for you to introduce bugs, it's easier for you to change things without having to recompile everything. The problem is with that solution is it was really designed with traditional keyboard and joysticks in mind, I mean like the engine has been around for like 20 years or however long. So, one.
I didn't want a unique feature of using motion controllers is there's two hands right so you actually have two of the same buttons on both hands. So, really wanted a system that.
would understand, would be intelligent about the fact that it's the same button, but it's actually different hands.
And on top of that, dealing with the fact that actually there's on PC, we have drastically different kind of motion controllers, like Vive controller versus Oculus controller, sort of similar buttons, but not really.
And we really wanted a much cleaner way of setting up bindings for this platform versus this platform, instead of doing a bunch of ugly platform checks within the code.
We built this new INI-driven system SVR input based on the Unreal vanilla implementation.
It dynamically modifies the input bindings according to which motion controller you have.
Also, we built in control scheme selection, so once again, all INI-driven.
This cleans up a lot of the code, but now we just have to tell the input system, hey, this is the by string, this is the variant they're using, or this is the variant using, and the system automatically rebinds everything.
And we also have dominant hand selection.
So whenever you switch to left-handedness, we typically switch all the buttons from left to right hand.
So all the bindings, once again, just automatically get swapped around.
When you say, this is the binding, you say specifically, do you want it for both hands?
Do you want it for primary hand or secondary hand?
And also, you can explicitly turn on input components for left hand versus right hand.
So this is really great for the interaction system, because when you grab an object, you could be grabbing it with the left hand or the right hand.
And with default Unreal, you're basically just binding to both buttons and then checking which hand is holding it.
Now you just bind to the event and you just, when the interaction starts and stops, you just basically tell the input component which hands are holding it.
So this is just a really messy example of how we were doing some stuff in raw data.
We're checking which...
Which platform we were on, we're still using input component in one place, but we also had these hard-coded button events on the interactive component.
And this is really messy.
And this is an example of how we're binding stuff for fluid locomotion.
It's all config-driven.
We have this concept of ambidextrous keys, essentially, as in for both hands.
I don't know if that name makes sense, but that's what I'm using.
And essentially, you now say, like, this is the left button, this is the right button.
This is the buttons how I'm going to refer them to this is the platforms that I'm going to be using it on. And when you add the binding you say what side mode it is, whether it's for the secondary hand the primary hand or both hands.
And, yeah, so this that has really helped us clean things up so in conclusion, yeah, definitely try to break your systems into decoupled modular blocks.
It might mean there's more maintenance and more boilerplate code connecting things, but over the long term it's going to be easier for you to manage this.
If you want to do multiplayer, definitely keep multiplayer in mind, but definitely keep multiplatform in mind.
If you're trying to do, and nowadays VR is so fractured, try to develop systems with all the different controller variants in mind.
Stay nimble, I guess.
And yeah, think about future games.
Think about when we're picking projects, we're explicitly thinking about, OK, how can we pick projects that let us develop tech that will then help us with future projects?
So yeah, thanks.
Do you come up with an interesting system and then design a game around it?
Or do you start with, I found a game and now I'm going to make an interesting locomotion or modular gun system around it?
Both kind of happened, but we definitely do like to think about what challenges in VR we haven't solved yet, or other people haven't solved yet, or at least haven't solved to the level that we think is great.
And then we see, particularly lately, what IP fits into that, which really helps with the marketing angle.
I mean, with game, it's always about the gameplay mechanics.
You can have a bad story, but good gameplay mechanics, but if it's the other way around, then it's not a good game.
