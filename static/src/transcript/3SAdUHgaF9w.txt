Hi everyone, good morning.
Thanks for joining today.
I'm happy you can make it.
I'm happy I can make it because my flights got crazy yesterday and I got here at 2 a.m.
It's okay.
It happens.
So I guess before I start, I was asked to remind you to fill your evaluation forms.
So you can scan your badge and it will send you a link so you can add that.
It helps us make sure that the talks are better and also some of us get into this really cool car deck.
So evaluate your talks.
All right, so welcome everyone to step into your player shoes and how we made the most of Team Playtest on Skate.
Today, I'll share with you a process that we applied to the internal template testing to make it more user-centered and to gain more relevant feedback from the team.
So let me start by introducing myself.
Hi, I'm Andrea Gonsalves, and I'm a senior X designer at Full Circle Electronic Arts.
I'm originally from Portugal, so I might have a little bit of an accent, but I'm currently based in Vancouver, Canada.
I have a background in both design and programming, and I've been working on games for over seven years now.
I've worked in mobile, PC, and console games.
And I've also taught, mentored, and spoke at different schools and online platforms.
Currently, I'm working on Skate, and I've been on the team for over three years now.
Before we jump in, I want to set some context to the talk.
First, I want to share that we're still working on it.
Skate is not out yet.
So in this talk, I'm going to be focusing more on the process that we used and not so much on in-game footage.
So if you were hoping for a sneak peek, I apologize.
I will not be able to share anything that is not public yet.
But you can see a lot of what we've been working on on our YouTube channel.
The next thing I want to talk about is maybe you are wondering why we're talking about team playtesting at the UX Summit.
Because it's not testing with users, right?
So I wanted to start the talk by stating that team playtesting does not replace user testing.
And in fact, in our team, we've been testing with players from early on.
We have videos in our YouTube channel that you can see of Wacky Graybox World.
But team play testing is another tool in your toolbox to ensure that you're going in the right direction with your experience.
And we have found in our team that there are ways that you can make this process be more useful in your UX journey so that you can leverage an existing source of feedback to increase awareness of UX.
So with that out of the way, let's look at what we'll talk about today.
I'll start by introducing team playtesting and its strengths and weaknesses.
Then I'll share our process, which internally we call focused team playtesting, to get the team to step into player's shoes.
Then I'll walk through how to put this process in practice in your own teams.
And at the end, I'll share some learnings about using it over the past years.
So let's start by looking at team playtesting and why it's worth leveraging.
First, let's talk about why do we test.
The reason we test as devs is that our goal is to make a good game for players.
And we have tons of tools to help us assess this.
There's the team play testing, there's heuristic evaluation and expert reviews, there's quality assurance departments to make sure the game is working as we plan, and there's external research such as external usability tests with players and external freeform tests with players.
And today we're just looking at one in particularly over all of those tools.
So what is team play testing?
I'm sure a lot of us are familiar with it, but let's define it just so we're all on the same page.
At its core, team play testing means you're testing with members of the team, as opposed to people that are not in your team, like your players.
And it's often a common practice in studios and game teams.
It's also often recurring, such as happening weekly or monthly.
And this team playtesting can be done with different size groups.
So sometimes it's just two to three people that are actively working on something, but you can also be with a whole team and there's people that haven't seen any of what you're working on before.
In my experience, in the teams that I've worked on, we always had at least a team-wide playtest that happened weekly.
Template testing has a really key strength, and that's that it gives you quicker and lower cost iteration loops.
This is for a few factors.
One is that it overall usually has less logistics.
You don't have to recruit external participants and plan extra studies.
Developers already have access to what we call dev builds that are easier to create and access.
And it's not just quicker to implement your design for the first time, it's also quicker to act on feedback and implement changes.
Because of the things I mentioned, like logistics and access to the dev builds, but also because the target quality that you need to make for the dev team might not have to be as high as what you would have to do for players.
But team play testing has a really key weakness, and that's that you're not testing with your target audience.
Developers are a very different audience and come with their own set of biases.
So what do we mean when we say that we're not testing with the target audience when we test the dev team?
Usually, it means two key things.
One is that developers are too familiar with the game and are, well, developers.
And this means that they can see the game from a fresh perspective, which is a cognitive bias known as curse of knowledge, which makes it hard to look at something through the eyes of someone that doesn't have as much familiarity as you do.
But developers also usually use the game in a different way than players for their workflows, where they are testing, what they are implementing and iterating on.
Some workflows that the users will never use.
The other thing we usually mean is that developers, and actually any player and any person, has varying amounts of personal tastes and motivations.
They can value different features in games, be excited by different things and different genres.
And that doesn't always align with what our audience is looking for and the game that we're trying to create.
This is not a negative because it's awesome to get so many ideas and it's great to look at different genres.
But you need some parameters to be able to interpret all the feedback and suggestions.
External user testing usually solves this for us.
When we externally test, we look for participants from the target audience.
And sometimes we look for new players so we can specifically get a new perspective.
And these tests can be sometimes freeform, where players play more organically, or focused, where we look at a specific part of the game and get deeper insight.
But external testing is not always available, since sometimes there's no bandwidth or budget, and some smaller teams may not have access to these all the time.
So, so far, I covered the benefits for iteration that come from team play testing.
But in our team, we also saw so many benefits for the team itself.
Full Circle is a remote studio, and this team play test became a weekly touch point for us to come together with a shared goal of making a great game.
Over the pandemic, we had to adapt how we work, and tools like Zoom, a video conferencing app, and Miro, a digital whiteboard, became part of our day-to-day, as well as these tests, and directly impacted how we were able to conduct them.
In this test, we share our design and intentions for different parts of the game.
And this has helped build a shared vision among the team.
It has also allowed us to give a chance for everyone to give input on parts of the game that they don't always work on.
And lastly, because we're working on a multiplayer game, it actually became a fun activity where we can connect and have fun together as a team.
Our studio is committed to diversity and inclusion.
We want to have different perspectives in the team, and we want to listen to them.
So this team play test became such an important practice to support this.
But with all these different perspectives, there also came a need to interpret the feedback.
So by now we saw that team play testing can give you a quick and low cost iteration loop, fresh eyes, and different perspectives in the team, and can help with team alignment, engagement, and creating a shared vision.
But it has the big downside of developer biases.
So how could we leverage the pros of team play testing while minimizing its weakness?
Although we can never remove all the biases at play, what if we could try to find the closest to our players in our team?
This is what we try to do.
With our process that we call focus team playtesting, we get the team to step into the player's shoes.
We do this by providing them with scenarios to try that are rooted in player goals, and we segment their feedback based on relevant traits that they have in similarity with players.
And this has resulted in more meaningful feedback, which for the context of this talk means that the feedback that we saw from the team became closer to what we were seeing from players.
And more team engagement and UX buy-in.
So we saw more participation from the team and higher understanding of player pain points.
So now that we covered team playtesting and talked about focusing playtesting, what does this look like?
So for us, focused in playtesting has two key pieces.
One is a player lens that you use to assess the game from, so that you can look at it from your player's perspective.
And this includes scenarios that the team will try, and the user traits that you want to use to collect and weight feedback differently.
And the other piece are the logistics around planning and running this playtest the most effectively to get relevant feedback.
Let's talk about the player lens first.
The player lens captures who a feature or a game is for and how it's going to be used.
And it's composed of the scenarios and user traits.
Scenarios represent what players will actually do and how, and is what the team will actually try in the tests.
So what does the scenario look like?
A scenario can be about performing a task in the game in context.
An example where we use this was with our capture feature, a feature that allows you to capture photos and videos in the game.
We saw that some players spend a lot of time in this feature making perfect photos and videos, but a lot of players just want to quickly go and grab a photo of something cool they just saw so they can save it and share it with others.
So when we asked the team to test, we asked that they try using this feature in that context of just seeing something cool and wanting to quickly grab that.
So that has taken us from asking the team to do things like just take photos and use these two features, such as camera angles, to perform a trick that you're really proud of in the game, then go to the tool, take a picture of it and save it, and continue your gameplay.
Scenario can be about using a feature for different goals.
We saw this with our building feature, which is a feature that allows you to place objects in the world.
We saw some players use this feature to create skateable objects.
And these need a lot more precision because people need to skate on them.
But other players just use it to reach higher areas in the world and explore.
So when we asked the team to try these tools, we asked that they try it for these two different purposes and give feedback on how well it works for either.
A scenario can also be about placing the team in a certain situation or context that is similar to what players will experience.
An example where we used this was testing our social experience.
In this case, we placed the team in breakout rooms of a certain size so we could get feedback on how it feels to play our game as a party.
And lastly, a scenario can have constraints.
An example where we used this was also with their social experience.
And in this case, we asked the team to not be on video conferencing and not use voice, so we could get feedback on what it feels like to play the game if you're by yourself at home.
Now that we looked at the scenarios, let's look at user traits.
So the user traits represent different aspects of a player that are relevant to consider for a certain design.
An example of a trait can be the level of familiarity with a genre or feature.
We use this one a lot, but two examples where we use this was with our capture tool for capturing photos and videos, that when we collect the feedback from the team, we ask that they give their feedback in different areas of the board that we were using, whether they were new or more experienced when using this tool.
Another situation where we use this was with our gameplay controls.
When we tested the controls, we asked the team to give their feedback in different areas, whether they had low, medium, or high familiarity.
Because we're asking the team to place themselves in what user trait they think is closer to them, we gave them a proxy of how long you've been on the team is mapped to your level of familiarity.
A user trait can be about a motivation to play the game or use a feature.
An example where we used this was with our building tool, the tool to place objects in the world.
We saw that some players want to make huge creations with many objects.
while other players just want to modify something that is in the world quickly.
And even other players are not interested in using these tools particularly.
So when we ask the team to test these tools, we ask that they give feedback in different color sticky notes, depending on which motivation they most align on.
So that when we look at the feedback, we can consider the player type that they relate to.
And lastly, a user trait can be about a shared pain point.
So we did this with our gameplay controls.
We saw there are a few people in the team that had difficulty with the control complexity.
So we identified those members in the team and we actually did a team play test just with those team members to get more information on the pain points and then test improvements.
So now that we looked a little bit at the focus template testing process, I'll show you how we've used this in the past to iterate through a feature, and we mix and match testing approaches to get it from not working to a desired state.
We did this for our capture feature, the feature that is used to create photos and videos in game.
So we needed to do improvements to this feature.
We saw from externally testing with players that it was not working and it was expressed through metrics that we were targeting and that were below what we wanted.
When looking at the actual feedback of what players were saying, we saw that the feature was too complex and complicated and in one of the tests, users were overwhelmed and almost no user actually ended up using it.
So we went back and we created a player lens to help us assess this problem from.
And we saw with taking photos and videos, there are users that want to do complex creations and spend a lot of time in the tool, and users that just want to take a quick photo and continue playing.
And we saw that the issue was occurring for users that wanted to quickly create, so we decided to prioritize that.
And it kind of was supported by the research that we were seeing where players said, I just want a quick way to take a photo.
We have another external user research test in the future for the whole game, but we didn't have any bandwidth to do any further testing milestones along the way to make sure that this feature was improving.
So we looked at how we could do that with the team.
We started by doing internal one-on-one tests with the team.
We recruited participants quickly on Slack through a survey where we asked motivation that they have to create and how new they were to the team.
Then we conducted tests over Zoom in a one-on-one scenario where we asked the team members to do tasks that were related to quickly creating.
And after each session, we would debrief and see how well they were able to do each task and what were the problems and things we needed to improve.
We did a total of two rounds of these one-on-one tests, each of three participants each.
And at the end of each of those rounds, we had a new version of the design to test.
So after the end of the internal test, we had one version.
And at this stage, we wanted to get wider perspectives.
So we went to the team-wide playtest.
At this stage in development, our team-wide playtest had about 50 people, all at once, in a sort of usability test at scale.
These days, these tests have like 100 people in them.
I know it can maybe feel a bit overwhelming by looking at the picture, but actually when you're participating, it feels so energizing to feel like we're all playing the game at the same time, and we're all providing our opinions, and we're making the game better together.
Because this was not a one-on-one test, we had to find ways to deliver these tasks to the team at scale.
We used Miro, the digital whiteboard tool, to lay out the tasks that we had used in the internal one-on-one tests, a simplified version of it, so the team could do at their own pace.
And in the board, we created two different sections for them to leave feedback as they go, and one area if they were new to the tool or more experienced.
And after the test, I had tons of feedback and sticky notes to go through, but it was really cool.
And I was able to find themes and see differences between what issues were affecting new participants versus more experienced participants, and was able to prioritize and decide what I would action on.
So after this test, we had yet another version of the tool to use.
And it was actually time to test with players again.
So we did an external test with players at this stage, and we saw a lot of improvements.
After these tests, we saw that players were able to accomplish all the tasks that they were meant to do with this tool.
But we didn't quite hit all our targets.
And in particularly, there was a usability target that we didn't hit.
Based on feedback, we narrowed down the remaining issue to a learnability issue.
So there was a part of the feature that players didn't understand right away, but once they use it, they found it intuitive.
So we decided to solve this in particularly with a tutorial just to walk the player through using it for the first time.
And we decided to test in a team-wide playtest so we could get a lot of perspectives.
We got positive results from this internal team play test, but we also saw that we didn't have enough new people in the team to truly be confident of these results.
So we really needed to test with external players.
So we tested again externally and we finally hit all our metrics.
So we achieved our targets and there were no more critical issues with this feature at this stage in development.
So by leveraging the focus team play tests, we got internal feedback that helped us improve the solution in a way that met player needs in the end.
And we saw the improvements happening within the team, starting to map to the improvements to our target audience more closely.
And this was available to us when extra user testing was not possible.
I've introduced the focused in playtesting process and we saw an example of it in practice.
Now I'll cover an overview of what goes into planning and running one of these tests.
In the last section, we looked at the player lens part of focused in playtesting.
And now I'll cover the logistics, which are tightly related with how to run these most effectively.
We'll cover the overall process, as well as some tips and tricks I want to share with you.
My hope is that these will help you implement these with your teams.
Maybe, who knows, next week.
For us, running a focused team playtesting has three important stages, planning, running, and action, and key activities to do at each stage that relate to the process.
In the planning stage, you want to create that player lens that you're going to use to assess the game from.
And this sets the user and the context of use.
So you're setting who it's for and how it will be used.
In our team, usually this is done by designers, so game designer or experience designer.
We create this based on assumptions or research that we have.
In the planning phase, you also want to define this test format.
So how are you going to test to approximate the context of use that players will actually be in?
Do you need to place the team in certain constraints or contexts?
Which tasks do you need to ask the team to perform?
And is it better as a team play test so you get tons of perspectives, or a one-on-one scenario so you can get more deeper insights?
Then it's time to run this test.
There's lots that go into running a team playtesting, but the most important thing in this process, I think, is the facilitation piece.
Here, you want to deliver clear instructions of what we're testing and why we're testing it, so people can participate.
And then you want to make people feel they can give you honest feedback, and that their feedback and time is going to be valued.
The last step is action.
So, depending on the size of your team, you're gonna have tons of feedback, and you need to interpret it and define the actual improvements you wanna make.
And here, it's really helpful to contrast the feedback that you receive from the team based on those different player types that you were seeking out.
After this, you will have a list of improvements that you wanna make, and you will implement them, and then you can use this process, or a different process, to validate that those improvements were actually effective.
I will cover just a bit more facilitation since I think it's an important part of getting the engagement from the team and good feedback.
The first piece is delivering clear instructions and context.
You wanna tell the team what we're gonna try and why, where it's at in the stage of development, and how it fits in the broader picture of the game.
This is gonna empower them to give you feedback in relation to that and feel like they're part of the journey.
it's really important to create an environment for honest feedback from the team.
And this comes from having the team feel like their feedback, that they are safe and that their feedback is valued.
This comes from really encouraging everyone's opinions, especially saying that if people feel like their opinion goes against the majority is even more important.
and also providing inclusive ways for everyone to participate.
So Miro has been really great for us because people can deliver their feedback by writing instead of having to speak up in front of everyone.
And to show the impact, you always want to look back with actions and results of what you do with the feedback so people can see the outcome of their participation.
Another thing we found helpful is asking participants to tag their feedback with names.
This has helped us follow up if we don't understand a piece of feedback or we need a little bit more information, or also helps them understand that team member across playtest to have a better understanding of their player type and better support it.
And lastly, and this is often a difficult one, is to always direct feedback to your collection method.
What I mean here is that, sorry, to the collection method.
This can be really hard in the team-wide playtest where everyone is there all at once.
And someone may feel like something is so obvious they should just ask the group what it is, and people are going to hear their question and feel compelled to answer.
This is hard to go around, but this actually is the feedback that you want to get.
When someone feels like something in the feature is so obvious they should just know it, but they don't, and they feel like they need to ask the group.
So you want to intercept, ask that they provide the feedback in your collection method, and then find a way to unblock them so they can continue with the test.
So now that we saw the focus template testing and what it looks like and cover how you can plan and run one of these, I'll share some learnings from using this process in the past few years, and we'll wrap up.
There was a lot to share, but I picked a few ones that I wanted to share with you today.
One thing that we learned is that it's really important to reduce friction for the team.
At times we wanted to ask more questions, give them more tasks, or get more granularity on their segmentation.
But when we increased the steps and the complexity, we saw the feedback decrease and the quality of the feedback decrease.
So we always try to be mindful that the team is seeing something for the first time and they're playing the game which takes a lot of resources.
So we need to be mindful of their cognitive load.
We also saw that the effectiveness of team playtesting and this process can vary.
Although applying focused team playtesting has been better in general than when we don't apply it, it's not ideal for everything.
And one case that we saw is when testing a tutorial or first time user experience.
There's such a really heavy bias in the team of knowing too much about the game and being passionate.
And it's not realistic to have a constant influx of new team members in the team to test with new perspectives.
Another thing we saw is just how powerful first-hand experience is.
So when the developers actually get to try the game as a player would actually do, they get a deeper understanding of the player pain points.
And this makes them rally around fixing them.
So instead of being a juror of something that sounds like a minor issue, they realize how frustrating and big of a deal it is.
So this has been particularly effective on showing how sometimes simple tasks are really hard for players to do, and it's something we can forget because we use the game all the time.
The last learning is that diversity is really key.
You need to have diversity in the team to have access to all these different perspectives.
This will give you more chances to find people that share different relevant traits with players.
So how do you get your team to step into your player's shoes?
You can do this by running more focused in-play tests, where you create a player lens, where you provide scenarios for the team to try, and get feedback based on relevant traits to the focus.
And you plan and run these tests in a way that creates a context that players will actually be in.
This will give you more relevant feedback and confidence that you're solving real player pain points.
Because you set a focus, you get feedback on that focus.
Because you give scenarios, you get feedback on things that players will actually try to do.
And because you're segmenting, you can see which issues affect different types of users.
This lets you weight the feedback to solve problems players will actually face.
And it also gives you team engagement and UX buy-in.
Because you provide a clear purpose and context, the team can give you feedback in relation to that.
Because their feedback is more relevant, it's more likely that you're gonna apply it, and they will see the outcome of their participation.
And it's also a chance for anyone to give input.
And we heard many times from our team members mentioning that this way they can feel that they contribute to other areas of the game they don't actively work on.
Overall, this process has been a tool to understand and empathize with the end player experience and help rally the team around solving player needs.
So we looked at team playtesting and how it's a useful source of feedback but can become more user-centered.
And we saw that focused team playtests help the team try the game closer to players' perspectives.
And that focused team playtests lead to more relevant feedback and higher team engagement.
So after today, I hope you have the basics to invite your team to step into your player's shoes by running more focused playtests to help improve your internal feedback and increase empathy for player pain points.
Thank you for listening.
I wanted to share before we go for questions that Full Circle is hiring.
So if you enjoy what you saw today and are interested with working with us, you can follow the QR code or check out our website directly.
And with that said, I think we are out of time.
But we can take a few questions.
So we can take a few questions here.
And if we have more questions, we can do it in the wrap-up room 2014.
Please go to the mics if you have questions.
First question there?
Yes, hi.
I have a quick question about, so with the way that you did the team playtesting, you had it all be happening at the same time through like a team's call or something.
Would there be any change to the system if you're hosting this kind of playtest asynchronously where everyone's kind of doing it on their own time over like, say, an afternoon or evening that they have the time to?
So if you were doing in person this playtest, or more so asynchronously, like people doing it basically on their own time when they have the chance to?
Yeah, I think I would prepare the collection method the same way, so providing all the tasks and where things are going, where to give the feedback, and make sure that the instructions for how to test and how to deliver the feedback are very clear so people can do it on their own time.
But I think the format works well for asynchronously as well.
Awesome, thank you.
Thank you.
In a game like Skate where creativity is such a big part of it, how do you maintain a level of abstract interpretation of the level design in internal playtests where people are so familiar with the game world?
That's a good question.
I don't know if I have like a clear answer for that, but I would say the team is really diverse.
So you have, we actually have people that are very aware of that.
And there's people that don't have the same familiarity.
So I think that's where the strength of diverse team comes because you have people that are super passionate and are going to give you like the fan feedback and you have people that are newer and can give you a perspective of someone that maybe is a potential player.
So I would say it comes from the diversity.
It will give you perspective from different people.
Thank you.
Hello.
You guys are lucky that you have your own external playtest team that's normally supposed to do most of these playtests.
Developers are busy.
Every time they're playtesting, they're not developing the game.
That's why you have that team.
What is the ratio of developers playtesting versus your external team playtesting and does that change as the game matures where you want more external versus more developer it's obviously gonna be a balance of love to your perspective Yeah, I mean, I would say probably in the beginning, there's a little bit more internal, but you should be testing with players as soon as you can.
But you do have access to your team a lot of the time.
So maybe in the beginning, there's a little bit more internal.
And later, you have access to players, which is the best target audience.
So we still do internal because it gives you, like when we're starting out features, we can test them really quickly.
But we start testing with players much quicker because we have all the infrastructure ready.
One thing that I wanted to mention about like having developers testing the game and when they're doing it they're not developing is that there's kind of like unseen gains of having developers test the game and I kind of mentioned with like the shared vision and now they understand how the game is supposed to work and when they need to do their day-to-day decisions that you don't need like a full design spec to decide how to do one thing versus another and Because they participate in these sessions and they understand where we're going, they're able to make those decisions based on our best understanding of players.
So I think you get a lot of benefits from the team culture that you create with these tests that sometimes can be forgotten because it's like, oh, the gyros are not moving.
We're not completing them fast enough.
Thank you.
Hello.
First of all, thank you for the wonderful talk.
A lot of actionable stuff.
A question I had, when you're doing focused playtests in a team-wide setting, how do you avoid the issue where if one person vocally focuses on one issue that the other, the rest of the team basically starts snowballing into recognizing that specific flaw and minimize the bias that people have regarding that?
Yeah, there's definitely an issue that we had, especially in the beginning when we started introducing this process.
And what we tried to do is set an expectation that first we want to collect things by writing, so people contribute as they go in writing, and then we try at the end of the session create like a forum for discussion so people that prefer to speak vocally can do so.
But of course in the beginning people are going to try to talk if they're comfortable talking.
So you want to direct, oh, we don't want to get into those issues or bias the feedback.
But that's a great point.
Can you write it in the board so I have that and then we can discuss more at the end.
Thank you.
Thank you.
Great talk.
I got a couple questions.
First one is how early did you start doing focus play testing with your team?
Was it in pre-production or were you already in full production?
I cannot answer where we are in the process of the game, but we we started quite early I think the first couple playtests Were more freeform because we had like very bare bones But I would say like half a year in we were doing focus team playtesting Especially like once we started playing with players.
We kind of start seeing like maybe there's a bit discrepancy here that we want to get us off Okay, second question is Are your weekly playtests always with your full team, or did sometimes the one-on-one sessions replace those?
In our team, the team playtest, we have one weekly.
So it happens every week.
And because it's scheduled, it reduces a lot of logistics of preparing and inviting everyone.
And then different groups make their own extra playtests or one-on-one sessions when there's a need for one.
So we have our more focused team playtesting once a week, but we also have a casual playtesting that is just for team, how do you initiate this process?
Because my assumption is this team testing process can be very or kind of time consuming to every discipline and say, how do you convince the producers to say, how can we sleep that much time and energy on our team to do such a thing like set up those scenario and have weekly meetings, group everyone together, you know, how do we make that happen in a team?
So, there's two scenarios here.
I think one is a lot of teams already do internal team playtesting.
So, that's a great opportunity to just try to sneak into that process and be like, oh, let's try this other way of running it once.
The other part is kind of pitching for having weekly team playtests.
And I think I would suggest kind of starting to do it by yourself and show the value that that creates.
And then people will be more likely to invest in that.
And then you can increase the amount of effort you put into it.
Like maybe your first one is not focused in playtesting.
It's just playtesting more formally.
And then you're like, oh, maybe we should add this as a next step.
I think those are kind of the main things I can think about.
But usually if people already do internal template testing, it's easier to try something new.
Something else I wanted to say about the cost is that You don't have to create this player lens and format every single time.
Once you create it for a feature, it's really likely that you can just reuse it over playtest.
So any changes you make, you're just trying.
OK, we made these changes.
Let's try it again from the player lens and see if we're getting better.
So the cost decrease over time once it becomes something that you do often.
Thank you.
Thank you.
All right.
That was it.
Thanks, everyone.
