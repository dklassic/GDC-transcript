So my name is George Ng, and I'm the Chief Technology Officer at GGWP.
The topic is supplying methods from other domains to identify gamer behaviors.
So while I've been a gamer, I'm relatively new to any professional application to gaming.
I've been a data scientist in a wide variety of domains, including defense and intelligence, financial services and healthcare, and professional sports.
I've also taught game theory at UC Irvine and American University, and machine learning at UC Berkeley.
A bit more about GGWP.
GGWP aims to help reduce toxic and disruptive behaviors in games.
More specifically, we use machine learning to identify gamer behaviors.
I can share some examples of the type of work throughout our presentation.
Now it goes without saying that context matters, not just in identifying particular expressions or events that may be of interest, but to try to factor in context in a scalable way where possible.
A quick recap.
Most of you are familiar with the Fair Play Alliance Disruptive Conduct Framework.
We have four areas expression, delivery channel, impact and root cause.
Expression is basically what form does it take.
And here we look at detecting behaviors and I walked through an example to start.
The delivery channel, where does it happen in and around online games?
We won't cover that here.
Impact, who is affected by it and in what ways, what are the consequences?
We'll talk a little bit more about this.
And root cause, why does it happen and what does it express?
So we start with looking at expression.
And I'll give you a little bit of background in terms of how we think about building models.
So there's top-down, which starts with hypothesis.
And on the left, you see a data scientist Venn diagram.
So first, I'll describe each component in it and why it's relevant.
So in order to have a hypothesis requires domain expertise, and having the right mathematical background is helpful to translate a hypothesis into an appropriate test. So when dealing with large data sets and running things at scale, this requires engineering expertise. In gaming, obviously, because of machine capture, there's the potential for a lot of data. So let's look at some specific examples from ministry. So suppose I'm trying to identify hackers.
Domain matters because it's helpful to understand how hacking works and to understand the motivation of hackers. Is it a financially motivated attack like with the crime syndicate? Is it a nation-state trying to demonstrate strength? Is the goal entertainment like with script kiddies?
So cheating can be seen as a specific type of hacking that modifies the game.
The point here is that the initial intuition gives us guidance on how to ultimately scope a research question.
So what would relevant proxy data be for my question?
How do I translate my question into data, in other words?
Does that target data require additional collection?
Does it require pre-processing, aggregation, normalization in some form?
And ultimately, the statistical modeling helps inform how do we evaluate a hypothesis.
The other approach we look at here is bottom-up modeling, which basically is data mining or searching for patterns within data.
Oftentimes we don't know exactly what we're looking for.
Even when we have a specific hypothesis, we can easily misjudge the distribution.
behaviors. So for example, just because we know there's shooters out there, we may not be aware of the different proportions, right? Most games you'll know aimbots because people are very sensitive to it, but maybe some other types are a little bit like things that just reduce gun recoil are less easy to detect initially because players don't report them as much. In general, Bottom-up modeling is often used to identify the baseline in an environment.
Common applications are like identifying cheats and fraud.
In these cases, we first define normal to be able to use anomaly detection algorithms to then identify abnormally bad.
We use both top-down and bottom-up modeling in our work, and I'll use it here to illustrate a single event detection example.
So suppose we want to build a model that detects players wrongfully not reviving down teammates.
Well, one definition of wrong is that the player did not perform some action that the designers intended, or that others perform regularly, or that improves game outcomes in some way. So let's use the second definition. So we're looking at normalcy.
We first perform some basic data analysis, and we show the results in these charts.
So the left chart illustrates, so on the y-axis you see the distance to the enemy, and on the x-axis you see the distance to the downed teammate. We also capture how much time is left to look at a revive, and we break this up into six different charts. Blue dots represent revives that happened, and red dots represent cases where the teammate was not revived.
So for the top row, we have a short time left to revive. We only see successful revives when the teammate is very close to the downed teammate.
When there's more time left on the bottom row, we can see that the upper left quadrants on the chart tend to be mostly blue.
And what that means is that the player is relatively closer to the nearest teammate or relatively farther away from the opponent, which is what we'd expect.
Note also that we do see much more variation when the time to revive increases substantially, which also is what we'd expect.
The right chart summarizes information by showing the average probability revived based on the distance to down teammate.
The different colored lines illustrate our way of factoring enemy distance here.
This view really highlights the correlation between distance to a teammate and probability revived.
Even a very simplistic model clearly has some signal.
But I think we can do a little bit better here.
So we've confirmed our initial hypothesis that it's possible to predict reviving likelihood.
Here we further show that example by using concentric circles around the player to illustrate the probability of revive based on distance. And this overlay on a map I think really helps highlight this.
We acknowledge that this is far from a perfect model.
The distance doesn't factor in actual walking distance and sight lines and doesn't do well in buildings.
But again, even this baseline model does reasonably well.
We can also look at other.
potential characteristics that might be relevant. So if we look on the right, we see things like health of player and downed teammate, the number of previous revives, and time available, which is related to the number of previous times revived, and other teammate position, right? It makes sense that, you know, unless someone's covering an angle or taking fire, that the closest teammate would be more likely to make the revive.
And finally, since it's a battle royale, we factor in circle position as well.
In other words, if a team is far from the ring, it's more likely to not make sense to revive in that situation.
We build a richer model by using these attributes.
And we use an ensemble tree that captures these elements to determine the probability of a revive.
We then take the residuals from this model to identify likely cases when teammates should have revived but did not.
Or in other words, when the model predicts the high probability of revive and the player did not revive, then we'll actually detect these incidents.
So next, I want to get a little bit deeper into context.
So when we look at root causes, we also want to understand other relevant factors that could be tied to the event, such as the type of player or the personas, expectations based on game experience, now showing example of skill level, initiation, and triggering.
So within the scope of a match, or potentially across multiple matches, if there's history between players, like who started it, did someone get provoked?
And then finally, the social factors and social environment.
And what I mean here is just expectations when playing with friends is very different.
If you're playing with a club, clan, or group, a full premade versus a partial premade and solo queue are all very different environments.
So next, I'll illustrate some of these examples from games as well.
So first thing I want to look at is personas.
So the chart here shows different types of players as clustered by features such as damage and kills per second, average walk and loot speed. Depending on the game, we can also look at things like character selection, items and ability usage, health and ammo sharing, etc. Some of these just highlight skill differences. Others show differences about how a player likes to play the game.
So using the non-revive example, does our view change really if the player tends to play support roles?
It can, right?
The expectation would be that this player would be more likely to revive in most cases.
So we do want to come up with positive attributes as well.
We want to make sure that we're just not rewarding certain types of play styles, unless it's good game design to reward those play styles for some reason.
But ultimately, we do think that the persona could be a consideration, right?
Different personas get triggered differently, different personas react to situations differently.
So we want to factor in these elements where possible.
So what about expectations?
Players often go into games with some expectation in how their teammates will and should play.
Something that appears to be simply suboptimal strategy.
or not following the meta at low ranks could really be viewed as griefing in higher ranks.
So how do we consider that difference? So the chart shows friendly fire proportions by rank and ability. Higher ranks as you go down. Unsurprisingly, more of the friendly fire occurs in lower ranks.
There's some exceptions with Raze's Showstopper and, you know, Sova's Hunter's Fury, which if you play the game, it makes a bit of sense given the spread and impact of one.
And Hunter's Fury, of course, goes across the entire map, basically.
We factor in ranking experience where possible.
when we try to determine deliberateness.
How that translates is usually we look at low-rank situations very differently.
Things require a certain level of experience at the game before you can see something as being deliberate.
And this is supported when we look at the distribution of play behaviors and strategies and even character choice by rank.
So what about initiation and responses?
Well, we believe that this is very relevant.
and can help inform severity.
Obviously, being triggered is different than being unprovoked.
Here we provide some examples from the League of Legends Tribunal.
These are from reports, so we expect more toxicity than average.
Some interesting summary statistics to share.
When we use simple models, such as just block list detection and word embeddings derived from them, We see that 35% of all players made some potentially offensive statement, and over 90% of these matches have more than one player making toxic statements.
We also tagged some examples of what our models flagged.
Even using our more conservative models that does leverage some game-specific language, still finds that over 50% of matches have more than one player making toxic statements.
So not as high, but still very high.
Also to qualify things, I should note that not all statements are equally offensive, of course.
There's general toxicity, which we're generously tagging here, but you can further filter to identity hate.
You can look at targeted versus non-targeted. There's a lot of things that are criticizing gameplay.
We also look at conversation level stuff, both negative, like with bullying, and positive, with mentoring and encouraging players.
Not surprisingly, this gets harder and harder, right?
It can be problematic to sift through the sarcastic, you know, good jobs and unsolicited advice that isn't well received often in League.
Going back to the main theme here, we note that nearly 20% of these cases had a player other than the offender appear to start things.
We try to consider the context like this when evaluating severity of these different events.
So related to different triggers, there's environmental ones as well.
You know, think about the term rage quitting, for example.
Players are often triggered by losing, especially in these competitive PvP games.
The table shows Valorant Friendly Fire by score difference.
It's clear that when players become less concerned with avoiding Friendly Fire once the score difference is large, in particular when a team is losing by a lot, we see evidence of higher Friendly Fire rates.
In our case, while we see this as like regular behavior, we don't adjust the severity for the different cases. We decide not to lower it or increase it. Indirectly, we do factor this in in win probability models, which I'll describe later. We do try to capture still because it can still be relevant information to help inform game design. For example, should a game consider allowing surrender even earlier in lopsided contests?
So another contextual factor is social ones, right?
We rarely consider events where an offender and victim are part of the same full stack pre-made.
We also factor in requeuing and adding to a friends list where possible.
Essentially the standards for behavior change when you're playing with close contacts.
There's enough of a relationship where the parties chose to play with each other.
So in the requeuing case, even if it was with solos, the potential victim chose to requeue with the defending player, which provides some evidence that either the event wasn't so severe or the victim thought it was worth being carried, so they were willing to requeue.
We evaluate these events differently, in particular when we have a group playing with other groups or solos.
Sometimes these cases can be even more serious and take on a bullying-like quality, where groups gang up on solos. Going into this anecdotally, we had cases where people told us about the reported player actually being the victim. But even in our own analysis, once you're able to look at report data in conjunction with gameplay data, we've observed some of these cases where the reported player is actually the victim of group harassment.
This chart shows that for both ranked and unranked cases, PUBG players that are part of premade stay way closer together, maybe even too much based on optimal strategy.
In casual games in particular, solo players stray more than 100% further from their teammates on average.
Another event severity consideration should be the impact to the victim.
There are special cases, maybe you put cheating there, certainly you put things like predatory behavior there, that are beyond the scope of games.
But for competitive games, a primary consideration is just the focus on rank achievement.
And in these cases, the timing of the event and impact on win percentage matter a lot.
So we look at a few examples that highlight this.
So the game state matters substantially.
You know, griefing in general is bad, so friendly fire can be bad.
On the chart on the left, what you see is an example of a case where the enemy's already cleared.
You only see red dots left, which is one team.
They are the defending team.
You can see a green dot representing a spike in a game of Valorant.
This means that the attacking team has been already cleared.
Because the enemy has already been eliminated, friendly fire this time is very unlikely to cause the team to lose.
And in this case, and in most cases, it does not.
If you look on the chart on the bottom right, we can further see friendly fire by round.
Note that the highest friendly fire rate occurs in round 12. Round 12 is the last round prior to sides change, so you don't get to keep your weapons or abilities or economy. And note that this example occurred in round 12, so in this case the victim didn't even have to rebuy shields and weapons.
We should still make the argument that the victim's death impacted his or her KD ratio, so maybe it's not zero impact, but you can see that it's lower impact than other potentially similar cases. So because we feel that in competitive games the change in the victim's win percentage is important, we decided to build a win probability model for these different games.
And the expected win percentage then will fluctuate throughout these games.
If we go back to the previous example again, in that case there is a greater than 99% chance that that team would win.
This doesn't change at all, even with the friendly fire.
There's four teammates left, you kill one, it's very likely that one of you still manages to defuse.
This change is zero when the kill happens in transition time.
So again, it's just more evidence that the enemy cleared case is lower severity.
I built similar models to this for the NBA and major league teams to evaluate the value of different plays. For example, in the NBA, I wanted to evaluate the impact of a pass that didn't directly lead to a score, so a swing pass.
And here we can compare the expected points for that possession based on spacing, location of the team and opponents and skill sets on that possession after the pass and prior to that pass. And that difference will be the value rewarded or taken away from that player.
So games like PUBG are a bit more complicated than a sporting event because there are many teams.
So in all these battle royales the most obvious change or impact is just the number of players and teams remaining.
So the chart below shows a team that ultimately wins, and it shows their probability of winning across time throughout the game.
So even though the team is skilled, the initial probability is very low due to the sheer number of squads in the game.
Notice that each time the team engages and does positively, their probability increases quickly.
And if they engage and do poorly, their probability decreases.
So until the team is eliminated, their probability should slowly climb over the course of the game, basically because they're surviving and not being eliminated.
We build a model that also factors in the type of squad, so premades generally have a bump because they have better communication, although some games only match premades with premades, so it can be a moot point.
Pre-game skill and in-game performance.
The in-game performance idea usually is around momentum.
So the concept of readiness to engage in battle royale is there.
I think there's a more explicit one here.
When you defeat or kill off other teams, you're often the ones to collect their loot.
Also, we look at things like team health and equipment and distance from the circle.
So let's go back to our non-revive example for a second.
What would happen if the teammate decided not to revive his teammate?
What was the impact to that win percentage?
So we can see that depending on the game state, that not revive could have made a big difference.
So maybe we would want to use that as a severity consideration as well.
So once we create the models, the more important question is, how do we actually use them?
And how do we interpret these outputs?
While the models will have the benefit of greater consistency and lower costs as compared to any form of manual review or moderation, all ML models will suffer from some inaccuracy with both false positives and false negatives.
It's important to consider the ramifications of applying each model.
Thoughts positives are going to be more impactful in different situations.
For instance, for many consumer recommendation systems, the damage of recommending the wrong movie is fairly trivial.
You can just stop the movie, you can watch something else, right?
What about some financial service applications that I've worked on?
Well, what about credit card holds?
If you charge your credit card outside of your location, they'll often get blocked, right?
So you have to inform the credit card company that they're traveling.
But the adverse impact is pretty small.
So a false positive just means that, you know, someone has to send a text to validate or make a call to validate that they're actually using the card and then it gets reactivated.
What about loans?
You know, the amount matters a lot.
What if it's something large like a mortgage?
What if someone wrongly gets denied a mortgage and then is unable to purchase a house?
You can see the ramifications of that on someone's quality of life.
I've had the unique experience of working in a wide range of environments where models were applied with different stakes.
The highest stakes were around threat identification, where a false positive could have led to a wrongful arrest or even possibly wrongfully taking someone's life.
So in these situations, obviously, false positives have to be taken very seriously.
And at best, models are just the components in form, a manual analysis decision.
So in general, what does this mean? What should we do?
Well, in theory, we always want to build the best algorithms or best models.
In practice, there are things like development costs and ongoing maintenance costs.
For gaming, there may be further adverse game performance impact to store more events, right?
In general, we're more willing to take on these costs when the stakes are high.
So, if there are...
If there's greater value in accuracy, we want to be conservative and minimize these false positives.
In these cases, we need to accept that we will have more false negatives because of this trade-off.
So how does this apply to gaming specifically?
I think we can be greedy when applying warnings or other interventions that have little adverse impact to players.
They can even appear to be general warnings, non-targeted warnings.
An example, during load times, a reminder to show mission statements and player grades have been found to be effective.
We have to be more careful when applying penalties rather than warnings because of potential false positives.
There are many ways to implement this.
For example, we can apply access restrictions for different durations and different channels based on different severity levels listed here in these charts as illustrative examples.
In general, higher severity again means that we should be more conservative when classifying events and players as bad.
The most serious punishment in a game is generally to ban a player from all channels.
We should note that a seasoned player who has spent a decent amount of money and time in a game, will be much more impacted by a ban than a relatively new account.
There isn't a single solution for everyone.
We should consider the tradeoffs by thinking carefully about each application and the type of players that would benefit and be adversely impacted by any proposed change.
One partial solution that we considered is looking at reputation scores.
So what if we're concerned with uncertainty of any single modeled event or report?
Could we create a more complete view and look at players across time and across multiple games?
So one solution is to create this holistic reputation score that would factor in inputs from multiple sources and events.
This reputation score would be more complete and consistent since it considers different sources so it wouldn't suffer any biases from a single event and would have more of the advantages of large samples.
you could factor in chat, audio, gameplay, and different types of group events together.
While there still could be individual issues, such as faulty reports, again, the larger sample size helps the system be right more on average.
And while we could create numerous subscores and components from this, at the top level, I think it's important to have a single number, which makes it easier to interpret and use.
This is ideal for factoring different types of events weighed by many different severity levels.
And that simplicity matters because it helps people use this in some way.
Right. There's a reason why things like FICO score, while a lot of modelers believe they can build a better one, are so widely adopted because it's simple and interpretable.
This isn't a solo approach. It's a complementary piece and should be used with other tools, such as stronger penalties that should be enforced on more extreme behaviors.
But this gives a pretty good overall view of a player that can be used in things like matchmaking and aggregate.
can be used to provide a view of the overall game health.
So speaking of overall game health, here we look at a sample of more than 40 games that have been released for more than two years, so we can evaluate retention in a more apples-to-apples type way.
Here we're trying to evaluate the impact of toxicity in general.
There's a lot of studies that try to look at this, but it's impossible to find a definitive one.
So we're continuously trying to refine ours.
We use Reddit because, you know, while we have some game data, we certainly don't have it for like every game.
And for the study, we use some custom detection algorithms on subreddit posts and comments.
And we use this as a proxy for toxicity.
And then we group games into three different buckets of toxicity to examine the player retention impact for each group over different one-year periods. So when we compare to high toxicity, which not surprisingly has the worst retention, we find that medium toxicity games have about 10% higher retention and low toxicity games have more than 16% higher retention than comparable high toxicity games.
This is significant when we consider the number of players for some of these games and the revenue impact of even a few percentage points can have, given games of this scale.
This impact can further be amplified when we think about it across time, as retention rates can be compounded, as some of these issues with retention aren't addressed by game teams.
And finally, just to recap.
Given the proliferation and richness of data available in games, and the vast number of reports that the popular games need to deal with, it makes sense to automate event detection.
Teams should also account for contextual factors where possible, and we showed some illustrative examples of things that we look for.
And obviously, we're just getting started as well.
We recognize that there's a lot more that we can do here.
And finally, companies should...
consider the potential penalties and tie them to the models that they are using. So they should consider the penalties that they plan to impose and optimize models to best support these use cases. If you stuck with me until then, again, thank you for your time.
