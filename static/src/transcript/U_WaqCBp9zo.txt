Hello.
All right, I think we'll start.
We got a lot to go over, so we'll be pretty punctual.
I'm Ken Brown, I'm the technical art director on Battlefront.
I'm in charge of things like workflow, pipeline, performance, optimization, that kind of thing.
I'm Andrew Hamilton, the lead environment artist from Battlefront.
I was involved in pushing photogrammetry further into style with Battlefront.
So.
Photogrammetry.
Buckle up, this is actually a really packed presentation.
Tons of content, a lot of information.
We'll be going over kind of everything.
So, like I say, be prepared.
For those of you that don't know, photogrammetry is just the use of photographs to make measurements, and in our case, generating models and textures from those measurements.
The basic process works like this.
You take a series of pictures.
You align those pictures in a photo scan software, in this case, Agisoft.
You generate a point cloud.
From that point cloud, you generate a high density mesh, high res mesh.
And there you can project textures.
Oh, we're really early.
So this presentation is a bit of a retrospective of our process of the.
So we'll go over the beginning, the research and development that we did.
We had a really great window of time to do some R&D in the beginning of the project.
What the plans are when we go on the scanning trips.
How we go about capturing the planets.
The mentality and workflow.
So workflow for photogrammetry, but some of the mentality around why we use it and how we work.
Game assets in a photogrammetry world, so that is assets that aren't photo scanned when used in conjunction with photo scanned assets, terrain, and the resulting level construction kits that we create, and then a quick wrap up of our key learnings and takeaways.
So, before we get into the actual photogrammetry, some of the big changes that we had going into this project were we were a new console generation, Battlefield 4 shipped on Gen 4, but this Battlefront was our first exclusive current Gen title, a PS4 and Xbox One, what we call Gen 4.
It was a brand new franchise for us.
There were really high expectations to deliver after BF4 for a lot of reasons, which I won't get into here.
but you may be familiar with.
We had a clean slate, this was a brand new project, so we really wanted to set the foundation for doing this clean and correct from the beginning, so we had a kind of platform to build on going hopefully well into the future.
For us, PBR was not fully implemented yet when we started.
We were one of the first games on the Frostbite Enchant to be physically based.
So some of the challenges with those changes are, this is Star Wars, this is a revered franchise, everyone knows it.
Everyone is familiar with the location, so you have a lot of expectations going into it, a daunting number of expectations.
We had a lot we were learning from the past that we wanted to implement.
Photogrammetry is pushing the boundaries of quality and speed, so how do you work with this new technology?
We had a really tight development schedule.
We wanted to be 60fps, or we are 60fps, and when we said 60, we wanted it to be 60, we didn't want it to be 45 to 50, so that's challenging for those of you that know.
And then keeping this all together throughout production, like keeping the, Craftsmanship was really big for our process, so you want to make sure that everyone was always aligned throughout the development process.
So, because this is GDC, I thought we'd super quickly go over what our development structure was.
We had three month dev cycles, with milestones at the end of those three months.
We would do content development up until the very end, the very last day of that.
That made us realize that we really needed a two week buffer after that, after it created quite a bit of traffic jam, so we put a two week buffer between these three month development cycles.
During that, During that two week buffer, we would have an approval lock where you would need a producer or a DD or a director to approve your check-in, and during that time, we would be stabilizing the build.
So that was our basic process.
We were really happy, actually, I would say, with how this went.
We'll be continuing with this basic model as we go forward.
This was a successful structure for us.
Yeah, so photogrammetry is not our first outing in photogrammetry.
We did this in Battlefront III on most of the characters, and in Battlefront IV, pushing it a bit more, you know, a few rocks here and there, but.
going into Battlefront, we wanted to make sure we were utilizing this to the best we possibly could.
So we had a lot of great access to things like the Star Wars archives with all the real props from the old movies, especially considering we were aiming for authenticity.
Some of the scans turned out really great.
Other ones were from old materials and things that didn't quite work out, so we were combining with other things as well that wasn't photogrammetry, such as marvelous designer to create cloth.
supplementing that with things with hard surface modeling, just a typical, the usual way we do it.
And combining all those pieces into kind of a final result with many, many techniques.
Here's some quick in-game, quick final renders of the in-game assets themselves.
And even the beard itself is scanned from.
Yeah, that was my beard.
I'm very proud of that.
So yeah, with the learnings from Battlefront, Battlefield, we were.
seeing where we take this.
Do we scan everything in the environment?
We wanted to naturally try and bridge that gap between the photo-scanned characters with the surroundings, so they would kind of be on par with each other.
And it was the goal, obviously, to make the best-looking Star Wars game that's ever been seen at 60 FPS with very little time.
So this is from Battlefield 3 on the kill, the level we did, and the goal here was to make something realistic, photorealistic, but this was without any techniques like photogrammetry, using traditional methods.
So going from here into Battlefront, we introduced a lot of new, interesting pipeline and workflow options.
So we'll go into how we got from there to here through this presentation.
Here's just a quick video of us on location.
Let's take one more coffee and then we go skiing.
So it wasn't all fun and games on these trips. It was a bit tough.
Not always glamorous.
So just a quick overview of some of our goals for one of these trips.
Obviously to capture the natural asset library for all the planets we plan to build.
We focused on capturing only what was essential on the trip.
There's certainly a lot more in the locations to capture, but we thought about what we essentially needed to reconstruct these.
Yeah, with meaningful variation and features, things that would give the highest bang for the buck, basically.
Visiting the real locations was great for true inspiration and understanding of actually how to build the environment.
So I'd say going on those trips was even more important to be there rather than just bringing back the assets themselves.
On the trips we kept it very simple.
We had, I mean, there's plenty of great rigs we could have taken, but for our case we wanted to be on the move as much as possible, so we kept our kit as simple as possible as well to make that possible.
So just a regular Canon 6D 24mm lens, tripod, color chart.
That's really about it.
Ideal weather conditions, obviously not under our control, but we tried to plan for these as best as possible.
Going to Iceland is not so good where it's always cloudy, things like that.
And breaking down the location.
We didn't want to just arrive on location and just capture what we could.
We needed to be very thorough and plan through exactly what we needed before we got there.
And thinking in terms of usable pieces, we could have captured really great organic shapes, but we need to make sure the things we were capturing were tileable, modular, and that kind of stuff.
As well as being aware of frostbite strengths and weaknesses.
It's very good at terrain systems and things like that, so we pushed more on that side.
Yeah, and you know, there's not a, sometimes with photo scan it's a bit difficult to get great captures, especially when you're out in the wild, so we wanted to make sure we captured a few things but of good quality to make sure we spend time with them, rather than just capturing the entire environment at lower quality.
Here's a quick shot of being on location when we first arrived to the Redwood Forest.
Just to show you how we break down the environment.
There's a lot here that you can imagine that you capture, but when you break it down to smaller pieces, there's not really that many elements that was needed to create this and get a sense of the environment.
So things like the dirt on the ground as a texture.
the mesh scattering on top of that, the clover as a ground, the clover mesh scattering, ferns of different sizes, logs of different sizes, trees of different sizes, some unique trees like stumps, and vegetation that is captured on blue board.
Yeah, so some best practices as well.
These are very well known best practices, but it's very easy to let these things slip by when you're out in the field.
We always have to remind ourselves about this.
Overlap, naturally, again, very easy to forget.
And the number one thing that just destroys are good to capture.
As well as covering all the angles.
Just filling in blobby holes and things through bad scans, through photo scan is not ideal.
Avoiding cast shadow, something that is sometimes out of our control, but trying to capture in shady locations, in overcast conditions, things like that.
And most reliable results from our trips were from capturing assets when they're in full frame.
We'd start in the first few trips by capturing things a little bit too close, and then you run into the problems of overlap and covering all the angles.
So it was best to kind of step back a bit.
We didn't need the extremely high detail quality in the end anyway, so it was useless information.
And photo scan is not great at the moment with things like grass and noisy textures, so we just avoided those kind of surfaces and rather broke them down into components.
Yeah, and just being sure that the capture methods are consistent and clean, making sure everybody runs through the same camera settings, the same way they capture things.
So every time an asset is brought back to the studio, it can be processed in the same way.
And preparing yourself and equipment for those conditions, something that sounds quite simple as well, but when you're going through snowstorms and off to Death Valley, I mean, the conditions are variable in the extremes, so we need to be sure we're quite serious about that.
Here's a little slideshow of some of the locations we have been to, to show the variation across all the biomes.
Very varying weather conditions, quite extreme.
I have to say that sometimes I actually lose track of which ones are the capture shots than the in-game shots now.
The guys did a really great job.
Brought back a great range of textures in our library.
3D scanned textures for the ground surfaces across the terrain and vegetation on blue boards.
We'll get to that a little bit later.
And as you can see here, the objects are quite boring.
They're very generic, and that's intentional.
We didn't want them to stand out.
As we're making a very limited set of assets, for the limitations, we need to make sure they were easily rotatable and placeable without feeling too repetitive in the environment.
Even Duggar owned trenches in Hoth.
So.
In general, they were pretty successful trips.
They were progressively successful the more we did.
We learned a lot along the way.
But having a small and manageable team was definitely a key to the success, so we could make sure we could keep focus on what we're doing.
Yep, keeping focus on what we're doing and structured with a list of what we need to get.
And mentioned before, I do think that being on the trips was, and being in the locations, understanding how they are constructed is more important than actually the assets themselves.
Coming back from the trips, we had a huge amount of data.
And that was, you know, we manually sorted this, and it was really horrible.
So it's something we need to look forward to improving in the future.
Hundreds of thousands of photos, assets and video to categorize.
No real storage solution on our first round considering the compressed time we had to work with this content.
Much room for improvement.
So, workflow.
We've got the data now so let's get to it.
Before, really quick, I get into the actual workflow itself.
This was a question that came up during the early days of our photo scanning work, and that's, okay, we're seeing these high quality assets, but there were a lot of people challenging us on the management level saying, okay, but is this actually faster?
Like, what's the reality of this?
I hear people saying that this takes twice as long, or this is four times as fast, or what is it?
So we did have a bit of a straw poll.
This wasn't a scientific poll, but we asked people around the team.
their experience building assets on BF4 and their experience building assets with this new pipeline, with the photo scan pipeline.
And a lot of things stay the same.
Retopology stays about the same.
Texturing stays about the same.
But you have little savings here and there.
And you save a ton on the high poly time.
So, I mean, the quick answer was that, yes, that it was faster for us.
And we felt that...
as we kind of go along and become more experienced at this and we develop better techniques for this.
And this is something we're still kind of continually improving on.
And one of the exciting things about this is that there are a lot of room for improvements is that we actually expect this to be even faster kind of going forward.
So the actual workflow.
We have the source photographs.
You take those into Photoshop with a Photoshop camera raw plugin.
Simply remove the vignette.
We kept distortion because Agisoft, the software that we're using, can handle the distortion of the lens.
We would do a batch calibration with the color charts, and this is actually an example of one-on-one that's improved.
At the EA Capture Lab, they've actually a much better batch and color calibration method that creates much more accurate color results and is much faster.
But this is a good example of a step that could be or should be entirely automated.
So once you have those calibrated photos, you take them, we often first, for most assets, we would mask out the asset.
That is like, you know, draw an actual mask around to help isolate it.
It would just ensure slightly more consistent results.
We would bake at high for normal assets, for props and objects, and we would bake at ultra for our terrain.
It's worth noting that your file sizes can be pretty daunting at that point, but those are the settings that give us the best results.
We would use Mosaic rather than Average Blur, blend rather, within Agisoft.
We found that it gave us sharper results.
We did quite a few comparisons in the beginning and we were happiest with Mosaic.
Agisoft, or photo scanning in general probably, will give you quite shredded UVs.
So we would take them into additional software to do our reprojections, so Zebraish or similar software.
We would take the high res mesh and in some cases we would just automatically generate a low res mesh in software like MeshLab.
Or if it needed a little bit more of a artist touch we would take it into TopoGun or any retopology software of your choice and retopologize.
We would make use of detailed textures, so that meant that we really needed to have as much, or as little stretching on the UVs as possible, so that was key to our workflow, and we had to be kind of conscious of what the UV direction was because we were using detailed tile normal maps, so we needed consistent direction.
Within XNormal, we would generate our base map, so color, tangent normal, object normal, height map, AO cavity, just your standard maps.
And one of the maps we would generate is the optics-based normal map, which we would use to remove ambient lighting, and in some cases, cast shadows.
There are other techniques for doing this, but we were quite pleased with how this worked.
To go into it a little bit, the light removal process, the 16-bit images are your friend.
There's just that much more color information for you to use.
And with the Photoshop Camera Raw plugin, there's actually a shadow remove.
slider, which is fairly effective in a lot of cases.
It's not perfect all the time, but it will get you a bit of the way there.
Once you remove the shadow, then you can move on to the ambient light.
And as I said, object space, number maps for the wind.
I'll show how this actually works.
So in this example, you can see.
It's not particularly distinct shadows, you don't have harsh shadows, but you do have like bounce lights and ambient occlusion, things like that.
And you want to remove that lighting information, that's the final goal.
So you have just a generic albedo base color.
So we generate an object normal map, giving us directions for each of the objects normals.
And from Photoshop you can then create a black and white mask based on those colors which will give you distinct directions.
Something like this.
And then you'll be able to basically just do standard Photoshop tweaking, take something like that to this.
And this is a pretty quick example.
So now we have the source data, now we have our base maps.
A quick word on that, now that we have them, on texel density, most of you are probably familiar with this I'm sure, but for those of you that don't know, texel is the graphical representation of a pixel on the screen.
of a texture pixel on the screen pixel.
This was an initial pitfall for us because the data we were getting back was so high resolution, we were just like, oh yeah, let's use it.
Like, we're getting 8K textures, 16K textures, let's use them because higher is better.
Like, that's an easy way to think of, or like a common way to think about it, you know, bigger is better.
So the screenshots coming up are 1080p, just as a point of reference, and this illustrates that texture pixel ratio to screen pixel.
pixel ratio.
So 1080p, at the bottom there is 512, that's 512 pixels counted on that screen.
And you see on this rock, and this is early, very early work, this isn't end assets or anything like that.
But the white square on that rock represents a 512 texture, and the red square represents I believe a 32.
by 32 texture and in this shot you have 25 texture pixels per screen pixel. So just way more pixels than you can ever generate. Then you can even use excluding the mip maps. So here still too many pixels in the texture, still too many, still too many. You have to get to this distance before you have more textures in the pixel than you have on the screen to render those pixels. So just a storm trooper helmet here for scale reference and you can see it's closer than you're ever reasonably going to get in.
in our game, in a first person game, third person game.
So, I mean, another aspect of this is that the data we're getting back is so good, like it's so, you just don't need that much texture information, so, and we're dealing with texture budgets like everyone else, so here's a rock with two 4K textures showing, taking up 55 megs of memory with our compression.
And here's a 1K texture taking up 3.7 megabytes.
And I'll flip between those, because you really can't see the difference that much when we're using our detail maps.
So the takeaways from this were, you'd be surprised what you can get away with.
Like, when you have such great source data, you can actually push it a little bit further than you traditionally could.
Detail maps are our friends.
Large textures have a huge streaming overhead.
This isn't a surprise, but I mean, it takes a couple seconds to stream an 8K texture.
It's completely impractical for us.
I'm sure there's ways around this, but for us it was a non-starter.
We had an 800 megabyte budget for our textures, and an 8K texture is almost 90 megs with our compression, so yeah.
So many reasons not to do it.
So another feature we have within our engine is texture arrays, just a texture array is an asset that has several textures kind of stacked within it, within an index.
And...
Because our base, we would opt for a lower resolution base texture, we could use these detail maps on top.
And then we would have a preset shader that all environment assets would use for ease of kind of modification.
And then within those preset shaders, we would have a planet-specific texture array with several detailed textures that we would use for each planet.
This was basically just a performance and efficiency saving thing, but that also gave us kind of consistent quality.
So this is how our texture array is set up.
You can kind of see the texture there in the middle, the one with the actual texture, for normal reflectance and color.
And then I've got the little multiply node circled there.
In this case, we'd multiply by a factor of four to give us an index from zero to four, so five indices to choose from.
And this is relevant because we would just pipe in a gray scale texture with gray scale values that were multiplied by that factor and you would get the five even steps.
So for example, you've got 25% gray for bark and if you multiply that by four you get a value of one.
So that's your second index if you're starting at zero.
And then, this also afforded us an opportunity for an optimization where we would fade the detail at a pretty quick distance, and then from LOD zero to LOD one, we could switch to a more efficient shader on LOD one.
So a nice performance gain for us.
And the texture array of five textures wasn't just, it wasn't limited to just the five textures because we pack different information into the channels.
So in this example here you see the red and green channel was used for our normal map.
We would derive the blue channel.
The blue channel was used for reflectance and A for like a gray scale overlay on the color.
So we're effectively five textures times three different uses of those, 15 different options within one asset.
And this is an example of how that works.
So in Photoshop, we would blur out the surface detail.
We would remove the high frequency detail, keeping the low frequency detail.
Important details, and this is the zoomed in shot here of that low resolution texture.
Quite low resolution, quite a savings in texture memory, because again, we were at like 512 texture sizes.
And then here is with the detail map over top.
And we would actually source this detail, normal texture from a photo scan source as well.
So we're scanning bark to get our normal map that we would be tiling on top for these details.
So great increase in texture resolution with very minimal impact on our texture budget.
So now we have these photo scanned assets, which as I mentioned earlier, are quite high quality.
So how does everything else fit into this?
With the old asset creation, where you get to a certain level of quality with a certain amount of effort and tech.
Suddenly with photogrammetry, your increase in quality is significant because of the tech and the techniques, not necessarily because of the effort.
So when you're unable to use that technique for an asset that won't...
that doesn't lend itself to photo scanning, how do you bridge that gap?
You can't just have this massive increase in effort.
It's not particularly practical to do that.
And there's not necessarily a bunch of, or a technique that will magically fill this deficit.
So now we're gonna talk a little bit about, I'm gonna talk a little bit about some of the ways that we worked our photo scanned assets together with our game assets.
And one of those was PBR technology, as I mentioned in the offsets.
Some quick thoughts on this.
It is one of those things that helps us bridge the gap.
It creates a consistent look because of the way physically-based rendering works.
I won't go into detail about PBR because that's an entirely separate subject.
We had the benefit of learning from our predecessors.
We were a little bit late in terms of our implementation to the PBR game, so we looked at what other studios had done when they rolled out this technology, and we were able to learn from that.
And one of those for rolling out a new technology was that if you try to get everyone to switch at the same time or try to force this disruptive feature onto someone, onto an entire team at once, that you'll have people who resist and then it can backfire in terms of your adoption rate.
But there are people who will get it instantly and they'll take it and they'll run and they'll be doing really great work with it and those will be able to lead your team.
So give it to the people that.
adapt it early and let them lead the rest of the team.
Because with PBR, there really isn't a middle road.
You can't half do PBR.
It doesn't really work that way.
So, the benefits that we got from PBR were that assets fit together.
That's the whole point of a physically based render.
But they do still need to be balanced from an artistic direction against the scene.
And finding true PBR values, because it's physically based, it's not 100% physically accurate, can be difficult at times, especially with things like blacks.
So we found that it was really important to constantly reinforce the knowledge and the way of working within the PBR environment so that the results were consistent during the course of the project.
This was big for us, kind of constantly coming back to sort of training.
Here's a quick.
slideshow of the different render layers, because it's always kind of cool to see that.
And our final result.
Yeah, so vegetation, obviously, like I mentioned before, we couldn't scan because it's too noisy.
So this is something we had to.
look at other techniques of getting in, but it still had to fit within the whole photogrammetry world that we're creating, so that was certainly a challenge, as well as reaching 60 FPS.
With building a photogrammetry world, I mean, it becomes quite realistic, so the vegetation and the density of vegetation also need to be realistic to kind of keep up to that standard.
So we'd integrate hand-built vegetation, basically, that we, as close as possible, was comparable to the quality of the photoscan assets.
As I mentioned, we opted for that because of the scans being too blobby and things like that.
These were captured on a blue board.
So I think you saw an image before where we lay out these as straight as we can on the blue boards, ranging from ferns from green all the way to the dead so we can reconstruct a fern based on photo reference.
All the vegetation in Endor would share the same textures.
So it was just a few atlases and separating the textures from close to far so we can lay them out in billboards.
And there was a huge amount of texture packing, just like everything else in Battlefront.
Quite a lot, especially vegetation, because it was naturally something that was tricky to get into performance right up until the last days, basically.
And normals, as we went to photo scanning, the normals had to be created by hand.
So those were created in Maya by modeling the shapes roughly around these kind of atlases we would build.
And those give pretty good results combined with other tools such as Endo and things like that.
Nothing too special about the way the vegetation was built in particular.
It was a photo scan based at the bottom from the trips we went on.
And then it would just blend into a tiling texture at the top.
We would also work pretty hard on blending assets into the terrain itself.
We wanted to kind of get rid of that visible cut to not remove the illusion of a photo scan environment.
And this kind of went through all the passes, not just the assets, the textures themselves, but we needed to go through radiosity and light and all that kind of stuff.
So it was a challenge to get it blending seamlessly.
Another one of the features we had implemented for us was mesh displacement.
This actually came from, I believe, the Dragon Age team, and we took it from them and used it.
This was a huge quality boost for us, and for what it's worth, we found that a distance of about 7 meters was all that we really needed to have the mesh displacement active, because this was kind of an ongoing balance between performance and quality.
So, here's an example of mesh displacement off, and with our mesh displacement on.
Mesh displacement also led to terrain displacement and height field displacement, which is also something that was implemented by the Dragon Age team.
So we were able to define the displacement shape via a mask on the terrain.
This was actually one of our must-have features.
It sounds maybe somewhat trivial, but we have these giant AT-ATs stomping around our levels, and we knew that we wanted them to be affecting the terrain that they were on, especially when we're on a level like Hoth, which is snow.
We have a terrain decal component within our terrain system that allows us to combine the height field displacement with the detail displacement shader.
So a low level displacement to create the actual physics displacement along with the nice mesh displacement.
And I'll go into this a little bit more in a bit.
But it's worth mentioning that because photogrammetry does afford you this nice quality vs speed ratio, and the quality vs speed ratio is very high, you can get creative. You don't have to worry about kind of playing around.
So this was kind of a fun, I would say experiment, except we actually ended up using this.
So we went to the hardware store and kind of fashioned ourselves a close approximation to an AT-AT foot.
We got a tub of flour and stamped it into the flour, scanned it in, got a result like this, and to be honest, in less than a day, we had this prototype working.
This is the absolute very first test, so you see popping and things like that, so this isn't what we ship with, but just really great to show how you can prototype high-quality results and techniques without that much investment.
Oh, so terrain, yes, speaking of.
This is kind of the final big chunk of our photogrammetry puzzle.
Yeah, so terrain in Frostbite has a pretty extensive terrain system.
And for Battlefront, it was typically eight by eight kilometer terrains.
And you'd play in about a one kilometer space, roughly.
And the detail from that one kilometer would be lower and lower the further you get away from the center point.
I would complement this with really large backdrop meshes, just regular backdrop meshes to extend the vista much further into the distance.
And all the textures for this were captured just like you saw in the presentation before with the photogrammetry.
And performance on the terrain, we need to keep the amount of layers as limited as possible.
So we had to pick and choose the most important sort of surface that would make up the terrain itself.
We used to work with World Machine a lot more, on BF3 and 4 days, and that was great back then.
It kind of fit within the style that we're going for.
But going into Battlefront with much more photo scan characters and things like that, we need to make sure the photo scan assets up close would blend to a realistic background, and World Machine just wasn't really cutting it.
This is where we got with the topography data, using real-world data, and supplementing that on top with all the photo-scanned surfaces and things.
We get the usual kind of maps.
So we get a height map and a color map.
And from that, we'd implement it into the Frostbite terrain system.
And here's just a quick shot of the extended backdrop.
So this is mesh.
It's probably around about 70,000 triangles or something.
And when working on Battlefield previously, we were generally creating more of a level in a bucket where the sizes were compressed.
As we're going for all these realistic visuals in this project, we wanted to make sure that even the distances were realistic themselves.
So the distances here are literally 40 kilometers into the distance, which would work great with all the proper fog values and all those elements.
Yeah, so a bit on the texture capture.
Again, we didn't really use any special rigs for this.
We just kind of tried to hone in on a standard that we used.
So it was roughly a three by three or four by four patch of terrain in the real world.
About five by five photos.
It was generally enough overlap in most cases.
And always capturing just one defined element such as just gravel or just sand as in the frostbite terrain we'd do all the blending between those in the system itself.
In our case, low ISO and high f-stop worked best.
That does introduce quite a bit of grain, but that was much less of a problem than blurry edges when you bring it into Photoscan, so it worked best for us.
And again, the point of not focusing on noisy surfaces, it was just not worth the effort.
Here's a few examples of capturing snow, which was interesting because we weren't even sure this trip was gonna work, but it was, resulted in some of the best scans we had.
Yeah, so in Battlefield, we didn't have terrain tessellation, so this was new for us on this project.
It was a huge quality boost, which we absolutely could not go back.
I would rather remove something else rather than this one.
It would stop at only about 15 meters, which sounds pretty short, but in the game when you're running around with all the other detail, it was far enough, and the performance of this was pretty trivial, actually.
So here's terrain without displacement, which we had previously, and in Battlefront, when we enabled that to only about 15 meters, it did the trick.
So basically everything we've been talking about leads to these things called level construction kits, as we call it, and that is essentially a package of all the props and surfaces and things we've captured on location and built into a small package of usable pieces within that biome to keep it all into one nice little location.
It was great for us, especially going into production, these were pretty much finished, so we could focus more on building fun levels and iterating rather than trying to build up the quality too much.
Yeah, definitely shift the focus.
And as these were very self-contained packages, it naturally meant that the performance was quite easily understandable.
There was no kind of outside influences being pulled into this biome, so what was in there was very, very quick to understand and keep track of throughout production.
What it would include is the eight by eight kilometer terrain along with the big backdrop meshes, of course, the ready-to-paint terrain materials.
And the library of assets, which was easily accessible and categorized in our database.
So again, not pulling in outside influences, which would affect performance and things.
And at the beginning, some lighting presets based on the films.
So we really nailed the look that we're going for early on while we're prototyping levels.
And we have one of these for each of the planets that we shift with.
We also built more of these kits in production when we needed more variety, such as the.
the Jawa Canyon red cliffs.
Here's a short video on just using one of these kits.
So here I'm just illustrating how we can easily place out the objects that are connected to this biome, painting out materials that are already ready to go with the ferns and that procedure to distribute.
Just shows kind of how fun it is to play around with these with the visuals already up to quality.
The idea going into the level construction kits was that eventually anyone could take them and create a space or a level that would be functional for the game.
And I would say that that's still the goal, but we ran into basically production time limitations.
But these are very much intended to be used by effectively anyone. That was very much the goal.
And then the idea being that you could get very high quality results with very minimal effort because the kits were doing so well.
And here's another example of Tatooine just showing, as you saw, it went from the LIDAR backdrop down to the ground, and it's already very highly detailed with the procedural shaders distributed across the terrain based on angles and things like that.
But here's just filling it out with more details and also showing the blending.
As you scale the assets up and down, the detail stays the same size, so it's all relative to the landscape details.
And this is actually slowed down time.
Our artists are really fast.
So this shot, which you saw before, this is just to illustrate a point.
Here is what it looks like.
A lot of variation.
It's hard to see kind of repeating assets and where things end.
But in reality, it's only built up of a few pieces.
And this was the point of these level construction kits, that we keep it very small and self-contained.
So each asset could be very well maintained, brought up to quality, and tracked very easily.
They're very generic, so placing these around was hard to see the repetition in most cases.
And it's the same goes for all the planets.
The same kind of very simple kits to build up the landscapes.
And Tataouine probably being the best example to show where the, well basically where not the connection lays between objects and the terrain.
Working very hard on the blending between them.
With very few pieces again.
And cellist.
Same goes here.
Here's just a very quick slideshow, essentially just to show the variation we have throughout the game with a lot of these kits that we created.
And I think the use of them was spread quite far.
And it goes to show that even very generic assets could make up quite interesting locations.
So, some of our key learnings and takeaways.
Having this process in place before production began was a huge win.
Like I actually can't really understate that.
We spent a lot of time and effort in the beginning days and months of pre-production, kind of getting this stuff in place.
And we actually had the level of construction kits almost entirely in place before production began, effectively entirely in place.
Certainly not at shippable quality, but at usable quality.
And this was just a great benefit for us because...
We love kind of white box design and doing prototypes and things in that method, but all too often you have that situation where your less creative people will come up and say like, oh but how will it be when it's done?
When will it, how much more fun will it be when it's done?
What do you mean when it's done?
And this actually completely bypassed that problem, I would say, during our production.
We never had that case where people were like, oh well, this is pretty cool, but we'll see what it looks like when the things are complete.
Like, I never heard that phrase uttered during production.
I'm not saying it couldn't be uttered, but.
but it really helps that problem.
Yeah, and having well-planned and structured photogrammetry trips was essential in having clean results.
We really made sure, before we went on them, we had great lists and knew what we're going into to capture the environments.
I think that was very key.
So yeah, we've talked about sort of craftsmanship and really photogrammetry is just one of the tools in our belt to sort of help us achieve that.
what was our core pillar for production of craftsmanship, focusing on the quality of the assets they were building and trying to keep everything to a very high standard.
There were people, especially, again, like on the maybe management executive level, who saw, or you'd hear the photogrammetry buzzword, and like, ah, photogrammetry, that'll save everything, that'll solve all the problems.
But it's, like all technology, that's not the case.
This is, it's just a tool in our belt.
Yeah, so everything you saw here, I mean, they're built by a very small team of artists, and it was crucial to have these artists owning their planets and owning the art that we're doing.
They were responsible for going on the trips, processing the data, building the levels.
So they went through the entire pipeline.
I think that was key in getting consistent results and keeping on track with everything.
Working towards PBR was an ongoing process, as I mentioned.
Actually, there's a quick little anecdote I can give about PBR that I meant to give during the slide, where, speaking of the values and kind of making sure that everyone was maintaining the standards of PBR, we had on Endor a case where all the values were about 30% brighter than they should be.
But it's really hard to tell because the exposure would compensate down for that increased intensity.
And as a result, everything in that level was like unexpectedly dark, and we couldn't figure it out why for the longest time, and it was just because We ended up having to go in and doing a pics capture or something and reading the values and seeing that we were about 20 30 percent Brighter than we should have been so it just emphasized that you kind of constantly have to be checking Checking your values Yeah, we mentioned this a few times, but much of the process was, it is brute force and it is manual.
And it would get faster as artists got better at it, but I think there's so much room here for proceduralism, for automation.
It's just that there's a lot of potential wins.
And I'm sure you guys can kind of imagine even after watching this presentation.
Yeah, same thing here.
Just a lot of manual labor, brute force.
I think we have a...
I think it's a quick reel of final results, basically, that we should put.
So sit back.
All I need is a little bit more Someday I'll be sitting in a dry well You you Thank you very much.
We really wanted to have a special thanks slide on here.
Unfortunately, it was really hard to not put everyone on, but this group of people represents actually a lot of the work that you've seen, and also those who were, I would say, directly kind of impacting our photogrammetry efforts.
We would thank everyone if we could, but these guys in particular were kind of instrumental in helping us kind of achieve our photo scanning goals.
And thanks to you as well.
This, we actually ended this somehow 10 minutes early.
This is like, in all of our, well, yeah, about 10 minutes early, which is 10 minutes faster than every one of our run-throughs.
But that's great, plenty of time for questions.
I didn't think we were gonna have any time for questions.
So if you have them, please head to the mics and throw them at us.
Hello.
Great work, thank you for sharing this work with us today.
I was wondering if you can talk a little bit about substance designer and painter and procedural texturing in general.
Have you considered this?
Have you evaluated these tools?
And how do you think it may complement your approach?
Yeah, so Battlefronts, our primary painting software was actually Mari, which...
the artists loved.
And actually within EA, there are several teams, there are several people using Substance as well.
We haven't incorporated any sort of Substance painter stuff yet, but we really like the software.
The kind of generic answers that we're always kind of evaluating.
It's actually one of the things that the studio is trying to get us to align a little bit more consistently around, because we're kind of all over the place, but we love that stuff.
Like, all that software is really cool.
Thank you.
Hey, so I was curious about how you set up, like when you see the edge of the tree or the edge of the snow, how you would see, like how, was that all just displacement?
Because it looked like some of it might be cards.
Or like moss.
Yeah, so the trees in particular, they were maybe up to about 20 meters or less that were regular meshes with displacement.
The displacement would fade away quite quickly.
And then maybe in 60 meters it was.
He's talking about barskis moss.
Oh, the moss.
Yeah, it's really cool.
Oh yeah, that's just cards, regular cards.
We have a technical artist wrote basically a script that would parent in a kind of creative, efficient way a bunch of cards to surfaces.
And yeah, it created really awesome results.
Actually, it's totally kind of a hack, but it looks really good.
Yeah, it just looks like it's just on the edges.
Yeah, there's no special techniques there.
Hey guys, fantastic work.
I was hoping you could talk a little about how you source the real world terrain data.
And also maybe a little bit how the texture array detail texturing stuff worked with the blending between the terrain and your terrain meshes.
Yeah, so sourcing the data, we looked around quite extensively for that.
I mean, that stuff can get extremely expensive.
But if you search the internet, there's places like opentopography.org.
They have free data with just the catch of giving them a special thanks, basically.
They pull in data from all places that supply it for free, the old data that is now available, open source.
And that data changes all the time, so you're going to be lucky to catch the right one.
Other than that, they can get very expensive, outside of free.
And the blending between, it's about the objects to terrain was the question.
Yeah, well, I didn't quite understand how the texture arrays worked with your terrain system being one thing and then your terrain meshes and being able to blend the edges.
We would primarily use the texture arrays as a way to introduce the detail maps on the actual objects within the scene.
And then as far as objects blending with terrain, that's a shader feature that the engine has that allows you to sample the terrain beneath the object and create a blend between the object ends.
But then it was just the case of having the same tiling values on terrain and object to make sure they were well projected.
So it would blend across.
Thanks.
In your level design kits, you have a bunch of textures and a bunch of objects, but what we saw was maybe like eight objects.
I was wondering...
That's it.
That's it?
We showed everything there is there.
That's impressive.
Yeah.
That's good.
Hey guys, I'm wondering if you thought about using photogrammetry for any non-organic, like industrial or architectural elements, and if not, would you do that in the future?
We did try a bit of that, but it was pretty quick to notice that it only worked very well for organic surfaces.
We did scan things from the archives, such as vehicles and things, but that was more for reference, shapes and proportions and things, but I don't think we ever used any...
photogrammetry for hard surface in the end.
But that being said, having scanned hard surface objects, particularly for volume and reference, was invaluable.
Because it's not usable, but it's great for sanity checking.
Great presentation, guys.
Oh, that's loud, sorry.
Two questions for you is when you're going on your photo shoots and taking a simple prop, is there a time where it's overkill in terms of the number of photos you're taking?
Yeah, well not really, I mean, Yoho comes in the later part where too many photos is just not gonna work out in the end for running through photo scan, but we generally end up somewhere between 200 and 300 photos for a single prop.
That's with a few rotations around on different heights and angles.
So somewhere around there was kind of the ideal mark for us.
And if you're looking at software like RealityCapture, which is one of the newer ones out there, which we've been using and it's really impressive, they have this feature where you can take a series of images and then import images or any other source scanning data later on and kind of keep building on it.
So it becomes maybe more of an issue, but also maybe less of an issue too, because it's more flexible in that sense.
Second question is, when you have this data and you're taking it back and trying to manipulate it, is there, obviously, similar to how MoCap is, where you can overwork it, what have you found that point for this type of data and this type of workflow that you're kind of pushing it too far and it just breaks it?
We actually had that slide removed, but that was a point we wanted to bring up.
Working too much on photo scan data obviously ruins the whole point of capturing something in the real world.
It's capturing all the imperfections with the asset.
So the more you work over and clean things up, it removes that whole quality of a real world capture.
So we definitely try to limit that as much as possible.
It was more about just cleaning up, pacing in details on things that didn't quite.
a scan world that created blobby parts, cleaning up those things.
It really depends on what your final goals are too.
Like in our case, we're going for believability.
So we found that really do as little as possible with the manipulation, like keep it as simple as possible, do as little as you can to the texture.
If you're looking to change the style or the tone of your game, and just using photogrammetry as a source, you could have kind of different goals, but yeah.
Thanks guys.
Cool.
Hi.
Thanks for the talk.
Really interesting.
One of the final shots you had in your video there was of ice.
Now obviously, photographers are really good with solid textures, non-reflective things, things that are brightly lit.
Obviously, there's lots of depth with ice.
There's kind of layering, and there's like bubbles inside it and stuff.
How the hell did you do that?
Yes, so, I mean, we didn't actually scan clear ice.
Where we went, yeah, it's impossible.
But where we went to in Iceland, it was a glacier which had basically the whole range from clear ice all the way to ice that had been there for a long time and compacted with dirt and things.
So it was the same shapes, so we could capture the things that was more compact and solid and then work on the shaders in Frosted.
It was a little misleading having that one, but it looks so cool, so you got there.
Yeah, I thought that might be the answer.
Thanks, guys, cheers.
Hi.
Hi.
Just first, it was the best photogrammetry I've ever seen.
Really great work.
I do a lot of photogrammetry and the way you compose them is really great.
But something that's difficult with photogrammetry is to get the materials to.
And what you get with photogrammetry is good geometry, if you are lucky.
And the.
an albedo, you have to unlight your texture and sometimes you have an albedo.
How do you get the full materials?
Is it something you have to make artistically?
Yeah, yeah, I mean, we kind of develop that through the whole project, how we get to that material, but from the first game capture we get the normal map made from a high poly and diffuse, and then we sometimes derive things like cavity map and things.
That was about it really.
Things like reflectance had to be created manually.
We're not doing any sort of source, I forget the names of the companies that do it, but you can, you know, they do the box captures of the material, but we're not doing any of that kind of stuff yet.
It's mixed in.
Yeah, yeah.
But within EA, there are a lot of people experimenting with different ways of capturing that data.
it'll make its way eventually to the game teams and stuff like that.
Using polarization.
Yeah, exactly.
Like all the kind of standard techniques.
There's nothing really, we don't have any particularly secret result, it's just the same stuff that everyone else is doing.
But for this, it was just artistic.
That was the point we made about all the manual labor Just asking to myself because in previous companies I tried to to make more photogrammetry with the artistic team and The the main problems was on the quality it was too good and the artists feel bad with this technique.
They don't feel artists anymore.
And even if the result is very great, how do you get all your team adopt this and feel artists with this technique?
So basically, as I mentioned, like the artists who capture the asset, they go through the entire pipeline of capturing, building assets, building the levels, polishing the levels, the entire thing, so they're responsible for the entire big picture.
And that's, I think, the point that brings in the creativity of building a very complete big picture.
If they were just building a single asset and then handing it off and not, you know, forgetting about it, that's not creative at all.
But if they're working on photographic assets within a big picture, that's, at least for me, very creative.
And it speaks a little bit to that slide that I had about having people who do enjoy working that way, who do adopt the technique, and can do great results with it to lead.
But you're gonna have people that don't, that hate it and leave, or that just refuse to do it.
There's not a lot you can do about it, but I mean, that's always the case, I guess.
Thank you very much.
Thanks.
Thank you for the presentation.
I apologize because I got here a little late, so sorry if this is a repeat question.
were the best kind of camera to use and what was the best photography format you found?
Yeah, so we went through a bit of iteration on there, but we ended up with a Canon 6D with a 24 millimeter lens.
And we shoot RAW.
We shoot RAW, of course.
Good choices.
Hi guys, so...
I'm sorry, this is kind of long.
So, I don't know if you mentioned this during the presentation, but I was just wondering what's your workflow from getting a tileable texture from a terrain in Photoshop?
So, the thing about the terrain textures, they were...
capturing just one element at a time, so almost tiling naturally in the real world.
And again, no trickery there, it was just the regular Photoshop offset or whatever you want to do.
No tricks.
Excellent presentation, thank you very much.
How did you source the terrain data for the Endor, the highly dense forest areas, including the river, and how did you do the river in there?
So we tried to, well we did capture the real locations, and the LiDAR was also from the real locations.
With the opentopography.org, that's mostly based within the US, so we had the location where they actually filmed the movie.
It wasn't as high quality, but it didn't need to be as high quality for Endor, because it was covered with all the junk on top.
So that worked well for that.
Death Valley was where they shot a lot of the, clips from when you hope so that's where we captured that one from and got the LIDAR for as well so it's also from the real real places Hi, with the 3D scanning, I know you've used Agisoft, is what you were saying.
Would you rather be going towards some other direction, or are you looking at some other options for future projects?
I mean, it's what we use at the moment.
There's definitely other options.
I mean, definitely be open to other options as well.
We're always looking into new software that comes up like RealityCatcher and things like that.
So we're definitely not stuck within that line.
We've even talked about doing our own photogrammetry solver within the company itself, so that we have full control of it.
Right.
Hi, guys.
Nice work.
So my first question is, how you do to capture sequoia with only 24 millimeter lens?
Because it's really, really high.
And how you can have so many details in the hay?
So the trees were captured.
I mean, they're absolutely huge, those trees, like four meters across.
So we captured maybe about 10 meters.
