My name is Vivian Tan. I'm the co-founder and CEO of Beast Inc.
And I just want to apologize in advance if I need to cough and my voice is cracking because I'm getting over a flu.
I'll try to keep that down.
So I want to start off by telling you a little more about our company and what we do.
We are a San Francisco-based startup, and we make AI-driven virtual pets.
We are the creators of Beast Pets, which is a VR sandbox experience and pet dragon simulator.
It's currently out in early access on Steam.
It's this friendly little world where you can go into and play with five baby dragons and they each have their own unique personalities.
And our goal for Beast Pets is to create your best virtual pet friend for VR and AR.
So over the past few years, my team and I have put a lot of work into creating these lovable, believable little creatures.
And in this process, we discovered that body language plays a huge role in creating very believable and compelling AI characters.
You don't really need a lot of hyper-realism to create a believable character, but you do need to look at a lot of nuanced movements and presence.
So what do I mean by body language?
Here I'm not really just talking about animations or what we traditionally think of as body language.
I'm using it in a more holistic sense, especially in a very spatial medium like virtual reality.
So body language in this particular context could mean physical presence or spatial interactions, eye contact, or even the player's gestures, not just the AI character's gestures.
So why is body language important?
This is something, I admit, right off the bat, we didn't realize that body language would play such a huge role in creating our characters.
When we first started putting together this wishlist for making awesome AI characters, we looked at all the sexy tech, basically.
Like, we're like, okay, we're going to have this very sophisticated audio communication layer with dynamic dialogue, chatbots, voice recognition.
And then we're going to add an even more sophisticated animation layer with procedural physics-based animations and mocap.
And on top of that, hey, why not...
Computer vision, machine learning, and all this for creating contextual awareness.
And then we just dove right in, like not really knowing what we're doing, except like all this text sounds really cool.
And then once we started building, we realized that we are missing something much more fundamental.
Body language.
Body language, as I mentioned before, not just animations, but how this character shares this physical space with the player.
How do these physical interactions play out?
And lots of other small things.
And in this process, we also realized that, you know, this wasn't immediately obvious to us because...
If you think about our real lives, on a day-to-day level, we engage in body language all the time, right?
I'm moving my hands to communicate.
I stand a certain way or I stand a certain distance from other people or animals.
But a lot of this is subconscious.
We don't really actively think about what we are doing with our postures, with our hands.
Not for the most part.
But when you're building this creature or this humanoid, this character from scratch, you have to build that in.
And then we realized that, and we kind of use that as a check now, it's like, okay, if we're creating a character that can talk or do something more advanced, but something still feels off, we kind of dial it back a lot.
We look at the foundation layer.
It's like, is there something about the character's body language that's sending the wrong signals on the subconscious level?
Because usually those subtle little things is probably something that can make or break the believability of a character.
And as we spent more time on creating these foundational interactions, we discovered two more things.
One, these actions are very much native to VR.
You now have this space in which you share with the VR character that wasn't previously available if you were designing games or interactions.
Even in 3D, if you're viewing it on a 2D screen, that was not nearly as obvious as when you're literally sharing the same physical space.
Two, this is great because that means all of these interactions are available to pretty much every VR designer out there, right?
You don't need to know necessarily anything about voice recognition or machine learning and all the extra layers, but you can, by building in VR, you can still take advantage of body language.
So we doubled down on focusing on body language for our pet dragon characters, and we also discovered, pleasantly, that you can convey a lot of emotion or context, relationships, even without going into the other layers. So that's why we decided to focus more on body language as a foundation, and that can serve as a baseline and a framework for more advanced AI layers.
So through body language, we can provide a lot of context for the player.
You can establish the power dynamics between the player and the character.
You can explain the context of experience without even using words.
You can convey the relationship between them.
And you can convey the intent of the AI characters.
You can prompt players to perform desired actions without explicitly telling them.
And very importantly, you can minimize reliance on dialogue, subtitles, and menus.
And on top of body language, you can build a foundation for more complex AI behaviors like we were just talking about.
So I want to give you some tools and examples that you can implement in your own VR experiences.
This is not intended to be a comprehensive, exhaustive list of everything you can do with body language, because we're still learning and discovering a lot of things about how we can use VR and build characters.
Nor do you have to include everything I just mentioned, but this is kind of food for thought and bring up more prominent examples that you can hopefully integrate.
So first off, scale and power dynamics.
Scale is one of the most powerful things you can use in VR to convey whether the player or the AI character has more power and more control.
So on the two extreme ends of this is you have a very large player and very small characters, or vice versa.
So if you're in a space with tiny characters and a big player, you are automatically conveying that the player has control.
And the player's roles could be a variety of things.
You can build, you can guide the characters, you can protect them, or you can manipulate the characters or their environment.
And the relationship here could be benevolent or malevolent.
It really depends on the context of the game.
But the size differential already conveys a lot of information.
An example of this is Moss.
In the world of Moss, you, the player, are the guardian.
You are this benevolent being.
And your role here is to guide and protect a little mouse named Quill.
So Quill's very cute. She has these really cute animations.
So all of those are cues that, you know, you have a friendly relationship.
You're supposed to protect her.
But right off the bat, even before you interact with her, you already know the power dynamic based on the size differential.
The other extreme are giant characters and tiny players.
In this world, you tend to have very little control.
And your role is typically to observe.
to avoid or to learn.
An example of this is the big whale in the blue.
So the whale's not really an AI character, it's more of an NPC.
But I think it's just a good way to illustrate how the size differential sets the context and the power dynamic in this world.
So you're in the underwater world, but you're kind of a tourist.
You don't have a whole lot of control or influence over the environment.
And nothing demonstrates that more than when a big whale swims past you and you realize that you're really just a very small part of this world.
Another example is the large life-size dragon in Skyrim.
So in this case, you aren't just a Meserver, you have to fight the dragon, but the size differential tells you that you're not the boss here.
You have to fight the boss.
So, in between these two extremes, we have normal-sized characters.
They don't necessarily have to be the same size as the human, but they're kind of a more realistic scale.
And this is where you have a lot of nuance.
So here on screen, you can see the representations, right?
The AI character, represented by the little robot icon, can be slightly smaller or slightly bigger than the player.
And on a 2D screen, it doesn't make too much of a difference.
But in VR, that small size differential could convey volumes about your relationship with the AI character and what your roles are.
So, an example that I really like, that I came across a couple of weeks ago when I was at OC6, and I sat in on a talk from the Oculus creators that created First Steps.
So First Steps is a really short intro app on the Oculus Quest headset where you encounter a robot and he, she, it walks you through a tutorial to teach you how to use the controllers and how to interact with the environment.
So this is a friendly relationship in which you're supposed to be learning from this robot.
And the Oculus developers that created this, they discovered that initially they had like a six foot tall robot.
But players come in all shapes and sizes.
So when they were testing this with players that are about my size, and I'm five feet tall, there's a robot that's towering.
It is sticking its hands in your face, like, use these controllers.
And you're just like, dude, get away from me.
So that is not the relationship you want to create when you want to have a friendly interaction.
I thought that was a really good example of, you know, when you are creating friendly characters, you typically want to err on the side of having the friendly AI characters be slightly smaller or shorter than the player.
Now, if the relationship is supposed to be antagonistic, then you have a lot more freedom, right?
The enemy can be larger or smaller.
And if you want to dial up how threatening the enemy is, you can just make them increasingly larger and tell them the size of the dragon in Skyrim.
One tool that you can use is to dynamically scale the character height.
to match the player.
So for instance, you measure where the average HMD height is on the player, and then you can scale the character down or adjust their height placement to match that of the player.
It's not always available, but it's one tool that you can consider.
And also, these are not hard and fast rules, and I also believe that rules are meant to be broken, especially as we're trying to discover more and more things in VR.
But it creates a good baseline on which you can kind of...
Measure. You can set player's expectations and measure and test the results.
Another tool that's at your disposal in VR is vertical space.
Where you put AI characters conveys a lot about how much threat they're posing to the player.
If they're coming in high above you, they tend to pose more threat.
If they're...
Far below eye level and we're on the ground, they present low threat.
Sorry, I think I had that backwards.
Coming in high, high threat. Coming in low, low threat.
And at about eye level and chest level, they present a neutral level of threat.
So you can use other cues to represent if they're supposed to be friendlies or unfriendlies if they're in the neutral zone.
Some example of this.
So all of these characters and entities on the screen are roughly one cubic feet, but they're starting in their default positions say a lot about the threat level they present.
So up high, you have the space pirate trainer robots, right?
When you start playing in space pirate trainer, you are on a platform and a robot is hovering high above you.
Eventually the robots will lower to about You're hiding, they zoom in towards you, but the starting position already conveys a lot about their intent and their relationship to you.
On the low threat level, you have a little robot dog in the VAL.
It's small, it's cute, but it's also on the ground, so it's not really presenting a lot of threat.
In the neutral zone is where we're putting our pet dragons, typically.
And I have the Beat Saber blocks in there.
I know they're not AI characters, but I think they're just kind of like the most recognizable example of entities you can react with.
And they're typically placed in the neutral zone.
And then the designers use other cues, such as the fact that you're holding sabers, to convey to the player what you're supposed to be doing with them.
So in the neutral zone, you have a lot of leeway.
And what's funny is that we discovered through trial and error that the positioning conveys so much about threat levels that even if your character design is that of a friendly being, if they're placed high above, they become almost automatically threatening.
So initially we did not limit how high our dragons can fly.
We're like, okay, well, they're the size of house cats.
They're meant to be cute and cuddly.
We have all of these contextual cues telling you that these are friendly creatures.
But then we also noticed that because we designed them to fly into the player's field of view, if players start looking up, the dragons fly higher, the players look up even more, and pretty soon you would have this swarm of dragons staring down at you.
And it doesn't matter if they're cute.
When you have five dragons just hovering above you, it becomes really intense.
And that's when we learned, like, okay, we need to put a limit on how high they can fly.
So right now we're aiming to keep them at between, like, chest level and eye level, so they can stay between the neutral to friendly zone.
Proximity plays such a huge role in context.
And it's one of those things in retrospect, it's like, well, yeah, obviously, right, proximity.
But it's like, when we started breaking this down, we noticed a lot of nuance and a lot of things that we can play with, with only proximity.
So one, you can indicate which characters are active or passive based on how close they are to the player.
So one of my favorite examples is Gorn.
So in Gorn, you play a gladiator, and you're inside this gladiator arena fighting other gladiators.
The arena is divided into two sections.
There's the pit in which you're actively fighting these AI gladiators, and then there's the spectator area that I like to call the bleachers.
That's elevated above the immediate play area.
So there are multiple contextual cues going on here.
So you have the players that you're immediately fighting, and then you have these spectators that are up there, and they're leering down at you.
They are not friendlies, right?
Hence, they are in the high-threat zone.
But they're also far away.
They're not in the immediate player area.
So you know by proximity that, okay, those guys are not an immediate threat, but they're also not friendlies.
Another observation here is that the other gladiators you fight have limbs, so they can attack you because, you know, they have the ability to run towards you, they have the ability to wield weapons, whereas the spectators are just talking heads.
They have no bodies. So at first glance, you know that those guys aren't just going to start jumping into the pit and attacking you because they physically cannot.
So I just thought this is such a great example of multiple layers of body cues at play, conveying the context, the intent, who your enemy is, who's a friendly or who's just a passive NPC.
You can also use proximity to prompt player movement, right? A character that's immediately within the player's reach encourages the player to be stationary because you don't have to move.
A character that is slightly out of reach encourages the player to physically take steps and walk towards the character.
And a character that is further away encourages the player to explore teleportations or other virtual types of locomotion in order to get to that part of the play area.
Eye contact is also a very useful tool.
Especially in VR, instead of just having the character stare out to the screen, kind of like breaking the fourth wall, as you would in 2D game, they can make direct eye contact with the player, and that conveys a lot of information, and you can use that to accomplish a number of things.
And when you use eye contact in conjunction with field of view, you can do a lot to direct the player's attention.
So when we first started building this, we were like, oh, OK, well, eye contact is so cool.
We never had this before on a 2D screen.
Let's make sure the dragons acknowledge you, look you into the eye.
And we kind of overdid it a little bit.
So they wouldn't break eye contact with the player, and that became really, really intense.
So like I said, all these little cues could and do your character design, your set design.
Because we had people cringing at the dragons.
We're like, reach out, pet them, they're cute.
They're like, they're staring.
So we're like, okay, all right.
Okay, we gotta break eye contact.
How do we do that naturally?
So now we kind of have the dragons just zone out a little bit, they'll stare up into space.
They look at each other, they look at toys.
So they're no longer just staring at the player.
And another thing we did was that we wanted to give these little guys a life of their own.
So even if you're not playing with them, they can be off playing by themselves or doing their own thing.
The problem is the players don't know that there are dragons behind them.
And this is not just us.
VR developers complain all the time about, you know...
You want to encourage the players to turn around and explore their play space, but if you're new to VR, you just go in there and you stare straight ahead.
So this is where you can use a combination of eye contact and field of view to direct the player.
So what we do is we have the dragons occasionally check for the player and just kind of fly into their periphery.
They don't have to straight on stare, but just kind of come into the side of field of view, maybe flit out a little bit, and just little actions, letting the player know it's like, hey, look to your left.
They are interesting things over here.
So it's a useful tool for getting players to follow or look around.
So another thing I like to talk about is using the aforementioned tools in combination to apply attention pressure to the player.
So we talked about proximity.
We talked about direct and indirect eye contact.
So these are two variables you can play with, but you can also adjust the number of AI characters.
And by combining these three variables and adjusting them, you can accomplish a lot of nonverbal cues to get the players to take different actions.
So, for example, if we dial up all three variables, right?
Lots of AIs, close proximity to the player, making direct eye contact.
You're creating high attention pressure and therefore high urgency.
This could mean that you want the players to fight them or that there's something really important going on and you want the player to pay attention.
Conversely, low pressure would put the player more at ease and encourage exploration.
So in the examples I mentioned with us accidentally putting too much pressure on the player with the dragons, we were kind of just making them cower and not really encouraging them to go out and explore this friendly world we had built for them.
So we learned that by easing up on the attention pressure, right, kind of have the dragons just...
Stay a safe distance away.
We're telling the players, like, you can navigate this world at your own pace.
And we're inviting curiosity.
We're encouraging exploration.
You can also switch it up.
If you increase attention pressure and decrease it, for instance, having a character come towards the player, make eye contact, and then run away.
And by alternating that.
you can create curiosity.
The player will be like, hey, this AI is trying to grab my attention.
Maybe I should follow.
So yeah, so by combining all of these variables and adjusting the parameters, you can invite the player to take a lot of actions without ever expressly telling them to.
Lastly, I want to talk a bit about hand gestures.
So this is yet another tool that's available in VR that we didn't quite have before when we were just creating for like a 2D screen play space.
So you can use hand gestures to reinforce the player's roles and also the character's roles.
One thing we learned is to treat hands as tools, and I'll elaborate on that a little more in a second.
And two, hand gestures comply not just to the AI characters, but to the players themselves.
So this is where you can have a lot more dynamic interaction.
One of the things we learned is to only allow the hand gestures you want the players to use.
And this ties back to using hands as tools, right?
If you don't want them to attack the AI, then don't give them a fist.
Because we learned the hard way, too.
Like, if you give them a fist, they will use it.
So a couple of examples.
If you give them an open palm, that signifies friendliness.
Because they can wave.
They can high five.
Closed fist typically signifies hostility, because they can punch.
Two examples of this in Alchemy Lab's vacation simulator.
You have these cartoony hands, but you can wave at the robot characters, and that gets their attention, and they know that you need help, so they come to you for help.
I don't have it on here, but in Rec Room, you can high-five other players.
They're not AI characters, but it's also just an example of how you'd be using hand gestures.
In boxing games like Creed, you have fists, you have boxing gloves, so you know that you have an antagonistic relationship.
But as we learned in Creating Our Dragons, even if you don't have gloves...
Just the fact that they can form a fist, people will use it.
So, again, kind of the incongruousness of the body language is compared to the context you're trying to, the tone you're trying to set.
Like, in our game, we wanted a friendly relationship, but when we put people in there, they had fists, they were punching the dragons.
We're like, okay, well, time to take those fists away.
Because that was not the relationship we wanted to reinforce.
And I should also mention that when we defined hand gestures, this was using the HTC Vive controllers or the Oculus Touch controllers.
So you could limit hand gestures.
Now that Oculus has introduced hand tracking, you pretty much have the full range of motion.
We have not yet looked into that, what that means, because now people can form fists if they want to.
So that will be a really interesting area to explore as we move forward with this.
Last but not least, so how do you know that any of these tools, tips, and tricks actually work?
It really just comes back to doing a lot of playtesting. I feel like as developers we're kind of primed to think about things a certain way already, so what we ended up doing was just testing it across lots of different people, people who are not in our genre, people who've never played in VR.
So we can get their immediate visceral reaction.
We were talking about body language operating on such a subconscious level.
It actually works really well to test with first-time people because that's the level they're reacting on and you can get a lot of useful feedback.
So that wraps it up for today and if you'd like to get in touch with me, that's my email and my Twitter.
And we're opening up to questions.
You know with the hand tracking, I wonder if it would be possible to game it so to speak, not to make too bad a pun, so that no matter what you did, no matter what the sensor was picking up, it would never show a fist.
Couldn't you do that somehow?
Program it that way?
That's a really, really good observation.
I'm sorry, let me repeat the question in case other people didn't hear it.
The question is, can you limit the hand motion range even with hand tracking?
So, like, even if you're forming a closed fist in real life...
All you'd see would be...
Right, all you'd see is this.
Yeah, that's really interesting because one of the reasons we didn't look into hand tracking yet is because we wanted the haptic feedback.
But then I've been told by a lot of developers that you get this sort of ghost sensation, even though you don't have haptics. And so I wonder how much incongruity that would create, like if your real hands don't match your virtual hands.
It would be like forcing people to be friendly. And it would be interesting to observe what effect it has on them. If your anger can never find the muscular expression.
How will that affect you ultimately?
And it could be an interesting psychological tool as part of the game.
That is, that is really interesting.
We would, when we get into hand tracking, we would definitely test that.
Thank you.
Any other questions?
Anybody?
Okay.
So, you indicated that you tried to get the dragons to play around with toys.
Did you give any audio cues that the dragons were playing around, interacting, that the player could listen to and try to direct their attention to the audio?
That's a really good question.
I'm just going to repeat it.
So you're asking if we looked into using audio cues to indicate what the dragons are doing and what direction they're coming from.
OK.
Yeah, so that's something we tried a little bit, and I thought about including the presentation.
I didn't, so thank you for bringing that up.
We have. It didn't work as well as we thought it would.
One, your 3D spatial audio in VR right now is not entirely reliable.
It's still very subtle, so visual cues...
are at this point are more powerful.
I have heard of like really good audio designers creating entire experiences using only audio cues as guides.
But I think to accomplish that, you need one really good audio designer because it's much more subtle than visual design.
And two, the player perception is still not as consistent.
Like, we kind of, we tried out audio cues on me, because I have trouble picking up where sound is coming from, like even in real life, like if an explosion goes off that way, I can't really tell.
Like, I'm probably an extreme example, but I'm just not as sensitive to audio cues as I am visual cues.
So we do—to answer your question, that was long-winded—we do use audio cues, but we don't find them nearly as reliable as visual cues.
Thank you.
Thank you.
Hi, I'm Eric. So I recently have been doing a lot of playtesting for the tutorial of my game, and just as a design principle, I challenge myself to have zero text and no, you know, like panels or anything like that.
And you know, throughout the different iterations, no matter how streamlined and how many guides there were, and only like one event at a time.
I too like to play test with people who are completely new to VR.
And even if it's just the most obvious single visual slash audio cue right in front of them, they might miss it because they're so.
over one with the environment. I was just wondering if you could share any sort of best practices of onboarding or tutorial techniques without, you know, overt text to guide them or voiceover.
Yeah, that's a really good question.
So to repeat it, you want to know if there are some best practices for onboarding new players without relying on text.
Or voiceover.
Or voiceover.
OK.
Yeah, so we have some insight into this.
In our initial product, we actually built it for VR arcades.
So we were expecting a lot of new players.
And we tried to minimize the amount of text.
Full disclosure here, unfortunately we had to add a menu and things later on, so what you described is something we're still struggling with, trying to minimize the amount of text and voiceovers entirely, but we still haven't.
But some of the things that we discovered that really worked well is to...
Control the environment your players are in.
When we first started, we would drop players in this very lush, green environment with birds chirping, lots of things going on, dragons flying around.
And we're trying to get them to pay attention to the tutorial.
And they're not. They're like, there's a dragon. I want to go over here.
They're doing all sorts of things.
So what we do is we drop them now off in a starting zone.
No teleportation.
virtually nothing to interact with other than the thing we want to pay attention to.
So one way you can think of it is kind of like stage lighting, right? If you black out the rest of the stage and just put the spotlight on the thing you want the audience to watch, you can recreate something similar in VR using that. It doesn't mean you need to have a blank background, but the proximity we're talking about, right, in this sense, think of scenery, right?
out of reach, fading into the darkness, or something like that, and just focus the players on the thing you want them to interact with.
Oh, another thing, and I didn't mention this in the talk, is when we're talking about coming into the field of view, even in that situation, your player will probably still look around.
It's funny, like, they don't look when you want them to look.
When you don't want them to look, they're looking all over the place.
So have whatever you want them to interact with just inch its way into their field of view.
So we not only do that with the dragons, we also do that with the menus sometimes, just to have it come within their field of view.
Okay, thank you. You're welcome.
Okay, cool. Thank you very much.
