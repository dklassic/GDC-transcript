Okay, so playtesting is in my opinion one of the most important topics in game design.
I find it exhilarating.
It's the moment that everything comes together or falls apart.
The moment where your bainies turn out to be monsters or your quick fixes turn out to be lovers.
However, most indies I talk to see playtest as a necessary evil, a painfully confronting moment.
And honestly, I don't think the biggest problem here is that it's difficult to receive feedback on something you deeply care about.
No, that's something I think you can overcome relatively quickly.
I found that the biggest problem is that a lot of indies use conventional playtesting methods in finding testers, in setting up playtests, in gathering feedback, in using surveys or using analytics, and they end up with data that is hard to interpret, that is confusing or even contradicting itself.
Evil data.
It's dangerous data.
I'll explain how I define it in a bit, but it's the stuff that makes...
that makes making games only harder and makes playtesting way less appealing.
So the goal of my talk here today is to basically go over some of the things I found over making seven games in many years.
So I've been trying harder and harder to avoid evil data and I've experimented with playtesting itself a lot.
I've rebelled against a lot of conventional wisdom.
Sometimes that failed tremendously, but I also found a lot of these playtesting 101 things just not to be entirely true.
And so the goal of my talk really is to make you think about how to make your playtests more valuable and how to make the playtests easier to organize.
So on the schedule is, I'll be talking about what evil data is, where it comes from, and how to avoid it, and how to organize more valuable playtests.
The talk will get more practical towards the end, so bear with me.
Before I start, I think I need to remind you that QA is not the same thing as playtesting.
Simply put, I think QA focuses on getting things to work at all, while playtesting is much more about the quality of the things that work.
I thought I should mention it.
Okay, so my name is Adriaan de Jong, Adrian in simple English.
I'm a game designer, I'm from Amsterdam, the Netherlands.
I am best known for the IGF nominated Fingal, which is a finger rubbing game, which I made with the now closed Game Oven.
I talked about that this morning.
And I'm also known for Bounden, which I also made with Game Oven, a sort of mobile ballet dancing game made in collaboration with the Dutch National Ballet.
And I just released Hidden Folks.
Yay.
Awesome.
Thanks a lot.
So this was in collaboration with the illustrator Sylvain Tegrouge.
In the game there's a bunch of targets.
They are hiding behind things or inside of things.
And the only way to find them is by unfurling tent flaps or cutting through bushes or slamming doors or poking crocodiles.
I've been thoroughly testing him, folks, and all the games before that for more than seven years now and evil data was lurking all along.
So to explain to you what evil data is, here's an anecdote.
A long time ago, I playtested HintFolks at some event.
This is maybe two years ago.
And at the third area of this, there was a hint for a target that was in the garage.
Most people relatively quickly told me they didn't know how to open the garage door.
They like tapped on the door, but nothing happened, and they couldn't figure out why it wouldn't open.
And most people quickly quit after that.
So the situation was that you need to drag it open, right?
It's like a simple drag, but previously you would only have to tap on things.
The next day, I implemented a solution to this problem that the play testers suggested to me, which was a tiny hand that shows you how to open the garage door.
And this worked really well.
People know they can open the garage door now.
They sort of like act like their hand is a small hand.
But now the game has become really boring.
Any previous feeling of exploration is completely gone and people feel like the game is treating them like a baby.
And this bothers me for like a month or so before I realized I made various mistakes in playtesting and implementing this feature.
I listened to suggestions from people who didn't understand the problem or did not understand what I wanted to achieve with the game.
The people at the event weren't really patient enough, and so they didn't really take the time to explore, hence they didn't really find out how to open the door.
So, turns out the problem wasn't necessarily that the game did not tell people what to do.
The problem was that the game was not telling people what not to do.
I realized that players would almost always tap on the garage door, right?
But in reaction to that, the game did nothing.
And so what I did, after like a month of thinking about this I got rid of the tiny hand and added that if you tap on an object you're supposed to drag, it wiggles a bit in the direction of where you're supposed to slide it.
And this worked like a charm.
People more easily discovered that you had to slide it open, but most important of all, people had a giant smile on their face and they discovered it by themselves, you know?
And this is just one example in which the playtesting results I got from, you know, all the playtesting distracted me from making my game better.
I was looking for the wrong things and I listened to the wrong ideas, and I think this clearly demonstrates that some data is time-consuming and it can make your game worse.
And that's evil data.
So evil data is playtesting results that are distracting, that are unclear, or that are misleading.
And in turn, that may lead to you guessing what to do.
It may lead to a worse game.
It may lead to a less unique game, longer development time, and many other bad scenarios.
OK, so that's evil data.
Now let's talk about where it comes from and how to avoid it, because I've been seeing it in many places.
and I've been putting more and more time into modeling the playtests so I can avoid it entirely.
And I basically distilled this down to eight points, eight things I'm now actively paying attention to.
Physical location.
I remember John and Brenda Romero at Fingal's IGF booth, my second GDC, couple years ago, maybe six years ago, jeez.
after my like, it's like finger sex pitch, they reacted all like, oh, I'm not doing this.
This is disgusting.
Who would ever want to do this?
And they were saying it in a playful way, sure.
But they didn't play it then, right?
I could have seen this as like a sort of, I could have concluded now, oh shit, maybe I need to work on my pitch.
I should maybe make a different game.
But I bet that if they would actually be at home sitting together on their couch, that exact same pitch would have probably made their fingers make love.
So, the data you gather from playtests is hugely influenced by the setting, by the context of the playtest.
Likewise, I found that people playing Hinfolks at events tend to skip the bonus targets more easily and give up discovering the interactions more quickly than when they would play at home.
Bounden, the mobile dancing game I talked about, suffered terribly from this evil data.
In hindsight, the game was built on the results of hundreds of playtests that happened at events, playthroughs that lasted five minutes as opposed to an hour.
And as a result, we designed the game around those first five minutes.
And the rest of the game feels more like a repetition.
And this graph of Bounden sessions lengths, is really painful stuff for me as a designer.
Like nobody, 1%, probably like around 1%, has been playing my game for more than 30 minutes.
That's painful stuff.
So I want to avoid this.
And to avoid this, to avoid this evil data resulting from the setting of the playtest, you just have to make sure that you playtest in a setting that fits your game.
This could be, you know.
at someone's home, at a party, if you're making a local multiplayer game, wherever it fits your game.
Okay, so that's one.
Two, people.
It is tempting to playtest mainly with all the other developers at your office.
Or, you know, you have this group of testers, the board game night that you're always testing with.
But different people have different understandings of games and different experiences with the vocabulary, with controls, with conventional game ideas, with structures, different experiences with prototyping, with testing, giving feedback.
So in an early version of Hidden Folks, we had many of these dialogue boxes, and they appeared above these folks, these little characters, and tapping them would bring up a little joke, a very meaningless joke, to give the characters in the game a little bit more character.
So a lot of people who don't really identify as gamers thought they were actually really funny and then just continue playing the game.
But a lot of people who do identify as gamers really fixated on these meaningless jokes as if they were like very important information, which it was definitely not.
And it was actually a giant problem.
And these actual things gave very young kids the idea because they couldn't read, that they were missing out on a lot of information.
And I actually had parents come to me and complain to me about having to explain how the jokes would make sense and that was actually very confusing to their kids.
So it was very different, that different groups of people perceived this speech bubble thing completely different.
So generally speaking, I think this may happen to elements in your game as well.
I can imagine that if you're making a game for young kids, it's for instance often played by parents first.
So if the parents don't find it like sort of amusing or they don't understand it, the kids probably never get to play it.
And I found that people who play a lot of games are almost always more impatient than non-gamers.
And it makes me wonder what the other differences.
what other difference there might be that influence the way that people perceive and play your game.
And so, yeah, because of this, I think it's easy to miss bottlenecks entirely depending on who's testing.
So how to avoid evil data? Well, never restrict your testers to a specific group of people, even if those people are not part of your so-called target audience.
Everyone, I think, has something to say about the game, and, you know, my mom has given me amazing advice on the game, and she's not a game designer at all.
So yeah, I mean, think about like co-workers, other game developers, you're into Game Friends, you're not into Game Friends, family, young people, old people, people from different cultural backgrounds, fans, online community, just like try it with all these different kinds of people and see how they perceive the game.
And you'll get different kinds of results and it might actually help you understand how different people play your game.
Okay, third thing, your introduction.
This might be a bit of a beginner's mistake, but a lot of indies tend to give a full introduction of the game before the testers get to play.
The idea, the story, the controls, I think that is a huge, huge missed opportunity.
When people buy or download your game, they won't get that introduction, and they might hit numerous bumps that could potentially ruin the experience of the game.
So.
The trick in avoiding evil data here is to skew the data as little as possible.
I have learned by far most about how to introduce the game, the controls, the UI, and many other elements.
And I even learned a lot about marketing, but just simply throwing people in and seeing where they really get stuck, or what they eventually say to me.
You have to sort of allow people to discover how everything works and how the game makes sense.
And do not mistake the time that they need to discover everything as a problem.
So don't tell people what to do, do not explain the story, don't explain what's missing, don't explain the controls, don't explain the interface, just don't explain anything.
You'll see where the tester will actually hit any bumps.
There's maybe one thing I would mention.
And that is that the game is not done yet, because this gives testers a mental slack for when they, you know, when they're staring, when they know you're staring at their actions, like, while they can't figure out something.
And it sort of incentivizes them to speak their mind and be, you know, be honest about, don't feel so awkward at not getting it.
Okay, number four, surface problems.
For maybe a whole year, I pretty much ignored the difficult design problem that the difficulty of the targets varied very much, actually per player.
And that some players would get stuck on finding a required target and quit the game prematurely.
I noticed this pretty quickly after introducing the required and bonus targets paradigm, but as I didn't know how to solve it immediately, I slowly started to become blind to it over the months following.
And so I call these sorts of problems surface problems.
They are like really obvious, right there on the surface, but you neglect them over time.
And another example would be that You can focus so much on the details of a character jump in your four-player local multiplayer game that you may forget that not even 1% of the people who would probably like to play your game even have four controllers.
Now that's a problem you should seriously be thinking about.
And it's very easy to sort of like forget about these problems.
And there's actually design things you can do about this.
So how to avoid evil data coming from these surface problems?
Well, keep looking at the player's experience as a whole.
Don't always test with a specific goal or a hypothesis or subdivide your playtest into groups like concept testing or UI testing or usability testing or level design testing.
Playtest so often that you get a lot of data on the whole thing and it becomes harder to neglect all these problems.
Okay.
This might be a little bit of a controversial one, but...
We'll see whether you agree with me on that or not.
Because what happened to me super often in HinFox were layering issues, making it difficult to see something.
Sometimes because a character animation was randomly assigned and a target would blend with another bush or something, making it very difficult to see.
Or I would accidentally make a build of the game with the setting screen on by default.
So every time you loaded a new area, the setting screen would be right there.
It's super annoying.
And.
the player would have to close it every time.
And these sorts of tiny issues can have a big effect on the player's experience.
And so it also has a big effect on the data you extrapolate from the playtest.
People might quit more quickly.
This issue though, would probably take less than five seconds to fix.
And so.
What I do, and this is really against every advice I've ever received from other game developers, is I often make changes to the builds in between playtests.
And it's made a lot of my playtests way more useful because I wasn't afraid to then actually move the target a little bit or disable the setting screen right in between builds.
So yeah, that's a thing I do and I can recommend it.
Okay, next.
So what testers say.
Testers start playing and they start talking the things they like, the things they don't like, the things they don't understand, et cetera.
But it's your job to filter out all the bull crap that they're saying and distill it to the root of each problem.
Because people are great at saying what's wrong while never actually saying what's actually wrong.
I encountered this so often, it's really frustrating.
The first hidden folks UI is a good example.
A lot of people told me, I don't want the targets to be on the screen all the time.
So what I made is a thing that made this giant interface in the right disappear.
So you could press a button and it would go away.
But then I made that and nobody used it.
It was like, huh, what's, and also nobody complained anymore, so it was very strange.
And later on I figured that the problem wasn't necessarily that the targets were in the screen, but that the panel was simply too big and it was obstructing a giant area of the screen, duh.
But the dozen people that played it and complained about this didn't actually mention the panel being in front of everything.
So nobody said it, but everyone was sort of pointing at it.
And so second hand information, like what people are saying to me, aka evil data, can be a huge, huge distraction.
So what I encounter a lot is that people explain the solution to a problem they don't fully understand, or a problem that might not actually be the problem.
Or people give suggestions based on this thing they saw in another game that must be way better than what you have.
So yeah, on top of this, like social structures between you and the tester can make what people tell you very biased, even when it's anonymous through a survey or something.
So I think feedback filtering is probably the skill you'll have to train to do your place as well.
So you take in what people say, you always consider why, why the hell is he saying this?
Okay, next, surveys.
So surveys are by definition second hand data.
Think about ratings.
Ratings have not ever helped me pinpoint problems.
So this one star review is like useless to me.
I cannot make level three better without simply guessing what's wrong with it.
I have to guess, oh it's a shitty level, okay, now what?
Open questions.
People often these open questions with the solutions to problems that are difficult to read in between the lines.
Here someone said I didn't like level three because it was too difficult. I would make Johnny you should find So you'd say oh, maybe John is the problem, but actually it turns out the problem wasn't with John But the fact that people didn't understand they could drag a car away that would then job make John appear So I I needed to focus on making it clear that you can drag the car away nothing in this question ever hinted to that and Yeah, that's dangerous. I couldn't yeah, that's dangerous stuff So I also discovered that the way players perceive the game or its features, it changes over the course of the game.
I know this because when I ask people to record videos of their play sessions, and this is something I'll talk about later, and compared their videos to what they emailed me, There was a very clear correlation between the emotions and the frustrations that they had at the end of their video and their emails.
I could see the tone of their mail was way more in line with the end of their play session.
So, how do you avoid evil data with surveys and open questions and walls of text and feedback?
Well, my opinion is no surveys and no questionnaires ever. Just don't.
There are other methods to know what people think.
that might be more consuming, but at least they're way more reliable.
Next.
So making video games, you've probably sent over builds of your game to other developers, to friends, asking for feedback.
Maybe you've done a beta of your game.
The results coming from these playtests can be full of evil data, easily turning your designs and challenges and player obstacles into guesswork.
I explored a few ways of getting valuable data, valuable feedback from online testers, but each method has their own challenges when it comes to avoiding evil data.
So just asking people is, of course, by far the simplest thing, but it's also the most evil data-prune way of gathering feedback for reasons that we've discussed before.
You'll get second-hand experiences.
It's written after the player is done playing.
And it gives the player time to think about the experience and give you their solutions instead of the problems.
So I actually don't aim to get feedback from testers this way anymore.
It's just usually too general to act upon.
Using analytics to figure out the things you don't know paradoxically requires you to define a hypothesis and set up trackers in a way that the accumulated data will tell you anything useful.
So how can you prepare for something that you don't know?
So in this example, using game analytics, which I can recommend if you wanna go down this route, I can see how long it takes on average to find targets in the camping area in Hidden Folks.
The mushroom in a hidden folks takes people much longer to find on average.
And maybe that's not even bad, but this chart doesn't tell me why.
I know it's longer, but it doesn't tell me why.
And that's so I have to guess why.
And I think that's evil data.
So another common analytics method is using funnels, seeing how many players convert from level to level or menu to menu, or anything that has a sequence to it.
What you're seeing here is the level progression of Hidden Folks for the players who download the game in the first week.
So only 7% of the people who bought the game actually reached the end, which I've heard is actually a really big number.
So in this funnel, also from Google Analytics, loads of people quit at level 3.
You can see a giant drop.
But I have to guess what it is.
And again, like guessing work, very prone to evil data.
I might change the game for worse here.
Okay, heat maps though are way more spatial than regular analytics data.
So it's a bit more specific and you can remove some of that guesswork.
What you're seeing here is Unity analytics running inside the editor.
And I was able to conclude from this image that people are eager to tap on the back of the truck.
and the motorcycles somewhere like in the middle right.
And those things were actually missing feedback.
And so I added those and now people would have a much more satisfying experience playing the game because more things would react to it.
So yeah, I can do a little bit of level design based on this, it's a little less guesswork.
I can identify some problems, but it still doesn't come close to actually seeing people play.
Which brings me to using gameplay recordings.
I am super into this right now.
I'm not talking about YouTubers or Twitchers who try to entertain their audiences, but I'm talking about people making videos of their play sessions dedicated to me.
A lot of the evil data you get from physical playtests is still present in these kinds of playtests, just to a lesser extent.
This is currently the closest you can get to a first-hand experience of the tester through the internet.
You see what they do, when they do it, you hear what they say, when they say it, and they are doing it in their own time, at their own pace, in a relevant context.
So evil data still comes from testers.
They still give suggestions while never describing what the actual problem is.
Fortunately, you can actually see what they're doing, so you can, you know, it helps you filtering and also see what the actual problem could be.
On top of that, people are still very aware that you're watching their every move.
So they, actually most of them just talk to me personally as if, hey, like, oh, Adrian, look at this thing that I see here.
It's almost as if we're having a conversation sometimes, even though it's just a video for me.
But my biggest problem with this method is that it's a hassle to set up.
So there are people that wouldn't bother doing this, like my mom, which is a problem if, you know, the target audience you have in mind isn't that tech savvy.
So this kind of footage is actually super useful to me in many ways.
Think about timing, level flow, big things and small things.
And I think that if you want to avoid evil data, this is probably the best starting point right now.
But do keep in mind that this will not save you time over physical playtests.
You still have to sit down and watch hours and hours of footage.
Yeah.
Okay, so those are the eight things that I found that can help you think about evil data and how to maybe avoid it.
So how to organize playtests with minimal effort.
Let's talk about how you could do that.
The thing is, if you try hard to playtest throughout the development of the game, instead of when it's almost done, you'll have a much better grip on the player's experience.
You'll save a lot of time, because you won't be working on the wrong things, and you'll be able to work towards version one confidently.
If you play this throughout the project, you will see the nuances of your design.
For instance, we couldn't quite put our finger on what it was that made some of the hidden folks areas more fun than others.
And it took us, I think, maybe a year to understand what the parameters were that made the game fun.
The density of the visuals versus the density of the interactions.
The 17 different kinds of hints that we found.
The different kinds of interactions that we had.
The visual order or the chaos, et cetera.
All nuances that we found because we saw so many people play the game and thought about it so long.
But you can't just play test every day, right?
There's a lot of overhead.
You need to make a working build, you need to get people to play.
For physical playtests, you need to be there, take notes.
You have to reflect on your findings and all that before you can get back to making the game.
So the way I've optimized it is to trigger playtests at certain moments in my development.
I try to do a playtest when I implement a major feature, a minor feature, or when there's an event.
If any of these things occur, I would set an almost automated first step into the direction of playtesting.
And I would just roll with the playtesting structure that I had in mind, which I'll explain in a bit.
With Hint Folks, we set out to playtest with every new theme, for instance.
I organized a few offline playtests with non-developer friends and family, an online playtest with people who signed up to test on our website.
And the way I organized them was really...
I really came up with that to make it as simple as possible for me to start.
My setup came down to three things.
I made a spreadsheet of all the testers, I wrote a bunch of email templates, and every time I emailed someone, I would put follow-up dates on my agenda.
So to initiate an offline playtest, I would maybe text a family member or a friend, or I would email someone from a list that I keep with people who have indicated they'd love to try the game.
And it's really that simple.
Just send a text, send a single email.
And I would case by case schedule things with these people and would sit down and play the game.
To initiate an online play test, I go through a spreadsheet, I'd email somewhere between 30 and 50 people with a build of the game and instructions to install or record the gameplay.
And I would immediately set a reminder in my agenda like a week later.
for if they didn't respond, I would get back to them.
And again, case by case, I would handle questions and emails all in hope of gameplay videos with voice.
I have all these template emails I can pretty much copy paste, iterate on if needed and just send out.
It's really simple and it's surprisingly effective.
Everyone I ask for physical playtests, literally everyone, just joins me for a session.
Maybe this is also a social thing but everyone just likes to do this.
And about one third of the people I ask for online playtests, they respond within a week.
and then another one-sixth to a third within two weeks after I send them another email.
So, for him folks, this meant that for four months, especially during the last development of the game.
This meant about 20 to 30 hours of super useful gameplay footage.
This was like my breakfast every morning.
I would sit down with someone else playing my game.
And these are actually really great numbers for something that I initiate within a couple minutes.
So yeah, it all starts with a simple email, minimal effort, and I get pretty amazing responses.
So think about doing this.
It's really simple.
To inspire you to think about sort of alternative ways, Here's three other things I've also tried.
I playtested Bounden every other day, so we made a level on day one, we playtested it on day two, made changes on day three, playtested it on day four.
Very intense to do, super fun, super motivational.
Yeah, it didn't really give us any insight into level flow though.
Fingle is like a party game, you know, and all the finger rubbing is just weird and awkward and social.
So the parties were just really, really the perfect setting for it.
And I was still a student when I was prototyping it, so I just like take the game to parties every weekend.
That was perfect.
Play Dev Club, it's an event I organize for game designers in the Netherlands to come together and playtest each other's games.
As all playtesters, as all attendees are game designers, you can really talk about your game design and get feedback in the form of brainstorming together, really trying to understand the design of the game.
And people will play for longer than for instance consumer events.
They stick around to give feedback.
But this definitely doesn't replace playtest with consumers.
Designers tend to see problems that aren't problems yet.
So yeah, I hope maybe these three things will inspire you to figure out something for your game.
So to conclude, how to avoid evil data.
Playtest in the right setting, make sure the location fits your game.
Playtest with everyone so that you don't stare yourself blind to all these surface problems.
Don't explain anything, just throw people in the game, see what they, see where they get stuck.
Always playtest the whole experience.
Make those one minute changes if necessary.
It might have a big influence on your playtesting results.
Filter feedback always.
Never just listen to people and do what they say because they don't always fully understand the problem.
No surveys, no questionnaires ever.
There's better ways of gathering more reliable feedback.
Try getting gameplay and audio recordings because that seems to be a really solid way of gathering feedback.
And you know, make a couple email templates and make initiating a playtest super easy for yourself.
That's it. Thank you.
