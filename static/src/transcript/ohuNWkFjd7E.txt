Hi everyone, I'm Yifan Chen from Fuxi AI Lab, in that is company.
I'm pleased to be here to share you some game testing work with my partner Xiao Li, who is from the QA department.
The title of our presentation is Building Intelligent Game Testing System, in that is MMORPG game.
Okay, now let's begin.
Fuxi AI Lab is the first organization focusing on the research and application of technology in gaming era.
The lab is founded in September 2017.
Our vision is to unite gaming with AI.
We have five research directions including reinforcement learning, user profile, computer vision, virtual helmet, and natural language processing.
Besides, we also build our data and computing platform to support all the AI research and applications.
Game testing is an essential part of game development.
Each game team needs a game test.
Game testers are responsible for the game quality in the development periods.
And after game publish, game tester can avoid operation.
accident. Bugs in game will drastically spoil the game experience and cause economic loss.
For example, Bioshock 2, a great game, it supports several platforms including Windows, PlayStation and Xbox. However, Bioshock 2 suffered from bug issues. And in the year of 2010, 2K Games announced that they will not provide patch to fix these issues and stop to offer the downloadable contents, since they even can't work around these bug issues.
and another license from China, Dota Legend, a popular game developed by Lily's company in Shanghai.
On one holiday, the game officer distributed farewell to all the players at the Euro.
However, this time they made a big mistake.
They planned to give every player 20 sovereigns gold and 50 diamonds, but the amount was reversed.
They gave each player 20 sovereigns diamonds and 50 gold.
the diamonds were very valuable in the game. This mistake caused an economic loss of 5 million yuan.
So, all the cases show that game testing is very important.
However, on the other hand, the game testers often complain with their work. The task is repetitive and tedious, the workload is hard, and the salary is not satisfied, and so on.
Luckily, current AI and motion learning technicals progress rapidly.
All these technicals, such as evolutionary algorithms, reinforcement learning, and deep neural networks may offer new possibilities to build a more intelligent game testing system.
And this is also what we want to share with all of you.
We explored and tested our measures in one MMORPG game, named Justice Online.
It has become one of the most popular games in China since launch from June 2018.
The main gameplanes are game quests and game dungeons.
Game dungeons usually include PvE and PvP.
Most of them are fighting theory.
The testing work of Justice Online is very hard.
The building system will generate three versions in one day.
At each time slot, there are many resources involved, including programmers, game designers, and art designers.
There are more than 400 commits during one slot.
Ideally, all these three versions need to be tested in one day.
and each version includes more than 100 stories and more than 100 and 1600 gameplays.
The workload is so heavily in practice, it is impossible for our QA team to finish it with only 70 members.
So we choose the new launch or recently modified task to test.
However, this may lead the game at risk with its human neighbor.
Currently, we have already achieved some results and experience on the regression test of Quest and the class balancing test.
So today, our presentation will focus on these two types of testing work.
OK, let's begin with the regression testing of Quest.
I will invite my partner, Xiao Li, to share the background of this work.
Hi, my name is Xiao Li, and I'm from QA department of NetEase company.
Yes, I'm one of the QA for Justice Online.
Next, I will talk about the general testing work in our team, and the difficulties encountered for regression testing of quests.
Quest design is the cornerstone of MMORPG game, which help players involved in the game story and familiar with the game world.
When you create a new character, and enter the game, the first thing you encounter is a quest or a mission.
It will teach you how to operate, how to walk, and even how to kill a monster.
Besides, you can learn many interesting stories by finishing them at the same time.
There are mainly three types of plot-oriented quests in Justice Online.
Main story, side story and encounter story.
No matter which type, their main purpose is to tell a vivid story to players.
The implementation of this quest is relatively simple.
And at the same time, The difficulty for players to complete is relatively low.
However, to construct a colorful game world, there are a great number of these quests.
So it's really a big challenge for our QAs to deal with these quests.
First, the number of the quests is large.
We have 290 main stories, 371 side stories, and 475 encounter stories.
Each quest consists of several task steps.
Step means an independent unit, such as a goal in a quest, like talk with NPC, or kill some monsters.
For example, a quest may require players to talk with NPC A, kill master B, and then go to somewhere.
This means that the quest contains three task steps.
All steps are in a certain order.
So the total number of steps in Justice Online is over 15,000.
It takes 200 hours to complete all these quests just one time.
Second, the task steps are modified frequently, especially for the lately designed quests.
Implementations between quests are coupled.
Even different quests may influence each other unexpectedly.
So it's very important to do regression testing.
Regression testing for all quests is time consuming.
And we don't have enough time and labor to do this.
However, thank you for reading, and see you in the next one.
If we don't do enough regression testing, bugs may be hidden in our game.
This is a contradiction.
In addition to manual testing, we also have some automated methods to ensure the quality of the quest.
For example, we write test scripts to run the test cases automatically to check if there are bugs.
Similarly, these scripts need to be written manually.
And because of the frequent iterations, these scripts also need to be maintained manually, which is demanding.
This is an example of how different quests may influence each other unexpectedly.
One day, the game designer modified the boundary of the map for mission A.
Unfortunately, that map was also used in another gameplay of mission B.
Because of the modification.
One of the guide NPC was out of boundary of the map in mission B.
So players couldn't walk to the NPC's position. Therefore, mission B was blocked.
Actually, this bug is very easy to be detected if we do regression testing.
We have a complete set of automated testing process.
After a new build version is packed, a new task is established.
It will deploy the test environments, load the script library, and then execute the test scripts.
The test results will be collected after all cases are done.
At last, it will generate a test report and be sent to everyone in the project.
In the script library, there are not only scripts for quests, but also scripts for gameplays.
Each script covers the process of quest or gameplay and have some checkpoints.
When the script cannot proceed as expected, all the checkpoints fails, we think there may be a bug and we need to check the problem manually.
Most of our scripts are running on the game client.
and they simulate human players operation.
Let's look at an example of how the test script works.
This is a part of test script for quests.
This quest require players to go to the river to fetch water and then come back and pour the water into a bucket.
In our script, We use action sequence to simulate players actions.
Player need to go to the river, press FK to fetch the water, wait for fetching, and then go back to the bucket.
Use atom to pour the water, at last, wait for pouring, like a flowchart.
When we write the test script, Each action is written as shown in the picture.
However, this method also have several disadvantages.
On the one hand, one quest corresponds to one test script.
So totally we need over 1000 test scripts just for quests.
In our current situation, the coverage is very low as shown in the table.
Only some of the quests are covered, especially for side story and encounter stories.
Besides, new quests need new test script.
In one of the past expansion pack updates, we updated nearly 100 new quests.
Completing all scripts is a heavy and continuous work.
On the other hand, scripts are sensitive to iterations.
When designers modify the quest, test script should be updated.
For example, in the past, One task step require players to talk with NPC A.
But now it has changed to NPC B.
And the script, and the test script may not be able to find NPC B and it will be blocked.
This means that maintaining test script is also time consuming.
So we really need a new solution to deal with this simple and plot-oriented quest.
Okay, thanks for Xiao's introduction.
So we need a new solution to deal with this simple and plot-oriented quest.
We know that the old test is essentially code-based or rule-based.
We should write a script for all quests.
And if we can create a virtual test, who can complete the quest by itself, that will be great.
In order to implement such a virtual test, we need to remove the problems of completing quests in games.
We remove the problem from the view of human player, as the left board picture shows.
The human player plays a game by observing the screen state and performing actions through mouse and keyboard.
So first, we define state and actions.
State are abstract of current game information.
It should be distinguishable between different progress of quests.
Actions are what the player can do in the game.
In Justice Online, a state consists of task info, landmarks, items in the bag, enemies, and so on.
All these can be reflected in the screen, and they are important to decide which actions will be performed.
It should be noted that the items in the state get from the internal game engine, not from the screen image, which makes our measures more efficient in practice.
The action defines what the player can do in games.
It is not a simple operation about mouse and cable, but the function interface, such as run to object, which means go to the operation of object, candidate action, including two parts, action type and optional action parameters.
All the actions can be called by our algorithms, and the implementation of these functions is provided by QA team.
OK, our solutions of regression tests can be divided into three phases.
The first phase, learn how to complete the quest through inserting and building quest graphs at the same time.
The second phrase, we directly use the graph to get the best path for quest and replace the action sequence in best path to perform algorithm test.
In third phrase, we speed up the searching for new quest.
Since the problem is modeled at a macro decision process, we can build a graph of the quest by Monte Carlo Tree Search algorithm, as shown in the left figure.
The node A is the start node, and the node D is the terminal node.
Once arrived at node D, it means that the quest is completed.
During the searching period, If the end node is arrived by the algorithm, we can calculate a best path from A to D based on the build graph.
In current version, the best path is the shortest path, which can complete the quest.
minimal steps. In the example as shown in the right figure, the shortest path is from A to B to C and R to D. We assume that if the game code is not changed, we can re-complete the quest following the best path again.
The key of the searching algorithm is to choose the candidate action AI for node S.
Here, we adopt the up-confidence bound strategy in MCTS, which chooses the maximum actions values of all PI.
PI is initialized with equal probabilities and updated according to the UCB equation.
The CI in the equation indicates the chosen time of ActionAI so the probability of ActionAI will decrease if CI increases The advantages of this strategy are twofold First, it can treat exploration and exploitation which is a historic problem in MCTS Second, it can avoid an infinitely loop between two loads the number of all the candidate actions is more than 14.
So searching in the whole action space is time consuming.
In fact, based on the simple rules, we can reduce the action space a lot.
Here is an example.
In the load information, we can see that the flag of the UR open is false.
So the click mouse action is not available.
And if there is an item in the bag of tasks, we should try use item actions.
and the parameter of these actions is the item ID.
Another is problems.
If we found two enemies around the player, we may try to attack them.
All these rules are not complicated, but they show great effect in practice.
And please note that these rules give out all the legal actions or available actions under current law.
But all the legal actions has equal initial probabilities.
In the next slide, we will use NLP technicals to assign the more potential actions with high probabilities among other legal actions.
Okay, this is the workflow of our searching procedures.
We receive the information from the game and extract some key information to create state load and generate action space for each load.
The candidate actions, including action type and its parameters.
And then we choose the actions to perform in the game through functions interface.
And then the procedures is repeated again.
Okay, now for the second phrase, how to use the load graph to perform regression test.
That is simple.
We just need to replay the best path in detail.
When we enter the load A, we choose the action 1.
Then we will enter the load B and C, and finally arrive at D.
If everything goes OK, we will arrive at the terminal node and prove that the request will not be blocked and it passed in the progressive testing.
However, in some cases, the situation is not always as expected.
One typical case is that the game environment is non-deterministic.
You may find that after you do action 1 in node A, you will not enter node B, even into an NC loop.
To solve these problems, we take two measures.
One is to...
is that we will not assign the action in the best path with four probabilities, and we also give other actions some smaller probabilities.
On the other aspect, in an ANC load, the probability of all the legal actions are averaged, and this makes testing work more robust.
Meanwhile, we determined When a quest passed the regression testing, according to the total time, if a quest is not completed in replay mode in 15 minutes, it does not mean that there is a bug, but at least it means that there are some problems, because the quest may also block human player for 15 minutes, which is not what game designers want.
As previously mentioned, we currently adopt the shortest path in the graph to perform the regression test.
It is reasonable but not always valid.
Here is a sketch map of one quest we encountered.
To complete the quest, the playoffs first.
should go to NPC1 and then up down the cliff and then arrived at position of NPC2.
And this is also the shortest path to complete this quest.
However, some players tend to first jump down the cliff.
Unfortunately, they found that they were unable to up the cliff again.
And these situations were not reproduced.
during the regression testing.
So this case shows that the regression test of the shortest path only proves that there is an exit way to complete the quest, but can't guarantee the bug outside the shortest path.
Now the third phrase, speeding up the searching for a new quest.
Although the pre-real method can complete the quest, it was not smart enough because it needs to try every legal actions and waste a lot of time.
But human player will not. Human player will read the tip and the description of the quest first, and then they try the most potential actions.
So, enlightened by these ideas, Canvas speed up the building of testing using tip and description on screen.
The answer is yes, NLP technicals can.
Our method is illustrated in the figure.
First, we can extract the text of game description.
Here is an example in Justice Online.
Even from Army Camp with Qi Shaosheng, The Army Camp is a place and Qi Shao Song is the name of an NPC in the game.
When a human reads this text, they may realize that they should go to the position of Army Camp.
And before this, they may need to find the NPC Qi Shao Song.
We follow the same idea of a human.
First, we need to extract the entities in the text, like the Army Camp.
and Xi Shaoshong.
The two entities are important, and we can retrieve more related information such as positions and IDs from the game data.
Then we look for the word in the text.
Liu Fo.
We know that Liu Fo means to go to somewhere.
So the action run to position and run to the object are the most precious actions.
Although the common knowledge is simple, it is difficult to write rulers.
Here, we need to implement two models.
One is to extract entities.
It is not difficult.
We adopt JBAR, a Charles-Woods segmentation tool to achieve your purpose with custom dictionaries.
The other model is to map sentences or words to potential actions labels.
In session, it is a clarification task in motion learning domain.
It is not a difficult training task, however, it needs trained data.
So, while we can collect labeled dataset to train a classification neural network, fortunately, we already get the dataset when we finish the first phase.
The best path includes text and action-enabled pairs.
After collection data and training, we can feed into text and output the prediction probabilities of all legal actions.
And these NLP models are easy to integrate with the previous algorithms.
The prediction probability is used to initialize the PI of all legal actions, and it means that the legal actions will not average the change.
And the potential actions will have the large change to be selected, so the searching speed is elicited.
The result is as we expected, it works.
We trained a neural network with a dataset including over 2,700 samples and tested it on a validation set with about 618 samples.
The accuracy of entirety extraction models is 98% and the top five recall of classifications models is nearly 88%.
And in the real case, it speeds up the searching of new requests for three times faster.
Here is a video demo, which shows the processing of one regression test.
The boosters players go to the water port and then they use the items and wait for the box and then they go back to the NPC and then use the water in the bucket and the last waiting room is in the box. So at this time the regression task is completed.
Okay, and next, Xiao will report some results in the real application of regression testing in NetEase Game Justice.
Xiao, welcome.
Next, I will talk about our new automated regression testing architecture based on AI.
In the past, Our automated testing architecture only contained one game server and several game clients.
Because most of our scripts are running on the clients.
Now we use two major parts in our new architecture to do our daily testing work.
One is testing environment and the other one is training environment.
Testing environment is used to do daily regression testing.
There are one game server, several game clients, and one algorithm server.
Game clients, game server sends the states to algorithm server as input, which for the actions as output, then send them to the client and the client executed the actions.
This cycle loops until all tested cases are finished.
At last, it will generate a test report.
In this environment, the algorithm server uses the best path for the quest, which has been trained to finish the quest as stable as possible.
Now, we use three computers for clients.
one computer, one game client.
We rarely use other resources except the basic environment for game client and game server.
But when we add a new quest to the test cases, or some quests are modified, we need a training environment.
For these quests, there is no ready-made information.
and the algorithm server is based on training and searching to calculate the best path for these quests.
In our world, training environment is to generate the best path, which is used in the testing environment for regression testing.
When we want to add a new quest in our regression testing, We just need to put it in the training environment.
And after training, add it to the testing environment.
When the quests are modified, the testing environment also has a certain searching function.
When the change is simple, there is a great possibility to pass the test through its own adaption.
If the change is complex and the task step is blocked, it will be put in the training environment to update its best path.
This is an example of our test report.
Report contains test time, version, pass rate, and other related information.
The red circle shows the main test suite of our task regression testing.
In this report, all cases are passed.
This report contains field cases.
The table shows which task and which task step is blocked.
For example, here, there are two steps blocked.
number 7 and number 701.
We can also click the link to get to the log and the screenshots of them to check the problem.
In this page, I will give the result of our work and show some important data.
Our daily regression testing now covers all the main stories.
totally 290 quests and 4030 steps.
In most of time, all the cases are passed normally and accidentally there may be one or two steps blocked.
The step passing rate is over 99.94%.
For each time of regression testing, this method will save about 10 hours compared with manual testing.
We found five real bugs using this method.
Besides, we also use training environment to test a new quest.
We launched a new series of main story in January called Qingtianzhai.
It contains 15 quests and 348 steps.
For the first time of training, five steps were blocked and four of them were caused by bugs.
And during the entire testing process, we totally found six real bugs.
Compared with the old method of automated regression testing, the new method has a lot of advantages, such as saving more time and labor.
As shown in the table, in the past, we need one to two weeks to write a new test script for a new series of quests, and spend half to two days.
to maintain them.
But now we don't need to write a new test script or maintain them.
The only thing we should do is to put them in the training environment.
Our daily regression testing now covers all my stories.
Besides, Side stories are trending now, and they will be added into daily testing as soon as possible.
The step passing rate of the side story now are 96.4%.
So in the future, we expected all quests to be covered.
Moreover, the framework is easy to integrate.
MMORPG game always have some common features.
So this method can also be applied in other games.
In the future, we plan to use the method in other MMORPG games to improve testing efficiency.
The above is the work for regression testing of class.
Thanks.
Okay, let's begin the second part, class balancing testing.
There are six classes in Justice Online at its first launch, and one class named Noni was released four months later.
Difference classes in game are disastrous with its own trait and skill system.
Class balance is crucial for MMORPG games.
Once the balance is broken, many players will choose the predominant classes, and the game will be lack of diversity and interest.
However, to make the class balance is difficult.
Game design mainly relies on human playtesting before lunch or adjusting according to player feedback after lunch.
It's empirical and heterostatic.
We proposed a class-balanced testing framework based on self-played technicals in reinforcement learning domains.
As shown in the slide, we create a one-versus-one arena and set up new memory values for two classes.
And then the two classes are trained with the aim to win the opposite side.
The training results will give some information to the game designers of whether the two classes are balanced.
The process of self-play is to freeze one class and train the other class until the training classes reach a stable win rate of 75%, and then exchange the two classes and repeat the training.
The training curve is very interesting.
The win rate of Class A first increases since it learns at its turn, and then it decreases since it was frozen.
The T1 is the time cost the class A reaches the win rate of 75% in each turn.
And the T2 is the time cost the class B reaches the win rate of 75% at each turn.
The t1 is shorter than t2 in this plot, such that the class A is easy to start for player.
On the other hand, we also get real answers from the final win rate.
The final win rate of class A is higher than B.
It means the numerical values of the skill system is higher.
At last, we will make a report based on this result for game designers and they decide whether to modify the numerical value As we mentioned before, NongYin is a new released class and so how to assure the balance of NongYin with the existed classes We choose three classes to test whether the new class is balanced So we let SuiMeng, ShenXiao and XueHe to compare with NongYin And the result shows that the Nongin can beat all the three classes with the win rate higher than 90%.
And in most cases, Nongin finishes the battle in 20 seconds and remains nearly half of Blah.
Although the Nongin is a DPS class, it is much stronger than expected.
So the designers weakened these classes before release.
Since last time, we just gave some advice of the new class.
After that, we decided to make a comprehensive evaluation of the balance of all classes.
So we tested all the 49 pair sets of all classes in just this.
We listed all the results in these tables.
The intersections of row and column is the win rate and train time ratio.
Win rate difference indicates the combat power of two classes in games, and the train time ratio indicates the degree of easy control for a certain class.
From table, we found that the last column class, Suwen, is very weak in 1v1 combat, and it is in accordance with the fact of the game, because the class is a healer.
It can heal its teammates in team fighting, but it is fragile in one-on-one battle.
OK, to make a summary, we introduced two lower measures to perform regression tests and class balancing tests.
We tested them in the Justice game and got some inspiring results.
And in the future, we will try to improve all these two measures.
For regression testing, our next plan is to extend these measures to more MMORPG games.
And now, we get the shortest action sequence of the regression testing, but it may not be the best.
Because the shortest path is not always the player's choice.
Some players may complete a quest with their own style.
The shortest path may unable to cover these cases.
Another example, if the game tester want to make a full coverage of the testing, the shortest is obviously not the best.
Besides, we still want to improve the robustness of these measures.
For class-balanced testing, currently the cost of training is a lot cheap.
It needs a lot of time to train models.
And also it is just suitable for one versus one combat, and a lot for PVP team balancing.
Lastly, thanks for all the colleagues.
We are from two different departments.
We have different knowledge background.
but we make a happy and fruitful cooperation.
OK, also thanks for your time.
If you have any questions, please email to me.
