Visual effects and the rise of real time.
That's our talk.
If there's one thing that I want you guys to take away from this talk, it's that real time visual effects is expanding beyond games.
It's never been easier to get started, and let's build the future together.
So Keith and I have a whole lot to say on all these topics.
Like I said, we probably have too much to say, so we're gonna burn through it.
Overview of the broad sections we're gonna be covering, why VFX Bootcamp now, that's why we think the timing is perfect for this event for today.
Keith's gonna speak to building our community and ways that we can do that better and bigger.
He's going to talk about visual effects gems from our community.
I'm super excited for this section.
Then I'm going to take it in a little bit more of a narrow focus, talk about Tilt Brush, Case Study, taking old tricks in new places, and then we'll take the 10,000 foot view and try and get a picture on where all this is going.
So why VFX Bootcamp now?
Well, we're at an inflection point.
That's the most important feature of the landscape of visual effects right now.
There's this really dramatic inflection point.
And that's driven by a number of factors.
One of the most significant is this powerful common baseline for tools and engines.
We've got things like Unreal and Unity that we take for granted now, at least I take for granted.
But if you think back a few years ago, actually wasn't the case.
And my work in Tilt Brush in particular, Tilt Brush is built on Unity.
We wouldn't have left our job in games and pursue something different if Unity didn't exist.
So these tools are already changing the landscape and changing the options that are available to us.
Every year at the visual effects roundtable, we do one of these show of hands.
Who's looking for work and who's got an open position?
And every year it's absurdly lopsided.
There's never enough people to fill the open positions.
And if you believe the high-level concept behind this talk, that problem's gonna get a whole lot worse.
There's also a platform explosion.
And we'll be revisiting this again at the end of the talk.
Thanks to Matt Radford for the awesome stylized explosion.
Another important factor to call out, we started this group, Facebook, the Facebook group, Real Time VFX, maybe 2012, I think is when we started it.
And it's blown up.
It's almost 2,000 members.
It's grown really, really, really fast.
Kind of in response to that, my amazing co-presenter, Keith Garett, he took it upon himself to build us RealtimeVFX.com, which is a community, a place online where we can come together and we can share our ideas and we can grow together.
I'm super, super honored to get to share the stage with him.
If you're not familiar with Keith's work, he was the lead VFX artist at Naughty Dog.
He did some notable titles like Uncharted 2, 3, and 4.
He worked on Last of Us.
He's been active in the community since it formed, sharing his experiences and his knowledge with us.
Recently, he co-produced the Game Makers Inside Story.
He did that with Jenny Kong, who's a writer from film, moving into games.
It's a 10-part web series about storytelling in games.
It's free on YouTube right now.
You should definitely check it out.
And he just launched his own studio, Beyond Effects.
It's a visual effects studio in LA.
It has a super intriguing business model that very much relates to everything we're talking about today.
We don't have time to cover it right now, but if you're curious, be sure to ask him about it in the Q&A.
And so now, yeah, it's my honor, my privilege, hand it over to Keith.
Thank you.
Yes, Keith.
Thank you, Keith.
So, I prepared a few notes.
I wanted to go over all the cool things that people have been doing in our community and sharing with us.
And it's a long run.
No.
The truth is that I wanted to take a few moments basically just to express all the cool things that everyone's been sharing with us across all the different platforms that we have.
To introduce the subject a little bit more though, I wanna take a step back and just discuss where we're at with effects.
I think one of the takeaways you guys are gonna see from this week at GDC is that everybody across every little facet of the industry is coming up with freaking insane solutions for random problems.
And yet at the same time.
Our roles are changing along with that.
We're no longer just doing particle simulations, we're also doing illustration simulations.
We're rigging, we're programming, we're compositing, we're building tools, we're animating, we're modeling.
The tasks that we do daily is changing at an overwhelming, if not inspiring and breathtaking rate directly in front of us, and yet we're constantly expected to keep up with it.
Every single company that I've been to is trying to reinvent the wheel over and over and over again to maintain that competitive edge, yet we haven't really been doing that great of a job sharing it.
And that's kind of the reason why it means so much to me that Drew started the VFX Roundtable five years ago, six years ago, right?
We finally had a platform for us to come together and discuss all the pains and woes and awesome solutions that we were actually experiencing.
We could start to plan out the future of where our tools should go, where our hopes and dreams and ambitions as effects artists should be taking us.
At the same time, David Johnson and a few others founded the visual effects group on Facebook, and we finally had a place to share just badass references, cool explosions, the actual solutions and tutorials.
Around the same time, other artists like Jason Kaiser were creating amazing YouTube channels, which to this day is probably the best single resource for learning how to do illustrative effects that I've ever found.
One of two, in fact. And ViewFX is the only other place that I can point students to when they're learning how to do effects.
Unfortunately, Bill Kladis moved on to bigger, better things with Epic, but his resources from ViewFX are still some of the best.
And so, through all that, I spent time sitting here wishing there was a way to Google this.
Wishing that there was a way that I could go online and just go to Google and type in...
UV offsets per particle in Unity, and have it actually return something useful.
Like I wanted a way for effects artists to find resources to the challenges that they were facing.
And at the same time, I feel like businesses should be able to target us as effects artists to run into our community.
Other artists that want to make tutorials should have an audience somewhere that they could go to to find it.
And so we created Realtime VFX.
It's fully Googleable, it's fully archived, and that's freaking awesome.
Cool.
So.
What I really want to be up here talking about is the fact that this is our community.
We made this for us.
We're not trying to drive a profit out of it.
We're trying to enhance the visual effects industry because that's the career that we're all excited about, that we're all here for.
In that same vein...
Any opportunities that you guys see out there, please reach out to us.
We wanna help you make that into something that we can all use.
So for example, Jason Kaiser approached me about making Jobs Board.
And we've launched this.
It's still got some ways to go, but we're trying to make a resource for people, for recruiters to find effects artists, for effects artists to find amazing jobs.
Nathan Lane has been going through and actually building a community challenge.
concept where we're doing monthly effects challenges similar to like the 30 second animation club but for effects for us to step outside of our comfort zone.
In that same vein tonight we're actually going to be announcing our very first ever sponsored community challenge sponsored by Popcorn Effects and SideFX. So at the mixer tonight we'll have all the full details on that but the point of all this is to say that to all of you from us please get off your asses, help us make this better.
Our entire industry is about to explode.
That's going to be the caveat, the summary of what Drew's up here talking about.
And we have such an amazing opportunity in front of us to make our little niche industry into whatever we want.
So we're all here to help.
The other part of what I really want to talk to you guys about is just some cool freaking takeaways that I've been seeing coming out of the community over the last few years.
So everything that I'm going to share today is coming from a tutorial that somebody posted online, a post, even just mentions and answers to questions across the Facebook group, the forum, Twitter, everywhere.
I have a ton of content in here and I'm obviously skimming thousands of posts for other things, so the takeaway here is that these are all really cool innovations that we're sharing as a community and that are becoming adopted as industry standards.
To start us off with that, just to set the scene, if you think back to like 2013, the PlayStation 4 and the Xbox One had just come out.
For us as effects artists, it was a big deal.
I mean, obviously for the entire industry it was a big deal, but for us it meant that we finally had a little bit of wiggle room to start to innovate.
So we had stronger CPUs, we had way stronger GPUs, and we had a lot more texture memory.
Of course, we had absolutely no idea how to use any of it.
So we kind of lost our shits collectively over GPU particles, which, you know, we could spawn just thousands and thousands of particles and make awesome effects, really, really cool stuff.
The other side of that that we also immediately stepped up to is that texture space.
Like we could finally do a lot more flip books.
For those of you guys that know me, I'm pretty vocal about the problems with pre-rendered flip books, particularly if you're new and you're learning.
I don't really need to go into them, but since you guys are my audience and I'm on stage, I'm going to anyway.
So if you're doing a flipbook, you're bound by file size.
As soon as you are limited by a file size.
Uh-oh.
As soon as you're limited by a file size.
It's a lose-lose situation of the resolution that you're giving each individual frame of that file size or your frame rate.
So in this case, I've got a 64-frame simulation that's lasting 3 seconds.
Now, 3 seconds is not long.
You're going to be hard-pressed to find a flipbook that needs to last only 3 seconds.
But with this minimal case, 64 frames across 3 seconds, we're getting 21 frames a second.
It's not bad.
If you're playing in a 60-frame-per-second game, you will see that.
But you can see that each individual frame is fairly low res.
This is a shitty looking explosion, right?
It's super pixelated, it's not great.
So I could bias that, I could go 32 frames and give each frame more resolution.
But then we start to really hit that frame rate.
This is on a 512 texture, so of course, if I didn't care too much about my texture memory, I could go up to 1024, I could go up to 2K, 4K, whatever.
But then I'm burning through that texture memory.
And God help you if you want randomization.
So my general feelings on flipbooks is that we just shouldn't do them unless we're extremely, extremely careful about their pros and cons.
If you can be extremely careful about your pros and cons, I do honestly believe that flipbooks are the single most powerful tool in our toolbox because there's just no other way that we can encapsulate that much motion in detail.
So what I wanna go into is just some of the cool ways people are improving flipbooks.
I'm pretty sure most of you guys have seen this.
This is from Clemon.
And he's showing a technique of using motion vectors to sort of blend between the frames of a flipbook so you end up with super smooth motion.
I know that the team at Gorilla had been using this for a while and I'm sure a lot of other developers in odd pockets had, but Clemens Post I think was the first time that he collectively lit a fire under all of our asses to go and explore this approach.
So what's actually going on here?
If we go back to flow, and I'm going to throw back to my own presentation just because I had these assets in this very timeless GDC 2012 presentation, flow or vector displacement is just a technique where you tell each pixel to move in a certain direction over time.
So in this case I'm painting a texture, here's me applying it, right?
So you can see that I'm stretching that grid in very specific directions.
That allows me to do all kinds of cool and interesting things based off of textures.
But if we go back to this flipbook, and let's slow it down a little bit so we can see it.
What I can do is I can actually interpret vectors that I need to displace between these frames so that they blur together.
So from frame A to B here, the texture that that looks like is on the right.
Obviously this is skimming, but just trying to give you guys the idea.
There's a lot of different ways you can go about generating these quite easily.
Houdini has some. There's a great After Effects plugin called Twixtor.
But right now my absolute favorite is a tool called Slate Editor, created by one of our very own, Alexander Hill.
So here's my little plug for him.
I think the vectors that I've gotten out of his tool are the best I've ever seen.
It's got my approval.
If we apply that output to this when it plays.
Finally hit a negative on this.
I'm going to hop back then just to give you guys.
We'll go back to Clemens' example.
So what the explosion is doing basically is every single frame of the flip book.
It's getting the vectors needed to blend into the next frame and then smoothly interpolating between them.
You can see that obviously the image on the right is way way better than the image on the left, especially slowed down.
So this gives us a lot of flexibility in the way we're presenting our effects.
Forgive me as I hop way forward now.
The benefits of the of of motion vector flip books are kind of huge.
We can actually get away with.
super super slow flip books playing back and not killer are presentation of framerate uh... that in it of itself is a massive solution for folks the negatives however is that it does require for textural cups because you need a to be of the flip book and you also the a and b of the motion vectors for sales motion vectors can be small but there's some negatives associated with a sampling that many textures Of course, if you're looking for more resources, Clemens' tutorial on this is amazing.
He runs you through how to create this shader exactly in Unreal, so I'd highly recommend checking it out.
There's also a great discussion on the Realtime VFX Forum.
My little plug.
If you're Naughty Dog, you can actually do this at real time, which is kind of cool.
So you don't actually need to bake those textures.
You can actually, instead of taking up texture space, you could use GPU processing power to generate those motion vectors at real time and then smoothly have at it, basically.
So OK, that's how we've gotten around frame rate.
What other strengths can we use for flipbooks?
What about lighting?
I think one of the coolest notions that I kind of realized a couple years ago is that textures are actually just a way to transmit data.
If we take the flip book not as an emissive render and instead consider the way that it fits into a compositing pipeline, so a diffuse pass, a normal map, your alpha or density, maybe a temperature map, you know, take those passes that are actually used in compositing pipelines because our shader is nothing more than a real-time compositing pipeline, we can start to do some really, really cool things.
So I want to throw out to a post that the Cloud Imperium guys made.
Actually, Mike Snowden posted it, but it's a video made by Cloud Imperium in general.
And it shows this incredible breakdown of 20 different really freaking rad concepts.
One of them that I want to go over is just how they use data to pass into their lighting pipeline.
So if we just take a flip book here.
Should we try restarting?
the trial talk over this. If we take an explosion texture again if we don't take the render the missive render but instead we actually render out and passes so let's say you render out your diffuse lighting. You render out your temperature and then you render out your alpha all 3 of those are probably not going to need color they're just masks which means you can put them into one texture because you've got your red green blue channels and then you can start to use that in interesting ways momentarily all show you guys how we approach normal mapping.
Brutal.
Well, okay, let's skip forward then.
Imagine this was just an explosion playing that just has the red is the alpha, the green is the temperature, and the blue is diffuse.
what we can start to do as well for normal maps normal mapping of fluid is really hard right uh... you've got all kinds of different challenges the way that i like to do this is actually not it's not a correct normal up in a way it's it's kind of a bubble should render process but it works really really well uh... it takes advantage of the like my pre-rendered tools lighting uh... and light scatter so what i do is i render a uh...
a light from the top and throw it in the green channel, and a light from the right and throw it in the red channel.
And then I do the exact opposite with a secondary texture from the bottom and left.
And then what I can do is just using basic math, which you can't do in After Effects.
You have to go to Nuke or write it yourself, like because you're going with negative numbers, you need to be a little more complex about this.
But the math itself is not hard.
You can blend these.
And then once they're blended, you can just compute the blue channel and you have a normal map.
I'm really hoping this video plays.
Yes, okay.
So this is nothing more than a diffuse render with an alpha and then that normal map created.
And you can see that I'm getting really freaking cool lighting.
The shadows look really nice.
And that's because it's coming out of the render package.
In this case, I'm pretty sure it was Houdini.
So back to this video.
How would I use temperature?
I think a lot of you guys have heard of BlackBodies, or the BlackBody shader in Unreal.
It's basically running on the concept that, please play.
no okay it's basically running on the concept that the input which means single channel uh... is going to derive color value if you think back to science in the kelvin scale like there's with with energy there's a certain color output the black body is just a mathematical operation that when you drive a certain input of kelvin's gives you this beautiful color uh... it gives you really really nice explosions and you get that transition from fire into smoke accurately what we can also do is actually just use ramps So think of this like a lot where if I just take one single channel as an input just a mask of of temperature that can drive my texture look up and give me really really cool highlighted areas on the slope.
So that's just the temperature mask a single channel going into another texture which is a lot in this case.
It's actually the top left texture.
We also probably want to bloom it, which is what the math on the left does.
If we keep going with different lighting techniques, Matt Radford presented some really cool options for ray marching.
I know there's a lot of research going on from a lot of great posts of different ways that you can use ray marching.
I think there's even a really cool talk from Epic this week about fluids in 3D textures with ray marching, so keep an eye on that.
Remarching is a little bit more expensive, but the results are astounding, so I'm excited for us to keep exploring down this path.
One of the other cool concepts that I've seen out of the Dice Guys for Battlefront with regards to lighting was this concept of a gnomon material, gnomon texture, where what they did is they actually took a pre-rendered cloud, or smoke, or whatever it is, and took those renders with really nice light scatter from Houdini, and just baked them all, or compressed them all into a single texture.
And then using these 2 texture inputs just lurk between them at real time based off light direction.
It's kind of hard to argue those results because you can take advantage of the full pre processing and all you're doing is just blending based off of direction.
Let's keep going let's talk about some a vertex animation stuff.
Norman a long time ago posted this really freaking cool.
Like fluid sim basically working on real where he stored all the animation as a texture.
I don't know how many of you guys are planning on doing chocolate bars in your game.
But this concept kind of sparked a lot of different interest and so we have people like Louis working at SideFX figuring out great pipelines for us to compress animations into textures to use on a particle.
In this case, the entire simulation here is a texture that's running.
It's not an animation, it's not joints.
And that means that these can be playing on mesh particles or individual particles.
Here's a really cool fire sim that he did.
Okay, so these are kind of abstract.
Naughty Dog and a lot of other people are actually using them for allowing us to finally do what we've always wanted to do with birds.
Actually have birds flapping their wings that aren't playing just a silly flip book, right?
We can do full animations now.
So how does this work?
Well, if we go to math really quick and we consider just a position on this grid, that's my vertex.
it's not really handsome enough or friendly enough so uh... here's drew uh...
now if drew were a vertex and you're moving around playing as animation we kind of know what the outer bounds of this animation are going to be right so far out the box around this i know the max balance of this animation are ten and that means i can start to do cool math of this so let's say one frame in this uh... at this point it's uh... it's six annex and it's 4 and y.
And the max bounds are 10, so if I divide by 10, then I get 0.6 and 0.4.
If I normalize that.
I got a color value, right?
Like I can actually bake this numerical data into a color and a texture.
That means that I can actually build out an entire texture and play back a texture as an animation.
So in this case, the vertex ID, each individual vertex gets a column on this texture, and time is simply the y-axis of this texture.
So if we just play back this animation, we can do things like goats as particles, or crowd sims.
that don't need to be super detailed.
right like fish birds, everything that we've always wanted to be able to play mesh animation on we can do this way. It comes with one caveat. This is a caveat that I naively ran into and I'm sure most people that explore down this path do and that's that if you're moving verts around at some point you're going to move it far enough that you need to reconsider where the normal is pointing and that complicates things. You probably have to pass another texture for your normals, but just as a general caveat. So what else can we do with these concepts of vertex animations?
Again, the guys at Dice, man, they're coming up with some pretty freaking stellar solutions.
So this is all a vertex animation playing on a single mesh particle.
One.
One single particle.
That particle looks like this.
Or something similar.
It's just a series of quads with every single vert of a full quad having the exact same vertex color.
And that means then that you can do...
there's a term, Euler integrated equations on it.
Like deterministic math basically to say that this quad here moves in this way and this quad over here moves in that way.
You can bake all that out of Houdini and other badass tools and you can come up with some freaking amazing looking effects for really, really cheap because it's all just one mesh particle.
Obviously you're limited on interactions, but as the core of an effect like explosions that have this many sparks probably look pretty good.
All right, let's talk about some trails.
I've seen a lot of really cool improvements in trail technology lately.
One of my co-workers, one of my peers, Jay Hoon, has been working with some really cool spline techniques.
So this is just a standard spline.
And what he wanted to see is, can we art direct this a little bit more?
What would happen if instead of just the basic tangents of your spline, you actually gave yourself artistic control of the entire silhouette of the curve?
And so he wrote this awesome tool that just gives you a profile curve.
that you can go in and shape.
Now in this case, it's geometry, because it was an easier example for me to record, but imagine this on particle trails, and actually being able to break up the silhouettes and do really, I don't know, artistically interesting effects out of trails for once.
Bill Kladis has been doing a ton of really cool stuff with extruding custom profiles along geometry.
These he actually kindly gave me out of his presentation, because he had to rip them, but.
So you can see he's actually extruding a star shape or a human shape.
And ending up with really, really, really cool end results.
So that's my kind of swath of.
interesting new techniques that are coming out of the visual effects community.
I hope the takeaway from all that is just like share with us, go in there and learn from us because there's so many really freaking cool things that are popping up everywhere that the rest of the industry is adopting and progressing together with other companies, other artists.
But the real reason I think this presentation is going to be cool is because I want to throw back to Drew who has Just a phenomenal perspective on the way our industry is changing.
I remember the first time that I used Tilt Brush, where you put on the VR headset, and you pick up the controllers, and you realize that your hands and the space around you is a tool.
And I think I spent about an hour just making something, and I remember feeling really sad when I took it off and realizing it doesn't exist.
But it was such a liberating, like an artistically liberating experience for me.
And so I want to throw this back to Drew to run through just his amazing retrospective on Tilt Brush.
And take it away.
Yeah, awesome.
Man, thank you, Keith.
That's the first time I've seen a lot of that stuff.
That's amazing.
Yeah, it's the first time I've seen a lot of that stuff, and I cannot wait to use it.
I'm actually gonna bring it up in some, like, want to have some wishlist stuff coming in this section.
But yeah, so a beautiful, beautiful, like, high-level view of all the amazing ways that everybody out here has been contributing and making all of our work great.
I wanted to switch the lens, the kind of big picture.
I wanted to focus it down on Tilt Brush as a case study.
So in this section, I wanna run through some more tricks that we use in Tilt Brush and really point out how all of the effects you just saw, all the techniques you just saw, are about to have like a much, much, much bigger audience.
A lot of different fields, a lot of different places that these visual effects can go.
Obviously for me, Tilt Brush is the biggest example of that.
So if you're not familiar with Tilt Brush, it is a virtual reality painting application.
It's built for the HTC Vive.
You wear a VR headset, use controllers, paint in space.
I'll get into what I mean by paint in space.
This is a prototype from our very first version back in 2015.
It's a real fun, from the very beginning, it was a pretty fun project.
This is how it looks now.
I'm going to play a little snippet of the video.
Actually, maybe I'll just play the whole thing.
How many of you have used Tilt Brush before?
Okay, awesome.
And is anyone not, no?
Is anyone unfamiliar with Tilt Brush?
A few people, okay, cool.
So I'll play this.
It should have audio.
I think, there we go, cool.
So.
Takeaway for this video is really just that painting in space, it can be anything.
And starting Tilt Brush, that's really the project we wanted to take upon ourselves.
What happens when you are in a virtual world, you can wave controllers around, and the paint you leave can be absolutely anything.
Now I'm a visual effects by trade, and so for me, I think of what are the visual effects that you can make in the air.
And some of those visual effects are concrete, some of them are amorphous, some of them are particles.
It's a pretty fun playground to explore.
So everything that you saw in that trailer, or not everything, but a lot of it was really inspired by a GDC talk I did back in 2010 called Rock Show VFX, Bringing Brutal Legend to Life.
This was back in, yeah, back in 2010 with Pete Demare.
And back then, we were desperately looking for all of you guys.
We didn't know where the community was.
We were all just inventing wheels over and over again.
It was a really frustrating time.
And so putting this presentation together, I was really fascinated to see how many of those old school tricks apply in VR and apply in Tilt Brush.
And one of my favorites is UV distortion.
So this is a slide from that presentation.
On the left, you have a fire that does not have UV distortion enabled.
And on the right, it is.
If you're not familiar with it, UV distortion is a technique where you take one texture and you use that texture.
You kind of animate that texture, you use it to distort the UV lookup of a second texture.
Super simple technique.
It's like old school bread and butter for us.
And you take it outside of games and it has this whole new relevance.
So, no surprise, in Tilt Brush, our fire is done using UV distortion.
You lay ribbons of mesh down with the controller, and then we apply a relatively simple UV distortion shader on top of it in order to create the illusion of fire.
I'm gonna have this shader, and actually most of the shaders I talk about at the end of this section, so I'll be giving those to you guys.
Another piece I wanna call attention to is the embers that are coming off of that fire.
Particles are super important to Tilt Brush.
Like I said, when you can paint anything in space you want and you're a visual effects artist, a lot of that stuff ends up being particles.
But there's a little bit of a catch, which is that in a creation tool, you don't have a budget. Like in games, you're running around, you're a first-person shooter, you're running around a hallway, whatever, you know, you've got two milliseconds, you've got five milliseconds, you've got some amount of time to work with.
In Tilt Brush, there is no limit.
You simply need to make every element that renders as blazingly fast as possible, because our job is to get out of the way of the artist.
We do not want an artist to ever care about budgets, if we can help it.
So we do that with stateless particles.
And I threw some code up on the screen.
This is a vertex shader.
Your eyes can glaze over, it's fine.
You don't need to pay attention to this.
But what I do want to point out is the alpha channel of the color stream.
So this is the only state that we're passing into our particles in Tilt Brush.
We randomize each particle, we give it a random seed, we send it down the alpha channel, and then we can do anything we want in the vertex shader.
We also make extensive use of curl noise, which is another one of everyone's favorite tricks.
This is also from the GDC presentation.
And that, plus all of our basic.
just a few sines, a few cosines, some other fun stuff, and you can start making really, really beautiful things.
So this is an overview of some of the particles that are on Tilt Brush.
This is snow.
You paint the particles in space, but then we actually raise them up, we fade them in, we drop them down, we drift them back and forth with some sine and cosine, and as they drop, we fade them back out and then warp them back to the top.
So it's the illusion of a simulation, but there is zero simulation.
It's entirely analytic.
This thing just runs forward.
Here we've got stars, which are a random atlas texture to get some different shapes.
And then again, we just are flickering them differently with that random seed.
Embers are the opposite of snow.
So we start the particles at the bottom, we raise them off, we fade them out, and then we warp them back to the bottom again.
Smoke, this is pretty subtle, but you can see a little bit of a curl noise effect in there.
Like I said, it's subtle, so I'm gonna switch to bubbles, our bubble brush, which is a more dramatic example.
Anyone in this room who's used curl noise looks at that and goes, ah, curl noise.
This is not a brush that's in Tilt Brush.
It's just a really fun experiment I did that kind of shows how far you can push stateless particles.
Now, I got super excited with Keith's section because I hadn't yet thought of using a texture to drive my stateless particle animation.
So maybe I'll put some birds and crowds till brush next.
Switching from particles to more traditional geometry, this is a fun breakdown of one of my favorite brushes, which is neon.
This is just this kind of neon tube that you lay down, colored pulse racing down the interior.
I wanna do a quick breakdown on this, especially for those who are a little bit newer to visual effects and who are curious about it.
Again, you get the shader code, so don't worry too much about what's on the left there.
But I do wanna just decompose these elements real quickly.
So you've got a really simple physically-based material.
You've got some specularity, some opacity.
It's a little bit dim on the screen there.
But that's it, and then we're gonna add a animated, use animated UVs to send a ramp down the length of this thing.
Super simple.
All brushes in Tilt Brush are driven by vertex color.
So the color for every brush comes from vertex colors.
So the next thing I'm gonna do is pass in vertex colors so that we see the blue that is the actual color that was picked by the artist.
Spoiler, we joke that on Tilt Brush everything is better with Bloom.
We're gonna brighten that up, really make it nice, nice and neon looking.
But here it looks a little bit, it's a little bit blown out.
It doesn't have the characteristic look of neon that we want, and so we're gonna throw an edge alpha effect on top of this to really tone it down and make it feel more gaseous.
Edge alpha, for those who aren't familiar with it, is you're using the facing direction of the normal to figure out, to basically modulate the look of your effect, in this case of the interior neon tube.
So that's one of our simplest brushes.
On the more complex end of the spectrum, we have our audio reactive brushes.
And again, all this shader code is gonna be available in a second.
So in Tilt Brush, you can play anything on your computer.
You can play any music on your computer, and that music will be analyzed by Tilt Brush, uploaded to a whole bunch of shader constants, and then every brush that we have can use those attributes in different ways.
This was definitely one of the most fun aspects of Tilt Brush to work on over the last year.
And going back to Keith's point about the community, I wanna have a huge shout out to Keijiro Takahashi, who is incredible.
If you're not familiar with his work, you should go to his GitHub repo.
He's got so many amazing examples to learn from.
Reverse engineer his stuff and you will be a better person for it.
In particular, Reaction is a audio analysis library for Unity that we used to prototype the early versions of our audio reactive brushes.
We've built heavily on top of it, but really his work was the inspiration, it was the seed, it was the way we could bootstrap our way into this really unusual place.
So a massive thank you to Keijiro, and again, love this community for things like this.
Now, looking at a simple waveform texture, I just wanted to show off some of our shader attributes that we have access to and describe how we achieve a really simple waveform effect.
Spoiler, it's again just using UV distortion.
So we have a procedural line that's totally flat, but then we're going to offset it using the audio waveform that we've analyzed on the way in.
You can see that audio waveform is the kind of top rainbow texture there.
And it's a rainbow texture because we do a bunch of different processing on the different channels.
And we pass high pass, low pass, a bunch of other different modulations.
We pack them all into this one-dimensional texture and then we use that to distort the UV lookup.
We also have an FFT texture which we can use for fun things.
And all those blue bars on the bottom are the shader constants that we additionally have access to.
Before I go into details on that, Kajiro also has an awesome GPU particle simulation.
We brought it into Tilt Brush just because why not?
It's amazing, of course.
This most definitely is not stateless, and so for that reason, it's probably not gonna ship in Tilt Brush, but it's still beautiful.
Beautiful, beautiful, beautiful.
So I keep talking about how I'm giving you guys these shaders.
I'm giving them to you through the Tilt Brush Toolkit, which is a GitHub repo that we launched about a month ago.
The Tilt Brush Toolkit has all of our shaders, with the exception of particles, those are coming, has all of our shaders, all our brushes.
It's a toolkit to help you take Tilt Brush art and bring them into your own experiences.
And yeah, so everything that I mentioned, you can find in here and reverse engineer it.
In addition, my favorite part is all of the audio analysis stuff is in here, so you can just drop that into your projects and go nuts.
I was kind of our way of trying to get back to the community a lot of this work was done by Fernando Ramos show.
So thanks for Nando for giving back.
And and that's a little bit of my experience on tilt brush pushing pushing visual effects outside of games and seeing how it applies in these new areas.
Now I want to talk about the big picture more generally on top of the rise of real time I want to talk about beyond tilt brush beyond games.
where I see this stuff going.
And I'm really, really excited about this.
Simplest example I have to talk to you guys about is actually my badge, which is over there.
Google?
What?
I've got Google on my badge?
A few years ago, if you said I'd be up here with a badge that said Google, I would have done a double take.
The big companies are recognizing the value of real-time rendering and are seeing the necessity for it And it's going to start tugging on everyone in this room and a lot of people outside of this room If you look around you might spot badges that say Facebook might say Apple and might say Magic Leap There's a lot of eyes that are turning into our humble field And traditional real-time VFX, again, Keith touched on this a bit.
This is where we came from, right?
Consoles, PCs, smartphones more recently.
This is the primordial ooze that we were all birthed from.
But the future looks a lot different.
We're still gonna have incredibly high-end consoles and PCs, those aren't going anywhere.
Smartphones are sticking around, but they're getting more powerful and they're getting ridiculous sensors slapped on top of them, like this is a Tango phone.
So imagine a world where every phone in your pocket has a depth sensor.
It's gonna get real crazy.
Mobile VR is growing up very quickly.
Right now we have mostly.
headsets that only have rotational tracking, but soon they're gonna have positional and rotational tracking for true VR experiences.
Desktop VR, the kind of premium VR experiences, those are going untethered, and so I don't know what that's gonna be like exactly, but I can't wait to try my first untethered desktop VR experience.
Augmented reality is on the horizon too.
It's a little bit hazier.
It's a little bit unclear when that's gonna land, but when it does, that's gonna affect all of us.
Okay, so that's some of the exciting hardware that's coming our way.
But what really matters to the people in this room, and the reason we're here, is what we can build with that hardware.
So now I wanna rapidly run through really where real-time VFX can go beyond games, starting with creation applications.
So no surprise, Tilt Brush is a creation application.
We see tremendous potential in this area.
It's near and dear to my heart.
But I want to pause for a moment and talk about how the visual effects community is already impacting all of these areas.
So Tilt Brush, our team is composed of a couple visual effects artists from games.
But we also have a lot of people from film, from Pixar and from ILM.
Quill is a beautiful, absolute gorgeous application made by Inigo Quiles.
Inigo, I'm sorry, I probably just butchered your last name.
But if you guys don't know who Inigo is, he's a legend.
He pretty much lives and breathes real-time rendering.
He co-created Shader Toy.
If you're not familiar with Shader Toy, you should absolutely check it out.
Another gorgeous application is Medium.
That's created by a visual effects artist from games and an engineer from games.
Those guys have done absolutely incredible work.
And a huge shout out.
I think they were one of the first teams, if not the first team, to recognize the potential for creation experiences on these new platforms.
I want to move along, but other applications that are up here on this page also have roots in VFX.
I think that's so cool.
This is Google Earth.
just showing the potential for the impact of these tools on education, data visualization, science, simulation.
All this stuff is going to kind of slowly pull us away from games.
And you can see that a lot of the problems that need to be solved for something like Google Earth are problems that we've been banging our heads against for a long time.
Theme parks.
This one was a surprise to me, but Disney has a huge investment in VR.
The next time that you get on like Toad's Wild Ride, you might be handed a headset and maybe it'll be VR, maybe it'll be AR, maybe it'll be mixed reality, who knows, but a lot of theme parks are turning their attention to this one.
Mixed reality I mentioned, this is a little bit more subtle, but I'm really, really enthusiastic about what's happening here.
This is Alchemy, shout out to Alchemy for putting this guy together.
Mixed reality is where you take real world elements and you take real time rendered elements and you kind of mix them together and you use real time graphics and real time visual effects to feed back onto the video feed.
This turns out to be one of the best ways of communicating and sharing some of these virtual experiences with people.
Super fun for us.
Definitely.
You guys will love playing with this stuff.
Music videos.
So back in the 80s, music videos were one of the driving forces behind the exploration of video.
And no surprise, in the world of VR, music videos and VR music videos are actually yet another place where we're pushing the boundaries of what's possible.
This is Chocolate, which premiered at Sundance.
It's a film by Tyler Hurd and his company Gentle Man Hands.
Yeah, VR music videos, pushing boundaries.
I'll let the video speak for itself.
Yeah.
Good times.
It's good, it's very good times in the world of visual effects.
Show It To Me is another film, another VR music video that is done by our amazing collaborators, Tipmouse.
That's gonna premiere at South by Southwest.
That's done entirely with the Tilt Brush Toolkit.
So it's all Tilt Brush assets using the Tilt Brush audio reactive code.
Can't wait for this one.
Another great example is how real-time visual effects is impacting film.
So film, traditionally pre-rendered 2D content, but we're even seeing really, really cool ways where our work feeds back into these areas.
On the left, you have Atom, which is a short film made in Unity and rendered entirely in real-time.
On the right, Oculus Story Studio continues to do really groundbreaking work in virtual reality.
They've been pushing the boundaries of Henry and with Dear Angelica, which you should definitely check out Dear Angelica if you haven't.
But this stuff isn't hypothetical.
Like here at GDC on Wednesday, we have real-time rendering for feature film Rogue One, a case study.
So this stuff is happening as we speak.
Notice Natty Hoffman, one of the speakers there, yeah, he wrote the book, Real-Time Rendering.
So people care a lot about this stuff right now.
But my favorite example is Pearl, which is a short film by Google Spotlight Stories.
And it is the first VR Oscar-nominated short.
Play a quick trailer of this.
But what I really want to call your attention to is the fact that this is entirely rendered in real time.
So this is the 2D film that was submitted and was nominated for an Oscar, but it was actually originally built in VR.
So the Oscar is for the 2D version of the film, and they just happened to also have a VR version of the film.
So this really speaks to the power of their real-time rendering engine and just the fact that people are becoming open to our medium in places like the Oscars.
But the best part, the best part of this is that they, this is the director Patrick Osborne, this is him doing the cinematography for that film.
So he actually used a smartphone to drive the virtual camera that then they edited and then cut and then put, you know, and then submitted for the Oscar.
So all those things, what they have in common is really the Next Computing Platform.
That's a buzzword that you guys are probably gonna be hearing a lot, or are already potentially sick of hearing.
But it's the common thread across all of the different experiences I just showed.
It's really driven by the need to influence anything.
It's driven by the need to look anywhere.
You need to be able to have the computer understand where your eyes are, where your head is, where your position is, and you need to update all of those things at a blazingly fast frame rate.
It's going to be bumpy.
This is the new technology hype cycle, which is worth pointing out, everyone just to be aware of.
Like, we've got all these new technologies coming our way, which is really, really, really exciting.
But some of them are going to go through this up and down.
You'll get people really, really excited.
They hit that peak of inflated expectations.
And then you go down in the trough of disillusionment before you crawl back up.
And I don't really have a, I'm not going to argue about where we are on this curve with different technologies.
The point I want to make.
is that for us as visual effects artists, it's smart to target lots of them because that insulates us from the bumps and ups and downs.
And this real-time rendering world is upon us, so we can embrace it, but you might be wary of just doubling down on any single technology.
Okay, so recapping.
The one thing that Keith and I are hoping you take away from this talk is that real-time VFX is expanding beyond games.
It's never been easier to get started.
So let's build the future together.
And thanks.
And yeah, so before we take questions, real fast, I want to take the opportunity to burn through just a few thank yous for the boot camp.
These are the people making it happen.
These are the people to buy beer for.
Christina Wan, Jason Kieser, Keith Garrett.
Buy these guys beer.
They're the co-organizers.
They made it happen.
All of our speakers, thank you guys.
Thank you guys so much.
Megan Boondey coordinated things from the GDC side.
David Johnson did the party that we're going to have Thursday night.
Matt Oztolay, the party that we're having tonight, spacing.
All of the volunteers out here, thanks you guys.
Jeff Hanna, he does a TechArt boot camp.
He paved the way for us.
So say thank you to him.
And of course, buy yourselves a beer for being here.
On that note, if you guys have feedback, please tell us in person, tell us on the forums, fill out your reviews, be candid.
This is the first time we've ever done this, and we have no idea what you guys want.
We're just trying to lay a stake in the ground and say let's come together and let's build stuff, and so next year, who knows what it'll be.
I want to add that it's for everybody.
If you buy this presentation, then this community is going to grow.
It's going to grow tremendously.
And that means that every single person in here is going to influence the culture, which Keith pointed out.
So we want this to be for everyone.
We want it to be inclusive.
We want to take care of each other and just be friendly and build an awesome environment.
On a letter notes, these are our parties.
Tonight at Jillian, 7.30 to 9.30, VFX Mixer on Thursday, 6.00 to 8.00 at St. Regis.
And now we've got a few minutes, so we can take some questions.
Yeah, we did it.
Yeah, we did it.
In spite of the PowerPoint ghosts.
That was 178 slides.
Questions?
I got one.
About the bird and the goat vertex animation, are you still authoring that with like a rig and then exporting it?
Yeah, so the pipeline, in that case, those examples came from Naughty Dog.
Where the pipeline is, they're still animating that goat in their classic 3D package.
And then every single frame, you just compare the difference of every single vertices position versus its original bind post, which is gonna be frame zero.
And you just store that difference as a pixel on that texture.
So the engine renders it in like its old position and then like the vertex tells it to move.
Correct.
That's neat.
Yeah, so the mesh without that texture is gonna be the goat staying in its bind pose.
And then as soon as you start playing the texture, it's saying every single frame offset each vertice, here, here, et cetera.
That's why if you push it too far, you have to start to consider the normals as well.
Yeah, that's cool.
It's a neat trick.
This is for Drew.
Thanks for your work on Tilt Brush.
It's an amazing tool.
Hey, thank you.
I agree.
As someone who's a traditional artist as well, it's fantastic to be able to move off a plane and draw in 3D.
My question is, is there any hope for doing like in-betweening in Tilt Brush, set up keyframes?
Yeah, that's a great question.
So the question is about in-betweening in Tilt Brush.
We haven't done in-betweening, but we have done flipbook animation prototypes, really sequencing prototypes.
And they are incredible.
Like, I cannot believe how much fun it is and how quickly you can create a sequence of three-dimensional drawings that evolve together.
It's super, super fun.
We haven't tried in-betweening yet.
That would be probably the next prototype to try.
But even without the in-betweening, it is just mesmerizing to go frame, frame, frame, frame, frame.
And you can do in five minutes, you can do ridiculous things.
I should caveat that with we have our hands full with our current roadmap.
So that's not coming out anytime soon.
But it is something we're playing with.
Thanks for the question.
Feel free to ask general things about the boot camp, if you guys are curious about how it's unfolding.
If you want to ask Keith about Beyond Effects, which is a pretty awesome example of the success you're having pushing Beyond Games.
Or you can all go get beers right now.
OK, cool.
Wait, we got one?
So since you asked about, I want to ask about Beyond Effects.
So what do you guys do in LA?
The plug.
So for the past two years, so I stepped away from Naughty Dog a couple years ago.
And I was freelancing and just exploring.
And I was realizing that every AAA company, game company, needs help.
Right. I needed help and I thought we could never find enough effects artists.
We could never bring in contractors, even find remote freelance workers.
And then at the same time, I realized her Drew's discussion like every theme park company, every film company, every single entertain.
Revlon has a VR studio.
Do you guys know that?
The Human Genome Project was trying to explore using VR as a way to synthesize and process data so that humans can better represent it by walking around it and solve and find patterns more easily by being in that space.
There's so many opportunities for visual effects artists out there.
there aren't really that many companies that are trying to explore it.
And so what we're trying to do is see if we can find a new business model that allows us to help take advantage of those types of contracts, to work with remote artists around the world, to build secure pipelines so that artists that are willing to moonlight, that are able to moonlight, have an avenue to take on other cool, passionate projects and just sort of inspire and drive more fun creation into our industry.
So we're still early, we've actually opened our doors.
About six weeks ago, but it's all going well and there's just a ton of opportunity in front of us to try to open up the business model of our industry.
Sorry for another question.
I've been doing traditional VFX forever.
I moved over to video games because something I've always wanted to do was procedural visual VFX or effects work so that every time you saw an effect, it was something new and something different that you could look at from any point of view.
Your presentation was fascinating.
Do y'all see like this sort of procedural approach to VFX in?
real time happening anytime soon.
The benefits of doing procedural work are astounding.
Where we're struggling is just that the tools suck.
It requires a very, very technical and a very, very patient artist to learn how to use it.
And so I think one of the biggest things that I'm hoping that the fact that we're having these discussions that all of you guys out there are now thinking because you asked that are now thinking what ways can we build better tools to help with procedural effects?
Like, I think there is gonna be a ripple movement forward where we're gonna start to see artistic tools.
that allow us to create amazing procedural and technical, like highly, highly technical effects using the most artistic of workflows.
That's kind of a dream.
Houdini and in fact, Popcorn Effects are pushing really hard and they both have extremely technical workflows still, but they're trying.
Louis is doing a phenomenal job with the side effects package to try to expose the power of Houdini into a more artistic workflow.
And I think that's only gonna benefit everybody.
So, yes is my answer.
Yeah.
Thanks for the talk guys with the almost standardization of Engines and the amount of resources out there for people to get into effects. Do you see effects moving in the future towards?
As an artist being able to work remotely Much more easier than you are now like say something like a concept or a character artist can work ease more easily remotely Or do you think it will still be so integrated that studios will want in-house?
Preferably that's my dream I think that's all of our dreams and I think that, yeah, I think it benefits the studios tremendously to standardize that workflow as well, right?
Like we can't hire artists because it takes two months for them to learn our tools.
Versus if we had standardized workflows and Andreas here is...
bored for the next three weeks, I can hire him because he can just hop straight in and make freaking amazing artwork and then carry on with his day.
So I think on both sides of that argument, there's a win-win.
And I think that's definitely the path we're all pushing for.
And actually, I think you mentioned this, but also Tyler Hurd, who did the music video, mentioned it.
As these new areas grow up...
There's also an awesome way to dip your toes into it.
Like the chocolate video, that was done, he had help part-time from a visual effects artist in the games industry, but it tends to skirt non-compete because you're not doing games.
And so even if you're a company that traditionally doesn't let you do other games work on the side, some of the, like you're doing a VR music video, that's not very threatening to Blizzard or to Riot, right?
So it's a good way of just kind of like exploring what's out there.
And none of those other ventures have time to build their own tools.
So they're using Vanilla Unreal, they're using Vanilla Unity, and it's just like how fast can you hop in and make cool stuff.
It's very, very liberating.
Would you recommend that artists who don't have that limitation with their studios, they're able to work outside, they should start trying to build some sort of freelance remote work base or be ready for when it moves that direction?
Absolutely, yeah.
I mean, everybody is searching right now for exactly that.
So if you can put yourself out there, you're going to find a lot of really fun projects.
Cool.
Thank you, guys.
Yeah, thanks so much.
I'm just throwing a.
throwing the boot camp schedule up here.
Perfect.
And I think that's it.
Yeah, that's it.
So hey, thank you guys so much for coming, joining us, kicking us off.
And.
