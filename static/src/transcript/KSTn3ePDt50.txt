Oh, there we go. I'm super excited to talk about this, you have no idea.
OK, I'm going to talk about motion matching in the future of games animation today.
But before I get to that.
Let's see. My name is Christian Jean-Diouc. I'm the animation director from Ubisoft Toronto.
As you can see, I've been in the industry since 2000 with various different styles of game under my belt.
Then in 2004, I decided to lean more towards animation-driven games, such as Assassin's Creed, which led me to a couple of studio moves before Ubisoft Toronto tempted me back and gave me the animation director position on Splinter Cell Blacklist, which was shipped in 2013.
I've since been working on various Codev projects and a couple of unannounced titles and some of the cool tech that you'll see in here.
You will also notice if you're keen that Dan and I work together at Bazaar.
Okay, so, quick disclaimer before we get started.
This presentation demos a new animation prototype.
This is tech that the team is very, very passionate about.
And because of the potential applications Ubisoft is keen to invest in and push our games forward.
The tech is being considered for future games, but the assets seen here do not represent any game currently in development.
Is that clear?
Good, alright, okay.
As long as everyone's clear, we've all signed NDAs, we're fine.
Alright, so.
Okay, so I'm going to give you a short overview of what I'm talking about. I'm going to give you a quick explanation of what motion matching is.
Then I'm going to talk about the process of how we work with motion matching.
Then I want to describe quickly ways that we can manipulate the system.
Then I'm going to show you a couple of cool tests that we've tried out, just because we can.
And then I'm going to kind of wrap up with a conclusion, talking about successes, failures, things we want to do for the future, and various different things.
Okay.
Motion matching is a concept that's been around for a while and has been known as a few different things.
It's been known as things like motion graphs or motion fields, but it's only really been this generation of consoles that has made it possible for practical use.
So, to understand motion matching, we really need to first understand what the current standard process is.
So, I'm massively oversimplifying this, but...
Once we know what we want to do, we'll go to MoCap.
We'll capture loads of individual cycles and movements, making sure that we can carefully capture as many actions as we need for our systems.
So then once we get the data back, the animators will painstakingly cut up the clips, tidy up the poses, create the loops and polish the animation, done a ton easier by Dan's tech.
But for years, games animation has been using things like state machines as standard.
With node-based options becoming the option favoured by most projects.
So, if you're here, chances are you know what one of these is, and you know how difficult it can be to use correctly.
So we have to add nodes, we have to set up rules, we have to add blend times, loop points, etc. etc.
And then, if you get all that working, then you should hopefully get something that goes into the game, if you're very, very lucky.
So, of course that's oversimplifying the process, but what if there was another way?
Okay, so the idea of motion matching is really to find the best possible way to go from A to B with as little fuss as possible, allowing animators more time to focus on what the actions actually are instead of having to worry about losing fidelity of their work and manipulation during implementation.
So to highlight one of these issues we wanted to address, here is an example of weight shifting due to a desired change of direction.
So let's see if this works.
There we go.
This seems like a simple enough move, but is incredibly hard to recreate in a state machine and to remain responsive.
So, if we break this move down in slow motion, you can see the moment I decide to change direction, where the entire body will move and will bend, and it'll just, to remove momentum, shift towards that desired direction.
Solving this in a state machine would be difficult, as we would need to likely use a transition animation to replicate this actual move.
But the problem is, is we would lose responsiveness, as I said, and we'd probably lose quite a lot of fidelity.
And it also wouldn't look grounded, because we would probably have to do it with blends, and it'd just look a bit weird.
Anyway, this is kind of the core of what we wanted to fix.
But it would also be a good example of the sort of quality that we were trying to achieve.
So, none of this would be possible unless we assembled a team of animation experts passionate about gameplay animation.
Starting with the animation programmer, Marco Butner, whose idea of motion matching had been in the first place, along with Simon, who's here as well.
We were just looking for the right opportunity to push things further, and this seemed like coming to Ubisoft Toronto was the perfect time.
We then added gameplay programmer Mike Wazalewski and technical art director Alexander Brezniak, who I think is also in the room somewhere.
Then we added Bettina Marquis, and she's the perfect example of the kind of animator you want on a crazy project like this.
And of course, holding it all together is me.
So, with a nice beard there as well.
OK, so we set out a clear mandate of four goals that we wanted to improve upon.
Realism, control, simplify and variety.
So, with many Ubisoft games leaning more towards realism, we wanted to improve core locomotion and allow for biomechanically correct human movement.
Ultimately, this is the feel of the character whilst retaining animation quality and allowing the player to always be in control.
Simplify the way we want to implement data.
This would be allowing animators to focus on animation, creation and rapid iteration, giving them the ability to test or change the style of their character quickly.
So, and also we need to be able to add high quality variety quickly and easily, and this isn't just about replacing a walk cycle, we wanted to look at replacing entire core locomotion.
Excuse me.
So, which brings me to what motion matching actually is, albeit a very, very high level description of it.
So we describe a small amount of characteristics on what we want the character to do over a certain amount of time.
Let's just say one second.
We then take into account things like root position and velocity, the past and present trajectory, joint positions such as feet and hands and their velocity, as well as any tags that we may add.
We take all this and basically find an appropriate matching section.
And it would be like in an unstructured library of poses, effectively meaning we could jump anywhere in a piece of data and it matched our current pose at the time of input.
Okay?
Stay with me.
So...
Okay, which brings me to our first test.
So I'm going to skip forward a few months, and with that I wanted to share our very first playable test that proves that we thought this was something special.
And as Dan said in his presentation, it was quite hard to demo this live, so I've pre-recorded a video for you.
So here's some of the first tests that Michael and I worked on for Motion Fields.
This involved me getting into a motion capture suit, running around our studio, walking, jogging, running, going through various different movements, plant turns, stopping, running around in circles and sneaking.
We felt like this would give us a good, widespread base for everything that we needed.
So this is about as real as a simulation to a human as you can get.
And it's very weird for me sometimes because I can see that this is me.
So, the arrow at the bottom is the player input.
And as you can see, it's relatively responsive with just, I think it was maybe 10 minutes worth of data at the time.
We added the ability to jog.
On the left button, you can see there's a very smooth transition and the camera would very easily move behind you.
And you can see that there would be some quick change of direction in there if it fitted.
So sometimes the plant turns would work, sometimes it wouldn't and that was something we worked on a little bit better.
All this was in one file, so we didn't separate anything out and we felt that if everything was in one file at the time, that we would get a better combination of all the different moves.
So as you can see, if I let go of the left shoulder button, it slows down to a nice walk.
So conversely, there you go, and you can see that I rubbed my nose, which became kind of a running joke on the team.
If I hold the right shoulder button...
Then I quickly move into a sprint.
And the cool thing about this is that it completely recreates the weight shifting from the snaking that we put in there.
Also there was quick changes of direction.
So you could see the quick change from left to right.
And if I wanted to go right...
You could very quickly, if I wanted to double back on myself, I could.
And also, something that we found that was really nice was, if I was running around in a circle, I could take my finger off the button and...
It would actually slow down and decelerate in a circle.
So, and also what I could do is I could start Sprinty again and I could completely let go of everything.
And you see it would come down to a nice quick stop.
This works really well in an open gym obviously with anything out there.
But then it would probably cause more problems if there was going to be any obstacles in there.
But as a first test we felt that this was really successful.
Okay.
So, after seeing this first test it immediately raised a few questions with the team, a few fears and a lot of assumptions.
We understood that no one had tried to ship a game with this before and we would need to lead the way to convince everyone not only was it worth the investment, but that we were worth following.
But as an animator, it absolutely terrified me.
So, what does this mean for animators to work with this? Can animators work with this? How would we implement it?
What if we can't get access to the MoCap facility?
How do we prototype new ideas if it only works in MoCap?
Exactly how much data will we need?
And would producers absolutely hate me because of how much it's going to cost them?
So, with our initial success it appeared that all we'd need to do would be to plug in the data, just leave it at that.
So would animators even be needed anymore?
So, as you can see, this is why I didn't want to tell anyone about it.
Okay, but this brings me to the process. We need to find solutions to some of these problems, so I'm going to break it down.
So first step for us was to create a routine, which we called dance cards. These would be in order so we could capture data and would help us capture more efficiently.
These would have everything we needed to create core locomotion set, and we would repeat for like walk, jog and run.
But it could only really, but it could be really tiring doing everything in one shot, and as one shot could last upwards of four minutes, you would need to remember a lot of moves, you'd need to be extremely fit, which took the actor basically out of character, i.e. me.
So we split up the dance card into smaller files to make them a little bit more manageable.
We broke them into starts and stops.
We broke them into plan and turns, circles, acceleration, deceleration.
And the acceleration, deceleration, we'd have like a longer walk, jog and run cycle.
We'd also have the transitions between all speeds of locomotion.
And the key for me was snaking.
So, let's see.
The goal of these dance cards was to find a way to capture the minimum amount of moves possible needed to create the maximum amount of coverage to create a basic locomotion set.
We would start off with start, forward and stop.
Then we would basically be moving back, we'd go over the left shoulder, we'd go over the right shoulder, we'd go 45 degrees right, we'd go 45 degrees left.
We'd try and do everything we could for that core locomotion set, trying to cover us, basically like scribbling on a piece of paper.
Then we'd do large and small circles to cover different types of banking, longer circles to give more consistent loop coverage for like walk, jog and run, and then snaking to handle smooth changes in direction, which goes back to the thing I was talking about before.
So, once we had all this data back, the files would look a little something like this.
And you've got this example here of circling.
We would then export these FBX files straight into the engine without any touch-up, just to see what happened.
So, you ready?
OK. So, here is the result.
Completely untouched, in-engine, 100% player-controlled and responsive.
Sure, it was an open gym and nothing in the way.
But we just created a better looking and feeling locomotion system in four hours.
That would take the best teams in Ubisoft at least three months. Awesome and terrifying in one video.
So as you can see, there you go.
So we'd have things like the snaking there, and you can see the biomechanically correct human movement.
And there's no IK yet on this, so foot sliding is still minimal at this point.
We get proper weight transfer, acceleration, deceleration, the whole nine yards.
And this was also using the split dance card method.
We couldn't always be sure it would pick the move that we wanted, and the system tended to pick a move that was always the best match for the pose it was at at the current time of input, providing some happy accidents, but equally unpredictable results.
So we made a few changes to the routine and adapted it to work with what we call second-person movement.
We wanted to know how well the system would handle more complicated locomotions such as strafing.
This was much the same dance card as before, except we would face the same direction during the capture as if the camera was always behind us, which resulted in complex foot crossovers and torso twists.
So again, we processed the data, exported it, giving us this.
The player movement was connected directly to the camera, placement controlled by the player. We had eliminated the need for complex foot crossover transitions and added biomechanically correct weight transfer when changing direction on top of what was connected to the previous dance card data to give us a really interesting transition between second person and third person movement. Just let it run for a bit.
So we approve that adding raw data could get really decent results.
But for us moving forward, we'd need to learn how to manipulate this data.
Which brings me to some of the most frequently asked questions by animators about motion matching.
So, how can animators control quality versus responsiveness?
Can this be integrated into our existing system?
And how can animators work with this system?
We wanted to push the boundaries of what was possible, so we decided to try and add different types of locomotion using human as a base, but with varying degrees of success.
So, here we have what we called ApeNav.
We thought it was good enough for Andy Serkis, it's good enough for us.
The interesting thing about this particular type of locomotion is that we actually attempted to motion capture it.
But it was so complicated and the data was so absolutely useless that we used this as an opportunity to deconstruct how the system worked.
So it turns out Andy Serkis does actually know what he's doing.
So we brute force keyframed the dance card, stitching together keyframed clips that would match the dance card format.
Then we exposed the debug settings.
Let's see. Oh, sorry. Here we go.
Then we exposed the debug settings.
So we could see frames and which files that the system was using to adapt accordingly.
It was a bit long-winded, but it was like taking something apart to understand how it works and then putting it back together again, which gave us an invaluable insight.
So that's a bit slow to catch up.
For each move to be read by the system, we would need to add about a second to either side of the move.
That way the trajectories of each move could be accurately seen by the prediction model.
So this is what a start forward and back would look like.
By adding a longer pause at the end of the move, it would highlight that this was a stop, even though we wouldn't see that part of the animation.
So removing this pause highlighted the left and right plant turns.
So, so far nothing too crazy, this is kind of standard stuff.
Then once we knew how the system was being used, basically the basics, we could add more natural, complicated motions.
So we had the key to knowing where to polish, a character.
This also meant that we could start adding all manner of interesting motions depending on how we read the prediction model and fill in the blanks of player control.
So this basically gave us a good insight.
Oh, frame rate's terrible.
This gave us an insight of how the system worked.
Okay.
So another way to control this was by simple tagging of interested areas or waste.
It meant that we could highlight areas we wanted to favour in files such as idles, garbage, or interesting walks depending on our needs.
This would mean that we could get more use out of a single file and we could add extra elements of character.
Like, that's it.
This added an element of predictability to a system that can be at large, largely at times, unpredictable.
To tidy this up, we would need to implement a simple IK solution to remove foot sliding, but the results were a step in the right direction.
I myself don't have many videos in one slide.
Okay, so...
So I owe a huge debt of thanks to Michael Butner for the next two technical slides, so you're going to have to excuse me if I get this wrong.
So, motion shaders were created as a node graph system to allow animators and designers more control over how the moves would be seen.
It was a way to define the cost functions of how well we wanted to match current, future and past trajectories of limbs or root nodes when we were looking for a match.
So essentially we could say that a position was 10% important, then when the velocity was 30% important, and hands or feet would be 80% important.
Meaning that we could change the look and feel of the locomotion as it would use the available poses differently.
This could be in the form of sliders between 0 and 1, and it would be used to say what was more important.
So respecting the positions in the animation, or favor players' input.
This could be created using the same animation pool as a base.
We'd also look to adjust the amount of dampening on input of stick resulting in longer or shorter acceleration or deceleration.
It means we could make something feel like a car if we respected the animation more, or we could make it respond immediately if we respected the input more.
OK, to bridge the gap between this system and existing systems, we added the use of motion matching as a traditional animation node, which can sit inside of a state machine.
So even a motion matching node, which would become active, it would understand what your current pose is and find a relevant match to connect to.
Meaning it would be great for things like transitions in and out of existing systems, as you wouldn't need to set up any complicated rules, and you could just use a motion matching node instead.
And it can also be used as a straight-up replacement for your existing movement system.
We've only really started to scratch the surface of what this is capable of, but we really want to push this a little bit further.
So what else can be done with motion matching?
Using the dance card, we thought it would be fun to try out a few different types of locomotion.
So we spent some time capturing off-balance movements.
When we added this to the system, it gave the appearance of pretty successful off-balance stumble movement, where we would be in complete player control.
So we could be pushing the character around everywhere and it would be completely responsive.
You can imagine this being used for things like persistent injury locomotion, where we could manipulate the inputs to define just how responsive a player was.
So using the dance card, sorry, Michael and I thought that it would be fun to try out something other than locomotion.
So we captured a bunch of impact motions involving being pushed in all sorts of different types of direction with all different types of force.
As we didn't know how well this was going to work, we attempted a few different methods to see what kind of results we might get.
Here you can see some of the team being just a little bit overzealous with me.
I think I might have pissed a couple of people off that day.
Yeah, I got him back at the end, it's fine.
But with the system being pose-based, if we could create an impact pose using a combination of physics, momentum and motion matching, we could use that pose to connect to a sort of impact animation.
This would give us a seamless transition from locomotion to the sort of impact locomotion that could actually be based on physics impulses.
As this would also be a part of motion matching, we used the poses in this range of motion to connect directly back to locomotion.
This would effectively close the loop between animation and physics, and back to animation, whilst retaining complete player control of the character.
So this is something we were kind of proud of, kind of cool.
So as you can see here, the results were pretty successful.
And again, it's something we plan on exploring a lot further.
OK, which brings me to some of our early conclusions.
So remember our initial fears and assumptions.
How will animators work with this?
Well, for me, this is a mentality shift.
This is potentially the biggest transition since the introduction of motion capture to the pipeline in the first place.
Will this only work with motion capture?
Keyframing within the structure, small amounts of data to set up basics and then fill them out from there.
So you can use keyframing with this system, you just have to use a lot of keyframing.
Can you just plug in the data for the finished result?
Well, you can plug in data for a finished result, but originally I felt that this would yield the best results by just throwing all the data at it.
But it seemed to muddy the waters a little bit more.
So I have a little analogy.
Okay, so take Chris Pratt.
When he was in Parks and Recreation, all he wants is burgers.
He clearly loves burgers.
And why not? In-N-Out burgers are particularly delicious.
Um, let's see. Oh, come on. Timing. There we go. Perfect. Okay.
Um, so, he needs to work out. He needs to start eating right.
Then he became nice and lean and starts getting roles in films like Guardians of the Galaxy, Jurassic World.
But what does this have to do with motion matching except for me wanting to show a bunch of pictures of Chris Peck with his top off?
Which is weird. But anyway.
Well, basically we found the system performed better if we fed it the right data as opposed to all the data.
If we were efficient with the type of data that we fed into it, it became a lot more manageable, it would perform a lot better, and would actually look a lot better as a result.
There you go. So, which brings us to some of our successes.
Minimal setup is required. Once initial setup is created, it's possible to just throw entire locomotion at the system and just see what happens.
We get higher quality motion, and we can get to see actual character in the actor, so you can imagine it being used for things like sports games, or if there was someone that had particular nuances.
I mean, you can see from the demo earlier on, and if anyone knows me, I walk like that, it looks like me, I scratch my nose, and yeah, it's weird.
But anyway, by using the idea of dance cards, we were able to easily break down all the moves we needed in the smallest amount of time and space.
And it was super simple to add a wide variety of locomotion styles to the system.
But it wasn't all successes.
There was a steep learning curve to working with motion matching.
So it's only fair that I highlight some of these areas that we feel like we can improve upon.
So, editing data can be tough. The system gets used to about 70% really, really quickly.
But the problem is that last 30%, which is kind of where Dan's tool comes in, which is cool.
It can be tough to manage. We found that sometimes removing the data from a file, even if we felt it wasn't being used, would actually cause more problems.
The system was very data heavy.
We would basically put large amounts of wasted data into the system, but then of course the cost and availability of motion capture facilities.
And currently the system is restricted to human-like rigs.
Everything we had to do with this system was on the same rig, even when we did different types of locomotion.
And as well, all the animation on there was full body, so we didn't use anything like layers or anything, yeah.
It was originally quite difficult for designers and animators to work together using motion matching, as design would alter the data, the needs of precision, and alter the numbers that we used to actually see the poses in the animation.
This would result in really undesired results and would actually break the animation.
For some reason, mirroring didn't work.
We thought that we'd try that, but again, it seemed to muddy the waters more than it seemed to fix it.
So we felt like it needed a clear choice.
And currently, as I said, we've had a few tests.
But our main focus at the moment has been locomotion.
Which brings me to the next steps, at least as we see it.
So far, we've only really used this base for locomotion.
We've touched on a few of the systems, such as traversal, cover, and various AI.
But there's plenty more work for us to do here.
We would also like to make improvements to a hybrid system, more to kind of stop animators from freaking out and to kind of bridge the gap.
But it's to help bridge the gap, and we try and add things like layering, which is also something that we've been looking at.
So it wouldn't be completely reliant on four-body motions, but this is something early stages.
With a greater understanding of what the system is reading, we can make improvements to the dance cards, splitting files up, making them easier to manage and allowing us to get cleaner, more readable data.
If we have looked into improving the routine, sorry, we've also looked at improving the routine, so we get better data from that as well.
And to get more from the data, we need to combine it with other tech being developed at Ubisoft and their engines.
Things like Alex Bresnik's IK rig, which I'll talk about in a second, and of course Dan's automation tool.
So, for more information on some of the really cool animation innovations that are happening at Ubisoft as a group, I suggest you check out these two talks in particular on Wednesday and Thursday. Ubisoft has been really good with letting us talk about stuff like this so early on, and who knows, after today they may not let me do it again. So, get it as you can. But, for us, it's super exciting, and it really is just the beginning. And thank you, that's my talk.
I don't know whether there's any time for questions or anything.
Three minutes. Oh yeah, I got it under.
Okay, anyone have a question?
If you do, please approach the mic that is there and I will do my best to answer.
You mentioned mixing it with state machines.
It's all full body. Can you get it so you can do upper body stuff like shooter stuff?
That is something we could definitely look at.
And it's something that...
I think it would work very well for a system like this, but we haven't done much on that just yet.
I know it's really early on, but do you think this would ever make it to like third party software?
Into what, sorry?
Like third party software where everybody else could use it someday?
I mean, it's certainly, if Ubisoft kind of feels it's successful, then who knows what they might do with that.
But that's not really, it's so early on that I don't think we'd know.
Right.
Thanks, man.
No problem.
Anyone else?
Please approach the mic.
Or say the question and I'll repeat it.
Hey, what's up? How long was the whole R&D process and overall time that it took for you guys from start to the current stage?
Well, one of the programmers that was working on this, he kind of had it as a seed in the back of his mind for a while.
And then when he came to Ubisoft, it was kind of an opportunity for, we gave him some time for some R&D, and then he came up with this idea.
So to get the initial demo that you first saw, we had that working in I think it was about two months, the initial stick figure demo.
And then obviously it was just kind of iteration, getting into various different engines and then trying it in there.
So it was actually quick to get to a point where we got the data playable, but then working out how to work with it was the hard thing after that.
So we had to work out the process and what we were going to do with it to get it into a state that would be shippable.
You said you thought you had problems with abandoning data you thought was useless.
Could you go into what you had happen?
Well, it would be the thing is when you do the dance card, you have things like if you're walking, you'll repeat the same walking action.
like in between each transition because what it's really looking for is those transitions and then it will find, and then when it comes out of one of those transitions it will go into a walk.
So what was happening is like my walk, so unless I change my walk every single time then it would be the same walk. So we've kind of find that if we had eight transitions in there you're going to have eight or nine similar walks that we didn't really need. So when we added the tagging we kind of found ways to not use that which meant that at runtime it wouldn't see that data which meant that it would process a lot easier.
I think that's it. Thank you very much.
