Hi everyone, we are very glad to have you at our speech today.
Our topic is real-time data processing for multiplayer online games.
I'm Lei Xia, a senior data mining engineer from Thunderfire UX team of NetEase Games.
This is my colleague Zhechen Xu, a senior project manager also from Thunderfire.
Before entering today's topic, let's think about such a scenario.
A new game is launched today.
Your boss is eager to know today's in-time new players and the cash flow.
Can you give him this data in real time?
Or a batch of new items have been launched.
Can you let the boss knows which are the 5 best sellers now?
Finally, these new items were copied illegally by some players.
Can you detect it in the first time before the boss goes crazy and yells at you?
Why didn't we find them in time?
To solve these problems, we need a real-time data processing system.
Therefore, we will mainly discuss three questions in today's presentation.
First, how to build a real-time data processing system for online games?
Second, how do we process real-time data in NetEase games?
And third, how to optimize real-time data processing.
Most of you probably have had little knowledge of big data.
So I'd like to introduce some big data tools that are frequently used in practice before digging into today's topic.
Firstly, Kafka. Kafka is a distributed streaming platform which can be used for log transferring and temporary storage. Storm and Flink are systems for real-time data computations. Android is a real-time analytics database. The specific functions of these tools will be explained later.
In addition, there is a commonly used ELK system in order to search and query the logs.
And ELK is actually the abbreviation of the three words Elasticsearch, Logstash, and Kibana.
They are tools of log search, log collection, and visualization.
This picture presented here shows how ELK works.
Logstash collects, passes, and filters the data, and then sends them to Elasticsearch for indexing.
Elasticsearch is used to index, store, and extract your data.
Kibana allows you to visualize your data stored in Elasticsearch, helping with data searching and data analysis.
By far, we have had an overview of the big data tools.
So now, let's begin our first topic.
How to build a real-time data processing system for online games?
Corresponding to the real-time data processing system is the offline data processing system.
So you may ask, what is the difference between these two systems?
A main difference is timeliness.
Offline data processing system is not immediately and usually causes delaying for hours or a day.
as we often call it T plus 1.
While real-time processing system reduces the delay to minute level, second level, or even millisecond level.
Secondly, the processing modes are different.
Offline system uses batch calculation, while real-time system uses micro batch calculation or streaming calculation.
In terms of resource consumption, offline data processing has a higher demand for disk and network I.O. due to the relatively large amount of data processed. Real-time data processing requires higher CPU and memory usage for processing real-time data.
Finally, they each have different representative technology.
For offline data processing, these are Hadoop, HDFS, and MapReduce Hive.
While for real-time data processing, they are Spark, Streaming, Storm, Flink, and so on.
Now, let's look at a typical real-time data processing system as shown in the slide.
The source of it is log collection and transmission. Then transferring and storage stands at the middle of the process, while computation and application is the final step.
The corresponding tools for this step are Logstash, Kafka, and Flink.
This is a brief overview of the data processing pipeline, and we are going to have a more detailed analysis of it.
Players in the game will have a variety of behaviors.
These behaviors will be recorded by the log system on a local game server.
For example, some typical behaviors such as logging in and log out, truncating a character, paying for items, and the use of items, etc.
In addition to server-side behavior, some client-side click behavior can also be recorded.
such as the operation of starting the app.
In order to achieve real-time processing, these logs need to be transmitted in real-time.
We can use log collection tools, such as Logstash or Flume, to transfer logs from local to a stream processing platform like Kafka.
Kafka is a transfer station for the entire real-time data processing system.
It plays a key role in the system.
On the right side of the slide, we can learn that Kafka works as a transfer station for game data of all kinds.
Data are written into Kafka.
from producers, after which various applications start consuming data from Kafka.
Through this, the whole data processing pipeline is built and the real-time data analysis is achieved.
Kafka's advantage lies in its higher throughput and low latency.
It can process millions of messages at the same time, and has local storage and data backup.
So it has a certain degree of fault tolerance.
No failure is allowed.
As a distributed system, Kafka is very easy to scale.
so you can add new nodes without stopping the server.
The last step of the real-time data processing system is computing and application.
Earlier in our presentation, we have introduced the ELK system.
which is a typical log search application.
However, in daily BI applications, we will more often deal with database systems and encounter the following two processing modes.
One is to process the data through the engine like Flink or Storm, and then write into the database system.
The other is to directly transfer the data from Kafka to real-time data analysis database like Druid for online analytical processing, so-called OLAP.
What is Flink?
Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.
For the advantages of Flink, first of all, it has the flexible windows, such as tumbling and sliding window.
This will be explained to you in detail in the following cases with my colleague.
Secondly, there is another important feature, which is the exact ones.
The exactly once means that all data flows over and the data will be processed only once.
This is very important for streaming systems to avoid data loss and repeated data calculation.
Finally, in order to deal with possible service failures we may need recover to specific time or data points.
The checkpoint and save point mechanism provided by Flink can solve such problems very well.
The Flink programming models.
as presented in the slide, can be divided into three modules.
The source module receives various kinds of data source.
The sink module is to output all kinds of storage.
The transformation module in the middle refers to various operations of data transformation, such as map, reduce, filter, and so on.
Now I would introduce you my colleague Zhechen Xu, a senior project manager in Thunderfire for the next part. He will show you a lot of use cases in NetEase games.
Firstly, let's start with a common using case as an example.
how to calculate key metrics such as new accounts, DAU, and revenue.
Following what we have discussed before, the first step is to build a pipeline.
We use LogDash to collect log, Kafka to transfer it, and Flink to calculate and analyze it.
The result of computation is written into MySQL database.
Then we can get a desired result through querying the database.
In order to calculate these indicators in real time, the first thing we have to do is to record some log behaviors.
And to give you an intuitive feeling, we have listed some log examples for log in and log out and pay in.
The very first step is to transfer logs to Kafka.
Here we take logstash as an example.
A standard logstash configurations include input and output.
On the slide, the input is a local file and the output is Kafka.
With such straightforward configuration, we can achieve our goal.
Finally, we use Flink to consume the data in Kafka and write the result in three MySQL data tables, new accounts, DAU, and revenue respectively, based on which we calculate the data we need.
The example hopefully has clearly presented you with the way a real-time system is built up, along with how it serves for our application.
In the next part, I will introduce some more complicated technologies and cases.
Based on Flink programming models, we can achieve real-time ETL very conveniently.
The so-called ETL here refers to a condition that extracts the transform load is in the exact match with the source transform sink in the Flink.
Through sinking the data into HDFS, the creating external tables in Hive, we can create the tables for analysis in a very short time.
The example we gave before is a typical real-time BI application combined with Flink's tumbling time window function.
We can easily know some key elements and monitor some in-game items and currency follows.
For instance, if we want to monitor the total coin acquisition of each account every minute, we can use a Flink programming model to achieve our goal.
First group the data by account, then use tumbling time window and set the time window at 1 minute, and finally, we calculate the sum of the coins.
However, if the time window changes to the latest hour by now, then we should choose sliding time window rather than tumbling time window.
And then we calculate the total number of each account and make a top key.
So far, we have clearly understanding of how Flink's programming models works.
However, we wonder whether we can achieve a real-time data analysis and query without Flink.
Here, we will introduce another tool, DREAD, to give you another way to solve problems.
The data can be transformed directly from Kafka to DREAD, and then query-based on DREAD.
So what is DREAD?
DREAD is a real-time analytics database that can store much larger data volume than MySQL while still maintains very fast query performance.
With DREAD, we can no longer need to write Flink codes over and over again to implement the previous case, but only to use SQL to query after the data is loaded in DREAD.
So analysis process become more flexible.
In this screenshot, if we want to check all payment data in the latest day by far, all we need to do is just to query the database directly, and we don't need to worry about issues such as performance.
For this more complex top-k account problem, here we put the corresponding DREAD query, which is fast and direct.
and has no need for programming ability, it is suitable for data analytics to analyze the data quickly.
Finally, let's look at a more special case.
We need to monitor some special keywords in the game.
Let's call them bug words here.
Firstly, we can monitor the real-time chat data.
Then, with the word segmentation techniques and bug word dictionary we set, we can quickly monitor the bug message in the game.
Thanks for Zerton's wonderful speech. By far, we have solved two problems I mentioned in the very beginning.
how to build a real-time data processing system for online games and a real-time data processing in net-based games. In the section to follow, I will present a more technical problem, how to optimize the current system, how to tackle some detailed technical issues.
An important indicator of distributed services is high availability, which means how to automatically restore services even after some kind of node failure, such as configuring Flink on HL young and setting up the system checkpoint, etc.
Also, Flink can guarantee processing exactly once, but if the upstream data is duplicated, Flink itself cannot avoid dealing with duplicated data.
We need to pay attention to the logical deduplication, which is the so-called idempotent calculation.
In addition, there are some more details.
questions. For example, what if flink's parity zones exceeds the number of partitions in Kafka?
Well, we should use rebalance option. Also, how to ensure the usability if we restart or update flink? Then we should use save point.
The future of real-time data processing system might, from my perspective, focus on the following three points.
Firstly, Real-time OLAP is a very promising direction as it can reduce the complexity of programming.
Secondly, we would continue to improve the performance and efficiency of the real-time processing system.
Last but not the least, we could work on monitoring the real-time processing system so that abnormal alarming is issued in time. Thanks you all for listening our talk.
