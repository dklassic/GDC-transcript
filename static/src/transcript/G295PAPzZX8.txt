I think the official title of the talk is lessons from VR prototyping.
I'm going to try to give you 50 lessons in 30 minutes, so it's going to move a little bit quickly.
The title says 80 plus VR and AR prototypes.
We've done about 90 so far.
We're currently on our 90th.
My name is Rob. I'm a software engineer with the Daydream team over at Google.
We do both VR and AR prototypes. Lots of prototypes.
Usually we work in pairs. We try to move as quickly as possible.
We kind of started doing about a prototype per pair every one to two weeks.
It's getting a little slower now as we're moving into the social space and co-presence is a little bit harder to prototype for.
but we try to move as quickly as possible and learn what we can. This is going to be my attempt to distill our wisdom down into the top 50 lessons that I've seen. Some of these are probably going to be familiar to a lot of you. So I maybe apologize. Hopefully I'm not too elementary in the lessons that I'm giving.
also some of these rules are really flexible and I've seen them broken to great effect. So don't think that these are the ground truths. Not only that but today's kind of best practices may be tomorrow's bad ideas. So we understand this system is evolving quickly. I'm just going to tell you what we think we're working with right now.
and I hope there are at least a few new things in this deck for everyone. So let me get started. My first category is audio. Which might already come as a surprise to you. Why audio?
Because my audio engineer would be really happy to see this listed first. We usually put it last in our VR experiences and our experiences suffer as a result of just like thinking about that second. So, um, so, um, so, um, so, um, so, um, so, um, so, um, so, um, so, um, so, um, so, This brings me to my first point which is great audio is absolutely critical to immersion.
Especially in VR, it just can't be an afterthought. It's absolutely essential to the experience. Number two, spatial audio is more than just dynamic volume. I think a lot of people don't really understand what's at play when we talk about spatial audio or volumetric audio. Oftentimes people think, oh, okay, so if there's a sound source over there, I'm going to play it louder in this ear than in this ear.
That's kind of like the first step to spatialized or localized audio. But the next step is delaying that sound signal so that it arrives at this ear ever so slightly later than at this ear, which is another cue that the brain uses to tell you where that is in the room. Really great spatialized audio is also taking into account whether that thing is in front of you or behind you. Because you know what? Your head and your ears are big sacks of liquid, basically, and they change the way that your ears receive the sound. And if you do it right, if you process those sounds right, you'll be able to distinguish between what's in front of you and what's behind you.
the next step in spatialized audio is taking into account the physical properties of the room to get a little bit of echo or a little bit of dampening. When all these things are done right, the immersion really gets multiplied. Also, it's easy.
Google has an SDK that makes it easy to just drop into unity or drop into your C++ project and it just works. So there's really no excuse not to do it. Just use the spatialized audio plug-in.
Next up, process.
process is really important to me because we're trying to run through these crazy prototypes so quickly. So the first thing that we do, a whole lot of iteration. You know, I mean, I think most people in this room kind of have been in VR long enough to have some idea about what's good in VR and what isn't. But...
you know when I talk to people who are new in the industry their first instinct is roller coasters, racing games, flying games. Now I've seen some of these done well but for the most part these experiences make you really sick and nobody has ever, nobody's first instinct was tilt brush, right? So what you need to do is iterate. And you know tilt brush was kind of a happy accident. Iterate, find out what's interesting.
Incidentally I give a lot of credit to the playful team who spoke yesterday. They were doing five to nine ideas a week which is quite impressive compared to my team's one or two. Next up, test in VR as soon as possible. There are so many things, especially with user interfaces, when you sketch them out on paper they seem like a good idea, you put them in VR and they just don't work. Get that thing into unity as quickly as possible.
Next up scope. Ask yourself what you really need to do to test that hypothesis that you're really after and just do that.
It's so easy to get distracted. Something that I call nerd sniping in reference to an XKCD comic. It's just like where are the types of people who are interested in everything and want these little technical challenges. It's so easy to get distracted in nerd sniping. So just try to scope it as small as possible.
Number six, not every experience is going to be better in VR.
There are just things that are going to suck.
You know, we spend a lot of time taking existing game genres and porting them into VR.
But there are so many things to still discover in VR.
Focus on those new things.
Number seven, don't be constrained by reality. I think our first instinct is to try to recreate the experiences we love in the real world. People like puppies. Let's make a game where you play fetch with puppies. It's VR. You can play fetch with a T-Rex or whatever. Don't feel like you have to be constrained by reality. Think bigger.
Number 8, design from the ground up specifically for VR. I'm going to talk a little bit about this at the very end as well.
The best VR experiences that we're going to see aren't going to be adaptations of existing media forms. They're going to be experiences that really understand the strengths and weaknesses of VR and are going to be built from the ground up.
I'll talk a little bit more about that later.
motion. It's really hard to get away from motion in VR. A lot of new experiences we're trying to avoid motion with teleporting or keeping you in a single physical space. But it's hard to get away from.
So let's talk a little bit about why emotion is such a challenge. When your eyes and your vestibular system disagree, that's when you get sick. This is why roller coasters are such a bad idea. Your eyes are telling you one sense of acceleration, your ears are telling you another sense of acceleration. The time when those things tend to disagree is when you've been poisoned or drunk and so our body responds in the same way as when you've been poisoned or drunk. One of the things you can do is minimize problems either with constant velocity.
or by using blink teleportation. I'm going to dig into this more deeply. Typically when we're designing motion curves for a game experience in 2D or a website, whatever, we apply a constant acceleration.
and this gives us a velocity curve like this. So you have your positive acceleration, you increase your velocity, you have your negative acceleration, you decrease your velocity and it gives you this nice smooth S curve of your position, looks about like this. And the thing to note about this is that your acceleration is always non-zero. So in this kind of situation your eyes are the entire time disagreeing with your inner ear about what your acceleration is.
So instead, what we found in many cases is the best practices for VR.
And again, I've seen these rules broken to great effect.
But generally speaking, what we try to do is apply a constant velocity, which gives you an ugly looking position curve like this.
But here's the key is that it gives you an acceleration curve that looks like this.
You have this instantaneous impulse of positive acceleration, this instantaneous impulse of negative acceleration, and most of the time, your eyes and your inner ear agree about what the acceleration is.
So this tends to work pretty well.
The other thing you can do is blink teleport. So basically just like an instantaneous transportation from one location to another location gives you a position curve like this.
Number 11, just be creative with teleportation preview and other types of motion. There are games out there like budget cuts that let you see where you're going before you teleport. I've seen all kinds of interesting things where fantastic contraption, you pull the dark helmet onto your head and you're in another world. There are all kinds of really cool ways to teleport while maintaining your spatial presence. So, you know, play around with those sorts of things. Yesterday the playful team talked about their success with swing arm locomotion. I've seen the developers of hover, HVR, talk about...
how he addressed motion sickness by keeping the camera rig always rotationally locked in all three axes and these things work pretty well. Experiment. Take a few minutes to spend some time on that crazy idea. The other thing you can do to minimize motion sickness is narrowing the FOV while you're moving.
generally speaking we're not likely to get motion sick when we're watching a television because there's a small region of motion but we have the room around us to anchor us in space.
So some of the experiences you might have seen narrow the field of view and bring in this ring of black around you while you're moving forward and then slowly move that out again when you stop. And the reality is most people don't even notice this or maybe they notice it at first but then they just get used to it and it really doesn't bother them but it allows them to anchor themselves in space.
In Google we're using some applications where rather than bringing in black, we actually bring in a gray grid that's locked to the frame of the room to give you that sense of spatial coherence. Next, when you're teleporting, make sure you're teleporting to a psychologically safe location.
It really sucks when you teleport and you have like a wall right in front of you. Especially in a lot of our experiences where we don't even have textures on these walls, it's just like...
you're like where am I? It can be very disconcerting to sort of like teleport that way.
Next up, my next section is interaction. Players love experiences that allow for creativity.
And, you know, tilt brush is an obvious example here, but we've built some other prototypes at Google that allow very simple ways of just like being creative and expressing yourself. The idea behind this prototype was let's make it as easy to create an animation as it is to play with dolls.
the first thing you do is you grab an item from your inventory and place it in your scene. If you grab the item again, you are now animating it on a timeline. That timeline is simple enough that you can layer a bunch of animations on top of it and create just really, you know, the mechanics are so simple for this application. But I've seen people literally like rolling around on the floor and waving things over their head, making these crazy animations. You know, just let people, give people an opportunity for creative outlets.
number 15, let them throw it. Anybody who has been in VR enough, you probably tried that experience where you grab something and you're delighted that you're moving it and you go to throw it and it just sticks in space. It's just kind of underwhelming. You really want it to behave like it would in the real world.
you know, this brings me to the next point which is think about gestures. One of the things you can do with throwing it is like make that your discard gesture. You throw it off the edge of the cliff and you're done with that item. In terms of other gestures, fantastic contraption, another example where you can use your tool box or you can reach over your shoulder or over your head and pull items out of your inventory that way. Especially when those gestures really make sense for the context of the experience that you're creating.
think about how you can use those gestures to make the user interface simpler or eliminate the user interface altogether. Next up, let sloppy actions have structured results. This will be a little bit related to my next point. Oftentimes there's enough context, let's see if I can get this playing here. No, that's not it. Let's try this. Messing it up again.
Just bad with technology. Ironic, right? All right. So, I'm going to show you a little bit So oftentimes there's enough context in an action and throwing something or picking something up or scooping something up that you can guess algorithmically what the user wants to do. Go ahead and just do that. Figure out what the user's intention is and take advantage of that. Take advantage of that context and make a structured result.
from that otherwise sloppy action. And this brings me to number 18, this sort of feels like you're giving your player super powers. So spawning objects into the virtual world, picking up huge items, pulling in distant items with magic attraction, or just like I was saying before, like throwing something against a wall and having it grow to an enormous size or align itself appropriately. All of these feel like super powers and your users really are going to love that. Controllers, this is a tricky one.
I think we've all had the frustration where we're in Photoshop and we're trying to crop something to exactly 500 by 500 pixels and getting X and Y at exactly 500 by 500 is just a big pain in the butt.
When we're in VR, we're working with six degrees of freedom simultaneously.
And that's just too much to get all those numbers right at exactly the same time.
So I strongly suggest thinking about ways of snapping both in position and also in rotation.
In this example, what you do, you see the ghosted item in your actual position, but it snaps into the closest orientation and position.
Keep interactions in a comfortable space. This is a pretty typical one. Users don't want to have to spend a lot of time reaching to areas that are uncomfortable around their body.
So just keep, when possible, keep the key interactions nearby. Some of the things you can do is scale the user or scale the world as appropriate to make that happen. With the dollhouse application that we were just looking at, you can scale between being big or being small so that you can both experience the dollhouse in an appropriate scale, but then when you're building it, you can also build at the appropriate scale and don't have to walk around a lot.
When it doesn't make sense to scale yourself or to scale the world, think about mapping controls in a non one-to-one manner. This maybe, I don't think this has occurred to a whole lot of people yet, but think about, you know, it sounds strange, like if I reach out, my hand goes way off to the back of the room.
The reality is the motor cortex is very plastic and the brain adapts very quickly to these kinds of ideas. We even played around with experiments where when you're close by everything is one, is scaled one to one, but as you reach beyond a threshold you basically get go go gadget arms and they like extend out to the other side of the room and you can grab things. And this, or they kind of like turn into laser pointers. These are great ways to allow you to access a larger physical space or maybe I guess a larger virtual space without moving beyond that useful physical space.
Next up, use haptics. One of the rules we try to operate by is when visuals, audios and haptics all come together, that's when the feeling is really magical. That's what really completes the feeling of immersion. I know haptics aren't available on every platform, but when they're there, please take advantage of them.
Next up, this category is environments. Simple thing. We love huge spaces. This is another one where you don't need to be limited by reality. You have vaulted ceilings and infinite stairways and towering ruins. These places are so compelling to humanity. But, we have to be careful. We have to with those large spaces, still divide up your world into smaller spaces that fit into your sort of physical tracking space. So if you can divide up your world in such a way that within a typical tracking space for one of your users, you can teleport into that space and do all the interactions that you need within that space, it's going to make it a lot easier for you to teleport around that space. We've even divided up some worlds in interesting ways where...
maybe on, we'll think about our tracking space.
On that corner of the tracking space, if you enter that space, you get into a tramway.
That tramway takes you up to a different corner, or I guess it has to be exactly the same corner in another tracked space.
By laying out these transformations between tracking spaces creatively, you can really make your world feel enormous and your user will never have to leave, they'll never even feel like they need to leave the tracked space.
Next, place objects in a way that keeps the player in the physical space. This is kind of what I was talking about before.
So here's an example one. Let's say you're playing a game of golf. So you hit the golf ball, it's over there, I want to teleport to my golf ball. The typical thought process is, oh, I'll teleport such that that ball is right in front of me.
Now, if you modify that ever so slightly and instead teleport the ball to the center of the room, you might need to take a couple steps to get in front of that ball again, but you're never going to leave your tracking space.
In contrast, if you teleport the ball to right in front of you, you're building up small amounts of error over time and eventually you're going to run into your physical wall.
So just think about that. These are subtle differences. Number 26, texture everything, even if it's just subtle noise. I think we've all had the experience where we have false convergence on a chain link fence or something, a repetitive pattern where you sort of have a hard time figuring out the depth of something.
In VR, when we have untextured surfaces, we almost never encounter these untextured surfaces in real life.
There are always fingerprint smudges or scratches or something.
Give everything a little bit of a subtle noise texture just to allow your eyes to focus.
Number 27, show the physical bounds in the virtual world. We all know when we're using something like Vive, we can see the edges of where the world is. We have that little glowing rectangle. When we get close to the wall, we have our chaperone grid. Don't make the user have to get that close to know where they are. That information is generally exposed by the API. Put something in your virtual world that represents that and nobody will even be tempted to go beyond those bounds.
This is an easy one. Number 28. Everyone loves particles. Just put them in everywhere because they're freaking awesome. I've seen so many experiences that we spend weeks on and somebody will just say, oh, man, I love the rain. It took like ten seconds in unity. Just put particles everywhere. All right. Scale. Scale is such an incredibly powerful tool in VR that I wanted to have a whole section on it.
Number one, accurate scale really matters. This is one of the reasons to start prototyping in VR immediately. Because if that door is 20% shorter than it's supposed to be, you immediately notice. And there are also very powerful things that you can do to use scale.
Scale can convey power or vulnerability.
So, you know, whether you're Godzilla size and marching through the streets of Tokyo or you're tiny and you really feel, you're feeling what it's like to be, for instance, at the side of a blue whale, scale can be used to convey very powerful emotional experiences. And even very subtle changes can have an effect. We found that if we have two, if we're doing a co‑ presence situation where we have two vibes in different rooms and the rooms are calibrated ever so slightly off so that one floor is a little higher than the other floor.
those two players, if they know each other in real life, will immediately notice the height difference in virtual reality. You can use this in really interesting ways, too. You can make everyone the same height, you can deliberately make someone taller, you can make it so that somebody's, their virtual head is a little bit, a little bit on top of where their physical head is, which means everyone looks taller to you.
or you can make everyone look shorter to you.
There are all kinds of interesting things that you can do that shape the social, like kind of the social shape of the world that you're working in.
And then use scale as a tool. This is a little bit of an extension of the dollhouse example that I showed earlier. One of the pieces that we have in this dollhouse is what we call our little chess piece. You can put that into the dollhouse, teleport down and shrink down and see that dollhouse from a doll's size and walk through that. It really just gives you a really powerful feeling of being able to manipulate and then see from the inside.
32, when you change scale, don't forget to change other things around scale. Physics and audio. Physics is a really tricky one. If it's a single player experience and you're teleporting between different sizes, you sort of expect physics to behave, for acceleration to be at 9.8 meters per second squared regardless of what size you're at. So it should be relative to your size.
Now this isn't always possible. If you're in a co-presence scenario and you have one giant player and one tiny player, physics is going to have to be different for those two players. My suggestion would be to anchor physics to seem correct for the smallest player. Because it's a lot less disconcerting to be that giant and see things kind of moving slowly through the air than to be that tiny player and see things moving incredibly quickly. You know, 980 meters per second squared just doesn't feel right.
Audio also, if you have a little tiny player, pitch up their audio. It just feels right to people. It feels more immersive that way.
All right, co‑ presence. This is a tricky one. Number one, players expect something to happen when they touch. Anyone who has experimented with co‑ presence scenarios and created some prototypes in this area, they've seen a lot of these same behaviors. The first thing players do when they get into co‑ presence is they try to touch each other. So something should happen. A shower of sparks, a high five.
But there's also, like, you know, there are some negative consequences that come from this desire to touch each other in VR.
And we need to figure out better ways of dealing with those.
One of those is that we don't want to reward bad behavior.
So this is an example where we have a couple dogs playing poker.
And we, you know, this is, poker, there are very specific rules about how things should work.
And here we have, you know, we have the opponent win.
and the other one kind of walks over to take retribution.
And we have very simple things here.
The world fades to gray, the audio dims down, there's a glowing sphere that tells you where you're supposed to go back to, and you disappear from the world.
Now, none of these are terrible punishments.
It's not like a flashing sign that says, please return to your seat.
It's just enough to say, you're playing this experience wrong, please go back to where you were supposed to be.
And that's really all that it takes.
So just very slight negative influences for antisocial behavior.
On the flip side, of course, do reward pro-social interactions. If you reward pro-social interactions, a shower of sparks when you do a high five but not when you punch the face, people will do the pro-social interactions and not the negative interactions.
Number 36, enforce social boundaries.
So people, and this is, you know, people will do this thinking that it's a very innocent thing.
The first time people go into a co-present scenario, they'll say, I wonder what it feels like when I stand inside your body.
Because you know, this is something you can't do in the physical world.
This can be really uncomfortable.
It is completely innocently intentioned, but it can feel very uncomfortable for the other person.
So we just need to anticipate this.
Especially right now at the beginning of VR, a lot of people are having their very first VR experiences. If a lot of those experiences are bad, we are shooting ourselves in the foot. We need to be the ones responsible for creating a healthy ecosystem. So simple things, you know, make the opponent's body disappear as they enter your personal space bubble. Let people define their own personal space bubbles. Allow someone to ban someone from a room or make them invisible. We really need to think about, you know, innocent behaviors are bad enough.
or can be bad enough, we really need to think about when people are intentionally trying to be negative antisocial behaviors in VR.
Number 37, this is an easy one.
Voice chat really multiplies immersion.
We find that if you have two people in different rooms, other people who are not in the vibe can still continue a conversation with those people until they're connected with each other by voice. At that point, the rest of the world just falls away. Everything else becomes noise. Voice chat really, really increases immersion.
38, put shared objects at the center of the space. We did a couple different versions of a co‑president's music making scenario. In one, all the instruments were on the outside of the room. And what we found is that people would kind of just like do their own thing. Like they could hear their friend behind them doing something else, but they weren't playing together.
Some of this has to do with the field of view that you have in VR.
If that other person isn't within your field of view, you kind of just feel like they're somewhere else.
If you instead put all those instruments at the center of the room so that if you're playing one and your friends are playing another, you're forced to face each other, people start making music together.
It was just a complete difference in behavior.
Number 39, think about asymmetric experiences. This is more of a thought experience. When we get started in VR, there aren't going to be a whole lot of VR devices out there. What we're going to need to do is find a way to involve the people who aren't in the headset in the experience of the other people. So we've done some pretty crazy experiments where we have maybe one person in a daydream, but everybody else can be on their phone and be connected to that experience.
and maybe they're talking by voice. We did one last week where the person in the Daydream headset is a cat trying to brush its teeth but it has no control over its arms. It can only turn right and left. The person on the controller is actually the one controlling the arms with forward kinematics and they have to grab the toothbrush and get it to the mouth. I know this is crazy but these kinds of experiences are going to be really important for bringing people into the fold in VR. All right, next up, avatars.
full body avatars, I mean avatars are tricky. This is the place where I know people are going to disagree with me. That's totally fine. In our experience what we found is that full body avatars, they can work very well if you're looking at another full body avatar. But when you look down at your feet and you see them oriented in a different way than your physical feet are, people find it uncomfortable. And the further down you go on your body, the more uncomfortable that gets. So maybe you can find a good way to simulate the elbows, but when you get down to the legs, There's a good chance you just want to make those disappear when you're looking at your own avatar.
They can be visible to everybody else, that's okay.
41. Eyes are so important. Forward staring non blinking eyes are so creepy. They're really simple things you can do to give those eyes a little bit of a brain. We literally call it the eye brain plug in on our side. These eyes are really simple. They blink randomly. As you start to turn your head, the eyes move forward. They accelerate that motion to anticipate the direction that they're going. It just makes them more lively. There are some other great demos I saw yesterday where people were creating, they were setting up the third party or their NPCs to look, basically what was appropriate to look at, kind of action. So you don't want them to just stare, but you might want them to kind of move their eyes between areas of interest in the space.
Next up, when you pick up an item, that item can become your controller. This is a simple one.
I've seen this done pretty well. Rather than crowding your visual space with both your virtual hands and also the item you're picking up, just make that item that you've picked up your controller and make everything else disappear.
beware of the uncanny valley. We all instinctively know this rule. But the thing is, it's so easy to follow in VR. The reality is when you're in co‑present situations, you don't need a particularly realistic head or hands to convey a ton of emotion. So here you can just see, like, the emotion of these players comes through so clearly. You don't need gesture. It just comes through. Just send a couple of transforms over the network and you're golden.
All right, user interfaces, another really tough area in VR. Simple one, avoid user interfaces that require you to turn your head. A lot of people think, oh, we're in VR, especially when you're first getting into VR. They're like, we basically have infinite screen real estate.
We'll put stuff all around us. So overwhelming. You need to really focus it right up in front of you.
It can be just so overwhelming. Number 45, not too close, not too far. This is kind of a tricky one. It depends on what the scenario is and what your headset is that you're using.
You know, we can think about the way that we render in VR, we think about our eyes as two separate cameras. There are two ways to focus cameras. You have convergence, so where those two lines meet going forward. And then you also have focus. So just like turning the focus knob on a camera.
most VR headsets are designed with a very specific distance at which convergence and focus are the same. So the focus will basically be set at 1.5 to 3 meters, somewhere in that range.
If you're spending a lot of time in particular in the UI, if you can put most of your stuff at that location where your headset is designed for focus and convergence meeting together, you're going to be able to spend the most time in that experience and keep your eyes comfortable.
46, keep the text big. Big is subjective. Right now in this first generation or maybe second generation of VR headsets, we don't have a whole lot of pixel density. Generally speaking, you should sort of assume that you can get a lot less information into your UI than you could if you were designing for a typical mobile phone screen. Think about that as a constraint.
A couple points on smartphone VR. This is sort of like its own little challenge here.
number 47, a well designed elbow model can make interactions feel natural. There are a lot of ways, sometimes we think with our daydream controller coming out soon, you think all we really have is a laser pointer. That laser pointer, first of all you can get a little bit of acceleration information out of there. But also if you think about rather than that laser pointer being attached to your belly button or something, If you assume, let's say your player is right handed or you let them choose their handedness and you take that orientation information and assume that that controller is attached to a rigid surface like an elbow, you can get a pretty good estimate of where that hand might actually be in physical space.
And this is kind of just an example of where we're using those sorts of heuristics that we call an elbow model.
Some of you are probably already familiar with the neck model as well.
Like we generally in VR, we sort of assume that this is not the center of where we're pivoting our head.
It's actually somewhere back here.
We take that into account.
It's the same kind of idea applied to your elbow instead of applied to your neck.
So just take that into account. Number 48, performance matters.
I mean, performance matters for everything. We really need to keep our frame rates 60fps, 90fps, 120fps. But with smartphone VR, we just have a much smaller CPU and GPU budget.
All right. A couple of minutes left. Let's get into the final takeaways. Number 49, check out what others are doing. You don't have to go down all these dead ends yourself. Like, you probably know this, which is why you're here, right? Check out other people's successes.
Check out other people's failures. Talk about what people have done. Share your prototypes. Encourage other people to share their prototypes. We're not in it alone. And I really give credit to Valve for establishing a very much like a culture of openness and transparency from the very beginning in a lot of the design of the hardware and software.
And number 50, finally, you're a pioneer. Think big. This is just the very start. And we have barely scratched the surface of what's going to be done in VR. So don't just iterate on something that you've seen or tried already. I really want to encourage you to have the courage to take a big step in a totally new direction.
And I really believe that the best VR experiences aren't going to be evolutionary offshoots of other forms of media. They're going to be experiences that can really only exist in VR.
Rather than thinking how can we adapt game X for VR, think about what new doors have been opened by this hardware that weren't opened before. And with that, I'll open it up for questions. Thank you.
So I think we have time for maybe one or two questions.
And please use the mic.
Hi.
You talked a little bit about the VR, like the Daydream controller.
Could you give more details about that?
The details are out there online. The basic idea is you're sending orientation. So it has a magnetometer and accelerometer and gyros. So you have access to acceleration and orientation. So it's a 3DOF controller. But just because it's 3DOF doesn't mean you can only get orientation. You can get a lot of data out of that controller. We've even done some experiments where, for instance, by moving forward or backward, we sort of try to guess.
of circular spaces in front of the user and try to guess which one you're in to get a little bit of 6DoFness out of that controller. So we're looking forward to a lot more experiments there. Google has released a thing called the playground. I think that's its external name. Where we basically give 8 to 10 different demos of how you can use that controller for interesting applications. Other questions?
I think we might have one more there.
Okay, so you've made like 80 plus prototypes now.
Are any of those becoming real things?
I sure hope so. I mean, definitely a lot of this knowledge we are putting into our various products across Google. There are lots of little prototypes that we've done where a team will say, hey, we want to test out this particular user interface model or this particular interaction, but it's just like, you know, working on our first party tools with rigorous code check-ins, we can't do it quickly with what we have. Can you try that out and then we'll take those gems that you figured out and put it into our products. But there are some products.
that I'm really excited, for instance, that animator system, currently we're working on making that a multiplayer system, which has a whole new level of complexity that gets added on top of that. Mindshow has already done an amazing job of this. I'm really looking forward to that being out. But I'm hoping that some more of these will become real products.
Me too. Thanks. I think that's all we've got time for. It's about 10.02, but I'll be around if other people want to ask questions after. Thank you.
