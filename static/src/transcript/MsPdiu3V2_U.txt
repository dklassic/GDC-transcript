I'm Ralph Barbagallo here to talk about the reality of developing HoloLens games. How many people here have developed for HoloLens at all? Oh, okay. A bunch. So I think, you know, this is sort of like if you've developed for HoloLens, this might be maybe a little basic. But if you've just started developing for HoloLens or you're investigating it, this is kind of like a brain dump of what I learned developing my first HoloLens game, which is Ether Wars. And Ether Wars is kind of like a...
Oh, nobody can hear me. Oh, this is, apparently we're supposed to use this mic. Alright. Well, I will use this mic. When it doesn't fall over. Okay. So, basically this is a brain dump of what I learned, uh, developing Ether Wars, which is, uh, a real time strategy game. It's a mixed reality real time strategy game. It's kinda like a combination between, you know, StarCraft 2 and, like, playing with toys in your room. Uh, there's a little video of it here, which is, basically, you know, it's using the HoloLens' ability to know the shape of the world you're in and your position in the world. So in this case, you know, these little ships are coming out of the wall. They're attacking this base that's floating around in your room. It's not in this video, but there's actually, like, ground units and stuff. There's tanks and bases that you can build on the floor and stuff. And it's using HoloLens' ability to sort of, like, build a 3D mesh out of your world and play off of the surfaces in your world.
So it's kind of like if you've seen those weird commercials with Arnold Schwarzenegger for mobile strike, but it's really just like a regular mobile game. You tap your hamster wheel type tap finger thing. But in the commercials they show planes blowing up conference room tables and bases and stuff all over the office and tanks driving on your floor. I'm trying to make that the actual game.
which you can kind of do with HoloLens. It's early, but you can kind of get there. So we're going to talk about how you develop this type of game or really any game for HoloLens. Your first step is, of course, getting started. You know you need your equipment.
and so you need a hollow lens obviously. There's two hollow lenses you can buy. They're all the same. One just happens to cost $5,000. The other one costs $3,000. There's no difference between them except the $5,000 one has some firmware stuff that you probably don't need. So you can get the developer edition.
you need to get a Windows 10 PC. You can kind of develop for it on Windows 8 but it's kind of weird because the HoloLens itself is a self-contained Windows 10 PC. You might as well have a Windows 10 development machine. You need Unity 3D 5.5 beta or you can use 5.4 if you use the HoloLens technical preview.
So if you're stuck on 5.4, I'm still on 5.4, I use the HoloLens technical preview. But if you get 5.5 beta, it's all in one build. Basically what this means is if you check VR support in your player settings in Unity, HoloLens will just appear as another SDK you can support. So if you've developed applications for Oculus Rift, for instance, you're halfway there because they kind of treat the HoloLens as any other VR platform. The main camera is your head model. It basically works like that. Of course, HoloGraphic Academy, I recommend everybody go through it. It's Microsoft's tutorials that they put on their website. It covers every major functional block of HoloLens and some of the code in it is really good, too. I've cut a lot of the code out of their examples and put them in my own projects. And speaking of which...
there is Holo Toolkit. This is the next thing you need to look at when you're getting started. Holo Toolkit is ‑‑ well, the Holo Toolkit Unity version that's on GitHub is part of the greater Holo Toolkit SDK. Holo Toolkit itself is sort of the native SDK. If you have your own game engine written in C++ or whatever and you want to have it work on HoloLens, you can use the Holo Toolkit native SDK. And this Holo Toolkit Unity SDK, contains all the prefabs and scripts and build scripts and stuff that you can use to build HoloLens games. It has a lot of common functionality. You can see here input sharing, spatial mapping, all the basics are covered. And it's really good. It's updated constantly. In fact, a lot of the information I give you in this talk is probably going to be obsolete by the time I'm done because there's probably been like 15 polls by the time, you know, while we've been talking. It's super active. It's contributed by the open source community. So Microsoft and the community have been adding a lot to it.
So I always start a project with it and I always keep it updated. I've never had an update break my project. They've been pretty good. So the first thing we'll talk about in building HoloLens application is the interface. This is probably the easiest place to start. It's probably the least sexy part of making a mixed reality application but it's required.
And HoloLens, Microsoft's whole sort of interaction model they use for HoloLens, they say gaze, gesture and voice is basically how you're supposed to interact with a HoloLens application.
Meaning you look around with your head like if you've used any VR application.
and when you see something you want to interact with, you use a gesture, which is this air tap thing, which we'll talk about, or you use a voice command, because Cortana is built into the hollow lens. So that's kind of their interaction model. You can see an example of it here. This is a video taken in my office of the Ether Wars dialogue system. Now my office is a mess. There's a reason for that, because in AR, a messy room is better than a clean room. Gives you a lot of stuff to track. I'm going to use that as an excuse.
But this is a world space canvas floating around in my room.
And you can see a little circle there is your cursor.
And it's pretty simple.
When that cursor looks at that scan button, it'll highlight yellow.
And I select it with an air tap, which is kind of out of the camera range, and it selects it.
Pretty simple stuff.
This is a world space canvas, which everybody is seeing in unity, no big deal. I think I put a collision volume around it so the cursor has something to bounce off of. But bottom line is it's just a standard world space canvas. The unique thing about this interface in HoloLens, which I didn't really show in this video, it floats around in front of your vision. If I walk around, the interface will sort of follow me around. If you turn, it will sort of scoot into your vision and stuff. It keeps it floating around in front of you. The way that is done is use this thing called a tag align.
This is in Holo toolkit.
one of many easy to use scripts and prefabs.
This tag along script or behavior that you add basically does what I was describing.
It keeps the object sort of a certain fixed distance away from you and as you walk around it follows you around and if you turn your head the little thing will scoot and try to get into your vision so you can see it.
You can apply this tag along script to any object.
So, you know, let's say you're making a game and you, I don't know, you have like a little sidekick or familiar or something that floats around near you.
You could use this tag along script for that.
But in the case of an interface, you know, you want it to always face you so that you can access the buttons.
So that's why I have that little billboard script there.
So the interface always faces me.
That's also part of the Hull toolkit.
Once you have your little interface floating around, how do you select stuff in your interface? You have to use a cursor. And once again, it's kind of like making games with Legos these days, once again there's a cursor prefab in the holo toolkit, you can just drag that into your scene. There's a couple different versions of it. I took one of these and I customized it for my own uses, but they're pretty well rounded. You just drop that into your scene and then the way you have that cursor interact with the interface, whoops, let me go back one.
is you use this HoloLens input module.
Now, this is, you know, if you add a world space or you add any canvas to a Unity project.
It will add an event system object which has the standard input module which pipes all the mouse input and stuff to your canvases. So what they've made is this HoloLens input module which basically routes all the cursor input through the interface so that when you're looking at a, like you saw in that video, the scan button gets highlighted because you're looking at it. This is all done through the HoloLens input module. So, so far, to build a HoloLens interface, I've had to write no code. It pretty much works with the existing Unity interface if you just follow these steps.
but once, so now that you were able to look at and highlight the GUI, you have to like do stuff with it. So now we're going to talk about the gesture part. So this is the air tap.
I mean, I don't know, if people have used HoloLens, you basically stick your hand out like this and you sort of tap your finger, like no knuckle. It's like a really hard gesture to do.
Nobody knows how to do it. You know, when I do these demos, this is a picture of one of my demos, I have to spend like 15 minutes showing people how to do it, because nobody really, it's a really weird thing. I'm sure it's the easiest gesture to be able to detect in a self‑contained device, but it's kind of a tricky, unnatural thing. Once you get used to it, it's easy, but for people who never use it, it's a little tricky.
this is the main interaction method for HoloLens. You keep your hands sort of in front of the camera and it's basically a mouse click. The air tap itself is just kind of like a mouse click. Which you would think doesn't give you a lot of interaction possibilities but inside that gesture you have sort of a bunch of different things you can do. So you've got the air tap which is basically a mouse click.
but you have a couple different phases of this tap. So you have the ready position, your fingers pointed up like you're about to click, that's kind of the equivalent of a mouse over, almost like a hovering action. You have the hold, so you can click and like a mouse down event, you can just hold it. And then you have the drag. And so one of the tricky parts about teaching people to use HoloLens is when you immediately put it on, people start poking at the interface with their fingers.
And the thing is, is that HoloLens doesn't know the absolute position of your finger in world space. So you can't actually touch anything with it. But it does know...
the sort of delta between when you held your finger down and where you moved it. You can use that delta as a drag or scroll or even sort of like an air joystick type of thing where you can do a cross in the air and control an object. It's pretty easy to do. There's a thing called the gesture manager. It's another behavior. You just drag into your scene and you tell the gesture manager through the API which gestures you're looking for and then you just assign delegates to the gestures.
Whenever it detects an air tap it will call your delegate. It's that easy.
The final, well, the other input method is voice. Cortana is built into the hall lens. It's a fully self‑contained Windows 10 PC that you wear on your face. You can navigate the whole interface with Cortana just like you can in Windows 10.
But you can also use Cortana's voice recognition capabilities in your own app. So in my particular game, to start the game, you have to say the word play. I do that by using yet another easy‑to‑use script.
in the hollow tool kit, the key word manager. You can see here there's an array of key words and responses. I only have one of them open, four voice commands that I'm using here.
But the key word here is play. So you just type in the word you're looking for and I just drag an event or delegate to the response event here. So it's going to call this on play command when, you know, it's on play.
it will call that play command to start the game. It's that easy. Of course this is an SDK.
You can do this through code. If you want to, like, for localization, you want to switch to different languages and stuff, you can do that. But it's pretty simple. So next we'll talk about spatial mapping.
The interface, that's cool, whatever. If you've made a VR game, you've pretty much probably built the same interface a million times. It's really nothing unique to mixed reality. But here we'll get into the capabilities of the HoloLens that make it a unique platform. So spatial mapping is the ability for the HoloLens to sort of build a 3D mesh out of the world you're in and track your position inside it. And this has a lot of implications for what kind of applications you can design. You can see here, here's an example of it.
it will start making a mesh when I start triggering spatial mapping out of my office. The mesh itself, you know, it's cool. It's not super duper accurate but it's pretty fast how it builds it and it's very useful even though it might be considered a little rough. I might be cheating here a little bit because the HoloLens does cache the scan so if you're in the same room and you turn on the spatial mapping again it will kind of like load the previous scan and then progressively add to it. But it's still pretty fast.
pretty easy to use. If you want to build a 3D mesh out of your environment, all you do is use yet another prefab, the spatial mapping prefab in the toolkit. You can see here that the special mapping observer is the behavior that starts building the mesh. So.
You can govern the resolution of the mesh here, how many triangles per cubic meter and the size of the mesh, in this case one unit is one meter. So this is a 10 by 10 by 10 meter volume. And then the spatial mapping manager script is what governs the properties of the actual mesh that it generates.
So in this case you can give it a physics layer, so if you want to do collision against it you can tell it what layer to use and you can apply material to it. So I'm using the wireframe material here, that's why you can see all the polygon edges.
But in some cases you might want to not draw to the color buffer let's say, but only draw to the depth buffer. So you can use a different kind of material if you want to have, for instance, the real world occlude a virtual object, you can just draw this mesh to the depth buffer and then you can have, for instance, a virtual object go behind your bookcase or behind your door and it will be occluded by it. So as an example of what you can do with a spatial mesh, this is an example of me using it for collision.
So you can see here, and this is also an example of the tap and drag, but this little drone here is going to hit my ceiling and you know, it's just a mesh collider. Boom, it just stops.
It hits my ceiling. If I didn't draw the polygon edges, it would be a pretty convincing match for the real world geometry. When you start using this mesh for occlusion, it's not really accurate enough to really give you a convincing effect of characters walking behind stuff. But we'll get there at some point, you know. So...
There's a lot of great possibilities of making a game that plays over your real world surface when you can build a mesh collider out of your environment. But that's cool, and it's largely cosmetic to a certain degree. You can make some interesting games and applications this way, but what you really want to know is you really want to know more about your environment and more about what is in the room. Because when you're making a game in mixed reality, you know, you want to know, especially if people can play the game in any arbitrary space, you're going to need to know where stuff is. I need to know where there's a chair or where there's a table or different things about my environment. I need to know where's a good empty space to put a game character or something like that. And so you can do that by analyzing the mesh that's generated by the spatial mapping system, but they've actually done a lot of the hard work for you by providing this spatial understanding module. So...
I don't know if anybody's seen, there's a game called Fragments, which, there's like one point in the game, it's a HoloLens game, of course, this character just like walks into your room and sits on your couch and starts talking to you, and it's like the craziest thing I've ever seen.
And, it turns out that the developer that built that game, I think it's called Asobo, I don't know how to pronounce the name, but they built this spatial understanding system, which basically creates a new kind of mesh, which...
I guess generates a special mesh and turns it into a voxel volume and turns it into what I call surf holes and I'm sure it's made up for marketing purposes or whatever. But it's a thing. And using this new kind of data structure, they can analyze the space and you can query it to figure out where stuff is. So to sort of make that a little bit more example of that. So here is a real simple case. I'm doing a simple trace through the voxel volume and it's telling me this is a wall. I did this with the spatial mesh anyway. You check the normal of what comes back and you can pretty much determine you've got a wall. And that's cool. But you can see here it's like here is a wall, here is a floor. But now look at this. I'm going to say, hey, let me find a chair. So I tap the chair and this little blue thing flies down and there it goes. I found my chair.
which seems to me, to the untrained eye, seems kind of magical. But if you look at how they well, I mean, internally it's kind of a black box. I don't think the source code to this is actually provided. I could be wrong. I haven't looked at the native SDK too much.
But the way you query this system is sort of you have to kind of like create a heuristic. You kind of have to say hey, I'm looking for something where the back is twice as tall as the bottom part and the bottom part faces up and that's what I'm going to call a chair and I'm going to search your data for that chair. So you can define objects in all kinds of different ways and then it will go try to find the closest matches to them and bring back all the results. You can also more details here. You can also say I'm looking for a safe space. I'm looking for an empty space this big so I can fit my game character and I need it close to a wall. You can give it all these kind of queries and it will come back and return to you data that tells you where to put stuff if there are places or where these objects are if there are matches.
So, yeah, it's pretty incredible. It's not super duper performance. These are the type of things you need to do before you play the game. Even with spatial mapping. I basically scan first and then play. But you can cache all the locations and stuff and use them later.
So the next thing we need to talk about is persistence.
So in a regular game, you're just kind of like, I can drop, in Skyrim, I can drop my sword at the XYZ coordinate, you know, one, two, three, and then I load up Skyrim next week and it's still there.
You know, big deal, right?
Games all just use a coordinate system.
But in the case of the real world, what is the game coordinate system of the real world?
you know, when you turn on your HoloLens, you're at 000 of your game world, wherever you're standing. Or, you know, when you, let's say, when you start scanning a mesh of a new room that you've never been in before, you're now at the origin of your own coordinate system. So you have to be able to place objects in the real world and have them stay there. So what is the coordinate system of this table? And how do you tell another HoloLens where this object is in the real world when they both have completely different coordinate systems internally? Well, the key to that is the spatial anchor system.
So what they do with this is, it's kind of weird, it's like, you can create coordinate systems inside the real world.
Basically, you add the spatial anchor behavior to a script, and what ends up happening is it anchors that game object to like a chunk of the room scan.
and then the room scan itself gets refined over time. So you might see those spatial anchors sort of like slightly move as you keep playing the game. But basically you can attach objects to that parent object that has a spatial anchor attached to it and now that object is anchored to this chunk of the room scan in the real world. When you turn on the device again you can use the spatial anchor store to load and save these things.
it will load up the spatial anchors and try to find that chunk of the scan again and place that object there again.
So you're basically sort of saving a little chunk of the real world with your virtual object and then it's going to place it there again. Also, this is how you do multiplayer experiences. What they call shared holograms. So it comes in handy because For instance, there's this JPL experience where it's like a Mars experience thing where you all can walk around this room and sort of explore Mars together. Everybody is in a room wearing a hololens and they're all seeing a quote unquote hologram in the same position. Every hololens can see the same shared hologram.
And the way they do that is there's a sharing service that comes with the HoloToolkit, which is kind of like a regular game server written in C++.
And what that does is it just sends those spatial anchors over a network, because it's just a chunk of data.
So, if I spawn an object at this location, then I create a spatial anchor there and I can send that spatial anchor over the network and the game server will send it to all the attached hall lenses and all the other hall lenses will be looking through their room scan and be like I know that, I see that chunk of table, I can match that and they put the spatial anchor there. So that's how you can have a multiplayer experience where everybody shares.
the same hologram. In theory. I've done this a bunch of times and sometimes it works, sometimes it doesn't. I think sometimes it depends on if the room is scannable and trackable enough or whatever. I've seen the spatial anchors appear two feet off from where they are on the other device. It doesn't work all the time. But, yeah, essentially it's just sharing data over a network. It's not a big deal.
Finally, building and deploying on the device. You build on the device, you used to have to build a visual studio project, which you kind of still have to do behind the scenes, then build inside visual studio and copy that to your HoloLens. They now have a script in the Holo toolkit that does that with one button. But once you get on the device, you're going to come into, you're going to encounter the limitations of the hardware.
which there are several. FOV is the one everybody talks about.
It displays an object sort of like a postcard sized augmentation in front of your vision. But that means you have to redesign your application a little bit. If you have no peripheral vision, a game where someone has to walk around and look for stuff is going to be problematic. You might want to have floating arrows that guide the user where to look. In fact, In holo tool kit there are some prefabs that do just that.
They're like guidance tag alongs. They kind of point at stuff and tell the user, hey, you should look over here. Also you're going to run into battery life, CPU, GPU limitations. If you've developed for gear VR you're probably in a good position because it's a mobile device. Microsoft recommends you know, like under 100,000 polygons per scene, which is a lot of polygons when you're not drawing a background. So it's pretty good. The performance is pretty good. Battery life, you know, you might get about two hours of constant use on one charge, which is also pretty good. And also there's control constraints. So, you know, using air tap can be kind of constricting. If you find that to be a problem, you can use an Xbox One S controller with it. So...
I'll leave you with this. It's called mixed reality for a reason. When you're planning your HoloLens content, I see a lot of games in the app store that are VR games that float in front of your face which is a cool parlor trick but it doesn't do anything you can't do in VR or any other type of platform like that.
With mixed reality you can use the floors, you can use the ceilings, you can make persistent objects appear in reality. It makes it kind of a pain to debug because instead of just moving your game character to a level and seeing if it crashes you have to walk to someplace in the real world and test it. It makes it a little bit more annoying to develop that way. But that's the unique property of this platform. Mixed reality I think is going to be as disruptive to software as the app was 10 years ago. We're just getting started.
So when you start developing for HoloLens you really need to use those.
those unique abilities to really understand your environment to make a compelling experience. So I think we have some time for Q&A, but if you want to contact me later, you can contact me on my website, on Twitter, whatever. I'm pretty accessible. I'm on LinkedIn. Any way you want to contact me, you can figure it out. But, yeah, if anybody has any questions, feel free. Or not.
Okay. There's a microphone. Sure. So the shared mixed holograms. Shared mixed realities? Yeah, whatever. So then with the holo tool kit it's done through a PC server, is that correct?
Yeah, which is kind of annoying. You can rewrite it. I haven't done it yet but you can use UNet because it's just sending data.
They just don't have an example of it. Because it's weird, the hall toolkit sharing example is a complete game server that does everything UNet does. It seems like they did a lot of work that they didn't need to do. That's lobbies and message passing and stuff. But yeah, you can just use UNet because it's just sending data. Sure. What were the major challenges for you and what's next based on the learnings that you've had?
debugging, like I said, having to walk around my office to debug is a real pain. It just makes the development time a lot longer. The build test debug loop is so much longer than VR.
And also you don't know if it's going to work. I have to go give a demo somewhere. I don't know if it's going to work in their office. My game needs to use the ceiling. I've gone to have meetings. Oh, my God, there's no ceiling in this room. It's too far up.
So you really have to test in a lot of different physical environments.
So it's almost not a programming challenge, it's more like a practical challenge of how to debug something that can be played in any arbitrary space, basically.
I would say that's the biggest challenge.
Hi. Back to shared experiences. Would you say, you showed an example of more the educational aspect. Is it a little too early since the anchors are way off to go about with doing a game? Or would you recommend waiting until maybe?
I never recommend waiting for any reason. I always like to do something first. Sure, go ahead. I'm planning it with my game. I just haven't gotten to that point yet. I'm going to design the game around the anchors. Originally this game I was working on for project tango. That one has a single coordinate space for all the devices when they all sync up. You can create objects and they theoretically would appear where they're supposed to. This one is a little different because you have to have them with the spatial anchors. You have to be a little different. What you can do is you can do things like I'm going to launch this ship from here to here. I'm going to spawn it at the spatial anchor but release it from the spatial anchor and it goes through normal space and attach it to the spatial anchor when it arrives. You have to play with it a little bit. Whether the spatial anchors are accurate enough is a whole different issue. That's just going to be a factor of testing and making sure you play in the right space. At some point with mixed reality you have to tell the player you need to turn on some lights or whatever. They're not going to have a trackable space.
So this is regarding the shared user space.
Do you get a registered player position for all the players who are there?
Let's suppose there are 10 people who are on the Mars.
So they actually know where they're located?
Yeah, in the example, I think it's the final example in Holo Academy.
They show that where they actually have, on the other players, they put a little avatar head over the other Holo lenses.
So I think that base, it's been a while since I looked at it.
I think that whenever a player I think joins the session, they create a spatial anchor of themselves and then they kind of update their position through the server. And it's kind of accurate. It works a lot better than I thought it would work. So it kind of works.
Rolf, talking about waiting, we are, I'm on the investor side of things, we have a fund that specializes on augmented and virtual reality.
One of the challenges that we're finding is to convince people, LPs or people that are willing to put money on this kind of future technologies.
that we're not like 10 or 15 years away from this. They are very, not skeptical because they are excited about what is going on, but they think that these kind of things are like in 10 years. I know that HoloLens is not very public about timing and triangle. In my opinion it has been slower than other, than should.
But what are your thoughts in, one, monetization opportunities for these kind of opportunities, specifically around games, but in all the industries? And number two, any thoughts or recommendations about, you know, educate these people? Well, I mean, I just don't try to waste time educating investors because it's pointless. I mean, like...
I agree. I mean honestly do you want to spend your life having meetings with these people? I don't. I'd rather hang out with developers who are smart and know how to do things and we can make stuff and I'll leave going to lunch with the investors. It's somebody else's job.
I don't have any answers to that question because I think it's impossible. It actually might be ten years. Whatever this is might be ten years away. As far as a consumer device, maybe. But right now two thirds of my business is enterprise. I've had a lot of interest in warehouse logistics.
construction, building information, modeling, visualization.
It's, but we're still, I mean, I kind of feel weird telling people that, yeah, I can deploy this now, because there's still a lot of variables with the device.
I think right now you can build the application, but you might not be able to launch it in a real battle-tested enterprise environment at this moment.
But you might be able to do that next year.
So enterprise maybe, three-ish years away or something. Consumer, I don't know. I do think at some point every device on the planet is going to have some sort of dual camera slam solution in it and people will be like, of course it's obvious. But when that point is, I don't really know. I just think that you know, but what has happened to me is I've had clients, real giant clients come up to me, huge shipping companies, we want to use this technology now because we have an actual problem to solve and we think you can do it with this tech and we're willing to suffer through all of the beta problems of small flv and stuff like that to get a job done. So I think that's where the money is now. It's not a huge market but there are a bunch of big clients that really need problems to be solved and they can only solve them with this tech. So I guess that's, yeah.
That's probably what I would say about that. Sure. All right. I'll be around. So, you know.
