Hello everyone. The title of my talk is Applying Reinforcement Learning to Develop Game AI in NetEase Games.
My name is Tangjie Li. I am the senior engineer from Fuxi AI Lab in NetEase Games.
First of all, I will introduce Fuxi AI Lab.
Fuxi AI Lab was established in September 2017.
It is the first game AI research lab in China.
and our vision is enlighten games with AI.
Combined with NetEase's powerful game development capabilities, Fushi AI Lab is committed to use AI technology to create a better game experience.
Today, my talk will introduce some of our work on using reinforcement learning to develop game AI.
So, what is reinforcement learning?
Reinforcement learning is a kind of machine learning to solve MDP problems.
Its process looks like the left picture.
The dog is the AI we want to train.
So how to train a dog?
When we want to train a dog to learn the command down, we will give it some food if it does down action.
From the dog's perspective, when it hears down, it will get some positive reward.
else it will get nothing. The formal representation is shown on the right picture. The agent interacts with the environment to receive some state from the environment and does some action according to the state and then it will receive some reward. The objective of reinforcement learning algorithms is to get the optimal policy to maximize the cumulative rewards.
As we know, there are many algorithms and concepts in reinforcement learning.
So it's very difficult for game developers to learn and use it.
To solve this problem, we developed a reinforcement learning framework, which we call RLEs.
RLEs is a framework to make RL easy.
It integrates common reinforcement learning algorithms.
reduces the difficulty of RL applications and simplifies the game access process.
It has many advantages, such as it contains common RL echo themes, it supports multi-agent, hierarchical RL, self-play, it has high performance, supports behavioral diversity, ERO or true skill evaluation. RLEs is a general reinforcement learning framework.
that provides standard interfaces.
And we only need to implement these interfaces or we want to add a new reinforcement learning for everything in the framework.
There are some advantages.
First, the framework is distributed and high performance.
So the researchers don't need to care about the engineering work.
Second.
All algorithms are use features provided by the framework, such as self-play, AIO, or true skill evaluation.
And the third, it is easy to test new algorithms with net-ease games, which has been integrated in the framework.
Someone may know ILEAP. ILEAP is a well-known open-source reinforcement learning framework that supports a variety of higher algorithms with high performance.
and the Node support, multi-agent, self-play, behavior diversity, or ELO evaluation, which are very important for game development.
Compared with IRE, IRE not only maintains high performance, but also makes many optimizations for game development, and makes it easy to lend reinforcement learning to real games.
As the table shows below, we can see that our yeast has faster speed in the same hardware and can get similar results on a tiny environment.
Today, I will introduce some of our achievements in developing game AI using reinforcement learning.
In the past few years, we have applied reinforcement learning to a number of games.
And today, I will focus two of them.
The first one is Justice Online.
Justice Online is an MMORPG developed by Netis Games.
It's a very successful game in China since it launched on June 29, 2018.
This is a promotional video for Justice Online.
Thanks for watching!
This is another video showing the Liu Pai Jing Wu gameplay of Justice Online, which we will introduce and the two classes are both long in.
Okay, in this gameplay, human players challenge hierarchy players who may not be online, so ask AI to substitute for him.
This is a 1v1 scene, and the game designers want us to meet these demands.
First, the highest level AI should be strong enough.
Second, we should provide a different levels AI for different players.
And third, the AI should have behavior diversity.
Of course, there are traditional rule-based AI in Justice Online.
But it's very hard to write a high-level AI with multiple behavioral styles.
So we try reinforcement learning to solve the problem and use the Impala algorithm in our ease.
However, there are many difficulties in this gameplay.
The first one is the AI's potential action space is very huge.
Human player can choose 5 weapon skills from 8 and 5 class skills from 12 to use.
There are about 40,000 combinations, and AI should cover all skill sets.
The second, the gameplay's map is very big.
The attacker and defender are both on either side of the map, and the distance of them is about 67 meters.
The agent can only move one meter at one step.
It's about 10 steps per second.
So it's difficult for AI to explore the environment.
Third, we should provide different levels of AI, including high level.
Especially, the highest level AI should defeat professional or semi-professional players.
Fourth, the AI should have behavioral diversity.
to enrich players' experience.
Finally, the AI should cover cross-class battle.
That means that the model can be used against different classes.
When facing with a real game, if we want to use reinforcement learning to develop a game AI, the first important thing is to build a MDP model.
In response to the difficulties mentioned just now, we have proposed some solutions in terms of modeling.
The first one is how to deal with the huge action space.
We use a zero-one vector state to express equipped skills.
For example, the vector of the picture on the right side, I show on the below.
And then, equipped skills are random initialized, proportional to human data.
As we know, state, action, and reward are three important elements in reinforcement learning.
The state space of this gameplay contains position, status, equippable skills, legal actions, and other important information.
And the action space contains move, jump, and cast skills.
And the reward contains the action space.
The reward has two components.
And the first one is the final result, to mean win or lose.
It's essential, but it's very sparse and only can be received at the end of the game.
As we know, this is very difficult.
It's very difficult for reinforcement learning algorithm.
If the problems, reward is very sparse.
So we introduce a dense reward.
which we call HPDiff between game frames.
It's dense, effective, and make RL easy to learn.
There are many combinations of actions.
So we use two ways to improve exploration efficiency.
The first one is use legal action as a mask layer like the right-hand side network.
The second, we introduce an action via reward.
which is a negative reward when the agent uses an illegal or unavailable action.
These two methods accelerate training and improve the success rate of cut skills.
Another problem is the representation of special states.
In this gameplay, there may be more than one combat unit, such as battle pets, illusions.
Besides, There are some creation skills, such as traps, terrain creations, or air swords.
The common solution to solve the problem is to use the image at the import state.
Of course, it's a good academic solution.
However, if we use the image at the import state, we shouldn't need.
add convolution or ResNet technology, which will need a lot of GPU and computations.
That will make game AI development very expensive. In this case, we don't use image representation.
Instead, we use the LiDAR representation like the red picture. So we can use small neural network and drop convolution layers.
that makes faster convergence and less computing resources.
Now we have the model of this game play, but how to train the AI?
The first idea is to train with the role-based AI.
We found that by training with the role-based AI built in Justice Online, the agent got a 100% win rate very quickly.
However, we found that if we use this AI combined with a human player, the agent got confused and acted bad.
The reason is that this agent has never seen other opponents in the training process except the robust AI.
So we use self-play.
Self-play is a good way to solve the problem of overfitting to a particular opponent.
But a new problem arises. It's difficult to make sure whether training is converging when using self-play. So to solve the problem, RIS provides a module that supports self-play automated testing. RIS can evaluate with fixed models in the training history.
and compare with these models and plot the result on the TensorBoard just like the red picture shows, we can easily recognize that the ability of our agent is increasing.
When we use reinforcement learning to develop game AI, we encountered more problems. First, the big map and original fixed ball positions will cost a long time to learn drawing near.
and poor model generalization.
Second, too many timeout samples at the early stage lead to low training efficiency and increased training time.
Third, chasing and fleeing is more difficult to learn than fight because of sparse rewards.
Fourth, due to the large action space and the settings of random equipped skills, some common combos become hard to learn.
To solve the problem.
we use curricular learning.
The first curricular learning is to learn chess and three.
The method is we set agent birth distance.
The first, we set from near to far.
First, learn to fight, and then chess and three.
Second, we will randomly set birth positions.
to improve the model generalization.
Another curriculum learning is to learn equipped skills, which start from fixed to random, which accelerate learning process by starting from simple tasks.
The third curriculum learning is fine-tune models.
Fine-tune models with more samples of common skill sets to achieve better combat performance, such as skill combos, dodged enemies.
high damage skills, and remove negative effects.
There are seven classes in Justice Online, and we need to cover all class pairs.
If we use one model for one pair, we need to train 14 two models, which will take a long time to train.
So we use cross-class training to solve the problem. That means that one model for one class.
So we only need to train 7 models.
The details are shown on this page.
Each team randomly selects one out of 7 classes to run a game.
Although we can train a high-level AI, but an AI with a fixed behavior style tends to bore players and make players easy to find a way to cope.
So we need to generate AI with different behavior styles.
Self-play using multiple seeds with different reward shaping is a way to solve the problem.
The detail I show on the right picture.
We start a train.
We start a train with n groups.
Each group has its own seed and a reward shaping function.
During training, they put models into the same model buffer and select openers from it.
Using this method, we found that each group can train a high-level AI with different behavioral styles. This page shows three behavioral styles of Shenxiang, including aggressive, balanced, and defensive.
As we know, people have different levels, and game designers don't need an AI to defeat all the players.
So we should provide different levels of AI.
The method is that we first train the highest level AI, and then use different difficulty parameters to imitate human actions.
We use three dimensions to control AI's level. The first is agile, which means the frequency of cut skills.
The second is accuracy, which means the action, random action rate. The third one is perception, which means that there is some noise in the input states.
Now we have three dimensions, but how to compute them?
the parameters of different levels of AI, we use an EIO rating system.
At the first step, generate all AI players with different parameters.
The second, match opponents for each player according to the rank.
The third, compute EIO changes according to the combined results.
Fourth, update the leaderboard. If not run finish, go step one.
Finally, output the leaderboard after reaching the max round.
A table on the right shows we can see that with the increase of the difficulty control parameters, the ability of the AI decreases continuously.
In addition to previous solutions mentioned just now, we met a lot of problems when we use.
when we use reinforcement learning to develop game AI, the most common problem is that the poor model performance, including no convergent, low level, unusual behavior.
We summarize some debug methods and the list here.
First, check the correctness of the game interface.
Second, check the correctness of the training code.
Third, check whether there are missing states.
Fourth, build a simple task for faster validation.
This video shows the reinforcement learning AI combats against a top human player.
As you can see from the video, our AI has learned a lot of tricks.
And by properly using custom skills and moving, the AI defeated the human player.
and the red one is the AI.
Since we deployed the AI service online, the peak TPS exceeds 8,000, and the daily request about 16 million.
It can be seen from the table that, compared with the road-based AI, the high-level RAI has significantly improved in terms of win-win and battle duration.
In addition to these indicators, the AI, the new AI has generated a lot of discussion in player forum, and most players are interested in the new AI.
Some players said, oh, I thought it was a real person, not an AI.
And someone said, the Su-1 AI is completely reasonable.
It suggested that human players should learn from the AI.
In addition to developer game AI, we found that reinforcement learning can do more things, such as balance tests.
Before launching Longin Class, we used reinforcement learning to test the class balance.
Game designers made several iterations of skills and values according to the test result.
On the first few pages, I introduce our achievements in single-agent MMORPG environment.
And now I will introduce our work on the multi-agent sports game, FIWARE Basketball.
The gameplay is a 3D scene.
And the designers want us to develop a human-like AI.
Of course, they need different levels AI for different players.
The biggest difference between Liu Pai Jing and FIWARE Basketball is that.
It's a marting agent problem.
We use two different methods for offense and defense.
For offense, we use a strategic model.
Why use a strategic model?
Some primitive actions are important for team offense, such as passports and pick and roll.
Introduce a coach role to perform team actions in order to improve team rewards.
which simplifies learning advanced strategies, such as pick and roll, reduce the action space of the original model, since some actions can only be executed by the coach, which is controlled by the strategic model, and then avoid passing the buck among teammates.
Just as the red gift shows, the AIs pass the buck when the offensive time is up.
when there is no coach.
This page shows the structure of the original model and the strategic model.
On each tick, the coach outputs the strategic action.
Then the strategic action will be sent to the corresponding player if the action is not known.
Finally, each player executes the strategic action if he receives it from the coach.
and else execute the action from the original model.
We found that defense and offense are a little different.
When you play on the defense team, the most important thing is to decide who to defend.
However, it's difficult to design rewards to achieve defense or coordination.
Besides, it's difficult for a model to learn coordination behaviors.
by primitive defense actions.
To solve the problem, we use a hierarchical multi-agent reinforcement learning for defense.
There are two level policies on defense.
The high-level policies are learned by multi-agent algorithms, such as ComNet, QMIS.
And the low-level policies are learned by single-agent algorithms.
The result of our experiments on the right picture show that the policy learned by the Hierarchical Multi-Agent Reinforcement Learning are significantly better than that learned by the Common Reinforcement Learning algorithms.
This page introduced the details of the Hierarchical Multi-Agent Reinforcement Learning for Defense.
High-level policy decides who and how to defend, which we call the goal.
A low-level policy outputs primitive action.
An agent's low-level observation only contains the information about itself and the target offensive player who is chosen by the high-level policy.
Each goal is executed for up to 15 steps.
High-level policy gets a sparse reward of minus one or plus one at the end of the episode.
easy to shape rewards for low-level policy with a goal.
The left video shows the one match that the RL AI combats with human players.
Like Justice Online, the RL AI also generates a lot of discussions, and I selected a few of them and put them on the right.
Besides the Justice Online and the Fiber Basketball, the reinforcement learning technology has been applied to a lot of games.
This page shows the AI we trained in other four games.
Our reinforcement learning team also published many high quality academic papers based on the result of game AI development.
In the future, we'll do more work.
We may explore multi-agent scenes in MMORPGs.
We want to make human-like AI in FPS games.
We want to explore model-based algorithms in card games and board games, such as Alpha-Beta Search, MCTS.
Also, we want to make smarter AI for auto-test in games.
As we know, this is very important for game development.
I will appreciate my colleagues.
Thank you.
