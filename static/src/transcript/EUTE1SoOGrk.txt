Hi, my name is Mario Palmero.
I'm lead programmer in a small project at Tequila Works.
And I'm going to talk to you about sampling textures in Vertex Shader and using that for some cool techniques.
So the techniques I'm going to talk to you about have been developed by Norman Shar and me together or apart, depending on the technique.
So let's get started.
This is the team of the project I'm working on right now.
I'm, as I said, lead programmer here, and also tech art.
And I'm going to talk to you about my tech art part in this presentation.
So what is a vertex shader?
A vertex shader is a programmable part of the graphic pipeline.
And that means that it's a part of the rendering of images.
where the vertices are modified and we can modify position, normal, vertex color and UV mapping.
So before having textures in the vertex shader, what we had to do is use mesh attributes to fit information into the vertex shader.
to modify, for example, the vertex position normals or creating pivot points.
Pivot points, I'm going to talk about pivot points because we use them in several techniques.
Pivot points are transformation points that we can use to transform the position or rotation on several vertices at the same time on a mesh.
So since they're at x11, we can simply sample textures in the vertex shader.
And a typical use is this displacement map that we have the original mesh, but with a texture, we can create a morph target and blend from the original mesh to that new geometry, displacing the vertices depending on an amount set in that texture.
However, I'm going to try to talk about more interesting cases for sampling textures in the vertex shader.
If you think of textures as matrices of vectors, pixels transform into cells of information, like vectors of three or four elements, and depending on the number of bits that you are using will have different levels of precision.
So let's get creative about using those textures.
This is an example where we have a rain animated by vertex shader.
We are moving the vertices depending on the pivot point of each drop in the z-axis.
So from top to bottom, they are animated.
But.
how we know where to stop the drain.
In this example, we have a port, and we don't want the drops to get beneath the port, so what we do is read, we map the drops into a texture, this texture in the bottom of the slide, we read the height of each drop.
And we know when we have to reset the position of that drop and move and teleport that drop back to the top.
A similar example is to fix some meshes to the ground and to get them really tight to the ground so they can deform with our ground level.
Again, we read the same height map.
In fact, in this technique, we are using the same height map to read the position of each vertex corresponding to a pixel.
And we read that pixel to know the offset that we have to apply to that vertex so it can fit perfectly to the ground.
So it's very easy for artists to place.
their assets. In fact, if there's any artist here, maybe he or she can recall doing something like in the left part of the slide. Well, I think the right one is easier for everybody.
This is one of my favourite use cases. This is a smoke I did collaborating with Simon Trompe l'Eau.
It's a smoke animation, so we are baking animation into a texture.
What we are baking right here is the hand position.
So we transform x, y, z into RGB, and each pixel is a frame of this animation in an uncompressed texture.
So what we do is for each frame, we create a pixel in this texture and record the hand animation.
We are normalizing the position between 0 to 1 in the bounding of the whole animation.
And that is scaled back in the shader when it's played.
The smoke that you are seeing has several rings of height.
So from bottom to top, we are reading the current pixel in the bottom ring.
And as we progress to the top of the smoke, we are reading previous frames.
So it propagates the movement along the smoke.
So that's cool, but what if we want to bake particle animation and play it in game?
Well, maybe I would say Alembic.
Alembic was created after, or at least published after this was made.
This is a technique of Norman Shar.
And we have 4,000 particles in this sample and 180 frames of animation.
And what we do is each pixel of a texture is a position.
Each row is a frame.
So in a row we have all the position of the particles in one frame.
And each column is the position of a vertex along time.
So if we read that texture of 4,000 by 180, if we read the column of that texture, we could play one particle animation.
And if we read them all, we have the whole animation.
The thing is that making use of bilinear filtering of a texture, we can interpolate between positions.
So an optimization of that technique could make that we could delete every other frame and probably nobody would notice.
So again we are normalizing positions between 0 to 1 because it's better for precision, but even with that precision is obviously lost.
But what if we want to make rigid object animation?
Of course, we can do vertex cache, but that would mean that we will be storing every pixel in this animation.
Every vertex, sorry.
But we don't want that.
Because we can create pivot points.
We can group vertices sharing a pivot, so every vertex in a group shares the movement and the memory improvement.
of brute force is a lot.
32 objects, just recording 32 objects over the number of vertices that that mesh could have is a great improvement.
So you can see at the left of the slide, you can see the position texture.
And at the right is the rotation texture over 128 frames of animation.
Here.
The rotation is being recorded as quaternions, as RGBA.
Again, we can delete every other frame using bilinear filtering.
So, we have particle animations, we have rigid bodies animations, but what if we want to have a liquid simulation?
So again, somebody could say Alembic, but this technique was made by Norman Shar again before it was published.
I thought about Norman Shar before meeting him as the chocolate man because of that gif.
If we split this hell of a long name...
In 2, morph targets means that we have a different morph target for each frame.
And vertex count agnostic means that we don't care of the number of vertices of the mesh in every frame.
And that means that in a frame we can have 4,000 vertices and in the next frame we can have 3,000 vertices that we don't care about that.
What we have is this position texture where we are recording the position in every frame.
Again, it's raw, it's a frame.
If we look closely, what we are doing is pairing vertices as triangles.
So we know the position of each triangle of this mesh in every frame.
And if a frame has less triangles than the maximum, at the right of the texture you can see those black gaps.
That means that the triangle is being collapsed to zero.
And if the triangle is collapsed to zero, the pixel shader won't notice and won't process that information.
So, instead of interpolating between positions like in the other techniques, what we are doing here is teleporting triangles from one position to the other, and that position doesn't have to be coherent with the previous frame.
So, in the shader, we don't linearly interpolate.
We jump from one morph to the other.
That's an optimization that we can do.
that is triangle pairing.
If instead of storing triangle by triangle, we store pairs of triangles, we are saving up to 33%, just storing four vertices instead of six vertices.
But ideally, we should have different options of geometry compression.
And as John Carmack stated, it's a little surprising that we don't have any locality-based vertex compression schemes for static meshes in hardware today.
Maybe somebody will work on that in the future.
So I have some of those techniques before getting to the last one.
But I want to talk before the last one about advantages of these kind of techniques.
The CPU doesn't know anything about what's happening in the GPU.
So the CPU doesn't have to create buffers, doesn't have to communicate in any way with the GPU to create that kind of techniques.
Just the texture being fed into the GPU.
Second one is we are working with a standard format.
This is not vertex cache.
We can't share our vertex cache animation by WhatsApp or posting it into Instagram, but we can do that with this.
The third one is the order.
This is an important one.
Textures have an intrinsic spatial coherence that we can use in our favor.
For example, we always know new...
where the previous frame of a vertex will be, just in the row before.
But it has some drawbacks.
We are lacking precision, and that's something that we can't avoid.
We can try to palliate that, normalising, as I said, but it's something that you have to work with.
I can assure you that the first time that you try something like that, you won't see the mesh.
The mesh probably will be in the infinite or maybe collapse to zero, and it's a hell to debug that to know what's happening there.
And creation.
You have to have custom tools to create those textures in your pipeline.
Luckily enough, Houdini is working in...
has worked in implementing those techniques, the previous one at least, as far as I know.
And now you can play that into Unreal Engine transparent to the artist.
So we were thinking about, in this project I told you, Norman and I were thinking about making a lot of animation and cloth simulation into textures using those techniques.
but that meant that we have a huge amount of vertex data to handle.
So we were thinking about compressing that data depending on the amount of movement, depending on the animation, but during a break we had a new idea.
And that's the last technique I'm going to talk to you about.
The idea is, what if we transform simulation into bone animation?
What if we put the bone transformation data into a texture?
So the thing is, we were fiddling a little with Hans Goddard's solver.
That's a plugin that transforms simulation into bone animation.
Even morph targets and blending targets into animation.
So into bone animation.
So if we store bone animation instead of vertex animation, we're saving a lot of data, a lot of texture in the process.
Can we actually do it?
Well, the skinning of the mesh is already being done in the GPU.
So theoretically, yes.
But we need to have access to all the information in the CPU.
in an unconventional way because we were using Unreal 4 and we didn't want to modify the pipeline, the engine, to feed that information into the shader.
So through mesh and texture information, we achieved to do everything.
We stored the translation of the bones that we need in a texture.
We store the rotation of bones in another texture, as you can guess.
But here we have to store the weighting of the vertices.
If you know animation, how it works, its vertex has some bones affecting it.
So.
We have to store that information, and we do that into two extra UV channels.
That means that we only have four influences per vertex.
That's the limitation for our implementation, but can be improved.
Where we store the index, the indices of those bones, into vertex color, again, another limitation.
That means that we only can have.
256 bones affecting its vertex.
I mean, affecting all the vertices in that texture.
And where we store the initial offset of those bones, we need the initial position of those bones, and that's an easy one, more textures, but what we do, what we really do is store that in the first row of the position texture.
So the process, how we actually do it, how we actually play animation into textures.
We have a vertex.
We have a vertex color, four indices for that vertex, and two XRUV channels, four weighting values.
We read the index of the bone affecting that vertex.
We read the weighting of that bone.
We read the position and rotation of those bones from both textures.
Knowing the index, we know the column of the texture that we have to read.
And with the time being fed into the vertex shader, we know the current frame.
So we have the pixel position in both textures to be able to transform that vertex.
So we have all what we need to apply the linear skinning algorithm.
And the result is this, a full animation baked into two simple textures.
Here's those textures.
So here's the animation through another lens.
But what are the numbers?
Why would somebody do that instead of animating, per se?
Well, it had sense for us because we had simulation.
And we calculate the amount of data that we could put into textures without the CPU noticing and was worth it for us.
These techniques meant to be inspirational, not a technique that everybody should follow.
So.
How much animation can we store in a texture?
Can we handle facial animation for all the cinematics?
Or which frame rate are we talking about?
The numbers for us, 166 minutes of facial animation in 2K textures, 30 frames per second.
For us, it was a great improvement.
But what if we want awesome facial animation, 450 bones?
Well, 20 minutes, 21 minute, probably more than what you need in your project for awesome facial animation.
And the same frame rate and the same size of textures.
And from here.
What can we do with that?
Well, our roadmap for this technique is to improve normal solving.
Right now we are doing regular normal solving.
That means that we read the normal from the vertex and we are modifying it by the rotation of the bone.
But there are more accurate solutions to that that we want to implement.
Implementing animation blending, what we're talking about here is about cinematics or simulations, but could we use this technique for animation blending?
Yes, we can.
If we store all the animation, different animations of a character into a huge texture, we can, knowing the raw that we have...
to offset, to read different animations, we can blend between them just with one sampling.
Support bone scaling, that's an easy one.
Just some more maths in the vertex shader.
And another texture.
and enhanced compression.
As I said, probably we can, with a smarter compression system, we can compress animation depending on the amount of movement of the character and interpolate between rows.
So, some final thoughts about those techniques.
What we get as output for those techniques when we were doing them is that you have to know your tools.
You have to know what textures in the vertex shader means, because textures in the vertex shader is a powerful tool that most people are ignoring.
Textures can be used as data containers.
And we can approach problems from different perspectives.
So from here, I want to thank Hans Goebbels.
You should check this video for his solver, because that was the spark that ignited this idea.
Also thank Norman, because we worked together in those techniques, but he couldn't make it to be here today.
More or less, that's it.
Thank you for listening.
So if you have any questions, go ahead.
Anyway, you can contact us for whatever you want about this.
You have our emails there.
Hi.
Hi.
I was wondering, a lot of times you need the positions of bones or vertices on the CPU for gameplay stuff.
So do you have a parallel implementation for cases when you do need to access stuff on the CPU or do you just make it work with textures?
So the question is, if I have some kind of metrics to know when to use bones or when to use texture?
We don't.
I don't have it.
I don't have them.
I would go for try both of them and see what works better for you.
It depends on the amount of animation, it depends on the amount of bones, it depends on the implementation of both.
So it's like if you ask me, would you use Alembic or these techniques?
It depends.
Alembic has several implementations, different implementations.
It depends a lot.
For us, it made sense because we wanted to implement simulation.
And we couldn't do that with Bones.
Thank you.
Thank you.
Hi.
Hey, thank you so much for the presentation.
It was really cool.
I had a question in regards to the height map-based pipelines, say for object population.
First of all, obviously, how do you guys deal with precision?
Because whenever we try something about it in a landscape that's a little bit bigger.
we get stuff that's floating around, but the other part of it is also workflow.
Because even though artists don't have to fit assets anymore to the terrain every time, anytime they change the terrain, you have to make sure that the height map is updated appropriately, otherwise you get weird artifacts.
So have you had any workflow experience in that regard?
Yeah, our workflow is to have the tool ready for the last building.
It's like rebuilding lighting.
Every time that you change something, you have to rebuild it.
We created a tool and we wait until the last build to rebuild all the...
all the texture. The thing is, you can iterate on that and make it easier for artists to work with that. In our case, the artists aren't changing the topology of the level too much because animation complained about that and we had to make a compromise.
Cool, well that's good to know that you implemented it as part of the build step.
Thank you.
Thank you.
Hi.
Hi.
You said that when you're using like rigid body backed animation you had instead of storing every vertex you store a pivot point and you rotate around the pivot point.
How do you, where's the data that relates the vertices to the pivot point?
Is that in the texture itself or is that, does it just use like the mesh origin or something?
Can you repeat the question?
Sorry, I can't hear you.
So when you're storing pivot point animation rather than each vertex, how do you relate each vertex to the pivot point?
Is that in the texture, or is that just use the mesh origin?
OK, the pivot point is being, you're asking about where the pivot point is being stored, don't you?
Yeah.
OK.
The pivot point here, at least, is being stored in the vertex color, I think.
This is Norman's technique, as I said, and he did it by his own.
But I would store it probably in the vertex color, or if I feel that there's not enough precision you can do it in extra UV channels.
You have no more option I think, maybe texture but I don't see it happening.
Awesome thanks.
I've just got one more quick question.
You said you store the index of bones in vertex colors as well, why do you have 256 bone limit there?
Sorry?
Why do you have a 256 bone limit when you store the index of the bones in the vertex colors?
Your question?
Because surely the vertex colors would be 32 bits.
So I don't quite hear you well.
Sorry.
Am I too quiet or no, no, no.
Is my accent.
It's my English, sorry.
So you store the bone index in vertex colors.
Yeah.
Why do you say that you're limited to 256 bones?
OK, it was enough for us to do that.
But in later implementation of these techniques, we are thinking about implementing that into another texture, and a small texture.
And that will give us.
virtually infinite influences to have.
Okay, awesome, thanks.
Thank you.
I have a question and a comment.
One question, did you consider using principal component analysis, which is a pretty common technique to compress these types of vertex animations?
Yeah.
Is it common to compress vertex animation?
With principal component analysis.
It's a technique, mathematical technique.
If you haven't considered it, I would recommend looking it up.
It can be very useful to compress this kind of lengthy vertex animation into effectively a small number of morph targets that you then just blend between and mix them in different ways.
Also I had a comment...
you were representing like using this or Alembic, but I don't think it's necessarily an either or, because Alembic is more a file format for communicating between tools.
And this is more of a runtime format.
I could definitely imagine a pipeline where you use Alembic and at the end you bake it down into a texture, unless you're specifically referring to using Unreal's Alembic loading.
I agree, as I said, this is meant to be inspirational just to make the people think about the power of having textures in the vertex shader.
Each one has to know their tools.
I don't quite have used Alembic.
by now, but I'm thinking about implementing some hybrid techniques.
The thing is, it was meant to be inspirational, not just you have to do it like this and like that, but thank you for the comment, yeah, really.
Thank you.
Any more questions?
No?
Well, thank you for being here.
