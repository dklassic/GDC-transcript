Thanks so much for coming.
I really appreciate it.
My name is Mike Amender.
I'm an experimental psychologist at Valve.
I get to do a ton of different things there, but one thing I focus on is hardware research, or alternative hardware research, looking into input devices that might be around in the future.
So the purpose of the talk today is to give you guys, or at least share our thoughts on how we think Valve feedback might be useful going forward on this, at least creating compelling new experiences for players.
I'm going to be coming in kind of hot on this talk.
I do apologize.
There's a lot of material I want to cover.
So I might be speaking a little bit quickly, because I do want to get through it all.
But I promise to stay and answer any question you guys have.
All right, so let's get to it.
So we talk about biofeedback and gameplay and how Valve is using physiology to hopefully enhance gaming experience.
Goals of this presentation, hopefully I'll give you guys an overview of what biofeedback is.
We'll walk through potential applications.
I'll show you some examples of things that we're doing at Valve.
We'll discuss the different physiological measurement techniques you can use in biofeedback testing, walk through some of the pros and cons, and then, I guess, intersperse throughout the talk.
Well, I guess, I'll try and put in nuggets of kind of where we think this technology can be taken in the future, what we think is possible with it.
So let's start off by defining biofeedback.
This is my definition.
It's not really grammatically correct, but essentially I wanted to kind of, I guess, give you a clear picture of what's possible, what we mean by biofeedback.
And essentially it's the measurement, display, analysis, modification, manipulation, and response to physiological signals.
And what we're doing is recording biological signals from the body, performing analysis on them, displaying them, and presenting them back to the user in the hopes of kind of eliciting a response.
And the analysis part is actually a way of kind of quantifying.
player sentiment or emotion.
So it's a way of detecting signals from the body and saying, you know, this person's feeling frustrated or happy or aroused or sad or angry and so forth.
One, I guess, one, I guess, thing to think about in terms of when you're doing this sort of research is that there is a feedback loop possible where signals you detect from people or from users or from players have a feedback loop.
where if you're recording someone's heartbeat and you're displaying it to them and they see their heartbeat in real time, they see they're getting more aroused, that might induce a further increase in arousal.
So you get kind of a feedback loop where subsequent signals depend on prior states or prior emotional states.
And one thing to keep in mind beyond that is that when we talk about emotion, people are not stable, emotions are not stable, they're transient, they're volatile, and they're subject to manipulation.
So we can record emotion in real time, but it doesn't necessarily mean that.
because someone's feeling aroused now, they're gonna be aroused five seconds from now, or a minute from now, or an hour from now, and so forth.
And so, you know, it gets a little tricky when you're kind of doing analysis, at least, you know, post-hoc, on physiological signals.
I guess it's important to keep in mind that, you know, you're looking at beat-to-beat measurements of emotion or arousal of physiological signals, and it's not necessarily something that could be stable or could be long-term.
Just a visualization of kind of, you know, the biofeedback loop, where you start with a...
No more laser pointer.
You start at the top of the loop.
You can see we start with a physiological response.
It provides an input to a system.
The system performs analysis on that, creates a response, outputs that back to the player, which can induce a further physiological response.
So this is the feedback loop I was talking about in terms of biofeedback mechanics.
So why are we interested in biofeedback?
What's so great about it?
Well, if you think about current control schemes, your mouse and your keyboard, your gamepad, and so forth, they provide one-dimensional input.
But one dimension, what I mean is that they're mapping player intent to on-screen action.
So, you know, the player wants to fire a gun, he presses the B button.
The player wants to accelerate their car, they hold down the right trigger.
Things like that. You're mapping player intent to an on-screen action.
What you don't get with current control schemes, at least at the moment, are other aspects of cognition, maybe long-term planning.
You might not know what the player is planning to do five minutes from now, what the long-term strategy is.
And you're also most likely ignoring player sentiment.
How is the player feeling?
So there are other aspects of gameplay, or at least the player's experience, that are not visible through current control schemes.
And so we'd like to attack that problem. We'd like to see if, you know, investigating you know, these other axes of input, whether it's other aspects of cognition or player sentiment, and see if we can create better gameplay experiences because of it.
So here's just a silly little metaphor where, you know, you're mapping player intent to traditional control schemes, and you get kind of a fuzzy picture of the player's experience.
You'll see I have a game pad, and a mouse, and a keyboard.
I don't have Connect, and Wiimote, and 6Sense, and SunnyMove style devices up there.
I'm going to ignore the natural user interface devices in this talk, not because we don't think they have potential, and not because they can't be used for about feedback type, I guess, measurements.
But right now, they're commonly used for, again, mapping player intent.
You're swinging a baseball bat, you're waggling the Wiimote, or whatever.
You're mapping player intent, again, to on-screen action.
They're not used as much for about feedback.
And also because I wanted to make this more of a future-facing talk.
I wanted to talk about less common technologies and not mass market devices that are available now.
So it's kind of like, what's going to be possible in the future?
So again, I'm not ignoring them because we don't think they're valuable, but just because we wanted to make this more future-facing.
So again, why allow feedback?
I've been kind of touching on this, but we think that adding player sentiment into the equation will create better gameplay experiences.
It's a new and it's been mostly ignored dimension of the player's experience in most games and pretty much all games.
And so we think it's possible if you add this component into the gameplay experience, you can create, you know, it's kind of a stock sentence, but it's true, we think, you can create more immersive, dynamic, and calibrated game experiences.
You can adapt the gameplay or the game to how the player is feeling at a given point in time and hopefully create a better experience because of it.
So we go back to my silly little metaphor.
You can see I like using Windows Clipart or PowerPoint Clipart.
But we're wrapping player sentiment to traditional controllers.
You see a fuzzy picture of the player's experience.
You add in physiological signals.
And if it gets better, right?
At least we hope it does.
If it doesn't, that's fine.
And hopefully we'll learn something along the way.
But we think that we can create a better gameplay experience if we add in notions of arousal or emotion or whatnot.
So let's get to, let's actually walk through how we do this sort of research.
So we need to define emotion. If we want to measure player emotion, we need to define it.
Again, this is my definition. It can be fuzzy, that's fine.
But I view it as a subjective internal state induced by a response to external events, at least for the purposes of this research.
And if you want to think about ways in which you could analyze emotion, you can think of it as a vector.
So the vector has a magnitude and direction.
In this case, the magnitude of the emotion vector is arousal.
So how strong is it?
And the direction of the emotion vector is the valence.
So the positivity or negativity.
Is it a positive emotion or a negative emotion?
And just with those two components, you can create, you can get kind of a nice feel for how you can map different emotions onto this axis.
So you look at the Y axis, you can see arousal going from negative to positive.
X axis is valence going from negative to positive.
And so you can see, well, if someone's, you know, kind of at a medium level of arousal and it's really, really positive, well, they're happy, right?
kind of not really aroused at all, and it's somewhat negative, they might be fatigued.
If they're really negatively aroused, or sorry, really highly aroused, and it's a really negative emotion, they might be angry.
So if you can measure valence, so positivity or negativity, and you can measure emotion, you can do quite, or sorry, you can measure arousal, you can do quite a lot in terms of actually ascertaining what the player is feeling, what emotions they're feeling, whether it's jubilance or being happy, content, relaxed, engaged, bored, fatigued, and so forth.
So the physiological signals I'll walk through, and again, there are lots more that I'm not going to talk about today, but here are five common ones.
Heart rate, SCL, which is your skin conductance level.
It's essentially how much energy your pH content of your sweat changes and you can conduct more current through your skin at any point in time.
It's correlated with arousal.
It's the same thing as GSR, galvanic skin response.
So those two terms are interchangeable.
Facial expressions, eye movements.
EEGs, then I'll mention that there are, like, you could look at pupil dilation, body temperature, posture, and so forth. But I'm only going to really talk about the first five.
So let's start with heart rate. It's a beat-to-beat interval of blood flow. Basically, you've got to measure index of arousal here. You can measure baseline rates and changes over time.
And there's some research that indicates you can actually deconvolve the components of the heartbeat waveform and actually get...
you know, more complex emotions like maybe engagement or boredom and whatnot.
So like I have the diagram on the right is the P or the QRS complex.
It's kind of the main spike of the waveform and there are various ratios involved there that people think if you look at kind of the patterns of, you know, the patterns of the QRS complex in various folks that you can actually get more complex emotions beyond just arousal.
But it's a little bit hazy, but I wanted to mention it because it is a potential possibility.
For each metric, I'll have kind of a pros and cons slide, just so you can get kind of like a quick summary of what's good and what's bad about these things.
Heart rate is, you know, it's got a great index of arousal, very cheap to do, very common, people are familiar with it, very easy to measure, and you can possibly do some advanced math to get valence with it.
Problems with it is that it's prone to movement artifacts.
Actually, I had to cut a slide of a mouse we made that had a heartbeat, we actually had a heartbeat mouse where you could.
had a blood volume sensor and you could detect heartbeat just by having your hand on the mouse, but we get movement artifacts because of that.
And so you kind of need to find a stable place on the body to measure heart rate, otherwise you're gonna add noise into the equation.
You also get a delayed onset to stimuli.
So you don't actually know what events are eliciting increases in arousal necessarily because heartbeat increases gradually.
I mean, you can get that pulse pounding effect sometimes, but I'm still, heartbeat, especially compared to other metrics has kind of a longer lead time.
And then again, I said you could possibly do the math to get valence, but right now it's really difficult to determine if it's a positive or negative emotion.
You just know that someone's aroused.
SCL, again, skin conductance level.
This is measuring the electrical resistance of the skin.
Basically, it could be you're sweating more, but it's actually the pH content of your sweat changes.
You can conduct more current through your skin.
And what you get is, essentially, the more current you can conduct is very highly correlated with arousal.
So you get a waveform of arousal over time, which is actually pretty useful.
And probably the coolest aspect of SEL is that you get spikes that usually have a lag time about one to three seconds, maybe two to five seconds, depending on who you talk to, in response to eliciting events.
So if someone is having a SEL measure, I come up behind them and startle them, you'll see a spike, like an arousal, like one or two seconds later in the waveform, which is pretty cool.
So you get almost instantaneous responses.
Pros and cons of SEL, so it's a great index of arousal.
Tonic and phasic responses.
So tonic just refers to the overall level of arousal over time.
So like heartbeat, you can tell generally where folks are being highly aroused or not.
And you get the phasic responses, which are the spikes.
So these just kind of transient little blips in arousal that we could actually do cool things with in gameplay.
So again, minimal lag to stimuli, very cheap, very robust to movement.
We actually have a mouse that you can.
The sensors are located on the palm, and it works pretty well.
People can move around.
It's not a problem.
And you can measure in a lot of different places.
We're focusing on easy-to-use devices that are kind of tied to the hands.
But you could measure up by the mastoid process.
You could actually do heartbeat here, actually, if you had a headset and had a clip here.
But yeah, it's pretty robust and pretty intuitive, I guess, or at least pretty easy to use.
One problem, although, again, you can get the spikes.
Because it is a waveform, or because it can be noisy, it can be difficult sometimes.
to associated listening events, especially if you have multiple events in combination.
If you're playing an action game, it can be difficult to determine which event actually caused the spike if a lot is going on.
Again, difficult to determine valence, you know, it requires interpretation, which is not something that we want to do necessarily, but it's something that we kind of have to because we don't really get valence, we only get arousal.
And one other interesting, I guess, concern with SEL is the range is variable across subjects.
So you can get people who go from, you know, essentially it's one microsiemens to like two, you know, they could be really highly arousal and someone else could go from like two microsiemens at baseline to like 15.
And so is that the same thing?
Like, you know, is that two to 15 range equivalent to a one to two range?
And how do you map, you know, the subjective feeling of arousal that someone is feeling to those ranges?
So it's something to think about is that the range is a lot variable, whereas, you know, in heartbeat, you know, people, typical resting heartbeats, you know, fall in, you know, the 50 to 80 range.
And, you know, you can kind of compute maximum heartbeat based upon.
you know, age and gender and so forth.
But with SEL, it's a little bit trickier.
So facial expressions, this is just recording movement of the facial expressions, or sorry, facial muscles.
It's really good at classifying emotion.
You can get both balance and arousal pretty well by recording someone's face.
You can do this remotely with cameras and have people code after the fact.
There are companies that are starting to try and automate this process, which would be amazing.
I don't know.
I don't want to evaluate them here in this talk, but it's something that people are thinking about.
You can also do it intrusively via EMG, which is electromyography.
Essentially, it's just putting sensors on various facial muscles, and you can measure how much they contract.
And you get really good measurements of what someone is feeling at any given point in time.
So again, the work on facial expressions has been around for a really long time.
It's well established.
So here, this is the example of six.
I guess, canonical human emotions, anger, surprise, disgust, sadness, happiness, and fear.
And so you can detect all of these pretty readily.
And you can detect the magnitude of these pretty readily by recording facial expressions.
So the pros and cons, you know, it's great index surveillance, great index arousal.
So you get the full picture of emotion and you get this instantaneously.
You know, people, you know, your faces will respond involuntarily.
I'm really bad at it. I can't hide my emotions even as much as I try.
And so I will respond.
really quickly and it can be frustrating, but again, at least in terms of its potential applicability to as an input to gameplay, having this instantaneous response is pretty cool.
People will respond almost automatically to various things.
The downsides, definitely intrusive, if people don't like having a camera on them or if you actually have the sensors on you, it's crazy intrusive.
It's expensive at the moment, both in terms of time and money.
Again, people are working on bringing this down to.
automation through webcam, but we're kind of not there yet.
And processing requires a lot.
If you have a human coder who's watching people and categorizing, that can take a while.
And if it is a human coder, they can be subject to bias.
And then they also require training.
But if you're depending on some kind of automated system, it's a black box, right?
We'd like to kind of validate these techniques ourselves.
SCL and Heartbeat, it's a lot easier to do that with software that's evaluating folks out of our hands.
We don't like that, actually.
That's kind of a a personal preference, but we want to kind of validate these things in-house, and so you do have to kind of depend on external algorithms to make determinations.
And so it's up to you as to kind of how valid that ends up being.
But I mean, just in terms of the potential applicability, facial expressions have a lot of promise.
Fourth one is, so eye movements.
Basically, we're just tracking where people are looking in real time.
So you can use remote or head-mounted cameras to measure the reflectivity off of people as you shine an infrared light off the people.
bounces off, and you can kind of get an idea of where someone is gazing.
So you get a real-time record of where people are looking.
And we require both eye movements and fixations.
Eye movements are called saccades.
Fixations, when the eye is stopped moving, or its eye is always really moving, but when the eye kind of centers on something for a little while, that's kind of like an index of processing.
And I guess it's just a fun fact.
So when your eye is actually moving, when you're saccading, visual input to the brain is suppressed.
And so since we're making eye movements for about 2 and 1 hours per day, you're effectively blind about 2 and 1 hours per day.
And we don't realize it.
Just kind of something to keep in mind.
So if you think about display is where you update the display when the eyes are moving, you could do whatever you wanted.
And people wouldn't detect it until they stop moving.
And so it's just there's a lot of potential use there, at least in terms of introducing surprising elements into the display when eyes are moving.
We're a ways away from that.
But again, trying to think about what could be possible in the future.
So here's an actual, I'll just show you guys a quick video of eye tracking, this is Portal 2, so if you don't want any spoilers or see any levels in Portal 2, you can look away, I'll let you know when the video's over.
But yeah, just a quick video of someone playing Portal 2 and visually consuming a map.
So the red circles are fixations, the size of the circle is proportional to the length of time spent fixating, so when the eyes have stopped moving, and then the lines connecting the circles are the saccades, the eye movements.
And so we actually, again, the focus of this talk is not really on playtesting, but we do use these a lot in playtesting in terms of determining how people are consuming our levels.
You can actually do, and I had to cut these slides, unfortunately, you can actually do quantification of eye movements and determine kind of what cognitive mode people are in.
Like there's different patterns of eye movements that correspond to maybe scanning the display, just visually consuming it, versus searching for a particular item, versus problem solving.
You can actually quantify these based on the trajectory.
of the eye movements and the length of, I guess the length and number of fixations.
And so, we'd like to think about that in the future in terms of like when we're play testing our games.
Is that okay, we're like, you know, we can tell what people are doing at any given point in time.
And so you get rudimentary insight into cognition just from where people's attention is landing, or at least where they're looking, which we think is pretty powerful.
So pros and cons of eye movements.
It's an index of attention, right?
So it's not, you know, as tied to emotion as some of the other.
I guess some of the other physiological signals, but again, you know, it's still pretty useful.
And as I mentioned, it could be a rudimentary index of thought.
And if you couple this with pupil dilation, which is, you know, a great correlate of arousal, you can get arousal as well.
And so pretty much any eye tracker you buy will be able to measure pupil dilation.
A unique metric and very, very reliable eye tracking technique is kind of mature now.
It's still very expensive, but the technology is mature.
So you get pretty good and pretty robust measurements of visual attention.
The cons, it's incredibly expensive, the most expensive, probably the most expensive of all the signals at the moment.
You know, I'm talking in the tens of thousands of dollars for an eye tracker.
There are companies that are trying to bring that down by an order of magnitude, you know, in the five grand range.
And then there actually, I know there's an open source project where people are trying to do this with webcams.
I don't think it's there yet, but again, it's nice that people are trying to kind of bring this technology down to, you know, consumer price points.
So, just something to think about.
Other cons, it definitely requires a lot of analysis, post-hoc analysis.
You can automate stuff and you can write scripts, but if you want to tag various features in the display, it can require a lot of analysis to do at least the quantification of scan pattern that I was talking about.
That requires a lot of work as well.
So you can get great data out of it, but you have to work for it.
And then just one fact to it is that it can be intrusive as well.
They've done studies where...
You tell people you're tracking their eyes, or you don't, and the people who know their eyes are being tracked, you know, have different eye movements.
Like, they're more deliberate in their eye movements.
So again, as with any kind of measurement technique, you know, we're introducing bias into the equation, unfortunately.
But, you know, the data for the most part is pretty clean, but I didn't want to let you guys think that it was completely bias-free.
Um...
Alright, so...
Last one to talk about is EEGs.
So this is measuring electrical potentials of the brain.
EEGs are primarily time-based signals, so finding out when an event occurs.
But you can put sensors all over the head, and you get coarse measures of location, which is pretty cool.
And I guess what they're primarily used for, at least in terms of their immediate applicability, is you can look at the frequency spectrum, so the frequency kind of bands of electrical potential in the brain and the latency of the spikes.
But if you deconvolve the frequency spectra, you can get measurements of emotion.
Like there are alpha waves that are kind of low frequency.
that are correlated with relaxation, and there's beta waves that are correlated with alertness, and there are delta waves that are correlated with kind of like focused attention.
So you could get these measurements, and you can measure engagement in your games, for example, or detect when a player was engaged, or detect when a player was relaxed, and so forth.
Just the upper left picture is potential scalplications like EEG studies, and in the labs, acquire hundreds of electrodes.
There are consumer grade headsets that are a few hundred bucks.
that do a pretty good job of measuring emotion.
They use far less sensors than a couple that only have two sensors on the forehead.
The right graph is just because I like cool looking graphs and it's just kind of showing you how different regions of the brain can light up at various points in time for different stimuli.
So EEGs, positives, you can get arousal out of them, you can get valence, again, not.
a complete picture, but you can do a pretty good job.
And you can get rudimentary insight into thought.
I do not want you thinking that you can get complex thought through consumer-grade EEGs at the moment.
We can do some stuff, but the brain is very, very noisy.
And so to actually extract signal from noise usually requires hundreds of trials with minimal differences between conditions and so forth.
And so you can get localization.
You can tell when people might think in about a leftward action versus a rightward action, or maybe a pushing or pulling action.
But we're not close actually to playing Halo with your head yet.
Maybe in the future, but we're not there yet.
Cons, it's expensive, intrusive, so the data again is not clean.
It can be very noisy and very hard to extract signals sometimes.
And then difficult to validate, right?
Again, you're relying on someone else's product to validate stuff for you.
And again, we're trying to come up with our own solutions to this.
That's a point I'm making.
I guess I want a heart.
Well, I bring up, I guess, just because it's something to think about is that there are disagreements about how to quantify waveforms in the brain or EGs in the brain.
And so it's important to realize that the data you're getting is dependent upon one person's interpretation of what's going on, or one company's interpretation of what's going on.
Other techniques, so we have pupil dilation, body temp, body posture.
Yeah, I'm not going to talk about them.
I will mention that people have actually done studies.
And so if you couple body posture and pupil dilation, so you could do pupil dilation remotely with an eye tracker.
You could do body posture remotely with a depth sensing camera like Connect or whatnot.
You can account for about 80% of self-reported frustration just to those two metrics, two non-intrusive metrics.
So not too shabby in that regard.
There's also lots of stuff we haven't thought about.
Some stuff we have that I guess I'm not talking about.
But this is a partial.
painting of the picture.
There are many more ways you could be extracting information from the body or from users.
And so these are some of the common ones, some of the most useful ones.
But it shouldn't feel like I'm constraining the notion about feedback to the metrics I've mentioned.
All right, so let's get to the good stuff.
So how can we use this data?
We could start with the most basic, like passive viewing of biofeedback data.
I show you your arousal in-game, and I'll show you a video of that later on.
which actually is pretty cool.
You can think about modifying game experience based upon a player's internal state, what a player is feeling at any point in time.
I'll walk you through an experiment we did where we modified the director in Left 4 Dead, which is an algorithm that kind of controls the gameplay experience in Left 4 Dead, where we added biofeedback to that and tried to create a better experience.
You can think about adapting difficulty in real time.
So if you could detect that a player was frustrated, right?
or that a player was feeling challenged, or a player was not feeling challenged, right?
You could modify the game in real time to adapt to that.
Yeah, it's something to think about.
You could think about detecting when a player was bored, and figuring out a way to kind of re-engage them.
These sorts of gameplay techniques, which are not possible with current control schemes, could be possible.
I mean, I'm not gonna say that we could definitely do them, but they could be possible with biofeedback devices.
We're really curious about optimal arousal patterns.
We wonder if they exist.
Like, you know, is there a pattern of arousal?
or a pattern of emotion in game, while someone's playing a game, that will lead to the most enjoyable experience.
And again, it probably is not the same for all folks, but just thinking about what would contribute to an optimal pattern of arousal.
And so with these techniques, you could start trying to attack that question.
And say, yeah, it's, I just have three silly examples of what the pattern, it's peaks and valleys, or maybe it's steadily increasing, or it has a giant peak and then a valley, and then tiny peaks, or maybe it's steadily increasing peaks or valleys.
Who knows?
But just trying to figure out and seeing if it's possible to figure out if there is a pattern, at least in various levels or maybe across specific genres, that would lead to the most enjoyable experience.
We think it's an interesting question and worth attacking.
More applications.
So you can think about actually inputting physiological or using physiological data as direct inputs to gameplay.
Just a couple quick examples, like tying health to arousal.
Like if you stay calm.
your health decreases gradually.
If you get highly aroused, your health ticks down quicker.
You can think about in-game prompts that are tied to emotional state.
If someone's engaged, you don't want to distract them.
You minimize pop-ups, for example.
If someone's kind of bored or you can tell that they're wandering around and lost, then you could kind of detect that in theory and add in prompts to kind of help them out.
You can think about NPCs responding dynamically.
It's like, hey, Mike, why are you so sad?
Like if they detected that I was frowning, right?
I mean, you can think about experiences you know, characters or players in-game are responding to what you're experiencing at any given point in time, you know, could create a really powerful experience.
Um, and you can think about, you know, requiring, you know, subjects to remain, or people, players to remain calm, right?
Like, you know, having, you know, or getting really aroused to proceed in-game.
You could imagine a lie detection game, where to fool your interrogator, you had to maintain a calm, or a low level of arousal.
And, you know, if you, if you got aroused, or you got nervous, um, you know, you'd lose, right?
Or you, or they would detect your lie. Um, so things like that.
Again, beyond that, there's lots of stuff we're thinking about.
So you think about actually doing matchmaking based on physiological profiling.
So maybe you don't want to pair a passive player with a really emotional player.
Maybe that is a worse pairing than a really bad player with a really good player.
Maybe that leads to a worse experience.
And so maybe if we could pair passive players together or emotional players together or easily frustrated players together, we could do a better job with matchmaking.
I'm not saying it's possible.
exit is possible. I'm not saying we can prove it, but I'm saying it's something we're thinking about, we think it might be worthwhile.
Along with the passive viewing of your own behavior in game, you think about competitive play.
People browsing through a list of matches and trying to figure out what match to watch.
Watching the arousal patterns of really skilled players could be interesting. It could be a way to filter how you expect a competitive match, or if you're just watching a competitive match in general and just knowing.
if the competitors were feeling aroused or scared or whatnot at any given point in time, could be pretty interesting.
And then beyond single-player stuff, we could think about ways to use this in multiplayer.
You could see a spike in your teammate.
You may detect when they're in trouble.
Or maybe if you want to grief the other team, and you can create a spike in arousal, you can earn points for listening responses I guess in your opponents.
And then final application.
Again, I'm not really going to talk about playtesting.
I think I have one slide on it.
It could be a whole other talk.
This is more about the gameplay side of things.
But we can definitely, there is so much to be gained in terms of quantifying how a player is feeling.
In playtesting, we want to know what they're doing, why they're doing it, and how they're doing it, and how they feel about it.
What they're doing is easy, right?
The other two are subject to noise.
And so if you have kind of biofeedback stuff.
In there, we could start making better attempts at quantifying emotion and getting a better understanding of how people are feeling as they're playing our games.
All right.
So I'll walk you through three, actually, main experiments we did.
They're all still ongoing, but I'll walk you through what they are and why we think they're interesting in terms of using biofeedback.
So the first is modifying the director in Left 4 Dead 2.
Second is actually using physiological signals as direct inputs to gameplay in Alien Swarm.
And then I'll talk about playing Portal 2 with your eyes.
So actually using your eyes as controllers.
And then I have a couple of slides on kind of passive viewing and some multiplayer tests we ran where you could view physiological signals of your opponents and your teammates.
And then I have one slide on play testing.
So Leopard Ed.
So in Leopard Ed and Leopard 2, the player gets a different experience each time they play.
And the way this works is we have a kind of a system called the director that.
creates a dynamic experience, I guess, throughout gameplay.
And what it does is it modifies enemy spawns, where health and weapon are placed, which boss monsters occur, and so forth.
And it does this based upon an estimated arousal level.
So the director has an estimated arousal level in mind.
And it says, yeah, they're highly aroused.
I'm going to let them kind of drop down for a little bit and not bug them.
Or they're not aroused based on my estimation of it.
I'm going to give them a smoker to attack or a hunter to attack or I'll have a mob spawn and go after them.
And so what we wanted to know was, if you replace this estimated arousal level with actual physiological arousal from the player, can you create a more enjoyable experience?
And then beyond that, going back to what I was talking about earlier, is can we determine optimal arousal patterns in this regard?
So really quickly, just going over how the director works, you have survivor intensity as a single value.
It's increased in response to in-game trauma.
So you get attacked, you lose health.
You kill an enemy that's close to you, for example, intensity increases.
Intensity decays to zero over time.
I think it's, I don't know, 30 seconds to a minute, depending on various factors.
And the goal of the director was to create kind of peaks and valleys in this estimated arousal level.
So you can look at the graph here.
The middle line is the most important, where it says survive.
Hi.
I was actually just wanting to know, did you use experienced gameplay players for your tests?
Yeah, so it actually varies.
So we did, for the Left 4 Dead 2 study, we had these are all not Valve employees, but external play testers, but people who had played Left 4 Dead 2 before, who had come into the lab.
So yeah, but the question, what you could do with novel gameplay players and are those patterns of analysis going to be different?
Yeah.
Absolutely.
Yeah.
Yeah.
Yeah.
Hi.
I actually work on very similar problems with eye tracking.
With our play testing, we've actually found that.
uh, that when people have to focus on the screen for a long period of time, they start to get headaches and we get problems with eye strain after a long, like, more than 15 minutes of gameplay. So I'm kind of curious if you tested beyond 15 minutes or what kind of problems you may have run into? So the question was, um, is whether or not we can use eye trackers for longer than 15 minutes, um, because there is potential for eye strain. Um, yeah, so we've tested people for an hour and a half.
Um, you know, people can, the eye tracker we use is pretty robust so people can look away from the screen.
Um, you know, they can stand up and sit down, um, if they want.
So we've had, you know, hour and a half long, two hour play testing sessions and haven't had really any complaints of eye strain.
Um, you know, we kind of, you know, we have, I guess the, you know, the bounding box in terms of what the eye tracker can detect lets you kind of move your head around, you don't need to stay in the same position.
Um, so we've been okay in that regard.
Can you force arousal in the human player by giving them signals that their in-game character is getting highly aroused?
Like if, I don't know, the character's left for dead, start breathing heavily, or their vision gets blurry, will that force arousal in the player?
Yeah, so the question was, can you kind of induce arousal or various responses in the player by having in-game avatars mimic those same responses?
And so absolutely.
We have neurons in the brain called mirror neurons that kind of fire when you see an action being performed.
And so it actually is kind of analogous to you performing the action yourself.
If I see someone swinging a baseball bat, the same neurons that would fire when I was swinging a baseball bat fire when I actually see that process occurring.
And so you get those kinds of triggers where if you see your in-game avatar panting heavily, for example, absolutely would you see a change in respiration rate.
It may not be.
a large change, but absolutely will players kind of be affected by what they see on screen.
Hey, so the Alien Swarm testers, how did that work out for them?
Which one, sorry?
The Alien Swarm testers?
Yeah.
Did they actually succeed?
Yes.
So the question was, did the Alien Swarm playtesters actually succeed?
And yeah, so I showed you a small snippet just because I was pressed for time.
But yeah, the Alien Swarm, yeah, so I think.
about 60% of people made it through.
And again, this is our algorithm, right?
So we're tweaking it so we can make it harder or easier.
But yeah, about 60% of people made it through so far.
So did they have access?
Did they see the time left?
And did they see the color change as well?
Yeah, so we've done it both ways.
They always see the time left.
They always see the color shift from green to yellow to red or back or whatever.
We've done studies where we show them the arousal and not.
When we showed them the arousal, they got more frustrated because they didn't understand why the timer was speeding up sometimes.
When they didn't see the arousal, they were more likely to kind of be cool with it.
OK, cool, thanks.
Hi, I was wondering if you were able to track flow and mastery from players.
I mean, if you master player, it would be calm during the gameplay.
And if you were able to design for a flow experience.
So the question was, did we track flow and mastery of players?
Or players who are really good or experts at the game, did we look at differences between them?
And I'm assuming people who are not masters or novices or whatnot.
We've done some kind of comparisons of novices and experts.
We haven't checked flow.
That can be a little tricky to operationalize sometimes.
But no, we haven't done anything systematic in that regard.
We are really curious in novice expert differences and how that correlates to patterns of arousal, patterns of physiological signal.
But I guess it's stuff I didn't plan to talk about and can't really talk about at the present time.
Okay, well, yeah, thank you guys very much.
Oh, sorry, I guess one more question.
Hi, I teach at Arizona State University.
I'm game design, but I'm doing a collaboration with bio design.
And I was wondering if you would be interested in working, perhaps, with people who are doing physical probes into the body, and perhaps expand your research that way and work with academia?
Or are you all commercially-oriented, and you want to stay in that world?
Was the question, what devices are we using, and are we like?
to using devices that are actually attached to the body?
Is that the question?
Sorry.
Yeah.
I can see how our research might complement each other.
But I come from academia, and there's no commercial application that we're going for other than, well, the biodesign guys are going for designing prosthetics.
Right.
So I was wondering, there's an intersection there.
Would you be interested in perhaps working with us?
Oh, sure.
I mean, well, someone's.
Yeah.
Oh, I mean, of course, yeah.
We're definitely interested in seeing what other people are doing.
I mean, the whole point of this talk was to say, hey, we think this is really cool, and we definitely want to encourage more people to be thinking about both how to improve the techniques we have and then how they could be used in gameplay, so both the commercial and the basic and applied concerns of this research.
So definitely we can talk and see if we can see if there's potential down the line for a collaboration and whatnot.
So I'll send you an email?
Sure, that's fine.
Thanks, guys.
Appreciate it.
