Hello, my name's Adam Ritchie coming to you from sunny Sweden to talk to you about our latest project, The Division 2.
In the past I've worked at Midway, Jagex, Lionhead Studios, but now I work at Massive.
And I'm here to talk to you about MPC voice design in Tom Clancy's The Division 2.
We're going to look at the work on NPC voice systems for The Division 2, covering the role of the voice designer, a look at the project itself and our goals, NPC behavior, bringing context to their dialogue, sharing the player experience with friends, voice production, use of statistics for balancing, and wrapping this up neatly with a nice conclusion.
The question we're going to try and answer today is how can we make non-player character barks, systemic voice lines, relevant and engaging to players? How do we make these voice lines more than just background? How do we keep them as mood setters, but also make them matter? How do we use them to bring our characters to life?
What is a bark? I hear you say. I'm glad you asked.
Greeting the player.
investigating a noise, throwing a grenade, reloading their weapon.
Bark is a word for a voice line designed to rationalize and telegraph the actions or reactions of an NPC. People call these different things battle chatter, VOs. We use bark because it's like a dog barking.
Each one can sound similar, but it can carry meaning in different situations.
You'll hear me refer to them as just VO or voice lines as well.
Bucks are world builders, ways to tell a story and to entertain.
And whose job is it to make these bucks?
Well, among other people, it's the voice designer.
And what is the role of the voice designer? What is this job?
Have you ever heard of it?
Unless you're a Ubisoft employee, you might not.
Bye.
Maybe you know a dialogue coordinator or voice producer.
Our responsibilities are pretty broad but I've tried to condense them into four areas. Systemic Barks Design. What an NPC can potentially say that gives them purpose and meaning? What should I say as an NPC? Does context matter? What do I know?
What should I not say?
Will a lack of context for what I'm saying break the player experience?
What should I not know and not be able to say?
We also need to consider how they say it.
Are they talking via a megaphone or a radio or in person?
Are they wearing a gas mask?
And how often can I say it?
Is it a one-time thing? Should I be able to say this thing a lot without getting annoying?
And how important is that information that I'm conveying to the player?
We're heavily involved in casting of the actors that play these roles.
Some things we have to work with are location. Where are our characters from?
Where can we record? Where are the most suitable actors? Can they do the accent easily?
The level of casting, what the budget is, how important that character is going to be and how much we're going to hear them.
The schedule, when do we need to record, how intense is the script going to be for the actor, do we need to spread it out, when's the talent going to be available.
And the quantity, how many actors can we afford, how many different voices do we need for the game.
Will there be recurring voices?
And then when it comes to recording, there's a lot of stuff to decide and work around.
The technical specifications of microphones, preamps, the studio, and other environments that we're gonna use.
The session setup, a lot goes into this.
Voice directors, engineering, engineers being on boarded, training staff with new tools.
We like to get developers, directors to come to the studio and meet the team, see the project.
We also want to make sure that we've got the right people involved in terms of voice coaches or researchers.
And then the flow, how we start, maintain and finish a session, how much material we can cover and still hit our quality targets.
And how do we create the best assets in that environment?
Then communication and support.
When and how do we send the scripts? What format, what time do we give the actors time to rehearse?
And can we send extra material for the actors and directors like trailers, images, gameplay, media references? And then editorial process. Who's going to edit that material? Do we have time?
We don't. Do we keep breath in the voice lines or do we clip it close to the words?
And then we've got the file naming and loudness settings too.
Putting the results into the game, we have an in-house database which we update the text and audio from the recording sessions, reviewing the selects, making sure that we've got the right ones, updating any metadata and flagging any session changes that may affect other departments.
And then there's the sound, the quality assurance pass on that, making sure the material sounds right and that we've used the right microphones, the right takes, putting this into Wwise, getting the material to play in the right way, 2D or 3D, the right fall-offs, bussing, how much world reverb is applied.
We also have processing, editing to animation timing or other tweaks and changes, adding special effects and the mastering process.
And then we've got testing. How does this work in the game?
Has it been hooked up? Does it sound right in context?
And then we iterate and repeat this process ad infinium, because when you work on a game as a surface, the work never stops.
So how did we get here in the first place?
Well, the Division 1, Tom Clancy's The Division, was released March 2016.
The story is set in post-disaster New York City during a harsh, cold winter.
Gangs are roaming the streets, boisterous, full of character, noisy and aggressive.
The game suffered from some technical issues at launch but quickly established itself as a leader within the genre.
Some of the barks that the NPCs said were tied to behavior, whilst other barks were driven by timers to give NPCs something to say and to make combat and safe spaces feel busy.
And some barks were often overused by design and ended up playing a lot more than was actually intended.
The team had limited ability to fix this or control playback after release, and it took some time to fix some of these bugs that appeared at launch.
What did we learn from the first game after it released?
The barks that were driven by behavior made the NPCs seem clever and enhance their connection to gameplay.
But some characters were sometimes talking for the sake of talking and we wanted to improve on that for the sequel.
Players play the game very differently to developers.
And repetition can be funny, but it also breaks immersion.
When the first game released, whenever you killed an enemy from a particular gang, their teammates could do a friendly down bark, and could call out a name to show that they had some connection to that teammate.
But, due to a bug, it always picked the first character on the list, which was Alex.
So players shot Alex a lot.
You can buy t-shirts with that on now.
There's an achievement named after this in the game as well.
We embraced this as a company and we even called some of the bosses Alex, making fun of ourselves.
But we also committed to avoiding this in the sequel.
The Division 2 released in March 2019.
It's set some months after the first game with themes of regrowth and recovery.
Now we're in summertime Washington DC and New York City again in the DLC.
We've got extreme weather, heat, rainstorms, magnitude of technical improvements have been made since the first game. NPCs have got improved behavior, they can do more, they adapt to your play style. Better AI means that the NPCs need to feel more intelligent too. We also set an ambitious development schedule giving us a smaller time frame to build upon the foundations of the first game.
So let's look at how we aimed to use NPC behavior to make their barks relevant and engaging.
And any American viewers are gonna be twitching at the UK spelling throughout this slideshow.
So working with the NPC AI team, and based on our experiences from the first game, we developed some ground rules for our system that we hoped would fulfill the ambitions whilst keeping the amount of writing and recording manageable, because economy is king when it comes to barks.
Every bark will be triggered by an AI decision and will have a purpose and a reason for being said.
They're not random. Barks should be simple to implement. The database will do the heavy lifting of any of the rules and VO line selection. And I should add a note here that we opted not to do a complex rule-based dialogue system. Barks will be connected to the NPC AI state, for example, guarding or fighting.
The context or circumstances will alter which line is picked to play, make them more relevant and useful, which we'll cover in a later section.
And NPCs should seem aware of their team and will respond to things set around them. They won't ignore their friends.
This is an AI behavior tree in Snowdrop, our engine.
It looks like a simple flow diagram at the top level, but it can go quite deep.
As the AI makes decisions about what actions to take, we looked for opportunities where a person might say something.
Broadly speaking, there are two approaches to designing barks driven by behavior.
Literal barks, describing actions or reactions.
These are useful for the player to get immediate basic gameplay information.
Grenade out, advancing on enemy, shields being broken.
And then non-literal barks. Things a person might say when doing a behaviour but that are not tied strictly to that behaviour. We can do this Bravo Squad, flank enemy. Contextual spawn, the fallen shall be answered. Being revived by a friendly, you get my rations this week.
These are things that build the character or the world. They're mood setters and flavour.
A bark that on the surface seems very literal can become non-literal with creative writing.
Reloading can be literal or it can give us some insight into their character and the world.
The writers are very involved in the design process and we discover which barks are best written literally or not and even then a lot can change when it comes to the actual writing.
Something that sounded very sensible on paper may just not work for writing and it needs to be redesigned.
So iteration is central to this process.
When we talk about barks at Massive, we group them by a stimulus.
An individual bark is a line said by an NPC, but a stimulus is a parent to those barks.
This simple word can be used by an NPC designer to call a bark at the appropriate place in their behaviour.
when we, the voice team, will control the playback of that.
So here we have the design for the stimulus Enemy Down.
The NPC has killed an enemy and decides that they're going to announce this.
We keep the design simple and useful for several audiences.
The implementer knows which characters need this bug and when it should play.
The voice designer knows which characters are involved and which conditional metadata to tag the line with.
The writers have context that they can write for, and there's several writers involved, so these examples are very helpful.
And the voice director knows what the NPC's motivation is, which they can then communicate to the actor.
In session, they'll refer to this quite a lot and become very familiar with our Barks database.
Asim and I are stored in our internal database and script management tool, Oasis.
On the left is our AI Scenes Explorer, where all the barks for the different factions and families of NPC are built.
Central is a script writer.
Below that is the Excel-like interface where we can amend all kinds of data per line.
And on the right, we see the Character Explorer and Media Player, where we can audition the robo, voice, temp, or the recorded lines.
Oasis also stores and organizes our story, scripts, cinematics, menu text, and all of the other localized languages.
There were a total of 645 different AI stimuli that NPCs could use at the launch of Division 2.
This accounts for around 65,500 AI voice lines or barks, split across 288 different characters.
Here's an example of some Barks stimuli from the first Division game.
Each one of these had to be hooked up manually in Snowdrop for each NPC archetype.
And if the stimuli moved inside Oasis, it could break.
This was due to the stimulus being called by its location path in Oasis, like a website URL.
The barks were also evenly weighted between the build-up to combat, actual fighting and detection.
But players spent a lot more time in combat, so the NPCs quickly boiled down to a central, core set of combat barks that are played the most frequently.
For the Division 2, knowing that our NPCs would be spending a lot of time fighting, we expanded the combat bark section a lot and covered more new behaviours and special abilities, growing the combat bark's line count too.
Some stimuli now have as many as 16 barks per NPC to help avoid repetition.
Philip Dunstan's AI team also simplifies the classification of a stimulus to just the last word of the location path, making it much easier to implement.
And if anything moved in OASIS, that link wasn't then broken.
The designer now just had a simple list of words to pick from instead of a page of URLs and needing to find the exact one.
we had 288 AI characters in the game at launch, all pulling from 645 stimuli, so this was a huge time saver.
Once the stimulus exists in Oasis and has some barks, the data runs through a build system into Perforce and then can be implemented in the Snowdrop engine.
The NPC then gets a stimulus that will trigger every time it runs a behavior.
Some of these behaviors are rare, but others can trigger repeatedly.
This is where the dialogue controls come into play, throttling the barks from spamming.
Let's see a short 30-second clip of some different contextual barks in action, and we'll return to this later to break down what's happening.
And why not use a logic or rule-based system for BARCs?
Well, a series of contextual ordered rules that control the flow.
We looked into this early on, and it would have given us greater control, but it would have also grown complicated and ended up being time-consuming to implement.
We'd have still been developing and testing the theory when we actually needed to be recording and implementing.
We opted for a data-driven solution with simple controls that are tuned centrally and can be overridden at a more granular level.
This can tie you in knots when you're tuning values, but if you stick to the principles that you set at the beginning, this can be a good solution.
We also tested out conversation systems, like in the example here from the first game.
The NPCs had a selection of starters which they could play when they were not in combat and they had a larger variety of responses.
This could yield conversations that varied and was economic with the line count.
To improve on this for the sequel, the obvious goal was reducing repetition because there were far fewer starters than there were responses, so they quickly became familiar.
What we decided to do, rather than invest a lot of time in an elaborate rule-based conversation system, was to create a matrix system of starters and responses that were interchangeable.
This proved quite the writing challenge, but the result is a seemingly fluid and hugely variable simple conversation system.
Here's an early working prototype of the system with some OASIS-generated RoboVoice.
Nice day, isn't it?
Yes, lovely day.
Nice day.
In the example of a character reloading their weapon, they'll trigger the reload stimulus, and this will then ping nearby friendly NPCs in their fireteam within a radius, and pick one that is allowed to speak.
They will then respond to the reload bark with their relevant conversation response stimulus, which in this case is cover ally respond.
Furthermore, the clever AI team were able to grab the timing of the starter voice line and feed the length of that to the client, which then cues the selected response bark to play after the elapsed time.
So the timing feels natural.
Here's another video of an early test.
They're exposed! Take them down!
This is of huge benefit to our localization team, because the system doesn't care what language the player is using.
This is fantastic because it means that we can give the localization teams no strict timing conditions for the barks.
And so the actors have the freedom to deliver the lines at the right cadence and rhythm for their own language without being constrained by the English line length.
Reloading. Go. I've got you covered. They're out in the open.
We've got a target.
From this angle, you can see the debug timing arrow here better, and the countdown timer that shows when the response line is about to play.
And this is what it looks like in Oasis.
This column shows the relevant stimulus name, reload.
And then here are the barks themselves.
And this is where we tag each line with a conversation response stimulus.
In this case, cover ally respond.
The response itself is a stimulus.
And so here's an example of what that looks like.
And the text lines for recording.
So the starter calls one of the above, for example, ammo is gone, and the response is any of the below.
I'll give you cover.
As you can imagine, this can sometimes be challenging for writers, and to make things more complicated, we use the responses in as many situations as we can.
Cover ally respond can also be triggered by the advance on enemy stimulus, for example, when an NPC wants covering fire when they want to move up.
The idea was to build responses that can apply to as many situations as possible and fill the world with these mini-conversations that make you feel like the NPCs are engaging with each other.
Let's see this working in-game as a shootout between some civilians and the True Son's militia reaches a conclusion.
Is that... are we okay?
There'll be more tomorrow.
We love to talk about the weather.
We felt that to make our NPCs more human, we should let them talk about the weather too.
Especially in an environment where the weather is more and more extreme.
This could have made our NPC behaviour graphs even more complicated, but with our excellent AI programmers Mons and Daniel, we conceived of the tag system, where the server can check the conditions such as the weather, time of day, or even what object an NPC is hiding behind, and change which barks will play as a result.
This system turned out to be so useful, it was this kind of set and forget kind of logic, that these tags are now used for various systems in the game, such as contextual animations and the living world system.
We started by making a list of the conditions we thought would be the most interesting for characters to talk about, or would be the most relevant to the player.
Things like talking about a recent story, changing the barks based on the time of day, or the weather.
Even target health, miscellaneous events, have they killed an enemy recently, have they lost a teammate.
The NPC team made a debug display to reveal which tags are active on NPCs and on the player, or on a cover prop such as a car, crates or dumpsters.
Here's a clip from another test environment.
And we found that enemies didn't really indicate enough threat to the player when they targeted you.
So if the enemies could call their target out by some kind of identifier, it would be pretty cool.
We thought about using player customization, hats or the size of the gun or a new gear.
But as we add more customization to the game regularly, we risked having to record new lines with all of the combat actors.
And there were a lot of actors.
our game being heavily dependent on taking cover, it made sense to involve the cover props in our box tags.
With the help of the Environment Art team, we polled a list of the most commonly placed cover assets, which we categorised in the first round.
Big thanks to Miki and Sonja and the props team for their help with that mammoth task.
This gave us a shortlist of the most common cover props.
And the NPCs could then call out as they flanked, attacked or searched for somebody hiding behind them.
Here's an example of the car tag in action.
Behind the car! He's flanking us!
Get out of there!
The result? The player feels singled out and special.
It gives you a bit of a thrill to hear them spot you when you think you're being sneaky.
The other big point of context for us was how people talk differently alone to when they're in a group or with company.
If we make 10 barks for a stimulus, 5 of which can only play when the player is alone, and 5 when the NPC is alone, and 5 when the NPC is in a group, in either case we only have 5 lines to choose from.
If down the line a design change means that the NPC is used alone a lot, in missions for example, then those 5 alone barks will get heard way more often than the other 5.
If instead we don't tag 7 lines with any group information, and then they can play whenever, and we tag 3 with in group only, when they're alone we have 7 lines to pick from, and when they're in a group we have 10.
The advantage...
is it gives us maximum variety for the same line count and it provides us with plenty of options in either case and it is more efficient. The disadvantage is it makes three of them rarer and introduces new writing challenges.
Take a specific example, reloading. Why should the NPCs say they're reloading?
To inform teammates that they're going to be vulnerable.
If they're on their own, what should they say?
Is this still a moment that we want to signpost to players so that we tell them that they're going to stop shooting for a second?
The answer is yes.
So we still want to give the player barks to react to, but the NPC is alone, they're now talking to themselves, their weapon, the enemy, or out to the world at large.
And the writers got quite creative with these.
Here's an example from the closed beta when tagging was still a work in progress of why it's important to get this right.
Oh I'm hearing reloads.
Very tactical of you to scream out when you're reloading.
He's by himself. I need to reload.
Oh I feel so bad for that guy.
Here are some writing examples.
Enemy down is a stimulus that triggers when somebody has killed an enemy.
by using the group tag on the lines, we can set the script up for the writers so that they then know what context they're writing for.
Let's run through some of these and look at why they can or can't play.
So here, the line is, got you.
They're talking to the dead enemy and it works well in all scenarios.
There is no tag, so it can play in any scenario.
Did you see that one?
They're talking to their teammates.
It needs a tag to say that I'm only in a group, when I can say this line, and I'm with my allies, so I can play this line, yes.
I need to be faster next time.
I'm talking to myself.
Works in any scenario.
There's no tag on this.
It can play regardless of whether I'm with friends or not.
And it will play if I'm alone.
And hey, I got him.
I'm definitely talking to a teammate here.
So it needs a tag, and if I'm not with friends, this line cannot play.
So, wrapping up this section, let's return to this clip from earlier and break down what's happening.
The player kills an enemy, and another enemy responds, friendly down, showing awareness.
The player uses a health pack and the enemy starts to call, that agent's healed, but is shot mid-sentence.
Any other player would still have heard this line stop here at the same time.
A barely audible enemy is also shouting a taunt in the background.
A friendly civilian NPC arrives to support and they see the player and say, told you they're here to another civilian teammate, because they're in a group.
The civilian starts to attack the enemy shouting insults and using their faction name True Sons.
This shows that she knows who the enemy is and it gives us some idea that she hates these guys.
This is because the dialogue system rolled a line with the tag target faction True Sons, which has a higher chance to play than untagged lines.
The True Sons start to panic and shout.
We've been flanked and they've brought reinforcements.
Another true son shouts, get back, which is a response to one of the first two lines.
Another civilian shouts, oh no, as she is flanked in turn by an enemy.
The flanking enemy calls out stupid civilians, making targets of themselves.
This line has the tags target out of cover, faction, civilian militia and group in group.
He's pointing out an easy target's attention to his team.
At the end there, another True Sun ran out of bullets and shouted, got to reload.
And the final True Sun wants to destroy the player's hive, which is there to revive downed friendlies.
And he orders, destroy that thing, but is killed mid-sentence by the player.
Some brief notes on our online experience, which I think is one of our coolest, yet most under-recognized achievements.
As mentioned in the Alex example, in the first game, most barks were played on the client and were not synchronized across players.
In the sequel, we want players to experience the same voice lines, regardless of location or language.
The NPC AI team move the entire dialogue system from the client to the server.
This now does the heavy lifting of VO bark triggering and instructs the local client which voice lines to play.
This also works hand-in-hand with localization, allowing people to share the same adventures in different languages.
It opens up server-side updates as a way to tune the system once it's online, making fixes quick and simple to do.
And it gives us access to central, anonymized statistics about Barks playback, because it's not coming from the player client.
Here's another closed beta video of a team enjoying the bugs as a group.
God damn it!
I'm pinned!
God, I love the swearing.
Oh wow, good shot!
I'm already 95,000 damage.
You didn't even let him finish his sentence.
God damn it, I'm pinned!
Brother, I am pinned here.
I'm pinned.
I am pinned.
And the voice production.
I'm pinned.
I'm pinned.
I'm pinned.
I'm pinned.
I'm pinned.
I'm pinned.
I'm pinned.
I'm pinned.
I'm pinned.
I'm pinned.
I'm pinned.
I'm pinned.
I'm pinned.
I'm pinned.
Our development cycle looks like most games, starting with design, writing, going through implementation and testing, and then casting and the studio.
With an ambitious scope and aggressive schedule, we needed to optimize our recording process where we could.
To achieve the recording of our 65,000-odd barks for more than 189 combat characters across a summer, we relied on a well-trained team of voice directors, engineers who supervised and ran the recording sessions on a micro level and on an editorial army put in place to deliver the results to us here in Sweden.
And most of our recording sessions were run remotely.
It's not as fun, it's not always as smooth, but it's green, it's safe, and it's efficient.
Although the director was usually present at the studio as well.
Due to necessity, we've worked in lots of different studios over this project, so consistency is always a challenge.
We send reference clips for the actors to make extensive recording specification documents, though we always have to make some adjustments to suit the studio.
Ideally, we'd have recorded everything in one place with one director, one engineer, and so on.
But with our aggressive schedule and actors all over the world, we became very good at adapting.
Skype for Business and other call services became our necessary ally. I even wrote an etiquette guide for our team so they had a structure to our calls and sessions, which is a whole other GDC talk in itself.
We had a more or less open talkback communication in the sessions so I could talk to the director, the actor when needed, the engineer, and in this instance they're all in the same room but we used a lot of different studios due to the amount of recording we were doing in so many different places. And microphones. To avoid losing takes and to suit the various ways dialogue can be heard in The Division 2, we used four microphones set up at all sessions.
If we'd had more shootouts at the beginning, maybe we'd have dropped the MKH-40, which was often roomy in the smaller studios.
Here we've got the MKH-60 at the back, and then the 40, which was our backup front mic, and Jed posing in the background there.
Then we've got the COS-11 LAV, forehead mounted.
and a U87 AI or U87, which is not pictured. I don't have a picture of that.
Having all the mics set up at all sessions became a bit of a burden, but it did save us a lot of time where we might have otherwise had to go back and re-record something or set up another microphone for another part of the script.
If there's air on a mic or an actor is off-center, then there's usually a perfectly good take that's a close match.
And the MKH-40 often served as a great backup for the 60 or for quieter lines where the 60 wasn't picking them up as well.
Microphone purpose.
The MKH-60 and the 40s are backup for shotguns.
They were backed off for in-world barks and mocap with booms.
And we've got the COS 11, forehead mounted, for all of our radio calls.
And the U87 for our special characters and any cinematics or voiceover.
Friendly combatants nearby.
I'd also like to say something about how many files were cut and sent to us by the editorial team, but it became impossible to count. It was a lot.
But the voice directors, and sometimes us, would call selects and alts in the sessions so that we didn't have to get every clean take and go through it all for delivery.
Otherwise we'd have never finished the game.
Sometimes we did have to go back to a session and ask for more alts on a line, but that happened so infrequently that that was a huge time saver.
Balancing All the barks that NPCs can say lead to a very noisy situation if uncontrolled.
Spam is definitely not engaging.
Our simple dialogue control system is used to prioritize and restrict the playback of the voice lines from being a spammy mess to a more natural cadence.
Using cooldown timers, we balance the ebb and flow of the combat barks experience, aiming to keep the listener engaged by hearing fresh content.
We also have a chance to play.
The server makes a dice roll every time it calls a stimulus, giving some barks a higher or a lower chance to play, and it gives us another tool to balance playback and reduce the occasion of the more memorable and specific writing.
Lastly, a priority system that we can set on a stimuli shines a light on the most important decisions that an NPC is making.
We can apply these rules at several levels.
At a broad level, just to start with, ensuring the NPCs don't all talk at once, or that we don't hear from one individual too often.
Then, more granularly, control of our contextual tags, stimuli, and voice lines themselves.
For example here, our NPCs love to chat about the weather when they're not fighting, so we put a cool down on hot weather.
to prevent them from complaining about the incessant heat, which is the most common weather type.
Even though this is exactly what would actually happen in reality, it gets a bit annoying in the game.
And we have situational priorities.
This is a dialogue restriction rule set that can prevent certain barks from being played after certain situations.
For example, we stop small talk after a firefight has killed one of their teammates.
The game server is populated by a living world system containing NPCs that exist and continue to operate, regardless of whether the player is nearby.
Their behaviour calls barks all the time.
If the player is here on the map, not to scale, then the NPCs that are nearby will appear visibly and move around, and their barks are clearly audible.
Here is a bark that the player might hear.
Another bark.
And all over the server this is happening.
But the player can only hear within this radius.
Much smaller than that actually.
So none of these barks actually get past the initial state.
Because Snowdrop is very good at managing millions of events and calls per frame, this doesn't add any overhead.
So, if an NPC entity is placed on the server and it barks and no one is around to hear it, does it make a sound?
No, we early out.
With the data analytics team we collected statistics from the server and used that data to inform balancing decisions in the dialogue system.
This provided us with lots of data to sort and filter.
These are some of the data sets that we collect.
barks per MPC faction, location, whether it's an environmentally contextual line or not.
Through filtering and analysis, we could identify and balance barks that were triggered too often and using cooldowns on specific stimuli or lines and via our other controls could reduce that.
Likewise, we could identify barks that we wanted to increase the frequency of and attempt to improve their playback rate.
Some statistics from the Open Beta.
The most commonly played stimulus is Reload, playing roughly 745,121,000,000 times to players across all servers during the Beta. And that wasn't for a very long time. Even the least common stimulus, Faction Training Mentoring, played around a million times during the Open Beta.
There were a total of 645 different AI stimuli that NPCs could use at the launch of The Division 2.
This accounts for around 65,500 voice lines in one language, of a total of 73,500 including the story, cinematics and radio.
This is split across 288 characters, of which 189 of them were combat characters.
If we shared this evenly, it would amount to an average of 228 lines per character.
But in some vendors and gatekeepers, we only have a handful of lines, whilst our largest characters in terms of the line count are the combat civilians, and they have around 630 barks each.
And the smallest...
was a guard with only 11 lines.
This was indeed a massive project, especially when it came to localisation.
So I have to drop a little nod here to the team who have done a fantastic job of maintaining the standards that we set for English.
For the Division 2 release, we localised into 17 languages for text.
And, including English, we had 10 languages that were fully voiced.
Our team translated 547,000 words per language, which is about the same as the Lord of the Rings trilogy and The Hobbit, reaching a total of around 6.5 million words translated across all languages.
We recorded over 73,000 lines of VO per language, for the total of 662,000 recorded lines.
1,217 actors recorded for us for the launch across all the languages and we had 57 localization testers during peak time.
And since launch, after a year of updates and with our new DLC, Warlords of New York, the game grew a lot.
We have now got close to 900 AI stimuli, which accounts for around 78.5 thousand lines of barks alone, not counting the story and the missions.
That's just barks.
This is split across 364 characters, of which 217 can fight.
And the total voice lines in the game at present are 93 thousand VO lines per language.
that times 10 for all the other languages. The total words are 477,000, so that puts us somewhere between the original Lord of the Rings novel or Two Order of the Phoenixes for younger viewers and Atlas Shrugged, although we're still a full 110,000 below War and Peace.
use subtitles. Around 30% of our players use English and localised languages use, about 20% of our players use localised languages, subtitles. And then the other half of our players don't play with subtitles at all.
So, conclusions. In closing, behaviour.
We ended up recording much more than we actually ended up using due to tight iteration time, so there was a little bit of wastage, which you should plan for.
Statistical analysis helps us to identify and remove redundant data, and we're still finding uses for the barks that no longer had a purpose at release.
We recorded barks for tanks being aware of grenades when the NPC team made the tanks not.
Thanks for watching!
reactor grenades. We had enemy too close lines for snipers that virtually never played because they're always at range.
We went big and then we went smaller with context.
Contextual Bach's played more in some areas and less in others, meaning that some are still very rarely heard.
This gives our players a thrill when they hear a line that they've not heard before after around 300 hours of playing, but we needed more iteration to finesse and balance them better.
We have since tagged more objects to increase the chance of hearing the cover proc barks, and we've simplified the tag usage in our latest DLC to focus on the most common and obvious contexts, such as the faction, weather, and time of day.
Not many people have noticed our innovations in synchronizing barks across the server.
This to me says that it is so obvious and fundamental to our shared enjoyment that it should always be like this, and it's necessary.
We can't all sit in the same cinema watching different films and then expect to be able to relate to each other's experiences afterwards.
This brings us together across borders and across languages.
And using statistics helped us to find issues with the dialogue system, but also with the game as a whole.
When barks for the wrong character are being heard in a mission that doesn't have that faction, you know that an NPC boss has been given the wrong barks character, and now sounds like a huge armoured tank instead of a stealthy sniper.
If we'd devoted more energy towards fast analysis of the statistical data that we were fed, we'd certainly have been able to find...
and make better decisions about what not to record or have spotted stimuli that were not working sooner.
And those bugs, that when they were fixed later, could potentially unbalance the vocal landscape when turned on. Next time I'll be asking for a system like this from the start and making time to analyze the data after every internal play test.
and iteration. With the ambitious schedules, iteration time is always going to be tight.
But with an online game, your product is also never really finished. This gives you opportunities to polish and improve your product after release. And with clever tools such as server-based AI box control, the job of patching can also be made simpler and easier.
So what can I do with this? What can you do on Monday?
Look at your barks as more than just background filler.
They're world builders and they deserve at least as much attention as the main story.
Weave your barks into the setting better. Why is the angry person angry?
Investigate how contextual triggers could enrich your barks.
What are the three biggest conditions that affect your NPCs?
Consider how you're planning to balance your BIOCS experience.
How are you going to get data?
And find ways to save time and resources by recording actors remotely.
Learn to embrace it and find methods that work for you.
It's good for the planet and it will save lives.
And I'll finish with a broad thank you to everyone who worked so hard in making this game great for our players and for our community.
Thank you for listening.
