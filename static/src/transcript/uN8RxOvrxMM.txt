My name is Dominic Vega.
I'm a senior sound designer at Avalanche Studios.
And I was on the sound team in our New York City studio for Just Cause 3 and Just Cause 4.
Among other areas of content, I was responsible for the creative and technical mix of Just Cause 4.
At Avalanche Studios, games are made in our proprietary Apex engine.
And the audio middleware we use is FMOD Studio.
While we work in FMOD, a majority of what we're going to talk about today is openly available and applicable to other audio tools on the market.
Now, in designing Just Cause 4, some new ambitious features were added that changed both how we needed to functionally approach the mix and how we were going to join everything in an aesthetically engaging way.
Before we go a lot further, I want to contextualize a bit of what we face as sound designers on a Just Cause game.
For those of you more unfamiliar with the tone of the franchise and where Just Cause 4 was headed, here's a little peek into the world of Just Cause and what we were aiming to create.
Welcome back!
So while the gameplay and the tone of our franchise is really bombastic and over the top, we do actually put a premium on nuance in our approach to sound design.
Subtle details and dynamics are really important to us, and something that we feel makes worlds really great to be in.
Now to bring all these elements of an open world together, we place a strong emphasis on the game's mix.
For Just Cause 4, we wanted the mix planning and prototyping to start at the design phase and grow and improve with the rest of the game.
What we didn't want was lots of huge last-minute changes going in, as is unfortunately common in a game's production.
These sort of last-minute mixes risk masking fantastic sound design and music moments to better suit bigness or finality.
To do all this, we approached the mix on Just Cause 4 as its own separate system, instead of as an afterthought in the finishing process.
We started establishing pillars which would become guidelines in the framework of our mix.
Now out of the gate, we prioritized balance.
This is an early first step, setting this overall static mix of the game, and determining aesthetically where we want specific packages of audio to be set at.
We have thousands of assets we want to deliver in an intuitive way, and we want to control and maintain balance of the overall sonic weight of the mix.
We want to emphasize clarity, and adding a layer of focus to what the player may be most interested in or interacting with.
We never wanted to lose the punch, keeping the sound of the music hitting hard while retaining dynamics and economy of scale.
And finally, style.
We wanted to infuse a little bit of the Avalanche DNA into the overall audio experience, delivering something unique and that highlights the fun of the franchise.
We also wanted the mix to have brains and modulate more accurately based not only in the sound being made in the game, but by the game's design itself.
So now that we have our pillars defined, let's look at some of the problems we encounter when trying to mix a Just Cause game.
Firstly, we're never able to gate the player's experience to deliver more nuanced content.
We have to be completely prepared to make the most intimate dialogue sequences be audible in the most chaotic scenarios we could find.
Now, in Just Cause 3, which was using more of a dynamics-driven mix structure, sound designers working in assets in low-priority buses often fought against the mix to be audible.
This led to really hot asset designs and audible pumping as the dynamics have essentially been removed.
Also, relying on compressors for a mix, is not only expensive in terms of CPU and processing, they're often one look.
We often found ourselves either over or under compressing, but worse, the mix is rather unintelligent in making space for assets as compression can only yield some very small specific results.
Now, a new project with a fresh start, you guys would have the benefit of starting your mix just needing to imagine what your last day of production looks like.
What's crucial in designing a mix is deciding what framework you want to build and ask necessary questions of what your mix can do to better serve your game.
So for us, to better serve the design of Just Cause 4, we knew we were gonna have to rethink the mix process.
We had only just finished the mix on Just Cause 3 a few months earlier when we moved into planning for Just Cause 4.
So instead of putting the mix off and relying on what we had for a few years, we decided to attack the mix head on.
Let's take a little bit closer look at our first pillar of the mix.
Now, early in the project, we had this really long runway without a lot to test out.
We took the opportunity to suss out where we wanted basic levels of mixer groups to sit in the final mix, and investigate how to remove layers of processing between a sound and the listener.
With so many assets in the game, we needed to parse out and silo content that was designed for player interaction from what was more diegetic in nature.
We could set up pseudo mix priority buckets, then leave room for distinct sounds the player maybe more locked into experiencing like driving a car and so on.
In doing so though, we expose some issues with traditional audio signal flow that arises from our gameplay model.
Here is a simple vehicle engine event from Just Cause 3.
Now, due to the fact that you can hijack a vehicle mid-drive in Just Cause, unlike other games that might allow some more creative content swapping, we allow the player to hijack a car while it's running, throw the driver out, and resume driving.
In 3D space, we may want ambient cars and traffic to be in a lower bus, where guns, explosions, and destruction will push down on them in the mix with sidechains.
This is completely logical until you enter that vehicle.
In Just Cause 3, we solve this by just boosting the level of the engine based on a parameter called isPlayerDriving.
This, however, causes low-end buildup.
It still pumps against louder sounds.
And the max distance is stuck being as long as the furthest camera distance from the player in order to make the sound audible in 3D space.
The solution, in order to silo content in Just Cause 4, was descended around the signal flow based on those same parameters.
Moving an FMOD return bus up in priority so audio is sent and bypasses output ducking to a higher priority group allows greater control, less DSPs, and easier intelligibility.
Since the player can only control one vehicle at a time, we can easily set up mixing DSPs reserved for player vehicles and resume and bail out of those states as quickly as the player chooses.
Now let's expand the idea a little bit further.
By doing the same thing with the component parts of vehicles, like tire interactions, chassis rattles, transmission whines, physics interactions, and exhaust, you now have every level to all assets involved with the vehicle in their own returns.
We can then put a snapshot on every vehicle's event, just checking for the condition isPlayerControlled to become true, which then triggers a microstate for that individual vehicle should the player enter it.
This snaps the mix settings for all available layers and groups we want to mix uniquely for each vehicle.
This also allowed us to take most areas of vehicles excluding engines and use more global content, saving on performance and memory.
Let's take a look at some examples of increased sonic variety with more general use complementary sounds just using modulated mixes.
Now next here, every layer except for one is exactly the same as before.
What's different is pitching, spatialization, and mixing of the component layers of the vehicles.
Quite commonly, we were able to get away with making extremely small changes in a singular layer of content and achieve really drastically varied sounds.
Routing this way with precise control over the output of individual layers and using microstate snapshots, which we're going to dive into a little bit later, for each specific vehicle, reduces the authoring time of a single vehicle mix from several hours each to only a few minutes if need be.
The content being siloed, available, and largely unprocessed gives the mixer the necessary freedom to focus solely on making this singular instance sound great, without needing to worry about the other 100 plus vehicles that need mixing down the line.
Once content was more siloed and accessible should the player interact with it, we started cleaning up layers of the mix through imaging.
Where we situated sounds and surround space has as much impact as amplitude does on output.
Since we're mixing in 7.1, clearing up that space has trickle-down effects through the fold-down mix.
As states become more balanced and controlled, cleaning up substates and behaviors to disallow output on speakers let previously masked audio come through the mix.
To show some examples of outputs to major groups, you can see how more specific output paths enable space for a more detailed soundscape.
Let's take a closer look at some examples.
Now, while a really low priority sound, world audio and ambiences play a big role in games like ours and set a lot of the audio tone.
They're a persistent character in our games, but in order to make room in the mix, we keep them extremely wide.
There are two layers, the core ambience and the details.
Details are spawned at random points in the point clouds.
The player has this effect of moving through them, and the core is a quad ambience rotating around the player's head, making the image feel more varied and alive.
Land vehicles present a challenge for a lot of games, as they are a very noisy and full-frequency source.
Their placement at the end of the mix chain is almost a full 7.1 image of being driven by the player, excluding the center channel.
If broken out, though, they can have up to nine component player-specific sends that all use different outputs.
Here's a deconstruction of how all the individual layers of a vehicle's sound are imaged, and how they sound all put together to increase clarity and get rid of audio clutter.
And lastly, player weapons are 5.1 with discrete subwoofer content.
They're big character sounds, so we want to reserve quite a bit of space for them.
However, to retain clarity, keeping layer separation a priority kept the weapons from sounding muddy or just simply loud.
We only allow the mech layer of guns into the center channel.
The close and medium layers are front heavy stereo spread based on whether you're aiming down sights or firing from the hip.
And tails are a stereo asset that expands and spreads to quads as the shot blooms out.
So now with a more granularly compartmentalized mixer and channel separation in mix packages, we could tackle improving the perception of size and scale in the game and make the mix punch.
Now, a really obvious statement I'm about to make to everybody in this room, you're probably well aware, is that low-end management is important in any mix.
Now, especially for us in Just Cause 4, it's something we paid a lot of attention to.
We have to accommodate barrages of explosions and gunfire and keep it from becoming muddy or too bottom-heavy.
We also want to maintain a decent dynamic range in the mix that doesn't feel overly compressed.
As with guns, we opted to use discrete LFE for surround mixes in Just Cause 4, and not force sound designers to concede on their setups.
We designed discrete content for medium and large gunshots, and small, medium, and large explosions.
And in the end, we still reserve the option to send any sum of an event's mix to the sub should we need to.
Removing all subwoofer content makes mixing on a surround system more informative as it's easy to rely on the sub as a tool in your handling of your bass designs.
But by removing it, and especially removing all sums to the subwoofer, you're forced to decide where it's desired and manually add it back in.
But why did we opt for using discrete LFE content instead of just a simple frequency roll-off to the subwoofer?
For us, we found it cleaned up the subwoofer immensely, but not allowing more in than we wanted.
And we could design cheaply by making LFE effects for weapon types instead of per weapon.
Most of all, though, sound designers were never forced to make concessions in their designs to accommodate all playback systems.
In the past, we've had issues where a sound worked great over headphones or full-range speakers, but with a subwoofer, it became muddy or maybe not pronounced enough.
You typically only have an LFE slider in most middlewares, in events and output buses, so you can only increase or decrease its overall volume send to the sub.
Now, in this case, we sat with a consumer-level subwoofer and designed our sounds playing through that.
Simple, low-pitched, sign-based sounds without a lot of upper harmonics worked really well through its low-powered output and sounded really crisp and tight.
Now, a lot goes into maintaining the low end of a mix, which could be its own 30-minute talk, but first accounting for where low end could and wouldn't be handled on the highest end of systems paid dividends as sound designers had more reasonable expectations at the sound design phase of content.
This just simply empowers more people in the audio team to positively contribute to scale and a punchy mix without over-complicating the process.
Now our last pillar was style.
Style was actually where the mix became most fun.
With the previous priorities in place, we had a really solid foundation to now modulate the mix in really creative and unique ways.
Find space for assets previously deemed too low of priority and contextualize the surrounding audio in ways that best fit our game's design.
This would lead to us designing our adaptive listening mix.
Now I said earlier that we wanted to make mixing simple.
At this stage, the mix has largely been an exercise in deliberate voicing and subtraction.
These are important stages, but with games, we're also accommodating for focus and feel.
These are harder to quantify, and as we continued with the mix, we wanted to strike a balance between prioritizing simple locked-in experiences where the player's action is more easily known, and maintaining interest based on the surrounding context of these locked-in states.
With the existing framework and a shippable static mix in place, mixing had now become simple, but that doesn't necessarily mean that it was easy.
Now, working with designers and teams on game features opens up a lot of avenues for discussion and debate.
In the early stages of remaking the mix, before the idea was fully realized, extreme weather was being worked on really heavily.
I had these scheduled sound design reviews with the product owner of the feature, one of our lead designers, and he kept wanting more from the tropical storm in this review.
I was a bit stuck not wanting to overpower the player with wind and rain and lightning to an exhausting degree.
A bit frustrated as this review was not going very well, I also said offhand, I don't know how I'm gonna get enemy gunfire over this mix at this point.
I had now lost his attention completely, and he was on his phone scrolling through emails, and he was walking out the door, and he sort of just said, I don't know, I don't really care about that, I just want the Stormpicker.
It was actually a really great point, it was kind of a light bulb moment for us.
We needed a smarter, stylized mix to deliver a great sounding experience that could break some more traditional rules of priority.
But before we move into some of the more creative solutions we used, let's talk about how we functionally began the process of reworking our mix logic to support style.
An early course of action we took was to remove all bespoke narrative mixes.
We then started prototyping dynamic states and microstates that would allow us to blend between mixes seamlessly and deliver on a unique and always-changing soundscape.
We then removed most compressors and dynamics-based mixing DSPs from our mixer, along with side chains that previously determined priority audibility.
Now, since the inclusion of front lines and extreme weather kind of rocked the way the player perceives things like enemy gunfire, we took wider liberties with reprioritizing enemies firing at you whenever we could in favor of something more threatening or interesting that we deemed worth forcing your focus.
Now, to clear up some terminology, states and microstates are just mixer snapshots in FMOD Studio.
Like other mixer states available in almost all middleware, these are just simple terms to account for any player interaction or action and how we want them to sound.
Now when we talk about states specifically, we refer to settings triggered more globally.
Things like players inside Tropical Storm is a state triggered when you enter that storm.
Some other examples of states are car chase combat, exploration, dynamic combat.
A microstate is a snapshot that's triggered with sounds in the world in 3D space.
They can be equally context driven, but most likely the intensity is simply faded in the closer to the sound source you get.
An example of a microstate is enemy AI is targeting Rico.
This microstate sets some overall levels quickly and is played when enemies are firing their guns at you.
Microstates were really effective in creating space more intelligently and making important or loud sounds in the world stand out.
Adding logic, like direction, simply lets us make vehicle explosions more focused and clear, as we disallow the microstate if the player isn't looking at it.
This isn't possible with just sidechain compression, as that looks at a pure amplitude output and ducks accordingly.
With this taking no compressor DSPs, no side chaining, and no metering, we get improved punch with better performance.
Now keep in mind as the mix is now very separated and routing, our granular control lets us even make changes to EQ and balance if certain moments become too overbearing.
Some other examples of microstates we made were AI is targeting player, enemy vehicle is chasing player, enemy helicopter is engaged with the player.
So we have a baseline for what states are and some examples for their use.
How do we use states to attack the problem with a tropical storm?
We want to enforce the design that storms are all powerful while not drowning the player in sound to an exhausting degree.
Where the mix became most stylized was when we exercise one of the most powerful tricks in setting unique mix states, modulating between states or within states themselves simply because we can.
Since data we use to set the mix and other aspects of audio is already available, we can insert some chaos and randomness into the experience to account for fatigue, varying playtime or scale. For these persistent storm events, we opted to use time as the main modulator.
In the tropical storm, we set the level upon entering the storm quite loud.
We wanted to feel overbearing.
But after 15 seconds, we start fading the base of the storm down by about 10 dB.
We trade off and start mixing the thunder and lightning up to compensate.
And after another 10 seconds, we lower the lightning strikes and only then do we allow enemy gunfire to duck anything inside the storms.
Mixing like this lets storms feel chaotic and oppressive when you enter them.
They're extremely disoriented when you're getting struck by lightning, but eventually some more traditional room for hierarchy can resume in the mix.
In Sandstorms, we mute the player's gunfire tails to make the storm feel claustrophobic, boost the barks of the enemies should you not be in combat as they're yelling, searching for you, and track the direction of wind to balance the weight of the sound design of other noisy assets to keep the mix image even.
When the player is nearing the tornado, we mute the player's car engine as the tornado base is really full frequency, and we want it to feel like the player's cars are losing control near it.
Now one state that was most complicated and dynamic was the dynamic combat mix state.
Easily the backbone of our mix, it managed streaming asset counts and handled various sizes and types of combat experiences.
One of the first functions it performs is it scales the mix of ambiences and surrounding low priority sounds.
The more enemies, the less ambiences.
This gets noise out of the quad image and in a compression based methodology would be ducked by gunfire anyway.
We have the ambiences completely muted if the enemy engaged count is greater than 5.
This also dumps the need to stream those quad files, makes space for the music, and lightens the load on audio performance.
So if the player is to get into a car, we now, from the mix perspective, consider that a chase.
The mixer takes one second to fade to this new second layer of combat and changes levels accordingly.
In the case of car chases, we quickly duck the vehicle mix package down to alert you to gunfire, lower the rears of the land vehicle's group until the chase combat is complete, measure the distance between you and enemy vehicles chasing you, and inversely lower your engine and boost the chaser.
Then we raise the level of the rear speakers of the enemy helicopter's bus.
This really quick state change makes car chases less frustrating because you can hear all threats as they're out of camera view and briefly modulate the mix to be more rear speaker heavy.
Now in attempting a major rework of our mix, what did we learn and where did we maybe fall short?
Now, one aspect of this mix approach that makes it difficult is its heavily reliant on feedback.
Since we're still day-to-day game developers, I can't ever spend all my time and focus on the mix.
It was great that iteration time dropped drastically and sound designer's content was being complemented by the mix structure, but we needed play tester and developer feedback to address areas of the game we may not be looking at.
Aside from the sound and music being a synthesis to the visual and the experience, the mix can be a vital tool in aiding the game's design, but communicating that option to developers can be very difficult. __________________________________________________________________ Sometimes we had to dig through feedback, searching out ways that sound could aid in areas of the game being labeled frustrating or challenging.
Listening and conversing outside of our audio walls was crucial in gleaning insight into what developers and players expect beats and gameplay to feel like.
And lastly, one thing we can't avoid is that by reducing the layers of processing between a vehicle engine event and the listener, the amount of vehicle-based groups and returns alone in the mix are multiplied ten times over.
Processing-wise, this makes no difference, and in fact, more granular structures actually contributed to where most performance savings came from in the mix.
The mixer group count grew exponentially over the project.
Various areas of the game were similarly restructured like vehicles were, making the mixer quite large with hundreds of groups and returns.
The authoring of snapshots becoming so vital to the mix itself meant that an increase in workflow efficiency directly correlated to more states to create and manage.
On reflecting, we still felt all of this is worth the benefits and shows the extent to which granular control on a huge scale can look like.
So we removed layers of processing between our sounds.
We found space in our mix image to better complement and declutter the soundscape, and enhanced the sense of power and economy of scale.
All these steps act as apparatuses in our sandbox to reconfigure the way we prioritize and feed players sounds based on the surrounding context and our own internal stylistic leanings.
Open world sandbox games can present an enormous challenge to delivering a subtle and interesting mix.
The principles with which we mix though can be practically applied to any type of game.
By not treating your mix as an afterthought and pushing the limits of the tools you have, you can better integrate your sound design and music into the fabric of your game.
With that, whether you work on a large audio team or a one person shop, the foundation is set to use your mix as a powerful instrument for reinforcing your player's experience.
The things we haven't gotten into today like loudness, mastering, dialogue mixing, design changes, tone, all these are important and worthy topics, but the framework and questions we've established can better pull solutions to the above problems into focus.
I truly believe that mixing should be simpler and perform both a functional and aesthetic role.
What I hope is taken away is not only the importance of your final mix or improving your skills as a mixer, but the importance of the questions you ask when you're designing your mix, ensuring that you're always mixing for the games that you're actually making.
Now, before we break for questions, I do want to take a few seconds to thank the entire development team at Avalanche Studios for helping get this game out the door and with this presentation, both in Stockholm and New York City, as well as the entire audio team that was really, really important in helping us ship this game.
Thank you so much for taking the time and coming to this talk.
Have a good GDC.
If you have any questions, we have a few minutes now to go over stuff.
That's it.
Hello. Well, first of all, thank you for the super interesting talk.
I was wondering, well, you said you mixed it in 7.1.
Yeah.
I was wondering if you did like separate mixes for stereo, 5.1, 7.1, headphones, this kind of thing.
Yes, we did. We mix and review in every format, but since 7.1 is kind of our master, that's the back of the box feature, that's what we're kind of...
aiming for. As far as fold down, there's obviously fold down steps at every layer all the way down to stereo that are quite complicated to go through and a little bit of hidden magic in middleware. So we do, that stuff does matter, but we didn't really, the The spatialization and imaging thing starts at 7.1 and moves down the mix.
Okay.
And another question.
Does the, I didn't play the game unfortunately, does the player have a lot of control over the mix?
Like do you expose like dynamic range settings, this kind of thing?
Now it's something that I think is really cool and something I'd love to do in the future.
The only controls in the game that we have for sound are the more typical VCA sliders, sound effects, music, volume, stuff like that.
There's other areas of mixed things that didn't make it in that would be good, like accessibility features that are in there, night mode, those are not in there.
Those are all just states and microstates that didn't get made.
But yeah, no.
Cool, thank you.
Yeah, thanks.
Hey, so you mentioned that going from Just Cause 3, you had a lot of heavily compressed assets on the way in, which made it a little more difficult to mix.
Did you have any guidelines for dialing that back? Did you have loudness standards for the assets on the way in to help it pre-mix a little bit before you even got it to the engine?
Yeah, I think I may have failed to explain that well enough.
I think what we experienced in Just Cause 3 in the final mix is something that I've experienced in games I've mixed before.
You start seeing dynamic range disappear as you're finishing the game, as you're doing these mastering passes and things are getting pushed together.
But also the fact that when you're doing sort of a strictly priority-based mixing structure where dynamics are involved, If you're a sound designer stuck working on sounds in the lower bus, you can never hear your work.
So they start pushing it up in volume because they're trying to focus on it and make it sound really good.
So that's what we were trying to solve there.
I think it's not really an issue of the content not being dynamic.
It's that they're pushing up against a mix that's fighting them all the time.
I've seen that in quite a few games where...
You'll see the side chain almost always active on the lowest bus because there's always something higher up pushing on it.
So I don't know how anybody could ever actually sit there and reasonably mix their content.
If you're the ambience person, I don't know how you could do that.
Okay.
Yeah.
Cool. Thank you.
Oh, and we did not do content mastering guidelines.
That's not something we do.
Any other questions?
For coming out.
