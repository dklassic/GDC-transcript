Hi, everyone.
Thank you guys for joining us for our presentation.
Obviously, we're very sad that we won't be able to meet everyone in person, but still very excited to deliver our presentation today.
My name is Jeff.
I'm a lead product manager here at AI Unity.
I'm joined here with Dr. Deng, who's a researcher also at Unity, and Robin Lynn Nielsen, who's a co-founder of Keri Castle.
Today, we're going to be talking about specifically using deep reinforcement learning and testing in MPC development.
So you may be asking why Unity at the GDC ML Summit?
Here at Unity, we see a variety of games, whether it's 2D puzzle games all the way to first-person shooters.
So for us, we see a lot of studios trying to implement deep reinforcement learning using machine learning in their games.
So one of the things we want to provide today, some of the aspects and learnings that we see at Unity.
So first we want to start out, why are so many students trying deep reinforcement learning?
Well, first they want to ship games faster, less code, make sense, spend less time on testing, and of course maintain the quality. So it's one of those situations where, you know, we want to have our free lunch, we have our cake, we eat it too, and then you sort of insert your favorite idiom.
And this is really the core of why a lot of students are trying this approach.
So there is a lot of poems applying deep reinforcement learning in gaming.
It can actually solve quite a bit of challenges.
But we should be mindful about some of the difficulties.
And really, that's kind of the hardware we want to talk about today.
A lot of the challenges that studios face when trying to implement deep reinforcement learning.
So how are studios using deep reinforcement learning?
Well, the first is from the player perspective.
Now, this is creating some sort of player bot or virtual player.
And this is usually for game testing, game balancing.
The second is from the non-player perspective.
So these are.
And these are enemies, can be companions, can be passerby characters.
And the last one we call the invisible scenario.
Now these are the scene itself or other experience you've not seen by the player.
So this is where the reinforcement learning is trying different things for the player.
And they're in content generation, difficulty tuning and player engagement.
So really today we're gonna focus on these two use cases.
This is really predominantly where we see quite a bit of studios trying the game testing and creating enemies area.
In game testing side, one of the things that we do constantly see is that students are trying to use deep reinforcement learning to test new levels. So this is just some sort of generalization of a player bot to test new levels or content.
The second is around enemies. Now this is like natural looking enemies. Now these think about like making the enemies you look and feel real without having to write a lot of code.
So really that's what we're gonna focus on today.
You know, testing new levels, you know, in content using RL and then obviously natural looking enemies.
There are obviously many more scenarios and a lot more use cases, but today we're gonna focus on these two.
So with that, I'm gonna pass it over to Ervin.
So we're talking about reinforcement learning.
Thanks Jeff.
So, To quick recap, what is reinforcement learning?
So many of you are probably already familiar with this.
But if you're not, think of traditional machine learning.
It's taking an input, maybe it's an image of a cat or a dog, and learning how to map that to an output.
And this output could be like a label, right?
Like, is this picture a cat, or is this picture a dog?
So reinforcement learning isn't too different.
Let's imagine you want to train a behavior for an agent in a game.
You collect a bunch of observations in your game.
This can be things like the agent's location, its velocity, how close the enemies are.
And then the agent basically learns a policy, what we call a policy, that takes those observations and maps them to actions.
So just like the classifier would take an image and map it to a label, it maps observations to actions.
And these actions could be, for instance, moving around in the game, shooting the enemies and doing other actions, right?
But how does it learn how to map these observations to actions?
And this is where reinforcement learning is a bit different than traditional machine learning.
So in reinforcement learning, or RL, the environment, which in this case is your game, gives a reward to the agent when it does something desirable.
So through the agent playing the game, exploring and trying different actions, it learns how to maximize this reward.
it receives during a playthrough of the game.
And we call this playthrough like an episode.
So this is really good for creating behaviors in games because you don't have to explicitly specify how the agent achieves what you want.
You just give it a reward when it does.
And it kind of, through exploration, figures out a winning strategy.
So, Let's say you wanted to use reinforcement learning to create a bot for playtesting, right?
So you want a bot that you can use to test and balance your game.
And reinforcement learning sounds great because you don't have to hard code the behavior.
So what does this bot have to be able to do?
It has to, first and foremost, obviously, be able to play the game and win the game, just like a human player.
But the bot also needs to do a couple of other things.
First, it needs to be able to deal with changes in your game.
So if you're tweaking your level, it needs to be able to deal with these changes and adapt.
It needs to be able to play new unseen levels to evaluate them, things it hasn't seen in training.
Otherwise, why are you testing, right?
And it also needs to solve the levels in a human-like way and not find some exploit to get the reward but not actually play the game the way you want it to.
So.
To give a concrete example, let's take the game Snoopy Pop from Jam City.
So Snoopy Pop is a bubble shooter.
You can see on the slide, there's a screenshot from the game and you see that Snoopy is holding a red bubble.
And the objective of the game is to shoot the bubble at some angle up to the top of the screen.
And if you get a match in color, it'll clear those bubbles off the screen.
And you win the game by clearing enough bubbles and enough of the special bubbles with the yellow bird inside of it.
So what GemCity needed from a testing bot was a bot that could play all of the existing levels of SnoopyPup with similar performance to human players and be able to play newly designed levels and determine A, if they're solvable and B, how challenging the level would be to a human player.
Would the player need to use power-ups?
Would they get frustrated and quit?
These metrics would be really invaluable to the level designer while tweaking the level.
So this seems simple enough, but what makes this game especially difficult to train a bot for?
Well, first of all, there are over a thousand levels and they all have different layouts and different ways of solving them.
Furthermore, each level is randomly generated, so the position of the bubbles and the color of the bubbles can change from playthrough to playthrough. So that means if you scripted a bot, or if your RL bot simply memorized the actions it took to solve a particular level, it would not be able to.
And to make things even worse, every couple of levels, a new element is introduced.
So these could be different obstacles, they could be different power-ups, they could be bubbles that explode in different ways.
And your agent would have to learn how to deal with all of these things.
So how are some ways that you might try to approach this problem?
So we could take the naive approach and we could say, hey, each level is a separate game.
We'll just train a different agent to play each level. And this will work, but because now you have thousands of levels, you have thousands of training runs and thousands of neural networks that you're creating. If each training run takes several hours to a day, this could get really expensive really fast. Not to mention, Having to retrain a bot for each new level that your level designers create makes them all separate and you can't really directly compare the performance of one bot to another because they're different bots.
Secondly, we could also train the agent the way a human would play the game starting from level one, then once it masters level one, train on level two, then once it masters that, three, and so forth.
If we do this though, the agent is likely to forget the things that it saw in the earlier levels.
And in machine learning, we call this problem catastrophic forgetting.
And basically neural networks have this issue where when it's learning something new, it tends to quickly forget things that it has seen in the past.
Humans don't really have this problem, but we do have to worry about this in reinforcement learning.
Finally, if you imagine that for each of the levels that you've created, you probably have at least a couple human playthroughs of the game.
You could train a bot to directly imitate these human playthroughs and hopefully be able to solve the level. The problem is, as you remember, the levels were somewhat procedurally generated. There were some random bubble positions, there were some random bubble colors. And it's impossible to have human demonstrations for every single possible combination the agent could see. And so you can't just imitate what you see in the demonstrations because you won't have demonstrations for all scenarios.
So it's clear that no one of these approaches would work by itself, but what could work?
Now, we can't say we've solved all of the thousands of levels of SnoopyPup, but these approaches did prove effective in this game.
So if we want one agent that can play all of the levels, and we don't want it to forget earlier game mechanics, we will have to rely on domain randomization to keep the agent robust to variations in the environment.
In the SnoopyCloud case, this means being robust to new levels and different bubble configurations.
Furthermore, while pure imitation learning might not be a good strategy, having those demonstrations, having those human demonstrations, can really help, especially if you combine it with reinforcement learning.
It can guide the agent towards a good solution, but not prevent it from learning how to solve situations that were not in the demonstrations.
Okay, so let's talk first talk a little bit about domain randomization.
So it's a common technique used in reinforcement learning to make sure you're a trained agent can generalize how to solve a problem rather than just memorize very specific instances of your problem.
So the idea is that you find aspects of your domain.
So in this case, your game levels that can vary.
For instance, lighting textures layout of the level type of enemy and so forth.
And during training, you randomly vary these aspects so that the agent sees all of the variations.
Then the agents will learn to cope with or ignore these variations.
So I have three examples of domain randomization here on the slide.
On the left is a classic example from OpenAI, where they had a virtual robot hand trying to manipulate a Rubik's cube.
And it was learning how to do that.
And during training, they varied the background color, as you can see, the lighting, and the physics of the actual robot manipulating the cube.
And since the trained agent saw all of these variations and learned how to ignore the ones that weren't important and deal with the ones that mattered, it was able to eventually manipulate a cube in real life.
So in a game, there's no real life test, but this real life test is now instead training on a new level, right?
And it's kind of the same idea.
All right, so.
In a game, you can do domain randomization in a couple of ways.
For instance, if you have thousands of levels like you did in SnoopyPOP, you could randomly sample the different levels and treat that as variations in your domain.
Likewise, if you have levels that are procedurally generated, you can use procedural generation to create an infinite number of variations of that level.
So if you want to read a bit more about how to use domain randomization to train robust agents, I'll leave these links here.
And they'll also be available after the presentation.
OK, so let's talk a bit more about what did I mean by we can combine demonstrations with reinforcement learning to improve performance.
Well, so when you're training a reinforcement learning agent, the first thing you have to do in your game is define a reward function that describes what you want the agent to do.
Now, the easiest types of rewards to define are sparse.
For instance, you can reward the agent when it wins the game, right?
And give it a negative reward when it loses the game.
And this gives the agent the freedom to find the best solution towards that goal.
But sparse rewards are much harder to solve, as the agent needs to, through luck, stumble upon them.
So reinforcement learning is essentially guided random search.
The agent tries different actions, and it sees what kind of rewards it gets.
And it's very unlikely that the agent will randomly find a way to win a complex game.
However, if you give small incremental rewards, you may spend a lot of time tweaking these rewards, and they may lead to unintended non-human-like behavior for your player bot.
So what we can do to solve this issue, then, is to mix reinforcement learning with human demonstration data.
So rather than purely using the reward functions to train our agent, we can use some of the demonstrations provided from a human player to guide the agent's learning.
This can not only speed up exploration towards those sparse rewards, but also result in the agent solving a level in a more human-like manner as it chose to follow that path while exploring.
So here's two examples.
On the left, on the right of SnoopyPOP, we've seen that.
On the left is an environment that we've created where the agents.
needs to hit a switch and then grab a green block before it even sees any reward.
And it gets plus one for touching the green block.
Now, it's very unlikely, though possible, to find this by random search, but it's pretty obvious to human.
And just providing a couple of demonstrations, around 10, we can cut the training time in more than half.
And this is because it kind of pushes the agent to get the switch and to find the green blocker.
So this is a sparse rewards.
Even with dense rewards, like Snoopy Pop, where we do give a reward every time bubbles are cleared on the screen.
It makes sense.
Your score goes up.
Demonstrations still can help you reduce your training time.
So a similar number of demonstrations brought the training time down around 25%.
So why would this be worth it?
Well, in some cases, a few minutes of human playtime could save hours in training.
And in many cases, you may already have this data.
Maybe you as a designer have already played through the level a couple of times, or maybe there were human testers that did.
So you can do a bit more with demonstrations.
You can also solve problems that were not solvable using pure reinforcement learning.
A good case study of this was during the Unity obstacle tower challenge, where the competitors had to create agents that could play the platformer game shown on the screen.
Now, you can see that the agent basically has to solve this fairly complex block puzzle to pass this level, but push the purple block onto the switch and then go through the door.
This is pretty much impossible to find through random luck.
The agent will never figure out how to jump on that thing and then push the lock down and then go through the door.
So furthermore, the level is procedurally generated.
So every time you play, it's a slightly different layout and the blocks in a different place and so forth.
So it can't just memorize the actions it took to be able to solve the level.
Now, our winning competitor, Alex Nickel, used play throughs of the game to guide the agent's learning.
leading the agent to discover the winning solution.
So after enough iterations, the agent learned how to solve even the randomly generated levels that were not in the demonstrations.
Now, a little caveat here, it took way more than 10 playthroughs to solve this level.
In total, Alex used more than two days of human playtime to be able to solve the levels of Obstacle Tower that he did.
You can read a lot more about his approach, which I think is really cool.
And check out his code at the blog post link on the slide.
And again, they will be available at the end of the presentation.
So if you want to read more about how you can use demonstrations to help reinforcement learning agents learn in a timely manner, I'll leave these links here on the slide.
And again, they'll be available.
So something we've hinted on a little bit.
is that RL takes a lot of time.
Often it requires the agent to take millions of actions in a game to learn anything useful.
And this would be OK if it wasn't for the fact that modern games are fairly complex and they have simulated physics, they may have complex 3D scenes.
And running a million actions in a game isn't free.
Techniques such as domain randomization would make the agent required to like even see even more samples because it now has to deal with all of the variations of the environment.
So how are some ways we can train these playtesting bots more quickly?
Well, any sort of speed up is gonna come in one of two forms.
It's either gonna be through sample throughput, which is.
the, you know, how many actions we can take in our game per unit time or sample efficiency, which is how many samples our algorithm takes to train, right? So some ways to increase sample throughput, you can speed up your game, you could increase the interval between physics calculations, you could increase frame skip, decrease rendering quality and so forth. It's nice because you can use the same hardware that you're running on before, but...
It's limited.
And if, for instance, if you speed the game to, if you compute physics too infrequently, you may have bugs that you didn't intend.
Stuff may start tunneling through each other and so forth.
So if you don't want to do that, then you can run many copies of your game.
This doesn't require any modifications, doesn't change the dynamics of the game.
But it does proportionally increase your computation cost.
depending on how many games you're running.
That's pretty obvious.
Now, OK, so that's sample throughput.
And for sample efficiency, we've already talked a bit about using demonstrations to reduce the number of samples needed.
You do have to produce your demonstrations.
But something else you can also do that we found effective, especially in the SnoopyPod case.
is making sure that you're using a sample efficient deep reinforcement learning algorithms.
There's kind of two main categories of deep reinforcement learning algorithms.
I won't go too much into detail today, it's a bit out of scope, but...
Basically we have on policy and off policy and off policy algorithms tend to require far fewer samples to learn behavior, but they tend to require larger neural network models and more computations spent on training.
So this trade-off could be beneficial depending on how quickly you can take the actions in your game.
And in the Snoopy Pop case, which was the game itself was limited to in speed up because we couldn't increase the physics steps too much.
Otherwise, the bubbles would tunnel through each other.
Using a sample efficient algorithm, off-policy algorithm, we saw more than five times training speed up versus an on-policy algorithm.
All right, so to quickly recap.
using RL for testing.
So if you want agents that are robust across levels and could be able to play your new levels, you can use some form of domain randomization to make sure your bots can deal with all the different scenarios.
Now, if the problem in your game is too hard to solve by pure random exploration, You may want to use demonstrations to guide your learning.
This can reduce training time as well as produce more human-like behavior.
And finally, we did talk a little bit about how we could reduce long training times.
You could leverage parallel computing, you could use demonstrations, or you could try sample efficient reinforcement learning algorithms to reduce how long it takes to train your agents.
So that's reinforcement learning for playtesting.
And I'll pass it on to Robin to talk about reinforcement learning for non-player characters.
Thank you very much.
Hi, everyone.
My name is Robin.
I represent Karakassel, a small Swedish game company started together by me and Pal Fonando.
We are a small team, but we love technology, and we use it to make great content for our games.
Right now we are working on a game called Source of Madness.
Source of Madness is an action roguelite. As an acolyte from the cult of knowledge, you have to close the box of Pandora and end the madness.
dynamic world, but every time you die, you start over and the whole world changes.
The world is full of horrible monsters from the void, and these monsters are different every time you play.
So we asked ourselves, how can we make millions of monsters and keep them interesting, even after playing for many hours?
We are only three people.
So these were our challenges.
We wanted to make the monsters move by controlling their muscle physics to look unique and super creepy.
We have millions of different procedurally generated monsters and it needs to look natural.
This would be almost impossible to code by hand, especially with only three people.
So we turned to reinforcement learning as a solution.
To train the monsters, we use the following setup in this slide.
The AI gets input data about the environment and its body, and it learns to control its muscles.
They can also grab the ground.
Monsters are rewarded for moving towards the player, but we also use some additional rewards to get different behaviors.
And I will show you an example of that now.
In both of these videos, the main goal was to walk right towards the player, but we also gave them a smaller extra reward.
So on the top, as you see, they got rewarded if they stayed close to the ground, so they got this tumbling movement.
On the bottom, they got more reward by staying above the ground, and they learned to jump.
So as you see in this way, we can get much more variation without spending too much time.
The AI extends well to different body structures as well.
The neural network will learn how to best work with its body.
For example, a quad pad as you see on the left side or a spider on the right side.
Many of you are also maybe using machine learning or want to try it out.
So I will now share some things we've learned during this project and hopefully it will be of some help to you. So our main challenge with machine learning is to understand why when it doesn't work. So if your training doesn't learn there are many potential reasons.
So when it doesn't work you want to find out what to change.
and you want to avoid start guessing randomly because that takes a lot of time.
So from our experience if you're working with machine learning in real time like for a game the biggest pitfalls are probably the reward function and the handling of actions because these are done by your own code and as we all know it's easy to make mistakes.
Our key takeaway is to always make sure that these are working as intended.
Whenever you change your code, just check them again and make sure that it still works as you want it to work.
Otherwise, you'll maybe waste hours on a failed training and it takes time to find out what went wrong. Or even worse, the AI might work fairly well but not optimal. You might not even notice at first.
That was the case with our monsters recently.
We had not focused on the AI for a while, and we just started to notice that they didn't work as good as they used to.
Finally, we found that a lot of the inputs were always zero, and it had been like that for two months.
And once we fixed that, the monsters got a lot better again.
To avoid this kind of mistake, it is important to make good visualizations of your data, so that you can easily spot if something is wrong. This is an in-game gizmo that we made early in the project. As we can see in the engine, it shows the reward in real time.
So when this little monster is walking right towards the player, we should see that the bar goes more green.
And if it didn't, then we know that something in our code must be wrong.
Either the reward function or the training itself could be wrong then.
So this will save you a lot of time if you can just see what is going on and not just guess randomly.
Let's now consider these monsters again.
As I said, we have a main reward and several other small rewards.
We added them together.
Here comes a problem if one reward is very big and the others are small, then the small rewards would have almost no effect.
So we need to balance them properly and how do we do that?
So to help with that we modified the reward gizmo.
You see this on the right. This is the same gizmo as before for the reward, but it shows each kind of reward as a separate bar.
So this monster is getting a reward for moving.
That's green, the big bar on the left.
And it's getting a punishment for being upside down.
That's the red bar.
And then if any bar is too big or too small, we can adjust the reward function and avoid training in a long time to realize that it didn't learn what we wanted to.
As Erwin talked about, that it could get a non-human-like behavior if you're training a human agent.
You can balance the reward function better if you see what's going on.
So here is another gizmo that we made to see when a limb is grabbing. You see these little blue balls. With this gizmo we could easily see if an agent learned to use the grabbing or not.
It also helped us to find out quickly when the agents were not grabbing at all due to a code mistake.
that it was simply not able to grab, so it wouldn't help if we trained it longer.
That we realized very fast in this way.
So I hope this advice will be useful and inspiring. It's a really fun topic to work with machine learning and if you're interested to know more about this game that I'm working on, check out sourceofmadness.com.
And finally, thank you all for watching. If you point your camera at this QR code, you can find more information related to this talk and also the links that are being talked about.
So thank you all for watching and have a nice day. Thank you everyone.
