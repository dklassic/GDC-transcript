Okay, so, my mic should be working.
There we go.
So welcome back to the AI Summit.
Hope you had a good lunch.
And so, we have this great new session coming up, but before we do that, silence your cell phones and all that, and don't forget to fill out the forms for evaluating the sessions.
as well.
Okay, so let's just get started with this thing.
So this is a first time we're doing this session at the AI Summit.
It was an idea I had, and it sort of embodies what I love about GDC.
I love coming to GDC to meet a lot of people, but I also love to be, like, inspired.
And so with this session, hopefully someone in here, at least a couple of people in here, will be inspired.
And in order to inspire you, I've assembled a crackpot team of presenters here, who will share their crackpot ideas with you.
And so let's just go ahead and begin.
OK, so for my crackpot idea, I started with this thought experiment.
How would you get a first-person shooter bot to move exactly like a human player does, say, in death matches and such?
You want to emulate different skill levels and play styles.
But what I'm trying to get at with this crackpot idea is you want to emulate the route and the movement strategies and exactly where people are facing their guns and such.
and we start talking about it, you know, emulating people in exactly, right?
You're almost talking mocap at that point.
And mocap has some interesting things about it.
It captures the subtlety of motion because you're recording it.
And you don't even have to understand why the people are doing what they're doing, and yet you capture all of that, those subtle motions.
That's why mocap is so well used in, say, sports games, for example.
But I am not suggesting that we do this in first-person shooter games.
That's too crackpot, right?
For, I'm a Call of Duty player, and I, what would you have to do?
You have to like make a sound stage, and then actually rebuild a Call of Duty level, and then have, and mo-cap people through it.
That is, that's pretty crackpot.
So, but that doesn't even solve the right problem, because that's not how humans play the game.
Human play, humans play the game.
with a controller on their couch in a reclined position.
So that's not what we're trying to capture it.
But this image here embodies my crackpot idea and it was sort of the inspiration.
And so if you record a lot of people playing through a game, say a first-person shooter, and their routes and exactly how they're moving and how they pause and aim their gun and everything, you have a lot of data like this.
And the different colors might represent different play styles.
And if you have enough data, maybe not big data, but enough data, you can then find little crossover points between these, and the AI can maybe play back some of these and infinitely recombine this data in order to give eerily realistic player behavior.
So the recording strategy I'm advocating is that while people are moving through a level, humans are moving through a level, you're going to record the route, you're recording the pace, the style of movement, any pauses.
any hesitations and uh... the gaze and weapon direction are critical obviously because as you're going you know in these levels right you're going around corners in just a certain way aiming just in certain places hopefully right where someone's head will be and uh... of course the weapon that you're using is very important in this part so the high-level strategy once you've recorded all this stuff uh... you've recorded the humans uh... you'll need to tag the styles somehow uh... you could probably write algorithms to do this or hire interns Of course.
You want to find these crossover points within these styles in order to be able to switch between them and then the AI plays back these recordings, they're randomly switching between these snippets and crossovers.
So let me just kind of show you this in a diagram form.
Here's someone's playthrough in blue there and here's another person's playthrough and maybe these are all the careful playthroughs, we've tagged them as careful.
And then we can identify places where they cross over.
And these are the spots where I could be playing back and play through and then decide to switch to another one, just kind of like threads I showed you before.
And similarly, we could have different play styles, aggressive one with the crossover points.
And so we have all this data for the ANI to go through and perhaps choose.
And I could maybe even switch between these different styles and if you want to find the crossover points for that.
And so, just a week ago I discovered this little thing on the web called the Infinite Jukebox, and it uses the same idea, basically.
So, for what this is, is it's, you load up a song, here's Billie Jean by Michael Jackson, and it starts at the 12 o'clock position and goes clockwise, and this is the whole song, and an algorithm analyzed it and found crossover points in the music.
and so every time you see one of those arcs it randomly decides yes or no to take the arc and so it's recombining the the recorded music so that you hopefully can't tell it's even changing to the different part of the song and because it's bouncing all around it will play the song forever infinitely and it's basically the same idea that I'm advocating.
And in fact, I think that the strategy is so general, just like I showed you with the music, that you could probably use it for a lot of different things.
So you want to record humans somehow, tag the styles, critical to find those crossover points within the styles, and then you're just playing back the recordings, randomly switching between the snippets and crossovers, and probably the case is, the more data you have, the better this would actually work.
And so that's my crackpot idea.
OK, hi, I'm Alex.
So there are five stages to dealing with a crackpot idea.
But first, I'd like to point out that this is a completely sensible idea.
These guys are crackpots, but this is a sensible one.
In fact, I think there are no crackpot ideas in machine learning in general.
So anything technical that you can think of, within 6, 12 months, 18 months, it'll be done.
So as an example, I just want to dig into image generation.
So in June 2015, researchers at Google figured out you could inverse a neural network and get some interesting visualizations and patterns that came out of that.
Then in August 2015, researchers in Germany figured out they could reverse two neural networks and get the style from one transferred onto the other by mashing them up in the middle.
It's called style networks.
And in 2016, just January.
Researchers also from Germany figured out how to transfer these patches from one image onto another using a neural network which blends them together in a more visually pleasing way, so it's not just a mashup.
And you can take an image like this and apply it onto an image like this and generate a circuit board city, for example.
So combining them together generates new variations of content, for example.
And then just a few weeks ago, we showed that you could take the same idea with annotations, like take an impressionist painting and then annotate it, and then doodle things on another image, and you can transfer the style from one to another, and generate paintings like this.
It only takes a few minutes, and it's pretty crazy.
But it works, so machine learning has no crackpot ideas.
But the really annoying thing about this is that it's not the technical thing that will hold back your ideas, it's the actual implementation, getting your designs in and getting them into production.
Those are where most ideas fall flat, and that's the most annoying part about having these crazy ideas, is that you have to get them by the producer.
So why not instead apply artificial intelligence to fixing those production problems?
You know, screw the system, let's figure this out.
So I have a deal for you.
Let's take our traditional art pipelines where we take our artists, we put them into the pipeline and they generate art, so it's kind of a simplification, let's say, of the process.
Let's turn it into a generative pipeline instead where we...
create annotations, we'll call them reference art, and then we're using a neural network, a patch-based approach to recombine bits of these images, and because it's the neural network, it has some sense of what fits together visually and it blends things in a very pleasing way.
Then we have our level designers, they're basically creating art-free but annotated doodles of levels, for example, and then generating the final content on the GPU.
So the good thing about this is that you could potentially use it as an interactive tool to guide level designers in creating levels that are visually pleasing by simulating rather quickly, or instead you could do a high quality HD render overnight.
So whatever technique works for you, we can make this work, right?
There's lots of different options.
Now the sad part about this is that it's going to involve a lot of changes.
And certainly a lot of members of the team and the art teams will have their roles jumbled around.
A few people may no longer be required, their skills may no longer fit if they can't work within this system.
But there's new positions that will open up as well.
So even though the nostalgia of working with individual pixels may disappear, or well, people will be more nostalgic about working with pixels.
You'll have artists that are working on creating style that transfers well to levels, and junior artists that may be embedded into level design teams to make it better suited to style transfer, or even technical artists supervising the whole process and making sure the quality remains high.
So I think these types of ideas, their time has come.
For AAA games or indie games, there's always money to be saved by doing it quicker, even if you're taking programmer art and turning it into 90% quality art.
And there's a lot of value in that.
So it's an open source project, you can go and download it, and it actually does work.
The resolution and the performance will be improved over time, but it's already a pretty good proof of concept.
So I encourage you to go and download it and try it out, and see if you can get this actually into the real world.
That's the exciting part.
Thank you.
Thank you.
Thank you.
Hi, I'm Brian, I'm gonna go right into it because I have a bit more crack than most, so.
My first one I call non-CPU AI data manipulation.
This actually isn't too much crackpotty because I've done a little bit of it.
Like using influence map techniques on the GPU, using shaders, you can do character and environmental craziness.
have a whole slew of them that get all mashed together, additively, multiplicatively, with booleans, all sorts of craziness on the GPU and come back out.
And now you have a whole slew of better influence map techniques that you can do directly on the shader.
Or you could even go crazier and start doing computer vision or more advanced image analysis using compute shaders.
There's so many different ways to use segmentation to determine territory or resource areas.
Edge detection could.
do front lines between armies or even financial front lines in particular simulations. You could find a number of different features, like high-value features like cover points or resource bottlenecks. And then full-blown pattern recognition in a more transient way so that you could potentially find map scale events like there's an ambush happening to me.
or there's an opportunistic moment where my guy has seen an unguarded resource or base that I need to take care of immediately.
And then again, as Alex is mentioning, one of the things I'm doing in my hobby time is learning some deep learning stuff because there's new GPUs that are coming out that are really, really ideal for running large-scale neural nets like in real time.
It's actually pretty exciting stuff.
My second idea I call APBAIs. This is pretty damn crackpotty. And this is my, this has been my kind of hobby I'd say for about 15 years. It's like a mental, mental game that I play with myself. And this is asymmetric player based AIs. It's sort of like Ender's Game on Crack. Take two or more games and you use the players of each one of them to provide the AI for the other game without their knowing.
And what the thing is, is the reason I say this is a crackpot idea, but it's actually kind of fun to think about, is that it forces you to abstract the inputs and the outputs of your AI system in a very, very hardcore way to try and determine...
if you have two games where you can cross over like that. And so you identify is the particular sort of temporal flow of the inputs and outputs, what types, the dimensionality of the things that you need and you can actually find some pretty decent matchups like an endless runner game could...
based on how you traverse through an endless runner level could be determining which fighter moves that you're playing against and feed that into a fighting game.
And meanwhile, the fighting game guy could...
the moves and when he blocks and that kind of thing could be determining the actual level that the endless runner is playing.
Sort of feeding it into the procedural system to drive the actual level that the endless runner guy is running on.
same thing with a match three, based on which colors of bejeweled things that I hit or the combos that I do, I launch particular waves of enemies of particular types to an AI director system for a first person shooter or a full blown shoot em up and then in however the human handles those waves of enemies, the order in which they kill them, how much life he has left, you can then feed that back into the random block generator for the match three and get yourself a nice little match up that way. Likewise you could see something with a tower defense and a raid boss sort of situation. The spells and behaviors that the party is using on the raid boss become the creeps for a tower defense game and on the other side the way in which the user handles those creeps become the raid boss' AI.
That's crackpot. And my last I call the AI long con. This one sounds kind of weird but I, I enjoy it. Make building AI a rewarding player activity. So, uh, uh, imagine a game, maybe it's a, a large scale persistent, uh, um, like asteroids game.
and you have a home base and you have a number of small ships and you can fly around, you can collect resources, you can slowly gather up technology, you can do all this. And in the meantime, what you can do is like...
give a small script, a small AI script to any one of your little robots that you have in this world. And so you can pick one off the shelf or you can make your own little quick, little script, quick script for it and we can actually give you premium currency for, for doing so.
Especially if you make your own little script and then you put it on the, on the...
the list of available scripts and other people start using it.
If it's popular, maybe you get more money from it, that kind of thing.
Just to sort of incentivize actually doing a little bit of AI yourself.
Obviously the bots have to exceed some fitness function that you write, or like I said, maybe it's some sort of like crowd, crowd, thumbs up, thumbs down sort of a situation, but you have to worry about collusion then, and that's weird.
Obviously this is good with shifting metagames so that it forces the botters to kind of like reevaluate from time to time and then plus you don't have to worry about massive optimisation over time although I don't know if that's a bad thing. And then eventually the botters might actually start making bots that make bots. Um the thing that I've found over time is that if you give premium currency to anything people will do it.
And so the reason I bring this up is because I honestly think that if you did something like this in a game, people would start creating AI to get their stupid little internet points.
And this is the thing, like you start to say, oh, but you're having human beings do the AI and it's like...
that's what I want. Because the reason I call this the long con is that I think over time if we just have every... a larger group of people just try making an AI, just try having some control over something that it's going to give them that that little bit of taste that they want that that'll keep them going and keep them keep them making AI in our games and eventually maybe that'll be a direction that's good. So that's my stuff.
Okay. So we tend to focus a lot of time on the sexy, front-facing agent intelligence. The stuff that the players interact with at run time, and rightly so. However, I believe there's a lot of scope to spend more of our AI budget on the behind the scenes, the development workflow and tools. So this is what a typical modern level editor looks like currently. Or, God forbid, you're using something like this.
They're highly complex software, and they require a lot of expertise to get the most out of them.
However, there's a problem.
They're all designed about getting polygons into a virtual world.
They're not designed about getting game design principles iterated on.
It's like forcing coders to write assembly language directly or opcode manually instead of high-level programming languages.
Perhaps this is why games get better and better looking every year and our gameplay tends to have plateaued, unlike board games which have seen a soaring rise in new potential. Perhaps we're just not using the right tools. So I think game editors should look something like this. Oh wait, maybe this.
You don't need designers spending hours manually tweaking the placement of geometry.
We want them to instead think about the design intent of the space at a higher level.
So I'm talking about embracing designer-directed generative content here.
The designer could sketch out how he wants the level to play and tag certain areas with particular styles or encounters.
And then an AI system can interpret that sketch.
It's a little like Alex's neural doodles.
and actually generate the level geometry.
Place the decorations, place the encounter spawns, and so on.
All the grunt work can be automated.
Something like grammars, or L-systems are a popular example of them.
They would allow artists to develop rules for laying out particular styles.
You could come up with rules for a French cafe, or an Italian restaurant, or an Irish pub.
designers then just pick the style they want, tweak some generative parameters, and off you go. Voila! Instant level.
You want that level to be slightly bigger, 10 meters larger, no problem. Just tweak the size, and it automatically lays it out for you. No more spending three days kind of moving and repositioning every little bottle on the shelf of the bar.
Now, this isn't rocket science.
It's not groundbreaking yet.
I'm not sure why we're not doing this more, to be honest.
We've seen some uptake for foliage placement, speed tree, and that.
There's been some work on building facade generation, but nothing at the encounter level gameplay.
And that's where I think there's some room for AI techniques.
Now, grammars are effectively a way to semantically break up the space and place objects.
So there's no reason why we can't automatically apply these semantic tags to those objects and areas in the game. We could feed that into our agent systems, the box system for example, the last of us do this, right? And they say, he's behind the car! We can also potentially feed it into the behaviors and have richer information to reason about at runtime. Let's crank it up another notch. Grammars can be tedious to author. So what if we could train an AI system by looking at designer-made examples?
artists could layout specific example areas and then we can get the AI to learn the style of those areas and transfer it onto the sketches for the designers have made. If we can learn painterly styles with deep neural nets, why can't we do the same with layout grammars? I think that would be pretty wicked. Let's crank it up another notch. So your QA should be playing through the game constantly during development.
So why don't we have AI systems monitoring their playthroughs?
We can try and classify different play styles, learning how the players encounter different areas of the game.
We can retrain this constantly during development and adjust for design changes and balance changes.
It could even help highlight degenerative strategies early on in development rather than catching them later on.
We could also learn how the movement speed and the engagement ranges affects the play.
And we could feed those metrics that you've made up your game design rules about, feed that into the generative system, and generate layers that are optimized for your specific game mechanics.
With systems like this, we can have designers focusing more on how they want the level to play, instead of dealing with the tedious minutiae of placing all these individual decorations in the level.
They can work about the intent of the design and the intent of the space.
and it'll allow them to iterate very quickly and be more efficient.
So the bottom line is, we can use AI techniques to make smarter tools, and with smarter tools, we can make smarter games.
Thank you.
Alright, my name is Tobias Karlsson and I work at Microsoft and my crackpot idea is to explicitly model the play experience as a story.
So I got this idea from listening and thinking and hearing about the encounters in Halo, particularly Halo 2 and Halo 3.
In Halo, each encounter plays out a story, and it's the same story in every single encounter.
And I was thinking, wouldn't it be more interesting if the designers could customize a story for their encounter and explicitly model that story as the player's experience?
So the way I'm thinking this would work is the designer marks up a timeline of the encounter and on this timeline there would be significant events and this is where the designer says this is how the experience would change.
example of a mock-up I made, what it might look like. On the left here I have what I'm calling the Oh Crap Tom Rocking Scale. So the player starts to encounter reasonably confident. After a while something happens, perhaps reinforcements arrive, and he drops down to point three, which is definitely in the Oh Crap territory.
The player eventually deals with that and he bumps up to 0.8 and he's probably thinking he's rocking pretty good.
And then something really bad happens.
a boss or some big monster or a tank or something like show up and he drops down all the way to point one and then the player eventually works himself through this encounter and finishes all the way at one.
So the way I imagine this would work is that we have an AI manager that tries to manipulate the encounter in order to match the desired scale.
And the AI management will do this by spawning more or fewer reinforcements, or just weaker or stronger enemies, changing the AI aggression, weapons, vehicles and tools used by the AI, for instance an AI that can revive its fallen comrades.
Throw grenades will put more pressure on the player, while if the AI refrains from doing the same thing, it will ease up on the player.
And as always, we have the standard low-hanging fruits, like changing AI shatter and animations, music, and other emotional manipulators.
So the idea is that the designer sets the limits and the AI manager acts within those limits. So the designer may say that you're allowed to spawn 2 to 4 enemies here and the AI manager will have to spawn 2, 3 or 4 enemies and can't spawn a whole battalion or none at all.
And this still allows the designers to script up, say, various approaches to a particular encounter.
So if the designer wants to make it hard, if you attack the encounter head-on versus sneaking in the back door, we can use different timelines or adjust different limits for the AI I managed to work with.
So the whole point of this is to think of an encounter as a story and explicitly model the encounter as that story.
So we have something in our game that actually shows the story and what the encounter is about as opposed to what most games do today where it's just a collection of spawns, geometry and perhaps a scripted event or two.
So at this point you may be thinking that the Left 4 Dead's AI manager is doing something very similar.
And it's sort of true, but the difference here is that Left 4 Dead's AI manager works on an entire playing session at a time.
And it's non-scripted and an algorithmic approach.
And more importantly, it deals with pacing.
deal with how the story plays out.
That's just a side effect of the pacing and the setting.
Also, dynamic difficulty systems do something similar as it tries to manipulate how the player experience its encounter, or the entire game.
But again, the goal is something different.
So the advantages of this is, first of all, it allows the designer to explicitly express his or her intent with the encounter.
And because we have this done and explicitly written down, this is something that can be shared with the rest of the team.
And you can now.
have this vehicle to actually look at this and discuss what this encounter means for the entire game and with the rest of the team. Also, of course, it does allow the encounter to be more interesting for a wider range of players and playing styles.
so we can ensure that more players actually experience the encounter the way we intended.
And finally, it makes quality assurance better, because not only can the QA test the knowledge that's played from the encounter say that it works, they can also look at the timeline and ensure that it actually matches the desired result you see when you play the game.
Thank you.
Hi everyone.
So I'm gonna jump right to it.
I absolutely love procedural generation in games.
I've given talks here at GDC about procedural generation in games and how much I think that it's great.
I also love story in games.
Story is a huge part of why I play games and why I enjoy them.
So what if we could marry those two things together?
We kind of already do this a little bit with things like Skyrim is a good example.
But really those are tertiary quests and things that are kind of push you out in the world.
That's not what I'm talking about.
What if we can generate something like Dragon Age?
The entire story?
What if we can analyze the works of J.R.R.
Tolkien and build a derivative work from that?
Or have an A.I. write a lost Stephen King novel?
I think we can.
It's all about studying and analyzing stuff, right?
We analyze story.
We've been analyzing story for centuries.
This is the hero's journey.
This was popularized by a guy named Chris Vogler.
He took that work from Joseph Campbell, who created Hero with a Thousand Faces, who in turn was just looking at myth.
He was just studying myth.
We pulled structure from the way that humans tend to tell story.
We know that somewhere in the beginning of the story, we have to actually learn who this protagonist is and care.
We know that at some point, some shit's gonna happen, and it's gonna throw his world upside down.
And if it's missing, we miss it.
Many of you probably recognize this screenshot.
One of my biggest problems with Dragon Age Inquisition is that the inciting incident was on the main menu.
As soon as I pressed that menu button, boom, right?
That's the inciting incident.
I spent the first 15 hours of the game with absolutely no connection to my character from a story point of view.
I was like, I'm going to run around and attack things, yay.
And that was great and it was fun, but the story for me didn't kick in until after that.
compared to Dragon Age Origins, where it's like, I'm a mage and I'm going on a harrowing, and I got some people who maybe I care about, or maybe I betray, who defines, that defines what my character is.
So I think we can build an AI that looks and says, hey, right about here, something needs to happen, something kind of big that forces me into this special world, or about halfway through the story, something really big needs to happen that spins everything upside down, and maybe that should relate to a thing that happened before.
So can we take...
an AI and analyze some creative work and derive other work from that?
Well yes, we already are.
David Cope is a brilliant man who looked at all of the works of Bach and Mozart and others and has created effectively a Bach generator.
You can actually find it on his previous, if I go back here, on his site, you can go find it.
And you can listen to what sounds like the lost works of Bach.
Jeremy Leach is another one who's done very similar things.
It's a different technique, but it's the same idea.
So we can totally do this.
The branching factor is admittedly considerably higher, but we can absolutely do this.
So my final questions on this one are, why can't I have this?
Because I think we all want a game master who can just be this computer and can tell me Dragon Age stories forever and Mass Effect stories forever.
But my other question is, Should we actually do this?
What does it say about our own creative process and our own creativity that we can create a machine that does it for us?
Thank you.
All right, thank you guys.
One observation, please raise your hand if you actually kind of tapped into this a little bit yourselves.
Is that...
We were, like, halfway to Skynet, the Matrix, and the Singularity at some point or another during the past half an hour.
Wait until they come for you, Mark.
Anyway, let's thank them once again, and I'll see you in 20 minutes for, what is it, it's Eric Martel and the non, it's a movement thing, it's really cool.
So let's thank him once again.
