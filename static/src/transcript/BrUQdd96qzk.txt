Hello everyone. My name is Kohata Shuji. I'm an audio programmer at Platinum Games.
Before joining Platinum Games, I worked at Roland as a guitar effector.
I'm Andrew Blasher, from Platinum Games.
I used to teach in the company, and one of the students was Mr. Yoko Taro.
We've been preparing for this event for a few months.
I hope it goes well.
I'm just going to introduce myself for a second.
My name is Andrew. I work in localization at Platinum Games.
I also...
Probably, I didn't work on Near directly very much, but I think the biggest connection that I had was I did a small English class in the company briefly and only about three people came, but one of them was Yoko-san, so I think that I was pretty happy about that.
I've been working with Kohata-san for the last few months on this presentation, so I'm happy.
hopefully everything goes well. Uh, just wanted to say also, just to take this moment here, um, that we've been asked that if you have, uh, your cell phones, that please put them on silent mode. You know, we'll be recording this, so, uh, please turn anything off if you've, uh, got that. Thank you. And...
In this session, we will talk about the various audio techniques we used in Nier Automata to create a real-time audio process for the game.
Sorry.
Wait a minute.
I messed up.
There are a lot of delicate sounds in the demo, so I would appreciate it if you could keep the audio as quiet as possible.
Yeah, so in this session, we're going to talk about the different sound techniques used in Nier Automata, focusing specifically on real-time audio processing.
Yeah, he's just saying to be super sensitive to anything that makes any sound, because of course, we'll be playing some video, and we'll want you to pay attention to the audio.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
OK.
First, how many people have played NieR before?
Thank you!
As you can see, Nier Automata has a very sophisticated world view.
In this session, we would like to tell you what the role of real-time audio processing is to express this unique world view.
So, those of you who have played would know, Nier Automata has a very unique, carefully crafted world.
In this session, I'd like to discuss what role real-time audio processing played in tying this world together.
The interactivity of the game is greatly influenced by the real-time audio processing.
By following the player's movements, the game can be more interactive.
Real-time audio has a powerful influence on a game's interactivity.
Receiving audio feedback right after pressing a button can strongly affect the player's immersion in their environment.
Also, I think it has the role of strengthening the expression by sound designers.
It has the possibility of realizing a variety of expressions while reducing material and workload by automatic control.
Furthermore, giving our sound designers this technology significantly improves their ability to communicate audio to the listener in a more direct way, and you can get a lot of mileage with even a simple tool set.
With this in mind, I'd like to discuss how I approached audio processing in Nier Automata.
Okay, so let's briefly touch on the points of this talk.
First, we'll discuss spatial audio effects, we'll talk about our 3D audio effect, and the new reverb we designed for Ritmir Automata.
Next, we'll talk about some electronic effects in the game, specifically transitioning the hacking sequences and how simple lo-fi effects played an important part in the game.
The two main topics are audio processing, but Nier Automata also uses other audio processing techniques.
After these two topics, we can spend any leftover time on a few other audio techniques we tried in Nier Automata.
First, let's discuss the spatial audio effects employed in the game.
In terms of spatial audio effects, So, when I say spatial audio, I'm trying to encompass reverb, occlusion, and other 3D audio effects in one term.
If you're more comfortable with graphic terms, you can think of this as your model's shader.
It adds a layer of realism that creates a more immersive experience for the player.
This helps properly structure the atmosphere of a scene and makes certain every sound works together.
Nothing feels out of place.
In games, spatial audio effects are handled in real time, and this is one reason why real-time audio processing is so important and I think will continue to be important to game audio in the future.
For this talk, I'd like to focus specifically on the approaches used regarding 3D audio effects and interactive reverb in Nier Automata.
So, we may have had a somewhat uncommon approach to 3D audio for Nier Automata.
The biggest point is that we tried to create effects that do not depend on the playback environment.
Nier Automata is not a game that has the characteristics of VR or headphone recommendation, so we wanted to make it a mechanism that allows you to feel three-dimensional in the stereo speaker environment in a normal stereo home.
So first, we decided to create effects that wouldn't rely on the user's speaker setup.
The audio in NieR Automata isn't tailored to headphones or VR or some special playback environment.
So I wanted it to feel like the sound enveloped the player even from regular stereo speakers.
At the same time, we also made sure that the sound was not affected by the gain.
I also wanted something that wouldn't alter the original audio to the point that sound designers had to be mindful of the 3D audio effect when creating the original sound.
The original sound needs to be preserved when played from the front for balancing later.
Anyways.
I wanted to apply this to any and every sound, so I needed it to be as lightweight as possible.
If effects were only applied to some sounds, I think it would take the user out of a 3D sound experience.
Based on the concept I showed you, I was able to implement 3D audio effects in the game.
So, after a little bit of work, I came up with something close to what I was looking for.
I borrowed the name Simple3D from here, but in the end, I couldn't buy it.
First, I would like you to see what it is like in the video.
So, I started by calling it Simple3D and never really got around to changing the name.
Let me play a short video to demonstrate how it works.
The green sphere will rotate around the player and produce sound.
Please listen to the sound when the effect is on and off.
Thank you for watching.
So hopefully you were able to kind of see the difference between the effect off and on with that video.
I will introduce the DSP of Simple3D.
All right, so let's touch on the DSP for Simple3D.
As the name implies, it is composed of very simple processing.
The system creates a sense of direction by modifying the volume of each circuit and the parameters of the bandpass filter according to the direction of the sound source.
So, like its name, it has a simple design.
Simple3D takes the direction of the sound's origin and sends it through multiple routes to adjust its volume, one including a bandpass filter for continually adjusting the phase.
Japanese interpretation So on the top we have a 4K plus 500 Hz low-pass filter branch, followed by a 4K branch, followed by an As-Is branch.
Volume is adjusted among these three to find the right high and low levels.
The bottom line is to represent the change in frequency that is strongly heard when the sound source is turned back.
The result of looking for effects other than high and low frequencies was the bandpass filter that modifies this frequency.
So, the bottom route is used to emphasize the frequencies that are heard when the sound is behind you.
It's a bandpass filter that I stumbled upon looking for something that might create noticeable audio differences outside of adjusting low and high frequencies.
I'd like to explain how I designed the DSP for simple 3D and determinants parameters.
This should give you an idea of how the effect works.
This is a diagram.
I have two speakers.
One is fixed in front of me.
The other is placed in the direction of 2.
In the image on the slide, we have two speakers.
One is placed directly in front of the listener.
The other can be placed at a point of our choosing.
I would either carry the second speaker or have one of my colleagues carry it for me, usually the latter.
Thank you very much.
So, pink noise is played through both speakers with EQ for the front.
We recorded the parameter of the equalizer before the sound was played by comparing pink noise with the main tube.
We designed the effect based on this parameter.
So I would listen back and forth to the pink noise from both speakers, playing with the EQ until what I heard from both sounded as close as possible.
I'd repeat that for multiple directions, recording the EQ parameters for each one, then I'd use those parameters to build the effect.
I think that's pretty much all I did to perform the sounds we just heard, but I was pretty happy with the results.
The second part of spatial audio I'd like to discuss is interactive reverb.
I built this effect to automatically read the player's surroundings and create reverb to fit them.
I also wanted to continuously change all parameters.
Additionally, I wanted everything in the effect to continually update in real time.
I wanted to maximize the game's interactivity and try lots of things through automation that wouldn't have been possible manually.
I also tried to study the material of the terrain, and tried to reproduce the different reflections, time, and sound quality depending on the direction.
In addition, I also aimed to respond to the change of terrain in real time.
My goal was to take the terrain, find how it affects the time, strength, and quality of the reverb, continually updating any change found in surroundings in real time.
I used Raycast for terrain detection.
For terrain detection, we used ray casts.
YOSHIMASA ENJI, SPEAKING JAPANESE Every frame, we would ray cast in several random directions.
Each ray cast would record and temporarily store any collisions that occurred.
YOSHIMASA ENJI, SPEAKING JAPANESE Using these collision points in the player's position, I was able to calculate the proper distance, reverb, and filter that should be applied to each sound.
So here are two images to illustrate how we used ray casts.
The green dots represent where the ray casts are hitting.
On the left we have a small room and on the right we have a more open area.
It's a little small, but the UI at the bottom right is supposed to be a map where the green represents the size of the area.
I needed to take this collision data and use it to calculate reverb, so I guess the next thing on the agenda would be making reverb.
I wanted to create something that kept the reverb relative to its origin instead of following the player.
On top of the goals we previously discussed, I wanted to create something that kept the reverb relative to its origin instead of following the player.
As I mentioned in the request, I focused on creating a sound that was pleasant to listen to rather than doing a precise simulation.
So regarding the effects architecture, from throwing raycasts to the actual reverb itself, I opted to make the effect easily recognizable and audibly pleasing rather than attempting a precise simulation.
Keeping CPU usage to a minimum was also important.
This K-Verb is a temporary name for this effect, and I took the initials from it, but for some reason it's still there.
So I called this effect Kverb for Kohada when I started working on it and also never got around to changing that either.
So I called this effect Kverb for Kohada and also never got around to changing that either.
And also never got around to changing that either.
So, here's a video that demonstrates Kverb.
Try to notice how sounds change based on the size of the space the player is in.
So hopefully that, uh, you could see that. The difference there. So now I'll explain the DSP for K verb.
This is a bit complicated and it will take a long time to explain in detail, so I will just briefly explain the features.
So the design is a little too complicated for me to go into detail with everything here, so I'd like to just give a brief synopsis of the effects layout.
The input on the left is the mix for the dry component, and the AX on the top is the mix for the wet component.
We start with two sounds, one an input that will represent the dry level in the mix, and then anything that goes into aux will become the wet level of the mix.
The aux is the channel that comes from a listener.
We then do a parallel, absolute parallel mix to each channel for 8�.
The sound fed to aux starts as five channels positioned from the listener.
We first break this up into eight channels, each for a cardinal direction.
The loop in the middle is where the reverb is processed.
Data is extracted from the collision points from each direction to determine delay length, level, and filter strength.
On its own, the delay is too strong, so an all-pass filter and a cross-feed using each direction is applied to give a proper reverb effect.
The result is combined into four channels that are returned to the listener's perspective and added to the main output.
So next, let's discuss the electronic effects in the game.
I feel these effects were important in adding to the game's atmosphere.
I'll discuss the music transition during the hacking sequences and a lo-fi effect.
In Nier Automata, there is a feature called hacking.
When you take a code called hacking in a normal game, it will be transferred to a minigame.
When you hack, the music will continuously change to a sound like an 8-bit game.
In NieR Automata, the player has the ability to hack enemies and items.
When the player hacks something, they move from the regular game screen to a mini-game of sorts.
As this happens, the music also shifts to a more 8-bit version of the current track.
We didn't just cross-fade the music from the regular version to the 8-bit version, but we also used an 8-bit-like effect to enhance the sense of progression of the hacking.
Rather than simply crossfade, an effect is applied to the original track to bit crush it, after which at a certain point crossfading is applied.
So just for those who are unfamiliar with hacking, here are a few images.
YUSUKE USTUNOMIYA So, hacking Monenomy takes us from the regular game screen on the left to the low-poly looking interface on the right.
Next, I'd like to share a video that demonstrates the tone filter used during hacking transitions.
Some of the tracks in the game don't have 8-bit versions that are switched to, and this is one of those examples where the track simply uses the effect only.
So, let's look into the Tone Filter's DSP.
Tone Filter is an effect that takes the tones of a track and repurposes them into square waves.
The effect is not to produce sound by determining the pitch, but to transform the original waveform.
This time, we made the effect to emphasize the continuity of change.
What's unique about this effect is that it's not generating different sounds based on the tones it hears.
It's actually deconstructing the tone it receives.
The effect was designed to gradually deconstruct the sound for natural sounding transitions.
in Japanese.
Let's uh, so first input is made mono and we don't need stereo and we keep uh, keeping things in mono keeps down on processing costs.
Next we take 48 tones spanning 4 octaves and extract something close to a sine wave from them.
At this step we remember the volume of each tone, then clip it into something like a square wave.
At this point, we'll have too much noise, so we need to do some balancing and drop what's irrelevant.
Then we mix the remaining tones into the main output.
Actually, when we put it into the game, we didn't just use the basic part of the game, but also the spatial effects, and the gain and mix to balance it.
Ultimately, there are some area effects and other small details that come into play, but this describes the bulk of the process.
Next, we'll look at Lo-Fi.
All right, so next we'll look at Lo-Fi.
I think that low-fi processing is a relatively common process, but what I wanted to highlight here was that it is very difficult to produce a low-fi feeling that is not unpleasant.
I think that it is also one of the roles of real-time audio processing to make such expressions freely.
So creating a low-fi effect is pretty common these days, but I found it's hard to create an effect that doesn't perturb the player.
I found this is another way that real-time audio can really help provide some potential to sound crafting.
In the game, the effect of lo-fi is used in many situations, such as when the sound of communication or the player's senses are affected.
Nier Automata uses lo-fi in a few different ways.
It can be used for transmissions between characters or when players are hit with certain status effects.
Knowing how it would be used in the game, I attempted to make an effect that mimics old digital hardware.
Here's a video that demonstrate how Lo-Fi sounds.
This video shows how noise is increased as the player starts to become more and more corrupted.
Virus contamination rate up to 16 percent malfunction detected in NFCS circuit I have to keep it from spreading to other androids Pod give me a location that's low on android signals Alert virus contamination rate up to 32 percent Malfunction detected in visual processing system.
The Council of Humanity has a message for all of us.
Fighting on the surface.
Today, I have wonderful news.
This is a test flight.
Contamination rate up to 57%.
The function detected in FMCS circuit.
Alert. Button's contamination rate, 70%. Infiltrating system-protected region.
Alert. Button's contamination rate, 90%.
Alert! Unusual heat generation in central nervous system. Internal combustion imminent.
Detection! Natural malady detected in visual sensors.
Alert! Deterioration found in black box.
Alert! Damage to data backup system.
Step damage will make it difficult to retain sub-consciousness upon backup.
So next let's look into the DSP for the lo-fi effect.
Let's do this.
Let's do this.
Let's do this.
Let's do this.
The example diagram on this slide is a mono setup, but this flow is just duplicated to fit the number of channels needed.
Starting at the beginning, we take the sampling rate and cut it in half.
We don't need a high sampling rate for lo-fi, and it saves some processing power.
Next, we bring values closer to the values before them and then quantize.
Bringing the values close together helps relieve some of the jagged buzzing noise that can occur through quantization.
The closer we bring the values together, the more chopped up the sound will feel.
The looser we quantize the notes, the more buzzing will be heard.
So the setup is relatively simple in design.
What's important about this graph is how the later values are pulled closer to the values before them.
Then we just return the sampling rate to its default and send the sound to output.
YOSHIMASA ENOCHI, JR.: With Lo-Fi, I think we were able to take a pretty common effect and breathe some new life into it, which ultimately drew the player into the game deeper.
YOSHIMASA ENOCHI, JR.: JAPANESE I hope you enjoy this video.
Looks like it's time to help 2B get herself set up.
She's probably waiting in her room now.
We'll get brings of time now to be a staff job.
Fair enough.
Lady, we'll be right back.
We don't have much left to say to you, we're just here to help.
You're a monster, I say You're a monster, I say You're a monster, I say you. Okay. Thank you. So, in conclusion.
Nier Automata's sound design focused on utilizing the capabilities of real-time audio processing that recent consoles have made possible.
Sound designers crafted audio processing tools specifically for Nier Automata to increase user connectivity.
Sound designers and engineers can maximize audio expressivity by venturing into each other's territories.
Thank you very, very much.
Just some thanks.
applause so on the left, is the Niiya Automata audio team after receiving our SEDEC award everyone helped us out a lot And on the right, I have Andrew.
I think that we talked a little fast, so we have a lot of time left over.
But yeah, we can take some questions if anyone has anything.
Like I said, I'll try my best to convey them.
Sorry if this question is very technical, but for the ray casting reverb system that you made did you reuse pre-existing geometry such as the collision mesh to do the ray casting or do you create your own acoustic mesh and What kind of parameters did you tag this the acoustic surfaces with?
Reusing the geometry you recreate. What did you say? I'm sorry For the geometry, did you have to build your own acoustic geometry?
Or did you reuse something that already existed, like the collision geometry?
Yeah, it's just the geometry from, I guess, what, 3D assets.
Yeah, not audio geometry.
And did you tag the surfaces with specific parameters, you know, for like sound absorption and whatnot?
I don't know.
In Japanese.
In Japanese?
No, no, no.
I think he was adding a parameter to the surface.
So I said that we had material that was used for like footsteps if that answers the question?
Okay, thanks.
Okay, thank you.
I was also interested in the ray casting system.
How did you determine the amount to use, and was it different per level, per stage, or per sound?
What did you do with the amount you used?
So he would create different, I guess, mounts, and then the sound designers would kind of adjust those as necessary.
Yeah, I mean, our main goal, I guess, was just to get it as audibly pleasing as possible, just trying to find the way it sounded the best.
And then small follow-up, if the game needed additional performance, could you scale down the amount of recasts being done at a time, or was it always kept to the appropriate amount?
Oh.
so how much would you scale the amount of recasts so that you can reduce the load when playing?
the amount of recasts to be able to do that isn't determined by the location does the calculation and the amount is decided by one frame, eight retakes, yes.
So, um, we decided that at the game's beginning, just like to create a, what, like, standard value for anywhere in the game.
We just said it was going to be, like, eight raycasts per frame.
Yeah. Yeah.
Thank you.
I have a third question about the audio raycast system.
Okay. Sure.
I'm curious why you chose to do random raycast directions rather than either fixed directions or a predetermined sequence that you could just round-robin.
Random?
You can use anything.
You can use anything.
Random.
Random.
Random is...
Direction.
Direction is...
Random.
It doesn't have to be random, but it does need to be balanced.
And when that happens, there is a balance between horizontal and vertical, and it's a pain to do it evenly.
I guess it would just be easier just to find any differences between, obviously, there's going to be what differences, high and low differences, and then what differences in geometry with right and left, right?
So it didn't need to be random, but I think that making it random was probably the easiest way for us to catch all of those differences.
Thanks.
Thanks.
Hi. The DSP, was that, were they built as plugins to run within Wwise or somewhere else in the game engine somewhere?
So DSP, do you know what Wwise is? Do you know what plugin do you use?
Ah, subete Wwise plugin desu.
Yes, they're all Wwise plugins.
Okay, created by, by, yes, terrific.
Yes.
Hi, first of all, I must admit I have not played the game yet, but I will.
But I was very much impressed with the tone DSP filter, how it translated very well the original material.
My question is, was it...
applied to one single stereo file, which was the whole of the music?
Or was it applied to several tracks that you kept separate, in order to apply them separately?
The first tone filter, is it applied to one track, or to several tracks, like a stem?
I think it depends on the song, but I make it with the assumption that it can be applied to one song, to mix songs.
But I know that the accuracy changes when I apply the same effect to another stem.
Generally, it was just used in one track, but there would be certain situations where, you know, if we did use the stem, I know that it would create a different result.
Yeah.
So, just a follow-up.
This is essentially a resynthesis then of that original wave file?
Is a resynth...
Yeah, it's a resynthesis.
Resynthesis.
Is that being resynthesized on real time?
Sorry I stumped you.
It's a filter, a bandpass filter.
It's a very, how do you say it?
I don't know.
You don't know?
It's narrow.
High density?
Okay, he's just saying it's not exactly a resynthesis, it's using just like a high density bandpass filter.
Okay, I have to do some homework.
Okay.
Thank you so much.
Thank you.
Hello.
So you mentioned that it can be fruitful for designers and programmers to enter into one another's domain as a programmer.
Can you mention a specific example or a general example where classical sound designers would be entering into the more technical domain?
I'm wondering if sound designers and engineers can get along with each other.
What do you think about the sound designer working more like an engineer in this?
The most important aspect was to confirm the processing load.
Yes, a sound designer wants to give his best, but if you're not just giving your best, but also focusing on the process itself, that process will have some setbacks, I think that probably the most common example would be just to get them to understanding how much processing power the sounds that they were using was creating.
because obviously we want them to create the best kind of sounds possible, but if they get too crazy with stuff, then it will take away from different parts of the game.
So it was kind of just training them into exactly what level of processing power the sounds they're creating is actually causing in the game.
Thank you.
Thank you for your session. I have one question. When player hacked some enemies, music changed into 8-bit version. Do you put the 2-version music in the resident memories? Because, I mean, So, in the root, in the root, player can't hack enemy. So, only B root. So, do you put on the two version on resident memory?
You always have to switch between two versions of music.
I think it's the same with overclocking.
I'd like to know if you always have two versions of music on your memory.
The songs are mostly streamed.
They're mostly streamed.
We make a lot of settings for that as well.
Basically, only one stream is being played.
When switching, of course, both streams are being played at the same time.
I'm sorry, but I'm interested in this.
I think there was a picture of hacking with A root and B root.
Does that mean that the structure of the bank has changed with A root and B root?
No, basically, all songs are streamed in A root.
I think the question is just that, are there being two versions of the, is there an 8-bit version and a, a just regular version of the track being applied to memory in, um, all the roots of the game?
Because in the A-Root of the game you can't use, there's no actual hacking sequences I guess.
And he's saying that the song is usually streamed and in certain instances there needs to have both of the tracks in memory, but not always.
Thank you very much.
Hi, I was wondering if you could talk a little bit about the machine life form voices.
I found it's a pretty simple thing, but I think it was very...
I really enjoyed the sound of their voices.
Are you talking about the machine life form or the voice changer that we heard?
The machine life form, not the voice changer.
OK.
Here's our sound designer, Shindo.
First, I really wanted to get up on this stage, so I'm glad that you asked a question that would allow me to do so.
Thank you.
Thank you.
Thank you.
Thank you.
What was your focus when you were making the voice for the mechanical life form?
Focus?
What do you mean by focus?
Process.
Can I ask you about process or focus?
So just, you want to know the, what, the process of making it, I guess?
Yeah, like I'm curious about the specific type of effects that might have been used, or like if there was bit crushing, or like a vocoder.
What effect did you use?
Effect was...
Specifically, we need to have the pitch correct.
So we used the pitch correct effect from Nuendo.
And then...
I'll just say it.
So there's Nuendo.
And what we needed to do first is we needed to make sure that all of the pitches were corrected.
So we used the pitch correct in Nuendo for that.
And then I wanted to add some vocodering, so I used the plugin called Waves HDlay to flange the delay a little bit.
That's how I made the image of a vocoder.
I might not remember the rest.
I probably spent too much time on this.
We probably had something else but I don't remember.
But I think that if you were to use those two plugins, you could probably create something similar.
There was one point, and pitch correct has a lot of effects from a lot of companies, but NUEND's was not very precise, so we adopted it.
So there are a lot of different companies that have pitch correction software or plugins, I suppose, but we enjoyed the Nuendo version because actually of its poor performance.
Yeah.
That can be a good thing. That makes sense to me.
Thank you very much.
Thank you.
Thank you.
Hello, I'm just trying to ask a unprofessional question.
Okay.
So I'm wondering when the sound team and Yoko-san discuss his request for the game, like the sound team, do you know the story beforehand or how do you fit in all the schedules?
Okay.
I just want to ask you about your relationship with Yoko-san.
Again?
I just want to ask you about your relationship with Yoko-san.
Thank you for the question.
I'm glad to be up here again.
もう一言です。シンプルです。横尾さんに全て任されてました。 So, very simple answer.
横尾さん just left us to do everything.
Thank you.
He says that obviously we would have him check at the ending after we've made something, but usually he's just like, yeah, sounds pretty good.
OK, I like it.
And you know, I mean, sometimes he would request a change here or there, but usually he was pretty lenient.
OK, thank you so much.
Thank you.
SPEAKER 1 IN JAPANESE SPEAKER 2 IN JAPANESE We all agreed on the same thing.
Including Mr. Yokoo.
We didn't have to ask anyone.
We were all thinking the same thing.
It was great.
including Yoko-san.
Are we okay?
Okay.
Thank you very much.
