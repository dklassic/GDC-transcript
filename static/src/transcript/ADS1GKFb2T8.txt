I am Jin Hyun-Jung, the team leader for this project and the first speaker of this session.
In this session, we will share our experience of making AI agents for the Arena Battle of Red and Soul.
At first, I will introduce the problem we solved and the key ideas for it.
And then, soon, the next speaker will explain how we solved the problems and challenges and show experimental results, including demo videos.
Let's start with the problem explanation.
Bread and Soul is an MMORPG launched in 2012 and is now being serviced globally.
Bread and Soul has shown distinctive visual styles, movie-like scenes, and combat inspired by realistic martial arts.
This is the arena battle of Bread and Soul.
The arena battle is a PVP subcontent of Bread and Soul.
Two players fight against each other in an instant battle arena.
Two win, each should make the opponent HP to zero in three minutes.
If both of their HP is still above zero after timeout, the one who deal more damage wins.
For the fairness, their stats are standardized.
Our research goal is to create pro-level AI agents in one-on-one arena battle using reinforcement learning.
The reinforcement learning has been successfully applied to many games, including some Atari games, Go, Chess, Quake 3, Dota 2, and StarCraft 2.
However, the AI modules of many commercial games are still being composed with handwritten rules.
This tends to make agents simple with full of weakness, so as to give limited experience to users.
To make various and flawless agents, we tried to apply reinforcement learning to it.
To achieve it more effectively, we also tried to give styles to agents, aggressive, balanced, and defensive.
We thought that this would give various experiences to users.
In Bread and Soul, there are 11 classes Destroyer, Breadmaster, Assassin, Summoner, and so on.
To reduce complexity, we made a Destroyer agent that could fight against Destroyers only.
We would choose a Destroyer because it's a class that steadily appears in Bread and Soul World Championship.
Their skill settings are fixed to be the same for the fairness.
To describe an agent, we drew the Blood and Soul Arena battle in abstract form.
An agent would observe the environment, including its own state, the opponent's state, and arena state.
The observed states include hit point, skill point, the distance to the opponent, the skill cooldown, distance to the arena boundary, and so on.
An agent would choose an appropriate action for the observation.
The action affects the environment, and in turn, the agent observes the changed state to decide the next action.
The agent has a chance for choosing actions every 100 milliseconds.
In the Radiant Soul Arena Battle, action can be categorized into skill, move, and targeting.
These actions should be decided strategically, since they have a trade-off between cost and effect.
I'll explain the skills of Destroyer with examples of strategy decisions.
It has 44 kinds of skills, which can be categorized as crowd control skill, damage dealing skills, resistance skills, and escape skills.
To deal damage continuously, an agent should decide one of the crowd control skills before damage dealing skills.
If the opponent is hit by a crowd control skill, The agent would decide damage dealing skills consecutively to reduce the opponent's HP as much as possible.
To avoid such a situation, the opponent should use one of the resistance skills before being hit or use one of the escape skills after being hit.
Since the resistance skills and escape skills have a large opportunity cost of cool time, you should decide to use them carefully.
In some situations, you should save them to use later.
This makes it difficult to learn the optimal timing for skill decision.
In this project, we faced four challenges.
High complexity, real-time response, generalization, and guiding fighting styles.
The first challenge is high complexity.
When compared with the game of Go, our agents have larger action space.
In case of Go, the degree of freedom of each action space is about 10 to the power of 170.
On the other hand, our dead of hours is about 10 to the power of 1,800.
Longer game lengths and the combined action of skills, moves, and targeting make the action space larger.
This makes our problem so challenging.
To solve this challenge, we constrain the freedom of skill selection and reduce the decision interval of movements.
The details about them will be explained later.
Second challenge is the constraint of real-time response.
Since the Arena Battle is a real-time fighting game, late response results in poor performance, even if our agent decides the optimal actions.
This restricts the method for implementing the decision policy.
For example, we cannot use any search-based method, which requires a huge amount of processing.
The neural networks are very flexible and powerful computational methods to express a policy.
Using neural networks, the computational complexity could be easily controlled to satisfy the real-time constraints.
The third challenge is generalization.
Actually, we could not expect which pro gamers our agents would encounter.
To make our agent play well against any pro gamer, We needed to train our agents against the opponents of various styles.
For this purpose, we provide our agents with sparring partners from the opponent pool of selves.
Trained agents of various styles are added to the opponent pool continuously to have stronger and more diverse sparring partners.
Finally, we needed to guide fighting styles without hand-written rules.
Humans can easily adapt to the monotonous agents and feel bored.
The agents of various styles could reduce this.
We defined the trainable styles according to their aggressiveness.
The aggressiveness could be guided with carefully designed rewards.
The next speaker will explain how we tune the reverse to guide the different styles.
Hello, I am Seunghoon Roh, AI research engineer from NCSoft.
I will cover the methods applied for creating the Arena Battle AI.
Since our agents are trained with reinforcement learning, RL, I will first introduce the basic concepts of RL, and then I will explain the overall learning process, including features and rewards.
Finally, I will present some engineering techniques which facilitate learning process.
Okay, now let's imagine a baby trying to learn how to work.
He must learn by himself with numerous trial and errors.
He falls down again and again until he figure out how to control his body.
This type of learning is correspond to RL.
Let's get into detail.
These three boxes shows good and bad sequence of actions.
If the baby somehow falls down, he realized that something was wrong, but he cannot figure out exactly which part of the action causes the bad result.
Likewise, if he succeeded to keep walking, he knew he are doing well as a consequences.
The signal that tells whether the baby doing good or bad is called as a reward.
The objective is to find the best action sequence to maximize the cumulative reward.
During a lot of trials, the baby modifies his acting policy to encourage actions in good sequences and suppressing actions in bad sequences.
As experiences accumulate, he learns how to get more of rewards.
This is the core of RL, trial and error, and improving acting policy.
Then what is the reward in BNS Arena Battle?
What is the signal that tells you are doing well?
Of course.
Winning is the main key.
If you win, you are doing well.
But it is very sparse signal because it only happens at the end of the match.
We need another signal to follow, and that is HP.
If you took away the opponent's HP, you are doing well, and if your HP is reduced, you are doing bad.
Quite straightforward, isn't it?
With these two components, the agents learn how to win and how to take away the opponent's HP while preserving its own.
However, since our agents are required to have diverse styles, we introduced some additional rewards.
We decided to guide different degree of aggressiveness into the agents.
We have determined three dimensions of rewards to tune the aggressiveness.
The aggressive player's priority is dealing damage, while defensive player's priority is preserving its own HP.
So it is reflected in the first dimension.
Since aggressive play tend to approach while the defensive play tend to secure distance, other two dimensions reflect these perspectives.
You saw this plot before, right?
Here, the reward signal added.
The agent sends an action, then the environment gives you the next observation and reward.
I hope you are familiar with each component of the plot, except feature.
Features represent how the agent sees the world.
The agent decides an action based on the feature.
Features are basically the same that given to the human player.
So any accessible information to human during a game are also provided to the AI, such as HP, SP, distance, position, skill cool time, and so on.
Now let me show you the overview of the learning process.
The agent, which is composed of neural network, proceeds combat simulations in parallel.
In our system, 100 workers operate simultaneously.
When games end, networks are updated from the game logs with an RL algorithm called Acer.
What algorithm does is basically modifying the probability distribution of each actions.
I mean, encouraging good actions and suppressing bad actions.
This loop is repeated continuously.
Here I guess you might wonder of who the opponents are.
There are two conditions for good opponents.
First, opponents should be diverse enough.
If the opponents are limited, our agent just learns how to exploit the fixed strategy, but could not be generalized for different opponents.
Second, the opponents also should be reinforced along with the agent.
As both sides of the combat are becoming stronger, agents can experience higher quality of battle.
The self-play method with opponent pull satisfy both of suggested conditions.
We store each agent to the pull at fixed interval, so the pull gradually grows.
Then the pull provides a variety of opponents and the opponents becoming stronger with the agent.
From this to following two slides, I will explain more engineering-focused techniques.
And first technique is about learning the move action policy.
Learning the move policy is rather difficult because the effect of a single move is limited.
A single move indicates a movement of 0.1 second.
Considering the character's speed, how far could it go?
Therefore, single move action barely affects to the state.
To solve this, we enforced agent to maintain move decisions for 10 consecutive actions.
This allows the agent to literally move and lead to the change of subsequent states and rewards.
The method enables the agent to learn meaningful moving strategy, and finally, we're able to run away or approach to the opponent.
We also facilitate learning the skill action policy with help of domain knowledge.
Out of Destroyer's 44 skills, the skill that has longest skill reach is 16 meters.
Therefore, when the distance between the agent and the opponent is farther than 16 meters, we just blocked most of skills because it is waste of resource, such as skill call time and SP.
It helped to ignore meaningless exploration and boosted the learning process, especially in the early phase of learning.
I also want to give you some useful findings during feature engineering, which is discretization of continuous feature actually helped.
Each skill has its own targeting range and accurate distance recognition is essential to hit the enemy.
For example, there is a skill named Drag, and its range is three meters to 16 meters.
But the agent trained with continuous distance feature keeps miscalculating the skill range, and use the skill when the opponent is too far or too close.
This is because continuous feature gives burden for model to learn the exact distance threshold for each skill.
To solve this, we encoded the distance feature into one hot vector and continued training.
And then, the agent was able to use the skill, drag, precisely.
In this section, I will present some experimental results which includes diverse learning curves.
Our first experiment was conducted with an opponent called Built-in AI.
Learning how to defeat a single fixed opponent is relatively easier task compared to deal with multiple random opponents.
Therefore, before training by self-play method, we first test the viability of the method by training against Built-in AI from Infinity Tower.
Infinity Tower is a special dungeon in Blade and Soul where human players fight against NPC AI.
We've used this sparring partner as a test bed of various algorithms and tunable hyperparameters.
Let me show you the training results.
The first graph is time-win-rate graph.
It shows how win-rate increases as time passes.
The second graph is time-HPDIF graph.
HP diff indicates the difference of agent's HP and opponent's HP when the match ends.
As can be seen in the graph, our agent achieves 90% of win rate in just 30 minutes of training.
Although the learning curve was splendid, human player was able to easily defeat the agent.
It had a lot of weaknesses and it was even easier to exploit than the built-in AI.
So we moved on to the next step.
We now enter to a self-play training phase.
In this stage, the agent fight against the opponent, which is randomly sampled from the pool of past itself.
Since there exists hundreds of opponents, so we displayed multiple learning curves here.
Each block correspond to the training curve for each of the opponent in the poll.
As you can see, the performance against each of the opponent all stably increases.
Then we briefly tested this agent against human, and finally.
The level of difficulty also increased, and human player felt hard to exploit the agent.
Therefore, we evaluated the agent with more systematic approach.
The goal of the evaluation is, we want to measure how the agent fight well against the opponent it never had met before.
It can be called as a generalization performance or robustness of the agent.
So we made a match with this agent against built-in AI and skillful human player.
Note that the agent has never met built-in AI before because it is trained with self-play method.
Nevertheless, it defeated both of built-in AI and even skillful human player.
Maybe at this time, you might wanna see how the agents actually fight.
So I guess I should move on to some demo videos.
These videos tell how the agent is getting stronger as training goes on.
Female characters are AI, and male characters are dummy opponent.
The left most video shows the initial model which acts randomly.
We measured time consumption for AI to reduce dummy opponent's HP from four to zero.
It requires 120 seconds to defeat dummy model.
After a day of training, the AI agent was capable of dealing damage with basic combo.
Also, it used crowd control skills before dealing damage to make sure the opponents cannot react.
It requires 42 seconds.
Finally, within only 20 seconds, AI defeats the dummy opponent.
with a single long combo sequence.
As you can see, the initial model is still struggling with the dummy opponent.
For your information, our final agents that compete against pro gamers has trained for two weeks.
It means that each agent experienced up to four years of real-time gameplay.
Now let me show you how fighting styles are different between the aggressive and the defensive agent.
Black characters are AI, and white characters are human players.
Aggressive agent keeps approaching and attacks relentlessly.
Opponent have no break to think about counter-attack and assessing the current state.
On the other hand, defensive agent tend to secure distance and it waits for an opportunity for proper timing to counter-attack.
You can see that agents of each style manages the match in largely different manner.
Just before the live event called Blind Match, we invited two prominent professional gamers who are the former winners of annual competition.
For the fair evaluation, we restricted our agents' reaction time with a delay of 230 milliseconds on average.
which is similar with that of professional gamers.
As you can see in the table, the aggressive agents dominate the game, while other two types of agents have rather intense games.
According to the gamers' interview after tests, this is because human player often lead to some break between fights, for just think about current state and high level strategy.
However, the aggressive agent does not allow human to have a break between battles, but it only pours on attacks.
Both of gamers agreed on that the aggressive agent is impeccable and flawless.
They also added that aggressive agent plays in totally different way from human, while other two type of agents play similar to humans.
And finally, with these three models, we went for the blind match.
Let me show you the event highlight video.
Players here, of course you know, you know ColorEyes.
He's been fantastic there, over 9J.
I don't know who that is, I don't think they know.
A game right now, a bit of a...
What?
Okay, okay, alright.
The moment ColorEyes showed his back, he's like, alright, I'm going in.
There we go, but they both still, all persistence is actually out for 9J.
It's looking great for ColorEyes for now.
Ember Stop is down as well.
Oh, drag messy there. I J already moving forward and seeing what he can do. We can get the win another one Oh, but not if I J Has best does gusto Even with gusto. We'll see how he's gonna time that one up should be in favor of colorized trip the damage The chase We're going to win for Shanharan, coming in from China.
That's the retreat people.
Actually the drag connecting there is gonna force out the gust already so pretty much similarities between the last set as well and what hopping it's you know, but It's to punish instead. Oh Gosh, he's going at it against how round one Ember stump is also down. Okay, not you. Oh, he's just against again, though This might just be it if I Only a couple of seconds away. That's been 14 more seconds.
Disgusting Justin McLean.
And that will be the damage made with the wedge coming up on that grab.
And that will be enough for Shaman to take that game 1 victory.
Yeah, who just lost in the third place match with his team.
Comes back in this event.
Yeah, we'll have to see how this happens.
Oh my gosh.
What?
That reaction time is crazy, but I was saying no the first two destroyers were able to beat night J But this time around so the choice not looking great if he's the only destroyer that loses J This is devastating But there we go this is it three seconds not too long But it's gonna be looking a little bit better for a night J, but not quite good enough Oh No, no, we'll see once again the gusts got me coming back and that means like J If you hit the way now the heels coming back now Oh Wow playing with your food night J. Huh Wow The overall results including the blind match is as follows.
The aggressive agents won all of the game except one, and balanced and defensive agent made competitive result against professional gamers.
To the best of our knowledge, this level of performance is achieved for the first time in modern complex fighting games.
Until now, we introduced the overall process with specific engineering techniques used for creating AI.
I want to conclude with brief summaries of what we've made.
First, we succeeded to make pro-level AI in complex fighting games based on RL methods.
Second, we guided fighting style without any hard-coded rules.
Aggressive style is just one example, and any type of styles can be guided with real shaping.
Third, we trained robust AI with pool of opponents.
Last, we reduced the problem space with various engineering techniques.
It is the end of what we've prepared for today's session.
I hope you've learned some useful lessons from our session.
And please don't forget to evaluate our session.
We are Jinhyun Chung and Seunghun Roh from NCSoft.
Thank you.
And our company prepared another session on Thursday, 3 p.m.
It is about deep learning based inverse kinematics.
If you have interest, please stop by.
We have another two minutes if you want to take questions.
You don't have to if you don't have to.
OK, I'll just take one question.
We have two minutes, so if you have a question, I will take only one.
If not, it's OK.
I'll jump in.
Or did you want to go?
No, no.
You could probably answer this quickly.
It seems like the aggressive style was something that the AI discovered.
Did players start incorporating that into their gameplay in order, like, is it a discovery of a game style?
Or is it just an unfair advantage the AI has?
It is not unfair.
We have very carefully designed for the fair match.
So.
Although Aggressive Agent acts perfectly, it has a delay between its decision and its reactive time in-game.
230 milliseconds of delay, so yeah, it is a fair match.
I wonder if you're allowed to tell us, how did you adjust the weightings of the neural network based on the reward signal that came back?
How do we adjust the...
Adjust the...
The weightings in the neural network.
Weights of reward?
Yeah.
We...
Like RL method, we have a lot of trial and errors and to make the style...
Obviously, we tune the weights and try many set of weights and if they look similar, we made it far apart the weight of each reward and...
Yeah, ding.
Yeah, like that.
Thank you.
