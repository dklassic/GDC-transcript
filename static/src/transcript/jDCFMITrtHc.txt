Good afternoon, everyone.
My name is Geet Shroff.
I'll be kicking off the open world hour here at the summit today.
So first, a little bit about me.
I started off in the industry in 2004 at EA Sports, working on central technology and the FIFA soccer franchise as an animation and AI programmer for a few years.
Then I moved over to Ubisoft Montreal, where I was the programmer and realization director for AI on Far Cry 3.
And currently I'm at Avalanche Studios in New York, working as a programmer, helping lead character development in AI, player mechanics, and animation.
So the structure of the talk is a little bit high level.
More concepts and techniques I've worked with and are currently developing.
A lot of these ideas aren't limited to just open world games, but they apply definitely directly to them.
I'll be focusing on three major themes, particularly the design, behavior, and realization of open-world AI.
And I've tried to be as structured as possible, but I encourage you to keep these themes in the back of your mind as I go back and forth between them throughout the talk.
So I want to start by first talking about what makes a game an open-world game.
When we think of open-world games, we think about size and scale. We think about freedom, freedom to explore.
to pick and choose what to play, to kind of pick and choose how we want to play.
We think of a living and breathing simulation that makes up the world.
And all of these things are pretty important elements in open world games today.
And along with those elements, a key ingredient for us at Avalanche is this idea of a sandbox.
So we want to put the fantasy in the hands of the player.
And we want to allow the player to be able to craft their own gameplay experiences.
And we want to really try to have all these systems that interact with each other.
And we want to extend that idea to all aspects of gameplay, so be it missions, or world sim, or challenges.
And we don't want to limit it to any one of these.
And that sounds great, and it is, until somebody pees in your sandbox.
So in the sense that we can build this awesome world, but when somebody else is in charge, things that you don't expect to happen are most certainly gonna happen.
And they're probably gonna find their way onto YouTube.
So giving the player control means you as a designer or developer don't have it.
And that raises obvious challenges for our AI.
We need our AI to be adaptable and reactive.
We need them to scale well, to deal with the size and all the different contexts in the world.
We need them to be interesting enough and well realized for the player to be able to engage with them in a long period of time and discover all the ways that they can interact with the AI.
And in terms of realization, that's pretty much what players now expect from all our games, linear or otherwise.
So I want to start by talking about this idea of maintaining longevity.
We can think of our games as having several sort of core gameplay loops, right?
You can be in combat, you can be in exploration, you could be in a challenge.
And we want our player to have or create variation in these different loops.
So the challenge is really dealing with repetitiveness and providing the right motivation that encourages the player to be creative and keep playing the game.
So we can do things like add constraints that try and create variation.
You know, things like different environments, time limits for our missions, changing the amount of enemies that show up.
All of these things might work, but there's no guarantee that it will cause the player to actually try something different.
Okay?
Well, you know, let's introduce unique and scripted gameplay and force the player to engage in a different form of gameplay.
And while this is important, it works really well for linear and structured games.
And it's a little bit tricky for open world.
So the motivation to repeat a scripted sequence is really low, and there's a lack of discovery that happens.
You're not going to discover any new elements the second time around.
So what can we do?
So to deal with this, we can start with the player behaviors first.
This idea is not new in any way.
Paul Tesor has actually talked about this in the past, where he uses AI techniques and representations to actually analyze and profile gameplay.
So we can extend that to the player.
And the idea is to actually represent or script the intended player up front as sort of a design exercise, like you would do for any non-player character in the game.
And this is really easy for us to do.
We've developed languages and techniques on actually how to model perception and behavior.
That's what we do as AI programmers.
So doing this, we can really identify from an AI point of view what new actual enemy types or new enemy behaviors we're going to need to motivate the player to do something different, sort of like input.
And we can see this, you know, we can see if it can actually be clearly communicated to the player, sort of like having the right perception in place, to ensure that the player has the information he needs to make those decisions.
And, you know, to really drive the point home, think of a simple context of even picking a car for a challenge.
If you were to actually script that behavior for an AI, if you didn't give the AI any information about the race, he's going to pick randomly.
And then he's going to have a really poor experience without knowing, not knowing what the race is about.
And that's the same sort of exercise you go through.
Starting with the player behaviors up front makes the design player-centric, and that's really beneficial for open-world games.
And it's important to remember that we can trust the player to create variation on their own, as they interact with the systems within the world in unique and different ways.
So, for example, in Far Cry 3, you know, running into an outpost with enemies when a tiger is actually chasing you is going to create a different combat experience than just going in alone. It's this overlap of systems which is what's really interesting.
So, the next thing I actually want to talk about is dealing with this variation.
When you introduce freedom, you're going to have to deal with a wide range of situations, and that is one of the biggest challenges for us.
So if you look at a game like Just Cause 2...
The player here can actually travel extremely fast from one end to the other.
He can choose to get up in the air whenever he wants.
He can drive actual, you know, a wide range of vehicles into different combat situations.
Quite literally anything can happen.
And this makes a lot of things really difficult.
So, you know, things like staging our characters, positioning them, and things like controlling pace, all of this become very challenging.
And a key kicker here is that it's hard to deliver a consistent player experience that's positive.
And this is a huge advantage that linear games have over open world games.
You know, they can better control things like setting up pace, they can set up very specific situations that the player must play through, and even though it's a singular experience, they can guarantee that it's a solid and positive experience.
So for our AI and our characters, we definitely need a systemic and non-scripted solution to deal with this problem.
In our current games, we choose to deal with this in layers.
So if we look at this example of AI positioning, here we have a bunch of AI around the player, and we don't want to lose this idea of a sort of non-linear 360 degree combat situation.
So, our first simple heuristic would be to get within range of the player.
And this can work, has some obvious issues, you know, the player could end up feeling swarmed, there's no real feeling of progression, and with this sort of simple approach, there's absolutely no control on pacing.
So let's build on top of that.
So first we add a layer of behavioral annotation to the world.
In this case, you know, we can add cover objects.
Data driving behaviors is a constant theme in our solutions, giving our level designers this control to actually place objects in the world which have associated behavior allows them to now control and guide positioning while keeping things systemic.
Associating behavior with the environment also makes it really easy to realize those actions, and this makes it very convincing for the player.
Well, what about the NPCs that actually don't find or have any cover?
So we choose to add in a backup behavior, like utility-based positioning.
So using a simple utility map that's relative to the player allows for much better positioning than simply running within range.
And simple heuristics could include the current occupancy on the map to prevent clustering, the current distance to the player, or the distance from where I am to the new point I want to go to.
All of these things help stage our characters better.
So we've got our environment and we've got our utility fallback.
Now I want to come back to this idea of providing a good player experience, which is what linear games have over open world games.
So we can actually add a layer to specifically contribute to this.
And I call this on-screen realization.
It's an example of it.
So, you know, focus positioning based on what the player sees.
So some examples of this are...
AIs that are actually on screen choose to pick lateral covers over covers that are actually in front of them or behind them.
And this allows for a lot better staging and realization for our AI.
We can do things like sliding into cover sideways and it looks cool.
And it's engaging for the player.
We can do things like AIs that are actually on screen will choose positions that keep them on screen.
You know, this is really annoying when NPC characters run off screen and you have to chase after them.
Once spotted, AIs can tend to stay within quadrants of positioning, and so, you know, the player can have this relative knowledge of where they are, even if they go off-screen.
And, you know, we heavily also use this for behavior synchronization or distribution.
And all of these things really add to that player experience.
Finally, we can add a layer of pseudoscripting to provide our LDs a lot more control if and when they need it.
And I use the term pseudoscripting because what we really want here is to allow our level designers to treat our AI like We want them to actually be like free-spirited parents when it comes to our systemic AI.
An example of this is like area restrictions, where you can actually keep the AI bound to specific locations, allowing for better control and pace, and giving the player a sense of progression, while again keeping things systemic.
So to sort of summarize, try and use a layered systemic approach to deal with variation and huge heuristics that cater for the player experience.
So we want to build systems and not moments, systems that work with and for the player, not just in specific moments of gameplay.
So slightly shifting our focus now onto behavior specifically.
As players, we want NPCs in our games to behave compellingly.
And this includes having a wide range of actions to perform and meaningful ways to respond to those different contexts.
And in open world games, we need to worry about the size of the world and the complexity of decisions for our NPCs.
And for this next section, I'd like to go into more details about how we specifically deal with behaviors.
So for the kinds of games we're making, the goal is to have an annotated world that provides enough contextual information to allow our AI to behave systemically.
So we really wanted a way to represent our behaviors that reduce complexity and are easy to understand.
And this empowers our designers and our players.
So at Avalanche, we use behavior trees for our NPCs in our games.
I'm not going to really go into details about the pros and cons of different AI architectures, but I want to iterate our motivations for doing this and why that helps us.
So, currently, we have our designers that actually do all our behavior scripting.
And behavior trees are extremely designer and developer friendly.
They're easy for our designers to actually understand and script how the AI will actually behave in different situations.
Within the trees, we can also give them specific control on how those behaviors will be realized.
So here's an example of a behavior tree in our behavior tree editor.
It's been developed by our very own Joachim Jacobsen.
designers go in there, author, tweak, and debug behaviors directly.
So the tool supports debugging a frame execution through the replay of game frames.
We can pause, go back a few frames, step through, you know, find out what's going on just the way you would debug a regular program.
These visualization and debugging aspects have vastly improved development time and made the creation of new behaviors a lot more robust and manageable.
So going into sort of now the advantages of behavior trees for us, behavior trees inherently support a predetermined priority of behaviors to execute.
And this works really well for us.
We've chosen to have our NPCs have a unique set of core deterministic behaviors.
Now this helps us deal with complexity, and it also keeps our realization cost low and also makes it easier to debug.
But a key advantage is that it makes it much more obvious to the player.
So making behaviors obvious to the player in our games allows for them to interact with the AI in meaningful ways.
You know, if the player doesn't understand what's happening, it isn't happening at all.
Behavior trees also support a multi-layered architecture.
And this makes things like pseudoscripting really easy.
So, you know, we can override and influence behaviors simply by having a scripted layer exist on top of any level of our tree.
And we do this heavily for staging and setting up our characters, and also for setting up specific scenes for driving narrative forward in-game.
This layering also helps us maintain context when handling events that the AI must react to.
Open world games are extremely reactive.
And the advantage of this is that the deeper you are in the tree, the more clearer the context of the situation.
So we can choose to actually handle these events at any level, and at that lowest level if we need to, and have fallback handling in place for other cases.
And this helps us deal with realization and improve it.
So, you know, you might think that having a predetermined set of core behaviors actually limits behavior variety.
And while this seems limiting, we use the concept of sub-behaviors to sprinkle in variation.
So you can think of sub-behaviors as really small mini-behaviors that can either run in parallel with your core behavior to simply provide realization variety or temporarily actually interrupt a core behavior to perform a specific action.
And the choice is really up to you here.
So we use a lot of these little mini behaviors to create variety and break up behavior synchronization.
Sub behaviors are also really great place to use fuzzy logic in terms of, you know, when you want to choose to actually execute them.
So a quick example here is say you have two NPCs that are running their core cover behavior.
You know, they're peeking and shooting, they're evaluating for better cover, they're doing all that fun stuff.
And one of the NPCs suddenly decides to reposition.
So he simply writes that information onto our blackboard, as Michael pointed out, just information for everyone to read.
And the covered behavior itself can simply have a sub-behavior running that reads this value and plays a one-off animation to ask him to move forward.
This really doesn't need to be synchronized at all.
It gives the illusion of coordination, and it adds behavioral variety.
you can choose how often to select these sub-behaviors, and they have a high chance of actually being reused.
So you can reuse this sub-behavior in a search behavior or in any other behavior if you choose to.
So, recently we've actually extended our behavior trees to support external trees.
External trees are behavior trees that can be loaded in from an external source and are run through the main behavior tree.
So currently, these are plugged in throughout the environment, and game objects in the world themselves can specify these external trees.
And this, again, helps us data drive behavior.
So in my previous example, the sub behavior I actually talked about of actually playing that one-off animation, it could actually be specified in the cover object itself and executed by the NPC through an external tree.
Other examples of where we use this are contextual actions that are spread across the world of the world And NPC search for them and interact with with those objects So, in my final section, I'd like to talk a little bit about how we go about realization and making our actual characters believable.
So, realization is important because it's the secret sauce.
It's what makes everything come together.
I mean, Daniel Brewer yesterday talked about, you know, how everything came together when he was building that NPC.
And it includes everything from animation to sounds to barks.
But for the purpose of this presentation, I'd like to focus on behavior and animation.
So the biggest challenges here are maintaining fidelity for a wide range of situations and dealing with a limited amount of memory.
So when it comes to AI and animation, it's common practice to have a layered setup, where your AI and animation systems are quite separated.
Like in most games, in our system, we use state machines for representing our character states and animations.
AI info is actually gathered by our behavior trees.
The trees are evaluated, and they can send events to our state machine.
The NPCs can transition from state to state and perform their associated actions.
So in our actual system, we have several state machines that actually link to different animation layers within the animation system.
While we actually want to keep our animation system separated from our AI and state logic, there are actual advantages in keeping the behavior trees and state machines in sync.
There's a lot of situations I've seen where behavior tree logic continues to run even when the state machine can't, and more importantly, shouldn't handle the request.
So say, for example, you have an NPC that's sliding into cover.
Well, you probably don't want this character to be running his look-at behavior and try to look at other characters while doing this.
So it's important to provide this feedback loop from your state machine back to your behavior trees and have the trees actually understand what state the character is in.
So in our system, we use the concept of state bits that inform the behavior tree on what state the character is in.
And these state, it's a really simple concept.
The state bits are simply set on entering a state and are removed when you exit.
We've also extended this idea of data-driving behaviors to the character system as well.
So previously, our character logic was encapsulated in what we call character controllers.
These tended to be extremely cumbersome.
and very bug-prone, because they handled a lot of logic and were shared across many states.
So you can imagine a lot of special case branching, and it made debugging a huge pain.
So now we've actually moved over to more of a component-based setup with the introduction of state tasks.
You can think of state tasks as little snippets of code, and each state can actually pick and choose what state task it wants to run. So anything from handling specific input to updating camera settings to actually all the way to how we actually update our character motion is now handled in state tasks.
And you know, it's modular and it can be reused.
So this whole thing has been exposed to our scripting system as well, so we can actually data drive character logic. And this again speeds up development time and helps us deal with handling lots of different character behaviors and contexts.
Similar to external behavior trees, we also use what we call external state machines.
This is a way for us to actually load in an external state machine, along with an entire external set of states, transitions, and a default set of animations, and connect it up to the original state machine to extend it on the fly.
Once we're done with it, we simply disconnect this machine.
So, with this approach, game objects themselves can specify external state machines that they want to use, and we can actually go in and override those default set of animations.
All of our NPC interactions within the world are handled this way.
So, for example, I'll use ladders as an example.
The ladders can specify an external state machine for interacting with objects.
And the ladder game object itself can override the default set of animations with ladder-specific ones.
So...
the external state machine and the ladder animations aren't actually loaded in unless the ladders are part of that location.
And so with an even placement of these interactions within the world, we can save on memory for our animations.
There are also a whole bunch of non-standardish features that we do within our animation system, like heavily relying on warping techniques, using specific additive and IK systems that help us improve fidelity and save memory.
But I'll have to save that for another talk.
And you can find me later if you want more info.
But to summarize, if you haven't been listening for the last half hour, here are your key takeaways.
Focus on player-centric and systemic solutions when dealing with open-world AI.
Take advantage of layering, both at a behavior and animation level, and data drive as much as possible, not just values or parameters, but don't be afraid to data drive behavior and character logic as well.
I'd like to thank Niklas Noreen and Omar Shakir for their help in putting this presentation together.
I'd like to thank all of you as well.
Now, please welcome Aaron Canary, who will talk about.
Open World AI and Saints Row.
A quick show of hands, who has shipped an open world game?
Maybe 5-10%.
Who is currently working on an open world game?
Probably 30.
She's curious.
So I'm going to be continuing the talk on open-world AI.
I'm going to hit three main topics today, open-world response, scalability, and environmental interaction.
I feel like these are unique, but also really important aspects in making open-world games.
And because it's a short talk, I'll assume you already know all of the ins and outs of normal AI and implementation.
So you don't have to take pictures.
These slides will be available on our website.
My name is Aaron Canary.
I started my gaming career on Saints Row 2, where I made AI intentionally look drunk.
Red Faction Guerrilla, I made a number of gameplay systems and activities.
Saints Row 3rd, I re-engineered core combat for faster iteration and implemented a number of special AI behaviors.
And since row four, I was the only AI programmer before the expansion.
And then after the expansion, I became the AI lead.
But after being at Volition for seven years, I still feel like the new guy, because we have titles going back to 1995, and most of those guys are still around.
Also, just to update everyone, we were part of the THQ bankruptcy, but we were bought out by Koch Media.
Volition was picked up by Koch Media. You may know them as Deep Silver, also releasing Dead Island series, among many, many others.
And in case you've been living under a rock, Sancero the Third is a third-person open-world shooter.
adventure series with a comedic plot and a large variety of gameplay activities.
First topic, open world response.
So, in Sandstorm 2 we developed a lot of world systems and spawning systems that would react to things happening in the world.
When cars caught on fire, fire trucks would come and put the fires out.
When pedestrians died, ambulances would come, revive the pedestrians.
And when pedestrians saw something violent happening, they would pull out their cell phone and call the cops.
And then moments later, the cops would arrive and kind of walk around and see what was going on.
But we also had a system called Notoriety, and this was to kind of punish the player for being violent in the open world and to give that sense of a real place.
And this would spawn police cars, SWAT cars, boats, Apaches, and tanks.
This tended to escalate quite quickly, depending on how violent the player was.
But at the end of the project, it became pretty evident that we spent most of the time on this large scope of all the things that could come and be in the world.
And we didn't really focus on the player experience.
So Saints Row 3, we started looking at, how do we vary the gameplay?
And we developed a number of specialist AI that would encourage the player to react differently in these situations, cause the player to dodge, to kite, to take cover, to flank.
And this really helped break up the pacing of the gameplay, keep it a little bit more dynamic.
So that's the first key takeaway, vary your player's response.
And having AI and spawning systems respond to current events makes the world feel alive, but it's often better to provoke the player to play the game differently.
The early concept of open world combat pacing through the Notoriety system was to spawn a vehicle every X seconds up until you hit a cap.
And to escalate the fight, you would change the spawn rate and the cap.
And this had a problem where we typically called this the trickle or the flood.
Either there was not enough enemies to challenge the player or there was too many enemies.
And this often is very vulnerable to balance changes, but it's also dependent on who's playing the game, whether or not they can handle the spawn rate.
And we thought this was pretty dependable. But then in development SR3 we found there were really big unintended difficulty spikes. And it took some testing but we found out that this was actually coming from the ambient system.
Notoriety would spawn six cops to attack the player and the ambient system would spawn four cops to go get donuts. And these four cops getting donuts would see the player and turn around and attack him. So now your easy encounter just doubled in size.
And these kinds of accidental overlaps seem to be happening more and more frequently because we had more spawn systems in Saints Row 3.
We had flashpoints, which were kind of like outposts or static locations in the world where many enemies would spawn and they would kind of loiter there and have some...
activities that they were doing. And this conflicted with notoriety because if you walked into one of these and you already had 12 enemies and then notoriety, we changed notoriety so that it would measure how many of these people already existed and just backfill how many ever it wanted to spawn at the time.
It wasn't until Sans Serif 4 where we finally rewrote the spawning to spawn in waves.
This is kind of your typical survival mode.
But as you can see here, it still doesn't handle the variety of enemies that we had.
On screen here, you can actually pick out seven different enemy types.
And I assure you, there was no designer or piece of data that intended this to happen.
So, we've still got some work to do on it, but the takeaway is to try to have all of your systems work together to communicate or have a management layer to maintain the big picture of what is attacking the player.
So we kind of had this escalation and we had added some variety and but then we had just kind of you climax and you just stay there forever.
The only way to get rid of this notoriety was to just run away.
You had to go back to your safe house or go to some designated location to make the notoriety stop attacking you.
We really wanted to have the player beat Notoriety as a saint would.
And to do this, we wanted to look at using a dramatic arc.
So that way you can kind of climax and then fall off and have the victory condition of being really satisfied that you completed something.
But the problem with this was we never really put a resolution on notoriety before because we didn't want to reward players for just slaughtering police left and right.
Surprisingly, we have morals.
And so in Saints Row IV we changed notoriety to be aliens, and now it was perfectly okay for us to reward players for killing them.
This resulted in the Warden. This is the capstone AI for the Notoriety system.
And you can see this kind of follows a dramatic arc of there's really intense combat, followed by some suspense, and then finally some gratification when you finally beat it.
And now you know that Notoriety is done. You have completed it.
Later in production, we found that players wanted to exit Notoriety earlier.
So we came up with the Golden Sid.
This is a high-speed chase where you chase a character through the city, and it follows the same dramatic arc of high-intensity gratification and relief.
So the takeaway here is to try to make your open world experience rewarding.
and also dramatic pacing and cool animation and effects are a pretty good way to do this.
Saints Row has always been a really fast-paced game and Saints Row 4 introduced superpowers.
So our typical response of spawning a car 100 meters away and having it drive to you was completely obsolete.
So we needed to find new ways to bring the action to the player almost instantaneously.
And then also, there was a problem of players could simply jump over a building and exit all AI combat.
AI had no ability to follow them or chase them over top of buildings.
So our first response to this was the teleportals.
These were destructible monster closets that could spawn anywhere over open nav mesh.
And we could pop these in right in front of the player and have them start dropping enemies.
And thankfully, this was supported by the context of our game.
Second method we used was transmogrification, which is a fun word we made up.
it would transform an existing enemy into a more difficult enemy and do it in a very obvious, in-your-face way so the player knew something was going on.
And then the third method we used was enemies just falling from the sky.
We used some camera tricks to get this to happen right in front of the player and introduce him into combat immediately.
This was also used for a specialist, and I think at one time we had it running on Roddy Piper.
But actually the takeaway here is to be creative.
Try to find ways to present AI to the player that is in the context of the game and also sports the mechanics and the pacing of the game.
So the whole takeaway, I'm just going to wrap all this section up, is to bring all of your spawning systems together to provide a varied and rewarding experience.
Next I'm going to go into some scalability challenges we've had in open world games, and later you'll see how these tie into the environmental interaction.
So one of the systems I've been in charge of iterating on, on the Saint-Tropez franchise, is line-of-sight processing.
And of course, your line-of-sight is proportional to the number of NPCs you have.
We typically have 75 NPCs.
So, off the top of your head, you might think, well, worst case is all 75 are attacking the player.
Or 75 are attacking... 72 are attacking you and some friends.
And then, well, we also allow scripting for some reason.
So then there's a base invasion where you have a large number of allies and a large number of enemies.
And you can see this is almost growing exponentially.
So in this scenario, this worst case scenario, you're looking at around 1,500 line of sight checks.
And we throttle them at four per frame.
So this would take 12 seconds to resolve.
And that's way, way too long.
So here's kind of more of an average run of the mill case, 4 versus 17.
And we did a lot of work on iterating and to see how we can prioritize and filter which four line-of-sight checks you're going to do every frame.
And this had an additional cost to it.
But here you'll see the green lines are the completed line-of-sight checks, and the blue lines are the deferred ones.
And also, if you look down in the corner, you'll see delays between 1 and 11 seconds.
And then it gets worse.
So with this extreme bias down in the green letters, or the green numbers, you'll see you can get reasonable response time.
But this causes some starvation and some AI that simply just don't respond to anything off screen.
And then this happened.
Um...
They scripted a mission where there were three teams all hostile to each other.
And this just gets worse.
And so now we're looking at even longer response times.
So this happened basically right before we shipped Sensor 3.
And more or less, it was OK because in this scenario, there was so many things blowing up and so many people shooting each other.
As long as they weren't standing still doing nothing, it looked OK.
But Saints Row 4 we started looking at, okay, how can we do more than four a frame? This is ridiculous. We tried optimizing and prioritizing things. That didn't work. So let's just try some brute force methods to see what else we can do. So I started looking at multi-threaded raycast. And we wrote our own code to do this. So it would work on up to four processors or on all the SPUs. And...
And we wanted this to use it for immediate results.
We wanted to just say, OK, do all of these right now.
Do as many as we tell you to do on all the threads.
And just let me know as soon as it's done.
And this actually led to some interesting techniques I'll get into later, was the flying AI and the jumping AI.
But to get back to the line-of-sight casting, we had the perception system and the line-of-sight deferred system.
We'd build up a cue, and it would process the four in the main thread.
Same for four, we did the same cue.
However, we determined that while we were processing effects...
there was nothing going on in the worker threads.
So we said, okay, let's push all of them we can into the worker threads.
But there's more than one thread, and there's more than one system that is not multithreaded.
So now we're just using up all the idle space in the worker threads, and instead of doing four raycasts, we're now doing 80.
So now all of a sudden, prioritizing doesn't really matter as much because we can do so many of these.
And what used to take 15 seconds, now we can do less than one second.
And if you do want to prioritize them, to just do the guys on screen right in front of you, you're getting response time of 0.1 seconds.
And this was excellent.
So, takeaway here is to try to multi-thread whatever you can.
When you have this number of NPCs and AI running, you end up with, you know, N-squared hard problems, and throwing all the threads you can at it is sometimes the only way to solve it.
Next I'm going to dig into some architecture changes we made.
As you're probably familiar, finite state machines...
Really fun to use!
and over two projects they kind of get messy.
This was our third game in the series, and it was, we just called it spaghetti code at that point.
There was so much overhead and just adding, I just want to add one new action, and it would take me like a day and a half to just fill out the blank functions before I could start writing any code.
and the number of dependencies between the new state and all the other states that already existed.
It was just, it was ridiculous.
And the worst part was the more you add to it, the worse it gets.
So we were done with finite state machines, we needed to look for something else.
I mean, the world has moved on.
So we wanted something to reduce our overhead, so that we could really pump these things out.
We could work on the real code that we wanted to work on.
And we wanted something more modular, so that we could reuse things very easily.
We were also concerned with performance.
And again, we have 75 NPCs.
And there's also Sans4 is a very fast-paced game.
It's very volatile.
So we had used planners before.
We actually had one in-house.
But we didn't really feel committed to using it because we didn't want to plan that far ahead.
The game state and the world changes so frequently, we weren't sure if there was going to be a strong benefit in it.
And we also wanted something very deterministic.
We don't tend to roll dice because we consider the player the largest random variable.
So, to kind of walk you through this a little bit.
it closely resembles a behavior tree, but like I said, we were already pretty familiar with a planner. So we started with a list of goals, and you iterate over these list of goals until you find one that's valid at your current time. And then that list of goals has a list of actions to go through and try. And we'll go through those until it can find one. And there was essentially one behavior tree.
for all the AI.
And then this allowed us to work out all the dependencies we needed to.
And these dependencies were fairly strong because it was, well, the dependencies were represented in the data.
So it was very easy to iterate on these very quickly and determine where the problems were.
But this was complicated enough that we kind of said, after a point, this is programmer only.
Designers don't really want to dig in this deep.
However, designers are, we're highly encouraged to filter the goals and actions that each AI could do and tweak the parameters on those goals and actions.
And this became pretty powerful.
The first time we realized the power of this was a producer came in and said, hey, can you guys make Burt Reynolds a follower?
So since we, well, but he can't use guns, and he can't die for legal reasons.
We'd already had invulnerability as a flag in a table, and we already had melee behaviors and the movement and stuff that we needed.
So this took about five minutes.
It was just copying around some data in a table.
And Burt Reynolds was done.
Also...
Another explanation of this, if you find yourself writing is Burt Reynolds in code, you're not doing data driven correctly.
Burt Reynolds is the sum of the parts, parts that should be reused for other AI down the road.
To really hit this home, I'll go through the Warden. This is one of the most unique AI we had in the game and definitely the one that we iterated on the longest.
However, if you actually break down what his behaviors are, he had 26 behaviors, and 13 of them were completely copied from the Saints Row 3 cop.
Basic movement, getting line of sight, shooting, a lot of these things were exactly the same, had not changed.
And then a lot of the behaviors that we made for the Warden, we also reused on many, many other AI in the main game and DLC.
So you can see there's a lot of reuse here.
There's only three behaviors here that remain completely unique to the Warden.
So takeaway here is make your job easy, because you're going to be doing it a lot.
And reduce your overhead will pay huge dividends in the long run.
being data-driven is one really good way to do this.
And focusing on your efficiency upfront will allow you to be more flexible and creative down the road.
Next, I'm going to get into environment interaction, which is talking about how to connect the player and the AI to his surroundings in a more meaningful way.
And this can be difficult to do in a large city or world.
One of the basic ways we did this was action nodes.
Action nodes are a custom set of animations, props, dialogue.
And these were placed, I think we had nearly 500 of these placed around the open world and more inside of interiors.
But these were actually really important to experience because it would not only make the world feel alive, but also kind of ground the player when they're done doing something absolutely ridiculous.
It kind of brings them back down to something normal that they can relate to.
But what I really want to talk about is...
500 action nodes hand placed in the world.
That's a lot of data.
And that takes man weeks to do, even on a good pipeline.
So we're actually focusing a lot of effort now on how to improve that pipeline and make that more efficient so we can do more of these at higher quality.
Next, I'm actually going to hit on reusing markup.
We had some procedural systems, like the roadblock system.
And what this would do is it would find existing road data in the world.
And we made sets of two, three, and four lane roadblocks.
And it would match these to the existing roads around the player.
And this was awesome because it was perfectly scalable to the entire world.
We didn't have to add any markup data to the world to get this to work.
So, again, just reuse your markup.
However, there's a caveat to this.
Working without markup or reusing markup as it was not intended to be used can be very tricky.
Here we have an example of a brute who is going over to pick up a propane tank to throw it at the enemy or whatever.
However, the collision data on this is about twice as large as the actual propane tank.
So this doesn't line up. It looks pretty bad.
This was taken from a bug report saying, yeah, this collision data is terrible.
But that's really the point, is that when someone made this collision data, they weren't thinking about how routes were going to pick up things.
So you have to make some assumptions about your data, but then you also have to go back and correct it where it is wrong.
We were able to iterate this and fix most of the bugs, and it was fairly reliable when it shipped.
However, sometimes it's not as easy as marking a propane tank.
When we switched over to Saints Row 4, it was originally an expansion pack.
And we rethemed the city to be more alien-like.
And the biggest problem for me was that now players could run up walls and jump on rooftops where there was no nav mesh, no markup, because gameplay had not existed there before.
So, how do you have combat without NavMesh?
This was kind of an interesting problem.
So, we invented some jumping code, and this was completely not reliant on NavMesh at all.
This was using, leveraging the Raycast multi-threaded stuff I was talking about earlier, where you're just gonna do Raycast down from the sky, or down from your height map of where the helicopters fly.
And first thing you can do is find where they hit the ground.
And then run a heuristic to sort which ones are most appealing to the AI.
And then test jump arcs to multiple of these at the same time, since we can multi-thread these.
There's a little bit lower cost to batching them up.
And then you can evaluate and choose whichever one you feel is most appropriate.
Here it is in game. As you can see, the AI is actually jumping down to the ground in this scenario.
And it's running heuristics on all the phases of this to isolate the best end position.
But when we implemented this, the behavior was only intended to be used on the warden.
So we knew there was only one AI running this in the world.
Because we were a little afraid it might be expensive.
But the designers loved it, and they wanted to use it more.
So we determined, well, followers kind of need it to be able to chase the player around.
And, well, specialists need it to chase the player around.
And then it just kind of caught fire in missions and activities, and everyone just started using it.
And the profiling guy never hunted me down, so I guess it worked out.
And then we also wanted to do flying, because who doesn't want to do flying?
But we didn't have markup data. We didn't have voxels or anything like that.
So we started with just a simple avoidance algorithm.
And well, of course you can say, well, what about the U shape?
How do you get out of U shape without pathfinding?
So we actually just tried, well, let's do a raycast away from the player every frame.
And we'll cache these up over time.
And then what the AI can actually do is query this cache of raycasts and say, well, which one of these is further away than me?
And so then they knew, okay, if I move to this position, I will now have line of sight to the player.
And this didn't always work. It worked good enough.
And typically when it didn't work, the AI was stuck in a corner where they couldn't see you and you couldn't see them, so it wasn't really noticeable.
So, just some little tricks.
Here it is in game. These AI on flying bikes are chasing the player through a tunnel.
And this was really responsive and this is something that our helicopter AI could never do because they were not that maneuverable and also because there's no height nav mesh in a tunnel.
This was really useful for interiors and very complex overpasses and industrial areas where there's lots of catwalks.
And these guys would just follow you down a rabbit hole if you went down one.
This is really tricky though. I consider this high risk, high reward development because there's a lot of bugs with it, but when it works, it works everywhere across the entire city where you don't have to have markup data that takes up a lot of memory and takes up a lot of time to do it manually.
So, to wrap it up here, I think we should all consider in our open world games how to provide the player with a varied experience, a rewarding experience, with some pacing, how to make the AI and developer better, faster, stronger, more efficient at their job.
And how to live with or without markup data.
It's a very important question.
And Naya, do we have time for questions?
Five minutes for questions.
All right.
Thank you very much.
OK, great talk.
Question for Jeet.
First, I request, please make a full talk about player modeling, player modeling.
And the two questions are maybe linked.
Can you talk a little bit about annotation, what annotation, and what you did?
And do you use the external state machines for anything else than animation?
OK, so the first question in terms of player modeling, Basically, that's just a design exercise for you to be able to understand what...
what actual AI components you can actually add to create variety in the game.
And with regards to external state machines, it's about really overriding or connecting up.
So the memory itself of the actual state machine needs to exist as well.
So if you have them specified externally and split across the world, you can only load in what you need.
So it's more the states themselves and also the animations.
OK, but can you perceive that an affordance exists in the world when there is a state machine attached to it?
But because the actual game object is separated from the state machine, you can.
You can still search for those game objects.
And you can think of things like you only load in, you only connect up the state machine when you actually enter trigger volume, for example.
That's when you actually, on the fly, hook it up.
Thanks.
Let's thank him again. That was a lot of information. A lot of really good stuff. Appreciate it.
