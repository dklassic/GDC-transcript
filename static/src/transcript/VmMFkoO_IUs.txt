Welcome to the part 2 of this presentation about bringing feature film effects to real-time VR experience.
This talk will mostly focus on the technical side, well, a lot of math.
So I will try to make it not boring and show how we actually implement these effects in the engine.
First a little bit about myself. My name is 4Hu, this awesome guy.
And I used to be a pipeline TA working on AAA games like Dead Space and Battlefront.
These days I'm a look dev VFX DD working at ILMxLAB.
During the past three years I did a lot of amazing experience including the Oscar-winning Conan Arena and multiple critically acclaimed AR VR experience.
First, I want to talk about the shader effects we used in the Ralph Breaks VR experience since it's the most recently released project.
It needs to have a high visual bar as a Disney animated feature film and needs to run at 90 fps double-eyed in VR.
Also, we have a lot of complex gameplay, so we've got to finish all of this in a very limited time frame.
And here are some examples showing all the moments that were created inspired by the movie.
For example, you see the hub turning on, and we go through the hyperspeed internet cables, and we went into the Dunderdome realm, and it's sparkling.
And it's continuous sparkling, and you can see the awesome spaceship.
And also the binary transition between all different worlds.
All these effects serve the purpose of storytelling and give life to the environment.
And I think we did manage to get a feel like inside a Disney animated film.
Well, the internet is huge and each place is so different from each other, which means each place has got its own unique challenges, both visually and technically.
And Disney Animation Studio did a fantastic job in the film to give you this amazing experience.
And we are aiming to achieve the same and maybe more with hyper-reality technology.
So here I'm going to break down some of the details of how we created these moments.
So the first one is the hyperspace.
We need to make you feel like traveling through the high-speed internet cable like Ralph did in the film.
And it needs to perform at 90fps since it's VR.
So apparently, we cannot do 1,000 meter polygons inside an engine.
And I implemented all of this by separating all these elements into different layers.
And you can see the different color lines represent the netizens traveling with you at light speed, and gradient patterns across the cable that makes you feel traveling through a super long distance.
And with different layers, they got different motion settings in the shaders.
All this combined makes you feel you are really accelerating at light speed, while in reality, you're just standing there admiring all this awesome art.
So on the left is the shader parameters that I exposed to designers.
And on the right is the snap code I did the motion blur in one direction that created this illusion.
Since it's dynamic, we can iterate the look very rapidly to get what we want.
In the end, by just using a simple cone and a few textures, you almost got the exact same effects in the film.
Next, we come to the Dunderdome, which is heavily inspired by the 80s popular arcade games with a lot of fun gameplay.
I bet someone here still remember when you play this kind of games.
To match that special visual style, we have to figure out a way to get the glitches and jittering and all the crazy stuff happening in the old CRT screen, but it's got to all working in full 3D environments, while in back in the old days, it's just 2D pixel animation on screen, right?
And maybe only a few hundred lines of code to implement the whole game.
So how we convert the pixel art style in a full 3D immersive experience.
And this is what we come up with just using a simple polygon lines.
But you can see in the right corner, it's got all the connecting points and variations of the lines and skyline across them and all the detail features of the CRT screen look after applying the shaders, which is the middle.
where all this magic happened.
And here's the detail of the shader parameters.
And by changing just one of them, you already got a very interesting destroying effects on the right.
And you can see the X-Wing goes into lines and blobs into lots of bits.
I think the mission is accomplished, right?
So, from space mountains to spaceships, every element in the scene is built by the instance of these materials, and the various effects can be created dynamically through gameplay, as you can see the glitch and jitter effects here.
The benefit to do this, you can create different behavior that meets the need of storytelling, and have all the effects tied to the visual look at the same time.
And you can also adapt any changes at any time and don't have to worry about performance later.
The other very interesting thing is the binary transitions.
As I said before, each element, each moment's got its unique look, right?
So since we cannot do a camera cut like we did in the film, how we link all these different places together to tell a full story?
The answer we came up with is everyone knows the internet is made up of a bunch of data of 0 and 1's.
So it only makes sense that we use the matrix system to translate you between different worlds.
And this shader needs to have a way to seamlessly turn any object in the world into binary form.
And this is how it looks like.
I used the cell noise and world position to make sure it works on any kind of surface, no matter how it looks or shape.
And here is another snapshot of the shader.
As you can see, I enclosed all the math and logic and only exposed as few parameters as possible to satisfy the designers.
We all know sometimes designers can be really, yeah.
And all this combination can already give multiple different behaviors.
So here is some detail math about how to make it randomly switching from 0 to 1 across the surface during this transition.
And the key here is to treat the cell noise value as a threshold.
So any input value that's beyond that shows 1, and below is 0.
And on the left, you can see a line crawling effects and a spherical spreading effects in the same shader.
And the logic are on the right.
The secret is to use the cell noise to drive another cell noise.
And that's how you get the line crawling that match the look in the film.
And this is a way to show you can actually use the noise function that's building the engine to get a very complex look, and combined with hyper-reality technology, it serves the purpose of magical transformation and storytelling.
OK, here comes the fun part.
The other big challenge is that we want to have a pancake milkshake launcher in players' hands, which shows on the right side.
And we need to have a jar to store the liquids that make both pancake and milkshake.
And it is very close to the player, so we can't simply use the mask trick to fake the volume compression like we usually did in traditional games.
Since, okay, and since it's closely tied to the gameplay and all kinds of function as real time, it's almost function as a real time simulation.
And we can simply use the vertex animation technique that Martin mentioned just in the first talk.
And I need to figure out a better way to do a real fake volume in VR and with complex behavior.
And the trick to achieve that is use vertex shading.
Here shows how you can change the color and volume in the fluid.
It needs to look like two liquids mixed into each other, but not dissolving each other.
We definitely don't want to mix the flavor between milkshake and pancake, because it really tastes bad.
The other one, the volume decreases, but it cannot leak outside the glass drawer.
Otherwise it will break the illusion.
And the interesting thing here is like we don't have, we don't use any collision detection for the performance reasons.
Okay, what is the trick here?
The key to achieve this is to separate the top layer from the bottom as two separate part and by using a threshold.
So the top part will be implemented as a procedural surface with liquid noise, and here I faked the shading to simulate the translucency.
And the bottom just uses the world Z value to drive the logic to mix the fluids.
It's very simple and effective, and the result is that we meet all the gameplay requirements, while keeping it look really good and efficient.
All these moments are only parts of the effects that we did for the project.
And there's a bunch more down for the environment to get a feature film look into our VR experience.
For example, you can see the capsule bits go through the cards, electronic sparks in this hub, random animated billboards in the internet, bubbles going through the cool internal, and high-reflected fake glass cards that are used by the netizens.
In summary, because it's real-time and dynamic in nature, we can quickly iterate and create complex visuals through shading.
Now here are some bonus parts and some more detailed breakdowns about how we did the lava flea moment in the Secret of the Empire experience.
As you can see, the lava flea needs to distort and push the lava field around him while it's moving to better integrate it with the environment.
And as you see, the lava plane is just a simple polyplane, and all the visual detail here is driven by our shader procedurally, because I really, really hate using textures.
Haha, this is what's actually happening under the hood.
First, we assign a special vertex shader that expands the lava flea to gather surrounding volumes.
Then we use a render target to capture the area that the flea intersects with the lava plane.
It almost functions as a CTR scan we used in the hospital.
And then we use it to accumulate the value of distortion to drive the vertex offset of the lava material to get a trail behind the lava fleet.
And after that, we combined a vertex simulation of the lava splash that Martin talked about in part one, and the glowing belly material of the lava flea, and with all the other animations and audios, we got a full giant lava flea attack moment.
This technique was inspired by Rembok's talk a few years back.
The good thing about this is this works on any kind of surface with any kind of object intact with it, without any additional baked data.
And it can be used on other projects as well.
And that's a wrap.
The takeaway is you can really do a lot of cool stuff in shading, and it's a great way to implement the visual fidelity that serve the storytelling and gameplay while give lives to the environment.
Well, my philosophy is eventually everything comes down to just pixels that rendered on the screen that rendered real time.
So in summary, shading rules.
And I also want to thank for the whole team at ILMxLab.
Again, all this work is teamwork.
It takes every discipline's contribution to get the final product.
I'm glad that we keep pushing the boundaries of what is possible in hyperreality and produce amazing experience that never seen before and creating a more exciting future.
Thanks for listening, and have a great night.
Please follow us on social media.
And any questions?
Oh, another thing.
Please take on the online survey.
Yeah.
The other guy told me I should say so.
Hello.
Thank you.
Hi.
So in VR, did you find the render target solution was a little bit expensive?
Yes.
You're rendering another camera?
Exactly.
So we basically have to use that technology.
It is extremely expensive, especially when you're using a render target by default on real components.
So basically, I work with the graphic engineers just basically rewrite the whole thing.
And we literally only need to capture the one channel.
So it works pretty well in this case.
You just have to rewrite it sometimes.
Yeah.
Hey.
Hi.
So sometimes when making shaders in Unreal, I seem to find something that either it can't do by default or I don't know how.
So I'm.
I'm.
I'm.
I'm.
I'm.
I'm.
I'm.
I'm.
I'm.
I'm.
I'm.
I'm.
I'm.
I'm.
I'm.
I'm.
I'm.
I'm.
Trying to use the custom node a lot.
Same here.
Yeah.
Yeah.
Do you have any tricks for learning how HLSL meshes with Unreal?
Because a lot of that isn't like- GPU germs.
Read all of them.
And get all of- basically it's just computer science, graphic, knowledge, math, things like that.
You just do it all.
There's really no trick, man.
You know the maths.
You get it done.
All right, thank you. I'll try that.
Okay, yeah. It definitely works.
Hello. Just out of curiosity, did you run into any motion sickness shenanigans with the, like, screen space zooming example that you showed?
Yes. That thing, right?
For going through the hyperspace, so the trick to not get people sick is you shorten the time.
So we just basically do this design.
So basically, you're basically, well, first is a cable.
And you're constrained in a very finite space.
At that point, all the interest is looking forward.
So one thing I forgot to mention is that we add some vignette around the corner.
So once you turn this way, you don't really see that much interesting details.
It's almost just like a blur.
So that gives a way to always direct in the focal point and look forward.
And that really doesn't give any motion sickness.
Thank you.
Yeah, it's just tricks.
Yes.
Hello, hi, I'm currently a CMUETC student.
Oh, hi.
Hi, hello.
I know I graduated from our program.
I know.
Hi.
So yeah, our team is actually currently working on a discovery exploring project about like transition like.
adapting traditional film transition technologies into VR.
So I'm really happy to see all the things you did.
Since the title of this talk is called Bringing Feature Film Effects to Real-Time VR, do you mind talking a little bit more about your personal thoughts on future potential just like in this area?
Like combining feature film effects and feature film in future VR applications.
Well, my dream is, I'll talk about dream, and I'm really hyped about 5G technology.
And I think by implementing using the 5G stuff, the VR and AR technology will actually really going to take off at that moment.
Since at that point, we can actually offload all the computation power, and we can use high-end machines and high-end graphics, like just cloud rendering it.
And we can stream all kinds of crazy simulation data.
And now we have crazy graphics, right?
And we can do real-time ray tracing.
Back then, it was unimaginable.
So if we can shift all of this using a much higher computation power than with 5G, like real-time streaming, then, yeah, it will work.
And everything will be real-time, because it saves much, much production and just cuts all the crap about, you know, iterations, time, renderings, waiting.
Knights and not knights.
Yeah.
Yeah.
No more?
Okay.
Thank you.
