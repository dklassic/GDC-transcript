Game development sucks. It's hard and it's stressful. Our industry is notorious for chewing people up and spitting them out as burnt-out husks.
So much so that those of us who've been around for a while are called veterans, as if we've been at war.
Do you know what they call experienced people in other industries? Experts.
Are we experts at what we do? What is it that we do?
And if we call ourselves experts, what are we actually experts in?
Are we experts in working 80 hours a week?
Are we experts in dropping a huge day one patch to fix the thousands of bugs that we missed during testing?
What about losing sleep for a week because you're panicking about server outages and game client crashes?
Are we experts at that?
I don't know about you guys, but I'm an expert at those things because I've done all those things a lot.
But I don't want to be an expert at losing sleep or burning myself out or putting out a thousand hot fixes for a month after launching a game.
I want to be good at making games. Losing sleep and pulling heroics and crunching? That's not making games.
In fact, that's what I would call that. Not making games.
But what if game development didn't suck? What if it was easy?
Imagine if your team could deploy a major patch and take the day off to celebrate because you knew everything was going to be fine.
What if you could deliver a major content update to your game every week without working overtime?
Or if you could take your weekends to actually play games?
Well, I have news for you.
You can do that.
I know you can do that because I did it.
I'm doing it right now.
And if you want to know how, good, because that's why I'm here.
My name is Seth Koster, and I'm the CEO, game programmer, co-game designer, and co-founder of Butterscotch Shenanigans.
And today we're going to go on a journey that starts many years ago when our studio was flailing around, crunching, putting out fires, and burning ourselves out.
I'm going to walk you through some of those struggles and how we learned to use the three ways of DevOps to change everything about how we work.
We'll talk about QA testing, production bottlenecks, reducing waste, small batch deployments, scaling up your team, and more.
And by the end of this talk, you'll be able to use the power of DevOps to make your studio or even solo operation run like a clockwork machine.
Our story starts in December, 2015, when our studio was three years old and was comprised of my two brothers and me.
We had Sam as the artist, Adam as the web developer, and I was the game programmer.
We had launched four games up to this point, and they had varied in success, ranging from extremely bad to decent.
But this time, we thought we had a real winner on our hands.
We were building a huge open-world, crafting-based action RPG called Crashlands.
We had been working on it for two years and it had truly been a grind.
But now at the end of 2015, we could finally see the light at the end of the tunnel.
At this point, we were feeling pretty good about the state of the game.
And as far as we were concerned, it was ready to ship.
But just to really make sure it stuck the landing, we thought, let's do a little beta test, you know, just to shave off the rough edges.
So we reached out to our community and we enrolled 150 players in a closed beta test.
Immediately, we were hit by an avalanche of bug reports.
I mean, broken quests, crashes, missing recipes, people's saves getting deleted.
Anything that you could think of that could go wrong, went wrong.
And suddenly, we found ourselves staring down the barrel of over 2,200 unique bug reports, and we had eight weeks to launch.
So we did what everyone does, we crunched.
We put in 60 hours during the first three days of the beta and fixed as many of the hugely problematic issues that we could, and we hardly slept for the next two months leading up to launch as we tried to wrangle the remaining 2,000 issues.
So we weren't able to fix everything, but we did the best we could, and then we launched.
And the game was well-received.
So in the first week, we had about 100,000 players across our three platforms, Steam, the App Store, and Google Play.
So this was great news, but it also had a dark side, because those 100,000 players crushed us with another enormous wave of bug reports.
Some of these bugs were repeats of the issues that we weren't able to fix earlier during the beta, while others were completely new bugs, some of which we had introduced during our sleep-deprived crunch on the death march toward launch.
So we did what everybody does, we crunched again.
And instead of being able to enjoy the launch, or even get started on some new interesting content patch, we spent two months putting out fires, scrambling to get a patch out, just to repair our broken game.
So eventually we did get that patch out, and the game became relatively stable, so it was time to move on to the next phase.
Content updates.
We worked on some new features for a couple months.
and we were excited to get our first content patch out the door.
But by this point, some things had changed.
It had been just long enough, since we had deployed a patch, that none of our builds were working.
So we had to get everything back up to date, download SDKs and test builds, and finally, weeks later, we released the patch.
And this time, everything went perfectly.
Just kidding.
Just like before, we were hit by another wave of bug reports, which we crunched again to fix.
We carried on like this throughout 2016, and we managed to create only two content patches the entire year.
With each patch we released, it was like some kind of nightmarish Groundhog Day scenario, where we had to relive all the same stresses all over again.
Crunch, heroics, putting out fires, just generally being in a panic.
In the midst of all that chaos, we thought, this is too hard.
So we did what everybody does. We assumed that our troubles were simply due to us being short-handed, and we hired more people, scaling up from three to seven.
We immediately had a difficult time integrating these new people into our production processes, because it turned out we didn't really have any processes.
Almost everything that happened in the studio was ad hoc.
without any concrete systems.
And the three of us founders were doing so many things that we weren't able to take the time to create any kind of coherent onboarding or training structure.
So essentially, we brought new people into our studio and then just kind of set them adrift.
This was extremely stressful for them because even though they wanted to contribute and help, they couldn't insert themselves into this weird shape-shifting amoeba of game studio that they found themselves in.
And worse, when they did get involved, it backfired.
One of our hires was an artist who we brought on to assist Sam in making content for Crashlands and whatever our next game project was going to be.
And she was able to produce a pretty good amount of content, which became a problem.
Everything the art team created, of course, had to be brought into the game.
And as the sole game programmer, I was suddenly responsible for implementing twice as much art.
This left a lot less time for developing game systems, and we ended up struggling to figure out how to make reasonable progress on the next game.
So the art team was getting demoralized because they were creating all these assets, and those assets were just lying around, not getting used, because I couldn't keep up.
So we thought, maybe we should take a look at the processes by which we add art to the game.
And perhaps we can find some way to make that process faster.
So that's what we did.
At that time, our art process looked like this.
The artists would create an asset using Inkscape, and they would manually export it.
At some point later, I would find that asset in Dropbox and manually import it.
which I would then review for errors.
And if something was wrong with the art asset, you know, like a technical problem, then I would send a note back to the art team about what needed to be fixed.
So that artist would then fix it and re-export it, and the loop begins again.
And eventually, once the asset was finally usable, then I would implement it.
So to speed up this loop, we decided to focus on this part here, the import step.
If I didn't have to individually pull files into the game project by hand, then I would save a bunch of time, which I could then use that time to implement art.
So Adam, our web developer, took over this project and soon he had built a program called the Inkbot.
The InkBot allowed me to mass import all of the new and updated assets that the art team had created.
So now I could import hundreds of assets in seconds instead of one asset every few minutes.
Which is pretty sweet, right?
Except for one important problem.
For the InkBot to work, the assets had to be standardized so that the InkBot would know where to look and how to import them into the game.
And to standardize those assets, Adam also automated the export side of the process.
So whenever our artists would finish a creation, the InkBot would just pump those art assets right out.
The end result was that while it was faster for me to bring assets into the game, it was even faster for the art team to create new assets.
So we ended up with an even bigger pile of unused art.
The art team became.
even more demoralized, and I became even more stressed.
All right, so we carried on like this for a year, tweaking our processes and making things both worse and better at the same time, and constantly shrinking our cash runway without much to show for it in terms of new projects.
And by the time 2018 rolled around, everything buckled.
We knew this wasn't working, and we had to change something so that we could move forward.
So we scaled our team down from seven down to four, and we scrapped the game that we had been working on at that time because the game had also become unwieldy due to our out-of-control processes.
So with our scaled-down team, we started to work on a new game called Levelhead.
The vision of Levelhead was to capture the magic of something like the original Mario Maker, which is a game where people build levels and then share them with each other.
but we were going to do it as a fresh new IP with new and interesting mechanics, a better social system, and make it available on way more platforms.
The first year of Levelhead's development was pretty much just like our Crashlands development.
Lots of hard work and brute force.
And within a year, Levelhead was far enough along that we started to think about launch.
What was the launch going to be like?
And we thought back to the Crashlands days, and we realized that Levelhead was going down the same path.
Launching and maintaining this game was going to be painful and expensive and difficult.
The game wasn't being actively tested, so it was probably full of bugs, and we were going to have to crunch and panic and go through all the same motions all over again.
So if we wanted to make this game right, something was going to have to change.
As part of our research to find a better way to do our work, we started learning about something called DevOps, and we were inspired.
Over the next year, We started applying DevOps principles to our work and we turned the whole ship around.
We were able to put Levelhead into early access and maintain a 97% positive review score on Steam while deploying two to three content patches per month to our players on average.
It's so easy for us now to make deployments that we deploy up to four patches to our new internal QA team per day.
And Levelhead also now supports 11 languages, and adding a new language takes almost no development time.
In the background of all this, we've developed stable builds of Levelhead on Xbox, Android, iOS, Switch, Steam, and UWP, and we're gonna launch on all six of those platforms simultaneously.
And we even scaled our team back up to six people without any problems.
And with all of that going on, we're not crunching.
We're working regular work weeks, and we take weekends off like normal people.
We get to have hobbies outside of work during our free time, and we even take vacations.
I hope that piques your interest because the rest of this talk is about how we managed to pull all this off over the past year.
As I mentioned earlier, we were able to do all this through applying DevOps principles to our work.
So let's jump right into DevOps.
First, some definitions.
DevOps is a set of operating principles that focuses on faster deployment cycles, high stability and reliability of your product, and better communication between teams.
It recognizes that there's a mutually beneficial relationship between development, the creation of your software, and operations, the deployment and management of the software.
DevOps is broken up into three parts called the three ways.
And each way is essentially a different lens for you to use to get a better perspective on your work.
The three ways are managing the flow of work, amplifying feedback loops, and continuous learning.
So we're gonna go through all these.
First, let's jump right into the first way, managing the flow of work.
The first way is all about understanding and then managing the flow of work through the company and ensuring that the work flows in one direction from development to operations to the customer.
The core tools of the first way are to eliminate waste, make the work visible, deliver the work in small batches, prevent defects from moving downstream, and align everything toward the global goals.
We're gonna start by talking about waste, because understanding waste is the pillar that holds up pretty much everything else in DevOps.
So first, what do I mean when I say waste?
Waste is anything that you're doing that isn't delivering value to players.
In other words, waste is not making games.
Waste comes in all shapes and sizes, and it's the silent killer of your production process because it's always a lot worse than you think.
And it tends to be self-perpetuating, because waste creates more waste.
We're going to go through all the types of waste that you will come across, and as I talk about these, I want you to think about ways in which your own processes might be creating these types of waste.
So here's a little chart that I call the circle of waste.
As I explain each type of waste, this chart is going to become increasingly tangled to show how all these different kinds of waste have spillover effects in ways that you might not anticipate.
So let's start with motion there in the top left.
Motion is waste produced by the movement of materials or information between people or departments.
So think about this situation that might sound familiar.
You need something from someone, so you send them an email.
But soon, you end up emailing back and forth a dozen times, trying to communicate what you need.
And eventually, a week later, you just have a meeting instead.
That's motion.
And motion is waste, because the whole time you were doing all that, you weren't delivering value to your players.
And even worse, that process produces a lot of waiting and task switching.
To cut down on motion.
You need to create self-service systems like good documentation so that people can retrieve information or materials themselves when they need it, instead of requiring somebody else to bring that information to them.
Task switching, the next type of waste, is when someone has to move between various contexts, like perhaps going from a meeting to production to back to a meeting, then to lunch, then back to work, then another meeting.
Not only does task switching make things take longer, which causes waiting, all that mental exhaustion from context switching leads to excessive creation of defects.
Task switching can be minimized by batching work as much as possible.
So for example, save your meetings until the end of the day and check your email in one or two big bursts during the day instead of constantly throughout the day.
So there's tons of ways to batch your work and even doing it a little bit, even just yourself will go a long way.
Next up is defects.
A defect is when something that you have produced has something wrong with it, making it unusable by your downstream customer.
And by downstream customer, I mean just the next person to receive that thing.
So maybe it's an art asset that's the wrong size or a sound file in the wrong format or a blog post full of typos.
When someone runs into a defect, they have to stop what they're doing and send it back to get it fixed, which creates motion waste.
This forces them to task switch because they planned on doing one thing, but now they're responding to an error instead.
And this also leaves that person with an unfinished project while they wait for the defect to be resolved.
It's even worse when a defect makes it all the way to a player, because if it's bad enough, like a game-breaking crash, Then you have to engage in heroics, like pulling all-nighters to deploy an emergency hotfix.
To cut down on defects, you need to fix the processes that create them in the first place, which we'll cover in a little while.
Next up is waiting.
Waiting is when something is supposed to happen, but it doesn't.
Waiting is obviously a waste because it's basically just standing around, but it also leads to other problems.
When you're stuck waiting for something, you will have a tendency to not start work on a project because the thing you're waiting for could arrive at any moment.
So either you're gonna keep waiting or you're gonna start working on a project only to have to task switch away from it to pick up the thing that you're waiting for, which then leaves that project unfinished.
So to cut down on waiting, you should try to deliver work continuously in small batches, which we will cover in a bit.
Alright, so unfinished projects is next. Speaking, let's talk about a scenario that's probably all too familiar to my fellow game developers.
So you start by working on a feature in a game, and it's a really big feature.
It's taking long enough that other demands on your time are starting to pile up.
And eventually, you have to put that big feature on hold, and it sits off to the side, maybe on a git branch.
waiting to be picked up again. Months go by and pretty soon you realize that if you were going to pick that feature up it's going to be full of bugs and it's going to cause a huge amount of problems so it's better to either abandon that idea or just start over. That's an unfinished project.
When a project gets left unfinished, even for a short time, the context of the rest of the world changes.
And when you pick it back up, there's a high likelihood of creating defects as you try to bring the project up to speed with everything else.
And you might even need to pull heroics to get that project finished.
To avoid this...
break projects down into the smallest deliverable phases so that the project is always in some kind of finished and usable state, even if that state isn't fully ideal.
All right, next up is manual processes.
A manual process is something that is being done by hand, even though it's done the same way every time.
The worst part about manual processes is that they tend to create defects because people get tired and distracted and they make mistakes.
By definition, a manual process also demands task switching because someone has to stop doing something else in order to do the manual process.
And this also may create motion because people may have to communicate about the manual process.
This also makes it difficult to scale up your operation because to do more manual processes, you need more people.
And this takes longer than necessary, which also creates waiting.
If you do have manual processes in your organization, you need to think about ways you can automate those so that your people can be freed up to deliver value to your players in better ways.
Extra processes, on the other hand, are just things that you're doing that you don't need to be doing.
They lead to the exact same kinds of waste as manual processes for the same reasons, but the solution is different.
With extra processes, the solution is to just stop doing them.
In our studio, we use the focusing question, can we not, to cut through the waste of extra processes.
An extra feature is something in your game that doesn't need to be there.
Every feature adds complexity to your game, and complexity is where defects emerge.
When deciding whether to add a feature to your game, whether it's a super sophisticated AI, a new game system, or just an expansion of an existing game system, you should always be vigilant about whether that feature really pushes the vision of the game in a way that players will care about.
Because if not, you're just going to make things way harder for yourself without delivering value to your players.
The best way to deal with extra features is of course to not have them, but if you do have them already, see if you can cut them.
Alright, so this is a mess, but don't worry, it gets worse, because last I want to talk about heroics.
Heroics are when you have to perform far above and beyond what's reasonable in order to deliver a required result, like crunching to get a game ready for launch.
Once you reach the point of pulling heroics, you've fallen into a dangerous feedback loop.
Because when you're in heroic panic mode, cutting corners and moving as fast as you can on very little sleep, you will create more and more of almost every other kind of waste.
Heroics are the worst kind of waste because they are cancerous, they're self-perpetuating, and they consume everything else.
You're going to pull heroics because your processes have broken down, but heroics also make all of your processes break down, leading to more heroics.
So I want to hone in on this because heroics are the signature thing that the games industry is known for.
Every single week, you can find an article about some studio, big or small, crunching their employees in 70 to 100-hour work weeks.
And We indies are no better because we just crunch ourselves and then we pat ourselves on the back because we're crunching by choice, sticking it to the man. I think we should all take a moment though to appreciate what this means. Heroics are a signal that all of your systems have failed.
This means that our industry is best known for being really bad at our jobs and we're stuck.
We're trapped in this heroics loop.
So how do you get out?
The only way to break that loop is to completely stop production and take a step back to rebuild your processes.
And you need to get every person on your team on board for this, because this is everyone's problem.
So that's what we did.
We totally stopped production for several weeks and set out to reimagine how our studio would function.
So our first order of business, according to the first way, was making our work visible so that we could find all the places where we were creating waste.
To do this, we decided to have twice-weekly production meetings where we would all agree on what was going to happen in the next roughly 16 hours of work.
Because the sprints were short, the meetings were also short, which allowed us to be very responsive and iterative.
And we were able to have such short sprints because our team was so small.
Larger teams may end up needing to do a bit more planning and have longer sprints.
But this is what worked for us.
So we all transitioned to using Trello.
And we came up with a structure for managing our Trello boards that would allow the boards to guide the flow of work in a way that we could easily see.
So we gave our boards five lists.
Inbox, to do, doing, which is the current sprint, testing, and done.
And we gave these lists a few important rules for production planning.
The first rule is that new tasks should always go into the inbox first.
They can't be put into any other list.
At each production meeting, we evaluate the new tasks in the inbox and decide their priority.
And this meant that you knew exactly what your next two days of work were going to look like.
And you wouldn't get derailed by some random unplanned thing being dropped into your lap, because all the new stuff just went into the inbox instead of at the top of your current pile.
The second rule is that time is a constraint.
Only 16 hours of work could be handed to any one person during that sprint.
And if we gave someone 20 or 30 hours, it meant that we were demanding that person to crunch, which going forward, we were no longer going to do.
And refusing to crunch is the core way for us to bake quality into the production pipeline and the planning process.
The third rule is that all work items have to go through the whole flow, including the testing step.
The testing step creates an explicit requirement that you verify the thing that you just finished is free of defects before you send it to the next person.
This keeps work from flowing backwards because errors get fixed at the source instead of being found out later.
So as a practical example of how this flow works, we can see how this testing requirement allowed us to identify and fix a major problem in our art asset pipeline.
So under the old process, Sam would create a bunch of assets, the InkBot would export them, and I'd bring them into the game.
And then I would review them for errors.
And when I found an error, I would have to send it back.
And that's a defect.
So we would have defects then coming to me and then flowing backwards and going back upstream.
But let's take a look at what happens if we take the workflow from our Trello board and apply it to the ArtAsset pipeline.
So as normal, Sam creates an asset and then the InkBot exports it.
But now he has this testing step in here.
So instead of just immediately sending the asset to me, now he is the one responsible for reviewing it for errors.
And if he finds an error at this point, it's easy to fix because he just finished making it.
So he's already got all the files open and everything.
And since he is now getting hit by his own errors in real time, he's able to update his processes to cut down on his error rate.
So when he first started tracking his errors using this process, he discovered that his error rate was actually 40%.
But with all the fixes that he implemented, into his process, his error rate is now closer to 2%, which means by the time I get the assets, I can usually just go ahead and implement them without any extra work.
So with this small update, Sam spends a lot less time fixing errors because now he makes them a lot less frequently, and if he does make an error, he's not making it anyone else's problem.
Most importantly, he's not making these errors my problem.
And something we learned very quickly, once we started making our work visible, is that if something is my problem, it's everyone's problem.
Because I am the studio's production bottleneck.
What's a bottleneck? I'm glad you asked.
A bottleneck is a part of a process that's receiving more work than it can handle.
And every production process has at least one bottleneck.
It turns out that understanding where your production bottlenecks are is super important.
So let's take a look at our art pipeline example. In our case, we had Sam over here, able to produce three art assets per hour.
As the game programmer, I was only able to implement one asset per hour on average, because I'm programming.
I have a bunch of other stuff that I'm working on too. I'm working on game systems, fixing bugs, developing new features, all that stuff.
So in this case, I'm the bottleneck for art.
And it doesn't really matter that Sam is producing three assets per hour, because our process makes it so that the game will still only get one asset implemented per hour.
So the total output of our team is not determined by Sam, it's determined by me, the bottleneck.
So when Sam started doing his own testing, it cut down on the rate that he was able to produce art because now he had to spend extra time testing his art.
But he took that testing task from me, which actually saved me a bunch of time, which I could use to implement the art.
So with that one change, we were able to get twice as much art into the game in the same amount of time.
So just to reiterate this, by using the same people with no extra work hours, and simply moving a single task from one person to another, we doubled the amount and pace of art that we could get into the game.
Which brings me to the golden rule of workflow improvements.
The only way to increase your team's output is to increase the capacity of the bottleneck to move work through the system.
If you're making changes anywhere else other than the bottleneck, you're either making things worse or you're just not making them better.
So once we got our workflow under control, we were ready to tackle the next bottleneck in our production process, deployments.
By deployments, I just mean making builds of the game and getting those builds delivered to players.
Deployments had been a problem since day one, but because of how painful our deployment process was, we had been avoiding them.
But one of the core lessons of DevOps is, if a process is painful, you should do it more, not less.
If you don't confront the process, you can't learn about it, so you can't fix the thing that's making it so difficult.
So for us then, the ultimate goal was to have continuous deployments of our games.
In other words, it would be ideal if every time we made one single tiny change to our game, we could deliver that change to QA in a fully compiled build for testing.
Having continuous deployments brings a huge number of benefits.
It allows you to detect problems immediately after they're created instead of weeks later.
And if you do release a patch with very few changes, the likelihood of that patch containing some kind of catastrophic problem is quite low.
And if there was some huge problem, it's very easy to track it down because hardly anything has changed so you can easily pinpoint where to look.
And last, the more deployments you do, the faster you find problems in your deployment process.
So you can fix your deployment process as well.
So this is the ideal, but for us, our deployments were anything but continuous.
Just before we learned about DevOps, we had hired a few part-time QA testers and we were making builds for them once per week.
The process at the time looked like this.
At the end of the day, every Thursday, I would write up and compile all the week's changes into patch notes in a Google Doc, which would take about a half hour.
Then I would...
make a build, and verify that the build was working.
So that process could take me 30 minutes to an hour.
And if the build was good, I could then send it to the testers by putting it into Dropbox.
And if the build wasn't good, then I would have to make another build and start the whole process over.
The testers would then go ahead and make a checklist out of the patch notes, and they would begin testing and going through that checklist.
If they found problems, they would report those problems to me, which I would then review.
And if the issue was something that needed fixing, I would move that issue into Trello and then I would fix it for the next round.
But this whole process was a one week loop.
And this was only for Windows builds.
If we wanted to make builds for other platforms, then each additional platform would take me another 30 to 60 minutes to deploy to.
So at this point, the rest of our intended target platforms were completely untested.
So we set out to update this process with a new automatic deployment pipeline for our games, which we would call the GamePipe.
We had a lot of really big visions for what the GamePipe could do.
But if we did them all, it would take months before we had the game pipe up and running.
So instead, we asked ourselves, what would be the simplest version of the game pipe that we could start using immediately?
So this is back to that small batch delivery of work.
So Adam, our tools developer, took on this problem, and he created the first working version of the game pipe in about a week.
In this first draft of the game pipe.
Whenever I pushed a git commit to the game's master branch, the GamePipe, using a Bitbucket webhook, would create a Windows build of the game and then deploy that build into Dropbox where our testers could then access it.
So this is very simple.
But even though this version of the GamePipe was about as minimalist as it could get, we found that it solved a bunch of our problems right away.
So first, my time was no longer being used to create builds.
So I could just keep developing while the build machine was just churning away compiling builds.
I also no longer had to manually track down the builds and put them into Dropbox and deliver them to the testers.
The GamePipe was now doing this automatically.
So this saved me a bunch of time.
It saved me task switching, and it cut down on the chance that I would mess the process up and create defects.
And last, environment configuration was no longer a problem.
So all this means is that no matter what, we can ensure that all the builds always come out the same because they're always using the same versions of the same SDKs, because it's always being built on the same machine, no matter what machine we triggered the build from.
And just like that, we were able to go from making builds once per week to making them once per day.
So we rescheduled our QA team to spread them out throughout the week instead of coming in just on Fridays.
And so whenever we made a change to the game, it would be tested within about 24 hours.
But our goal was continuous deployments and once per day isn't continuous.
We weren't yet able to make the leap to continuous deployments because there was still one manual step left in the deployment process, writing patch notes.
Fortunately, there was a solution right under our noses, because every time I made a change to the game, I would write a git commit message explaining the change.
But if I'm writing git commits for every change and a patch note for every change, then I'm just doing the same thing twice.
So to cut down on this redundant manual work, Adam built a versioning system into the game pipe that would automatically convert my Git commits into patch notes, and then it would post those patch notes to our website.
So for example, if I wrote this Git commit, campaign, three new levels added to the end of the campaign, blah, blah, blah, then it would get automatically converted into this on the patch notes.
So with this patch note delivery system up and running, we found a few huge benefits.
First, and most importantly, this fully eliminated the manual creation of patch notes, which finally did allow us to make continuous deployments.
We could make deployments at any time.
Second, since the patch notes were generated out of my git commits, it was no longer possible for me to accidentally leave something out of the patch notes.
So that cut down on defects and made our testing more reliable.
And the third benefit is that because my Git commits were now publicly readable by other people outside of the programming team, it forced me to be much more clear and deliberate about my Git commits.
So in the long run, it became far easier to navigate our changelogs.
So now if we look at our deployment pipeline, you can see that we've fully automated all of the manual parts of getting builds made and delivered to QA.
So all the automated parts are the gold boxes there.
And over time, our platform specialist, Shure, took over the GamePipe project, and he worked with Adam to continually improve the GamePipe piece by piece.
So instead of deploying to Dropbox, we got it to upload directly to an internal staging branch on Steam.
So that meant that our testers would always have the latest version of the game automatically downloaded.
Later, we upgraded it to deploy builds directly to all of the rest of our platforms, like Google Play and the App Store.
Shur and Adam also created an automatic localization pipeline and connected it to the GamePipe, making it trivially easy for us to add new text and translations with every new version.
So by this point, things were looking pretty good.
The GamePipe had turned a fully manual process that would take up to an hour each time into an automated process that would only take about five seconds to trigger.
We went from weekly testing to continuous testing, which gave us the fast feedback that we needed to rapidly iterate on the game, and it allowed us to more easily track down bugs.
So this caused the builds of the game to become more stable, and most importantly, patching the game became incredibly easy instead of something that we had to agonize over for weeks.
The most important thing I hope that you all take away from this GamePipe story is that, for starters, you should have an automatic deployment pipeline, but also, it doesn't have to be perfect.
Make something simple that you can use and start using it right away.
Because once you're using it, you can then iterate on it to fit with your processes, and eventually you will get it doing the things that you need it to do.
So with our new continuous deployments, we wanted to make sure that we had quick testing on every patch.
This meant opening up jobs for new part-time QA testers throughout the week, which meant we had to hire new people.
So we had to screen them, interview them, and if we brought them in, train them.
So this started taking up quite a bit of time, which was putting a strain on our production.
We needed an operations manager, someone to take charge of the business side of things.
Fortunately, we knew someone who had been doing operations management work for quite some time and was looking for a new job, which happened to be my wife, Tampara. She was our first hire since the studio had collapsed the year before, so we were determined to ensure that her onboarding and training in the studio went as smoothly as possible and that she was able to contribute immediately. Now, we already had a process in place.
that was making work run really smooth and easily, which was our production meetings.
So we took that same exact framework and we created a training Trello board.
We populated that board with a list of training tasks with time estimates.
And some of these were solo, like reading certain books, while others were collaborative, like one-on-one interviews with studio members.
And the goal of each of these tasks was to generate knowledge and insight about how the studio works and what her role was inside the studio.
Twice per week, we had production meetings to review what she had done in her training and talk through the next training sprint.
And every sprint, we had her tasks broken down by hours so that we never overloaded her.
And she always knew exactly what she was going to be working on and why.
Because of this approach, she was immediately able to add value by taking over several business aspects of the studio.
And she quickly built up the context of how the studio fit into the games industry at large, despite not coming from a gaming background.
We refined this process further based on Sampada's feedback, and we later hired a new full-time QA tester, Jordan.
And just like with Sampada, his onboarding was simple and stress-free for all parties.
And all of this is thanks to the process of making the work visible.
So we can easily spot problems, give guidance, set clear expectations, and receive feedback from our new hires.
With Sampada and Jordan on the team, we increased our capacity in places where it mattered.
We hired for roles that were specifically meant to relieve pressure on our production bottlenecks, and it worked.
So before we move on, let's recap on the first way of DevOps and how we applied it to our situation.
Remember, the first way is all about understanding and then managing the flow of work through the company, ensuring that the work flows in one direction, from development to operations to the customer.
The core tools of the first way are to eliminate waste and make the work visible.
And we did this by building new processes to guide, test, and review what work was being done through our production meetings and Trello rules.
To deliver work in small batches, we created the game pipe.
And we were able to continuously deploy patches and changes to our QA team.
To prevent defects from moving downstream, we updated our workflows to move testing closer to the source so we could fix our problems before handing them off, like what we did with our art pipeline.
And last, to align everything toward the global goals.
Our goal is to deliver high quality games to our players.
And we do that by adding staff and developing tools to ensure that the game development team is to work unimpeded and that our bottlenecks are clear.
Second way is about amplifying feedback loops. So the second way says that while the work is flowing in one direction, we want feedback to flow in the other direction. Feedback is what allows us to catch problems, improve our processes, and build quality into the production pipeline.
And to get that feedback, we need to develop tools and systems to allow the people downstream from us to report problems.
So by this point, we did have our QA process giving us continuous feedback on our deployments.
So the question then was, how do we amplify that loop to get better and faster feedback on the things that we're doing?
To answer this question, we took a look at our QA testing pipeline.
And we discovered that if we ramped up the speed and volume of reports coming from our QA team, we would be unable to handle it because we still had this process.
every report had to be manually turned into a Trello card before it could be dealt with.
Although this only took a few minutes per card, it was a manual process, which meant that it created tons of task switching, motion, and defects.
And because of all that waste, a few minutes per card can easily turn into 5 or even 10 minutes per card as the volume of cards increases because fatigue sets in and I would become increasingly distractible trying to process 60 cards in one go.
So we used the Trello API to make it so that anyone reviewing an issue can just click a button to auto convert the issue into a Trello card. And all of the file attachments, tags, comments, and relevant information would just be baked into the card and we could then review that issue at the next production meeting. So that was it. Just like with the GamePipe, a cumbersome process that would take many minutes or more per iteration became an automatic process that would only take seconds. So now that we were able to review and process feedback much faster and easier, we could start looking for a way to amplify how our QA testers delivered their feedback. Under the current system, our testers were manually making checklists of what to test by looking at the patch notes, and it took a lot of time out of testing for them.
and it also caused them to sometimes miss things because this was a manual process and there was nothing keeping track of all these things automatically.
So the checklist creation step was clearly the bottleneck for everything else in the QA workflow.
If we could alleviate that bottleneck, then our testers would be able to spend more time testing and they would be able to give us more and better feedback.
Since we already had automated patch notes, we upgraded that system to turn the patch notes themselves directly into a QA testing checklist.
Under the new system, each tester would see little check marks next to every change in the patch notes.
And when they had tested something, they could simply check that change off.
So now the testers didn't have to spend any time compiling a checklist at the start of their testing session because they could just jump right in.
And on the back end of this system, when I log in as a developer, instead of seeing check marks, I would see a number next to each change, which would tell me how many testers have tested each change.
So this lets us know whether we're missing test coverage on a particular issue, which we could then deal with that more directly.
So that brings us to the current version of our patch deployment and QA testing pipeline, which now looks like this.
As you can see, there are more automatic processes than manual ones, and we only have people doing the things that only people can do. With all those changes, we were suddenly getting better and more feedback without being overwhelmed. We knew what was being tested, and our testers were able to each add a lot more value to the process. So that wraps up the second way, amplifying feedback loops.
The goal of collecting feedback is to allow you to catch problems, improve processes, and build quality into the pipeline.
Developing tools and improving the means by which you receive feedback will allow you to get better feedback faster, which lets you make better products.
But before you amplify the feedback loops, make sure that you have systems in place for how to handle that feedback.
So with all those changes we made to our...
deployment pipeline and feedback systems, we could tell that everything was moving along much faster, things were easier and things were cleaner.
But DevOps is all about continuous improvement, not just one big improvement.
And that brings us to the third way, continuous learning.
Your studio is always changing and the world is always changing around it.
If you wanna truly reach your potential and get the most out of your work, you need to be able to adapt, improve, and grow.
And you need to build that philosophy into everything that you do.
To do that, you need to ensure that everyone on your team has the right to experiment and try to figure out better and faster ways to improve the company processes.
People need to be able to offer up sometimes risky ideas and the management needs to be able to hear those out without just striking them down out of hand.
Which means, You need to create a work culture that reinforces psychological safety, where people know that they're going to get a fair shake and where they will always speak up about problems.
But culture doesn't emerge in a vacuum.
Culture is the collection of practices and beliefs that people develop in response to their circumstances.
And what DevOps has taught us is that your work culture is a direct result of the structures and processes that you use in the workplace.
So we had to ask ourselves, what structures do we have in place?
And how do those structures affect our culture of learning and growth?
Although we were doing twice-weekly production meetings, we realized that those would only get us so far.
They're too narrowly focused on specific projects, and they're all about just doing the work.
So we needed a higher level way to evaluate and make changes to the studio's processes as a whole.
For us, this meant creating a formalized quarterly review process.
Every quarter, we have our team members fill out a questionnaire to identify how they're feeling about their work and ways in which they believe that their time or their talents might be getting wasted.
We use those responses plus a studio waste analysis to make changes to the work processes of the studio as a whole.
So every time we've done this, we have found huge gains and solved big problems with some aspect of how we handle our work.
Whether this review process is quarterly, monthly, or somewhere in between, the most important part is that there are structured ways to make improvements to all the processes in the company.
Without that structure, the evaluation doesn't happen, and then the improvements don't happen.
So that brings us to the end of the three ways.
So before I tell you what happened after we did all this, let's quickly recap.
The first way is all about mastering the flow of work.
Make sure the work flows forwards, watch out for bottlenecks, cut down on defects, and eliminate waste.
The second way is all about amplifying feedback loops.
Make sure you're getting the information you need from the people downstream from you so you can make their and your lives better.
The third way is about creating a culture of continuous learning and experimentation so that you can constantly improve and deliver value to your players.
Now, it's a good thing that we started down this path when we did.
because our world was about to get flipped upside down, or at least it would have if we hadn't been prepared.
By February of 2019, we had been making our DevOps transformation for six weeks.
We were planning on launching Levelhead around June, so we had four months to go.
Then on February 13, we got hit by a curve ball.
Nintendo announced that Mario Maker 2 was not only in development, but it was essentially done, and Mario Maker 2 would be launching in.
June. This was really bad for us. We had started making Levelhead because we sought to fill the void left by the fact that there was no Mario Maker on the Switch. Pretty soon, that void wouldn't exist, and there was no way that we could compete with Nintendo in terms of marketing power.
So, we didn't have any options other than maybe beating Mario Maker to launch.
But how could we do that?
We looked out across the media landscape and we discovered that if we wanted to beat Mario Maker to the market, then pretty much the only available launch date for us would be April 18, 2019.
Two months earlier than planned.
So we thought, let's go for it.
And better yet, let's try to dodge Mario Maker entirely by launching on Steam instead.
So on February 13, we completely changed course and settled on a new timeline.
On March 1st, we would do a small closed pre-alpha test and we had two weeks to get this put together.
On March 8th, a week later, that would be the start of the main alpha test where we would bring in a larger group of players so we could refine the game further. And six weeks after that, on April 18th, we would launch into early access.
So suddenly...
Instead of having the game in the hands of players in three months, we suddenly had to have it in their hands in two weeks.
This is the exact kind of situation that would have been nearly impossible before and would have resulted in an incredible amount of crunch, panic, crashes, bug fixes, and everything else.
But because of our updated DevOps workflow, we already had stable builds on Steam that had been thoroughly tested.
so we didn't crunch. We just shifted some priorities around a bit to wrap up some final features that we wanted to get in before early access and we just carried on as normal, no problem.
And within two weeks we had players playing the game in our pre-launch testing period and it was fine.
Although our players did find some edge case bugs, it was easy for us to quickly patch them up.
And throughout the five-week pre-launch testing session across thousands of player hours of testing, we only had one single crash, which was easily fixed, tested, and deployed that same day.
On April 18th, we launched into early access.
We got the steam pop-up that weekend and got a decent influx of players, and...
everything was fine. We had no game breaking issues. We didn't have to put out an emergency patch. We just hit the launch button and then we just got to hang out with our players in Discord and chat with them about the game. It was actually pretty relaxing. So that was 11 months ago.
To this day, we are still doing our continuous deployments and delivering routine content to our players.
Levelhead is sitting at 97% positive on Steam Early Access, and the game is getting better all the time since we're deploying two to three content patches to our players every month.
And every time we send out a patch, it's calm.
It's no big deal.
And we're not crunching at all, even though we're gearing up for a six-platform simultaneous launch supporting 11 languages with a team of six people.
And all of this is thanks to our whole team's willingness to break the loop of heroics, take a step back and ask, does it really have to be like this?
And that's the story of how we rebooted our studio.
I hope what I've shown you all today is that the stresses that we all take for granted as a normal part of game development are not normal.
They are the result of bad development practices.
History is caught in a loop.
but we don't have to be.
You might look at all this stuff that I brought up today and think, this is a lot.
You might think that your team can't afford to do this.
No, you can't afford not to do this, because you're spending far more time and energy and money than you need to, and you're getting worse results.
That's not to say that making this kind of change is easy.
But the goal here isn't perfection, it's improvement.
The most important thing is that you start down this road because as soon as you take even one single step toward applying DevOps principles to your work, you'll see immediate benefits.
And when you do, we'll be there on the other side, ready to meet you, where we can all just kick back, relax, and make great games for our players without setting ourselves on fire.
Thank you all for your time.
I hope this was useful for you.
To give you some resources to pursue as you go down the path of DevOps, I've put together a little cheat sheet, which has all the core points, plus some books for you to check out.
I definitely recommend starting by reading the Phoenix Project.
And if you have any questions, you can feel free to just shoot me an email.
Enjoy, and good luck out there.
