Why are we here?
Caves of Qud is a game that I developed.
I'm Brian Buckley.
I'm a co-founder at Freehold Games.
Caves of Qud has a ton of procedural generation.
We use a bunch of normal techniques that you've heard of, Perlin noise, et cetera.
And we use a few novel techniques.
And today we're gonna talk about a specific one, wave function collapse, which sounds scary, but I hope by the end you'll realize it's not particularly terrifying.
So, we're going to first start by looking at some final results that we get by utilizing this technique and then we'll go through some slides to talk about how we get there.
So here's a map in Caves of Qud.
If you're not familiar with it, it's a tile-based game.
So each of the maps is 80 by 25 and it's very discreet.
So you can see walls and creatures and plants here and they're represented in a sort of abstract way.
So here we have a sort of rectilinear palace.
It has sort of an outer wall and inner buildings and they're populated by various things, blood splatters and centipedes and gardens.
And here we have an underground map where we have some very narrow corridors, gullies full of water, little rooms full of water.
This is a very sort of mazy, narrow map.
And here we have a circular tower embedded in some rock salt.
It's filled with bears, I guess, and asphalt.
And a little statuary on the right.
It's got sort of curvy labyrinthine halls.
And here we have a little town.
We've got maybe a big ruined civic structure up north, and in the south we have a bunch of rectilinear village buildings adjacent, laid out along a nice forested path.
So these are a wide variety of results, but they're a result of a single algorithm.
So these each had a very different character.
You had little mazy corridors, you had a...
fairly dispersed village with well-ordered buildings, you had water-filled gullies, and instead of using an individual technique to generate each of these, we use a powerful new texture synthesis technique called Wave Function Collapse, alongside a few other tools that we use to supplement its weaknesses when applied to maps.
So Wave Function Collapse was developed by a Russian mathematician named Maxim Gumen, released open source, MIT licensed, and the repository's out here.
It's in 20 different languages.
You can go out there and download it now and use it.
Hopefully after this talk, you'll be able to go out there and build into your projects.
Caves of Qud was its first commercial use.
Bunch of others quickly followed because it's such a powerful technique.
Wave Function Collapse has two primary modes.
It operates on tiles and it operates on textures.
If you do procedural generation, you've probably encountered long tile maps which are based on edge-colored adjacencies.
The tile map generation is sort of similar to that, the adjacency constraints are much more powerful than edge coloring.
You can get a much more powerful result.
So if you're interested in tile-based map design, this is great, we're not gonna talk about it.
Instead, we use texture mode.
Texture mode uses small training inputs.
They can be big, but it's most powerful when they're small.
Maybe eight by eight or 16 by 16 training images, and they can produce arbitrarily large outputs with the character of that input training image.
This is pretty amazing.
So on the left, you have three separate inputs.
On the top, you see a rectilinear room hole.
And you can see seven sample outputs here, just bigger.
You can see that each of them has a pretty different layout that shares structurally the same character as the input.
On the second row, you see some caves.
These are caves that you would typically generate via cellular automata system or something, or box filters, something very different from that first rectilinear room hollow arrangement.
And then third, you see an input texture that's five by five, simple couple dots and a line, and you get some very complex quilted patterns, and this would be some custom line drawing technique.
So instead of having to have three very different algorithms, you're able to have one single algorithm that's fed different, very simple training inputs.
So this is something you can pop open MS Paint right in a training input and throw it through the algorithm.
Very cool.
So I'm gonna run quickly through this text slide.
Don't feel like you have to load this into your brain.
We're gonna walk through it more carefully.
I just wanna prime your system for what we're gonna talk about.
I'm actually not gonna go way down to the nuts and bolts of this algorithm.
There are good papers to do that.
I wanna give you a intuitive understanding of how this algorithm is making decisions when it's painting its output.
So first, what it does is it chops it up into end-by-end tiles.
n is something you pick.
Three is a very typical size.
So it's gonna operate on individual three by three tiles.
Chop up your input image into those, and it's gonna operate on those as its basic atom.
You can choose bigger or smaller atoms depending on the character you want out of your output.
Then we output.
we set up the output as a superposition of all those possible tiles.
So you can imagine each element of the output being superimposed of each of the tiles that are pulled out of the input.
And this is the character it shares with quantum wave function collapse.
It's sort of a snide relationship.
It's not anything complicated.
What it does then is it picks one at random to start with and says, well, of all these possible tiles, we're going to select the lowest in-by-in entropy area and pick one at random and collapse it down to a single tile.
And then it's got some new information.
I know that this tile exists.
These adjacencies are removed from the list.
And then it continues until every output has only a single selection.
And that's pretty much it.
So, if you didn't catch that, we'll walk through that in a little more detail here.
So the first thing it does is it identifies all the in-by-in patterns.
So the example here I'm gonna use is a very simple three-color training input.
It's got red on the outside, and a green box, and a blue inside.
And so it's gonna chop it up into all these three-by-three patterns.
So I'm showing you five here, but there are obviously a bunch.
And a nine-by-nine box without wrapping, it's gonna be like 49.
And if you allow wrapping, it's like, what, 81?
If you allow symmetries and rotations, it's much more.
Then it goes through, and for each tile, it figures out, where do the other tiles overlap?
And it's gonna use this information in order to do the propagation.
And so here I'm just showing an example.
You've got two tiles and you can see that the first tile on the left, it overlaps with the two tiles below it, sharing those six red pixels.
So this is an example output of a WFC run.
So let's unpack what we're looking at here a little bit.
On the very left you see the training input.
What you're seeing is the first four steps of the WFC collapse.
In the very first frame, the white outline box is the three by three tile it's decided to collapse.
It then steps through one three by three section at a time and selects an output tile that correctly overlaps the existing tiles that it knows about and it steps through the image until all of them are collapsed from the existing input data.
So to give you a real idea what's going on, if you look at the area in the box, it's very clearly green-blue.
That's a single non-superimposed result.
The rest of the blending pixels is a visualization of the output tiles that remain available for that area, superimposed on one another.
So pixels that have sort of a green-blue box, they can still be either green or blue, depending on what tiles remain to be chosen.
So it's like.
very specifically at this first step, try to build an intuition for how it makes these decisions.
Let's look at this strip of three pixels here.
Unlike the pixels to the left, you can see these blue pixels are a little green, which means they still have some possibility to become green pixels.
So why is this?
Well, we look to the left of this pattern, and we see to the left of this pattern in the input is a strip of blue, blue, green.
And where can we find this in the input pattern?
We can essentially find it in these three places.
We can find it here, each of these magenta boxes is what the output column we're thinking about might be.
In this case, in the most leftmost case, you can see left of that is blue, blue, green.
In the second case, left of that is blue, blue, green.
In third case, left of that is blue, blue, green.
So these are all, these could all slot in there.
two of these outputs, that column is blue, blue, green, and the third possible output, the column is green, green, green.
And so we can see, here we go.
If we look at two blue, blue, green inputs and one green, green, green input, you can see that the probability is two blues and one green for each of those top two pixels, and it's always gonna be green in the output.
So you can see that though we've selected that, initial three by three box, we know quite a bit about what possible adjacencies exist based on the input image.
And so, again, looking at this, you should have a better idea, sort of intuitively, what it's doing.
Each of these white boxes, it's selecting for sure what these pixels are, and in each case, it knows a little bit more about the possibilities that remain, and it simply sort of flood fills around the area to make it work.
So let's watch this work, it's fun.
So we have five training inputs on the left.
We have a little white box, a little red box, sort of more complex temple.
You can watch in real time as it decides, usually from the edges, what the possibilities are, and then folds in those overlapping potentials until we've got a final image.
And you can obviously run this many times and get a different output each time.
Pretty fun, so you can get big complex outputs from little inputs.
So that gives you an idea of how the technique works.
Again, you don't have to understand it.
When I started experimenting with it, I had no clue what was going on.
So I'm sort of a hands-on learner, so I just tried to give it input and see what happened.
And so this was my first experiment in texture mode.
In respect to building tile maps, I wanted to build structured output.
So in this case, I was like, well, can I make it build little square buildings for a village?
So maybe.
I gave it a black box on a white background, and I did not get a bunch of little square buildings.
And it turns out that's because, what is the image telling it?
It's telling it basically, on the order of three by three pixels, that a line can turn left, or a line can turn right, or a line can go straight.
you'll notice that there's no overlaps.
But there's also no training in that image that tells that there's an inside and an outside of the square.
There's no spatial relationship between those turns.
And so you get the output there.
I said, well, what if I add a third color?
So now I've got white and I've got black and I've got red inside.
This suddenly produces the kind of spatial coherence that I want across the map.
You've got little red filled boxes.
That's awesome.
Because not only do you get the spatial coherence, but you also get some structural hinting.
The red pixels tell me, hey, that's the inside of a place.
And when you're building a map, that's a pretty interesting distinction to be able to pull out of a real simple texture.
Okay, well, and then suddenly I can put furniture inside and plants outside and that looks.
like already a coherent village with almost nothing going on.
We've got a three-color texture going into it.
Here's another example.
As I get more comfortable playing with it, hey, what if I feed a bigger training image?
Do I get noise or do I get something interesting?
I get something interesting, that's cool.
Another example, I said, well, can I produce sort of naturalistic gardens?
Sure, I mean, these are like eight-second experiments in MS Paint.
This is, I'm not spending 20 hours learning to do high-level math.
So maybe I'm in the wrong lecture hall.
Here's an integer shape, and keep this in mind, because we're gonna return to the star shape.
I added an additional color here, and you can see you get star-like shapes, not identical stars, but some variation, and that's pretty interesting.
how to use this in your code, probably the most interesting slide for people.
It's not complicated, it's three lines of code.
You initialize the model, you tell it to run, and you get the output image, and that's it.
And let's take a look at the inputs, because sometimes the complexity's hidden in the parameterization.
For WFC it's not, there's not even any complexity there.
You give it the training image, you say what is the N I want?
I want it to be two or three or four.
How big do I want the output to be?
Do I want the input to wrap across edges?
Do I want the output to wrap across edges?
Do I want it to sample with symmetry?
Symmetry is a little bit interesting, not very complicated.
If you're, for instance, putting in something where orientation matters, and you've got ground at the bottom and sky at the top, you probably don't want symmetry unless you want to end up with sky at the bottom.
If you've got buildings where you don't care about the orientation, you want to probably crank that symmetry up to eight where you've got full reflection and rotational symmetry.
Applied to map design, these are not problems with the algorithm per se, but only domain-specific problems.
What problems did we run into when we tried to use this?
Problem one was homogeny.
Outputs are, as you get bigger and bigger, just completely homogenous in all directions.
They're interesting at the small scale, but you don't really want to walk across an infinite homogenous plane of these buildings.
Problem two is overfitting, and so we'll return to this star shape.
One of the things you want to do in Maps is have more interesting details than walls.
You want to have doors, and where the monsters are, and where the furniture is, et cetera, et cetera.
And you might think that, well, let me add more colors to the WFC input.
And that, of course, was the first thing I tried.
The reality is as soon as you start adding more and more colors, less of these tiles tend to overlap.
And so you tend to get very non-varied outputs because the tiles can fit together only in one particular way.
And so instead you just reflect and rotate the shape across space.
So how did we solve these?
The solution to homogeny was very simple.
We segmented the output space and then used WFC to fill in details.
So in this case, this very simple example, we've segmented this map into three high-level chunks and each of them is filled with a WFC input with different character.
So this produces a high-level, graph that's more interesting than a homogenous output.
And then the interiors, that interior detailing, which can be quite difficult to get with standard procedural techniques, can be highly varied because you can pick from a big library of 500 templates you made in MSPaint and get some really interesting complex output.
How do we deal with overfitting?
The solution to overfitting was pretty simple, which was to use the output of WSC in order to generate high-level wall structure, and then use a post-processing pass to generate the more detailed things like doors and furniture.
You can imagine a bunch of ways to do this.
I'm gonna show you one of the full run-throughs that we did to get a level in Caves of Qud.
So here's a real simple example.
First, we pick some segmentation.
In this case, we're gonna pick a big circle in the middle of the map.
We're then gonna run a WFC template to fill the whole map up.
In this case, we're just gonna run a set of buildings.
We're gonna cookie cutter that circle out, and we are left with some walls.
You can see that this has created some disjoint rooms, and so what we do is we simply segment this map.
We figure out which are the disconnected spaces, and then we A star from segment to segment.
Anywhere we pop through a wall, we're gonna place a door.
So you can see in this case, we're.
A-starring here, you've got a little red cross where the door is punched, and we do it again, and again, and again, and again.
And once that's done, we are able to build a whole map, which looks pretty complicated.
You can see now that there's very little going on, right?
We've done a top-level segmentation, we've filled in the details with WFC, and we've punched some doors with a pretty simple A star.
And the output is pretty complex.
So let's revisit the initial maps, which looked complex before, but let's think about how we did these.
These are each accomplished through the exact same technique.
In this case, we've got a couple square areas that were filled with rectilinear WFC output.
That's it, you get this cool-looking palace.
Here we have a full map that's been allowed to run.
We had a tiny little corridor WFC.
It's maybe eight by eight.
We let it just run out across the whole map.
Then we flood fill in some of the edges with dirt.
So this looks like a buried set of gullies.
Here we've just got a circle and a square output that have been filled with real simple labyrinthian WFC.
And here we have a big square at the top that's been filled with WFC output.
and a big square at the bottom, which has been filled with WFC output.
And that's it, and you get this cool little village, right?
Pretty simple.
There are some really good papers in the wild.
Isaac Carth has written a good one called WFC's Constraint Solving in the Wild.
If you wanna go really deep down into how the algorithm works, you can go grab that.
The repository is a really excellent resource.
Unlike a lot of repositories, it's really excellently documented.
It's got source code in 30 different languages because as you can see, it's like a super easy technique and a super powerful one.
And so it's very easy to plug into your project and go.
And that is it, I'm Brian Buckley, that was WFC.
Do I have?
Yeah, if you have questions, come to the mics, happy to answer it.
So, how did you deal with communicating the algorithm to people who are more used to traditional level design tools or more traditional procedural generation approaches, which tend to work on room chunks or things like that, where there's discrete combat spaces or discrete spaces for different gameplay styles?
The question is, how would I communicate that to them, or how would you...?
Yeah, how did working with design people from that background work out for you?
Yeah, well, our team is, so the question is, how did it work out working with people who are more interested in discrete level design?
And the answer is we have none of those people on the team.
Okay.
So how did it go with you?
Well, I mean, this was a big challenge for Counter Spy because we ended up doing room-by-room chunking because that was easy to build out spaces and have combat AI work in it.
But I'm curious how you do that.
Yeah, if I can take that question to an adjacent space, I think there's a tendency to want to have a little bit more controllability from a designer's perspective.
These, in Caves of Qud, we let our generative systems run really wild, and it's part of the aesthetic of the game, and we can lean into it.
And you can see we do things like we just smash out put on top of each other, but because our game is about ruins that have been smashed out on top of each other, that's great.
I think that, I think that.
the, when you get into the design space where you want more control, the tendency is to move more towards prefabs which designers have control over.
So you want to lean into like the tile-based techniques in order to fit those prefabs together, and then allow the designators to design spaces that can work within a particular bound, right?
So within this prefab, this area can be filled in this number way.
So if I'm following you, you're suggesting maybe when you're doing the spatial partitioning step, you can do some of that WFC step to do the broad strokes and then chunk down prefabs into the appropriate size spaces?
Yeah, I would actually say probably the tile-based portion of WFC, which I didn't talk about, is probably more interesting to those teams.
I think there's a little bit more controllability to that output.
Thank you.
Did you try building any constraints into the wave function collapse algorithm itself?
Like no repeating tiles in this region or anything like that?
Because I've experimented with that with some other implementations of this and ran into quite a few problems.
Yeah, the question was, did we build any additional constraints on top of WFC?
The answer is no, because I tried it and I ran into a bunch of problems.
Max is a cool guy and very knowledgeable, obviously, since he created it, and if you have a specific question about constraint building, I would actually hit him up on Twitter and ask, because he's helpful, but I didn't have time to push past those problems in this case, and we were able to get really good output without any kind of complex constraint additions.
I would say there's a great C Sharp library on GitHub by a guy, Boris the Brave, or whatever.
I think it's linked within that repository.
Works great inside Unity.
Yeah, awesome.
Great talk. I had a question. Did you ever try having like a more detailed training set to be able to have more permutations of different sort of adjacent materials?
Yeah, it's really tricky because the reality is that those bigger training sets like the temple, really you can reduce those quite a bit and still get similar output. Those bigger sets.
unless they're very complicated, tend to just start repeating the same patterns.
And so a good, because as your training set gets bigger, the memory impact and the CPU impact of the collapse gets bigger.
it tends to be best just to try to figure out exactly what the character is and pack it into as small of a space as possible for actual production.
But, um, so I meant sort of when, um, when you have, like, so when you had multiple covers, colors, you got the repeating patterns, right? Yeah.
So, uh, by having maybe a bigger training set, maybe you could have more permutations of colors adjacent to each other to sort of still be able to get the detail, but...
Oh yeah, could you work around overfitting by having a bunch of them with different colors?
Yeah, I think you probably could.
I haven't really experimented with it, but I mean, that would certainly be an effective use of it, I think.
Something to experiment with.
Any other questions?
