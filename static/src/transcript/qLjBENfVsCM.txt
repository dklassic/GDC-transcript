Thank you for coming out to this panel.
This is something that a lot of us are really excited about getting to discuss, especially here at GDC.
So a few months back, when I had pitched the idea of doing a panel on ethics and AI here at GDC, I actually got a lot of feedback from game devs saying, ethics and AI?
But we're at a games conference.
If this was a social media conference or a conference that was maybe focused on machine learning, then sure, but why would we talk about ethics at a games conference?
And hearing this feedback from game devs was what made me realize just how important having a panel like this should be, especially as we're starting to see the spectrum of game AI grow into so many different areas over the past few years. When social media platforms were being built ethical and privacy concerns were not their focus.
And look at where we are today.
When machine learning was brought into the education system, ethical considerations were not made.
And it caused major issues because of biases that had been programmed into their systems.
With big data comes big responsibility.
When our panelists actually got together the other day, an interesting statement was made.
And it was that discussing ethics had always seemed very taboo.
And we wondered why.
Is it because companies and individuals are scared to admit that they aren't necessarily the experts in a given field?
That they actually need help?
That maybe a system, game, or tool that was built by a company was coded with biases that could be offensive or even dangerous to some people.
But where one would rather ship a product than actually have their product be scrutinized and redesigned for improvements.
Through AI we are able to do amazing things.
But without proper considerations around biases and ethical implications, things can easily go south.
So today, I'm joined by some amazing developers from different areas of expertise who work with and use AI across a number of very different areas, and who have all been advocates in their respective fields around ethics and games and AI.
We're going to cover quite a wide range of topics today, from biases in game AI to data and privacy concerns, and areas of AI that transcend from traditional video games like XR and digital assistants.
And then we'd like to spend some time where we open it up to the audience, because we really want to see what do you all think are areas that we should all be addressing when it comes to ethics in AI.
So, hello, my name is Alicia Leidecker.
I'm one of the advisors here at the AI Summit, and during my day job, I work on XR as Director of Developer Experience at Magic Leap.
Prior to that, I was lead AI on many of the Assassin's Creed titles.
So we're going to start off by having our panelists, if you'd like to start, Emily, talk a little bit about yourselves and some of the work that you do.
Sure.
I'm Emily Short.
I'm Chief Product Officer at Spirit AI, and what we do at Spirit is middleware for games.
So that includes...
a product called Character Engine, which does dialogue for NPCs and how they can respond to natural language input or other kinds of input from players.
And the second product, which probably has the greater application in this area, is Ally, which is a community moderation tool that looks at toxic behavior within communities.
and helps give community managers an opportunity to see in a triage dashboard who is causing the most trouble in this space and what are the biggest concerns that we should be looking at moderating.
So we're not just waiting for things to be reported by players, but we're actually able to surface issues in the community.
And obviously both of those products, and especially Ally, raise a lot of questions and things that we need to think about about how do we train to look for these things?
What data are we using?
How are we tagging it?
How do we decide what's offensive, what's racist, what's appropriate?
And then how do we make use of that information when we have it?
How do we protect the privacy of the clients and of the players that are making use of the system?
And then when we're applying characters that can respond to risk.
input in interactions, how do we make sure that players who are interacting with those characters understand that they're interacting with an AI and they form an appropriate rather than an inappropriate kind of connection with that character?
Thank you.
Celia.
Hey.
So my name is Celia Hulandt, and I am the least knowledgeable person about AI on this panel.
My background is in psychology, actually.
I have a PhD in psychology.
I specialize in child development and development of cognitive.
psychology.
I've been working in the game industry for the past 10 years.
I started at Ubisoft in France, I'm French, and then moved to Ubisoft Montreal.
I worked at the Playtest Lab there and also worked with the Rainbow Six franchise.
I moved to LucasArts, working on Star Wars games like 1313 that never saw the light of day, sadly.
And then moved to Epic Games in 2013 to be Director of User Experience at Epic, because now I'm specialized in game UX.
This is how my background meets game development.
And so I worked on all the different products at Epic, so Unreal Engine and of course Fortnite.
I left Epic late 2017 and now I'm a consultant in freelance and also wrote a book called The Gamer's Brain.
And so I'm very interested into understanding how we can use psychology in developing products for good or evil.
So all these questions are really, really interesting to me.
Awesome, thank you.
Timony.
Hi, I'm Timony West.
I'm the Director of Augmented and Virtual Reality Research at Unity Labs.
My background was originally in product design in social media and others, and I've spent my entire career trying to figure out how to get data from people, largely personal information, and then give it back to them in a way that makes sense and is useful for them.
And when it comes to spatial computing, that has been an even bigger conversation, because we are literally creating tools that let you record everything about your house, everything about the way you move, the way you're moving your device, and then try to put that both back into the engine and then into your game or to your experience in a way where it makes sense.
and actually adds additional value.
So I fundamentally believe that if we take in information from these devices and from our users, we have an ethical consideration to give back more than we've got, and that is, especially when it comes to having devices that have 10 different cameras watching your every move and listening to your every breath, something we really need to take seriously.
So I'm glad to be here talking about it.
So I'm Luke Dickin, I'm director for Central and Strategic Analytics.
You might have heard me talk here yesterday.
I will try not to shout at you so much today.
So I'm at Zynga and we are kind of well known as a data driven games company.
We've kind of come to prominence in maybe 2009 kind of era.
You know, there are quotes flying around that, oh, we're not a games company, we're an analytics company masquerading as a game studio.
So we've been kind of collecting a lot of data over the years, and I think that we do a pretty good job with it.
So I'm really excited to kind of come here and talk to you about the way that we do that.
Awesome, thank you.
So, sorry Luke, we're actually gonna start off by talking about NPCs, I know yesterday you ran into it.
And I'm gonna start shouting again.
About this.
Yeah, that's fine, let's shout.
So my first question to the panelist is, what are some examples that you've seen in video games where you've seen biases that might have been programmed around characters and systems AI?
Anyone want to start?
Yeah, I remember when I was playing Watch Dogs, there was a lot that the AI was always, you would see like women always getting violated and in trouble.
And as a player, so you're incarnating a male character and always coming here to help out the women.
And the women always need.
someone to get out of that violence.
And so the women are, in that game, are massively portrayed as getting beat up and needing some help.
I always like to diss Navi from Ocarina of Time.
How many people play that?
It's not even fair.
She's just clippy.
I get it.
But it was an early attempt to have the user sort of guided and given context as they go through experience.
But she was extraordinarily annoying.
She just wanted you to stay on task.
And the great thing about Ocarina was the first time you could really run around in 3D in Hyrule, right?
So that's what you're going to do.
And she was just there being annoying, telling you to go to the next temple.
And you're like, no, Navi.
I just want to ride my horse.
Leave me alone.
Anybody else?
I mean one that jumps out to me I think is, you know, if you look at sort of systems like in RimWorld, RimWorld has a whole kind of deep simulation system but a lot of it kind of gets represented as very, it's an example of a way that's very easy to get NPCs doing things that are very...
kind of stereotypical because it's leaning into things like, oh, this person has a misogynist trait, this person has a misandrist trait.
And it's kind of, it raises some interesting questions about like, does that actually need to be part of the AI system and what we're actually kind of representing?
Or is there something kind of, I guess it comes back to the question of taboo, right?
Like, if it's taboo for us to talk about it here, is it taboo for us to like?
Express it.
Exactly.
So it was interesting for RimWorld because that was one programmer, one AI programmer, who decided to program what they thought the rules were for how men and women should behave in the game and how sexuality should behave in that game.
What ended up happening is the community ended up going on Reddit and basically demanding the programmer to remove some of those biases.
And the only reason that that person made changes was because the community demanded for it.
So what do we think that we could actually do to help put in place processes?
Or how can developers be more mindful about coding systems without biases that have been put in place?
Well, I think some of the approaches that we see brought into narrative contexts around having sensitivity feedback, having people comment who belong to particular groups that might have a little bit of more understanding of what does this portrayal mean for us?
What is this kind of representation likely to convey about us?
Have a look at the work and give some feedback.
And intentionally seeking out sensitivity players has been really useful for story games.
And I think it's it's entirely possible to bring that kind of approach also into looking at systemic representations.
And I would add to that, it's not necessarily the case that systemically representing negative attitudes on the parts of characters, like having a racist character is not necessarily inherently evil.
But I think we need to be careful about how we represent and unpack that.
And so having people be able to intentionally seek that kind of feedback about their system is quite important.
Actually, that is the curious thing about being the creator of a system.
If you're trying to mimic a real world system, you may or may not agree with.
Because just inherently in creating and defining the system, it sort of feels like you're giving it the thumbs up, even if you're not.
You know, like, because I am replicating this, I feel like the sort of implicit acknowledgement that the system is here, even if I completely disagree with it.
So just an interesting tension.
There's no real solution there.
I know when we were on our call the other day, we brought up some interesting discussions around just what the role of a programmer for systems and AI means, and actually about the education of programmers.
I think a few of us brought up how, who here, actually who's a programmer, has done some type of ethical class when you were in university?
a lot of us, right? Or quite a few of us. I remember personally for myself that it was kind of that class that you didn't take seriously. And it was actually very focused on business rather than actual ethical areas that we should consider as programmers. So what do we think that we can do on the education side for programmers and designers?
Maybe don't even call it ethics.
It doesn't really matter what the ethics are.
That's like a social thing.
You know, it changes from country to country or even region to region.
But just being able to create or have a class that teaches you the underlying socialization systems so that you can confirm your own bias and know that it's a bias, right?
Recognize which part is the socialization aspect and how it differs across different cultures.
And then it takes out the is this good or bad?
And it's just like, this is a tendency that you have.
Acknowledge it and then see if you wanna build it into the system you're designing or not.
Yeah, maybe we should call it unconscious bias more than ethics because ethics is a set of values that we decide and who is deciding how we decide.
It's complicated, but if we talk about unconscious bias and we explain that as humans, we are fallible and we all have biases.
And it's okay, we need to admit it and accept it.
But then we need to work around it, because we can't, even if we know our biases, we still can't get away from them.
So we need to design the environment to go around them.
The example of...
I'm going to try to do that fast.
But the example of if you want to hire more women, we are discriminated against women, even women against other women.
And so if you take the example of the orchestra, I think it was in Boston, they wanted to hire more women because they figured, how come we don't have that many musicians in our group that are women?
And so to avoid being discriminated against women, they started to do blind auditions.
So they wear a curtain, and they couldn't see.
And also, later on, they added carpets, because they could still hear the high heels of the women.
But after they added the carpet and the curtain, then they hired, I think it was, plus 30% to 50% more women in the orchestra.
So we are biased.
It's happening.
We have to acknowledge it, and we have to understand how it works and how we perpetuate them.
And so in AI or in any system, if we don't take that into account, we are going to perpetuate all these discriminations.
Just going to give one more example from AI.
I love that example because it's pretty clear.
In Turkish, there is no gender when you talk.
She is a doctor, he is a doctor, it's going to be the same phrase.
You don't have he or she when you talk in Turkish.
So if you Google translate, he is a doctor, she is a doctor, you will have the same phrase that I'm not going to say because I don't speak Turkish at all, but you have the same phrase for both of these different phrases in English.
Now if you take that phrase from Turkish and you put it in Google Translate and you translate it back into English, then you have, he is a doctor.
Now if you take the phrase in Turkish that says he or she is a nurse, and you put that in Google Translate, you get, she is a nurse.
And so where there's no gender in one language through AI algorithms, it's going to perpetuate some discriminations.
This is where we're at and we need to take that into account.
And if we want more equity, we have to find a workaround.
So actually, I agree with what you were just saying, but I want to make the case for ethics is about more than just the biases.
Biases are very important.
But I think as a culture, we need more training in ethical thinking and how it works in general.
And maybe that's everybody needs to watch the good place a lot more.
But I feel like one of the things that's happened is that because we have, for many reasons, and you know, we won't.
historically, a lot of this kind of thinking about how do we figure out what is good and bad has either been in the realm of fairly academic philosophy or it has been expressed within a religious context. And moving away from those contexts, especially as more and more people don't necessarily share the same backgrounds, we need to have ways of talking about these problems.
How do we decide what is good and bad? It's humanities. We've got to still have them in schools.
So, I think it's broader than just identifying a set of issues.
It's about building that whole framework of philosophy.
Yeah, I mean, Cecilia was talking about, like, all we're doing is measuring the vector differences between words that are close together.
And that is how you get, she is a nurse and he is a doctor.
But that's also how you get, he is a doctor.
We're just, you know, something's running through a bazillion words and trying to figure out which words are close enough to each other that you can predict which word comes next.
That's how it works, right?
So, if on top of that we want to say, but don't say he or she, that's where we need this ethical framework of like, okay, go back through the data set again and now do basically kind of an ethical...
fix.
But the interesting thing, like...
Of the data. So, yeah, where do we get that list?
What is the list? Who defines the list?
Where's that list?
Who validates it? How do we get it part of our process?
But we already have it, right? I mean...
Do we? Is it in your pocket?
In as much as it's everything.
So, I mean, it's real easy for us to kind of step away from like fiddling about with demographic words in particular, but like everybody in this room knows what an imbalanced data set is.
And if you have an imbalanced data set, you know that you need to do something about it.
So like, if you can take that kind of tool set across everything, and then start applying that to places where you're not necessarily applying it right now.
Is that what a fix looks like?
Just how do you account for cultural differences?
There are countries where women cannot be doctors.
I mean, I feel like if we had an actual answer, we wouldn't be here, we'd be off doing it.
Yeah, fair, fair.
Fair enough, it's true.
So moving on from NPC AI, there's a lot of different areas and different types of games that use AI, from social games to online games.
Celia, I know you had some areas you wanted to discuss around like how we could be using AI data better to improve those types of games.
I don't know if you want to talk to that.
Sure. No, I think the only thing that was, just to go back to my previous point, our society is not, we don't have equity and we, I mean, a lot of people want equity.
I think if we can agree to that, then we need to figure out what is it that we are showing, what are the biases that are perpetuating in our games that...
It's not just our games, to be fair, it's perpetrating in everything, social media, in movies and books or whatever.
But we are also participating into that because we are part of the culture.
A lot of people play games now, so what do we want to reinforce there?
Are we going to reinforce discrimination towards women and towards...
people of color, so what is it we want to actually, how do we want to participate? Because we are going to reinforce some discrimination or we can reinforce a better vision of the world.
So since we are participating, even if we don't mean to, what do we want to do? How do we want to participate?
So coming back to, sorry, online.
Online and social games.
And I think it's mostly a few of you here, especially Luke at Zynga as well, and then from Fortnite.
They're very different types of games that aren't just about the biases for characters, but how you're using ADI data to influence those games in different ways.
What do you guys think about that?
Or what are some initiatives that you've done on your sides?
So one of the things that we do is we definitely don't feed demographic information into machine learning algorithms.
Partly because that would be wrong.
It's partly because we don't trust our demographic information.
It turns out that a lot of millennials lie to everybody they can online.
So as a result of that, we know that it's not necessarily the most reliable data, so we don't actually use it as a basis for anything.
But that's an example of how.
you can be a little bit more kind of buttoned up about this stuff.
Like, you know, machine learning in general takes the view that, hey, throw all the data into the machine and see what comes out.
It's magic.
But you can be a bit more intentional about it and you can kind of be a bit more structured and kind of, you know, we, we gather a lot of kind of in game behavior data.
you know, as you're playing through the game, we fire off counters for probably too many things, frankly, given what's in our database.
But at the same time, we can use that to actually back into a model of your player journey.
And then from there, kind of understand, like I was talking about yesterday, kind of what you like and dislike about the game, and what's resonating with you.
That feels like...
There's two different ways you can go with that.
There's the black hat, okay, let's just milk you for as much money as we can and then throw you to the curb.
There's also the way that you can approach it, which is like, let's actually work with you to provide a good experience.
And I think some of it is intentional, right?
Like if I come at it saying, hey, I'm an evil corporation and my comms team's gonna fucking hate me saying this stuff.
Um, but if I'm, you know, if I come at it with that hat on.
then that's probably not super ethical, whereas you can just change the framing and use the same kind of approach, but do it in a much kind of better way.
This actually came up this morning.
I was talking to Ingrid Ayesto, who's our head of monetization at Unity.
And we have the ability to map and track player behavior and change the game on the fly as a result.
And the reality is we could be manipulating the heck out of our users constantly.
So the ethics of how long and how addictive gameplay should be our top of mind, and we're actually working with the university right now to think about that.
And I think that again goes back to, we have the problem, we know the potential bias or the potential problem area, but then we also need to have an ethical base on which to make these decisions.
Because what is the cutoff?
Is it 1,000 hours of gameplay in a row?
If you spend two grand on one mobile game, do we cut you off?
Should we have the ability to cut you off?
It should be a percentage of income.
Maybe you're rich and $2,000 is nothing.
I don't know.
But obviously, it's not good for anyone to waste their life or their money on these trivial things.
So anyway, it's something that we're thinking about quite a bit.
Who are some of the people that you're working with to help synthesize that data?
Because like Celia was saying, coming from a psychology background, I would think that working with experts on that field is who we should be really synthesizing and going through the data with, rather than just the product owners or the programmers on that side.
So have you done any consulting with game companies?
that are trying to figure out what to do with all of that data, and then same question for you, Luke, how are you synthesizing all the data that you're collecting from Zynga?
I mean, one of the things that we do...
kind of poorly is the psychology side of things.
We are very stats oriented.
And I think that's partly because we actually don't have a user research group.
We have a kind of marketing and consumer insights kind of focus test group.
And then we have the analytics team.
And if you have an analytics team, they like numbers and they don't necessarily like people.
So they're going to focus more on the numbers, but it's definitely something that we could probably do more on.
Sorry.
Yeah, so I do a lot of the consulting now, and usually people ask me to help them offer a better experience to their players, so there is that empathy, that's willing for empathy.
But then you never know how things are going to be used, because even if I explain things like...
behavioral psychology and how we react to rewards, especially in a variable schedule of reinforcements, like in loot boxes.
And I explain, you know, this is exciting in a lot of cases.
That's why we love to play dice.
Or if you have critical hit, it's exciting.
So most of the time in games, these things are exciting.
Now if you apply that to monetization, form of loot boxes, then you actually pay money in it.
it becomes a little bit of a problem because now you're using something that is engaging, that we know is engaging, to make money.
Now, is it a bad thing to try to make money when we see all the studios that are closing and it's actually very difficult to survive when you make free-to-play games?
I don't know.
So it's hard because now a lot of people talk about how psychology is used for evil.
We can use it for dark patterns, but we can also use it to nudge people and to make people make better choices for their health or financial decisions.
So this is where it's not clear.
There's more people outside of the gaming industry that are now asking me for consulting for questions around inclusion and diversity more than the game industry so far.
So we keep kind of bringing up data and ethics and AI is kind of nothing unless we are talking about ethics and data collection and privacy.
Emily, I know when we had the call, both you and Luke had talked about different standards and regulations that you had to use to put in place in your products and in your games.
Can you talk a little bit about that?
Sure.
So in the EU, because we're working across sort of multiple continents.
In the EU, we have GDPR, which for people who are not familiar with this, is a regulation about how companies are allowed to store and process and apply personal information that they have from their players and users.
And it means that you have to be very specific up front if you're collecting data.
You have to let people opt out of having their data collected or used.
You have to let them know if somebody places a request.
They can actually, as a matter of transparency, ask what data you have on them.
And then as a corporation, you have to...
keep track of what kind of status do I have relative to this data.
Am I just storing it or am I a processor of the data?
It's a huge piece of bureaucratic overhead which made a lot of people very frustrated.
It took up many, many man hours and woman hours, person hours to make this thing work.
But I think that's the only way that we get around the sort of very strong incentives that exist for corporations to just consume as much data as possible and then do whatever the heck they want with it.
And one of the other really important pieces about GDPR is that.
it has a really serious fine attached.
So it's the larger of 20 million euros or 4% of your global revenue, which is enough to make a difference to a Google or a Facebook to actually make them pay attention to this kind of thing.
And I think that's something that we need to think about.
We can identify a lot of issues and problems and things that we hope that people who are in a position of power within individual institutions are going to take seriously.
We hope that they're going to apply these ethical considerations.
And we can talk about that.
And we can try to create cultural standards and norms about how they should be behaving.
But I think the business incentives in some cases, and especially around big data collection and big data use, are so strong that we really do need the effect of government regulation in order to keep that within parameters that are acceptable.
And for the audience, what's GDPR?
That was just what I was explaining.
What's it stand for?
Oh, general data, I actually don't remember the...
GDPR.
You just have been chanting GDPR for so long, I don't actually remember what the acronym stands for.
General Data Protection Regulations.
All right, and are you using the same at Zynga?
I mean, you have to, it's an EU mandate.
So if you don't want the fines, you have to be compliant with this thing.
So yeah, I mean, we had a pretty big initiative internally, partly because of the very significant fines, but also it is actually, I don't want to say it's the gold standard for what privacy and kind of personal data regulation should look like, but it feels like a really good first swing at kind of government legislation around this stuff.
Agreed.
Unity also supports GRDP.
Actually, when you were talking, it made me realize something I hadn't before, which is we talked, when I was just talking with Ingrid about how we should at least think about whether or not we have a responsibility to cut off players who are spending too much of their time or money.
Somehow ads gets like a free pass on this.
I had a friend who told me she stopped using social media and she stopped buying as much stuff because every third Instagram photo is actually an ad for something you probably were considering buying, just bought, or probably will want to buy.
And we don't really talk about the ethics of that.
Like how much money, how much more stuff can you buy in a year on the internet before the great ads god in the sky says no more, that's enough.
I mean, I run an ad blocker so much of the time that I manage to not encounter this.
But yeah, I mean, it is insidious because Facebook...
So this is like my personal story about how much I hate this, right?
Like I changed my Facebook status to engaged, and that same day I started to see Facebook ads for weight loss.
Like, and it was completely obvious what was going on and the creepy little mind of the algorithm back there.
But anyway, I don't know, I mean, I think that the ads question is even harder than the question of where do we cut off people in free-to-play, because in free-to-play, you at least have the record of like, oh, this customer spent this much money and it's a lot.
Maybe this isn't a good idea.
And so you could have kind of internal standards of how much is, how big is the whale allowed to be, basically.
So you can think about that kind of thing, but when you get into the ad space, it's very hard to know what effect the ads might be having on the person.
I didn't sign up for any of these weight loss programs as it happened, so unsuccessful there.
So, you know, it's kind of tricky to know, but it's a valid question.
But the thing is, what is it that we're value?
What is it that we're measuring?
What is it that makes your investors happy?
It's like how much money you're making, right?
What are the profits you're making?
So if this is what we're measuring, we're not measuring if people are happy using your product or if you give them a better life.
That's the whole question about what Tristan Harris is going around with the economy of attention.
And what is it we're measuring?
What is it we want to offer to our people?
Because we don't, like Facebook, they measure how much you engage with the advertisement.
They don't measure if you actually have a better quality of relationship with your friends.
So what is it we measure?
Because this is what is going to drive the algorithm whatever to optimize it.
So we know that, for example, the reason why click bait is working that well is that outrage is making us react.
And this is something that makes us react.
spend a lot of time on Twitter and get a little excited about something.
So if we measure, if the thing that we want to optimize is people clicking on the link, then all of a sudden their algorithm, of course, is going to favor all the big claims and the stuff that's going to actually make us fight each other. So it has a terrible impact on our relationship, on society. So what is it we want to measure? What is it we want to optimize? Is it always making profits and making the most clicks, the most views?
Yeah, exactly.
Like, it doesn't matter what ethical standards we have.
If some PM's KPI for this quarter is to increase activations 25% and you're like, well, you know, it doesn't quite match up.
That doesn't matter.
They don't get their bonus, or they lose the respect of their peers.
If you want to go to a little bit more high level, they're seen as a bad worker.
Like, we need to have...
what people do for a living and what they're rewarded for financially and socially match up to the ethics or they're just not going to implement it, which is what we're seeing today.
Yeah, I mean, I seem to be up here making big statements, but I think that part of what we see is that it is hard to debug the system, the systematic incentives, without sort of taking, here are the results, this is the kind of results that we're seeing is AI is allowing us to take what we were already doing much further than before and we're seeing bad results from that.
But what that is telling us is that we need to debug capitalism.
Like, it's not just the machine learning that needs to be fixed, right?
Yeah, I mean, it's a system. We can manipulate it.
Just turn it in a different direction.
It could be better.
Everyone still gets to make a lot of money, you know.
Yeah.
Well, but anyway, that's...
It's a different mode of data.
Timony, you mentioned this in your introduction, but to do XR experiences, it's even more important today to get as much data about the real world around you.
You mentioned that what you want to know is...
see what the user sees, hear what the user hears, but there are a lot of discussions that are currently happening around what does that mean and what do we expose to developers and to users.
So what are your thoughts around that?
Collect as much data as possible so there's too much and then no one can do anything with it.
Honestly, I tend to be pretty laissez-faire about data collection.
I briefly worked for the State Department and it was very Kafkaesque and I walked away thinking this is not going to turn into a police state.
They're not that organized.
And if we get to a police state anyway, that's always a question, right?
It's always like, oh well, people have your data and you don't know what they're going to do with it.
And if in the future you're suddenly on the bad list because you did something in the past that was recorded, you're fucked.
Okay, you are.
But if we get to the point where we do suddenly turn America into a police state, we have a lot of other problems as well.
And I don't know if data collection is top of the list necessarily.
That being said, I do know people in the audience who have been on lists and therefore going to the airport sucks for them.
Or because of their name, because of how they look.
So I do want to be sensitive to that.
There are a lot of inroads right now into new ways of anonymizing personalized data.
The biggest problem right now is it tends not to be performant because you can only perform.
calculations on encrypted data, which doesn't work well on mobile devices, which is basically what AR HMDs are, for example. But I think as computers get faster and smaller, we'll continue to see inroads in being able to basically obfuscate parts of the data set so that you can have a personalized experience, but it doesn't tie back to you personally. But then there's a whole other question of you probably do want a digital identity that's tied to you personally as you go from space to space.
or room to room in other people's homes, and you want to see what they have, or they want to give you permission to see the digital goods in their house.
But that's kind of for our field.
At that point, you end up with the same thing of you walk in to go shopping for a wedding dress, and you walk out and you see a digital ad about weight loss that says this way.
Yeah, and digital ad blockers.
Kind of riffing on the XR side, we're seeing a lot of talk, I think every platform is talking about doing digital characters in the real world.
Also with Spirit AI, like your whole focus is about how do you create these interactions and build like AI systems to interact with digital characters.
We're also seeing where how many people in here have Alexa or Google Home?
at home, quite a few.
And essentially that is also a digital character, just without the visual aspect of it.
There's a lot of talk about kind of the humanizing side of things, and again, what do we do with data, and how do we design these digital characters to not have these biases, similar to what you were talking about with watchdogs.
So, what do we all think about that?
I think what we're seeing, especially with some of the clients that we're talking to, are use cases.
There's the use case of it's just a digital character or an assistant in your home for adults, but there are also especially a lot of use cases that are even more sensitive than that.
So cases where...
People want to create an educational character, a toy.
It's an educational game series where this character kind of lives with your child for a period of years.
And it has DLC that teaches your kid stuff and remembers things about what your kid likes.
And of course, that runs smack into all of the child data protection issues.
But also a lot of sort of subtler things about, like, what does it even mean if you've got sort of an educational program that is shaping itself in response to the child?
And like, what are your responsibilities about what you teach them and don't teach them?
And it gets very diamond age, and it's kind of strange.
But that's an interesting area.
And then another big sort of point of use is in cases of people who've experienced trauma or who are suffering with autism.
And have or whose parents kind of want to help them get through learning to interact socially.
I probably shouldn't say suffering with autism. That's not a good way of putting it, but So in cases where the finding is that some people in some situations actually find it easier to talk to a digital character than to another human, but then you're again creating a context where there's the potential for the AI to do a lot of good, right?
There's the potential for the AI to make somebody comfortable talking about something that they don't want to talk to a human being about because they're ashamed or because they're uncomfortable for whatever reason.
So that's high value, but it's a high risk, high reward kind of.
situation because it's such a vulnerable space, you could also do considerable harm.
So I think there, again, a lot of the kind of impetus is on making sure that the people who are working on these materials are coming to it with the background of kind of educational experience and psychological experience, that it is not just a product being formed by an entrepreneur who like, came up with this because it was an easy story to tell to VCs.
Like, it needs to come from some place of much deeper understanding than that.
This is where I always get into. We need to tell the computer no, and the computer needs to be able to hear that and react to it and record it for future context, right?
Like, now today if Siri starts playing music or Alexa, I'm like, no, not that song. No, you're all wrong.
No, not Amazon Music. Whatever.
But this needs to be the case for...
all, especially in XR, when things are going to be coming up to you or trying to attach themselves to you or pin themselves to places in your home, you have to be like, no, that's wrong, no, that's wrong, no, that's wrong.
We're already starting to be able to do this with ads in a really systemic way, because the ads follow you all around the internet, so oddly it's kind of a good use case.
But yeah, we need to be able to tell the computer no, which means that we have to have that in every piece of software, the listener that listens to the no and records the no.
There's also the idea of as we start seeing that there's more and more AI Happening all around us. Do we have a responsibility to start explaining to people? What is an AI and what is not an AI?
I was at a conference once and we were talking about Alexa and a few people that were there were telling me that Their little kids started yelling at their parents because the parents had said Alexa play song X and the children Yelled back at the parents saying you didn't say thank you to Alexa And basically, the people had said that children are growing up where they feel that Alexa is part of the family, that it's a real person.
And as we're starting to see these digital characters come into the real world around us with XR to these smart home objects and really AI everywhere, what do we think we need to start thinking about?
How do we explain to users this is an AI versus this is not real?
I think it's actually, I didn't look this particular point up before this panel, so I may be misremembering.
But I believe it's the case that the state of California law is that if you have a chatbot or something similar to it, that if the user asks if it is AI, it has to respond correctly.
Like it has to admit that it is AI.
You cannot have a character that is going to say, no, I'm just a customer service agent.
I'm Tim, and I'm here to help you.
And that's clearly.
It kind of, there's a particular sort of customer service context in which that could be important is like, you know, if as a human user I really need to make sure that I'm getting accurate information back from this company that I've got somebody that I can hold to it or whatever.
So there's kind of a specific local case.
But I think more broadly it is.
important for people to understand what they're interacting with.
And the thing that I keep thinking about in this context is that many years ago, so I made a game very early in my career that was a conversation game where you're talking to this character, and it was a very low fidelity. It was all text. You were interacting with it by typing, so nobody was going to be confused that it was physically a person.
But at one point I got this email from this guy who said, like, thank you for making this game, Galatea.
Like, I keep it on my phone, she's become my best friend, and I talk to her every day.
And I felt like...
thanks, but I'm actually really quite concerned right now.
And so like that seems to me to suggest like some other areas where we need to be careful, right?
Like, you know, it's, is it okay to be using AI characters to resolve loneliness?
Is that maybe that's a good thing?
Maybe it's a little disturbing and.
Sorry.
So I agree with all that.
It's just because of my child development background.
I just want to mention one thing.
Children have a tendency to animate things that are inanimate.
So I'm not so sure about that part.
Do they really not understand that Alexa is not a real person?
Because children will have a tendency to say, oh, you kissed me goodnight.
You have to kiss my doll goodnight as well.
So I just want to make sure that we are not saying that kids are not able to discriminate between the two.
I wonder if we can come up with a new way of human, I mean we do this all the time, right?
To your point, or you know, if I make two circles and I'll smiley face, everyone's like a face.
We're really good at pattern recognition.
I wonder if over time we'll just start to create this new space for these digital creatures.
There are several excellent companies that make AIs that help people through depression and suicide and have great search, or great results.
And I would not want that to stop.
That's awesome.
Right, and clearly they've defined this relationship to this digital being that works for them, knowing that it's not human because it's very clear when you go there.
And I love that, I love that use of the technology.
What I don't like is when people kind of humanize this sort of malicious intent like, oh, it's all gonna be Skynet and they're gonna destroy us and we deserve it by God, you know?
Now that's just you predicting.
So yeah, maybe in teaching people about what AI is, we can start to come up with a language that allows for that middle ground where this is made by humans and it's designed to be interacted with by humans, but it is something else.
Yeah, I mean, I think if you present it as like this is an extension of the people or people who created it and they do, you know, want you to recover from whatever you're, you're dealing with or whatever, there is a personal connection being made through the AI, but it's not.
Yeah, yeah.
Yeah.
Yeah.
So that kind of transitions into our last question I want to ask everyone before we open it up to Q&A.
We're all passionate about AI and the ethics around it because I think we all inherently believe that AI can do a lot of good in the world around us.
So what are some recommendations you can give to the audience, ideas, areas you wish for this whole community to start driving AI towards?
What's to start?
Oh, I can start.
Yeah, look into psychology and try to understand how we influence people.
We all influence other people.
And we are influenced by our environment.
So we need to understand that better, understand behavioral psychology better, not just for it to make more money and to hook people up.
But we need to understand how we can use it to.
favor equity and to make people feel better because also it's at some point it's going to be a good business drive to have a trust relationship with your customers and that you treat them with respect and make them happy in the end and not just like make them pay, pay, pay, pay, pay, because if this is the only metrics that you have, then of course you're going to favor that.
So look into these things to understand it.
And again, this is not about, oh, you're a bad person.
We are all biased, and we have to be in peace with that so we can actually move on and solve the problems that we want to solve.
So I would add on to that.
Um, cause it's really cool to be like, okay, well now we are in the psychology portion and we're going to now think about that, but bring that lens to everything.
Like literally every decision you make, make it intentfully around what Celia is talking about, because it's really easy to like sideline it and go, cool, we're going to make a whole bunch of systems.
We're going to throw a whole bunch of stuff out and here's the kind of like ethics bit, but bring it to everything.
the ethics module.
I think another area, and this is a little bit of a tangent from some of the things that others have suggested, but I think the fact that machine learning tends to take our biases and exaggerate and expose them.
causes embarrassment sometimes, and that's a reason why we need to be careful about the balance of our data and so on, but it is also a way in which the process of working in AI gets us to think about the systems that we're part of in general, what is embedded in the world around us and in the data around us all the time that we ignore out of being used to seeing it that way or sort of a general level of comfort or having incentives not to notice it.
So.
Thank you.
first of all, sort of a practice of interrogating the things that we build with AI and saying, like, all right, well, what is this expressing back to me? And not only is it broken, but also what is that telling me about humanity and the systems around me? And should that perhaps be addressed in some way? And then that there's tremendous potential in our ability, and this does get back more into the kind of games and simulation space again, to build models of how we think pieces of the world maybe should be and explore whether we're satisfied with that, you know, potential way of doing things so that we can then move towards more just remediation in the real world.
I just have three things to write down to Google later.
The first is homomorphic encryption, which I sort of vaguely referenced as a way of anonymizing data sets.
The second is differential privacy, which is another technique used to do basically the same thing.
And then finally, and this kind of builds on what everyone was talking about, I highly recommend Shane Parrish's Farnham Street, which is an online rationalist community that has a wide directory of different systems of thought and mental models and collection of biases.
So if you want to delve into how you can start thinking more clearly and accurately, there's a lot of online communities, but that one is a great place to start.
Cool.
My final one would be consider accessibility.
And how do you create systems that can ensure equal opportunity and equal accessibility of people getting to try your systems?
Whether this is from a video game and making a video game where the system is not just about killing and shooting, but how do you make systems that allow the users to do many different types of interactions?
to having digital characters in the real world.
Not everyone has access to the hardware.
Not everyone is going to have access to systems.
So thinking about how can we make...
and build this community to be accessible to everyone.
So we've got nine minutes left.
We'd love to do some Q&A.
Please ask any question.
This is a list of references that our panelists had put together of books that we recommend reading, a lot of articles that dive into things from machine learning to standards and regulations.
So please take some pictures.
But we would love to hear from you.
Over here, I can't see.
Is that Neil?
Yeah.
Some of this echoes earlier GDCs or perhaps CGDC back then.
CompuServe and Genie had the same issues.
They called it credit card meltdown.
That is it ethical to let someone, for example, a sailor on a nuclear submarine who spent six months underwater, comes home and melts his credit card with online charges playing games on Genie and CompuServe.
And my suggestion there is that you'll know we've done this if we're not back here in 10 or 20 years doing the very same set of questions.
And perhaps there is even more history for us to go mine along this stuff.
You've all talked about the idea of labelling data in terms of the happiness and contentment that it provides for people.
So can you say a little bit more about any ideas that you have as to how we might go about that?
Can you repeat that?
Sure. So you've talked about the fact that if we label our data sets instead of with the amount of attention grabbed or the number of dollars we get from a person, instead we label our data with the satisfaction and happiness that it may have created for the user of an app or a game or a web page or an ad or whatever.
Could you say, because it's easy to be glib about that, and I feel the panel is maybe an opportunity to do a little out loud thinking about how we might label our data for happiness.
So the question was, in short, how do we label our data for happiness?
Sorry, I'm supposed to be repeating questions.
Yeah, I mean, it seems like probably more of a reinforcement learning kind of thing, perhaps, where it's not so much that you start out with labeling, I know that these things are going to lead to happiness, but I'm detecting something.
But then that leaves the heuristic completely undefined.
So I mean, I don't know is the short answer.
But I think it probably.
Our best attempts to work that out, I think, I am definitely thinking out loud, so thanks for permission to do that.
I think it's going to be very much on kind of a case by case and product by product basis, like what kinds of things in the context of this particular situation would equate to happiness.
There are all sorts of things that you can do with sentiment analysis and stuff like this, like how much expression are you getting back from the user?
This starts to get into territory where I actually have done some prior thought about it because one of the things that we're interested in with character interactions is being able to tell from inputs that we might have from language, from facial expressions, from gestures, that the user is in a particular mood.
Right.
Right.
Right.
Right.
Right.
Right.
something that you had entirely trained to, you know, be reinforced if the user is smiling and like, let's all smile at our computer all the time.
It's super creepy.
So really, that's not really the answer.
But I think there are some things where basically.
If we're in, what we really need in order to train towards that is something where the user or interactors experience is pretty expressive because otherwise we're training against inputs where all we have is like, oh, you completely watched this entire YouTube video end to end and that makes me think that you would like to watch these other creepy, like even more politically reactionary videos from end to end.
And we've seen where that leads.
So I don't know, but I think there are sort of things to delve into, especially like starting out in the spaces where we have a lot of information about how the user is reacting like XR.
So I would probably say, and I'm thinking out loud as well, but adding on to what Emily's saying there, the YouTube example is a single kind of axis of thing.
I think that expressive things like happiness and sentiment and that kind of thing are going to be multidimensional.
So Yeah.
If you define it in terms of, so as I was kind of sat here thinking about it, you know, retention for us might be a good one as a proxy for happiness, except it's not really, because if you take that to the ultimate extreme, then the AI system optimizes for sending somebody to your house and saying, fucking play the game.
So, you know, I think that if you actually track kind of multiple axes, then you end up with a more intricate function that allows you to capture something, but that's not super actionable.
I mean, there's also immediate pleasure versus long-term happiness, which, I mean, as humans, we're not even particularly good at working that out.
So...
That's why we need nudges.
Yeah.
But also, look at what Tristan Harris is doing.
It's exactly this question that he's exploring, the ethicist.
Because, for example, if you're Tinder, what do you measure?
You measure the time that people are spending on Tinder.
But ideally, you want people to match with other people and have fun with them, and so therefore not being on Tinder.
So what is it we're measuring?
What is it we're trying to accomplish, and how we can make sure that we can?
have a business and still provide what we want to offer to our clients.
That's UX, you want to actually offer the experience you want to offer and not just have like a business orientation.
We're going to go to the next question.
We have I think room for two, maybe three more.
Hello, sorry if I stutter, I'm a little nervous.
Oh no.
Being realistic, it will be probably a long time before we have global regulation in AI ethics.
So how can we go about justifying?
like for producers and managers, that we have to care about these things.
Like, for example, in the Facebook ad about weight loss, how could we justify that we shouldn't show that ad, even though if we show, we know that it can perform well maybe?
And in a different realm...
like uh in the realm of diversity for example um there is a lot of research about uh why it's important to have like diverse teams and like uh so we can use this information to justify to the managers why it's important uh so are there uh any questions?
Is there any studies being done like, oh, we, if we are more ethical, we can also perform better?
So that's a great question. We're asking, the platforms for ethics and AI just doesn't exist today.
And this is why we're talking about it and trying to see what people can do.
So two questions there. One, what can we do to help inform producers, people on teams to think about ethics?
And then what are some of the trainings that already exist that we can share with managers?
I'd say there's more studies on how diversity improves monetization and KPIs than ethics, but I think you kind of get one from the other, or at least a broadening of that.
Yeah, I mean, there's a lot of horror stories out there right now about like machine learning gone bad.
Right. Usually as a result of a lack of diversity.
Exactly.
And I think, you know, finding those kind of articles is a really good start for this kind of work.
We should do it more.
Yeah, also about having a great diversity in your team is going to help try to figure out how this system is going to work and to try to come up with all the different ways it can go bad.
If you only have people that look like you on the team, it's going to be very hard to come up with all these examples.
So that's a way also to talk about.
And I would say for us, it's talking about this, not just that advocacy.
conferences, but talking about this with the people actually working in the field, so they start being more aware.
Last question, Nicole, I think?
Yes, thank you.
Great panel.
Really awesome that it's here at the AI Summit.
And my question is just an extension of the idea of like, well, in the context of Facebook, I actually have a gender blocker.
I declare myself as non-declared.
And what I found is that I got less.
wrinkle cream and dresses, and you know, I mean you're going to get Zooli ads no matter which gender you choose.
But I thought that was a way of me as a user saying, look, that's not what I want to see and I will go out intentionally on the net and like go to some very specific sites in order to kind of crack the code that might be, that's the preferences that the AI is assuming about me.
So my question is, is there a lot of talk about giving users control over the platform?
Can we have a reset button?
We've talked a little bit, I've heard a little bit last year about having a reset button, not just for ads in general, but like my social media feeds.
I want to be able to hit a reset button and just start from scratch.
I may go down a really dark path, and am I going to be able to like my way out of that dark path?
And I've got multiple personalities, not as a medical condition, but just simply, I run a studio, I'm a designer, I'm also an engineer.
I want to be able to, and I'm in a lively environment, so I want to be able to switch.
And I don't have those kind of controls either.
So there's this efficacy of just being like one thing for everybody, and that's just like a fire hose, without that flexibility.
So are people talking about the ethics of user control?
Certainly they are.
The idea of especially the right to be forgotten is like how people talk about like your permission to have information about you removed from the system.
And there are specific applications of that under GDPR, but that could apply in a lot of other cases as well.
I know we don't have a lot of time to answer so.
Yeah, especially in Europe, there's a lot of people thinking about that.
And there's a lot of committees.
In France, you have the CNIL, C-N-I-L. They're looking at all these questions.
But yeah, in Europe, you can ask for your data to be removed, so I'm not sure.
Yeah, I mean, it's not necessarily easy.
It kind of depends on.
But I think it's possible to.
Yeah, I mean, but as people become conscious of that, building products towards a point where it becomes easier to opt out of things.
Or say, you know what?
I would like to change my gender presentation now, and that's across the board, so let's acknowledge that, like all of those kinds of applications.
Yeah, I mean, there's also, there is a flip side to that though, that is the ethical consideration of people using that kind of tool to remove things that people should know about.
You know, we, there are a lot of kind of right to be forgotten cases where it's a politician who doesn't want some image to be circulating, generally of them with some high profile donor or something.
you know, there's a counterpoint to the ethics, and I think there's use cases on both sides.
Yeah, but I think it's also not just the right to be forgotten in terms of what I publish, but as a consumer, I've got a right to, you know, adjust, you know, the difference, you know, I want to be able to go to a horror movie or a romantic comedy or something like that.
I don't have that flexibility in social media right now.
I don't have a wide variety of things.
It's just one thing.
And again, you can navigate yourself into some very specific feed content, and it's very hard to get out, especially if you're not a developer.
I think we're going to have to continue the discussion out there.
We are already over, but thank you so much.
Thank you to our panelists.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
