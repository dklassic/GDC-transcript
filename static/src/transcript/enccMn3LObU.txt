Hi, I'm Jeremy Abel, Technical Lead and Audio Programmer at Feral Cat Den, and I'll be talking about our game Genesis Noir and how we built a level around jazz improvisation.
So Genesis Noir is a poetic adventure game with an emphasis on exploration and a simple palette of interactions.
You play as a cosmic god in a world where the Big Bang is a gunshot fired at the woman you love, and the player's goal is to try and stop it.
The gameplay loop consists of entering the universe and exploring a series of standalone short stories, and each of these stories tells a story about a human character and illustrates some aspect of humanity. For example, a hunter character illustrates humans' mastery over their environment, or a scientist character who shows curiosity about the universe.
For the level focusing on human creativity, we really wanted to tell a story about a jazz musician due to the noir style of the rest of the game and how much jazz music kind of infuses the rest of everything else.
And we kind of thought that the most interesting way to interact with a jazz musician is through music itself, and we really liked the idea of learning how to play music and having the human character teach the player about their musical world. We kind of thought that was pretty cool.
And I have first-hand experience in this, having played drums and jazz bands all throughout my school years.
Learning to play jazz is different from learning classical music.
With classical music, you're expected to play exactly what's written on the page.
The sheet music tells you how loud to play, how fast to play, when to press the piano pedals, how short the notes should be.
And this is all done in the purpose of expressing the composer's intent.
It's way less about the performer's expression.
It's more about the beauty of what the composer has created.
But with jazz, the focus is much more on the individual playing of music rather than the composer.
If you look up 100 performances of Hungarian Rhapsody, the differences between them will be subtle, especially if you're just a student.
If you play it wrong, your teacher will probably yell at you.
But on the other hand, you can look up a hundred versions of all of me and there will be a lot of clear variation in speed, feeling, texture, instrumentation, and everything else.
The sheet music doesn't even tell you what instrument to play or how fast to play it.
It just kind of gives you a melody and some chords, and then you're expected to just kind of do whatever you think is good.
The other prominent aspect of jazz is the solo, which used to be a big thing in classical music, but eventually kind of fell out of fashion. I think it was called cadenzas in that context.
But teaching how to solo is really difficult, and in school they'll give you sheet music like this, where it just says repeat for solos, and then there's some chords and a bunch of slashes, and the slashes are just, they just mean play whatever you want.
But that's only in school. In the real world you won't even get that. You'll just kind of get these bad crx copies of handwritten sheet music and you could buy them in a book called the real book.
But the people who made this book didn't have the licenses for the songs to print the sheet music.
So the only way you could get them was from someone's car truck outside a jazz club or if you know who to ask at a music store.
And the other thing is you can't let anyone see you have one of these at the jam session because you were expected to learn all these tunes by just listening off the albums.
And reading off the real book in a jazz jam session would get you kicked out.
You're supposed to memorize them and listen to the original tracks and then figure out what you're supposed to play from that.
So we felt that learning to play jazz seemed like the perfect story for a video game.
Where you have a mysterious community that's distrustful of outsiders, strange rituals to learn, and a big book of secrets.
So you're kind of becoming a jazz wizard, I guess.
So how can we make this fun and interactive for someone who has probably never played jazz before?
Well, let's look at the traditional path for learning this stuff.
Step one, like everything else, is to copy the professionals.
In jazz school this is done by transcribing solos to learn how a solo was constructed.
And I guess this is kind of like copy pasting code from the internet.
You would usually spend, you know, a couple hours just listening to a song over and over again and writing the notes out. And then you would get graded on it, I guess, on accuracy.
So turning this process into kind of a Simon Says game made sense. I'll play a bit here.
And you can see how this works in the game.
So as usual, it starts out with only a couple notes, and it adds more as you go on.
We kind of have this rectangle style to evoke piano keys.
And after a few rounds of that, the player gets kind of blown away by the human.
This is meant to kind of evoke the common feeling of being inadequate as a musician.
But our character has an idea of what if we reverse this interaction.
It's inspired by my experiences in school where the teacher would say stuff like, improvise using only these four notes.
So the goal of the interaction is to pick notes and have the human bassist kind of play a cooler, spicier version as a call and response kind of thing.
So now we get to the big question.
How the heck do you make a computer play jazz music in the first place?
And this is caveated without machine learning because we don't have the time or the budget to really go down that route So I started doing some research and I found this great video by 8-bit music theory that analyzes a saxophone solo from a Mario Kart 8 tune and One of the things that he mentions is a technique called chromatic enclosures, where you start with a single note and you add notes semitone above and below the initial note.
And this sounds like...
A really great potential algorithm, I guess, for taking an input set of notes and spicing that up, making it more interesting.
And at the time I was really excited by this discovery.
I didn't really think fully through why you would want to do this, what's the benefit of enclosures.
I was just jazzed about there being a potential solution to the problem of starting with four notes and coming up with a cooler version of those four notes.
So you can even make really long enclosures.
You can start with one and then, you know, suddenly it turns into five by, you know, kind of weeding in and around this note that you're targeting.
So I got this idea of making improvisation mutators where you would take the player's input notes, run it through some algorithm, and you would get some spicy output notes.
So the first one I wrote was this enclosure one where it would randomly add enclosures to players' notes.
So if our input sounded like this, ðŸŽµ then the enclosured version might sound like this. ðŸŽµ And, yeah, it doesn't really bear much resemblance to the original input.
You can see that the targeted notes are in red, and they're just kind of enclosed by some other random notes that are above and below the targeted notes.
I also wrote one called Double Notes.
This one allows us to kind of, here I'll just play it.
So if our input is.
This kind of allows us to feel the swing rhythm more, which is a critical part of jazz.
You can play it called straight, which is without swing.
And then with swing, it's like this.
You can hear how it has way more of a bouncy sound, and that's kind of a quintessential part of jazz.
And then I wrote one called Adjust Durations, where it just kind of randomly changes the durations of the player's inputs.
So if our input is this, we can adjust that to...
Which sounds interesting, but I decided that we kind of needed to know the durations of the player's input notes.
which is called quantization, where you snap some inputs to a temporal grid, basically assigning sensible note directions to what the player had played. And it turns out this is a really difficult problem to solve, especially if you're recording inputs from someone who's trying to play a video game and not actually perform music. But at this point it's probably like, I don't know, 10 p.m. and I'm way too in the zone, I'm too busy writing code to stop and test the code that I've already written, so I come up with this a cheater solution.
So if this is our input, we can find the maximum duration between two notes, divide that into three buckets, assign note durations to each bucket, and then measure each input's duration and sort them into the three buckets.
And this turned out to be good enough.
I still haven't really tested any of this, but I'm pretty proud of my GE quantization method.
I commit at 1 a.m. and I come in the next day to test and it turns out that all of this sounds kind of like crap.
It's not very interesting. It doesn't really spice up the player's inputs too much.
It just kind of sounds like random stuff.
It turns out this is a pretty difficult task. A lot of work has been done trying to make a computer generate a jazz solo.
A lot of papers with lovely diagrams.
And great titles like Recombinant Jazz Improvisations.
And there's also a few open source tools like this one called Improvisor with some pretty great UI options like this button that says prepare critics and these check boxes where you can rectify and color your solo.
I have no idea what those mean but it's fun that they exist.
So at this point I realized that I'm making a video game, not a four-year PhD research paper, and I need to kind of step back. So I take a break for a few months, I work on some other things, and eventually I come to the realization that A generic solution is overkill. It just needs to be fun for five minutes. The player isn't going to be playing this for hours. It's just a small segment in a larger game. It just needs to be good once.
But can we find anything in the work that we've already done that might be useful? So let's take a look at that one more time.
So here's the add enclosures.
Like we said before, you don't really hear the player's input very much.
That gets transformed into...
It doesn't really flow, it doesn't really have much of a cool rhythm, it's just kind of, oh great, I added two more notes to the beginning of all of this input.
The double notes was interesting in that it helped us feel that kind of swing beat.
but it's not super interesting as a spiced up version.
And our adjust durations idea was also interesting, but our original implementation was dependent on knowing the durations of the player's inputs, which, you know, came up with this whole quantization problem that I just didn't want to deal with.
So I decided at this point to actually emulate what I wanted the game to do.
I wrote out a sequence of notes for the player's inputs and then set up the keyboard and made up my own Spice Step versions. And eventually I realized that since this just needs to sound good once, why not just pre-make the improvised responses and then build them to adapt to whatever the input is. And this is what we ended up shipping with. So let's see how this plays out in the game first and then I'll break down what's going on.
Cool. So what we do is we record players notes into an array and we don't really care about the timing since we'll be applying our own rhythm. We just record them in a list. And then we have these things called response midis which are used to impose a rhythm on the player's inputs. And this is what one of them sounds like.
And it doesn't really sound that great, but that's because we don't use the actual pitches.
Instead, the note pitches are used as indices into the player's input array rather than actual pitches.
So MIDI middle C, which is MIDI note 60, maps to the first index in the player's input array.
So for each note, we replace it with the player's pitch from the input array.
And let's see how that sounds.
So this is our input.
Then our remapped response sounds like this.
And that's a lot more interesting. We can hear the player's input in the response, and we have complete control over how it sounds, what the rhythm is, what notes are picked.
Instead of letting the computer pick randomly and chancing the fact that it would sound bad.
And one thing that we found was that it only really sounded good when the order of the player's notes was mostly kept intact, especially at the beginning of the response.
So you can see how we start with 1, 2, 3, 4, and then we just step back down that scale.
And this is the first one that we saw in the video.
They get more complicated as it goes on.
But if we start in the middle of the player's input array, it gets difficult to hear that the player's input was the source of the response, and it doesn't really sound like the human character is riffing on what the player is doing.
So there are three responses in total, and we let the player enter more notes each time, so we mirror the increasing complexity of the NPC-led Simon Says Game in the beginning.
So let's see how it plays out.
This is the third and final round.
So we end up getting blown away once again.
Clearly we need to practice more.
So the four blocks from the Simon Says interaction kind of expand out into this full array of buttons like a piano.
And now we have way more control over what the player can play.
So they can hold the bounce button down and drag across the screen to play a quick string of notes.
And this is kind of meant to represent the struggling practice phase of learning to improvise where you just practice scales for hours.
And you'll notice that we're not using the sample sax anymore, because playing fast notes like this, allowing the player to play fast like this, leads to problems with our sax samples, where the note-to-note transitions sound super cheesy, like a really bad sampler.
So this is what our first test sounded like. â™ªâ™ªâ™ª So we clearly needed to find a solution for this, just because it sounds really bad.
First thing we did was we tried the SoundFont, but that ended up sounding like bad 90s MIDI.
And we also considered physical modeling, which sounds really great, but it would take way too long to implement.
And, yeah, I mean, again, we're making a video game, not a simulated saxophone.
So we kind of decided to lean into the standard synthesizer tone built into Unreal Engine.
And we kind of enhanced that hesitant, I'm not good at this yet feeling by adding a lot of pitch wobbling and modulating the time of a delay effect.
And that by itself without the backing track sounds like this.
You can hear it just kind of trails off into this drunken kind of style.
So let's continue and see how the rest of the interaction plays out.
So once you satisfy the basis with your practicing, it's on to the next step of the jazz learning adventure.
So instead of just pressing buttons now, the player is making this jazz sculpture, a completed piece of art.
And we kind of represent this as a brush interaction, where the player draws their notes in free space as a geometric path of lines and blocks.
At this point, there's no notes to click, no buttons or whatever, you just hold the button down and it automatically makes a note.
And we use the vertical axis of the screen as pitch.
And this is a look at the prototype that we had built originally, where it kind of starts to look like a city, but eventually it kind of evolves into this more characterful style, where there's windows and everything kind of pulses to the beat.
And we also switched to a way more prominent backing track.
So this was designed really intentionally to make the solo fun to play.
So let's talk about how we designed that.
So, in jazz, the backing track, or the track that you're soloing over, serves as the scaffolding for your improvisation. You improvise using the chords and structure of the song that you're playing over. And this is what's called a lead sheet, which is just the basic melody and chords and nothing else. And an entire band, an entire quartet, or whatever, would be able to play the entire song with just this amount of info.
And in school, you'll get these lead sheets and will occasionally be given scales that you can use over each chord.
A scale is just a series of notes that increase in pitch.
So, like a C scale or a D scale or whatever.
And this is an example for Miles Davis' All Blues.
And it says stuff like G Mixolydian, G Dorian, etc.
And that doesn't mean much to me, but it makes more much more sense like this, where it just hears the notes on the piano that you can play that'll make it sound good.
And when I was working on this slide, I got distracted for like an hour just playing over the song.
And this is kind of how I figured out originally that I wanted to make a level about improvisation, as I was just messing around on my keyboard while listening to some albums and realizing, oh, this particular song is really nice to play over.
So I recorded a bit of my own improvisation to kind of talk through what makes this song interesting to play over.
a little bit more context than four slides of the game transcript . . . . . . . . . . . . . . . .
Apologize for subjecting you to my horrible improvisation.
But what you want to kind of do when you're making a solo is kind of have this interest curve is what it's called, where you kind of take the listener on a journey with sound.
And the song that you're playing over can really help you do this.
And that's kind of the aspect that makes all blues so much fun to play, is that you don't even really have to think about, you know, oh am I, you know, do I want to increase intensity here? Do I want to decrease intensity? Because the song kind of does it for you.
And that concept is called tension and release. So in music, in western harmony at least, certain chords feel like they need to resolve to go somewhere.
We can hear that in this short example here.
I'll play it again.
So we're stable, tension, release, like that.
And you'll hear that all over the place.
And it's really prominent in all blues.
So I'll play this segment again without my solo from the actual album.
And we'll see, I'll talk through it.
Stable, playing through the melody.
Oh, it's a lot, a lot of tension.
But a lot of tension adds to bigger release.
And that's the whole song. It just loops like that for five or six minutes as people take solos over it.
So, returning back to this kind of home sound adds this inherent sense of progression, like the song has been somewhere and now we've kind of come back.
This provides an inherently interesting structure to play a solo over.
So with this in mind, we kind of bake this same kind of tension to release into our backing track.
And our harmonic changes aren't as extreme as all blues are, and that's in order to not totally blindside the player and have them kind of focus on their own playing instead of, oh, where's this song going?
So I'll play our track here.
playing in fast-paced rhythm This is stable.
The more you repeat it, the more stable it becomes.
So we can also assign a scale for each chord.
In this case, we use the Scenics Lydian scale.
It sounds like this.
And then we switch to the C sharp Mixolydian scale, which sounds like this.
And we encode these directly as markers in the music track in FMOD.
And they're notated as a root note followed by the intervals between the notes in the scale.
So we have scale starts on C, then we skip one, two, one, two, just one.
1, 2, 1, 2, 1 again. And that gives us C 2, 2, 1, 2, 2, 1.
And this allows flexibility in changing the scales later since the backing track hadn't been finalized by the sound team when I was working on the interaction. And an example of this is switching to C sharp just one step up is only one character change in in the marker.
So let's check this out in context.
You can see we're stable here.
Now we've gone up to C sharp.
Eventually we kind of build up this whole cityscape and the human character will join you.
So at this point the players kind of gain the respect of the bassist and I was trying to figure out a good way to represent this through gameplay.
And around this time I had heard Ornette Coleman's Free Jazz, a collective improvisation.
Which is an album by Ornette Coleman where he has one quartet playing in the left side of the speaker and another quartet playing in the right side of the speaker.
And they're all kind of playing collectively.
And I was really inspired by this conversational nature of this playing.
Kind of sounds like a big discussion with their sharing musical ideas, talking back and forth.
And I'll play some here.
You can hear different instruments playing all together, kind of improvising together, like a big kind of shouting match. And this seemed like a perfect way of communicating that the player has gained an understanding of human creativity from this music character.
But now we're back here again. How do we make the computer respond to the player's inputs without machine learning again? But we've learned a lot, so maybe we can implement this kind of conversational duet idea.
in a simple and easy way the first time, instead of trying some stupid stuff for an entire day.
So let's look at the data that we have.
We store the player's inputs as geometric paths in space, which are stored as splines, and we can imagine those splines in a box where the height corresponds to pitch.
And we store one point per frame, and we store both the position of the point and the time since the last point was added.
This allows us to replay the player's inputs both in space and in time.
So we started by kind of randomly chopping up the player's input paths into slices and replaying them.
But the problem here was that if the path was too long, it could pick stuff from many seconds earlier, and the player would have entirely forgotten that they played it, which would lead to a disconnect in what the computer was doing.
So a better solution we found was to only slice the last two to five seconds of the player's inputs to keep what the computer played relevant.
So as the player draws their path, segments are replayed from the latest path data.
But this is kind of generating reiterations of the player's inputs.
What we wanted was the computer to be a bit more creative with responding to the player.
So, since the player's solo is just geometric data, we generate variations using normal geometric transformations.
We can move the curve up or down to shift the pitch.
We can scale it to increase the variance in the pitch.
And rotation looks kind of weird in this context, but if it's just a small segment, the notes will kind of start out the same and then gradually diverge, which you can see in this looping section here.
So if we combine this idea with our smarter path selection, it looks like this.
To add a sense of progression, we can increase the variance in these transforms, how much they randomize, the longer the player plays with the interaction.
And we can also move these points around in the X and Y axes to have some more visual separation without affecting the pitch.
So let's check this out in action.
So this geometric approach works really well because it always maintains the player's timing and rhythm, where if you play short lines, the computer will play short lines.
If you play longer held notes, the computer will play longer held notes.
And while the pitch is changed, the feeling is maintained.
And at some point, this kind of all stops mattering anyway, and the player is just kind of vibing in this moment.
So the last important thing we do is that we don't take this interaction away.
One of our core design principles was to never take away the player's toys.
If we have something fun that they're playing with, we don't want to have them accidentally click something that will take this moment away.
So the trigger to proceed here is this train where dragging it to the right will progress the story.
And we indicate that it will do this by gradually fading in the ending chord to the song.
And also these city sounds as the train is dragged.
This tells the player that if they keep dragging, the interaction will end.
And this avoids that situation of the player clicking something by mistake when they don't want to continue yet.
So with all this in mind, let's watch the whole interaction from the beginning and I'll shut up.
What screen are you looking at?
Top right corner.
The Ancients Mercedes-Benz PS4 Adidas Porsche Volkswagen Hyundai Ferrari Nissan Nissan (*donk*) (*donk*) (*donk*) Shh.
So, I'll end with showing some feedback from some players.
We had released the Jazz Level as a demo pretty early on in development.
And one of the functionalities of the demo was an email feedback form.
So it was really cool to see people enjoying the interaction.
But some of the coolest messages we got were from educators who were inspired by what we'd made.
And I can't find it, but we did get an email from a teacher asking if we could add some features so they could use the interaction for teaching their students.
Stuff like being able to jump to any part of the three stages of the interaction and stuff like that, which I think is something that we'll add in a future update.
But this really kind of highlights the potential of making a game not just with a jazz soundtrack, but making it about playing jazz music.
And I feel like this similar kind of concept could apply to any other kind of music as well.
So I'll conclude with...
Thanking huge thanks to Skillbar, our music and sound design team, for being just outstanding partners in providing me with so many good sounds and ideas for the interaction.
I can't imagine building this without them.
So thanks, thanks for checking out the talk.
You can find more about the game at genesisnoirgame.com.
It's on Steam for Windows and Mac, Xbox and Switch.
You can reach me at jeremy.ferrocatden.com at my email address or on Twitter as jablesjables.
So thanks again and take care.
Bye.
