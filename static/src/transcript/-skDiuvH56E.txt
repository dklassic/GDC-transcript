Hello, welcome to my talk.
It's titled, It Takes Two to Tango, Integrating UX Research and Production at EA.
So this presentation is about how we learn to dance together.
One dancer is production.
The other dancer is research.
When you start learning how to dance, you're not good at it.
You step into toes, ones go to one side, the other goes to the other side.
The two dancers are going to make mistakes.
Like in dancing, you're constantly moving.
Like in development games too, you're constantly producing, iterating, you're always moving.
There is no time to stop, keep things coming.
And it takes practice to get good at it.
So, how these two dancers met.
User experience has been getting a lot of traction during the last couple of years within the game industry.
You could say that it's one of the newest aspects within game development.
For instance, here we are, GDC.
It is 29th iteration, and this is the first time there is a UX summit.
That alone is very telling.
Yet, UX as a concept was coined in the 90s.
by Norman.
It actually, history of testing usability goes all the way back to the 40s.
So there has been a long history of measuring and assessing user experience in a variety of products, particularly in games.
You have heard about playtesting for a long time.
However, such concept has been used in a variety of ways and generally without the framework of research.
And I'll dive more in depth about this throughout the presentation.
Nowadays, every major developer has a dedicated UX research staff.
I surveyed these numbers mid-2016 and I sorted them out alphabetically.
Notice that some of them are quite sizable.
It could be the equivalent for a whole company or to a development team in itself.
And moreover, as our understanding of player experience matures, multiple disciplines are consolidating within larger departments.
And this is the case for Electronic Arts, Riot, and Ubisoft, where games user research, analytics, market research, and data science are part of an internal larger organization.
However, during the advent of UX research into game development, production might have been seeing research as a disruption to process that they have already established.
While UX researchers were still adapting their methods and their vocabulary for game development contexts.
And that interruption took many shapes, from production not seeing the need for research, to production wanting, but not knowing how to action on those findings.
Mind you, it was also for research to know how to convey those UX findings in a timely manner, and how to communicate it in such a way that were meaningful and actionable.
And this is part of the story I'm going to tell you today.
When UX research are designed and not fully synchronized, it leads to missing UX opportunities.
And opportunities doesn't mean that they have to be a major overhaul.
Opportunities are also within production's limitations of time, budget, scope, and technology.
And why I'm telling you all this?
Well, my name is Veronica Zamito.
I'm the lead senior UX researcher at Electronic Arts.
And over the last seven years, I've had the opportunity of working on amazing games of the EA portfolio.
And today I'm going to be sharing with you the stories on how we tackle UX research and production integration at EA.
Spoiler alert.
This is how we are now.
This is what happens at the end of the story.
UX Research at EA is a sizable team.
We're geographically distributed across eight locations worldwide.
And at EA, we have three prototypical tasks.
We have researchers, recruiters, and lab technicians.
Depending on the size of the company, all those roles can be carried out by different people or by a single person.
The latter happens more often in smaller companies or at early stages of UX research teams.
And that was actually my case.
Back in the day when I started UX research at EA, I had to do my own recruitment while designing the sessions and setting up the lab.
That was a lot of struggling.
So imagine that each task takes time away from the other one.
And it's not that they have to be done in sequential order.
So it's quite exhausting and it's actually not efficient.
Now with SIDA, we do have certain philosophies that actually define and help the way that we want to work.
So we aspire to work as one team across multiple studios.
And actually, what's at the forefront of our job is players, so it's players first.
And all these things might sound great.
It's actually great.
But it wasn't with many bumps on the road and misalignments at first until we got there.
So how did we get there?
Well, first and foremost, we didn't get that overnight.
It was a multi-year journey.
And sincerely, it's a path that will never end.
As the UX practice, the industry, and products evolve, we will have to evolve as part of that.
And in this talk, I'll share those examples, those lessons learned that shape, thriving, and integrated the UX practice.
This is a journey with key milestones, and I'm going to go through those.
It starts rough, and it starts with a wake-up call.
Back in 2009, the NBA elite was being developed.
Improving UX was a huge driver for that team.
There were high expectations.
They were introducing a brand new set of controls, mechanics new to EVO.
It was meant to be a reboot of the NBA Live franchise.
And consequently, the game team was very self-aware of the quality of the UX data and how actionable it was.
At the time, playtesting was part of development process.
However, it was led by designers and producers who self-taught themselves how to do usability.
And even though they have the best intentions in the world, they could smell that something could be better, something could be done differently.
And that was the seed of starting from transforming play testing into UX research.
In 2010, we started initiatives for new approaches to evaluate NBA with players and bringing the edge of science.
For that, I went super tech and I brought advanced techniques that it was the first time doing that for play testing.
So we're using eye tracking and biometrics and telemetry and all of those at once to assess visual attention, emotional valence, tracking their in-game behaviors.
It was awesome.
The idea behind that was to tap into insights that surveys.
couldn't get.
And the effort was to give the most we could to NBA for understanding the player experience.
There was a particular fascinating finding that players consistently looked at the coach after making a basket.
And bear in mind, the coach was there in the background doing nothing.
No animation, no voiceovers, anything that could lead to him.
So it was very unexpected and pointed out a missed opportunity for positive reinforcement.
That insight never appeared with any other technique.
I was getting insights that were completely new on that approach and all this marvelous work.
was new, so the process was a little bit slow, and actually, those findings couldn't be done.
There was no automation.
It was very labor-intensive.
So findings like those needed to be left for discussion for a next installment of NBA.
Time was passing by, and the team needed to focus on finishing the game.
And NBA Elite was coming hot.
As it's common, there was a demo scheduled and released to the public.
And I'm going to show you next some snippets from a video that went viral of a player playing the demo.
One thing I didn't like, they make you play in the up and down view.
I really don't like that fucking up and down view.
Oh, I can change it?
We'll change that shit.
Broadcast, there we go.
I didn't even know you could change it, I just noticed that.
You did?
What the fuck is going on?
Hold up man, there's something wrong with my goddamn buttons.
This shit is crazy.
I didn't have it on my lead, that's what it was.
Alright, let's go.
The response time to the buttons is slow as fuck.
What's that about?
These controls are crazy. You gotta get used to these controls.
This is crazy, man. I haven't played live in so long.
What the fuck?
Yo, y'all seeing this shit?
Yo, what the fuck?
This nigga just standing like Jesus in the middle of the court.
Oh my God, man. That's it for the demo, man.
I can't take no more of this shit, man.
It's hard these times, man.
Deuces, man. Deuces.
That was a funny video, but...
There were obviously friction points in the UI.
There were friction points in the mechanic.
And this demo actually was a nail in the coffin.
NBA Elite never ended up being released.
It was deemed that it didn't reach the desired quality, that players would be disappointed, and that the best decision was to not release it at all.
It was an extremely hard decision, and I cannot stress enough how much of a shock it was for everyone.
It was a wake-up call.
As a video game company, it was very clear that quality process needed to be better.
And I wouldn't go as far as saying, oh, better UX research.
We have been the silver bullet for this.
There were other problems.
But definitely, clear and more iterative UX checkpoints needed to be there.
With that, the new UX research initiatives were a business case to bring to the next level.
And there was work to be done.
We needed to improve the research practice from its foundations.
Even though biometrics techniques proved to be very insightful, we needed to change focus from advancing methodologies to establishing a UX process.
We needed quality research in an iterative process where production and research are fully synchronized.
That means having a plan, knowing your steps.
We tackled this problem from three different angles.
Strengthening the good research practice from a macro as well as from a micro level of research and production integration.
Regarding the basic good research practice.
There were imminent aspects to address, such as quality data, things like ensuring wording of questions in service are clean and not leading.
These efforts were towards researcher skills, and the goal was to have quality data.
So we have been certain that the research findings are good, and that also leads to trust.
Trust from production, it's key.
They don't have to double check the data, they don't feel they have to go themselves.
The entrust is the cornerstone for any relationship.
But ultimately, doing research is to communicate those findings.
And like in tango, communication has to be clear and timely because you're constantly moving.
You don't want your partner to go that way and you that way or accidentally step into each other's toes.
So I'll show you one example on this topic.
And you'll have seen research data coming in this form of prototypical question and a scale from one to five, all player data aggregated and presented in a bar chart like this, which is okay to know how good or bad things are.
However, it's not sufficient if you want to prioritize resources in production.
In cases when you have to look at multiple variables and do comparisons.
This is the case in Plants vs. Zombies Garden Warfare, where there were more than 40 characters that you have to choose from.
And productions wanted to ensure that all of them were hitting a mark, and if not, focus on those first.
And that was exactly the question that we had, which was great.
And looking to communicate findings, we look at different ways.
We put some tables, so now you can really see the spread of responses.
And even though the game team was happy with this, you can start to see that it's getting too hard to read.
And when you have 40 columns of those, you cannot see through that clutter.
So...
That's why confidence intervals were becoming a part of integral practice of research and how to communicate findings.
Confidence intervals are going to give you a way to represent that variability from players' responses, which allows to spot meaningful differences. You keep it simple, yet with a richer insight. So in that case...
It says that the target score was 3.5 and the option in orange is coming tight.
Just hitting the mark, could be better, sure, but it's coming at least where we want it.
Whereas the blue option, you can see it was actually looking at it first glance stronger, but actually it was inconsistent.
They were polarized options.
It was a bigger risk.
And gaining prioritization from production was at essence.
And that's how confidence intervals now are part of a basic research practice of doing research.
Now, at a macro level, at EA we have a game development framework, which is all the games developed at EA have to follow.
I have a simplified way here.
Which is pretty much all the different stages common in the game industry, pre-production, production, and release.
So it's applicable to everyone in the room.
The value of stopping and getting intimately familiar with the dev process is that we can really understand from research where the efforts are at different stages, what the biggest challenges are, what are they being asked by executives.
All those challenges are the clear information that needs to be common.
Otherwise, it leads to misconceptions about readiness, priority, and also vice versa, how research activities are aligned to the best impact for those UX efforts.
So I add some of those prototypical tasks in the chart.
But ultimately, that map...
It's a map.
That framework is a map that UX and research need to navigate together.
So that's what we did.
We tried to find that common language.
So for UX research at the A, in order to be part of the development process, we need to articulate how we were bringing value to the table.
And we work on an expansion of that framework, laying out all our prototypical tasks and endeavors on those stages.
And from there, we were able to work with production to further tailor their questions on a specific game characteristics and design intentions.
For example, in Battlefield 1, a key design intention was being epic.
We make it explicit that we were supporting that feeling of epicness throughout the whole development process.
Being a first-person shooter in World War I meant that, for instance, weapons had to feel old-school yet authentic, but not to the detriment of slowing down players' actions, which we did there.
And another example was the introduction of large vehicles, like the Zeppelin.
We need to answer those questions related to the impact of introducing those behemoths, those vehicles, during a full match, during our playtesting sessions.
Now, we have a roadmap, we know what we're doing.
All that is key.
Now we need to execute it.
So let's look at the micro level of how we work.
And I'm referring to this of what happens in each and single study.
And a default test can be divided into four big steps.
Preparation, execution, analysis, and reporting.
It informs production, action on those findings, and then we go to the next iteration to the next test.
We were doing fairly good with production in the kickoff meetings.
Everyone was attending.
It was going great during the execution of the sessions.
Production was coming to the observation room.
They were doing the test.
Now, the interesting part of the story here was between analysis and reporting.
And we were experiencing a couple of errors during that time.
At first, we were so eager to give them the best research ever that one of the deliverables for the final report took two weeks from final session to here, report.
And that was detrimental.
We forgot that we kept moving, that our partner was going over there dancing.
And by the time that they got the results, they had made changes.
A lot of those findings were obsolete, dying inside.
Those applications were gone.
Something salvaged it, but it was very clear that we were not aligning with their pacing.
It was also generating anxiety.
From the day of the test to getting results, they knew that we collected data, and they wanted to know how we were doing, what unnecessary settings for the dev team.
We went to the other extreme on the spectrum, and we tried to be very, very, very, very quick and try to deliver it within two days.
And analysis was hurt.
It was more like a data dump.
Not proud of that work.
So we need to find that middle ground.
We need to do something to give good data, good analysis, in a timely manner.
And that's when we introduced what was a top liner in our process, which is between.
Just 24 hours after the last session, production receives this top liner which contains the high-level analysis of the test, and then a final report two to four days later with the full scope.
And that helped greatly to our communication in providing information in a timely manner and keep moving forward.
It helped to the relationship as well as addressing their needs.
So now that we have a UX process laid out, I want to focus on organizational structure for UX research.
And with this, I'm referring to where within the company should UX research live.
And organizational structures have implications on relationships, visibility for the product, and how you're going to be prioritizing content.
At EA, we had a centralized UX research organization.
We engaged with multiple game teams across all EA.
And as you know, the EA portfolio is pretty large.
So in other words, we have a single team with just a handful of researchers carrying out all the research activities for the whole organization.
This means that the researcher was A, no part of the development team, and that the researcher was supporting multiple franchises.
For instance, someone would be working on FIFA, as well as UFC and NHL.
This type of organizational model was great for us at the time.
Remember that wake-up call?
We need a good sense of processes.
So this organizational model was great for that.
Why?
There are like four key positive aspects for this.
For once, it creates a really high, strong hub of experts.
This allows a lot of easy sharing of best practices, reinforcing that quality, which was very important to mature our practice.
The range of projects was varied, so it kept entertaining and refreshing, which for retaining talent long term can be important.
The accumulation of knowledge across the different products allow us to have meta insights.
If we put all our shooters together, all our sports together, we can answer bigger business questions for this.
And ultimately, it has an economical benefit too, because central teams can share those resources.
You don't need to build like two labs, three labs.
You can license products.
You can have your recruiter, your lab technician that serves everyone.
So that was great for us.
And with a handful of people, we need to cover a lot of games.
And that's where some challenges are.
We have to rationalize efforts.
We have one person with three titles, three big titles.
Well, we covered them all, but it's still one person.
And that leads to a constant reprioritization exercise of the portfolio, which can be tricky, because it has implication of what's important, what's not important, what we should tackle.
And you don't need a big portfolio.
I'm pretty sure that all you can relate to this within a single game.
You have different features, you have different modes.
Something has to give attention, something will not have attention.
And another shortcoming of this structure is the relationship with production, which tends to feel like it's more at arm length.
The development team can perceive the research as external, and the research are feeding out of the loop from production.
Sometimes the game changes a lot within a week.
So, what we do.
At EA, the research efforts and the improvements that we're doing were paying off, and we had great support.
It enabled to actually...
Oop, it's missing something, but it's a letter.
It was missing... no, there was animation, you saw it.
There it is. It was there.
So, we were very lucky that we could actually increase the size of the team.
And we could actually now be having people more dedicated to certain franchises and not dividing them.
So it was one person for FIFA, one person for Battlefront.
And that was increasing fantastic for our bandwidth.
But getting more headcount for more researchers is not a light task.
As in any organizational, for any role, it's something that just doesn't grow on trees.
I think about evaluating what a decentralized model will look like, and where people are independent from each other.
So however we concluded that the slower pacing of our practice didn't really trump having that close, close, close relationship with the team at the time.
And something that was huge for us is that the lack of having comparable results was a huge, huge risk that we could not afford.
It was almost like calling for another wake-up call.
So we wanted to keep the quality.
We wanted people to know how to dance properly, not just shaking their body in some way.
Not like this.
Thanks, Ian.
So by now, we did have a better, established, stronger relationship with the production.
We identified champions within the game teams.
And we work with them to iron the process.
So things have improved a lot.
But what happened for us was keeping each other in contact with our partner.
And one day, one of the key point of contacts from NHL moved to another project.
And that left a void into our synchronization.
And instead of waiting for the NHL team to come up with a backup field for that position, we stepped up our game and we decided to propose to go embedded with our researcher.
In other words, that the researcher will be working now on the floor with that dev team, being reintroduced as a team member rather than a partner.
And the NHL team was very supportive of this approach.
And it was awesome.
It really helped us to improve to a new deep level of those relationships.
We had that extra time and dedication with the team now.
So we could actually now answer more questions.
There were more people from production asking more and more to the researcher.
And it was also an opportunity for the researcher to keep developing those skills.
So because we have that strong foundation now, we could refocus on those advanced technologies and bring them back to the team.
In that case, we were focusing a lot on UI and accessibility to blind color players that we could go back to now keep advancing the field.
The one risk of going hybrid is that now that researcher becoming an island.
And that's a delicate because it's an island from the other researchers and actually how to allow tailoring for certain practices but yet being aligned with the overall process.
So we're working on that in a variety of different ways from our end and keeping that tightness of the researchers by allowing to have internal projects to work on.
Things that I can connect it and to can actually, can keep advancing our practice over and over.
Now, what I've told you so far is related to the grassroots efforts.
A lot of that change was from UX from the bottom up.
But really, to change culture within a company, it also needs to come from the top down.
There needs to be management buying into UX.
And at EA, we were all doing our dancing steps, but you truly dance when you're going with the music.
And the director of the orchestra is a key piece to keep us all together and dancing.
You need leaders in your organization to believe in UX and to support those efforts.
So in 2013, when Andrew Wilson became the CEO of EA, as a new leader in the company, he set a series of pillars for guiding EA.
And the most important ones was player first.
When I heard that, I was like, woohoo, I was over the moon with that.
Because he was really talking about that nothing is more important than our players.
That user-centered design is the way that we work.
that is player first.
And nowadays there is no discussion how good a game is without talking about player's insights.
With that was the whole journey of EAF-UX so far.
And I just wanted to recap on those shared learnings from today.
have a shared framework.
You need a good research practice that is effective and communicates clearly, but it sets a map to relate to each other for those steps at the macro and micro level.
So make sure that your findings are actionable and timely all the time.
Constant improvement.
Feel how your partner is doing and adjust accordingly.
Leverage on opportunities.
Find an organizational model that fits your needs.
And continue reviewing your deliverables and tech as you go through.
And ultimately, it has to be a UX culture.
And it needs both dancers and the music.
And to have those, is go from the grassroots, but it also need to find your champions.
Thank you.
