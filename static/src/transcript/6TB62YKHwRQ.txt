All right.
Hi, everybody.
So a little bit of housekeeping before we get started.
Cell phones to quiet.
Whoa, that's really loud.
And don't forget the speaker evaluations at the end.
That feedback's super helpful.
And thanks so much for coming.
My name's Simon Unger.
I'm an animator.
I've been animating games since about 2001.
I was lead animator at EA, Square Enix, Robotoki, and I'm now working at Phoenix Labs.
I also teach animation. I've taught at schools such as Nomen, iAnimate, Vancouver Film School.
Phoenix Labs, for those of you who haven't heard of us, we are an indie developer based out of Vancouver, B.C. We're a team of about 40 industry veterans working on our first original IP, Dauntless.
So these are a few of the teams that our company has worked in the past, the employees, sorry.
As you can see, we have a diverse and experienced crew who aren't strangers to big AAA titles and big teams.
Games like the Mass Effect franchise, League of Legends, Dead Rising, and Gears of War are just a few of the projects we've worked on.
Most of the team have spent the majority of their careers on big teams and projects.
Our motivation for going indie.
and remaining smaller came from the desire to eliminate many of the dynamics involved in larger teams that tend to slow down agile decision-making.
It lets our senior team focus on what's really important, which is making games.
So Phoenix Labs was started with the idea that if you assemble a small experienced group of people with great intentions, empower them to solve their own problems, they can make any project they throw themselves at great.
So what is Dauntless? It's a co-op action RPG set in a science fantasy universe. Four players band together to slay behemoths that threaten the survival of their fractured world. To give you guys a better idea of what that's all about, here's our announced trailer.
you R.I.P.
So incidentally, that trailer was made in-house with just a handful of the team, while we were also working towards a pretty major pre-alpha milestone.
So it's a good example of how we're able to produce a lot of content quickly, and at a quality bar usually reserved for much bigger teams with larger budgets.
We've got some lofty goals for Dauntless and for the studio, and no small amount of challenges to overcome.
in my talk today, so I'm gonna cover some of the ways we've found to make our work more efficient and more fun, and just our general mindset when we approach these challenges.
So what are we up against?
As I mentioned, we wanna keep the studio relatively small, no more than 30 to 40 people.
We wanted to challenge the convention of overcomplicated systems and processes, and instead work with less overhead, more empowerment, and less management.
Our animation team consists of three animators.
When we break down the scope of the game, that leaves one animator for player, one animator for behemoths, and another one to oversee the dialogue, NPCs, and narrative. We needed to avoid wasteful practices as well as optimize each person's contributions to the team. One or two people being blocked on a 100-plus person team won't really derail a milestone, but could have pretty bad consequences on a team our size and at the pace we move.
We all came from big AAA studios and projects, and didn't want to sacrifice the quality we're used to delivering, nor the scope of the project we wanted to take on.
We needed to find a way to deliver the experience we're after with the team and the budget we wanted to work with.
Specific to animation, the challenge was how do we produce a high volume of animation at a quality we're happy with, and ensure the animations we do produce are getting the most mileage in game.
We also wanted to create an appealing, stylized universe.
The easy choice would have been to choose realism and just charge head first into the uncanny valley.
Photo-based textures, unedited motion capture, default lighting, 3D scan props and characters would have allowed us to create a lot of content quickly and with little direction to maintain continuity.
For us, that didn't feel like an exciting process.
proposition to work on and not really an appealing choice for the universe we wanted to create.
But as many of you likely know, creating stylized, polished animation is pretty labor intensive and polished time is not a luxury many teams get, especially smaller teams, on their first try.
And core to our game are behemoth battles.
Boss encounters are hard enough to do to get right on their own.
And on top of that, animating believable beasts is a challenging and time-consuming task for animation.
There's a very good reason few games choose to make creatures a focal point.
It's just really tricky to do well.
So how do we approach all of this in a way that delivers something we're happy with without a giant team, massive budget, or unlimited time?
For animation, turnaround time at an acceptable quality was one of our most important problems to solve.
We didn't want to become a bottleneck for design, VFX, or audio, so creating and iterating animations quickly with as little effort as possible was paramount.
Keyframing animations from scratch at the fidelity we're after is an expensive proposition, especially when you factor in the cost of the many revisions we typically take before finding the right mix of timing, coverage, and appeal for gameplay purposes.
So what do we do?
We decided we would make use of motion capture as much as we could.
Luckily in Vancouver, we've got a few good options of mocap vendors nearby, but shooting externally can be expensive and not very conducive to iteration for a small studio.
Early on we determined that having access to our own motion capture space was gonna be pretty vital.
There were often times we would need only to prove out a small, very small aspect of gameplay and iterate on just a handful of moves.
Booking a shoot at a local studio, casting an actor, craft services, and so on, would have been way too expensive in both time and money, and ultimately work against the point of wanting to use MoCap in the first place.
One of the great aspects of being as agile as we are, making a decision like this and seeing the results becomes a really tight loop for us.
From when we decided we should look into getting a MoCap system and building our own and starting shooting was really only about two months.
So now we had to figure out which system to get.
Some of us on the team have had a lot of experience with various capture systems, mainly Vicon, which for a long time has been the standard.
But we wanted to give as many of the currently available options as we could find a chance.
Some of the main factors that mattered to us when considering a system was ease of use.
We couldn't have extra head count to run it.
It had to be easy enough that one or two people on the team could shoot with as little setup time as possible.
Scalability.
Shooting more than one actor at a time, capturing rigid bodies or props, or maybe even doing facial capture.
We wanted a system that could grow with our needs and upgrade easily in the future without having to retool the entire thing.
Access to support.
While we had a ton of experience working on mocap data and coordinating and directing mocap shoots, we didn't have very much experience solving and tracking data, setting up a mocap space.
moquette volume, sorry, from scratch.
Having instruction and using the system and the support line when things get a little confusing was pretty important to us.
And of course, cost.
We're working on a budget, and getting the most mileage for our money was a top concern.
And lastly, data quality.
Getting clean, easy to work with data was going to be crucial.
If it was going to take a lot of time for us to clean that up or imprecise capture data, we'd just lose a large part of the draw of using this productivity tool.
So we looked at those criteria and weighed some of the currently available solutions against them.
So these are the vendors we chose to evaluate.
There are others out there, for sure.
But we wanted to start with ones we'd either had experience with ourself, or we could get feedback from people we know and trust, or at the very least, had been used in a production environment.
We talked to most of them, researched ones that didn't respond to our emails, and because we're a little crazy about our data, we made a decision matrix to weigh them all against each other, as well as our criteria.
So which did we choose?
We ended up getting an OptiTrack system from Natural Point.
We've got 16 of their Prime 13 cameras on tripods, and in our current space, we're able to get about a 30 by 30 foot volume, and we've been really happy with how it's worked out.
We chose it because it gives us the best cross-section of data quality, ease of use, portability, price.
Our costs for all the cameras, tripods, suits, crash mats, everything, ended up being only about the same as a couple of shoots at an external vendor.
And so far on Dauntless, we've had about 70 shoots.
So it's fair to say the system's paid for itself several times over.
Excuse me.
Setup was surprisingly easy.
The time it took from the initial unboxing to our first capture was only about two and a half hours.
Mobility is also good for a trailer.
We rented out a larger office space in our building.
This is us doing it right there.
And we were able to get about a 25 by 50 foot capture volume to get a little more running room in.
It only took a couple of hours for us to break it down, move it downstairs, and set it back up again and be shooting.
And we only needed it for a day, so the mobility was really, really great for that.
It also satisfies our easy to use criteria.
One person can set it up, put on a suit, and capture themselves, and then solve and retarget the data onto the gameplay rig.
I've been able to do a small shoot on my own, about a dozen moves or so, and be back at my desk in under 30 minutes.
This allows us to talk about some new gameplay ideas in the morning and be playtesting them in the game before the end of the day.
This type of iterations allowed us to try new ideas quickly and find the fun in the gameplay without being a slave to labor-intensive content creation and long iteration times, which, as many know, is often the enemy of quality in game development.
You definitely don't need to jump into fully the same equipment we have.
There are a lot of different setups out there to choose from, could fit a wide range of budgets.
I should also say this isn't a paid endorsement of OptiTrack.
Although if anybody is here from OptiTrack, I wouldn't mind some more cameras.
We've just been really happy with the equipment and support they've shown us.
So as I mentioned in the previous slide.
A lot of what hinders good content creation becoming great is iteration. So often the first idea isn't the best one, but if you've invested a ton of time into it, or your ability to iterate on it is cumbersome, you're going to be a little reluctant to make changes, and even if they're in the best interest of the product. Creating biped animations using mocap is a fairly trivial way to prototype, but creatures are a trickier matter. As most animators know, nonhuman characters are a time-consuming thing to animate by hand.
In an effort to minimize possible throwaway work, we started prototyping creature gameplay using stepped or linear interpolation and only a few key poses to communicate the intent of the move.
Within only a few hours, we can have a handful of attacks and locomotion, moves in engine for the designers to create prototype encounters with and get a feel for pacing and staging.
They're also free to scale, chop up, repurpose these placeholder atoms until they get something that feels close enough to what they had in mind.
After that, we can re-export these as FBX.
FBX is to give us a rough framework to do a second pass on.
This way, the animators can be confident that all these moves have been vetted in-engine many, many times and play tested repeatedly, so we're not going in blind.
Working this way has greatly reduced the impact of revision work for us.
And we continue to update these animations, adding in-betweens and ultimately moving to final polish, always play testing at each iteration and getting team feedback along the way.
At no point do we ever put a polished animation in-game that hasn't been play tested many, many, many times.
Now this didn't come naturally for everyone on the team.
It took quite some time for the non-animators on the team to be able to see past step-keyed animations and understand what the end goal was.
I think this is really only possible after having worked through this a couple of times with the team.
If you want to work this way, definitely expect some growing pains and complaints about animation quality or frame rate for a while, but it's definitely helped us speed up.
So it was also important to get the most mileage out of these animations that we put into the game.
We wanted to create a lot of diverse behemoth encounters.
And spending all that time creating awesome animations would be a bit of a waste if they weren't being seen or used much.
We made the decision to create some high-level categories in our game called a genus, we call it, to slot these different creature types into, mainly based on their physiology.
Some examples of a genus would be here on the screen like a mixed flyer or an agile quadruped or a theropod.
Each variant in the genus shares the same skeleton type and rig.
We can reuse and retarget animations within the same skeletal structure and change or replace geometry, color, size, effects, audio, and create unique encounters with often no new animation assets.
We can even make small anatomy changes within a genus.
such as adding or removing tails or wings, and even changing leg types, like from a reverse knee to a planter leg.
Some variations required a few new animations, some a little more.
But this really only adds to the asset pool that Genus can use, and all the possible variations that we make in the future.
Basically just adding more LEGO pieces to our collection.
Here's an example of a behemoth Pangar with a variant Nasher.
By changing only a couple of animations, adjusting behavior and stats, as well as changing some play rates on base animations, we can create a unique and engaging encounter with much less work than creating a new behemoth from scratch.
It was also critical for us to solve complex animation problems with as few assets as possible.
Much like our approach to animation, our environment team has a pipeline that allows them to create massive variety of playable terrain quickly. These can contain multiple biomes, sub-biomes, obstacles, complex nab meshes to locomote around. Encountering a behemoth could happen on a slope, in a valley, next to a cliff, or even in a forest.
A traditional animation system would require us to create a massive number of locomotion and combat animations to handle turns and slopes and leans and collision volumes.
Again, we didn't want any solutions that involved just throwing people or assets at.
We wanted something that was a bit more elegant, more tunable, and allows us to use as few animations as possible.
So as an example, here is a behemoth traversing a curved path on a fairly complex slope.
It's a lot to ask for a single animation to solve, and a pretty common problem for games.
To make this trickier, he's quite large, and he's a longer creature with a tail navigating around multiple obstacles while traversing this slope.
We could choose to solve this with blend spaces and a bunch of animations for coverage, but we really felt like there was a better way to approach this.
We ended up using a full body IK solution, and we chose to integrate an off-the-shelf package called Ikinema.
And using these IK layers, we were able to do a lot with only just this single walk cycle.
So here's an example of that same walk cycle, same path, nothing's changed, with Ikinema turned on.
conforming to the path, foot planting, and a look at to lead the head into the turns are all working together to help turn just this same walk cycle into something that's a lot more versatile.
We also use this for hit reactions, aiming attacks and appendages, especially when they're on uneven terrain or a slope, such as when he does a tail attack on a slope.
It takes care of that and keeps the alignment.
So even with these measures and many others, we still have a really tight timeline and a lot of content to create, especially when we go live.
So we did have to outsource some animation work.
We were looking to do development better than we had in the past.
And how we approached outsourcing was no different.
Having a lot of experience with traditional outsourcing business model, we knew we didn't want to work that way for animation.
Outsourcing typically has a point of diminishing return.
The usual high in-studio overhead with pre-planning, managing, directing, and maintaining dictates that there's got to be a big enough bulk of work to send out.
We knew we weren't going to have hundreds of animations to send out at a time, and we couldn't sacrifice a lot of in-house cycles to overseeing animation outsourced work.
So we wanted to find a partner rather than a vendor.
a group who worked more as an extension of our team and collaborated with us rather than just completing heavily directed work.
We were super fortunate that we didn't have to search long for such a partner.
We teamed up with Steamroller Studios, whose creative director happens to be a friend, Jalil Sadul.
They're a group of amazing people based out of Florida, and they brought a ton of film and creature experience to the table that ended up being a really great match for our gameplay experience.
Instead of a faceless group of contractors in a far off land, we got new teammates who were invested in the quality of our product and could collaborate on ideas with us, help with simple blocking of test animations, polishing animation we'd already blocked out in-house, or just ideating on what a behemoth attack could be, were just a few of the ways that this partnership was a massive help to us.
Being able to hand off animations with minimal direction and knowing that they were reviewing and checking the quality of their work before we ever even saw it.
It was just like having a team of experienced animators in the studio with us.
Having outsourced several times, this was definitely a breath of fresh air for me, and something I'd always wanted an outsourcing partnership to look like.
So those are some of the high-level solutions.
But I also wanted to share some of the practical workflows and techniques we've been using along the way as well.
So one of the bigger time savers for me was the ability to use mocap for more than just human characters.
I've been wanting to try this for some time.
And I felt strongly that it had a lot of potential to be a potent productivity tool for us.
But sadly, my first attempts at capturing my movements and retargeting them to a creature didn't go as well as I had hoped.
I started by characterizing the creature's proxy model, essentially defining the skeleton for it in MotionBuilder and working with the solving parameters to get the data to fit the anatomy and default posture of the creature.
Maybe with a little more careful setup and fiddling with parameters, I could get a little closer to something that was usable, but from this vantage point, it was a little more comical than practical.
So the problem was I was trying to do this how we always solve mocap, by remapping on a joint level to the final character.
While there's definitely more elegant ways to retarget data to wildly different anatomies, using more complex IK solving, for example, the end goal was I just wanted a faster way to define weight, posing, and timing on my character, not necessarily a precise performance transfer.
It also needed to be done without having to write a new pipeline or involve a team of people to help me out with that.
So instead of forcing the anatomy to match, I chose to think of captured performance as a series of movements on centers of masses.
So in this way, I can sculpt the data to work with these relative masses, instead of trying to force joints to match on disparate anatomies, I just translated the performance into these collections of areas of movement.
I could sculpt these to work with different starting positions and orientations, and if a part of the capture didn't work with me, I could just chuck it out and key it myself pretty quickly.
My first attempt at this gave significantly better results than the first.
So this is literally my first animation I tried this on.
I found the scene.
It's a simple test, I know, but this actually only took me about 30 minutes to get here, and that included solving the mocap data.
It'd probably take an animator a considerably longer time to get to this same place, and I'm starting with higher fidelity base to work from when finishing this versus blocking.
So with just a little bit more work, keying the hands and feet, polishing the performance, and adding a little bit of texture, I can get decent results with much less time than it would take to keyframe to this level of detail.
So how did I do this?
I'm sorry to say, it is not a very technically impressive workflow.
I start by bringing in the exported motion capture into MotionBuilder and solving it to our player skeleton.
I've usually fixed any jitters, pops, or weird rotations in Motive, and that's OptiTracks capturing and editing software by this point.
But if there are any other issues, I'll address them here on the control rig before going any further.
I'll also often do a quick retiming pass using the Time Warp tool, as I like it a little bit better than the one in Maya, even though this one can be a bit more finicky.
I have a general idea of what my timing and spacing is going to be, and will get it as close as I can, but still thinking really broad strokes first at this point.
It's not on the final creature model, so it's really tricky to be super accurate here.
And then I export this as an FBX, bring that into Maya, and I import a set of locators I created.
ahead of time that live in a simple hierarchy. I've found that this was important, that these live in a parent-child relationship similar to how the skeleton is set up. And that avoids a lot of problems in the transfer on a lot more complicated physical moves, especially when the behemoth is inverted.
Each of these markers is used to define a center of mass that I want to transfer from the capture.
My main ones I want are hips, torso, head, hands, feet.
I'll usually grab movement from the elbows, knees, shoulders, but only use them if I really need it.
I'll parent the appropriate joint to the exported skeleton in a position that best represents the point of movement I want to represent, and bake that translation and rotation onto the locators.
At this point, I can blow away the skeleton, and I'm left with just the locators, which I'll save out, merge with my behemoth rig, and set my starting pose, and in this case, my idle.
Then just align those locators to the anim layer, on an anim layer, sorry, to the control points I want on the behemoth rig.
I just parent the rig to the locators, bake that translation and rotation, and then blow away the locators for my scene.
Now I'm free to insert my impose, retime the animation, and just start layering on top of this base movement to get the performance I'm after.
So what are the limits of this system?
Obviously, putting movement on a bipedal creature is a little more forgiving than a quadruped.
I can still use the timing and weight of the main center of masses, the hips and the torso.
I also sometimes use props in MoCap to approximate the difference in limb length if needed.
usually just giant styrofoam blocks.
Even just having basic info on a character's fuselage helps speed up the first pass and keying quickly using IK on the arms and legs and just makes getting a detail block out really, really quick.
It does get a little awkward when somebody walks in on me doing this in the mocap room though.
So a big focus of our culture at Phoenix Labs is making sure we're spending our time solving the right problems.
For animation, we try to remove as many pain points in our workflow as possible.
A pain point is something that causes an animator to either lose momentum while working on something or an unnecessarily repetitive task.
Basically anything that takes you out of your flow, as much as I hate that word.
Whether it be the creation or the exporting end, a lot of our processes and tools are built around minimizing these repetitive or time-heavy tasks.
For example.
I found I was spending way more time than I would have liked animating the tail on Behemoth, especially during the initial couple of passes to prove gameplay. This was even more of a problem on complex moves that required the tail to be interacting with the ground plane.
I wondered if there was a way to automate the majority of this using dynamic simulations in Maya. Not being familiar with dynamic sims at all in Maya, this took a little bit of trial and error to figure out the process that produced the results I was after. But here's how I set that up on our rig.
So I first needed to start with my base pose for the tails.
For this, again, I'm using the idle pose.
To avoid any circular dependencies, I need to decouple the simulation from the main character rig.
And because of that, I tend to leave animating the tail like this to the very end.
First, I need the animation for the tail's parent.
So in this case, it's the hips to live on something outside the rig.
I like to use locators for just about everything, so I'll go with that.
All I'm doing is basically just baking the translation and rotation out to a locator from the hip control.
I do this kind of thing a lot, so I've got a little Python script I wrote to make this a single button press or a hotkey.
Then I duplicate the joint chain of the tail and un-parent that from the main skeleton and create a curve that follows the tail with a CV at each joint and make that curve a hair simulation.
Parent the resulting follicle to the locator that we baked with the hip movement on it.
And then setting that curve as a spline IK for the duplicate joint chain.
I can then use this joint chain to drive my FK controllers on the character's rig via orient constraint.
And if the tail is going to be interacting with the ground plane, I'll just create a simple polyplane and use that as a collision object.
Or you can actually set ground plane as collision within the follicle settings as well.
But at that point I can tweak the dynamic properties of the follicle, things like stiffness, dampening, motion drag to make the tail feel like it has the correct weight and mass.
And I also key things like how much attraction to the original curve and how much gravity and friction lives on the ground plane to help sculpt that tail's movement.
It took me a while to figure out which settings in the dynamic properties I needed to adjust to get me close to what I was after.
There are a lot of them in there.
But now I can get away with only a handful of them and it'll get me most of the way there.
When I'm happy with how that's working, I just bake it back down to the rig.
And now I can key over that using layers or exaggerate the base animation just as if it was motion capture.
So here's the kind of results I can get now with about five minutes of setup work.
It's not perfect, but you can see even subtle interactions with the ground.
That could take a while to animate by hand if I didn't use this method.
With even a little more time adjusting and keying the settings on the rig, I can get a lot better results.
When we combine this with our Echinema in Engine, we can be more adventurous with creature designs and adding and removing appendages on our behemoths.
Rather than trying to make safe choices when we design these creatures, we can take a little bit more risk.
So this works great when I need to have the tail just arbitrarily follow the creature, but what if I want to define parts of that movement, such as in an attack or a personality move?
For that, I'm essentially doing the same thing only with a couple of extra steps.
First, I'll rough in the main poses I need the tail to hit on the behemoth rig.
Then I'll duplicate the tail joint chain just like last time, remove any constraints or dependencies, and then duplicate it again so I have two versions of that joint chain.
The first one, I'll constrain to the original tail and bake the animation back to that.
and then remove those constraints. So I'm left with block tail animation on one of the joint chains. I'll create the curve same as last time, but instead I'll skin that curve to the animated joint chain. From here, it's pretty much the same, turning the curve into a follicle, using the dynamic curve to drive the second chain, and then using that second chain to drive the FK rig.
In this method, curve attraction and dampening become the main dynamic property to key, and just make sure I hit the poses I'm after.
This method takes a little longer than the first, but still far less time than hand keying to achieve the same results.
We've since started incorporating this into our rigs and tools, so that's not a manual setup for me each time.
which makes it even faster. I've also used this for smaller things, things like the scales on his back. I don't really have the time to animate each of those by hand. You can see there's a lot of them. But being able to just sim that quickly and bake that back to the rig can save a ton of time. So I know this isn't a revolutionary way of working. I think Naughty Dog showed a similar thing in their tool set quite some time ago. But it could be used for almost anything.
You just want to get some really quick overlapping movement on, even if you just end up using the sim chains for reference and rotoing over it.
We've also used animation dynamics in Engine for obvious things like cloth, of course.
But we also use things like muscle jiggle on behemoths, player armor, or anything else that just might be easier to sim in Engine over top of animation state changes.
So I can't talk about animation pipelines without talking a bit about rigging and tools.
Because we have limited animation tech art support and a myriad of different behemoths and player rigging needs, not to mention everything else the TA does, we needed to find a way to approach rigging in a way that was efficient and reusable.
So we have a Python-focused tool set for creating and animating behemoth rigs.
Because our behemoths have a variety of proportions and variation that also needed to be expanded upon in an agile way, the rigging toolkit uses a module definition system to break up the workload of rigging the different parts of a behemoth.
This allows for a rule set to be applied to many behemoths based on the parts that they have.
If a new part or functionality needs to be added later, a new module can be written and added to the system.
The animation tool set hooks into these modules to search out what information it needs to affect those parts.
An example of this would be the IKFK switching tool knows about what modules it's looking for, and therefore what attributes of that module it's looking for instead of being name-based or hard-coded into the rig.
So these are examples of modules for different parts of the rig.
Some attributes are used during the creation process, and some are used by the animation tools.
Limbs get defined in an interactive way during a placement phase, and then committed during a creation phase.
This was generally successful for us.
It could be greatly expanded upon in the future for better create, to better create a definition of the rig as a whole.
Also, when a skeleton is already predefined, the modules needed to pass it, hooking up the joints it cares about, and that could be better automated as well.
We'd also like the tool set just to have a better friend and a wizard, so it's better for people other than just tech artists to use.
And also, because the physiology between the behemoths is dramatically different, this led us to some different needs that you might not think about.
The center of mass was a big one for us.
Generally, we may allow for different pivot points for the whole body and the rig to make it easier to animate, but when procedurally moving characters in the engine through Echinema, we needed to define a hard center of mass over the root and the skeleton.
For quads and bipeds, this is sometimes obvious, but for our biped quadruped hybrid on the right, it led us to think about where that behemoth would be pivoting from during its movement most of the time and create a chest-based center of mass.
This was contradictory to the bind pose and the rigging tool kit needed to be able to take this into account.
And finally, just quality of life improvements.
As mentioned earlier, removing as many of the repetitive and time-consuming tasks as we can helps to speed up iteration times and make the work more enjoyable.
And that leads to higher quality overall.
Some examples of this are automating mocap pipelines, tools to help with multiple frame range exports from a single file, IKFK switching, pivot point changes for any control mid-animation, and mirroring, and just easy to use pose libraries as well.
So lastly is something that seems to come up a lot in discussions about using mocap.
Often the thinking is, we're not making a realistic game, so we can't use mocap.
Well, it definitely takes a little bit of work to get the results you're after.
When you're confident with the process, you can get to the end result in much less time than all hand-keyed movement.
Another great thing about using Capture is that it gives you a fairly consistent baseline of quality across all your characters and your project.
So a common method used when exaggerating mocap is to lift key poses from the performance.
This is done by putting the motion capture on a base layer and creating an override layer with the weight turned down.
And then you set keys where you want to keep the movement, essentially giving you your block out and your in-betweens for free, and just allowing you to retime in the graph editor, or even in the timeline.
This works the same way in both Maya and MotionBuilder.
While this is a great workflow and something I still use from time to time, what bothered me was losing all of those micro-rotations and detail that you get from mocap, and we spent so much time on the polish pass trying to add that back in.
I wanted a workflow that allowed me to sculpt that performance without being too destructive to the original data.
So what this mainly took was a bit of a mental shift from thinking of motion capture as a recording of a performance and rather to a collection of curves to sculpt and manipulate, much like the centers of mass that I showed for creature movement.
So I'll explain this with a really simple example.
So once I get my capture onto my rig, my first priorities are timing and spacing.
It helps to have a clear idea of the goal in mind, especially before putting the suit, but sometimes this is a little bit of trial and error.
When I'm working this way with MoCap, I'm still always thinking, as I said, broad strokes first.
I don't want to get really, really caught up in small details right now.
For retiming.
I'll use either Time Warp in Motion Builder or the retime tool in Maya.
What I'm doing at this stage is just trying to recreate the tempo and feeling I have in my head for the move.
The actual movement isn't being exaggerated here, only the timing and spacing.
So it takes just a little bit of imagination.
It's probably not the only time I'll touch the timing.
I'm just trying to get it as close as I feel is right at this point.
When I'm more or less happy with that, I'll plot it back to the main rig so I don't have keys on my subframes.
If the move calls for it, I'll insert my start and end pose here and work the movement to come in and out of those poses properly.
For the most part, I use the lattice tool in the graph editor.
So remember, I'm trying to preserve a lot of the micro-movements in the capture, so just deleting large chunks of data to blend into poses kind of works against this.
So I use the lattice tool a lot, just trying to sculpt these curves and sculpt the performance.
When I get my poses insert, it's time to start adding in the exaggerated movement.
So I go to go with my timing.
For this, I use layers.
Again, starting with broad strokes, I'm trying to hit the main poses and movement changes to define the staging here.
In this instance, I'm trying to exaggerate the distance and timing of this jump.
I might pop back to retiming while doing this to get the first pass nailed down.
When I'm happy with the main timing and posing and spacing pass, then I'll start to refine silhouettes, intermediate details.
The final pass is for smaller things like overlaps, contacts, cleanup, and other details like that.
Much of this I still keep on layers to avoid touching the base capture.
And then I can just use an override layer to key the weight on and off if I need to really override a part of it.
So here we can see the difference between what I started with and the end result.
Once you get comfortable working this way, it can cut down the amount of time it takes to achieve the same result, doing it by hand considerably.
This example here, it's still pretty rough and needs a bit more work, but including capturing the data, solving it, and reworking it in Maya, I've only spent a couple of hours just to get to this point.
So even for a really fast animator, that could be pretty challenging to do with keyframing only.
So don't be intimidated to try out motion capture, even if your project might not seem like it calls for it.
Much of the hype.
around it has been centered on preserving the actor's authentic performance. But I encourage you to think of it more of a productivity tool. So now, not every move in our game was created this way. There's definitely a lot of hand-keyed animation as well, and it ultimately comes down to each of the animators' individual expertise to choose what method is best for the thing that they're creating that day. But for complex movement, we found this to be really, really valuable.
So as you can see, there wasn't one single magic solution to all our development challenges.
It took these and many, many more tiny changes in how we approach development.
Always asking ourselves the same question.
Are we working on the most important problems right now and what's the best way to solve them?
Curating that culture in the studio has been the main ingredient in our success so far.
So if you want to find out more about Phoenix Labs or Dauntless, you can head to either of our websites, phxlabs.ca, playdauntless.com.
And if you want, you can find me on Twitter.
And if you're interested in more discussions about animation and games, head on over to animstate.com.
And that actually went way faster than it did the last ten times I read it.
So I have tons of time for questions if anybody has any.
Otherwise I'll be available out in the hall afterwards as well.
Thanks, everybody.
I was wondering which game engine you guys are using and what the strengths and weaknesses you feel are of it Sorry, can you repeat the second part of that?
so why did you choose the game engine that you're using or what what no choose internal tech if you're using that and What are the strengths and weaknesses that you feel made you make that choice?
Right, so the question was what game engine are we using and what are the strengths and what caused us to choose that.
We're currently using Unreal 4, UE4, and really it came down to what's the best commercially available package for us right now that got us the furthest towards our end goal.
there wasn't much out there.
I think, in our opinion, probably building an engine from scratch was not necessarily the best use of time for a startup to do.
And it's been really great for us.
We've found it's taken us most of the way without needing to write a lot of bespoke tools for our game.
It's been great.
Hey Simon, great talk, thank you.
Thanks, Chet.
I was wondering if you could go into a little bit about your interaction with your outsourcing team and how you work with them day to day and kind of pipeline for that briefly.
Sure.
So usually what we'll do is, we actually treat them almost like how you would a film team, so we launch them on moves. We will have, typically I'll sit with the designer, our little behemoth team, and we'll come up with our encounter design for a creature.
When we figure out our base list of moves, we'll make sure that we're all on the same page and then we'll get on, we use Zoom right now for group calls at the studio.
And we will launch them on the moves, go into the mocap studio so we can run around and look silly while we're demonstrating them.
And then typically, I have a group chat with them almost every day, right after lunch.
We'll do reviews.
We can do, with Zoom, you can draw on the screen as well.
So we can do reviews.
I can do draw overs.
We can talk about the moves.
They also have access to a branch of our Perforce.
So when they submit, they can submit through there, and it is mirrored to our own internal one.
So it's quite seamless.
I just export from that base folder and put their stuff in Engine.
Typically, I'll work with.
the leads on there, but often the actual animator will come on the Zoom call as well, and we'll do it one to one.
But usually it's either with Jalil and the lead for us over there at Steamroller, whose name is Josiah.
And it's just like turning around and having an animator on the team.
We just hand them off some moves, describe them as best we can.
We'll see the blockouts.
We always work in a very, like, I want to see blocking.
And then we do in-betweens.
Then we do blocking plus, and finally polish.
And we track all of this in a giant spreadsheet that everybody has access to.
It's a Google shared sheet.
So they can.
flip flags, if they take a move, they can just turn it a certain color and it flags us that they've got it.
And then when they've handed it in, it checks it another color, things like that.
It's really, really quick.
I don't spend more than a couple of hours a week reviewing animations, really, with them.
They're very good.
So I have a two-part question regarding the key frame blocking when you were first prototyping your animations First I think I would like to hear a little more about the actual process details regarding like how you went about creating the move set in the first place and blocking out identifying what your key moments would be and then the Second part was, how long did the initial blocking phase of the key frame motion when you were prototyping for gameplay, how long did that last?
Yeah, so our process usually is we'll have, we make sure everybody's pretty clear on the Behemoth team before we start on what moves we want and a priority of how they're gonna be delivered.
If the creature has like I showed, we've got variations within the genus. So usually if it's a variant of an existing one, we already have a base locomotion set we can draw from. And then it's adding new attacks and new personality animations or things like that. If it doesn't, then we block out some of those as well. And really it's about coverage, spacing, and timing when we do these first passes.
So how much distance and coverage is an attack?
And what range is that covering?
For locomotion, what speed are we doing, the distance over time?
And we work very quickly with that.
So typically for a variant, and all the animations, so.
If we have maybe 10 to 12 new moves for a creature to get done for attacks, things like that, we'll have that game playing counter up within a week easily, usually only a few days.
We can have a first playable and be playtesting on the team and iterating on it.
I think our last variant we did took us three days to have that in.
It's very quick.
We never spend more than an hour or two per animation to block it in.
Thanks.
That's it.
All right, thanks so much, everybody.
