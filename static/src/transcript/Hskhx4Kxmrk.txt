Thanks for coming my talk superhumans of New York managing Marvel's spider-man's many faces.
I'm now all's I are a character to the at insomniac games and as the title of the talk might imply.
I'll be talking a bit about the work that went into creating the faces in our most recent game Marvel's spider-man which released last September.
To be more specific I'll be talking about how we started setting up our process with a 3rd party studio to create facial rigs.
I'll expand on how we added our own little twists along the way, and added edits to the art on our end.
I'll cover how we tracked those changes as subsequent re-deliveries came in.
And finally, I'll end with the work we did getting heads in the open world.
But first, for anyone who's unfamiliar with the game or hasn't played it in a while, here's a trailer that features some of the facial work that went into the game.
What is happening to our city, Yuri?
I don't know.
Feels like the end of the world.
Maybe it is.
Peter Parker!
How the hell are you?
The city is in danger.
It needs our help.
All of our help.
Alright, well, call the play coach.
Gang of costumed nutjobs is taking the city apart piece by piece.
Time I returned the favor.
This is opportunity knocking.
You know the closer you get to them, the more you become a target, right?
The closer I get, the better chance we have to stop them.
I've ordered Silver Sable and her team to shoot you on sight.
This city's had enough of your vigilante-ing.
You're officially an enemy of the people.
How do I call you?
I mean, do you have, like, a cell phone in your pockets or something?
Thank you.
So going in, one of our biggest goals was to create a living, breathing New York.
Since Spider-Man is a character so is often so inexorably tied to the city of New York, New York is often one of the most important characters.
And in service of that goal, every character, big or small, had to feel real, whether you were in a cinematic or just walking down the street.
We had a very high bar set.
We essentially needed to have photo reel quality to our characters, and the interpersonal relationships had to deliver a punch.
So we couldn't really skimp on anything.
They had to have realistic and nuanced performances in order to hit our goals.
So to fulfill those expectations, we'd also have to have lots of characters.
As with most superhero stories, especially the Spider-Man ones, our story we were aspiring to tell was about big action and big implications, but rooted in a very personal story about Peter Parker and the people around him.
Aspirationally, that's all great, but practically for character production and all that.
it usually means a lot of work.
And it meant that some featured characters in some of our cinematics were only featured once or twice in the entire game.
And so we had to have them up to a certain level, though, so that they wouldn't stand out like a sore thumb against the other characters for whom we were devoting more time and resources.
Final, oh, excuse me.
So finally, these higher quality rigs were fine for the larger cinematics, but it had to feel like New York.
And a little secret, New York is pretty heavily populated.
So we would have to have rigs that would also be able to run in the open world for when you're out on the street, and we didn't want to turn the game into a slideshow every time you were outside.
So realistic faces are very difficult to pull off.
And with a game like this, it can really make or break it.
And it takes a lot of time to get a system in place.
And in the first year of production at our studio, we only had three riggers in the Burbank studio working on Spider-Man.
And we hadn't really done a project production on this kind of level yet in our history.
Though we've grown considerably now in our rigging team even doing something now from scratch, you know, it takes a super long time and you know, there's lots of R and D and it would be a steep uphill battle at the even at this point.
For the higher quality characters, we might have a general idea of where to go, but it just it takes a lot of time and experimentation and make mistakes being made to kind of get to that point where we can create.
characters at a quality that we were aspiring to.
So, you know, we decided to instead approach a third party that knew what they were doing and had a system in place, and that party was 3Lateral.
As you guys may have already heard of these guys, they've been making a pretty big splash in the last few years, but a quick summary on them for people who might not know.
They specialize in data acquisition, processing, and creating rigs for interactive characters.
Rigs are often based off of photogrammetry scans in order to get those realistic rigs as it was done in their work with us.
No, they've worked with many methods of capturing data.
For our purposes, they did scans of actors in neutral pose as well as doing fax poses.
For anyone who's unfamiliar with FACTS or didn't go to Axel's great talk yesterday, I definitely recommend seeing it if it ever goes up on the internet at some point, the God of War talk.
I'll be talking a bit about it during the presentation.
So just to give anyone unfamiliar, crash course in a paragraph, it stands for Facial Action Coding System.
And it's a system that was popularized about 30 years ago by a man named Paul Ekman.
And it essentially breaks down anatomically possible muscle movements on a face.
It was originally kind of developed as a psychology tool.
So with this breakdown, since we're all experts, you can tell that I was having a kind of rough day when I took this picture.
But it tends to lend itself well to creating realistic facial rigs and going into performance capture, and thus is often a sort of baseline for creating realistic facial rigs.
Terminology can sometimes be different depending on what system people are using and things they set up, but this is the general thrust of it.
So the rigs they made for us were to be joint based with corrective facial blend shapes using streaming tech in our engine.
We were able to stream hundreds of blend shapes during cinematics and further polish and detail came from blending normal maps as well as blending color maps.
These were derived from those photogrammetry scans and used to simulate wrinkles and blood pooling under the skin in more extreme poses.
Finally, a custom embedded rig node that they shipped with each character.
kind of did all the calculations for the rig logic under the hood instead of using utility nodes which allowed us to kind of play them back in the viewport in real time, which is a pretty cool thing considering how complicated they were.
But anyways, this is all just for context, for this company and its methods.
A lot of companies and teams are using similar methods, but if you'd like to know more specifically about them, their work and their process, you can see GDC talks they've presented, their website, their YouTube.
And yeah, they're out there a lot.
So for all these characters we needed, we set up multiple tiers for quality when setting up work agreements with 3Lateral.
The way things worked out, we set up five tiers and first I'll go through tiers one through three since they were functionally fairly similar and then I'll follow with tier four and five.
So, tiers one through three were any named characters.
They were actors we had cast specifically for certain roles, and sometimes they were meant to be digital doubles, meaning it was a direct representation of the actor and their facial features, such as with Otto Octavius and Norman Osborn.
Then we had other rigs that we coined characterized, meaning that they were based off of scans, but then edited to fit a design that the actor might not quite fit.
And in the regular cases, it meant slight changes in physique for people like Peter Parker or Kingpin to kind of widen their necks and give them wider jaws.
But then in the more extreme cases of this characterization after the fact, it meant things like adding severe scar tissue to Electro or fully changing the shape of the cranium and adding more scars for with Hammerhead.
The series one through three essentially covered any characters that were kind of cast to be in a Golden Path cinematic more than once or twice.
And I say at time of casting because there were actually some characters where they had been written as bigger roles, but then as rewrites happened for story cohesion, certain ones were cut just because we had a pretty crowded story.
What we ended up doing with those facial rigs was just using them for other background characters that for whom we might have used a tier four or five rig.
There was actually one character that we had cast and we put her into three separate roles and, you know, completely separate people in the game.
I think able to get by with it because of some texture and hair styling wizardry from our character team.
Actually all three were in this slideshow and you may or may not have noticed, but it's pretty cool.
So the rigs were based off of scans of the actors doing the facial poses and the fax shapes I mentioned before.
With the classification for the tiers one, two, or three, we kind of set up how much iteration and blend shapes would be kind of done by 3Lateral.
So the tier one character was essentially the top and had a lot of iteration.
Well, Tier 3 had a fair amount of custom work, but it was still one of those things where little inconsistencies between that and the scan were OK, as long as things didn't look broken, because they were minor characters.
So we didn't go over it with as fine tooth a comb as we did with the major characters.
So the way it broke down, major characters like Peter or MJ were Tier 1.
Secondary but prominent and important characters like Captain Wapitanabe or Kingpin were tier two or three, with characters like Tombstone or the Rosemans Gallery Manager being tier three.
As mentioned previously, these rigs had blending normal and diffuse maps to simulate wrinkles and skin flushing.
So to that end, they delivered with eTrig, they sent us three wrinkle maps and three color maps, respectively, which were then masked out to create a total of 37 tracks that would blend in during animation to replace the normal in-color result.
For those who have played the game, they might be aware we were fortunate enough to have the late legend Stan Lee agree to do the cameo in our game.
And with him, we were able to also get the scans of him in the fax poses as well.
So his wrinkle maps and all of it is actually derived from his data to recreate his likeness.
It was awesome to be able to have him in our game and have it kind of have that.
stamp of a true Marvel creation.
For me personally, it made for a pretty surreal day making goofy faces at the Stan Lee while coaching him through the fax poses.
He's a great sport about it and, you know, yeah, he's great and he'll be missed.
So, tier four characters in Blow covered kind of just, yeah, background characters as needed.
The minor characters are background people.
They were based off of scans that we had done from an open call for studio employees and relatives who might be in town to go to a 3D scanning studio nearby where we did photogrammetry scans of them.
Some of the scans ended up heavily edited, but that's kind of how the sort of baseline started.
So if anyone knows any Insomniac employees or relatives of employees and might have seen some familiar faces in the game, chances are it was them.
The scans, however, were only in the neutral pose.
We never did the fax shapes and things like that.
So what 3Lateral did was they took the neutral poses and based on some statistical estimation, then derived their movements and everything.
Obviously, it was all still high quality and it had wrinkle maps and all that, and blend shapes and all that.
But, yeah, it was not driven by scan data as none of that was available in this case.
So, to that point, instead of the 37 wrinkle and color maps from the higher tiers, tier four rigs had 24 blending wrinkle map tracks and no color maps.
They say tracks because since they didn't, they didn't have that information, they just set up the tracks in the logic.
And what we ended up doing was kind of plugging that in and changing our skin shader a bit for these characters so then wrinkle maps would come in additively rather than replacing.
And with that, we were able to share wrinkle maps between all these characters.
And what we did was we made generic sets, one for male, female, and elderly.
Finally for tier 5, we fed in a bunch of heads that had already been created for the game as seeds for a character creator of sorts, complete with sliders that 3Lateral had delivered to us to make new models in Maya.
One of our character artists created a wide array of male and female heads of various races, ages, for various body types to cover civilians, thugs, heavy thugs, and police officers.
Those character creator values were sent back to them and they created rigged versions using that same statistical approximation model and sent them back to us.
All told, we had about 90 of them, so it was definitely a case where having a simple batchable command helped me out a lot.
If some of this is sounding vaguely familiar, it's actually very close to the GeneSplicer technology base that they showed off in GDC 2017.
At the, I believe it was at the Unreal Engine Summit.
uh... but however for purposes it was it wasn't about trying to create rigs that would like work the run in runtime in the engine and like you have sliders and all that but just about creating a wide array of faces so then we could uh... kind of prop out our uh... cinematics with uh... good background people Anyways, functionally for our purposes on our end, they behaved very similar to the tier four heads with the same mesh topology and shader setup.
So it was pretty much the same protocol except just on way more characters.
So, that's a lot of things to keep on top of.
As one might imagine, 3Laterals works load aside for creating all 150 of these rigs.
It was a lot of files to track, integrate, vet, keep track of, and edit, especially considering that it was kind of only supposed to be me doing this, and it was with the expectation also that it wouldn't take up all of my time either, because there were always more things to create, other tools and pipelines to maintain, myas to crash, all of that.
So I imagine lots more studios and projects are using these types of separate team situations to create facial rigs in the last few years, either through outside companies like we did, or in the case of the bigger publishers, they'll start to set up internal groups as shared resource groups for all their studios.
I figure this situation's becoming more and more common, a common thing that might.
be happening more and more in AAA, and even AA sometimes, hence the genesis of this talk, at least on my end.
I figure it's a thing where a conversation on best practices might be a beneficial thing to start, whether or not ours turn out to be the best or far from it.
So the first simple problem was processing the raw files they sent us.
Since they weren't working off of our Perforce branch, they sent clean files based off of their own environment every single time.
So anytime we sent feedback, a new version would come back to us and we'd have to process and integrate it again.
And this happened over many months with many characters.
So we wrote a tool to do that boring stuff and kind of batch through everything thrown our way.
This processing tool at first started out as just a simple thing for adding controllers and body joints and.
repassing textures changing changing their naming conventions to kind of just exactly fit with ours uh... and uh... you know set up attributes to control blend shapes and wrinkle maps and all that with our engine conventions but once we got the basic stuff down uh... we wanted to add hooks for you know extra functions for certain characters and stuff and all kind of go through all that uh... later This tool kind of started out as a simple, like, you know, couple button thing.
So excuse the interface, because it's kind of one of those things where a small solution had to turn into a bigger and bigger solution.
And it kind of turned into a gray sea of semi-buttons.
But it got the job done.
So one of the first things we tackled next was streamlining our feedback process with them.
They're based out of Serbia, so we wanted to stick mostly to email communication, because in-person communication was obviously out of the question to do, like, super often.
And also Skype calls would be considered kind of a last resort item, since, you know, we have that big time difference as well to deal with.
So, as part of their deliveries to us, they would go through...
They would go through the fax shapes and try and pose the rig to match.
They would then assess internally and send the controller values used to get to the, to match with those photogrammetry scans.
And in the margins of that spreadsheet, they would list any notes on discrepancies they had spotted on their own.
So then, because of that, we wouldn't necessarily try and waste a lot of time communicating issues that they had already spotted.
So that saves a little bit of time.
So, since we had that spreadsheet, I wrote a function to read those controller values from that spreadsheet, create a calisthenics animation, and import the scans to match those timings.
So then, with that, we could get a play blast of the delivered rig in Maya.
We could export the animations to a test cinematic in our engine and get a play blast of the scans coming in.
And all those captures were then brought into a template Premiere project to get a comparison between the delivered rig, how the animation was exporting to the engine, as well as a photogrammetry scan for a ground truth comparison.
This allowed us to spot extra issues with the rig where we wanted to communicate that to 3Lateral, as well as suss out any potential issues that we might be having with the way the rig animation was being delivered to our engine for fixes that we might need to make on our end.
With the help of the tool and the workflow we had set up, the process of creating these videos was actually turned into a simple enough process that I could kind of delegate it to a junior employee from our production services department who had very little Maya experience.
And because of that, that was kind of off my plate so I could do other things.
And an hour or so after a rig was delivered and processed, then my art director, Lied, and I, we would get a video.
of that and then be able to just do quick draw overs and send back to 3Lateral as feedback.
From that, I can say at least speaking on our end and our perspective, it made for a great working relationship where more often than not, we were on at least the same page about what we were trying to communicate.
There were rarely that many miscommunications, which is, I think, pretty cool considering how often emails can turn into miscommunications when you're with someone who's four feet away versus on another continent.
So when they delivered the heads to us, they always just had the standard eyelashes, brows, eyeball geometry, and all that.
However, in order to hit the art direction we wanted, the character art team would make new assets for things like hair, eyelashes, beards, jewelry, and all that.
And as time went on and iterative rig deliveries came back in, it became pretty clear that we would need a way to save out those extra changes we had gotten so then we wouldn't have to just keep saving it out and bringing stuff back in, transferring weights all the time because it would just be a big time sink and then potentially things might be forgotten.
So to get around this a function was added to save extra meshes to a and associated data to a character post folder and the way it worked was to simply duplicate the selected measures arranging them into a matching hierarchy relative to the geometry folder for things like a lady groups and all that and we exported those duplicates with their materials to a separate file called extra measures and along with those it exported skin weight values and all that to a folder using our save skin weights tool.
With that, re-importing those extra meshes, scanned it in their proper respective groups, was a fresh, into a fresh file that we got from them, was a simple button click with little or no thought or work required by us.
So, even though they were doing very good with the face, very good work with the face, any neck movement was just purely for Adam's apple, you know, swallowing, flexing of the muscles for, you know, that intense performance.
Basically, they were tasked with getting the facial movements, but neck deformation for head looks and body mechanics wasn't really under their purview under our work agreement.
so it wasn't really on them to do it.
What they delivered to us was a face with single joints for the head and neck, and the rest was just kind of skinned to a chest joint.
It was a very simple and generic setup for basic movement.
But as soon as we started getting those heads in Cinematics with some facial animation, it was pretty clear that we had to get the rest up in line with the level of polish with the face.
Now, one thing that I feel is often missing from a lot of game characters is that polish work does tend to end kind of just at the chin.
little or no work is done for creating a realistic and plausible feeling in the neck when the head turns.
So you don't get that sternomastoid flex in the neck.
And when you look down, there's often, you know, it's just, it's kind of trying to get things good enough.
So, you know, you might get some gradual twists or something, but that's often the extent of it.
Obviously, budgets and time being the way they are, it's, it's a simple, it's an understandable thing to kind of let go.
But.
In our case, we actually did, I think, since we were freed up in a lot of ways, we had the opportunity and time to push things a little bit further.
And since we already had a few hundred corrective blend shapes coming in for the face, adding a few more for left, right, head up and head down movements was just kind of a drop in the bucket.
We eventually also started to add fixers for combinations where it's looking down into the side because on some of those characters, things were maybe looking a little crunchy at times.
though the meshes were pretty dense.
Unfortunately, sometimes that last little bit of mesh density at the base of the neck to get that sort of sharp profile on characters who had more exposed necks wasn't quite there.
So moving forward, it would probably be cool to integrate some wrinkle map tracks and stuff like that.
But that would take some, you know, some, probably some schmoozing with the, with our shader tech artists, you know, in order to change up our shaders.
So, might I mention, they're incredibly good looking, well dressed, and very smart.
Anyways, for most of our characters, we sculpted correctives for them using some tools to streamline things using this workflow, where we generate duplicated targets of the sculpted corrective result, edit those and quickly update the resulting shapes by extracting deltas, essentially just creating a sort of link between all of that.
then a script would set all of that up and then when you extract the deltas again, a preset calisthenics animation would come up and let you see how the correctives is blended into each other.
It might be worth mentioning that we did our production in Maya 2016 before they added the shape editor.
So the creation of these tools was born largely out of some lack of reliability that we were having when using the old blend shape editor and using the sculpt tool where sometimes edits were saving to the base mesh rather than the corrective we were editing.
Now that we're on a newer version of Maya, I'm gonna be kind of...
I guess, evaluating the new stuff and seeing if we phase this out, if it turns out to be stable and useful.
Just a little note.
So if we ever needed any other custom shapes, there was also a tool to easily set up a cone pose reader on specified joints for pose-based deformation and add that to the arsenal of correctives for some of the more major characters.
And it was usually just things like, yeah, raising the clavicles for people like Peter, or also there were some cases where we had some edge cases for doing like skin-to-skin contact with the, you know.
the with any kissing that the game may or may not have ended on.
Once we're done with the correctives, using those same tools, we can just export it out to that extra, that character post folder and it would store anything so then it can be brought back in on future deliveries.
Since a pipeline had already been written to export shapes out to a folder and store them, we were actually occasionally able to find some secondary uses.
One notable one was that we were able to create a pretty easily scriptable neck jiggle rig for Kingpin.
As he is a heavyset gentleman, we realized that things were looking a little static under his chin.
uh... but the issue was that his uh... his regard he had neck influences from the facial ring so we can necessarily just throw join on top of it and uh...
you know not mess with the facial animation so that was an option But using the jiggle on a middle controller of a standard Maya muscle rig, we were able to set up driven blend shapes.
So we had three of them for movement in the positive axis, positive direction in the X, Y, and Z axes, which would then bring in and then drive in the positive and negative.
And we were basically able to kind of create a joint movement in a sort of faked way, so then the jiggle blend shapes and the neck could coexist.
For cinematics, we would go through and do a pass on baking those controllers for his jowl jiggle.
It was a subtle thing that no consumers likely notice in game, but it at least made me happy.
There you go.
So that blend shape stuff worked for cinematics, but we couldn't have those blend shapes streaming in gameplay because that is something that is very much just delegated to, very much, you know, specific to cinematics.
So we needed a gradual neck twist rig for the neck, and also while we were at it, needed a way to load in clavicle skinning since that area was just skin to the chest.
So we had to add them all after the fact, but at the same time we wanted to avoid messing with the three laterals skin weights for the face.
In order to edit the clavicle weighting and add gradual weighting in a clean scalable way, as with all other edits, which was the goal.
We added a feature to our import skin weights tool called relative mode, where weights could be brought in to only affect the weights of influences from the imported skin data file.
So that way, if the tool loaded a skin data file with weights only for the head, neck twists, joints, and clavicles and chest, it would only affect the weight share values for those influences while leaving the facial weights untouched, making it easy to load in fresh deliveries to other rigs completely as long as they shared the same topology.
So you can see this extreme example here where I'm gonna skin his cranium to the clavicle.
I can save it out and then load it back in in a relative mode.
onto a version of the rig that has all the face joints.
And you can see that it now only affects that area while leaving the facial joints untouched.
So to break down exactly what we're doing to achieve this, I'll go over a quick example.
Warning, there will be a bit of math, but don't worry, it's simple and quick because I went to art school and I was only required to take one math class and it was actually just called math.
So, say we have a head.
If we were to look at vert 7,729, we see it's weight values here.
So, if we load in a skin data file, the skin data file has neck, head, and neck twist joint influence data only, and not the hundreds of facial joints that are included in this sort of thing.
So, We look at the weight data for the same vert from the skin data file, and we see that it's 0.5 neck for twist and 0.5 head.
All that really matters here is the 50-50 split.
Since we're working in relative mode, we're only concerned with the influences that are in the skin data file, and so jaw bulge and jaw line aren't influences in the skin data file.
We'll leave them alone, and only affect the 0.35 remaining influence.
Then we basically multiply out the incoming influences by 0.35, zero out any other influences, and match them with the skin data file, and we have a normalized result.
So, once the weights were set, just like everything else, we needed a sustainable way to export them out, sans facial joints.
So, essentially we needed a way to do the inverse of that process that I just described.
So at Insomniac, we do have some tools for saving out skin weights.
What it does is it basically takes the weight values for each vert and then just exports them out as a JSON formatted file.
It's a thing that we just tend to do a lot quickly to just quickly rebind a thing or just share some weights between things as long as they share mesh topology.
So since our save skin weights tool output was a collection of data in a simple dictionary, I was able to interact with that data to get what we needed.
So while the regular one was saving out joints for the entire skin cluster, I was able to write something that would output as a separate file with edited values.
So along with exporting skin for this full skeleton, it exported a skin for the skeleton that had the body joints and none of the facial joints.
As you can see, there are a lot of facial joints, but fortunately they were all under two sort of root joints, one for the head and one for the neck.
So I was able to make something that could generically detect all face joints, whether they were under the hierarchy of the face or under the hierarchy of the neck.
The tool would then transfer weight values for all face joints to the head and all neck joints from the face rig and twist joint for the head.
The result would be an extra non-face skin data file for the mesh that could be brought back on the rig in relative mode for new deliveries.
Since we shared topology between all the heads, this would actually work on other files as well if we wanted to do some recycling to get that clavicle weight in.
So if there are any math whizzes in the audience going through these numbers in their head, they might have noticed a little bit of a problem with the approach from the last slide.
If the facial animation from the neck joints were being saved out to the end twist joint, it creates a situation where the end controller gets more influence as it's saved out because it takes more and more relative influence from the resulting file.
Let me break it down.
If we look back on our old friend, vert 7729, If we go by this formula of saving out the non-face skin data file, we take the facial joints and migrate the weighting to the head joint.
And if the head and neck forward twist ratio, which used to be one to one, is actually now very unbalanced.
So if we were to turn it back around and reload the weights back in in relative mode, we actually have a lot more weighting on the head than before.
And the more and more of these weights are saved out, the worse the problem gets.
you know, it still isn't messing with the facial joint, that's the facial joints, but that's still, you know, that was the main purpose of this, but things are getting more skewed.
So really all I can say is I concur with Dr. Octavius on this one.
Luckily, this process started to kind of come online when the rigs were closer to being completed.
So even though it was possible that it slowly started to offset things, it never really became an issue because we were always re-extracting deltas for those bun shapes because we were always saving out those bases and then just redoing it.
It's just a little like clerical error on my part that luckily didn't really balloon into anything.
And if I'm being honest, I didn't even notice it until I was preparing this presentation.
But, you know, what I'll probably change in the future now for this scheme is basically have it export out to a joint that can just kind of serve as a dummy that exists in both.
So like, we actually don't actually use the base, like neck joint at all, like the one that drives the twist joints.
So I'll likely just put the stuff into there.
So then it just serves as a container and then it all just goes back out to the face anyways when you load it back in.
But, yeah, if you guys want to do this sort of thing, just keep that in mind and be better than me.
So, for the rest of you who've been glazing over for the last couple of minutes, time to unglaze, because we're going away from the land of numbers and into triangulating quad meshes.
So maybe keep Facebook handy.
So with all that workflow solved, one confusing snag we ran into for getting our models in engine was things would look fine in Maya, but when we exported things out, there was this odd disconnect between the way it looked in Maya and the way it was in the engine.
We were getting this sort of sawtooth thing and jagginess along the silhouette of the neck at certain times.
I asked around to other riggers, our engine people, and did lots of frantic Googling to find out what was going on, and I was pretty perplexed.
We ran side-by-side overlays, had no idea what was going on.
I was thinking maybe there was something wrong with the engine, but I was wrong because our core department is amazing.
Please give us more features.
And, you know, what I accidentally stumbled backwards into was figuring out a lot about the way Maya triangulates quads.
For any four-sided face, Maya accepts that shape, but quietly triangulates those faces by connecting the two closest corners.
Since it does this all quietly, you don't really notice it, except for the telltale pops that you kind of get when a mesh is moving, and two corners are suddenly the new ones that are closest to each other.
There are ways that you can kind of display it in Maya, which you might be able to see here, but either way, yeah, you can turn on something where it'll softly show it, and you can see it switch when you move verts around.
So there was kind of the rub.
It was dynamically re-triangulating that stuff in Maya.
And, but your average game engine actually just takes the triangulation at bind pose and keeps that.
So that's where the mismatch was coming from.
The resulting model was different.
So unfortunately there's no way I actually know of to disable that in Maya, but fortunately I found a little hack where you can just throw a poly-triangulate modifier.
to get a more accurate visual of what it might look like in an engine.
But one trick to getting it to come in before the deformers is, you know, that's something that's actually necessary because if you throw it in after the deformers, it'll still dynamically re-triangulate.
You'll just get the hard line instead of the dotted line in that sort of mode.
So, it's, instead of triangulating the head mesh, what you need to do is triangulate the ridge mesh.
What's the RidgeMesh?
For anyone who might not know what I'm talking about, it's essentially a copy of a shape node that Maya creates when you throw a deformer onto it.
And that shape node kind of serves as an input to the deformer to kind of give it a baseline to offset for the things.
So if you run an operation like polytriangulate just on the regular head mesh, you can rearrange deformers, you can rearrange inputs and stuff on anything in Maya.
But Maya will actually block you from rearranging anything that edits the mesh until it still always has to be after any deformers.
It's probably something that they kind of put into place to prevent chaos and tears.
But we tech artists, we deal in chaos and tears all the time.
If you, the way you can do though, is just throw it on the ridge mesh instead, and then you can kind of just trick it into coming in before the deformation.
And luckily, the poly-triangulate modifier doesn't actually rearrange vertices or anything like that, so the resulting deformations are fine.
And here it is in the node editor.
It's all a little bit more simple than it probably looks.
But, that didn't solve the deeper problem.
The fact was, triangulation in Biden Pose was fighting our ability to get a good silhouette.
The triangulations weren't going along the sternal mastoid as we wanted, but they were actually going perpendicular, which was creating that sawtoothing.
So along the lines of the hack to solve the dynamic re-triangulation issue, I found a surprisingly simple way to kind of feed in a new base mesh.
What you can do is you can connect a new mesh to the inMesh attribute of the hidden arid shape and then immediately disconnect it and you've essentially kind of done a convoluted way of doing a set adder to just replace that mesh.
and as long as the order is still the same, you're good.
So we would create a duplicate of the mesh, run a poly-triangulate on the offending area, then flip the triangles in that area, and delete history, pipe it into the new mesh, and we've replaced the resulting triangle flow with something that's a little bit more pleasing to us.
I included all this because I figured it might be useful for other people who kind of might run into this problem.
uh... but i didn't really want to turn it into a big tutorial or anything but if you do want a big tutorial or something uh... actually did write one uh... about two years ago when i discovered it in a blog post at the time i was very heavily buried in the land of NDAs so i had to rely on placeholder assets and snarky flavor text but you should be able to you know get the point of all of it if anyone wants to do this So rather than listening me go into it, you can just snap a picture of this slide, ask me later, go to my Twitter or my website, noelsire.net, and it should be easy to find because I'm kind of lazy and don't blog nearly as much as I should.
So this actually came in handy as a secondary way to do other things to edit other base meshes if we wanted.
You could pipe in a new mesh for heavy thugs because we were deleting the blend shapes anyways and it was just saving those as deltas in the deformer.
So we wanted to give the heavy thugs thicker necks so we were able to just do that and no need to propagate that all out to the hundreds of blend shapes.
So, we did this to a few characters to change small things if we wanted, if we didn't want to deal with like, you know, sending it back and all that.
This also came in handy as a nifty way to recycle neck bun shapes between characters.
So you could bring in the base mesh of a character that had already gotten neck corrective bun shapes.
Then you could import the correctives, as you see here with MJ.
Then after that's in, then you feed back in the original mesh, and you found kind of a hacky way to recycle the bun shapes between the two of them.
And, you know, kind of the heavy lifting is done at this point, and it's just, you know, polish.
So that covered cinematics, but for gameplay, those heads were way too heavy to be in the open world for the amount of civilians we wanted on the street.
The plan was to create a bunch of separate but very simple head rigs to do blinking, eye looks, maybe jaw flap, all that simple stuff.
We played around with a few things at first.
One notable test that I actually thought was pretty cool was doing facial animation through basically through the shaders where we sort out blend shape deltas as textures and then they just kind of came in.
I couldn't actually find a video because it was super long time ago.
But.
Anyways, that was like kind of just feeding in blend shapes in a sort of cheap way.
What we ended up electing to do though was a simple joint rig like the one shown here.
And we were going to load those heads and attach them to the bodies at runtime, like per cycle, and using our built-in engine retargeting, animations could then just be shared across all heads.
And here you go. You can see, you know, the sort of first test at that.
Just simple stuff for a kind of face idle.
However, it became clear pretty quickly with the massive amount of civilians we wanted in our open world at once where you could just see a super long distance.
Even that was actually starting to become too heavy for our needs as doing that searching and matching per frame was actually starting to slow things down.
So luckily our engine actually has a vanity mesh system for keeping multiple mesh groups within one model that can be toggled off and on and then you can set up groups for drawing things in randomly and all that from a predefined set.
So we already had all possible shirts, pants, gloves, and shoes in our civilian master files.
So what we ended up electing to do was have the heads also be in that body model as sort of other toggleable submeshes.
However, it wasn't all sunshine and roses in that case either.
So ballooning file size aside, the problem with this approach was, whereas before head models were sort of separate, so they could have bespoke positions for things like eyeballs and eyelids and all that.
they had to all kind of live in the same sort of environment.
So, kind of to keep the situation we had before, we would have either had to move all of the facial landmarks to be in the exact same spot, or just have joints for every single head.
Thankfully, there wasn't much consideration for either of those as keeping all the joints in the head would be just a massive pain to track and just kind of grow out of control quickly because we had like 20 heads that could be kind of brought in for each of these types of characters.
But then also we didn't, you know, we wanted to have a variety in our open world.
So we didn't want to, you know, have basically just everyone have the exact same face but just slightly different like skin color and stuff.
We actually wanted more visual variety.
So that wasn't an acceptable option either.
So what we elected to do was create a 11-joint facial rig that only used translate to move the brows, lips, and even eye blink.
The way we handled different sizes of eyes and stuff was just to adjust the weighting, where the animation was always the same, but smaller eyes just basically had less eyelid weighting.
and uh... you know only the people with the largest features would then approach anything near a hundred percent waiting on things like the eyes Only the jaw used rotation, and the lower lip joint was parented under that.
And it just honestly didn't really cause that much in the way of problems, because the jaws did tend to be in a similar spot.
And again, these were just kind of open world people, and they weren't necessarily going all snake swallowing a cow or anything like that on us.
So still left eyeball rotation, though, if we wanted to do it.
For that, we actually experimented with using blend shapes to do rotation.
I even wrote a script to mass produce them.
And we were getting surprisingly good results considering that blend shapes only do that linear interpolation.
What we did was we just kind of got it to a rotation of about 15 degrees and then we capped things at a weight of 1.5 and with that, you know, we didn't necessarily have that much in the way of issues and we could rotate to about 20 degrees in each direction.
And yeah, it was pretty cool I think.
But before rolling it out to all characters in order to just suss out some things, we ran a sort of lighter weight version of the civilian rig in a sort of open world diorama.
So hold on to your seats, because I'm about to blow your minds.
Turns out eyeballs are really small.
I know what you're thinking.
Gee, Noah, that sure is the industry-redefining hot take we came here for.
Sure glad we flew people out, spent thousands on hotels and registration and all that.
It's crazy.
But before you crown me king of the nerds, basically what was happening was we had five professional game devs standing around a TV and squinting.
And we couldn't really tell the difference because they're on screen about that big on a smaller screen typically than we have right here.
So it was determined from the minuscule bit of benefit we got from it, it just really wasn't worth it.
And maybe when we have 8K TVs or whatever, it might.
We'll revisit.
But it's still a pretty cool result, I think.
So, sorry.
Before I go, I'd like to quickly talk about batch tools and naming conventions, because during most of the production, it was just kind of me interacting with the files, and it was coming from a company that also took its naming conventions seriously.
And, you know, it was fairly drama-free and stuff.
Then towards the end with the tier five rigs, we actually, rather than having me just be that intermediary, we had a character artist go through and add art to those files, add the hair and all that.
But as he was doing that, I gave him this big list of rules and so I was like, you know, you can't use pasted textures, you have to keep naming, you can't name it this, this, and this.
I was probably annoying the hell out of him with all those rules, but he stuck to them.
I occasionally even jumped in files and checked just to give him that sort of random inspection.
He was following them to his credit.
And as we were closing out the game, as breathing room became more and more scarce, you know, we were starting to hit performance problems and, you know, it became pretty clear we had to like do a lot of changing of LODs to be more aggressive and things like that.
Problem is we had to do this on 130 rigs.
So, you know, thanks to the conventions that we were keeping.
We shared UV sets and all that, kept strict naming, and we had a pretty good batch tool that we just kept upgrading through the projects to kind of prepare for a moment like this.
Going through and editing all 130 of them was just a script I wrote in a couple hours and then just ran over lunch, came back, checked it.
We threw it in game.
Oh, LODs need to be more aggressive.
Okay, whatever, just rerun it again.
It was a pretty simple thing.
Or, you know, we wanted to change certain weights on all of the characters.
Just all pretty simple.
So it basically turned what would have been a garbage week or weeks into, yeah, just that little simple thing.
Probably would have had to drag in the whole department, you know, because, you know, I think even doing that manually, even if I was sleeping under my desk and all that, just wouldn't have been possible.
So it's not really much of a story, but a story about a story that didn't happen.
And it's like one of those boring stories where something goes right.
But while those stories are fun to watch about other people, the crazy stories aren't the ones that you want to be in.
So follow your conventions, kids.
They do work.
So, as with all creative endeavors, we learned some lessons along the way.
And so, what might we change if we move forward?
First, a small one.
I probably should have, from the beginning, included a better way of doing these conditional additions for certain characters.
It was kind of those things where I didn't realize how much we'd be replacing eyebrows and eyelashes and stuff.
And this kind of just kept getting added to the sort of master file and all that.
So probably what I'm going to be doing is just kind of have stuff where it's like separate scripts for each character that then just gets sourced in according to whatever.
Because while it didn't really cause any issues, it kind of just hurt my soul a little bit to keep doing that.
And there are just so many edits that we're just like, okay, I'm actually making Aunt May remove her eyebrows and stuff when they come in because we're bringing in a new set of eyebrows.
So, again, another one of those small solutions that just kind of grew and, you know, probably should have anticipated it.
So, we had a lot of heads that were shared and randomly loaded on different types of characters, especially at the thugs.
And the thing is, all of those thugs were then just skinned kind of separately, never really kind of followed any sort of convention for no particular, there was no, you know, reason that we thought to do that.
But then towards the end we were getting a ton of clipping bugs because we had these shared head models that were coming in on unlike body models.
So in the future I'll probably push to at least try and do some sort of baseline for clavicle weighting.
So then, you know.
We'd still want to try and be able to support changing the weights a little bit more if it really needs to fit a certain look.
But either way, there has to be a reason to do that rather than just kind of what we did, where it's like, just do whatever.
Next, partway through production, I wanted to add those combination shapes that I was talking about, but you know, the way you have to deal with, like, blend shapes and all that, and all the attribute connections and stuff, I probably should have found a better way to track it because it was just kind of annoying manual work and, like, I sometimes had to disconnect things and reprocess them from scratch.
So, I started to add, like, a lot more just general tracking through network nodes and all that to...
be able to track everything, but either way it needs to be kind of done from the beginning.
I might investigate in the future figuring out a way to properly save out blend shape deltas rather than the absolute shapes, because then things would be able to be shared between characters or if the character model changed for whatever reason.
There's no need for all those extra steps.
That sort of method of sharing blunt shapes that I talked about before was discovered pretty late in the project.
Either way, like yeah, having something from the beginning that would have allowed us to share blunt shapes to then just polish on top of it would have probably saved a lot of reduplication of time and effort.
And having a proper plan in place from the beginning for the pedestrian rigs probably would have saved us a lot of time and heartache because I think we just we didn't necessarily stress test as often as we should have and yeah again still worked out but you know could be better in the future.
And lastly, I probably need to make a way to empower the character artists to do the updates for things like eyebrows and all that, because especially towards the end with like the important characters, it was often, you know, move one vert, you know, adjust this slightly, change the UVs here, okay, and then I need to rig it and get it into the game, because part of vetting that was actually about seeing it on the animated characters.
in the engine with the lighting and all that.
And either way, I was kind of starting to become, like I'd get these things like two or three times a day.
It was often just like Trello comments or Slack messages or just, you know, walking down the hall to the kitchen there like, hey, can you update that thing?
It's kind of turned into a bit of a tracking thing, which was like pretty bad.
More sort of damaging was that I was becoming a bottleneck and kind of getting in the way of artists.
So I want to remove myself as that sort of red tape and give them some sort of method for importing the new stuff with an emphasis on keeping things clean.
So yeah, something I'm investigating.
Anyways, that's all I have for today. Before I go, I'd like to give thanks to all my fellow Insomniacs for helping make a game that's been a dream project of mine.
I'd like to thank my fellow riggers, past and present. We share everything and, you know, we all...
I think if I walked out of here and got hit by a bus, we'd be pretty good for a while, because we all just know each other.
Also, the Shader Tech artists, the animation team, character artists, our awesome core department, my advisor, Jeff Hanna, 3Lateral, Cubic Motion, and our amazing voice and PCAP cast, and our producers at Sony and Marvel.
Lastly, thanks to you for coming to my first UDC talk.
Hopefully I didn't bore or disappoint.
As is obligatory, I'll mention we're hiring a lot.
So if you'd like to come and help us build some really cool things in the future, talk to me or someone else wearing an Insomniac badge.
We don't bite, and if you do, maybe you'll get some superpowers or something.
Anyways, if there are any questions, I'd be happy to answer to the best of my knowledge and legal ability.
I have a question.
Oh, OK.
So you talked about how you had to simplify drastically the rigs for the tier four and five.
How did you handle simplifying the one for tier one to three so that you didn't have all those many blend shapes?
Or was it the same rig in cinematics and in the rest of gameplay?
Well, the tier 1 through 3 heads, actually all of the heads were usually delivered with the LOD 0.
The tiers 1 through 3 had a shared topology and then the tiers 4 and 5 had a shared slightly lower topology.
But then the tier one had all of the blend shapes.
Then the, sorry, LOD zero had all of the blend shapes.
Then LOD one was usually like the exact same mesh, just minus the blend shapes.
Then LOD two was a kind of half mesh, half density one.
And then, yeah, they shared UVs, so in the case where we had to go through and make the LEDs more aggressive, we were able to just use transfer attributes.
Though the eyes were a bit of a tricky thing because, you know, they overlapped UVs.
So what I ended up doing was just writing something to kind of separate the side to side and figure out the size of the eyes to bring in, like, new versions of the eyes.
OK, so the guys from 3Lateral already gave you the second, the LOD one?
Yeah, they did.
And yeah, it was just basically, it was literally like half, like every other edge loop was gone.
And then, yeah, in the case of later, then we used simply gone to create more downgraded ones, or more optimized meshes, and then kind of bring those in.
Thanks.
OK, anyone else?
Going, going, yeah.
All right, thank you.
