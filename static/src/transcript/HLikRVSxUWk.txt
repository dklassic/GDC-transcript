Hello everyone, I'm very happy to be featured for this online GDC.
I'm Oranier Peters, I've been working at Azobu Studio for 10 years, and I was Audio Director on the Plague Tale.
So we are going to talk about the audio production of the Plague Tale Innocence.
We'll cover different areas of the game soundscape, the RAS audio, and the tech we used to design it.
The stealth mechanics more widely, how audio can extend narration, how we try not only to stick a sound on the picture, but to amplify and echo the story.
First, let's talk a bit about the game.
A Plague Tale, in a sense, is a story about two siblings' relationship, Hugo and Amicia, and the evolution of this relationship.
It's a story about two children confronting middle-aged brutality with inquisition hunts and plague on background.
So you get the big picture of the game and that's quite dark.
That's from this setup and story that we choose to deliver a third-person stealth gameplay.
The children are not powerful and they need to cover to survive in this hostile environment.
There's also some puzzle elements, a very strong narrative background, and on top of that we created a technology that made us able to handle 5,000 rats on screen.
Rat swarms. They are also a major part of gameplay and story.
That's it for a quick presentation of the game.
If you want to learn more about the story, don't hesitate to play the game.
On the production side, it was a new IP for Asobo Studio, with a dev team of 45 people.
We were 3 people for audio, with one sound designer, one audio director and one composer, Olivier Derivi√®re.
Creating a new IP is tricky, but it's also very fascinating.
For Up Like Tail, we had a local background.
The action takes place in the southwest of France, where we live.
So we really had this project at heart, and we did our maximum to deliver something personal and authentic.
The game took two years to be made, and quite frankly, we are very proud of it.
So I'll now move on with more audio-oriented subjects.
As I said, it was a brand new IP, so we had to build identity in every aspect of the game.
For the audio direction, and more globally for the whole artistic vision of the project, I always remember something our creative director said that describes well the vision, and that guided us.
Its crude middle age, a rough and rusty era.
The story we tell is very dark.
It was kind of our leitmotif, and it influenced many choices for audio.
We were willing to create something grinding.
We wanted the player to hear and feel this rusty and brutal atmosphere.
It took some time to find the right tonal balance, to make the sound design authentic without making it too thin.
So we looked for inspiration and influences to help.
For example, for Foley, we found an interesting influence with Game of Thrones.
We really liked the way they handled their Foley, brutal but realistic.
As for ambience, with the beautiful and very detailed environment the artists were creating, we knew we had to deliver something very detailed too.
We worked a lot on creating those ambiences.
With a lot of randomized emitters, with wind, birds, leaf, or whatever the environment needed, we wanted the audio to add something to the picture.
On the VO side, for a more authentic feeling, we decided to have children do the voiceover.
During the prototype phase, we did some recording tests with our kids, and it was really bringing something.
So even if it was costing a lot more time, we felt that it would bring something to the experience.
Talking about middle age, the other question was, how do we handle it? Are we going the full medieval folkloric way with old english dialogues and medieval instruments for the music?
Again we found inspiration with how some movies handled past era, such as Macbeth, released in 2015, or The Witch. They provided an artistic vision to their era without making folkloric or ridicules.
We wanted to go the same way, something contemporary with subtle medieval colors, dialogues with a light touch of medieval, and for the music, it was the same, some colors to remind of the era but not folkloric.
We had many other influences and it's hard to cite them all, some for the artistic audio direction, others for the technical audio direction, and they really helped us build the soundtrack.
So thanks to all these games and movies for that.
Another subject I'd like to mention is the workflow with the composer.
As on every game, at one point, the following question arises.
Who will be the composer and how do we work with him?
I guess every studio and production are different, and that's the same with composers.
I worked with several composers, and it was different every time.
So there's no production magic trick, no easy answers, but here's some hints to find a workflow with a composer, and what was your personal experience with the play table.
So we started by pitching the project to different composers.
We had some ideas about what we wanted and when we pitched it to Olivier Deriviere, we didn't talk about the music.
First, he wanted to deeply understand the vision of the game and what the creative director wanted to achieve.
He was really digging inside the project core.
We felt it was really appropriate to our game.
We needed someone that really understands the narrative and how the story was told, to be able to add a narrative layer thanks to the music score.
It was not about throwing music on a picture.
So on top of that, and that can sound a bit obvious, but you need someone that plays the game regularly, that's the only way to understand the interactive needs. Videos are great, but that's not what we are doing. So when you hire a video game music composer, you expect someone that understands the interactive media and constraints.
And to be honest, it's a relief for the audio team.
You know the composer will be able to provide music systems that will be applicable to your game and how to make them work in an interactive music engine.
So yeah, it's really nice to have someone that understands Wwise or FMOD, for example.
Especially on small productions, I would say it's really important to hire someone that speaks both languages, music and video games.
So if you want to work with someone coming from line art media, you need to be prepared to handle audio engine integration and editing.
You will need someone to do it.
So I guess it's better if we have the person creating the music score, handling the integration and editing.
After this audio direction and workflow talk, I'll now elaborate a bit on the technology used.
They were quite simple and relied on Wwise, so I won't spend much time on this since it's pretty much standard features that you can find in any audio engines. I'll spend more time on custom tools and specific stuff. So we use a custom in-house engine and Wwise is integrated in it.
We implement also ambient and animation audio in our editor for in-game.
where all the logic and dialogues are implemented.
If you want to, let's say, trigger music just after a VO line, this is where we put it.
So we have environmental reverb for acoustics and a Raycast array-based occlusion.
It's pretty simple, but it provided enough realism for our game.
I would say one of our most important tools was our VO manager tool.
It handled all the VO lines, WAV files and exported them directly into Wwise.
Thanks to it, we had a quick workflow and everyone was able to access and listen to all the VO files.
Narrative designers were able to write lines, they were directly exported into Wwise in text-to-speech format.
it was important that the team wasn't relying on audio people for VO implementation.
As a narrative game, VO was essential and level designers needed to iterate a lot.
As for handling all the WAV files, we got the WAV files directly from the recording studio and the VO import process was done by producers. The tool did some sanity checking on file names and so we were sure that the VO delivered were correctly named.
So that was great because audio people, we were focused on the mix and audio related stuff and not on managing WAV files. On narrative games, you can get lost managing files and stuff and this tool was a lifesaver for us because we had like 7,000 video per language.
For audio, we did some scripting with command line tools SOCKS and BS1770 to detect huge dynamic range or clipping video.
These free and open source tools are very helpful for small production like us because you can script with them and easily generate reports.
To end up the tool section, I will show you a feature we developed for Foley, a system based on characters' bone acceleration.
Following the acceleration of specific bones, it drives an RTPC, allowing us to have subtone Foley for every movement of the character.
This was really convenient, and most of the times it was enough for our Foley need.
Or when needed, we added audio events on top of it.
So here's a quick video demo.
So we had really simple but flexible tools.
Now let's talk about the main feature of the game, rats.
In game, rats are handled like a fluid system.
We are talking about 5,000 rats, afraid by light and ready to eat whatever they find on their way.
Roughly, they will behave like water, approaching and fainting like waves.
So we had to work with Olivier Deriviere to create an audio system that will be balanced between music and SFX.
We couldn't put music each time RADSphere featured on screen, it would have been annoying and we'd have loose contrast.
With only SFX, it would have been a bit boring and not contrast enough.
So we looked for a grammar that allowed us to complete each other.
We had to find when to put music and when SFX had to be alone in the soundscape, or how to make them fit both in the mix.
RATS SFX, they were here to feed back common states of the swarms.
It was an audio state machine reflecting rats' behavior.
As for the rats' audio, we used a lot of disgusting contents.
Squish, pig screams, horses hitting.
As I said before, we wanted the rats to be organic, so we used organic recording.
We were not able to go too crazy on the effects side, so rats were a blend of all this stuff.
On the other side, music was here to tell a story and showcase the rats, during cinematics or rats' brutal appearance. For the rats' music, Olivier used cello and, thanks to the virtuosic playing of Eric Maria Couturier, we had some crazy textures and fast cello lines perfectly evocating the aura of the rats. Let's have a listen.
So this grammar was really helpful to deliver a nice mix and as you all know the audio spectrum is limited and it can be tricky to explain for non-audio people in the team. You can have everything upfront in the mix. You need to take decisions and you can have an explosion, a crucial dialogue with the dramatic music on top of that. So here's a RAT video sequence where you be able to hear the relation between music, SFX and narration and how we balanced all these elements in the audio spectrum.
I'll subtitle videos so I won't be bothering you with my voice.
It's a nightmare! We have to get back outside Amicia! I know, keep moving!
Now another quick video to hear how we showcase the rats thanks to Music Stingers. The cello is once again featured heavily. When the player was hearing this cello, he knew directly there would be rats around.
In the light! In the light! Quick!
I don't see a torch.
How? How are we going to do this?
It seems to be taking care of them.
Are you sure?
Another jambon should be fine.
To end up this RATS talk, I'll show you one last video where you can see the implementation of the RATS state machine.
I'll now elaborate on another important feature of the game, stealth gameplay.
The goal was once again to reunite all the soundtrack's elements and make them breathe together in an interactive way.
Stealth sections were critical gameplay sections, and the player needed to perfectly understand the situation he was in.
In Up Like Tail, when you get caught by a guard, you have a big chance of ending up dying.
So audio feedbacks were clearly important, and on top of that, for immersive reasons, we didn't want to avoid too much UI, so audio feedback was crucial for this.
I now illustrate how to make the soundscape evolve around common variables.
Video, SFX, and music are tied to these variables, and they are evolving together following gameplay evolution.
In this example, two variables are used, the level of detection by guard and closest guard distance.
For VO, it's connected to Amicia's breathing.
When guards are getting close, she starts breathing faster, for example.
For music, music layers are evolving following the closest guard and the level of detection.
The closest you get to a guard, the more bars you'll have, for example.
SFX were played on top of that for detection state change, with the lower bit played each time she's detected.
It was really to underline state change.
The idea was to make it very immersive and connected to Amicia.
We didn't want the sounds to be UI-ish, we wanted them to come from Amicia emotional state.
This allowed us to extend what was on screen and we were a reflection of Amicia emotional state.
Let's have a look at the example.
Enough! I'm going to get you out of your hole!
So you're gone. Why, I wonder?
You again? Hey, you! Don't move, or I'll...
This was a short footage of stealth mechanics.
It's interesting because most of these nuances won't be clearly noticed by players.
If your game is played in playtest, they won't say, I like the fact that the low cello entered when I was getting close to the quad, but they will feel it strongly.
And that's when you start adding layers of emotions on top of the game, thanks to audio.
And we study on the engines such as Wwise or Fmod or Unity, we really have powerful tools for audio.
It's really possible to work on the whole audioscape.
And that's what interactivity really is.
making the soundscape evolve following player selection.
Let's get back to music and music grammar.
Creating a musical grammar is essential, but what is also interesting is to use this grammar in different situations and contexts.
It can help connecting gameplay and story for example, and that's the key for giving depth to a soundtrack.
Let's have a look at what last game sequence and how One Music Stingers is able to resonate.
all along a game section, and how a music stinger is able to serve gameplay and narration at the same time.
Look! We found it! The castle!
Wow, it really exists! Ch√¢teau d'Ambranche!
It's amazing! I had a moment of doubt, but that...
That's the Sword of Arken, it's a legend I like!
Come on, let's take a closer look!
I don't see you. I don't see how Never again Building We're very high up don't worry Hugo I tried to cover the main aspects of Up Like Tail audio direction.
It might feel like a glimpse actually, but I gave you the key elements of our soundtrack.
With Up Like Tail, we did our best to not only provide audio over GAM sequences, we wanted to really provide a new layer of emotion on top of the game that won't be only squeaks.
I hope you learned one thing or two, and if you have any questions, you can contact me on Twitter.
See you!
