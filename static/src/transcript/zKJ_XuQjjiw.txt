Okay, good morning everyone, thank you all for turning up.
Morning, Kurt.
Appreciate it's Friday morning, it's not necessarily the easiest one to attend, so thank you for coming.
You know by now all the house rules, right?
Make sure your phones don't go off, no, no, no, no, no.
Okay, I'll exit over there.
Fine, okay, so I'm Paul Weir.
I'm a composer, sound designer, basically do any audio production.
If you're going to give me some cash, I'm quite happy to do anything.
I've been around for quite a long time now.
I'm the audio director of Hello Games, as well as being an audio director for Microsoft.
So essentially I'm freelance with two jobs, which is quite nice to be in that position But today I'm going to talk a little bit about the process of sound designing No Man's Sky And the music creation process we went through as well I'm going to talk a little bit about some of the tools that we created and hopefully show them to you Some stuff will almost certainly crash. I'm going to warn you now There's gonna be a little bit of having to rig up things. So just bear with me. I'll get that in early I just again, a quick show of hands, I hate doing this, but are you familiar with the game No Man's Sky?
Stick it up.
So not everyone, that's fine.
Most people, OK, fine.
So No Man's Sky is a procedurally driven science fiction.
an action adventure game.
It crosses several different genres.
You can treat it as pure exploration.
You can treat it as an ambient experience if you want.
Or you can treat it more as a kind of proactive trade, almost a little bit like a lease in some ways.
There's no real story structure to it.
It's very open-ended.
And we have some light narrative on top.
So there's no real voiceover.
We got a little bit of voice, but it's mostly about the experience.
Hello Games, if you don't already know, is a small company. We're still a small company.
So, throughout development, there's probably between about 12...
The main part of development, when I was involved, about 12 to 15 people for the entire development team, which is obviously still quite small.
So, the audio department, there's obviously me, and that was it.
But I had a lot of help from other people, so I just wanted...
First of all, just give some thanks to some people who are involved, because it's important.
Sometimes, like, I'm seen as the audio person for No Man's Sky, but we had Harry Denham, who is a programmer at Hello Games, who isn't an audio programmer, but, bless him, helped me with all my audio issues, with the wires integration, things like that.
Super fantastic, super useful to have a coder who's on the side of audio.
Even if you're not the audio coder, someone looking out after you.
It's really lovely.
And I've got a DSP programmer called Sandy White.
So if you're British and old, like I am, Sandy wrote Ant Attack back in the 80s, I was at X-Spectrum, super bright guy.
If you've ever been to any of my talks before, I've talked about him in the past, because he often does kind of my tricky DSP problems.
And right at the end of development, I also had a freelance audio co-editor called Andy Hutchings to help profile-wise and get our performance up to scratch.
So it's just important, because I don't want to make it look like I did everything.
I did all the creative content and technical design, but obviously I had help with my friends.
So I'm going to show you a short video now, which is just a series of clips, different examples of sound design from the game, just to kind of show the range that you find in the game.
And so the obvious challenge with a game like this is both its scope and the simple fact that I don't know what a planet is before it's created.
And if you've been to any of the talks from Hello Games guys, it's the same problem.
How do you generate a landscape with all the features of a landscape without knowing in advance what the elements of the landscape are, what the landscape is?
So we had to develop certain tools in order to understand the environment around us.
Now in a normal game, if I take an example of.
things like maybe a building.
In a normal game you've got a building geometry, you know where a building's gonna be in advance, you can pre-bake things like obstruction occlusion, or you can add extra geometry to cope with that.
But we don't have any of that, so I don't know anything until the engine reports back to me.
So we had to build in Wwise a lot of systems which are very heavily state-driven and switch-driven in order to make decisions in real time.
So that's some of which I'm gonna show you some of those little tools there.
This next slide is going to be nothing new to many of you, but I think for our game in particular, it was really important that we moved away from simple one-on-one event-driven audio.
So, something happens in the game, it triggers a sound.
Now, obviously we still do that, right? It was the UI sounds that were where we do that.
and there's certain set animations that don't change, so we'll do that for that.
But wherever possible, I'm trying to boil down all the audio behaviors and all the actual audio content into as few actual trigger events as possible.
So a good example of that, which I'll show you next, is how we handle ambient sounds.
So that goes to system-driven.
If you can do it, because you do need data coming back from the game, it has to be a two-way relationship, makes it very flexible, makes it very extensible.
So for example, biomes.
Every planet type has its own kind of atmosphere, its own characteristic.
So you might have like a jungle planet or a frozen planet.
So if we introduced a whole new planet type, I don't need a programmer to do anything for me.
Literally, a programmer just tells me what the name of that planet type is.
I go off and do all my audio, and in Wwise, just allocate that as a new state, and it pops in.
Same for ship engines.
We have a new ship engine.
don't need any coding support, just add it straight in as a new state submit, it's going to be switched, like that.
And it's there.
So it gives me, as a sound designer, a lot of flexibility and a lot of control.
I'm not trying to sell Wwise.
Wwise was a good solution for us.
There are other solutions available.
So I was going to show you...
I'll get to an example in a second.
As a sound designer, though, there are certain key things I wanted to achieve right from the beginning.
And I always had in mind the sensation of being in a spaceship, but parked on a planet and having the rain on the cockpit window.
Which is very emotive.
And for a science fiction game, what I've tried to do as much as possible is to have very relatable sounds.
Like, sure, the sci-fi sounds, absolutely, the ship's a sci-fi, but wherever possible on the planets, make it familiar.
Like wind, water, birds, are all very emotive sounds, all stochastic sounds.
And through evolution, you know, we've learned to associate them with peace and with calmness.
And there's a certain kind of...
feeling we get with them. I think some of the nicest feedback I had was on Twitter with someone saying that when he was in the ship and it was raining, it kind of reminds him of being on his dad's farm in America in a pickup truck and just sitting there waiting for the storm to pass. That's lovely, that's an emotive moment.
I'm trying to manufacture those little emotive moments but I can't plan for that.
I can only build the systems that will allow for that.
I don't have a key event where I know that's going to happen.
So I was going to show you how we handle events.
So this is a good example.
I can attempt to show it to you in Wwise.
When I do that, you'll see how ambitious I am.
So we have one wise event with this name that handles all our global ambiences, all our local ambiences, the weather effects, whether there are creatures or not.
So it's a combination of, for biomes, it's gonna be state.
So you can only ever exist in one planet at a time, so that's a global ambience.
For localised ambiences, it's a switch.
So for example, you could be in a forest.
And we have to work out, through raycasting, which I'll show you, what a forest is, because...
There's nothing in the game to say I'm a forest.
Same for water.
Like, again, in a normal game, you would have some kind of emitters placed on the water.
And water's generated, or you know where the water is.
So you place some geometry, and you hear my sound emitters, and it's on top of water.
I can't do that.
I don't know where there's going to be water.
So again, we're having to calculate stuff all in real time.
And then the rain, it ended up being a bit of a joke.
I keep telling Sean Murray, but we've basically built a rain simulator because I can't get enough rain into the game.
I just love rain sounds.
So we started off with just the rain is on or off.
But as we've updated the game, we have things like, are you near a building?
In which case, play some rain on building.
Are you near some trees?
So play that.
So it's all additive.
They're all multiple layers.
I'll show you a video in a second of what happens when it's raining and you go into a building.
I'll talk over it a little bit.
But I'll explain how we're using systems, states, and then real-time controllers to kind of mix different elements in and out.
And it's fine if, as a player, you don't notice this.
And that's kind of.
I think as a game, it's one of the curious things about this particular game.
Many of the games I've worked on, you want those big moments, right?
Especially as a sound designer, like, you know, we've got fairly big egos.
So you kind of want to get into a key event and you want to fire off, you want to make it sound really awesome and go, wow.
We don't really have those moments.
We've just got long stretches of gameplay.
It's up to a player what they do.
I quite like that.
So what I've tried to do is add all these little subtle layers that maybe you pick up and maybe you won't.
But it kind of combines into a bigger thing where it makes it more of a believable environment.
Not realistic, I'm not interested in realism, but I want to give you a sense of presence in that planet or that space.
So let me show you this video about rain.
So I love rain.
So what you'll see is raining outside, I go inside.
I'll talk over it a little bit, as I say, to explain what's happening.
So as we go inside, the indoor rain will play.
And that's room dependent.
But we're now also detecting whether you're close to a window.
Just back here that little kind of rain against the window.
And as I go up here because it's room dependent then that we're gonna have rain on glass because that's a glass surface.
These are all different rains loops.
So spectacularly unexciting.
You know, you don't have to cut, it's vain, thank you.
Wait till the end, it's fine.
And really it's not, you know, it's not showy, it's just what it is.
But again, I don't know that there's a window there.
Like, we don't know.
So we have to keep detecting, am I near glass?
Is the glass a window?
In which case, then start to ramp up that particular sound.
We also do this little thing, which I quite liked.
We did it early on, is when you change states, so when you come from indoors to outdoors, we boost all the ambiences by 6 dB, just for a few seconds, just to give you that kind of sense of rush, same as when you come out of the ship.
And it's a difficult game to mix, as you can imagine with all these different elements.
So how we do that, ray casting is really important for us.
So again, like ray casting is commonly done for various different audio purposes.
We're using it to detect the environment.
So we have beams that go up to detect enclosure or enclosedness, that's not a word, but you understand what I'm saying, which is how covered am I?
And then we don't know that.
So, that's an analog value.
So if you're partially covered, I will start to change some of the sound design.
We'll have some kind of little rumbles coming in.
We'll start to have a low-pass filter on some of the sounds.
But after a period of time, if a raycasting system goes, I'm 100% enclosed, therefore the logic is I must be in a cave.
So if I'm in a cave, we'll change state.
And that will obviously affect the ambiences, it will affect the veins, it will affect all of the mix and filtering of external sounds.
Same for water, so we have to detect, am I close to water?
Is there anything obstructing me from the water?
And then we've got beams that go straight down to detect footsteps as well, whether you're in contact with the ground, because that's a bloody hard problem.
If you worked on games before, even normal games, it's actually sometimes quite hard to detect, am I in contact with the ground?
When you've got a surface which is continually changing over time.
It's a real issue.
And then we've got another beam which heads towards the sun, and I've absolutely no idea why we've got that.
Programmer put it in for some reason.
And similar for the water.
So we use a dynamic emitter system for water.
Again, wise handles for this kind of thing.
So we don't want loads of sound emitters stretched across the water.
And if you watch it live, it's quite cute, because these little emitters kind of dance around and kind of follow you around.
So we're always tracking your position so that you hear the fact that there's water.
But if you go into like a free cam and go near the water, you won't hear the water.
Because it doesn't know that the camera is near the water.
So we're always kind of just building stuff around you to give you a perception of things that are happening when it's, you know, in a sense it's all a lie.
But also we get lots of stuff for free.
So...
you come into the water, if you walk out of the water, you're going to have wet footsteps.
Because we have that data in the system.
So because we've gone so state-based, and getting all this data back, we can get loads of stuff for free.
I don't need any more support for that.
A good example, a really simple example, is storms.
Again, nice weather effects.
Behind the scenes, technically, the storm sounds are playing all the time.
But they're on a slider to go, is it really stormy?
If it's not, pull it to zero.
So all this audio is just as virtual voices.
But we know if you're indoors or if you're outdoors.
So recently, I was like, OK, well, I know you're indoors.
Let's just have indoor storm sounds.
Like, don't need anyone to do that for me.
I just need to put the audio in.
We are lucky in that.
We stream a little bit of textures occasionally, but pretty well all the streaming is for audio.
Like, 90% of the stream is for audio.
So I don't have any limitations on how big my file sizes need to be, which is splendid.
So, you know, we don't need to have a 30 second rain loop, I can just have a 3 minute rain loop.
And up until now, no one complains.
At some point, some code is going to tell me off for doing that.
So again, Sean Murray often says, like, the game, the actual game install is like 500 meg for the game and all the assets.
And originally, it was 1 and 1 half gig for audio, which is compressed.
It's kind of grown and grown and grown.
It's really nice to say, we've got a game where 2 thirds, 3 quarters of it is just audio, which is.
a win for audio people. I'm very happy about that.
Okay, so if we move on to the idea of procedural generation and as it relates to audio.
You're probably familiar with the discussions that have been going on with people talking about procedural audio.
I'm going to try and show you some examples of how we do that.
But just first of all, just talk very, very briefly on what is procedural generation content.
Just in case you're not familiar, I won't spend much time on it.
So you can define it as any kind of algorithmically created content, so any computer generated content based on some logic system. So very common to use it for things like textures, for things like landscape generation. And it's super useful where there's an issue of scale.
So hand crafting landscapes is hard and time consuming, so if you can get the computer to do it that's so much the better.
And it's not either or.
So usually you would have some procedural content, but then fully authored content alongside it.
Some of the difficulties though with procedural content, and it's quite common to do level design procedurally as well, is to give a sense of this kind of meaningfulness, like it feels like it's handcrafted.
An audio equivalent would be, and we'll come back to this, one of my big issues, is that if you have procedural audio, the perception of it has to be as good as traditional audio.
It's no good if you compromise and go, well it's procedural and it sounds a bit shit, but it doesn't matter because it's procedural.
That's not acceptable. As a sound designer, I can't accept that.
It's easier visually, you can get away with it. It's harder in terms of audio.
So there's this problem of does it feel guided, does it feel crafted, when it's not.
And also this really interesting issue that we kept coming up against in the game, which is you can generate variety, you can generate infinite variety, but as humans we don't perceive that.
We perceive this kind of sense it never really changes, ironically, because it's always changing.
So to illustrate that would be, say, sky colours.
If you have all the colours of a rainbow possible for the sky, Very quickly you go, oh, it's just a pink sky planet.
It looks all the same to the others.
It's not the same, but it feels the same, because you're seeing all the graduations.
So even though that's possible, in fact, what you end up doing is kind of chunking it up.
It's almost like quantizing it.
You go, OK, I'm not going to give you all the shades.
I'm going to give you specific shades.
We could generate all, but we're not.
And that was also relevant for some of the audio technology that I'll show you.
Yes, it's possible to do wide range, but you don't pick up what you want to use.
Ah, I haven't heard that before. That's different. I notice that's different.
So a good example of that, I talked about procedural generation.
Landscape generation, so the textures are photo textures, but the landscape is mathematically calculated, that's Terragent.
And then we get into the tricky problem, talking about audio, what is procedural audio?
Right, and I'm not saying that I've got the definition, I've got a definition, I'm not saying it's the definition, but as I've done it, I'm gonna say it's the definition, because I'm allowed to do that here.
But there's this real ambiguity, like what actually is procedural audio? What does that mean?
I have no end of students giving me their master's thesis and essays and wanting to interview about procedural audio, which is fantastic.
But no one seems clear, like what the hell is it?
So it could be, for example, physical modelling of sounds.
So that could be a good explanation.
So it's mathematically creating sounds.
And that would seem to fit quite well.
So a really simple example of that would be this violin sound, or cello.
And that's just reactor with a physical modeled stringed instrument.
We're really trying to play it on my keyboard and kind of failing.
But yeah, there's no samples, there's no recording, it's all mathematically calculated.
And stringed instruments and wind instruments work relatively well like that.
Procedural audio could involve granular synthesis.
We talked yesterday on that subject.
Or, I've often seen it as really any form of synthesis could be procedural audio.
or even I've seen people talk about just using DSP effects.
So the problem I have with that is it can't just be DSP effects, right?
Because we've used reverbs and filters and distortions, you know, literally for decades, right?
So if you're gonna say that's procedural audio, then just basically everything's procedural audio and it's pointless.
I object to real-time synthesis because from the 80s, late 70s, early 80s, that's what games were doing.
Arcade games had a little Yamaha chip, and we're creating the sound live using synthesis.
So yeah, okay, maybe that's procedural audio, but it needs to be more than that, right?
And granular synthesis, again it's not a new idea, been around in games for at least a decade, often used for car racing games, if you play Quantum Break from a time dilation audio effect is a granular synthesis effect, it's very nice.
And if we use physical modeling, is that in itself just enough?
Does it mean just I'm modeling a sound and that's it, that is procedural?
So my question is, if it is just that, and everyone's talking about it, where is it?
Where are the games that are doing it?
And I think there are certain issues.
I'll get to a definition of what I think it is.
But if you're going to make some new audio technology...
It's hard. It's hard to justify the cost.
I think it's a very good thing to do, and I think you should do it.
But it's a difficult argument, right?
Because everyone's comfortable with their tools, we have a job to do, we've got lots of sounds to make in a very short amount of time, with very little money, right?
So we need to get on with it.
And to have to say, excuse me, I can't do anything for the next six months because I need a programmer to build this for me, is a very hard argument to make.
But it's also very much a multidisciplinary problem.
So you need sound designers or audio directors who are either programmers themselves, and I'm not a programmer, or very technically literate, and are able to define a problem for a programmer to solve in a way that is coherent and a programmer can understand.
And many audio directors I know are very good.
Not everyone is very good at that.
Some people struggle to communicate in an effective way with coders.
There aren't any commercial tools yet.
There are some very, very nearly out of market, but there's still a lack of off-the-shelf tools to build things.
Yes, you could use Pure Data or Max MSP, Flowstone is another one, but they're difficult to implement in games.
They're not really game ready.
And of the existing procedural audio that I've heard, they tend to pick off a really obvious simple examples.
I'm not criticizing for that.
But it does tend to be things like wind and rain and sounds like that.
Now, as a sound designer, I object to that on a moral basis, because it's almost like an...
It's treating sounds as a function rather than a creative, emotive element.
So I've already talked about rain, but wind?
There's a thousand different types of wind.
It could be the gentle wind in the trees, which is reassuring.
It could be the winter wind through a crack in the door.
It could be the spooky winds that you get in a horror film, right?
It has a character, has a life to it.
And that's what we do.
We go off and record these sounds.
and we go, all right, that's going to make me feel a certain thing.
And so to have wind that is essentially filtered white noise, and I'm doing them into service, but you get the point, is saying, okay, there's wind, right, we've solved that, let's move on.
There's a bit of noise, and we add a filter to it, and that can be our lane, and we'll move on.
And there's a place for that, but it's not where I want to go.
So what I'm saying is that procedural audio to make it different, to make it procedural, it has to be driven by the game, right?
So there's something where it's not just, here's a sound, play a sound.
It's, what are you doing, game?
I'm gonna react to that in some way, and that's gonna be reflected in the sounds that I'm producing.
In order to do that, it has to use some form of real-time generated sound.
So maybe it could be recording with multiple layers of DSP, or maybe it could be real-time synthesis.
And what you're trying to do is avoid a sense of repetition.
Again.
I'm not looking for realism, but I am looking for something to be believable.
And one of the huge benefits, if you're generating all this in real time, is you can avoid repetition.
And finally, I really like the idea of making use of emergent properties, so that you end up with all these inputs coming into your sound generation, and it starts to do things you don't necessarily expect it to do.
It becomes a bit chaotic.
That's hard sometimes because we want absolute control over how things sound.
But it's also really freeing to go, actually I don't always know what it's gonna do.
And like for our game, we agreed that sometimes it may sound really awful.
It may do something that sounds a bit broken, but that was an acceptable trade-off in order to give us the range and flexibility.
So I'm gonna attempt to sum that up in a definition.
So I would say that procedural audio is the creation of sound in real time using synthesis techniques such as physical modelling with deep links into game systems.
Welcome to discuss that later, I'll take some questions on that.
But that's my attempt to at least define what I think procedural audio is.
So let's move on to an example of that and what we try to do for the game.
So we built a system in order to synthesize the vocals of the creatures.
And that was a very, very early decision because I knew that was going to be a problem.
Doing creature sounds is tricky in normal times, but to do it for such a massive variety of different types of creatures, different sizes, different appearances, I knew that was going to be hard.
Sure, we could use recordings, but we'd have to use a huge library of recordings.
So literally from the beginning when I was involved, I was like, let's attempt to synthesize it.
Because synthesizing a vocal tract is a known problem, right?
It's done in medical science, you know, we wouldn't be the first ones to do it.
So that in itself is reassuring.
I'm not trying to cover new ground.
I'm trying to take what's already there.
And We ended up building it as a Wwise plug-in, which has its pros and cons.
Wwise is getting better, but it's still a little bit limited in its synthesis capabilities and building a synthesizer plug-in.
It's much better with just processors, audio effects.
Anyway, we're all at working.
But we also quickly realized, and this is a key part to procedural audio, that it's not just synthesis, that there has to be a performance aspect to it.
Something has to drive the synthesizer.
And if it's an engine sound, it could simply be how fast am I going?
But if it's something like creature vocals, where's that performance going to come from?
So you can try and mechanize it, which is one of the things we did, and I'll talk about it.
Or you approach it from a different angle.
So in the game, if you've played it, all the creature sounds are created using Valkalien, including all the bird sounds and the ambience in the background, which I've never mentioned.
And I think they're rather nice.
And that does give us flexibility to go, it's nighttime.
Change what the birds are doing.
Like, literally change your vocal tracts.
Birds don't have vocal tracts.
But change the sound that birds are making, depending on the time of day.
Make them tweet less, make them sound like different creatures.
It gives us all that flexibility.
So RockAlien sits within Wwise, and I'll show you the structure of how that works.
It has to be low latency, it has to be really low CPU, because it has to run on PS4 after multiple instances.
And it's a game.
It's a console game.
It has to be super reliable.
And that, as we know, developing code, particularly chaotic DSP code, that is also 100% reliable, as close as we can get it, is a difficult challenge.
So it involves lots of testing.
And when we're building it, one option is to go with digital waveguides, which is a known technique where you pre-compute resonant frequencies, like in a pipe, it's typically used for that.
That's problematic because the patents are a little bit ambiguous.
I think Yamaha owned the patents, and it's not clear if they're still applicable or not.
So we decided not to go that route.
So what in the end VocAlien is, is essentially a pipe.
and the pipe has a reed at one end.
There's no traditional sound generator in it.
So there's no sine waves, there's no noise generators.
It in itself is not generating the sound.
It's much like feedback from a microphone.
You wouldn't say, where's the sound generator in feedback?
So it's a bit like swinging a pipe around your head, and it whistles at you.
So essentially what we've got is a feedback loop with resonant points within the pipe.
But because we've got the reed at one end, then the air is travelling backwards and forwards through the pipe as the reed pushes it back in and out.
So there's a whole feedback, literally a feedback loop in it.
And if you stimulate it enough, you'll start to get these, literally these points of noise, which ways, where you start to use really sharp filters, you can shape into what sounds like, sounds like essentially sounds, added with a kind of mouth element at the end, which is your phoneme filtering.
So if you're familiar with phonemes, it's all the aya, wawa, right?
Just moving of your mouth.
It's just a filter, right?
And phoneme filtering is very well understood.
So we could apply all the typical phonemes, in fact, invented a couple of new phonemes because we're alien creatures.
But it's also super important to have a sense of movement.
So not to create a pipe that's just static, that sounds really artificial.
And so our system has to be, also has to be chaotic.
So we've built into it the fact that things are always gonna be jiggling around, like values are gonna be jiggling around.
And that is the hardest point of anything we've developed.
It's not creating the code, it's not the implementation, it's the fine-tuning of those algorithms.
You literally have to sit there, Sandy sat there, literally fiddling with numbers to get those points where it sounded acceptable and it had a wide enough range, but it didn't suddenly explode and just create bursts of noise at you, or howls of sine waves and things like that, which it did during development.
I am going to attempt to show you this live.
It should work.
it's a high probability something's gonna crash, okay?
So I'll show you an image first, but you have to give me 30 seconds while I fire everything up, because if I keep it fired up, it will crash.
And you'll get the idea, okay.
So Valkalien sits and whirs, that's where all the DSP stuff happens.
But, how do we control it?
And it's really tricky to control it.
Effectively, we've created a wind instrument.
So we used an off-the-shelf MIDI control system and built our own MIDI control service that sits on the iPad.
So it is a synthesizer, and this is the MIDI control surface, which I will attempt to get up.
And there's actually multiple tabs that you can see at the top, but that's the main page.
And just to very quickly run through it, This enables me to, for example, change the length of a pipe.
We're actually not one pipe, we're four different pipes stuck together, right?
To create added complexity.
It allows me to add some filters.
We've got these abstract terms of harshness and screech.
And that's kind of like the roughness of it.
It's like the energy going into it, how rough that energy is.
So if you keep all those low, you'll get a very kind of pure sound.
We have, so I said we haven't got traditional synthesizers, we have just got some noise generators.
but they're effectively used as frequency modulators.
So again, it gives it kind of this shakiness, this roughness you wouldn't otherwise get.
And then traditional volume envelopes, attack, so how long the sound comes in, release.
And then the really fun stuff is all those presets, the format filtering, which is your phonemes, and the energy, so bottom right there.
So that, we usually play this without attaching to a gyroscope, which is why we went for the iPad.
So you can, as I say, you remove the iPad, you can change what the sound does.
I would say one of the really clever things that Sandy did was preset morphing.
So again, if you've used React, it's the same kind of principle.
Each of those 26 slots can be a different creature sound, but we will interpolate between each of those sounds.
So we could use it, and we do use it, to say a creature is happy, it's just burbling away, it's giving little wails.
through to the other extreme where you're shooting it and it's dying and it's screaming at you. So they're different setups but we can slide that morph slider and you'll hear that as the performance and we capture all that as well so I'll show you how we capture it. And then at the Wwise side, you won't be able to read this oh actually maybe you can. This is what we see in Wwise which effectively like the meta controls So it's like, how big can this sound ever get in terms of frequency range, how small can it be?
So we're just kind of bounding it a little bit, just to give it a little bit of control.
And it is, you know, this could be done much, much better, right? It's just a bit of a mess.
We need to evolve that. So let's attempt to get this running.
Well, I will say that if it crashes, it's not Wyser's fault.
It's not even our DSP.
It's because we have to do this flaky Windows things of getting MIDI into the PC and routing it internally.
And all the MIDI is bidirectional.
And it's really like Windows really didn't like that.
So if you look at it in one way, it will kind of fall over.
Never do live demos.
What I can do though is, it's not as impressive as seeing a control surface, I'll get this working after the session, I'm not going to waste your time and you can come and see me and you can have a play, it's fun, it's like playing an instrument.
What I will show you though is...
It's less impressive through here because I can't perform it, so you're getting one static sound. This is it running live.
Obviously you're performing it as an instrument.
What we end up doing is capturing that as MIDI.
So each emote state, and we have about 10 emote states from idling through to little growls, through to dying.
Each of those is a set of MIDI performances.
So I said before, but the problem of how do you perform it, well it's performed because I perform it.
So it's performance capture.
But we don't literally use the MIDI.
So we jitter the MIDI a lot.
We shake the values around.
We do things like change the speed and things like that.
So we load the MIDI file into Wwise on here.
So what you're hearing straight from wires is just as a static preset When I perform it when you play it in the game What you're also bringing in is that performance capture element Which is moving those morph targets and moving kind of the energy in it and it becomes more more lifelike Not always quite where I'd like it to be This is my cat which sounds a bit like a fart And I've got this slide on size But even just triggering it straight with no performance data in it at all, it will always sound a little bit different.
I can try and apply the performance data.
That's because he's very small, so if I bring him back down a bit.
So you get it sounds, so that's pulling in all that different bits of data.
And if you're that way inclined as well.
I can show you on profiler.
It's quite jibbing.
But it's using less than 2% it's about 1.5% per voice on a PC.
Just as good on PS4.
Okay, so that's the structure of how we work.
We've got the control surface.
Not working.
That bidirectionally communicates with Ys, so that if I change a preset on here, that sends information to Ys, but also change value in Ys, that goes back.
We need it hooked up into some kind of door, or sequencer, in order to capture MIDI performance to bring it in.
Then on the game side, we've got those MIDI files, and optimizing MIDI files for games is actually surprisingly tricky as well, because we can't take in pure MIDI.
running into Rock Alien and then the game is sending all the RTPCs to go, or some of them switches, to go, what creature type am I?
So we have about a dozen rough archetypes of creature. Am I a crab-like creature?
Which can be very different. Am I a rodent-style creature? But the sizes will vary from minuscule through to, you know, a dinosaur size.
So therefore we've got the RTPCs there, the controllers to go, what size am I? And also what's the ratio between my head and my body?
So that changes how nasally the sound's gonna be.
So there all the values are coming in.
When we were thinking about how do we perform it, so initially we thought about using Perlin noise because it gives us Perlin noise already, but that's a very kind of regular noise shape and it sounds very artificial.
It's interesting, if you see it landscape generation, people who know about Perlin noise will pick it up.
But otherwise it looks okay.
But when you hear it, you can immediately detect that it was regularities and it doesn't work so well.
And also it's hard to kind of create a sense of this little, the jitters that you want to hear, that you're expecting.
You don't really get that in Perlin.
So I've kind of covered this in a little bit of time, so we'll move on a little bit.
But certainly a potential moving forward, and because it's a super trendy subject, and so obviously I have to have a slide on it, would be some kind of machine learning.
That would be much more optimal, so that we can train it to go, this is what a creature dying sounds like, this is the kind of thing I always do.
I always shake it in this way, I always throw these sliders this way, and now derive from that, you can do it through Markov chains, derive from that a typical performance, and then let it run.
That'd be much more efficient, we'll use less memory, and give us more variability.
So to be critical, and I will be critical of what we've done in the game, my performance capture in the game isn't always great.
Like, I play the game and it makes me cringe sometimes at what the creatures sound like.
So hopefully in a future update I'm going to squash all that and redo all the performances.
Because it is like learning an instrument.
And when you're finishing off a game, obviously it's chaos, and you need to get everything done and everything in and working, and we didn't spend enough time on that performance.
and it was also incredibly cumbersome.
Now we've improved all that, so the whole pipeline is much smoother now.
And it's not, you know, the synthesis element is not that bad, but as you've seen, like, there's all this surrounding cloud of things that needs to be plugged in.
That's not trivial.
That takes time to learn, to kind of iterate on.
I'm gonna skip over this, because I want to talk a little bit about music as well.
Okay.
So for those of you who've ever come across what I've done before, you'll know I'll often build generative music systems for games or be involved in them.
Essentially, they're glorified random file players with a little bit of logic stuck on top.
I like doing that stuff. I do a lot of commercial-style installations for things like shopping malls, airports.
I've just done one for a cruise liner.
Unfortunately, I don't get to go on a cruise liner. That would be nice.
So we thought, this is a perfect opportunity to again build a system for the game.
Why wouldn't you in a game like this?
And we had a band called 65 Days of Static, who wrote the music.
Sean Murray's a big fan of the band, and from the beginning wanted to get them involved.
And we were kind of really adamant that as a band, I did not want to interfere creatively with their process.
So we always said, write an album.
Like, just write us an album, and we're not gonna start telling you how to do that, because you know how to do that, because you're the band, right?
So we let them go off and write a very traditional album.
but in the knowledge that we were going to come back to it later and just rip it all apart.
So it wasn't a question of saying, just give us the stems and we'll do it.
It was like, no, no, no, go right back, do us more performances, take out bits, give us more drum loops, perform new guitar riffs, create new stuff based almost like kind of remixing your original tracks.
So in the game, you don't get the album.
The game soundtrack is bits of what appear on the album.
It's lots of bits that aren't on the album, but it feels relatively cohesive.
That's our album.
So again I've got another little video which shows you some of their music so you get a sense of their style.
Most of this music is not generative, it's just pre-rendered sections that we have in the game.
And this is what I was saying about working with a band, you know, just write the album, but keep all the bits.
So we built the system after they'd written the album, but we just had in mind how we were going to work.
So our music system is called Pulse, and I will briefly show you that.
And really what it does is it takes individual WAVs, individual elements, and bundles them together into what we'll call an instrument.
And we have sets of soundscapes.
So we have planet soundscapes, space, wanted, which is combat, and the map, and just some of the special cases we have as well.
uh... uh... currently this sixteen soundscapes versus sixteen sets of soundscapes so that works out about almost fifty actual soundscapes in the game uh... got more to come And all this sits in Wwise as well.
So our tool is ugly, but it's very functional, which is what you need in game design.
I'm not going to make a pretty tool.
It's a waste of time.
And so we have instruments.
We have a little audio editor as well, in case you ever want to use it.
So someone could create a stem, literally a stem, and we'll automatically edit it and trim it all up, chop it all up, and stick it in to our system.
But really the key bit is we have this canvas on which we place these groups of sounds and the X and Y coordinates are just controllers.
The controllers are dependent on what's happening in the game.
So this is like, it looks complicated, it's not so complicated.
This is the map music, which is actually what I did, and it's all directional.
So literally it's directional, it's in space depending on which way you're facing.
different elements of music will come in and out, different drones will come in and out.
If you're moving or not moving, that also affects the mix.
But if you're on a planet, it's different. On a planet, it's how close you are to a building.
Or if you've been walking for an amount of time and nothing's happened to you, we'll start to raise the level of interest, just to add something to that environment.
So that's how we compose all the music in the game. Now, I should be able to...
So that's pulse up there. What I am going to vainly attempt is to run the game.
Again, dangerous, especially for so little time.
And this music it plays, again that's randomised, it's rendered, but we've got like 50 different options and it will just randomly choose. Same when the game starts, it will randomly choose a piece of music. You have to wait for the loading sequence. Blah blah, blah blah.
And it's doing all this jittering because that's all the universe generation process is what it's doing. It's not loading, it's generating all the maths.
So I think it's quite clever we're able to render a Starscape at the same time.
And this is the engine running, this is not artificial, this is the game itself running.
So we have, like most games, we've got our own editor tools, we've got some music systems.
I have to wait for this piece of music to finish before it kicks in the soundscape.
Fine.
But what you can say is I can choose from here what soundscape set we're going to play, where we are, and then I can manipulate where we're sitting within that.
So once this finishes, and this will relate to...
Don't worry, it's still running.
To this.
So if I go to variation six...
On planet.
So it will start to play this soundscape.
I was kidding off now.
And we have a little bit of debug information, what's playing, how many voices are playing.
There it goes. ♪♪♪ Hopefully if I increase the interest level. ♪♪♪ I think actually that was a high interest level.
Let's try a different one.
So we could have one, two waves, we could have 60, 70, 100 variations of each class of sound.
And when you start the game, we just create a random playlist and it will just randomly shuffle all the different soundscapes.
It's not based on what type of planet it is, it's just a random playlist.
But we'll also look at how long that's playing for.
So if you've been on a planet and it's played for more than, I think it's about 15 minutes, then we'll wait for the next sunrise or sunset and then start to kind of change the soundscape based on that.
Or if you go into space and come back down, change planet, we'll go, okay, we've played long enough, we'll just change the music.
I'll play it once and maybe...
Drums are tricky if you've ever built any of these systems.
Like having a loop of different drum loops that actually stay in time.
When your game engine is maybe dropping frames, so often we have to run on our own threads.
We have to be really super careful.
Actually it does work really well until the game completely explodes.
OK.
So to conclude, because we're limited on time, No Man's Sky did offer a really interesting opportunity, almost a unique opportunity, to kind of play the idea of randomness in sound.
I gave some examples I could have shown you, but I shouldn't limit on time.
And I, personally, as a sound design composer, I really enjoy ceding control for computer and just letting it run, giving it rules and letting it run.
I think there's a really wonderful potential in procedural audio, but I don't think we're at the point where either the tools are there, or that we really realize what that whole process, the whole pipeline is.
I think that process needs to mature a lot.
It's easy for me to say because I had the opportunity to do this, but it is wonderful to be able to develop your own auto technology.
I said at the start, that's hard.
But if you're able to do that, if you can convince your producer, if you can have a DSP programmer for a few months or even longer, I think it's an important part of our jobs as game auto people to not just accept what's there, but to be able to build new things if we can.
And I often get asked if our tools would be commercially available.
You wouldn't want them.
We wouldn't want to support it.
And the fact is, an awful lot of this, in a nice way, is smoke and mirrors.
It's not as complicated as it looks.
I've shown you what it is.
You can do it yourself.
There are lots of bits to do.
But it's really not that hard.
It's just more understanding the process.
The actual coding, I'm not saying it's simple, but it's not a massive hurdle.
So I better end there, in fact I won't take questions because you are late, sorry, but I will hang out outside, I'd be very very pleased to talk to you.
So thank you all for coming, it's lovely to see you, thank you.
