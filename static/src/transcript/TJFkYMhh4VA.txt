My name is Martin Pichlmeier.
This is Jessica Hemmer.
We've split this talk into two parts, and I'm going to talk about expert feedback, and Jessica is later going to talk about peer feedback.
I was asked to kindly ask you to turn off your mobile phones and any other noise-making devices.
And I would also like to ask you to fill out the evaluation after this talk.
If it's not in your inbox, it's in your spam folder, and it's a way of criticizing us, which is kind of.
making everything into a nice circle.
I will spend the next slides introducing myself.
I am a person that wears a bit too many hats at times.
I am a co-founder of a small indie studio called Broken Rules that has recently made a game called Old Man's Journey.
Has anyone heard of Old Man's Journey?
Yes?
Actually quite successful game and we're very proud of it, but I wasn't very involved in the development of that game.
Because I'm in the moment heading a games program at the IT University in Copenhagen, which is one of the, I think, oldest games programs on the university level in Europe at least.
Here on this image you see students working on a game that is actually nominated for, or it's not nominated, it got an honorable mention in the IGF student category, which makes me very proud.
And I'm also teaching mostly project-based classes there actually.
So whenever there is a class where students make an actual game, this is what I am teaching and this is also why I came to the university with an industry background.
When I'm not doing that in my abundant spare time, I'm actually trying to push games a little bit forward with the Copenhagen Game Collective, where we co-organize events or help events to be more interesting.
Or just weird games projects now and then and try to just break games until we can reassemble them to be something new.
But let's talk about critique in design education.
I guess most of you are educators or somehow as educators interested in the critique process.
So I'm not going to answer the question why it is important.
You all know why it is important.
I'm just going to frame it a little bit with design philosophy and one day I will submit the design philosophy talk to GDC because I think it's actually a super interesting topic.
So my understanding of design as a process is that you work on something and while working on it you realize what you're actually working on.
This wonderful illustration is based on the PhD dissertation of Henrik Gedenreuth and he bases it on John Dewey's pragmatist philosophers.
So, the first thing that you have to do is to create a design process, and this is a very simple notion of doing for the sake of knowing. So you execute an action in order to learn something about something. And that is, of course, also the problem with design processes. It generates, they generate new problems while you're working on them. And critique is one of the ways to constrain the design space, to help staying focused and to serve many other purposes in the design process.
One additional problem with design processes is that in hindsight, they look very linear, but they are not while you're in them.
So this is the development of the player character in different stages of the game.
And yet it moves.
It was made by Broken Rules, which is actually an interesting story, because I was one of the supervisors of the team when they were still students.
And later, I formed a company with them.
Or they formed it alone, and then I joined up with them.
And it looks like a linear development, but there were so many stages of errors and dead ends until we arrived at this final character that the design process more resembled this image by Dan Cook from Spryfox, who has an interesting article also associated to that image.
His blog is anyway fantastic.
So in the design process, you actually expand the scope of what you're working on, and then you contract again, and then you expand again, and then you contract again.
And the purpose of that is, of course, that you accumulate more and more what he describes here as good stuff.
Now, I've never seen any product that ends up with only good stuff in it.
So it's a bit romantic, maybe, this image.
But in general, this is the iterative process you're working in.
And critique comes in at several places.
So while you are expanding, one of the purposes of critique, of criticizing the designer that is at work, or the designers.
is to give them new perspectives, for example, because you bring an outside view to the table.
Then what I do a lot actually, because students are at university and they never get the chance to play in such a safe environment again when they're out there in the real world, they cannot experiment that easily. So I try to push them a little bit at least outside their comfort zone, because I think that is actually very enriching for them and is one of the purposes of education.
And of course, by just having more experience than them, I can also contextualize or help them to contextualize their work and to learn about in what environment they are working.
Then while the design process is contracting, you can help with critique to get students into a position where they have an easier time making decisions, design decisions.
They learn things like throwing away stuff that they have worked on, which is one of the most important lessons you can learn in design, and you can support that by criticizing them.
And of course, scoping and framing, and just generally keeping the direction is a super important aspect of the design process that you can support by criticizing the student.
And finally, of course, you also have to motivate them now and then, because as we all know, projects take a long time and you lose the motivation when you have a week where nothing happens.
And then critique can also be something purely positive or mostly positive by just motivating students.
And then one of the final purposes of critique is that also it enables students to reflect on their design process and to understand how design processes work in general.
For example, design processes are messy, as I said before.
You have to throw away things that you have invested time and energy into.
You go off on a tangent and then have to return to the main road that you were actually driving on and so on.
It is never a very linear or really only in a very boring case is it a very linear process.
And that's completely normal and something that students have to learn.
And realizing that leads to a process of reflection that makes them more aware of their own practice, turning them into reflective practitioners, a term that was introduced by Donald Train, by the way, in his fantastic book.
And finally, of course, just by presenting their designs over and over again in unfinished states and being aware of in what stage of design they are at any given time, they get better in their communication.
Especially if you consciously disframe how they are presenting their works.
And that is a very important skill to learn.
And there are different ways of talking about, of setting up a system of critique.
So on one hand you have peer critique, which Jessica is going to talk about in different forms.
Then you have plenary critique, which is a mixture of peer critique and expert feedback.
So if a student is presenting to the class and gets feedback from you, maybe the TAs, and other students, then suddenly you have a mixed mode of critique.
And then of course there is just one-on-one or one-on-N supervision.
And there is grading, which is another very distinct case in that it has—well, of course students can, I guess in your university too, they can file complaints, so it's not like grading is only in one direction, but in 99 percent of the cases it's a one-directional process.
And it's a very— The bluntest form of critique I can imagine, actually.
I could give a whole talk about criticizing grading.
But it's something we have to do still.
And it's still a form of feedback, a form of critique for the student's work.
So yes, to summarize, in the beginning you support the student to open up the design space.
Then during development you support that it becomes more focused.
And after the project you actually support the reflection on the process by the student.
Now in my next slides, I think I'm way too fast.
In my next slides, I will actually.
go through a couple of challenges that we all regularly face in the process of critique and offer some kinds of solutions to them.
But the important thing is while I try to make a mapping between one challenge you face and offer one advice of how to deal with that, in the end, of course, you should follow all the advices or at least come up with your own reflected process of criticizing students.
because they don't work in isolation and some of them are actually the answer to a couple of challenges and others are actually not just a part of the answer to the challenge I'm presenting.
So it's not as straightforward as it might look on the first glance because slides make it simpler, make it appear simpler.
And I will use some anecdotes from my own practice in order to underline what projects I was facing.
So this is Player Unknown Battlegrounds.
I guess a couple of people have played it.
I've heard it's something like 30 million or a crazy number.
And it came out about a year after a group of students I was supervising was presenting their idea on how to fix the problem that multiplayer games play very differently depending on the amount of players in a level.
And their solution was actually the opposite of Player Unknown Battlegrounds.
The solution was to expand the level the more people would join the server.
And they would write procedural animation algorithms to actually expand in a useful way.
Which is technically a very interesting challenge, but it is just not solving their initial design problem.
So their problem of the game not getting, or the game having to adapt to the number of players in order to stay interesting.
was exactly not solved by that approach.
And I had somehow completely failed at keeping them focused on the problem they were actually trying to solve.
I was facing challenges, and one of the challenges was they had different goals to me.
They actually, I thought, their goal should be to solve the problem of scaling multiplayer games.
But they thought they had actually come up with a particular solution for a particular use case, and that has to be great.
It wasn't.
And I would have to criticize myself for that.
But it is one case of students that were just not receptive to what I was telling them.
But other challenges we are facing are, for example, students that are minimaxing or maximining their efforts.
So they try to get the maximum return on their investment, with minimal investment of course.
or they try to just get through with the minimum effort.
I guess you all had students like that.
And it's not easy to work with them.
What works for me in any case, but in particular in that case, is actually getting them truly engaged by asking them questions about what they're working on.
In order for them to not be satisfied with just delivering or maybe they are not even able to find out what they have to deliver to you.
So they cannot just sneak through with minimal effort.
If you do not give them the opportunity to find out what the bar is, they have to jump over.
But instead ask them to get thinking on their own.
Then for students that haven't done anything, the thing there is One thing, of course, as I said before, design processes are messy sometimes, actually, having not done anything has very good reasons. You need a week to just, a week to, where you cannot come up with, now and then have a week where you just cannot come up with any new ideas. It's a very normal thing in a design process.
And your criticism of student projects should reflect the fact that sometimes doing nothing is a very productive thing and sometimes not coming up with the correct solution is still a part of a process that can lead to the right outcome.
So your critique, your feedback to any work that students do should kind of match the development stage or the design stage that they are in, in any case.
With the students that worked on this anti-player on battlegrounds idea, I think the mistake I made is that we didn't talk about the core of their problem explicitly enough.
So just drilling down to what is the problem they are actually trying to solve was just – it did not happen because I was not reflective enough in the process of criticizing them by myself.
So I should have actually asked them.
the right questions that would force them to actually confront themselves with what problem they are actually trying to solve.
Well, we have evaluation systems.
Denmark is huge in evaluating everything, especially education, and of course students are allowed to give feedback after the process.
So in this case, after they had finished the master thesis, they filled out an evaluation form and one of the feedbacks I got from them was that I kept telling them the same thing over and over again.
And what I told them was that if they want to solve the problem of an expanding level, what they should do is make a super simple 2D prototype with barely any gameplay, with just the bare necessities, and just try out if their solution is actually the solution to their problem.
They didn't. Instead, they started with 3D and had all the problems that 3D games bring with them.
Despite the fact that there are a lot of well-known, big AAA productions out there.
And also a lot of small games that started out as simple 2D prototypes.
So obviously I was not very effective in communicating to those students.
One of the things was that it was very obvious that their team leader took any criticism that I presented extremely personal, which maybe I could have mitigated by separating much clearer between criticizing the product, the process, and the designer.
I do not know if it is even a good idea to ever criticize the designer.
They are students.
But what I do know is that in that case I should have criticized the process most because the product was something that was so, they had invested so much, it was so dear to them that it was like criticizing the designer.
But the process was something that we actually could have agreed on together and that we could have fixed in order to fix their product also.
So I was kind of targeting the wrong...
I was just targeting the wrong thing in that case.
And that made communication very difficult.
Then if students, a lot of students, so we, as I said, my university is in Denmark, we are an English language program.
We have students from I don't know how many countries.
And most of them are not English native speakers.
That's a huge exception.
It's somehow, sometimes they are very bad at articulating what they're doing and you have to have a lot of patience with them.
Sometimes I am bad at articulating my critique, but what I have learned is that it is very hard to formulate critiques so that the positive and the negative comes on equal footing because it's not about, not only about what you say, it's more about how it is received actually, which is a difficult rhetorical exercise.
And finally, a very simple trick if students are easily demotivated or also if they're easily offended is just saying something nice first or maybe even sandwiching the critical part.
Those things work.
Let's quickly talk about an example where the process of critique was actually more fruitful.
And Ed Moffser mentioned it before, the game done by Broken Rules as their first game.
Bila was supervising them and later joined them as a partner in the company.
They had one issue where they wanted to make mouse-based, mouse and keyboard controls, whereas I wanted them to make only keyboard controls.
Because they made a platformer, a platformer should be played with a gamepad.
The easiest equivalent that you have on a PC is, or the most straightforward equivalent, is the keyboard and not the mouse.
Well, in a way I was very right.
I was very clear and told them in a very concrete way that they just should.
do that instead of having the mouse because it will be easier for people to play.
Funnily, later they reintroduced the mouse controls for when they ported the game to the Wii and suddenly their old control scheme was actually exactly what they needed for that port.
But in that case I could convince them because I just demonstrated to them that this is the established practice for platformers. And they believed me, or they didn't believe me, they believed the numbers, they believed the experience out there.
A trick that you can always take if you're criticizing students is just taking a step back and kind of aligning yourself with the student on a meta level.
I sometimes do that even when I'm grading, that I actually agree if I have very different projects in one class and have to grade them according to the same standards.
Then my standards are I agree on individual goals per project with the student in an explicit session, supervision session, where we agree on that.
So aligning goals can be something that is done in a very explicit way.
And then finally, should all feedback based on models and guidance be revealed to students in advance?
Especially Danish students seem to be very fond of that they like the formal structures of courses to be very clear.
At the same time, of course, you lose a bit of flexibility that way.
So it's not something I would universally recommend, but in the end, the process that leads to your, that encompasses your critique should at least be clearly communicated to the student.
Well, I already said that I have a very distinct relationship to the game and the other novice because I supervised it and then later I became financially invested in it in a way.
So the fourth challenge in critique is, of course, yourself.
In this case it was so that I actually got involved in that project that it was very clear that criticizing the game was criticizing my own supervision practice, it was criticizing myself, which is something that can happen very easily and the only solution that I actually have is that I'm nowadays very conscious about that and that I'm very actively trying to give students a lot of freedom in order to prevent that I get too attached to any project.
Because as a teacher, I think I shouldn't claim too much authorship over it.
It bent okay with the moves, but that is, I think, not the normal case.
So it's difficult, unless you explicitly set up a project to be a collaboration with students, of course.
But that was not the idea back then.
It should have been their game, maybe even more.
And finally, I'm standing here telling you how to criticize student projects, and at the same time I have a huge problem with imposter syndrome.
I feel like, who am I to tell you anything?
And I don't have a clear solution for that.
I'm pretty sure that there were enough talks about that.
There have been some at GDC.
And I did a quick Twitter survey to find out if XKCD on GDC slides is OK.
And it was like 84% agreed that it is actually totally fine to have an XKCD joke to end your slides.
But there is no solution for imposter syndrome.
The only solution is, well, obviously I have been teaching and supervising game projects for a while.
It must be 18 years or something like that.
So I guess I have learned something along the way.
I just have to trust that, and you have to trust that too.
OK.
So with this, we have covered at least some aspects of the expert-based critique, and I will hand over to Jessica, who will talk about peer feedback.
Hi.
I'm Jessica Hammer, and I just want to say that one great approach for dealing with imposter syndrome around providing feedback to students is not to do it.
Make students do it instead.
And that's one of the hidden benefits of peer feedback in the game design classroom.
Just for context about who I am, I'm an assistant professor, jointly appointed in the Entertainment Technology Center and the Human-Computer Interaction Institute in Carnegie Mellon University.
And I'm also a game designer.
And what I want to talk about today is these two formats for critique that involve students providing feedback on each other's projects.
And I want to start by talking about why peer feedback is valuable.
The value of peer feedback is actually different from the value of expert or instructor-led feedback.
And I'll say I'm framing this as student and instructor, but this can also work in peer designer situations.
But the student or the peer has actually two roles in a peer feedback situation.
They're receiving feedback from peers on their project in their role as a game designer.
They're also providing feedback.
And of course there's also a third stakeholder who is the instructor, right?
You're in the room while students are giving peer feedback.
And so feedback receivers, when students are in this role as feedback receivers, peer feedback actually helps students improve their self-assessment skills.
Because students are much more likely to be critical of feedback that comes from someone they don't see as, the great expert is giving me feedback.
It's, yeah, what do I really think about this?
It's easier to understand because there's less of an expertise gap.
The peer is more likely to be closer to their level of understanding and use a similar intellectual framework.
And allowing peers to give feedback.
There are many peers.
There is only one of you.
And so feedback can be diverse and copious and timely as opposed to students sort of waiting in line until you're ready to get to their project.
As feedback providers, peer feedback trains students in the norms, values, and practices of your discipline.
So it actually helps students recognize high quality work and internalize standards because they have to practice taking on this role of someone who is the evaluator or the feedback provider.
They're not just watching you do it, they have to do it themselves.
And finally, peer feedback is really valuable for you as an instructor because it exposes student reasoning, helps you see where students are at in that process of internalizing norms, values, and perspectives.
And it can also help a critique or feedback process scale to larger classrooms, right?
So an intimate studio class might be like 12 people with peer feedback.
You can actually have seen peer feedback work in classrooms up to 200 students, which is quite a remarkable shift in scale.
But peer feedback also has some challenges around engagement, quality, and reflection.
Around engagement, you sometimes see students check out of the peer feedback process.
Interestingly, students also...
get over-invested in the peer feedback process.
So going to sleep, whatever, I'm just not going to do this peer feedback thing, that's one sort of engagement-related failure state.
But students can also get hung up on arguing with each other in the peer feedback scenario rather than paying attention to the thing that they're supposed to be responding to.
Right?
Oh, I think you should make the character larger.
No, I think you should make the character smaller.
Boom, the students are off and the designers are not getting useful feedback.
Quality of peer feedback is also an issue.
And actually, this has less to do with students not being expert game designers and more with students being very reluctant to be critical.
So we've analyzed samples of thousands of peer feedback comments.
And very, very, very few of them are willing to directly criticize the work at hand.
Instead, what students will often do is jump directly to advice without providing any justification.
Here's what I think you should do.
Why should you do it?
Because I feel that way.
And often couched in these ways of like, they won't say anything negative, but they'll give you some advice about how to change your project, but they won't tell you why.
This is actually extremely unhelpful for designers to use to iterate their games.
And then finally there are issues with reflection, right?
So when you get a pile of feedback, what do you do with it?
How do you use it to iterate your game?
Well, one of the things you need to do is actually think and talk and understand the feedback that you're getting.
And there are some problems in this.
This is actually not unique to peer feedback.
We see this with expert feedback too.
Students will overvalue positive comments and undervalue the few critical comments that they get.
They will sometimes use feedback to justify exactly what it is they wanted to do.
Magic, how often feedback lines up with exactly the thing that they were already working on.
Or they'll treat feedback as a checklist.
They won't evaluate it critically.
They'll just say, oh, well, okay, I guess, you know, 10 people said we needed to do X, so let's do X, right?
So there are a number of failure states around how student teams make use of feedback and integrate it into the design process.
And so one of the things that we work on in my lab are methods for improving peer feedback.
And when I say methods, I actually do mean sort of replicable methods that you can teach or systems that we embody with technology because these students have to do it.
You can't do it.
They have to do it.
Right?
So this has to be a designed approach where you're training students or supporting or scaffolding them in some way.
Because it's them, not you.
And like Martin, we frame peer feedback as an end-to-end process.
So if you think about these formats for critique, right, so we're doing, let's say, a live in-class play test and providing peer feedback, or we're doing student presentations with a plenary critique, well, there's actually things that happen in the class before peer feedback happens, right?
So there's this...
It's preparing for feedback.
And there's also things that happen after feedback, like, as I've alluded to, reflecting on that feedback and integrating it into the design process.
So you can really think about these issues of engagement, quality, and reflection as they play out across these three phases of the peer feedback process.
So what I want to talk about today is two sort of contrasting cases.
Two different methods that we've developed and have deployed in real classrooms and have collected data on to understand how they help students become better peer feedback providers and receivers and how it can help instructors in the game design education process.
The first of these is something I call the Yoda method, and this is really meant for live in-class playtests.
So you can see over here on this diagram, we've got like some faculty and some designers.
They're standing off to the side.
The designers are silent.
They are not allowed to talk.
They must listen and take notes.
And then you have some playtesters who are peers, right, other students in the class and some observers.
Before this thing, this thing, this playtest, live playtest ever happens, everybody, right, so feedback designers, feedback receivers, but also the feedback providers, get something that I call norm setting around how peer feedback goes.
One of the ways that I do this is with an exercise that I like to call five spoons.
In five spoons, I divide the whole class into groups.
Every group gets five spoons, and I give them 15 minutes to make a game.
These games are not very good.
And that is intentional, because making a game that there is no reasonable expectation could ever have been a successful game, takes their ego out of the process.
All of these games are absolutely ridiculous, right?
And the spoons are meant to also help with this.
It's silly.
People will balance them on their noses or use them to flick things across the room.
And it's meant to help them say, all right, I made a game.
There's a ton to criticize about this game.
And I use this then to demonstrate high-quality feedback.
low quality feedback, and ask them to participate in a peer feedback process in a situation where their ego is out of play. And this is really successful at norming that getting peer feedback on something in progress is okay.
It's okay to show work that's half-finished and kind of crappy, and that if you are getting critical comments that help you improve your game, that is done out of love and to help you learn how to be a better game designer.
Right, so five spoons.
During peer feedback, right, so during these live in-class playtests on games to which these students actually do have some attachment, they'll have been working on them for several weeks, we use EOTA, which is the piece of this that the method is named for, and that stands for Experiences, Observations, Theories, and Advice.
So, basically, we break down what kinds of feedback who is allowed to give and when.
First, the players are allowed to give feedback about their experiences, and only their experiences.
What did I feel?
What did I experience?
What did I do?
Next, players and peers are allowed to talk about their observations, and observations must be concrete.
When this person got the water card, I saw them smile.
Then, and only when we've gotten a whole bunch of observations, are students allowed to create theories.
Why do these things happen?
And what they can then do is link experiences and observations and provide explanations.
Finally, the final phase is advice.
So both playtesters and peers can say, okay, well, based on these theories, here's what I think you should do.
Slowing it down and breaking it down like this addresses a number of problems in the peer feedback process.
It diversifies participation and perspectives because people can't just jump to, here's what I think you should do.
Slowing it down, everyone has eyes and that gives everybody a chance to say something or to offer observations, especially in the first parts of the process.
Having people think through sort of some data sources like experiences and observations increases the provision of correct feedback that is justified and critical.
And.
It means the teams aren't dependent on, that was some great advice we got.
Even if the advice they get is not terrific, they can actually use these lower level experiences, observations, and theories to make good decisions about their design.
Finally, after feedback, students are required to create a process document.
Designers, right, people who've gotten feedback have to create a process document, which can be worth up to a third of their grade.
And that's because when you design a game, the artifact itself, right, it's like you are here.
But it doesn't show the complex path that you took to get there.
And especially it doesn't necessarily illuminate the kinds of critical decisions you made.
Your ability to decide what the right thing to do is.
Blind alleys that you may have gone down and invested a lot of work into that didn't pan out.
A process document is meant to capture all of these things.
And part of that includes selecting what feedback you want to include in the document.
And reflecting on how that feedback was instantiated in different versions of your game.
Right, so writing it, just making students write it down and explain it helps students reflect and integrate feedback.
I want to show you a second system because there are many different ways to try to solve these problems with instructional design and pedagogical techniques.
So this second approach is actually a digital system.
It's a web-based and you can get it on your phone, your tablet, your browser, and it's led by my PhD student Amy Cook, and we've been working on this in collaboration with Stephen Dow over at UCSD.
And Peer Presents is designed to be used in a plenary format.
So designers are on stage presenting, peers and faculty are in the audience and they're providing feedback on the presentation.
And Peer Presents actually lets them go online on their phone or their tablet and provide their feedback live and in real time.
Before the feedback, before the presentation, before any feedback happens.
The feedback receivers have to go in and write questions so the students, the designers, would actually author the questions that they most want to get feedback on.
This gives them a sense of ownership of feedback.
You're not just telling me some random things, you're addressing the problems that I personally identified as things that I care about and I need help with.
With some training, students can learn to write questions that elicit higher quality feedback from their peers, and by making them have a team conversation, they all have to agree on what questions they want to put into the system, it makes them reflect on the kind of feedback that they want to receive.
During the feedback process, feedback providers can comment and they can vote, so they can type in that text box up there and you can see they can agree, or they can mark comments that they want to come back to later.
And during feedback, we've actually seen when we've implemented this in classrooms, these multiple levels of engagement actually keep students engaged in the feedback process for more of the feedback period, right?
So we would go in and we'd see active engagement during like a 20-minute feedback period.
We'd see active engagement from students about through about the first five to eight minutes.
And using this system, we would see students continue to engage in peer feedback all the way out to the 20-minute mark, which is quite a remarkable change.
instead of checking Facebook or sleeping.
What the digital system also does is it captures all the data from the peer feedback and makes it visible to the instructor, which lets you as an instructor reflect on what's going on in your classroom.
Who's not participating in feedback?
For example, we worked with one instructor who was shocked to realize that in this instructor's previous peer feedback system, only about two-thirds of the students were participating, which was about the same two-thirds as participated with the digital system.
But in order to find it out using the paper-based peer feedback system...
We actually had to go through and count a whole lot of index cards and the system just presented it to them immediately.
Finally, after the feedback process, there's a receiver reflection process that's actually built into the system.
And it's very simple.
It's a set of filtering and tagging tasks that students walk through in order to start to make sense of this giant pile of feedback they've just had dropped on them.
And so again, we ask teams to do this collaboratively.
They actually have to agree on, for example, which comments are helpful and which comments are unhelpful.
It helps them evaluate high and low quality feedback.
And structuring the reflection process tends to reduce the incidence of some of the sort of failure states that we've seen with open-ended reflection processes.
Like for example, ignoring all negative comments or deciding that the feedback means what you want it to mean.
So we think that there are a lot more techniques to be designed in this space, right?
More non-digital pedagogical methods, more digital systems that can help with improving the peer feedback process for student stakeholders, both in their role as feedback providers and receivers and also for instructors.
So I just want to offer this to you as a way of thinking about the challenges that you face with peer feedback in your own classroom.
Are your biggest challenges around student engagement in the peer feedback process?
Will students just not do it?
Is it feedback quality?
Are students incredibly vague or always really chipper and positive even when it's not appropriate?
Or are your problems with how students are actually using the feedback that they get?
Then you can think about where you have resources to address it.
So the systems that I've described are all end-to-end, but you can think about, let's say, your peer feedback doesn't start happening until the second half of the course.
You might have a lot of time to prepare students before peer feedback, before they ever start this.
Maybe that's where you want to design your intervention, or maybe you want to think about end-to-end processes, like the examples that I've shown here.
So between us, Martin and I have talked about these four types of feedback and critique, peer critique, plenary critique, supervision, and grading.
I just want to show you we made a couple of slides with some references.
Don't read them now.
We'll put up our PDF and you can get them later.
But we have them.
And what we're really interested in doing is having a conversation about expert and peer critique with all of you.
And I want to say we're going to open for questions in a minute.
But I want to especially point out that if you're interested in Peer Presents, my student Amy Cook is actively recruiting classrooms to work with right now.
So I'm putting her email address up there to prototype, to prototype, to reach out.
And there's also a couple of sessions on critique on Wednesday and Thursday led by Jeff Hesser.
Can you just raise your hand and wave it around?
Okay, so look over there.
I think that's going to be an opportunity to also continue these conversations.
I'm going to leave our email addresses up there.
You're welcome to contact both of us.
But as of now, we are open for questions.
Thank you very much.
APPLAUSE Hi.
I'm not in education, but I try to participate in a lot of local Bay Area, because I live here, local Bay Area sort of design groups, and I have a lot of friends who are working on board games and video games and things that they want to prototype and take to Kickstarter and things.
Not to toot my own horn, but I have been trying very hard over the past several years to learn as much about game design as possible.
And so sometimes I kind of feel like I know a little bit more and it can be a little awkward not being in a classroom setting and having like different social situations around.
the peer critique process and things have gotten pretty heated sometimes with some friends over certain projects where I see them making certain mistakes and caring more about well is the game balanced right now and really what they need to be focusing on is how hard is the game to learn right now and what do you folks think about those kinds of things just in general?
I will repeat the question.
So the question is, how do you deal with critique processes outside of a classroom situation where you don't necessarily have the same kind of buy-in to the instructor is an expert, though I will say not all students think of instructors as experts.
And where there may be other kinds of relationships at play.
So how do you do that?
I will say that taking some of these peer feedback techniques out of the classroom can be really, really fruitful.
So we have looked at a little bit about how they can be deployed in situations with working game designers, whether in industry or indie.
One of the things there is that...
Peer feedback actually sort of is deliberately designed to flatten some of these power distance hierarchies that it is up to the designers to decide whose feedback is important.
And one of the things that we're actually experimenting with as researchers is whether putting faculty feedback into the same peer feedback system as the student peer feedback helps designers be more critical of faculty feedback.
So I would say that...
When everyone involved is voluntarily there and it's a peer-led design situation, then even if someone has more expertise at game design, but socially your peers, and I would treat that as a peer feedback situation rather than an expert critique situation. That's how I would tackle this. And additionally, I think what I said before, making them realize that their problems themselves is always better than telling them.
So if you just ask the right questions, you might just get a little bit further than by telling them what you think And feel free to track us down if you want to dig in because sounds interesting Thank you both awesome talk a Question for Martin so you had a great slide on if someone's easily offended you separate the product in the process, etc I thought that was a great point I've tried to use that technique in the past.
I can tend to be, I'm looking at my former employee sitting in front of me, very bad at giving that feedback.
So my question is, when you build a culture of accountability, you want someone to own the product, to process everything together.
But then when you want to give critique, you want to separate it so there isn't that easily offended nature.
So I was curious if you could talk about some techniques of how you deliver that communication to be able to separate the product and the process and the review that's going on.
Oh.
Great question, thank you. So the question is how to, in practice, separate between the design process, criticizing the design process, criticizing the product of design, and criticizing the designer. I think a part of the solution is actually what Jessica was presenting, that you can standardize some processes, and that can help in Like, you presented the separating the talking about the experience from talking about the observations from building the theory and so on.
And I think the key is somewhere in there.
So by regulating what you're talking about at what point, you can be more conscious about what you're criticizing.
And I have a hunch that the solution to that problem is in that space, if anything.
But of course for all of this you have to be extremely reflected about how what you're saying arrives at the person.
Yes, and that is just, I mean, psychology is just a very difficult topic and rhetorics is difficult.
That is just very, very, very complex.
Maybe you have experience.
Yeah.
So I actually...
Although I'm talking about peer feedback today, as a researcher, as an instructor, I do have to grapple with exactly the situation that you're describing.
One key technique for me is actually reframing what the designer is attaching their sense of identity or their sense of value to.
So instead of saying, you're a designer who is good at design, right, to shift it to say, one of the things I really value about you is how committed you are to improving and getting better.
And therefore, I'm on your side with your goals as a human being, so now let's critique this project together.
So find a shared value that puts the critique as something that builds them up rather than tearing them down.
Thank you.
I enjoyed your talk.
Thank you very much.
These are issues that we all grapple with in one way or another here.
The question I have concerns the extent to which The peer critique aspect of a course is focused on in the course.
So you could have a course that deals specifically with this and how is that related to grading in terms of, I mean, so what pedagogical role would this have in the course?
What do you want the students to learn?
They could learn the critique or they could I mean, or is it producing a good product?
So what role does this have in different courses?
And how does it contribute to a grade, for example?
So I will say that I think that there's no single answer to that.
But I can give you sort of the way that many of my courses work, which is for every project that you put together, there's actually three components to your grade, which are each worth approximately a third.
So one of them is basically like, do you appear to have learned any of the course content?
Right?
So I often will call this theory.
It's like, are you using the readings?
Are you using game terminology of game designers?
Right?
So are you kind of, are you showing up?
A third of it is process.
So the process document that I talked about, which includes.
peer critique, it includes play testing, it includes your own critical judgment and your ability to decide what it is you're going to invest your time in, it includes things like how you manage conflicts within the team, and then third is a product.
And I, which is just like, what did you make?
How good is it?
And the reason why I grade like this is because, like Martin, I really want to push students to be ambitious and experimental while they're in school and can do that.
And so I want the person who tries something really safe to get the same grade as the person who tries something really, really ambitious, crashes and burns, but learns an enormous amount.
And both of those students, I think, are about as successful in my classes and in the process of becoming a game designer.
And the student who's going to really succeed and who's going to get the top grade in the class is the person who is both developing a sustainable game design process where they're trying things and really operating in that zone of proximal development, and they also manage to make it work.
I just have an anecdote about that. In the first years I was teaching a class called Game Design.
I had a co-teacher who was very much into turning everything into a game.
He was a game designer working in the games industry and just an external lecturer.
And he actually turned the grading into a game.
So he introduced a system of peer grading that was not really 100% of the grade, but about 50% of the grade was actually decided by peers.
And in the end...
Quality is something that is defining, if you just, the grade was purely based on the product in that case.
And actually the games that got graded highly or regarded highly by peers were exactly the games that actually deserved it, except for some exceptions.
So we kind of fixed it a little bit in the end, we could override the grades that the peers gave.
But in the end, we could have just given the grades that the students gave each other and it would have been fine in a way.
But it was a very nice exercise actually and it was packaged in a very nice game.
Thank you.
Hey.
So, like with these really structured peer review systems, there's like a lot of fluency the students have to build to sort of get good at them and use them effectively.
How...
Have you guys worked with other departments or other instructors in your programs to help that fluency carry on?
It's not just like I invested so heavily in this technique and walked off.
Could you talk a little bit about what that looks like and how you got buy-in from maybe people that were a little bit reluctant to do it this way?
I'm just going to repeat the question like I think I forgot to do last time.
How do you get buy-in from instructors?
Are these techniques, especially for peer feedback, is it high overhead?
So we've now, my team has now worked with people across multiple departments at multiple schools actually.
So we've spent a lot of time thinking about how do you onboard people to these processes.
And generally speaking, that if faculty are already bought into the idea of peer feedback having value, we can generally get them up and running with our processes within one to two hours of their time.
So it's actually compared to what they're already doing.
Faculty will actually generally say these things are less work.
It saves them time.
Now, of course, convincing them that it will save them time before they do it is its own tricky scenario, but now we have enough data from instructors saying this is about how long it will take you.
We typically actually have someone from the research team just come in and do like a 20-minute training for peer presents.
For the IOTA method, it's like a one-on-one meeting with the instructor just to walk them through it and show how they should teach the students.
So it's a modeling process.
But they can do actually live modeling with one of the games in the class, right?
So these are meant to be things that are pretty lightweight and you can try.
If you're not already using peer feedback in your class, or if it's mostly peer evaluation, so thinking about peers giving grades, then it's a much, much bigger.
And to be honest, we have not worked with instructors who are not involved with peer feedback in some way.
The research shows that peer feedback is great for students.
So one of the things we're hoping to do is to create systems that are so easy to use that you won't mind changing your syllabus.
But we're not there yet.
Actually we have a unit at the university called Learning Support Unit, which is a small department of just five people or so.
That researchers provide training in tools like you're producing.
And that is how my university solves the problem.
You can just book them to give a workshop.
But funnily actually, my part of these slides was developed in the teacher development program of my university.
So where I, and the reason why I even had the idea of talking about critique was because we have it in so many different disciplines.
You have it in, you have critique sessions, they are called code reviews, in programming as well as in design.
So I think sometimes it's also just translating your tools to their language.
that generates their interest.
Thank you.
Hi there. So I'm currently co-instructing a course with this guy over here, over at Purdue University and one thing we've struggled with a lot is this peer feedback and just evaluation and critique.
It feels like since we are part of a program called CGT, which is computer graphics, we're trying to cram in the art side and the programming side and the design side.
all within two courses, and so whenever we're spending time on these feedback sessions or like sharing presentations, the students sort of feel like they're losing that time.
You know, and not just the students who say, you know, want to get out of the critique, but also the ones who are really fired up to keep working on their game.
They feel like they just lose a whole day of development time that they're together cooperating.
And so we've tried a couple different systems.
And I do think norm setting is going to help.
I honestly just want to go back home and implement that as soon as I can.
But in the meantime, I was curious about the idea of sort of like remote critique.
So for example, you have everyone sit down at a computer with someone else's game, you play it, it has in-game analytics that are keeping track and say like logging that to a file, and then you step over to the next computer when you're done.
You know, it sounds reasonable from a logistical standpoint, but I'm concerned that it wouldn't have the ethical and empathic response that the stuff you've described would have.
Do you think a remote method would be in danger of losing the value because you're not getting that one-on-one or that communication?
Great question.
So I'm going to attempt to summarize it.
A lot of context in your question.
So the question is, if you're in a situation where there's a lot of demands on in-class time, are there ways that you can do peer feedback that sort of minimize the amount of class time devoted to it, but still get the benefits of peer feedback?
So...
One thing to keep in mind, so I talk about in-class peer feedback because there are systems that actually will let you do asynchronous remote peer feedback on other students' projects and it turns out that there are basically students find them very burdensome and they won't do them and so that's part of the reason why we're looking at in-class.
But you can make it more efficient.
And one of the things that we've looked at is breaking students into groups.
So you can actually do the IOTA method in parallel.
What I would recommend is having the students go around in groups of three so that they are actually hearing each other's feedback and each other's comments.
That's really important.
But one thing that I'll do is have a designer stationed at each station.
And then have students go around in groups of three.
So now you're dividing the right because everything's going on in parallel.
I think that could really help you.
I have a write up of how I've done that, which I'm happy to send you.
I have no experience with remote feedback, honestly.
I just can't say anything useful to this question, sorry.
Well also you're characterizing it as remote.
I wouldn't call it remote, I'd call it parallel feedback.
I'll also say that one area I'm really interested in exploring is actually live streaming for peer feedback.
So if you're interested in prototyping this in your class, let's talk and we've got some systems that can support you with it.
So, that might help you take it out of class time, but still create that sense of peer accountability and commitment to each other as human beings as well as designers.
So, we'll talk.
Awesome.
I know another instructor whose research is focused on live stream teaching too.
So, I would love to get you to talk about that as well.
Okay, great.
Make the connection and we'll, thank you GDC.
This is awesome.
Yeah.
Thank you so much.
Okay.
And I think this is going to have to be our last question.
Sweet.
So my question is for Martin about the expert level feedback.
And I'll give you some context.
I'm a previous software engineer and we do Agile and sprints.
And at the academy that I teach at, we do two week sprints.
And they last four months.
And every two weeks we get together and then we do feedback with the students.
And as an instructor, the problem to be solved is to make sure that the students get feedback.
the most out of it, that they can create, you know, critical analysis and provide useful feedback.
But as a software engineer, the product needs to be good towards the end. So my question would be, do you find yourself sticking to your guns throughout this entire process, or do you find yourself evolving and making a decision at runtime, I guess, when you need to kind of step into a more professional type of expert role?
So the question is if I'm sticking to my guns of actually, it's a very good question.
The question is if I kind of overrule my recommendations of forcing them to reflect on the process in favor of actually just giving them expert advice because I am the expert in the room.
Wow, this is a great last question that we could talk about for half an hour.
I think actually whenever you design a class or a curriculum or whatever, you always have to make this, you have this slider where you define if your class is supporting students learning about the process or if it is about having an optimal end result as a product.
I usually think that it's so complicated because students get motivated by working on, let's say, a great game if they're in a game development class.
So of course you want to support them making a great game, but at the same time you also want to support them even more to make it to fail spectacularly, as you said before.
A great game is easiest made if you make a very safe bet.
and you don't want them to bet safely.
Also, you want them to have communication problems in their team because they have to learn working with different opinions, with different paces of working, with different cultures, with different backgrounds and so on.
So one of the initiatives we have in Denmark is called DadiU and it's a collaboration of 12 universities where students make games in teams of 16 people, which is bigger than Supercell has, for example.
With the idea that the teams are so big that they mostly have overhead and hardly any productive work gets done.
Which is an awesome learning experience, but the games they make are, let's say, questionable.
I didn't understand why they are doing that for many, many years.
And nowadays I actually support that decision because I think it actually leads to the better learning outcome.
So I try to stick to Megan's actually, and I try to actually support students not having an optimal product, but actually learning how to make the optimal product for the rest of their lives.
or making at least a passable product under harsh circumstances, I think there is actually more learning in that, I think.
So I agree with you about the slider, and for me the slider is set different places and different classes, but even when things are more focused on the product, my goal is if, it's like, you know, teaching them, giving them a fish versus teaching them to fish, right?
So if I just tell them the right answer, it's not helpful, that's not learning.
So...
What I will typically do is I will say to them, you are struggling with this problem.
I will now show you three ways, speaking as an expert, that you can address this problem.
These are three examples.
They are not the only three ways.
But I want you to use these exemplars to figure out your own solution.
And then I give them three directions, using all of my expertise, and I do three in order to reduce something called sort of functional fixedness, where they'll like, if they just hear one idea, they'll sort of go, oh, well, I should do that, and it's going to be hard for them to get away from it.
But when it's more than one idea, they can actually compare and contrast, and that helps them generate new ideas of their own.
So that's when I'm more focused on outcome or product, that's how I still sort of thread the needle of if I tell them how to do it and then they make it, that's like, and then I put my stamp on it saying these students can do this themselves.
I don't know that that's true.
And so I need to make sure that whatever the product is, I can actually be confident that those students could do it again.
And that one of those techniques is to say, here are these three options.
Reduce the power distance so that they feel free to choose something else.
And then if they succeed, they succeed, and if they fail, they fail.
I'm sometimes even more nasty.
I actually tell them one solution, and then I tell them they're not allowed to use that one.
I'm stealing that.
That's great.
I love it.
All right.
All right.
So, listen, thank you very much.
Again, our email addresses are up here.
Thank you so much for coming.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
