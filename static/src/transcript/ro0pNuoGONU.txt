Hello, my name is Amy Phillips.
I am a tools programmer at Media Molecule.
And today I would like to tell you about the tools that we've written to keep the number of open bugs low.
So firstly, why do you want less open bugs?
Well, because if you are being stopped from working by bugs, you're not getting on with the fun bit of making games.
And we all want to do the kind of creative bit.
So if we can keep the number of bugs low, we can all get on with the cool bit.
So what we've done is connect a bunch of off-the-shelf tools together by writing a bit of bespoke code, but using other people's stuff as well, which means that we get the advantages of writing our own stuff.
Because we're quite a small team, we don't have the time to write entirely our own stuff.
A little bit about Dreams.
It is a user-generated content game.
It's kind of like LittleBigPlanet, only much, much, much more so.
So you can sculpt things.
You can then stamp them down to make an environment.
You can add a character to that environment.
And then you can completely change the character.
So you can like re-sculpt your character.
You can animate your character.
You can animate the environment as well.
You can add in sound effects.
You can create your own music.
So it's a really cool, creative sandbox.
And everything that we're shipping with Dreams, so we're shipping a game with Dreams, everything that we're shipping, we're making within Dreams.
So it means that our code has to be really stable.
Otherwise, the level designers, the artists, the audio people can't do any work.
So Media Molecule, we've got about 50 employees.
And in the tools department, there's me.
I do two days a week.
There's Daniel Kidney.
He's five days a week.
And we've recently gained Alex Parker, who also works full time.
So we've been 1.4 programmers, and we're now up to 2.4 programmers.
So we really want to get lots of bang for our buck in terms of these tools.
And also, because Media Molecule is a very flat hierarchy, it means that any tool that we make has to improve people's lives significantly.
Otherwise, they're not going to bother to use it.
So the tools I'm talking about today are in regular use in our studio.
So they've proved that they are useful.
So I'm going to show you a video of Dreams.
This is our trailer video to give you a better idea of what we're making.
Am I dreaming or am I awake?
Francis, you'd think the phony's around here.
The old fears, they still haunt me.
Hold on lads, we're coming!
That I'm on a train to nowhere.
You are the driver, sir.
When I awake...
Will I face the worst parts of myself?
I need to listen to that quiet voice...
Calling me.
That need...
To make something again.
We'll make it.
Me.
and you.
Sorry, my poor laptop was really struggling to play that video.
Right, so what I'm gonna do is give you a quick, quick go-round the life cycle of a bug, telling you a little bit about the tools that we've got, and then after that, so you'll know kind of where they fit, and then after that, I'll go into a bit more detail.
So if anything doesn't make any sense, then don't worry.
It will probably be explained in more detail later, and if not, there's questions at the end.
Oh, and also, you will be emailed an evaluation form.
Please fill it in.
I was supposed to say that at the beginning.
So we start off, a bug gets written, and it gets checked in.
So now it's gone orange because it's now starting to affect programmers.
It's not got as far as the artists, the designers, and the audio people, but it will be stopping programmers from working.
So our first line of defense is Autobot.
which is our automated testing system.
And I'll tell you a bit more about how that works and what we've done to make the flow of bugs smooth and to help QA to put all the information into the bugs from Autobot.
Then I'm gonna talk about our next line of defense, which is QA.
So when they find a bug, what have we done to help them to report that bug, make sure all the information is accurate and to attach information without having to faff around like copying screenshots from one folder to another.
We try and make everything one button click, so it's really simple.
You don't get any mistakes, and you don't waste time, which then frees QA up to kind of break our game in new and inventive ways rather than just kind of going through the boilerplate stuff.
So if a bug gets past Autobot...
gets past QA, and then it gets into the build that gets deployed and starts to stop our level designers from working. So that's where it gets to that red state, and I'm going to tell you about the report a bug button that we've added to a Chrome extension so that level designers can very easily report a bug and we can make sure that the information they report is accurate.
When the bug gets assigned out by production, we've already attached all sorts of information to it to make production's life easier and to make sure that the bug goes to the right person straight away so we don't have bugs kind of bouncing around and not being fixed because they haven't reached the right person.
So for example, in a bug, we'll have the assert message.
We'll have the programmer who added the assert.
We'll have the call stack.
We'll have a link to ReCap, which will have a core dump.
It'll have the TTY.
It'll have a video of the last 60 seconds before the game crashed.
It'll have the Perforce revision of the build that the bug was seen on, so you know which build it should repro on.
So if we can get as much information in there as possible, it means that production's job is easier when farming the bugs out.
Then when the bug gets assigned out to a programmer or a level designer, I'll talk about the tools that we've written to help them to repro the bug, including my favorite button in Jira, which is in one button press, it will replay the replay on your dev kit, and it will repro the bug for you, so you don't have to follow through like step-by-step instructions or anything.
It's just one button press, and it repros on your dev kit.
Then when you've fixed the bug, we've got some more tools to help you to check that you've fixed it, check that you haven't broken anything else whilst doing the fix.
Then once you've checked in your fix, more tools for QA to verify the fix, and of course it goes back through our automated testing system to check that nothing else got broken on the way.
And then job done.
You've deployed a new build.
Everyone can now work again and create cool stuff.
So starting off with Autobot, which is our automated testing system, it runs after every check-in.
So you check your code in.
Jenkins notices that there's new code, does a build.
Autobot notices that there's a new build and runs a suite of tests on that build.
and it checks for asserts, crashes, hangs, it checks for out of syncs, not a numbers, and some logic errors.
So it runs each test twice to check for out of syncs.
The first time round, it dumps out a bunch of sync points.
Second time round, it compares with the sync points.
So you know immediately if your deterministic game code is not actually deterministic anymore.
So it takes about between 10 and 20 minutes to run our smoke test.
We found that keeping it under 20 minutes is the sweet spot between making sure that you get enough coverage, but also making sure that you're not rolling too many revisions into each test so you can easily tell which check-in it was that broke something, and also making sure that programmers get immediate feedback on the stuff that they've checked in.
because you want your head still to be in the same space of the code that you've just checked in when you go back to fix something if there are issues with it.
So once it runs the test, oh yes, and it also runs some more in-depth tests overnight because we're not checking in as much overnight.
We've got some larger suites that test like sculpting in depth.
We've got some more detailed out-of-sync tests.
So if something is flagged as being out of sync in the smoke test, It gets added to the out-of-sync test, which then dumps out a lot more information about what it was that went out of sync.
And because it's less busy overnight, we can just, it doesn't matter about the whole 20-minute limit, we can just add in loads of tests.
Results are emailed out, so the email goes to QA, it goes to the tools team, and it goes to anyone who checked something in that was included in that build.
So you get to check.
whether you've broken something or whether your code has all passed.
So here is the top of the email.
You can see from the subject line that I summarise what the differences were because programmers are not going to read all their emails.
They're not even going to open all their emails.
So if you've broken something, you need it to be really obvious from the subject line.
So you get that assert plus one, there's an extra assert.
And then when you open the email, there's a more wordy description of what it was you broke.
So in this case, there's a new assert message, but it broke 88 tests, which is like nearly half of the tests.
So you've clearly broken something really quite important and need to fix it.
And then there's a link to the assert messages section where you can scroll down and get more information.
So I'm just gonna kind of nip through this.
You probably can't see it, but I'll describe it to you.
So firstly, you've got a new picture, so you can easily see what the new thing was, i.e.
this is something you broke.
You get the call stack, you get the assert message, you get the programmer who added the assert.
You get a list at the bottom of all of the replays that failed this test.
And included there is the command line that you would use in Visual Studio to repro exactly what Autobot ran.
to make it easy to repro and fix.
And you also get a summary of the last five results.
So you can see color coded when it broke.
And you can also see a list of the changes that went in to that build, who checked them in.
And if you click on the link, it takes you to a swarm page, which tells you what files got checked in.
And it shows you the diffs of those files.
So in this case.
It's run out of GPU-long memory, and you would have a look through those three revisions that got checked in and see who was using more of that memory, and then go and nag them to fix it.
By the new button, occasionally there's also a JIRA button, because I look up in JIRA to see if there's an existing bug for this, and it looks up based on a cert message and based on call stack.
but with the line numbers stripped out because they tend to change.
And that's a clickable link.
So if there's a Jira bug, then you can click on it, go to the bug in Jira, read a bit more information, fix it, mark it fixed.
So that's the email.
The results are also viewable via a web UI.
So the email is more useful for programmers.
the web UI is more designed for QA to use.
And they can filter the results, they can sort the results, they can see kind of what it was that failed about the results.
But we've added some interesting buttons to the web UI.
So there's the Jira button that you can see there, the orange and blue one.
And that, as with the email, it goes off to Jira, looks to see if there exists a bug.
If there does, and the bug is assigned out to a programmer, so it's in the right state, it's gonna get fixed hopefully soon, QA don't need to do anything about it, then that JIRA button is blue.
If the JIRA button is orange, like in this one, that means that the bug is not in the right state, so either someone's marked it as fixed and it's not fixed, or it hasn't been assigned out to a programmer, so QA need to click on that button, go to JIRA, and make sure that the bug ends up in the right state.
so that it can then be fixed.
If there is no bug in Jira for the issue, then you get the red bug button.
And that brings up the Jira create issue screen, and it pre-fills a whole load of information.
Basically, all the information that we've got out of AutoBot, we try and pre-fill it into the bug to save QA time and to save them copying and pasting stuff around.
So it will attach the replay.
It will mark which revision it was seen in.
It will put in the assert message.
It will insert the programmer who added the assert, because in Perforce, you can go and look up who added a particular line of code.
So that gives production a good idea of who to start with when they're assigning it out.
It will also do a link to Recap.
So Recap is a Sony server.
where you can upload core dumps, and you can upload your symbols, and it will give you a web interface for browsing those core dumps, and you can see the call stack, and you can see the TTY, and you can upload the last 60 seconds of video.
So we upload all of our crashes and asserts to recap, and then we put a link in the bug to recap so that programmers can click on it, can see very easily the call stack, and can easily see what went wrong. There is also the launch button, and the launch button takes the replay and launches it on your dev kit, on your test kit, on whatever build you're running, so it doesn't have to be like a build running from Visual Studio. You could be running a package build, and this will still launch that replay.
on your build. So you can have a look at the replay and check that it's doing what you think it's doing. So one of our early problems with the automated testing system was that we were replaying a replay. And, by the way, a replay is an initial state plus a bunch of inputs, which can then be rerun, and you get the same thing.
and inputs would be controller inputs, it would be like any text that you've input, any inputs into the system.
So you would, if you were replaying a replay, you would expect to get the same end state as well.
But say for example, you had a replay where you were stamping down like five cubes, and you were stamping them down using the trigger button, because that was the button at the time.
Then you've recorded your replay, Autobot is running it, everything's happy.
Then a bit later, someone changes the button mappings.
So now stamp is not on the trigger anymore, it's on X.
And your replay is still running, it's not asserting, it's not crashing, Autobot thinks that everything is good, but actually it's not testing what you thought it was testing.
It's not stamping down the cubes anymore.
So our first attempt at trying to detect this was dumping out the game state as JSON at the end.
And then we would do a diff.
So we'd have a control run where someone had watched the replay and said, yes, it's doing the right thing, and we would store the JSON game state after that.
And then for every subsequent run on later builds, we would diff the JSON game state.
And so you would be able to see if it had changed.
But it turns out that's quite brittle because people add things into the game state, they take them out, things move around.
And...
A diff of a JSON game state is not very useful to a QA person to know whether a replay is doing the right thing.
So our next attempt was screenshots.
And you can see up on screen, you get a screenshot from the end of the control run.
And then you get a screenshot from the end of this run that's just run.
And a human can very easily look at those two screenshots and see, yes, it did do the same thing.
It's put down the same gadgets.
Everything's in the same place.
So this replay has completed successfully.
And we don't do like a binary compare of those screenshots because they are actually different.
Our sky rendering is not deterministic.
The UI for the palette has changed.
The thermometer has changed slightly.
But to a human eye, you can see that it's done the right thing.
And so it means that QA can just scroll through the results.
and check by eye that it's doing the right thing, which saves a huge amount of time because we did have one poor guy, poor Geddon used to have to press the launch button and watch the replays on a regular basis to check that they were doing the right thing.
So then that frees him up to, again, do more creative breaking of our game rather than the kind of boilerplate stuff.
We recently added some performance stuff.
So Alex Parker, who joined us recently, has added in some performance tracking, but it's very much kind of work in progress. We're hoping that we'll be able to pick up when we have spikes in the frame rate, for example, and be able to see exactly which revision those came in at. So how does it all fit together from a tech perspective? We have a MySQL database, which we use to store...
which tests we have and which tests run as part of which suite, so what replays get run if you run a smoke test, what replays get run if you run a more in-depth sculpt, that kind of thing, so that's in the database, and we also store the results of each run in the database. We use a Django web UI on top of that, which then means that QA can add in new replays. They can browse the results.
And it means they don't have to do database queries.
It puts it all into a nice clickable web UI for QA.
So it means that QA have complete control, and I don't have to do anything to maintain it, which is excellent.
So to run the actual tests, we have some Python scripts.
So Django uses Python, so we wrote the rest in Python so that we could share code.
We've got a script that watches for new builds and puts in test requests.
We've got another script that grabs the test request, sends each replay off to a dev kit with the relevant ELF. It gets back the results. It parses the TTY.
It puts the results into the database. And then we have a third script.
that cleans up after everything else.
So this system generates a huge amount of data, and we don't want to make IT cry, so we have to delete the results after a day or two.
We delete the screenshots from the runs after a few days.
We have to kind of go in there and clean things up.
Now, all of this used to be on one box.
And that was when we had sort of five, six dev kits in the wall of dev kits.
And it was all kind of ticking along okay until we got up to like nine, 10, 11 dev kits.
And then it all went horribly wrong because we were using Orbis Control to launch our tests, which is a tool from Sony that takes your test and launches it on a dev kit.
And that was using 5% of CPU for each test that it was running.
So when you've got like nine, 10 dev kits, that's 50% of your CPU gone.
Network Neighborhood took another 30%.
So basically the tests were running and we were getting the results back and then we went to put them in the database and we didn't have enough CPU to do the database insert.
So we split it up into two boxes.
We have Perceptor, which does the database and the web UI, and then we've got Optimus Prime that runs the scripts and launches them on the dev kits.
and gets the results. And that works quite nicely. And we've now got a wall of 27 DevKits, so it's toasty warm in that area of the office. I was going to put a photo in, but apparently I'm not allowed. So we were using Orbis Control to launch our tests, and that was fine until we came across a really bad hang.
By a hang, I mean when the game is still running, it's just stuck in, for example, a while one loop on the main thread.
So it's running, it's not crashed, it's not asserted, but it's not actually getting anywhere.
So we were launching these tests, they were hanging, so that was then using up at DevKit.
We didn't have a timeout, because there's no timeout functionality in Orbis Control, and we had only written an overall timeout.
you could hang all of the dev kits, and eventually the overall timeout would timeout, but then most of your tests hadn't run, and you didn't get very useful results. So we replaced Orbis Control with our own PS4 launcher. There's a target manager API that we used from Sony, which you can use to launch builds on your dev kit. I wrote some timeout functionality in the launcher.
And this also meant that we could upload our core dumps to recap, which gives us a lot more information about crashes and hangs.
Because when a PS4 launcher detected a hang, it would send a signal to the dev kit saying core dump.
And then once you had the core dump, you could see where it had hung.
So you had a call stack for where your while one loop was that someone had put in your main thread.
And we got some bonus features as well.
in that you can change the language, you can change a pro dev kit to behave like a pro or behave like a normal PS4.
So that's kind of nice too.
A little bit more about the replay system, which I kind of touched upon earlier.
So our game update is deterministic, which means that if you have an initial game state and a bunch of inputs, you can rerun them and you'll get the same result.
There's a couple of drawbacks we've had.
The first is serialized revision.
If you've got one of these replays, you want to be able to run it on later builds as well.
You don't want to just be able to test a single build, which means that when you serialize in your game state, you need to know what you're going to expect to be reading.
So if I've added variables, I need to bump serialized revision so that then I will know whether I'm supposed to be reading in that new variable or not.
But having this one kind of central serialized revision, firstly, it's a point of contention for programmers because anyone who's adding or removing stuff from the game state needs to bump it.
And also, if you branch your code, it's a bit of a nightmare because it doesn't deal with being in a branch.
You can't increment serialized revision in the branch if you're also gonna be incrementing it on trunk because then you've got to revisions that mean different things. So what we do is when we branch to do a green build, we have a very short-lived branch, and if anyone needs to do a fix in the branch that bumps serialised revision, we pull a whole new branch, which is not ideal, but it kind of works for us. And the replay system is very important for us for reproing bugs.
So, that's Autobot. On to QA.
So for QA, I've written a Chrome extension.
And if you've not come across a Chrome extension, it runs in the Chrome browser, and you install it into your Chrome browser.
It's written in JavaScript, and it can access REST APIs.
So it can query Jira, it can query Recap, it can query our game server.
It gets information about what tabs you've got open in Chrome.
and you can change the HTML within those tabs.
So if I detect a Jira tab, I can add extra buttons into the Jira tab.
And I did look at doing it as a Jira extension, but I backed quickly away from that and the Chrome extension was much simpler.
It can also access your cookies.
So we use that to find out what your PSN login is, which then means that I can...
find your dev kit via our server and talk to it.
And it can display some notifications on your screen.
So we use the Chrome extension for QA to add extra buttons to Jira.
So there's one button that attaches a screenshot in one click.
So you press the button in your Jira web page.
It goes off to your dev kit, takes a screenshot, and attaches it to the bug that you're currently creating or looking at.
So in one button press, it's done all the work for you.
and you don't have to be browsing around to find the screenshot, and it makes sure that you get the right screenshot as well. We have the same functionality for starting and stopping replays, so you can start a replay from the Jira page, you can repro the bug on your dev kit, and then you can stop the replay, and it will attach the replay to the bug that you're creating. And finally, there's a button for them that will fill in details of...
For example, the Perforce revision that you're currently running and the game location where you are.
So which stream you're in, whereabouts are you within that dream.
And again, with the Perforce revision, because it's like five numbers, it's really handy just to be able to click a button and get it filled in without having to kind of try and remember those numbers, make sure they end up in the right order, because you tend to get quite a lot of mistakes.
Oh, now I've gone the wrong way. Sorry.
So, on to the designers, the artists, and the audio people.
So, we want to make sure that everyone is running the game within the same environment.
And that is where Dashboard comes in.
Dashboard is a tool that we've written in-house.
And firstly, when you boot it up, it checks that you've got all the dependencies installed.
So...
if you've got Target Manager, Remote Viewer Server, all the dependencies that we use from within the Sony tools, it checks you've got them installed and it checks that you've got them installed at the right version.
And if you don't, it gives you a button that you can click and it will run the installer.
So that makes sure that everyone in the company has the right software installed, it's up to date, so we don't get any weird bugs because of that.
Then once you've done your dependencies, you can click onto the Builds tab.
And that gives you a list of the current builds that we have available on our server.
And you can fill in build notes.
So QA will fill in build notes and you can see kind of what's broken.
And there's a button that will update you to the latest green build.
Because we want everyone to be on the same build and on latest code.
So we want to make it really easy for them to be on latest code.
And finally, there's a button to launch that build on your dev kit.
And that also checks that you've got the right SDK installed on your dev kit, because we've had weird bugs in the past.
So if you don't have the right SDK, then it will go away.
It will install the SDK on your dev kit for you, and then it will run the build.
Quick note on asserts.
So we have fatal asserts, and they're included in the build that we give to our level designers and artists.
We decided this near the beginning of the project.
because we wanted to make sure that if you add an assert in your code, it really is a broken thing.
And we want to make sure that people fix them as soon as possible.
So if you hit an assert, then it stops.
And it's not recoverable, it's not skippable.
You have to go and fix it.
So what does the Chrome extension do for designers?
They have got a report bug button, which is.
the yellow button in the middle there.
If they click on that button, it will open up a Jira webpage in the create issues.
It will pre-fill what revision they're running because that's not been particularly accurate in the past.
So it goes off to their dev kit, says what revision of Dreams are you running, and fills that in for them.
It fills in the game location for them.
And they've also got the same buttons that QA have got to attach screenshots and to attach replays.
So it makes it very easy.
for a level designer who's having an issue to fill in a bug and then send that off to QA, and QA don't have to do very much extra work. They might add in a repro rate or something, and then they assign it out to a programmer. So very quickly, all the relevant information has been distributed to the programmer who can fix it. There's no Chinese whispers, and you know it's accurate.
is a picture of the Chrome extension, and as you go down the screen, it's kind of checking that everything is working properly with the Chrome extension. So if any of this goes red, then the artist knows that they need to take some action, and it does explain to them what the action is that they need to take to fix it. So if you get down to the bottom and you get a screenshot, it means it can talk to your dev kit, everything's working fine.
So we've now got a bug in Jira.
We need to route it to the right person.
And we've auto-filled lots and lots of information, which is really useful for production to farm the bug out.
So we've put in the assert message.
We've put in the programmer who added the assert.
And there is a bit of a gotcha here, in that we have an assert in our game.
for if it goes out of sync.
And it's part of a macro, which means that one programmer added this assert.
So it means that any out of sync bugs, it says, yeah, Dave added this one.
So give it to Dave.
So poor Dave ends up with a lot of out of sync bugs.
And luckily Dave knows the code very well and can kind of farm them out to the right programmer.
But at some point I need to go in and, maybe change it so that it looks up the variable that goes out of sync and see who added that variable, for example, instead.
That would be more useful information for tracking down and out of sync.
I haven't got around to that yet.
I can also, if anyone wants to talk to me about Perforce Auth Joy afterwards, I'm not gonna go into it here, but I've had all sorts of fun and games trying to auth against Perforce.
So yes, you got the assert message, you got the programmer who added it.
got the call stack, you've got the Perforce revision where it went wrong, you've got a replay so you can reproduce it, you've got a screenshot, you've got a link to recap where you've got the core dump, you've got the TTY, you've got a video of the last 60 seconds of gameplay. So you should be able to figure out who to give the bug to.
So then the bug is assigned out to a programmer.
and there are two more buttons in the Chrome extension.
So there is the launch replay button, which is my favorite button, and I've got a video of it here.
So I start off with Dreams booted up in a debugger on my dev kit.
Then when I go to a bug, in Jira, which has got a replay attached to it, I get a button at the top and this button is added by the Chrome extension. And when I hit this button, the Chrome extension downloads the replay data from Jira. It sends it via our server onto my dev kit. So my dev kit then starts replaying that replay data and Eventually, it hits the assert.
And there we go, we've reproed the assert in the debugger.
So I hit break, I can see what's asserted, I can fix the code.
And then once I've fixed the code, I can rerun the replay and check my fix.
Cool, so that's my favorite button.
It's very cool.
We also have another button to launch the game location.
So replays are very handy for programmers, but because you've stored the initial game state, they're less useful for content people because any fixes to the content won't be reflected in the replay.
So we also have this idea of game location, which is like a dream ID and a location within that dream.
So launch game location will launch an artist into their dream at a particular point.
So for example, if QA were saying, this tree over here is too green, they would attach the game location and then the artist could launch into it and see the tree that they were talking about and fix it.
And then when QA go to check the fix, they can launch the game location and see the new less green tree.
So once you've fixed the code, how do you test it?
You can use the Launch Replay button again, and then check that the replay finishes rather than asserting or crashing.
You can also submit a build to Autobot.
So we have a script, and it doesn't take any command line parameters because I wanted to make it as simple as possible to run, so you can just double click on it.
And it takes your local code.
builds a package, uploads that package to Autobot, and then kicks off a smoke test, which will then, when it completes, email you the results.
So if you've made a fix that touches lots of bits of code, or we all have bits of code in our code base where you know that if you go into that and change one fix, you will break three other things.
So if you've been in one of those tangly bits of code, you can kick off an Autobot run before you check in and just check that you're not gonna break everything.
And we also have a script to mark bugs as fixed.
So previously, when you checked in a fix, you would put in your Perforce checking comment, a reference to the bug that you had fixed and a comment about what it was that you had changed.
And then you would go to JIRA and...
you would go to the bug and you would put in what Perforce revision you had just submitted, which again, a bit error-prone, and you would put in a comment about what you did.
So you're kind of putting the same information in in two places.
So we've just got a script that parses the Perforce check-in comments after every successful build.
And if there is a comment of the form with the exclamation mark and then a number.
That means I fixed that bug number and it will go off to JIRA, check that that bug is in the in progress state.
If it is, then it adds a comment to the bug saying, Amy fixed this bug in Perforce revision 320.
This is the checking comment that she put into Perforce and then it marks it as fixed for me.
So it just means that when you're fixing bugs, it takes out a little bit of effort.
And if we can take out lots and lots of little bits of effort, then everyone is working much more quickly, and we save time overall.
Right, so you've fixed the bug, you've checked it in, now QA need to verify your fix.
And you can use the...
Chrome extension Jira button to launch the replay, so QA can launch the replay and check that it doesn't reproduce. They can add a replay to Autobot, so if it hadn't been picked up by any of the Autobot test suites, then they can think about adding a replay, and they would want to add a new replay. They wouldn't want to use the replay for the bug because you want to keep them really, really short.
and you want to make sure that the screenshot at the end shows everything that you have changed as part of that replay. You want to make sure that the screenshot at the end shows you that the replay did what you thought it did.
I would also like to track stuff that gets fixed by Autobot, because if Autobot has reported a bug, and QA have kind of prefilled that bug in.
AutoBot can also detect that that bug has been fixed, and it can add a comment onto the Jira bug so that then when a programmer goes to fix it, they can see that it's already been fixed.
But I haven't done that yet.
Must do that, maybe next week.
Oh, no, I've gone the wrong way.
Right, so in summary, the tools we have.
We have AutoBot, which is our automated testing system, which is a MySQL database, Django Web UI on top.
and a bunch of Python scripts.
We use Jira for our bug tracking, which has got an awesome REST API.
We use Recap for storing our core dumps and TTY and video.
They have a REST API, which is improving rapidly.
We use deterministic replays, and they're essential for the reproing of our bugs.
Our game server can forward a message from the web side.
on to a dev kit and then can send the result back to the website, which is used by a bunch of these tools. And it's all linked together with the Chrome extension, which has kind of got fingers in many, many pies. So it's all about trying to make sure that we find the bugs early. We attach all the information that we need.
to route them to the right person and to repro them.
And then we want to make it really simple to repro the bug and to mark it fixed.
And it's just a bunch of little things trying to make sure that people have the information in front of them at the right time, and trying to make little improvements to the workflow.
And all the little improvements kind of add up to a really nice, smooth, slick workflow.
And then suddenly you've got less bugs, which is awesome.
So does anyone have any questions?
Hi.
Super cool talk.
I'm curious about the human validation of screenshots for the smoke test.
You mentioned that testing the game state was too error prone or had too much variation.
So does that mean that for every...
smoke test that's being run, there is a validation by a QA person for every test?
Because that seems like a lot of stuff.
Yeah, they don't check every one.
So I think it's about once a day, they will pick a test and will just scroll through.
And then if they...
The ones that are more likely to fail, I guess?
Well, no, they would look at all of the results from a smoke test.
So a smoke test is about 200 tests.
So they can very quickly scroll through.
and just kind of see by eye.
And yeah, they wouldn't do it for every single one.
Thanks.
Hi there.
I was wondering what you do to track which programmer added asserts.
Is that some sort of metadata, or is that just like the diff history that you look through?
There is a command.
So the P4 command line, I think it's P4.
Thank you.
P4 Changes.
So you can run that and you can pass in which revision you're interested in and which line, I think.
Yes, so you can run that and it will tell you who submitted that change.
That makes sense.
So that's used to route to the programmers.
A follow-up question, what do you do when programmers leave?
Either go on leave or leave the company when you route those?
Then you would have.
I guess the programmer would have handed their code over, so production should know kind of who they have handed each code area over to.
But yes, it would say in the bug this assert was added by Fred, who has left.
So then production would know who Fred's bugs should go to.
Okay.
Thanks.
Thank you.
Hi.
At what point did Media Molecule decide to have designated tools programmers?
How did you manage having the benefit of a designated tools programmer as opposed to them working just full-time on the project?
I guess as far back as I can remember, we haven't always had a dedicated tools programmer.
I can't remember when.
we hired our first tools programmer.
I guess it would be when we got to about 12-ish programmers, although I'm not sure I can remember.
But actually, it's been really, really useful.
Because you write all these tools, and then all the other programmers become more productive.
And it's not just the programmers.
The QA people become more productive.
Everyone gets more stuff done.
And it's quite hard to kind of put aside time to invest in tools and infrastructure, but actually we found that it's been really useful and it has kind of paid back the time that we've invested.
Cool.
Thanks.
Hey, I'm a QA manager and I'm kind of interested in like how did you implement the process at the very beginning?
together, maybe together with QA, like how did that whole implementation kind of work?
So you mean how did we implement the process of like the bug flow around?
Exactly, and automation. I don't know, maybe it was a process you iterated on back from LittleBigPlanet or was this a completely new...
It's definitely been very iterative. So I guess it started off...
with Daniel Kidney writing Autobot.
So he kind of plugged together Autobot for us.
And we've added lots to it, and we've kind of connected it up to other things as we went along.
But yes, very iteratively, in that we kind of added that.
Dashboard was completely separate.
We already had dashboard.
And yeah, we just kind of added things in.
Whenever we see...
a problem with the workflow, we try and fix it.
And then it kind of gradually gets better.
But there wasn't some kind of grand plan at the beginning.
This is what we're going to do.
It just kind of happened.
Thank you.
So the last two questions answer my question.
I just want to clarify.
So this whole summary of your current tool setup, it started, you started arriving that.
during Media Molecule's earliest projects?
Is that true?
Like, it didn't just start with your current Dreams project, right?
I suppose.
So Dashboard we had for LittleBigPlanet and Tearaway.
So we've had Dashboard for a long time.
Autobot is new for Dreams.
We did have some automated testing on LittleBigPlanet, but it was very error prone, and it was mostly broken.
And we didn't kind of give the resource to kind of develop it and make it stable and make it useful.
I guess we were more concentrated on just kind of shipping the game rather than investing in tools, whereas like this time around, we've had a bit more time, so we've been able to write more cool stuff.
And the Chrome extension is completely new.
because I was messing around with Chrome extensions in my free time, but I went along to a game jam and was writing a Chrome extension, and then realized quite how powerful it was, the fact that you could take this HTML web page and just insert your own HTML, so you could just add buttons wherever you wanted, which is really cool.
So yeah, that came about just kind of by lucky accident after a game jam.
So yes, most of it is new.
Thank you.
Oh, and I just thought of another tool that we have that I didn't actually mention, but it's really cool.
So we have a tool where you can record video and you can record performance data in the form of telemetry, and then you can display them together.
So you can...
see where you've got performance spikes.
You can click on the performance spikes in the web UI, and it'll take you to the video of where the performance problem was.
You can see what was on screen and what the game was doing at that point, which I didn't talk about in this talk because it didn't fit.
I didn't have time, but it's very cool.
Okay, sorry for my accent, but...
You, what was the principle motivation to do your engine deterministic?
It was for doing a deterministic replay or to do network and to do some love step and it was interesting.
Ah, so you're saying why did we make it deterministic?
Because.
Your engine is not deterministic like most of the engine.
And even if I see the benefit of having deterministic replay, I'm not sure if I want to invest in making my engine deterministic only to have deterministic replay.
Yes, fair point.
It is a lot of work making your engine deterministic and keeping it deterministic.
So we use Autobot to check for out of syncs.
But we do regularly find out of syncs and have to fix them.
I guess we already wanted it to be deterministic, because if someone makes something in Dreams, we want it to play the same on everybody's PS4.
We don't want people to be able to create levels that will run in multiple different ways, because then they could create something where you press the button and the door opened, and then if it wasn't deterministic, then some people might play it and they press the button and the door doesn't open.
And that would not be ideal for players.
They want to be able to create stuff and know that everyone will see that stuff exactly the same.
So that's one reason.
Also multiplayer.
It's very handy for multiplayer.
And yes, for reproing bugs.
And also another quick question.
You said for assign the bug first, you go to who was the asset.
But probably on your company, I probably don't want to be the guy that implement that area out of bound.
I will be annoyed by everyone after.
Have you tried over a joystick, like, who do the last step change in the call stack, whatever it is, even if it's not the assert, it's more on the top of the call stack.
Have you tried that or not?
So one thing we do do is, if we go back to the email.
Oh.
So in the email, you've got.
an idea of who checked in and broke it.
So you can see that there's three revisions there.
It's kind of small, but there's three revisions.
And you know it was one of those three that broke it.
So you do get extra information there.
But no, we've not tried anything other than looking at who added the assert.
And yes, it's a good point.
If you fall off the end of an array, I don't know who added that assert.
OK.
Thank you.
Thank you for the talk.
My question was, you mentioned that you use Autobot to test for really scary changes.
Have you thought about making Autobot part of a pre-commit, or like pre-check and process rather than post?
And I was just wondering if you had tried that out and how it went.
We haven't tried it.
We did think about it briefly, but IT are very nervous about messing with Perforce, so they won't even let me add.
a commit hook to kind of get the check-in comment at the moment, because you just don't mess with Perforce.
Great, thank you.
Hey, so it sounds like devs can use Autobot for one-off changes before they submit, and do you automatically run Autobot when they submit and they get results?
Is that every check-in?
And then, is that gauntlet?
It doesn't go to the main depot before it passes tests or...
So you're saying, how does it work in terms of when does Autobot run compared to when you check in?
And is anyone else protected from me submitting a change that breaks if I just say, do I get it right away or does it have to pass Autobot before it actually goes in?
So what happens is you check in the change, so it's immediately available and anyone can get latest and will get your code.
Then Jenkins puts the build together of the latest code.
Then Autobot spots the new build and runs the tests and then emails the results.
So yes, you do have like a 15, 20 minute window where the code has not been tested.
We don't really know what state it's in.
So you might get latest and get completely broken code.
So do you have solutions for that, tools to help with that?
It's a constant problem in the studio when people sync, and it just doesn't build, or it crashes, or something.
We don't.
What we do is the person who broke it has to fix it really quickly.
That is our current scheme.
Yes, we could think about kind of, I know that some companies have schemes whereby you check into a branch, and that branch only gets merged across if it passes a bunch of tests.
So we could look at that.
but we haven't yet.
Thank you.
Hi, so if a member on your QA team comes up with some sort of a test case for something that should be covered by automation but isn't, the code necessarily isn't broken yet, but it should be covered via automation because maybe future code's gonna break it.
How do you have that QA team member handle that?
Like, do they write it up as if it were a ticket?
Do they contribute to the automation scripts themselves?
Or I'm just curious as to that process.
So the tests that get run in Autobot are completely controlled by QA.
So if a QA member came up with a new test, they would record a replay of that test.
They would use the web UI to submit that replay to Autobot.
Autobot would run that replay once and check that it completes.
Then it would be added to the database as a new test.
And then that QA member could go in to the web UI again.
and say I want that test to run as part of the smoke test, or as part of the sculpt suite, they get to choose which suite it goes in.
And then every time that suite runs, that test will then run.
Thank you.
You said that keeping a game deterministic has been difficult for you.
Could you give any examples of the main problems you see with determinism?
And are there any tips you have for people that want to make their games deterministic?
Initialize your variables.
That is like, I'd say 50% of our out-of-syncs are uninitialized variables.
Don't use rand, so use a random number generator that you seed and use deterministically if you want a deterministic random number.
Pointer values as keys.
Ah, yeah, we don't, we've not come across that one, but yes, pointer values as keys.
Sorry?
Fixed timestamp.
Fixed timestamp.
Oh, for physics.
Yes, I don't know in detail how Anton has made our physics deterministic.
So Anton has written our physics engine, and all I know is that it simulates stuff as if it was made out of balls.
But that is about the limit of my knowledge, I'm afraid.
Thanks.
My question is about integration of Autobot.
I guess with each game it might be sort of idiosyncratic how to get the replay working.
I mean, you can drop a user right into a certain location on a map in a game, but to recreate the exact game state, I imagine from game to game it's a little bit different, even if you have atomic...
unit tests and asserts in there.
I was just wondering if you have any recommendations for best practices for kind of writing that in an abstract enough way that would work across multiple games?
I suppose you need a bunch of functionality for it to work, so you need to be able to serialize out your game state in some way.
You need to be able to then apply inputs on top of that, so you would need a replay system.
I'm sorry, I've kind of lost the gist of your question.
Sorry, yeah, I guess it was...
I guess I'm just sort of wondering if there are any recommendations overall for writing a replay system and writing your code abstract enough to do the things like you're saying, like serialization of your game states.
I guess we've not really structured it particularly around...
the whole replay system, our code, although the replay system did go in very early on.
So, I mean, we've written some code that makes it easy to serialize out your game state and to take into account serialized revision.
So when you add a variable to the game state, we have macros that you use to add that variable or to remove that variable, which will automatically do, will generate the code for you that kind of checks against serialized revision. So we do have that. And then, yeah, I mean, the stuff for kind of saving out the inputs just kind of serializes out the inputs. And then the stuff for serializing out things like text that we've input, we have a buffer for kind of extra messages.
that you can serialize into, but different subsystems serialize into that buffer in slightly different ways just because different people wrote it.
We do have quite a variety of code at Media Molecule depending on who wrote it.
Quite often you can look at it and find who wrote it just by looking at what the variables were named and kind of how it was put together, which is kind of handy.
Cool, thank you.
