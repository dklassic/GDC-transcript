All right, so hi everybody, I'm Martin Walsh, and my talk is Modeling AI Perception and Awareness in Splinter Cell Blacklist.
It's a 25 minute talk, so I'm gonna try to keep it pretty tight, including this intro, but I do wanna let you guys know I'm from Ubisoft Toronto.
I was the AI lead on Blacklist, the AI tech lead on Splinter Cell Conviction, and I have about 10 years experience in the industry.
So why perception and awareness?
Well, I mean, if the AI in your game doesn't always magically know where the player is, you need some type of perception and awareness model.
And with more and more games incorporating stealth elements, how these models function can really shape the quality of the experience you provide.
And Blacklist obviously has a very heavy stealth component.
So for us, these models are extremely important.
But before I start describing them, I just want to talk about three characteristics that I think these models need to display to be really good.
So first of all, they need to feel fair.
I mean, getting detected can really be the difference between success and like chucking your controller at the screen, especially if you feel that that detection was unfair.
And second is they need to provide consistent feedback.
So you need to have some idea of how the AI is going to react, what they're reacting to, so that you can strategize and improve.
And the third is that they need to demonstrate intelligence.
If your opponents feel dumb, then you really get no satisfaction in beating them.
But what's interesting is not being dumb doesn't necessarily mean being smart.
And what it actually means is always being plausible.
And that's something that I'll come back to a lot throughout this talk.
And finally, it's interesting to note that there's often a conflict that exists between fairness, consistency, and intelligence.
And that's something that's important to be aware of.
And I'll allude to that as well throughout the presentation.
I'm going to talk about four types of perception and awareness.
But before I jump in, I just want to let you guys know, I'm going to stop saying perception and awareness because it's really long and annoying to say.
And although they are two different things, awareness almost always follows right after perception.
So in this talk, I'm going to use those two terms interchangeably.
OK.
So the four models I'm going to talk about are visual, environmental, auditory, and social and contextual.
with the two most common obviously being visual and auditory.
So let's jump right in.
So visual perception.
So often, this is how we think about vision in games, the classic vision cone.
And this does a decent job of modeling what's directly in front of the NPC and what he should detect pretty quickly.
But it's not a great model.
It doesn't do a good job of modeling peripheral vision.
It doesn't really model vision at a distance very well.
So this is vision in real life, right? This is how we see in real life. So we actually have close to like 180 degrees of vision, but as things move away from our focal point, we start to see them less and less clearly. This is vision in blacklist. So if I were you, I'd be thinking like, what the hell is all this crap, right? But honestly, all that stuff really is there for a reason. I'll describe a couple of things that you're seeing here. So first of all, we do actually have the standard vision cone, which is that orange V that I've marked out there. And this reflects the area where the NPC. So this is can see detail and should be able to spot the player pretty quickly.
And we actually had multiple nested cones.
So a player obviously would get spotted faster in the innermost cone.
And then we also have those coffin-shaped boxes that you can see on the outside marked by those yellow lines.
And this defines the entire area that the NPC should be able to see.
What's interesting to note about these boxes is that as opposed to a cone that expands at a distance, they actually shrink at a distance, which models the fact that you'll still tend to notice things that are directly in front of you.
even at a distance, but not necessarily the things that are off to the side.
And these are also nested.
And here we're displaying the outermost.
And so for each of these, our designer had to specify how many nested shapes there were, their sizes, and the linear detection range for each, where the min of that range is the time to detect the player when he's closest to the NPC and the max, obviously, at the farthest.
And our designer had to specify that per player stance, per difficulty, and per archetype.
So that was a lot of fun for our designer.
And then what happens at run time is these shapes get checked in order, and then the time range of the first inclusive shape is used.
And there are also other shapes there that I've pointed one out, which is the peripheral vision shape.
There's also those kind of wings that you can see in the back of the NPC, which is our sixth sense shape.
I'm not going to talk about those, but you sort of get the idea.
But obviously, being in a shape isn't enough to get detected, right?
You have to be seen, in other words, unobstructed, and you have to be lit.
And so what our detection model does is it recasts to eight different bones on the player's body, one per frame.
And then depending on the stance, it takes more or less bones to be detected.
So in this case, you can see Sam Fisher is in cover.
And so that's the stance that actually takes the most bones.
And here, there are only two bones that are visible to the NPC.
So detection hasn't actually started yet.
So then finally, once enough lit bones are seen, that's when the player starts getting detected, right?
And so that's when that range-based timer kicks in.
You can see it represented by that HUD, which is actually a growing arc.
And the tuning of this timing and feedback is really critical to the player.
This represents really his window of opportunity to break line of sight or maybe kill the NPC, otherwise he's going to get detected.
So naturally, with a model this complex, we tuned it a couple times by eye, and then we were good to go.
No, it was brutal.
It was brutal.
Tuning it was a nightmare.
And actually, so what happened was our designer tweaked it until he thought it was fun.
and our creative director thought it was fun.
And then we went to play tests, and the results were awful.
They were all over the place.
Some people thought they were getting detected too fast, some too slow.
So our designer combed through hours of play test video, and he identified three major issues that we had.
So the first was that we had some special case code.
So our designers wanted it that.
In certain situations, if the player was in cover, the NPC would walk right by him without detecting him, even if our model said he should have been detected.
This was actually fun in a lot of situations.
But we found in the video that actually what was happening sometimes is that the NPC's flashlight would be lighting up the player at random, and then he still wouldn't be detecting him.
So our solution to this, unfortunately, was just more special case code, which is essentially just if lit by flashlight, ignore the other code.
So not necessarily our finest moment, but it worked.
So the second issue that we found was that the players were getting detected too easily at a distance, and specifically at a distance when they weren't directly in front of the NPC.
So obviously, this is what led to us changing.
We originally just had regular boxes, and we changed them to these coffin-shaped boxes that I talked about.
And the third thing was that we were finding that players were just getting, that it was actually too easy overall.
So once we kind of got rid of those other issues, then actually the players found it too easy.
And so the solution to that was that we just kept tweaking.
And we literally did that till the end of the project.
And what happened was that after every play test, the results started to converge, so that by the final play test, very few participants actually mentioned vision one way or the other.
And the couple that did were sort of split on the issue.
So we really kind of felt like we'd hit our sweet spot.
So in the end, we were really happy with the results.
But it wasn't easy getting there, obviously.
And one thing we realized was that the players actually wanted a more sophisticated and less forgiving vision model than we had initially anticipated.
And if I had to do it again, I'd probably try to playtest this a lot earlier than we did.
So the second type of awareness I want to talk about is environmental awareness, which our illustrious mayor, Rob Ford, is doing a great job of displaying there.
But for us, environmental awareness really means two things.
So the first is being aware of the layout, right?
So layout tells you stuff like, for example, I'm in a large room, it's connected to a hallway by two doors, or maybe if there was a window by a window.
And then once you have that type of information, you can start reasoning about the player's location in interesting ways, right, with respect to the environment.
So you can start thinking about stuff like he's hiding in a dead end, or he's sniping us from a rooftop that we can't get to, and I'll actually discuss that specific example a little bit later on.
And then the second is awareness of objects, or more specifically, like, awareness to changes in objects, like a door that got opened or a light that got turned on or something like that.
So why is this important for us?
Well, one reason is that our encounters can start anywhere.
So you could have literally stealthed your way through 3 quarters of the level.
You get detected there, and that's where our encounter starts.
So that's in contrast to a lot of more linear shooters where they can stage the encounters.
But unlike a lot of stealth games, we didn't want being detected to be a failure condition.
We wouldn't want you to reset.
And in fact, we wanted you to then be able to play the game like a cover shooter with the added ability of being able to vanish or maybe outflank the NPCs and then recreate those initial stealth conditions.
So older versions of Splinter Cell only supported a max of four active NPCs.
So those guys always had something to do.
And shooting actually wasn't really an option.
But for us, in Conviction as well, we had 12 active NPCs.
And the player can see them all the time.
He has vision modes that he can use to see them through walls.
He could outflank them during combat.
So really, those guys need to be doing something intelligent all the time.
And using the old system, which again, was just designed for four, and also all they were concerned with was getting line of sight on the player, what happened was a lot of times you just see NPCs that were staring at the player through walls, which obviously doesn't look intelligent.
So on conviction, we developed a solution to this that we call the TEAS.
It's a new AI model of the environment for us.
So TEAS stands for Tactical Environment Awareness System.
So what this consists of, it's essentially position nodes that are linked to other position nodes through choke nodes.
And what that represents here is you can see that red position node that I've marked out, and it's above that blue area.
And so what it says is that blue area is connected to that yellow area.
You can't see the position node for that because it's inside the room.
Through that green choke node, which represents the door.
And so now, NPCs have a lot more info about the environment, right?
And so, in the case where an NPC can't, or maybe even shouldn't, engage the player, he can now go ahead and guard the door, window, or whatever choke it is that's leading to the player, and that obviously produces much more believable behavior.
And what's interesting about this, we actually auto-generated it.
So the way it worked was our level designers would paint the nav mesh.
So they'd paint the blue area and the yellow area.
And then we'd auto-generate the position nodes and the choke nodes.
But then we allowed the designers to manipulate them.
And the reason for that was that we actually used them not just to identify the area that's directly below them, but they were actually fallback positions for our NPCs.
So if they couldn't find a piece of cover that would give them a line of sight on the door, they knew they could always fall back to those position nodes.
Like in this case, you can see the designer actually attached to it.
a second position node to that choke node that I didn't mark.
And we actually used the T's for tons of stuff other than just that.
We used it for coordinating room entries.
We did hierarchical pathfinding with it.
And we even used it to calculate audio distance.
And I'll talk about that in a second.
And as I mentioned, the second thing is awareness of changes to objects in the environment.
And we actually got a ton of mileage out of NPCs noticing things like open doors or windows.
And to do this, we used a really simple system called the Changed Actor System.
So the idea was that.
when an actor would change state, it would create an event with a lifetime, and then the first NPC to witness that event would claim it, and then depending on his state and what was going on, it could prompt a minor investigation.
And so, I mean, this could have been used tactically by the player, but really it was mostly there to give the impression of intelligence and awareness.
And this was a really easy win for us, and it was actually something that was mentioned specifically in a couple of reviews.
And it's kind of ironic, but not all that surprising that the tease was this long and complicated system to develop, but nobody mentioned the fact that our NPCs could guard doors, although they probably would have mentioned it if they were staring through walls.
But as soon as the NPCs remarked that a door was opened, that's what people noticed.
But yeah, that's okay.
So the third type of perception is auditory.
So in general, it's pretty simple, right?
Every audio event has a distance and a priority.
If an NPC is in range of that event, then they'll hear it.
And they'll react differently based on the event, based on who else is in range, his current state, et cetera.
We actually have two major issues with our audio events.
So one was actually accurately calculating the audio distance.
So obviously, the Euclidean straight line distance isn't good enough.
In a stealth game, you don't want to be heard through walls and stuff like that.
So we tried using our sound engine, but our sound engine wasn't actually optimized to calculate sort of uh... the distance between two random points in the world. And there was another issue as well which is that our audio data was often out of sync with our level data which caused all kinds of false positive bugs and stuff like that. So we started using the T's and the way that we did this was And the Ts could tell us sort of the room path, or like the path in rooms between the source and the destination, which would be where the NPC was.
And then we just took the sum of the straight line distance.
So in other words, it was the distance between sort of the source of the audio event to the closest choke node to the closest choke node of the room leading to the next node, et cetera, all the way to the NPC.
And the results were actually accurate enough that once we made that switch, we never looked back.
It worked really well for us.
And the second problem with auditory events is it's really a general problem actually, which is that it can feel really unfair to sort of be creating sounds that get noticed by NPCs that you don't even see, right?
A guy around the corner could potentially hear the sound that you make and you won't know why, and you may not even know that you made a sound, right?
Because you don't really directly control the sounds that you're making.
And so what would happen is, the way we found this was in internal reviews, our creative director, he'd be playing the game and then he'd get detected and he wouldn't know why and he'd check the controller and he'd.
And he'd look at me.
And sometimes, what we'd do is we'd pause the game, and we'd free camera on the corner.
And we'd say, look, Max, there's the NPC, and here's where you were.
And sometimes he'd agree.
He'd be like, yeah, I guess it made sense that he heard me.
But that didn't actually stop him from complaining the next time it happened.
And ultimately, he was right.
I mean, it's kind of irrelevant.
The player can't pause the game.
The player can't free cam through the level.
And so one thing that we learned here, and also in tuning some visual stuff, is that it's really only important what's plausible from the player's point of view.
It doesn't matter what the NPC can see or hear.
sort of from a simulation perspective.
It's what the player thinks the NPC should be able to see or hear.
So I mean, one way that we could have solved this was like brute force.
We could have just tuned down the distance of all of our audio events.
But then that would create a bunch of other problems, right?
Like you don't want to be kind of running down a hallway towards an NPC with his back turned, and then he can only hear you sort of at the last second and you'd stab him in the neck just as he's turning around.
Because not only does that sort of make them feel dumb, but also it's game breaking, right?
You just run through the entire game, stabbing everyone in the neck.
So this was a big problem, right?
I mean, if you think about it, for two NPCs at exactly the same distance from the player, it can feel fair to get detected by the guy that you can see and unfair to get detected by the guy who's right around the corner.
So I mean, we considered having some HUD feedback for this, which might have helped, but I mean, design really wanted minimal HUD.
So in the end, we solved this in two ways.
So the first thing we did was that NPCs that are off screen and far enough away from the player that it's sort of plausible that they didn't hear had their hearing reduced by half for some events.
And we actually applied this to some indirect visual events as well, like seeing an NPC get shot.
And the result of this, that literally the game instantly became more fun and almost as important for me, our creative director stopped complaining.
By the way, that image is not supposed to be our creative director complaining.
That's an NPC you can't hear.
And the second thing we did is we wanted to provide better feedback to the player.
And we actually overhauled our entire bark system, our entire dialogue system on Blacklist.
And because if the NPC calls out what he hears, if you're running and then you hear an NPC go, hey, I heard footsteps, then you instantly know what you did.
But the problem is you obviously don't want to hear that line over and over again.
It's pretty on the nose.
And so what we did is we developed a three-tiered bark somewhere.
the first tier were very specific, all the way down to the third tier that was much more generic.
And then what we do is as we ran out of tier one barks to play, we dropped down to tier two and then tier three.
And so the result was that as a player, you sort of got the specific feedback you needed the first couple times that you did something.
And then, you know, like I said, as we ran out of those specific things, then we could give more generic barks back to the player.
So the last type of awareness that I want to discuss is social and contextual.
And this is something that actually a lot of action games don't model at all.
So we did this for two reasons.
So first of all, it makes the NPCs seem more intelligent and more aware, right?
And in our game, you spend more time looking at the NPCs than you do in a lot of games.
But we also use this system to create anti-exploits in the form of group behaviors.
And I'll talk about that in a second.
So specifically I'm going to give you two examples. Um, the first I'll talk about our conversation system, um, and in the second I'll show you one of our, uh, combat anti-exploit strategies. So in a lot of games if you interrupt two NPCs talking, um, it usually does one of two things. It either kind of completely breaks the conversation and those NPCs just kind of walk off on their own, or the conversation restarts somehow, but it, it feels pretty robotic. Um, so in our game when NPCs are in a conversation...
they're actually in a group behavior.
And so what that means is that they automatically become aware of each other.
And that allows them to actually discuss minor events in context.
So we had our narrative guy wrote specific reaction and breakout and rejoin lines for each conversation we had.
And you'll hear that in a second.
And also, you know, if one NPC stops talking or doesn't respond because he went to go investigate an event, the other is aware, so he'll, you know, maybe go and investigate the spot where he thinks his buddy should have been.
And just those things really give the impression of social and contextual awareness.
So here's a little video that demonstrates what I'm talking about.
Hell of a night to be out here, isn't it?
What?
You get a little wet, you think you're gonna melt.
It's slippery out here.
A little wind, I'm taking flying lessons.
I'll make sure to...
I'm just kidding.
Good boy. You made it happen.
Make sure Sadiq and his last few family will be here.
There he is again. Nowhere to run.
Fine. I'll take a look.
So yeah, we really like that.
And so yeah, the last thing I want to talk about is the combat anti-exploit I was referring to.
In this case, it's unreachable.
So depending on where the player is and what the situation is, the AI may not be able to, or may not want to directly engage the player in combat.
And in that case, we had group behaviors, which would play out as the situation unfolded, and specific dialogue lines that would give feedback to the player on what they were doing.
But what's interesting to note, and what you'll hear in this video, is that those lines aren't just call-outs.
They don't just announce, I'm throwing a grenade, or I'm flanking, or whatever, but they were actually discussions.
So it didn't feel like the NPCs were just announcing what they were doing.
but it felt like they were tactically coordinating.
And this actually gave more interesting feedback to the player.
So in this example, what you're going to see is the players ambushing the NPCs from an unreachable rooftop, which we know thanks to the Ts.
And you can hear them call that out.
And you can hear them coordinate a strategy to deal with that.
And so the way we did this was, so first of all, like I said, we could identify the player was in an unreachable area.
because we knew that the choke nodes that were leading to, like all the choke nodes that were leading to the player in Ts were non-crossable by NPCs.
And also, I didn't mention it before, but we gave the LDs the power to sort of mark up those areas a bit more so we could know if he was on a small rooftop or a balcony or whatever.
And then when we detected that scenario, we would kick off a contextual group behavior.
And we actually had different varieties of those and variations within them.
So for example, if this was, the second or third time that they'd seen the player in an area like this, they would have used the again version, which just had some dialogue substitution.
So yeah, take a look.
We can't get up there.
Relax.
We'll find him.
He's still up there, right?
He's not here anymore.
Move out and find him.
So and one thing that's interesting there is if, so here, there the player sort of tried to outflank the NPCs, right?
And so that's what you saw.
But if you would have started picking them off from that rooftop, that behavior would have branched into the retreat behavior, where one of those guys would have called out, you know, he's ambushing us and.
um, um, started laying down suppressive fire while the other guys, uh, fell back to cover.
And again, we, we were able to find good spots, kind of hiding spots for them, uh, also using the Ts. So, that is it. Just before, I just want to say thank you to everyone who came out to support from Ubisoft, to Nick, Zach, and Heather for helping me out with this presentation, to the whole AI team who made all this cool stuff, um, and everyone we worked really closely with, Laurent, Chris, Josh, and the entire Blacklist team. Thanks a lot.
So yeah, I think I actually do have time for questions.
I guess those videos said it all, huh?
I have a question about the, here, over here, this side.
Oh, wow.
Great lights.
Oh, there you go.
The question about the coffin-shaped perception boxes.
Yes.
Did that help a lot in terms of if the player's on a different level, he doesn't expect to be perceived, or if he's climbing onto something?
So that's a really good point.
Yeah, I didn't mention that, but yeah, like you saw those boxes were capped at the top.
That was exactly it.
And that was actually a metric that was sort of used by level design so that when they would put pipes across the ceiling and stuff like that, they had to make sure that those pipes were outside of that box.
Yeah, absolutely.
Thanks.
So when you have these cooperative tactics like flanking and covering fire and things like that, who's making the decisions to choose those tactics? Who's like giving commands to those characters? Is there like a leader follower relationship? Is there a centralized AI that does that? So we have, yeah, we have essentially a director, an AI manager essentially that would detect those scenarios and then...
essentially kick off those behaviors and we would elect, so some of those scenarios did have leaders, but we would elect those leaders on the fly essentially, but we would prioritize certain archetypes.
You didn't see it there, but we had, for example, a heavy archetype, so if a heavy archetype was present, he would become the leader of that group.
Hi, so good talk, first of all.
One thing is, did you think that balancing the perception, because you said that you had some kind of balancing on the different difficulty level for the game?
Yes.
So is that enough for giving a good experience to the user?
Because sometimes, if you play an easy game mode, then you will get a really different experience for who plays the heavy one.
But you get an experience different from their behavior side, and not only from the fact that the game is simpler or harder.
It's true, and I mean it's a tough balance, right?
So yeah, you don't want to like completely nerf the AI for, you know, in low difficulty.
Yeah, absolutely.
It's a tough balance to, it's tough to balance difficulty during stealth, like overall combat balancing is a lot easier, I think.
So yeah, it's.
I don't have a good answer, sorry.
But it's more like, how much did you leave to the level designer or the game designer to balance?
Was it a bit less control to them to avoid that they were nothing to watch the AI?
We gave them a lot of control.
But I mean, we work really closely with our, the AI designer is with us, and our lead designer was always with us.
I don't know if you've been to any of the AI summit.
Sounds like there's this big divide between.
design and programming sometimes.
But for us, it's not.
We're really one big team, and so we work together.
They're not going to do anything that's going to make our guys look stupid, basically.
Thanks a lot.
Thanks.
Question on the perception again.
With regards to those coffins, did you link that to any kind of bone on the NPCs, in terms of if they were looking, or was that completely based on facing angle or the root node?
So that's a great question.
We actually did some kind of complicated stuff for that.
So when they're in idle, we actually linked it to the facing.
And the reason we did that was we wanted to give some ability to the animators to create some kind of cool idle animations without kind of like.
you know, it sucks if you're kind of sneaking up on an NPC and the guy turns to sneeze and because he did that, you get detected, right?
Whereas when they were in search and in combat, where their sort of facing was a lot more, or sorry, their head look was a lot more deliberate, then it was linked directly to the head look.
So, for example, to touch back on what Alex asked earlier, if they were going upstairs or if they were looking up, then the entire thing would tilt with them, basically.
Thanks.
Sorry, I think there was someone over there waiting.
Yeah, just a quick question regarding using the Ts for audio.
Yes.
I'm just curious, did that thing exist beforehand?
I was curious if you could have done the same with maybe like a, what do you call this?
Pathfind?
A pathfind function, basically.
Well, but the problem with pathfind is you need navmesh, right?
So then if you want to know if one guy in a balcony hears another guy in a balcony, you don't actually want the pathfind distance, you want some other distance.
Right.
So for us, any two areas that were linked, whether it was sort of walkably or visually, those were all linked in that way that you saw.
So there it was a door, but I sort of briefly mentioned that when Sam was on the rooftop, we knew that, and we knew it was unreachable because all of the nodes that were linked to him were non-walkable.
There was a teased node up there.
Exactly.
Yeah, just about the difficulty balancing.
Did you actually change the distances and the shapes of those objects, or did you more like balance the time that it takes to actually detect?
Everything.
OK, excellent.
Thanks.
I had a quick question following up about the T's.
So let's say when Sam was up on the rooftop and he would make a pretty significant noise, would the AI not really be able to hear him because the T's were not connected?
No, no, that's it. It was connected, right? So like I said, it was connected, but the choke node that connected it was non-walkable.
But they did that mattered for the NPCs to know that they couldn't get up there, but it was still linked through T's essentially.
Speaking of noise and detection and stuff like that, I noticed in one of your screenshots, Sam was crouching next to what looked like an air conditioner or something like that.
Did you model the noise that he was making with ambient noise as well?
So there's a relative thing, you know, if you're in the middle of a factory or crouching next to, or you had your rainstorm, for example.
Was there different thresholds of noise?
that we considered doing but we didn't actually do it. We had a couple of areas in the game where we did some special stuff for that but it wasn't a generic thing that we did. Oh, did you want me to make that announcement for you? So I have a special announcement from Dave. I don't know if Aaron canary is here but Dave has your laptop charger. Should have come. Hey.
if you had any kind of, when one AI actually found the player, if there was any other system that actually sent that awareness or perception to the other AIs, or if they all immediately knew about it?
No, I mean, we had to propagate explicitly, right?
So the idea was that if you, yeah, I didn't actually talk about that.
But the idea is that even once you got detected, you were only detected by that single NPC.
And he had to propagate that detection somehow to all of his buddies.
Either by gunfire or by yelling, basically.
So was there, would they have to, would they talk to each other? Would they have any kind of bonus to find Sam now that someone has seen him and is engaged? Or are they all basically stuck in the same awareness routine that they were already in?
I'm not sure I completely understand the question but basically what would happen is, you know, it would, it would, so, so, you know, let's say all of the NPCs are in idle because nothing's happened yet. And then, you know, the, the, one of them spots Sam and he starts firing, then because another NPC heard gunfire, that would then transition him into combat essentially. And then he would be in combat against the player and then, you know, like I mentioned earlier, if you were able to outflank them and disappear, they would sort of go back and start searching for you.
So, uh, kind of a follow up on that one. Uh, how much how much did AI's behavior change if they knew that your character was there?
Because, yeah, I was wondering because there's a lot of games that it's like, and the one I think of immediately because I've been playing it the most recently is Skyrim, where it's, oh, that was just the wind.
You've got a dead body right next to you.
Clearly it was just the wind.
Yeah, that's a really good question.
Yeah, we did a lot for that, actually.
So, first of all, we never, ever let them return to sort of pure idle.
you wouldn't be in a gunfight one second and then munching on a sandwich or whatever the next second. But the other thing, we actually had a difference between sort of pre-combat and post-combat search. So in that one slide where I talked about barks, there was this weird screenshot that probably nobody got, but that was a piece of our entire barks matrix. It was this huge kind of table that had all that stuff in there. So our narrative guys did a fantastic job with that. And basically, yeah, it altered their behavior so they would investigate differently.
and altered their dialogue. So there was a difference between, you know, like, like, we think someone's here, you know, he's made some, something's made a bunch of noise and we're looking around versus there's a pile of bodies or we, you know, there's a, we've just been in a gunfight. So yeah, absolutely. We took that.
So there were several stages of...
we don't think, you know, nothing's here to, there might be something here to, we know there's something here to, there's something here that's trying to kill us. That would change it. Yeah, even to there was something trying to kill us and now we don't know where it is. Yeah, absolutely. Okay. Cool. Alright, that's it. Thanks a lot guys.
