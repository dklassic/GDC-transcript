All right.
Looks like it's go time here.
You guys all ready?
All right.
So I just want to say thanks to GDC for inviting me out here.
It's a real honor to be up here on stage in front of you all.
And I really hope you enjoy what you're about to see.
My name is Rick Lico, animation director at the new VR game studio PolyArch.
I've been animating in the gaming industry professionally now for about 18 years, primarily focused on the gameplay animation role.
You may recognize my work with Quill here, or from many other games, series such as Destiny, Halo, Condemned, many, many more I've had the sincere pleasure of being a part of.
Today, I'm here to talk about animating Quill, the protagonist of our first game, Moss.
But for those who may not know yet, who is Quill and what is Moss?
Your time has come at last, dear reader.
But this story is not yours alone.
No.
It is tied to another.
When all the paths are overgrown And evening falls with crimson glow Into the forest I will flow And in its shadow fall Your bond with Quill grows strong.
And the journey you take together could change the fate of both our worlds.
Shall we begin?
All right, the world of Moss, it's vast.
And it includes many stories.
But we decided to start with Quill and the adventure she takes with you, the reader.
Luckily, people seem to like the idea of adventuring with her.
In fact, I've never seen people have such a stark emotional reaction to a character I've worked on before.
And much of this reaction happened prior to the release of the game itself.
People often talk to her like she's listening.
They'll feel protective of her.
They'll rely on her for help and want to help her in return.
People seem to feel a genuine emotional bond with this little virtual mouse.
But what is it about Quill that creates this emotional response?
Now, this is the question driving the purpose of this talk.
So over the next 45 minutes or so, I'm going to try and answer this question from the animator's perspective.
So online, and even here at GDC, you're going to be exposed to an incredible amount of information about the latest tools and technologies.
But it's really quite difficult to find information and steps you can take to draw emotion from players.
This is inherently a subjective topic, but I'm going to try my best to make it less subjective by describing the constraints we have faced and the steps we have taken.
My hopes are that by hearing our story, you'll be able to take home some ideas you can use for your own characters.
So to help frame our constraints, I need to give you a little information about PolyArch the studio.
Now, PolyArch consists of 15 full-time employees.
We're a handful of industry vets from the AAA space, and we're taking a risk making our first game as an indie startup.
And we're learning a lot about what it takes to make it on our own.
We've learned just how expensive it is to run our own studio, that people are the largest cost of business operation, so it's essential to keep the team as slim as possible.
that getting the attention of gamers and creating an experience that stands out is super challenging and that despite our situation, Moss will be judged in comparison to game staff by much larger and more substantial budgets.
So to help overcome these constraints, we focused our animation efforts towards these three main principles you see here.
Building a character from honest life experiences, creating compelling characters in VR, and creating a realistic workflow that enables us to accomplish our goals.
Now, let's begin by illustrating why we've chosen to go the VR route.
If you look at the history of our industry, the competitive playing field is often reset when new technologies and markets become viable in the retail space.
And a good example of this is mobile gaming from 10 years ago.
In these marketplaces, the competitive field is much smaller, with the expectations relatively undefined.
We knew it would be incredibly difficult to go toe to toe with well-established and well-funded console and PC developers as an indie startup.
And also, the technology is really interesting to us.
It's super exciting to be working in VR because it's unexplored terrain.
There's so much undiscovered potential, so we decided to take advantage of what we saw as the next industry reset.
But VR introduces some unique challenges we've never really encountered before.
Everyone's first VR experience is usually fairly shocking.
And it creates a sense of realism and a feeling of immersion.
Yet many of the early VR character performances found in most games were anything but immersive.
They often relied on the tried and true methods of the past, where characters are simple avatars who respond to basic gameplay inputs and basic AI routines.
Two years ago, when we started our journey, we made a demo for Moss where Quill was just that.
She was a simple gameplay avatar.
You can see here some of the early footage of the world of Moss and Quill herself, and she doesn't feel alive yet.
If we would have stuck with this approach and simply polished what's here, she would have been maybe proficient, but not memorable.
It's because she's merely a gameplay abstraction here.
Now decades of gaming have trained us to accept certain abstract concepts.
We press buttons on an input device, and a character on screen in front of us performs an action as a result.
This is completely normal to us gamers now.
But if you were to show somebody this from 300 years ago, what a traditional game was, they wouldn't understand it.
The interface and the resulting actions, they're really a learned experience.
And this learned acceptance of the abstract.
makes us more forgiving when we see our game characters merely react to controller inputs instead of acting like living, breathing people and creatures.
We've gotten used to seeing things like popping between animation samples, playing a limited, repetitive set of animation clips, or defying physics with reckless abandon.
These are all standard issues we see every day as gamers, and we've grown numb to them.
But VR, it relies on immersion to be most effective.
And characters who pop and play repetitive animations and defy physics even slightly, it really removes you from the experience.
But more importantly, characters who pander to gameplay systems, acting as simple avatars devoid of personality, have a substantially negative impact on VR's immersion.
You're inside a world that feels real, but you're joined by creatures who cater to you as the player.
it reminds us that we're just playing a game.
Now I'm not saying player avatars no longer have a place.
In Moss, you play the role as the reader who intentionally has no predefined personality, so you can define it with your own.
The reader is our avatar character.
I'm referring to characters you interact with inside of VR, such as Quill here.
In the world of Moss, you and Quill exist simultaneously.
You, as the character in the game, can reach into the world and interact with it in natural ways.
while the movement stick and the face buttons of your controller, they guide Quill much like a standard third-person action adventure game where X is jump, square is attack, both of them are dodge.
So to help Quill be more than just a standard third-person action avatar, we need her to be able to interact with the player in more natural ways, more meaningful ways.
When you're in VR, you're not just witnessing the character's adventure, you're part of it.
So we can and should break the fourth wall in VR.
But how do we do that?
Well, the most obvious way to do this is to simply have Quill make eye contact with you.
Looking people in the eye is a powerful way to connect and is no different here.
The frequency of this and the times when she chooses to do it will tell you what she's thinking.
Now, if you saw your friend from across the room and wanted to get their attention, what would you do?
You wouldn't look at him and press the X button.
It's just not how people work.
Instead, you'd wave at them, right?
This is one of the advantages of VR.
Waving is natural, and having a character that responds to what feels natural, it really begins to make them feel alive.
So, remember that fictional gamer I mentioned from 300 years ago?
They'd instantly get this.
Look at that. I am so glad I'm not the one sliding.
She's jumping on the lever.
Good job, Quill. Good job, Quill. Do not jump.
Oh, she went back!
Hi!
So, Quill will tell you when she's had enough waving, too.
Yeah.
By having her do this, we're establishing personal boundaries like real friends have.
From the player's perspective, she's not just a representation of a gameplay system.
She's asserting her own personality here.
Now being able to pet Quill and have her react to it is another way to make her more than just an avatar.
Every time we add something like this into the gameplay space, we help tell her story outside of cinematics and the traditional narrative.
Oh, and she will get sick of being pet too much.
Smack that hand away.
Setting those personal boundaries are really important in a friendship, right?
So a lot of people who played our E3 demo really wanted to high five Quill after beating some enemies or solving a puzzle.
But they didn't really expect her to high five them, did they?
I mean, that's not what a character does in a game.
They do now.
And if you let her wait too long, she'll high five herself.
You know, she doesn't need you.
She's a confident and secure, self-reliant mouse who doesn't need your approval to party.
You missed out.
So many games often use a hint system to help players stuck on puzzles or lost in an environment.
And this is often done inside the game's UI.
But we couldn't do this in VR.
There's no UI.
We had to find a way to communicate the same information, but in a way that fit the platform in the game's design.
Yes, you.
OK.
Oh, you're going to go around that way.
Pick him up.
OK, come back.
All right, cool.
I think I know what you mean.
So by helping you solve puzzles, Quill becomes your partner.
It implies that she's capable of complex thought, not just sitting in an idle animation awaiting your next input.
This helps to build a bond with the player.
Speaking of the player, having an elephant charge at you in VR is absolutely terrifying, unless you're not looking at it.
In VR, the player's head is the camera, so they may choose to look anywhere.
Traditional games have been struggling with this as well.
How do you make sure the player sees what you want them to see?
In VR, this becomes even more challenging since looking around in 360 is part of the experience.
So, forcing the player to look where you want them to, maybe that's really the wrong perspective to have of the problem.
For thousands of years, people have been entertaining others who can choose to look away.
So instead of approaching this problem like we would a traditional game or even a film, why not think about this as a stage show?
In the image above, events of interest will be happening on that stage, while the rest of the environment is really just there to support it.
By thinking about our characters like they're performers on a stage, it provides us with a more relevant perspective.
But even with the most interesting things happening where we'd like you to focus, we can't guarantee the player will choose to watch.
So what do we do?
Well, there's the obvious things, like using sound cues, lighting, and motion to help draw the player's attention.
We do certainly utilize these tricks, and they do help some.
We can also choose to accept it and simply provide players with enough opportunities to understand the game without seeing everything.
Hopefully, players might even want to go back and see what they missed.
But the most effective way we've discovered is to use interactivity as a form of camera control.
You're not just controlling Quill, you're partnering with her on her adventure, which means you're inside the world with her.
There may be a door too large for a mouse to open, but certainly not too big for a human.
By giving you a role in the world, we know where you'll be likely looking, and we can target Quill's performances around these interactive moments.
We can even give her multiple performances based on her position relative to the interactive object, since we know you'll see it.
So to sum up the first section, VR certainly adds new opportunities for character exploration beyond traditional gaming due to its more natural forms of interaction and immersion.
We're really just scratching the surface, but maybe, maybe if we do this right, we can make a name for ourselves.
But VR is not enough to make Quill stand out.
There have been tons of amazing, memorable characters who define gaming.
So I've heard somewhere that Mario's more recognizable than Mickey Mouse these days, which blows my mind.
The competition is absolutely intimidating.
So how can we possibly make Quill stand out, make her a character that people will remember?
Again, I'll illustrate our constraints to frame how we've chosen to answer this question.
Now, among our staff, the average number of years in the industry is 11 or so.
And most of us came from Bungie, a studio with an intense laser focus on gameplay feel.
So we as a team, we've got a lot of experience making solid gameplay.
But now we're faced with the challenge of creating Quill, a partner character which you directly control, who has the emotional depth of a friend.
And this is uncharted territory for us.
So we started by defining what we didn't want to do.
So to make Quill stand out, we knew we wanted to avoid common gameplay tropes.
So what are common gameplay tropes, you may ask?
They're the things that almost all game characters do out of necessity or even tradition.
They're so well-known that they transcend the character using it and become iconic in their own right.
Examples such as the ground pound here.
How many games have a ground pound in it?
It's insane.
Destiny has, what, three of them, right?
The fist clench, I'm going to get you evil guy, you know, like it's so common.
Giving Quill iconic actions such as these would be the best way to remove her personality.
Another example is the over idle loop used to keep characters looking alive while they stand still.
It was certainly a necessity of the past, but I don't think it applies anymore to modern game design.
And no cheese.
Like, we, honestly, it's such a trope. Like, we really just wanted to focus on honest actions taken from real life, not, not cheese.
OK. So, if we can't rely on tropes, what can we do? Well, we can observe people. We can observe the world around us. We could be students of the human condition, become amateur psychologists.
Should be looping.
Ask ourselves, who was Quill at her core, and how would she genuinely respond to her world and her context?
When confronted with a floating player head, how would she react?
Our goal wasn't just to write a narrative describing the events that took place in her adventure.
It was to create a world that you could inhabit and build a relationship with a mouse in.
Now let me ask this question.
What's the job of an animator?
Now, unfortunately, I've met too many non-animators that would respond with, well, animators create movement.
But as I'm sure you all know, the job of an animator is actually to bring a character to life.
To do this, you need to do a hell of a lot more than make a character move.
It's an animator's job to help create a history for their characters, to give them a perspective, to communicate complex emotion merely by seeing them think.
to further the narrative by showing how it affects your character's belief system and actions.
And who Quill is emotionally at the start of Moss is quite different from who she becomes at the end.
During your adventure with her, we hope you'll see her think.
We hope you'll feel her emotions, and we hope you'll believe that she and the world of Moss have a history.
So how are we doing this?
By giving Quill the ability to speak in sign language, we're implying that at some point, she learned this language.
This creates a history and a demonstration of her intelligence level.
But it's also very honest and based in reality.
Sign language exists to help overcome a disability.
Even though Quill can hear just fine, she's incapable of communicating verbally with you.
We've chosen to keep our use of signing fairly basic so not to alienate anyone who may not know it.
Within the proper context, even people who don't speak sign, like myself, will likely still understand the message she's trying to convey.
But honest performances, they don't end with sign language.
In this video, we expect most players to be excited to dig into Quill's world, so Quill invites you in.
This is one of the 40 or so response animations Quill will use to express her opinions and react to you.
By doing this, Her actions helped define her and reinforce the feelings we want you, the player, to be experiencing.
Quill was designed to amplify your experience, to be a mirror for your own emotional arc.
Her response animations were created with that reflection in mind.
So there may be the occasional moment where you're pressing a direction on the analog stick and she's just not reacting.
And honestly, isn't this like gameplay no-no 101, right?
Like, that's a bad thing, right?
And sure, it's often frowned upon by players and certainly caused our design team to puff up like pufferfish when I first suggested it.
But...
It's a calculated tradeoff to provide Quill with perceived autonomy.
To help minimize any potential frustration, we limit when and where we choose to allow Quill this autonomy.
Occasionally, we'll allow it after a player input, such as climbing a ledge or pulling a lever, actions that require a button press to activate, so it doesn't feel too out of place.
Otherwise, we relegate it to non-combat situations while Quill's standing still already.
Active players who often keep Quill on the move may miss out on a lot of her personality moments so we're actually still trying to figure out the proper cadence for this.
So I love Uncharted, The Last Guardian, Tomb Raider, the Assassin's Creed games.
They in my opinion represent some of the finest examples of characters who transcend the common game avatar.
and become real people and creatures.
Beyond cinematic performances, much of this is due to how well the personality of these characters are represented during gameplay.
They don't simply perform gameplay actions.
They show emotions.
They think.
And they react to their world.
And much like those games.
We focused on Quill's identity during gameplay.
By maintaining the uniqueness of her personality throughout her gameplay actions, we're supporting all the work we've done with her autonomous reactions and personality moments.
Our goal is to represent who she is at all times, not just give her a bucket of animation.
But gameplay feel is paramount.
Remember when I mentioned how pops, repetition, and wonky physics can ruin a VR character?
Well, by alleviating as many of those issues as we could, and there's still a lot of work to do, we helped craft a more believable performance that we honestly hope feels good to play.
And Quill also needs to feel fluid moving around her environment.
She should obey the laws of physics whenever possible.
She should be instantly responsive, respecting player intent 100% of the time.
She should have a robust set of animations describing her actions in unique and varied ways.
And she should do all of this without betraying her character or giving into common gameplay tropes.
And she is quite the risk taker.
Thank God there's no fall damage, right?
So just because we're a VR game, it doesn't mean we get a pass when it comes to the feel of combat either.
Combat needs to be instantly responsive, matching player intent.
It needs proper interrupt frames and a good flow to the sword swings.
Player will need to be able to evade with short invincibility windows like in a Street Fighter game.
So how do we do this without making Quill feel like a gameplay avatar?
Well, we animate her actions with honesty.
How would such a small, youthful mouse affect her world?
Well, she'd be doing it by giving 110% by wearing her emotional conviction on her sleeve.
Like any teenager, she'd approach her world assuming she'll never die.
So she'll act out of reckless abandon.
She doesn't just swing her sword, she swings her entire body and throws herself at the effort without a thought to her own safety.
We make sure she carries her personality across every action she takes.
And of course, we also have a traditional narrative to tell about Quill's adventure.
And due to our limited budget and team size, we actually choose to forego the traditional cinematic approach and instead tell our story inside of a book format.
Now, all of the honest performance decisions we make for Quill in gameplay we echo in a handful of animated pages.
And just because Quill is animating on a page doesn't mean you can't still interact with her.
We've made it possible for Quill to peer out of the book and track your location.
This interactivity helps to further the bond between Quill and the player.
And we're just scratching the surface with this idea.
And we barely make use of this in Moss.
This is actually the only time that we've got this working, because it was actually a real pain in the butt to set up.
But we honestly hope to take this prototype much further in future projects.
So acting autonomously, creating honest, personality-driven performances, focusing on gameplay while keeping her actions unique.
We believe these are the important factors in defining Quill as a character and giving her a fighting chance at standing out amongst an industry of titans.
But without enough quality animation content to support these performances, it actually just all falls apart.
And this final section details our process for creating enough animation to fill the world of Moss at a quality bar essential to his success.
Now, a basic understanding of animation terminology is going to be assumed beyond this point.
So let me just frame this for you.
Imagine you're leaving your secure job as lead animator at Bungie, so you may join your friends at their startup studio, making Moss as the only animator in Rigor.
You've got enough animation experience, but you've never worked as a professional rigger before.
Yet, you must make the animation pipeline from scratch.
Your wife and three kids come first, and you cannot sacrifice them to the game god, so a crushing crunch schedule, it can't happen.
And every dollar you put towards animation for Moss will shorten the time you have to make the game itself.
So you can't afford to hire or outsource indie startup and all, right?
One last thing, if the game fails because of animation, you've let your friend's lifelong dream die a horrible death.
No pressure, right?
So what do you do?
Well, looking holistically at the animation pipelines of my past, what does the animation team spend most of their time doing?
Well, number one, supporting a monolithic character pipeline.
Now, I certainly need a character pipeline in place.
But if I'm not careful, this can take a tremendous amount of time to create.
Now, how can I make a powerful and robust pipeline super cheap?
So number two, creating a system that makes iteration difficult or costly.
Making a game is all about chasing that elusive notion of fun.
What does fun mean?
So to do this.
There's obviously going to be a tremendous amount of iteration needed.
And it's traditionally a huge time sync, one of the biggest time syncs in game development.
Honestly, who nails fluid gameplay on the first try?
Even the 10th try, like, it's crazy.
It's so much work.
So I need to make sure we can iterate quickly, so animation was really never a limiting factor for design or engineering.
And the last thing, biggest time sync for animators is you.
Yep, you quill.
Animating to a high quality bar can be a huge time sink.
How can I streamline this process without sacrificing quality?
Now that we've defined the constraints and understand the importance and the needs of the project, we can focus on coming up with creative solutions.
So I just need to define one term before going into all of this.
When I talk about the rig, what I'm referring to is the animation control rig.
At PolyArc, we have the character setup, which includes the mesh, the skeleton, the sockets, the point weights.
Due to the constraints of working in a game engine, it's easiest if we don't alter any part of this character setup.
Other than adding the occasional leaf node for attachments, the bone hierarchy, it will not change once the character is finalized.
Now, our animation control rig, or rig for short, are the handles we interact with when animating inside of Maya.
When our rig is applied, it lives outside of the character setup and drives the bones using constraints.
The rig is not exported into Unreal and operates inside of Maya exclusively.
This allows us to alter our rig however we choose without altering the character setup.
So.
The best way to solve a problem is to make sure you fully understand the question and by not assuming that the optimal answers have already been discovered.
So I questioned the need for Maya files and storing rigs in reference scenes.
Now, we can save one source Maya file per character so we can preserve our materials, our display layers, and file settings, but beyond that, is there really a reason to save Maya animation files?
But what's the problem with saving Maya animation files?
Well, referencing prevents hierarchy changes to the rig, which is a workflow limiter.
And if we don't reference and choose to save Maya files with local rigs inside them, we'd need a batching process to distribute updates.
But more importantly, we use Unreal, which utilizes the FBX format as its source.
When we create new animations or make changes to animations inside of Unreal.
it outputs to an FBX format.
If we save Maya files and Unreal saves FBX files, won't this desync our data?
So my animation files come with a cost.
But aren't my animation files really a standard workflow practice?
Yeah, but each studio is different, and we need to forge our own path while being mindful of the reasons driving other studios' decisions.
So we decided to make Maya files optional for animation.
We can save them, but we no longer need to save them.
Any Maya file will be considered ancillary, not the archival format.
Unreal and all major animation packages, they use FBX, and so will we.
Future animators can work in whatever animation package they want, such as Max, MotionBuilder, Maya, they all use FBX.
Any updates that Unreal does to the FBX file is now accounted for.
Nothing's ever out of date, and we never need to batch.
But what about our animation rigs?
Where do our rigs live?
Well, we can store our rigs inside of a script.
We can start an animation by opening the source Maya file for the character we're working on, import any existing animation if needed from an FBX, and apply the rig over the animation data.
The trick here is making sure the rig doesn't cause any degradation of animation when applying.
But the benefits of working this way are simplicity and flexibility.
If I make a change to the rig itself, I simply update the script.
And the next time I apply the rig, I'm applying the latest.
And I can alter the rig as I please while animating.
And if the rig breaks, I simply bake it down, I burn the rig with fire, and I start over again.
But what about all the data traditionally saved inside of Maya files?
Are we really?
OK with not archiving that data anymore.
Well, fuck yeah.
Like, really.
Like, aren't those benefits the same constraints complicating iteration and requiring a huge suite of tools to support?
Like, tools that I don't have the time to create.
And we can still use animation layers and motion paths and the time editor and all these cool things that Maya has to offer.
And we can even save those Maya scenes if we choose.
We just need to be mindful that those scenes are temporary.
So as animators gain experience, they also gain workflow knowledge.
Yet in some circumstances, you may find brilliant scripting experts making workflow decisions for the animators.
Now, this usually happens because the person writing the tool or rig can be seen as the decision maker.
But the problem here is the workflow experts aren't the ones designing the workflow in that situation.
And when there's a lack of collaboration between the two, the tools can become inefficient or even a barrier to the creative process.
You may have a well-written tool, but it may not enable the animator.
So knowledge-based decision making is needed to empower the content creators.
Now, in our situation, we have the opposite problem.
I know animation workflow well, but I don't know Python.
And I need to make rigs that bake over bones without any loss or alteration of the motion.
Now, I'm sure I could learn Python, but that would take time away from the more important task, which is making the game.
And I could potentially outsource our rigs, but if they break, I'd be in the dark about fixing the problem myself.
And that costs money, too.
I'm also too much of a control freak for any risk like that.
So.
I know the output window in Maya will echo every action I perform as a MEL command.
And thanks to my experience as an animator, I can do all the operations to manually build a rig over existing animation without any data loss.
That's actually the easy part.
Therefore, I can use the output window's MEL to teach myself rigging by simply performing the steps and copying the output into a script.
Now, could a real rigger do better?
Abso-fucking-lutely.
But I'm all Polyarch has, and this is what I'm capable of doing within a tight schedule and on a tight budget.
This is actually how all character rigs for Moss were made.
And for future projects, I sincerely hope to collaborate with a brilliant rigger who respects knowledge-based decision making, because I'm fully aware that I should not be making or scripting decisions.
So how do we make the animation process faster and more iterative while keeping a focus on quality?
Well, we start by reducing the complexity commonly found in traditional animation rigs and focus more on flexibility.
Also by finding ways to automate some of the more routine steps.
Now, quick caveat.
I'm not gonna have time to explain how each and every trick I'm gonna show you here is made.
That would be an entire talk into itself.
But I want to highlight the high-level goals that make these workflows possible.
So optimal rig settings for the many different situations an animator will encounter is nearly impossible for one rig to do well.
For example, so optimal settings between posing and tweening can often be quite different.
Let's say you set up your character, your rig, with an FK spine, because posing your character is actually quite easy to do that way.
But tweeting with an FK spine, it requires a ton of counter-animating to prevent the head from wobbling.
And you wouldn't animate a bouncing ball by putting it at the end of a bone chain, rotating those bones to make the ball bounce, would you?
Even though I did, because I'm insane.
So why try and tween a spine this way?
It just doesn't make sense to me.
Yet, setting up a rig for optimal tweening has a completely different set of trade-offs that would impact how easy it is to do posing or some other steps.
Now, I can go into details on all the different approaches to rigging and the trade-offs they create, but the problem I always seem to run into is the same.
I need to alter my rig in meaningful ways as I work to be most effective.
Now this means being able to convert the animation data of any object into any space or relative to any other object.
It also means having the option to alter the hierarchy without any animation loss.
Essentially, opening the animation sandbox to as many options as we can, allowing for creative problem solving as we animate.
So how do we do this?
Most studios create rigs featuring numerous attributes used to drive everything from fingers to foot rolls.
The idea behind this approach is to empower the animator, to give them a fine-tuned control over the animation process.
But what it actually does is limit the animator's sandbox, forcing a manual animation process.
It prevents the animator from switching the relative space of the control without altering the motion itself.
Now this can lead to counter animating and editing curves that may not directly relate to the motion itself.
I'm sure many of you dealt with foot setups that have 20 million attributes on the foot and you're doing the foot lean, the foot roll, and then the balance and the bend, and then it's, what are these curves doing?
I don't know anymore.
It can get a little confusing.
Just to be clear, I'm not saying that custom attributes are a bad way to animate.
What I'm saying is that for me personally, they slow me down.
So at PolyArch, we're only using custom attributes for the facial rig controls only.
The ability to switch the space of any rig component is essential to our workflow, keeping all animation data in the form of translation, rotation, and scale.
maintains a simple and easy to understand interface, while also making a nearly unlimited sandbox.
And this huge sandbox allows us to create workflows which automate the animation process in many situations, rather than providing a ton of manual animation options, because manual animation options, they take time.
Any scripts I make are optional modules that can vary between broad and specific workflow optimizations.
Here, I'm going to give you a few examples.
So this is how I work with Quill's spine.
So I've got a motion loaded on there, and I've applied the rig, and the rig, when it first applies, is in FK, meaning I rotate lower in the spine, and it translates upper.
rotate their heads translating.
I do have everything set to world space rotations right now, so when I rotate lower, it doesn't rotate the children object.
I can switch between FK and IK.
I tend to like doing FK posing, because when I make changes to the spine, it moves it.
And then I can switch the spine into an aim space setup, where I'm just grabbing those controls and moving them around.
So I pose in FK, and I tween in aim space.
And the animation data is unaltered when I switch between those spaces.
Some of you may be asking yourself, well, how are you baking SplineIK into your character?
Well, I'm not.
I'm using a hierarchy of embedded aim constraints for the up vector as a child of the existing rotation of the individual bones to avoid any of the gimbal issues that you normally get with aim constraints.
And when I translate the rig components inside the hierarchy, it alters the rotation of the bones as an offset from what they were previously doing, essentially acting as spline IK, but it works with your Easy Bake Oven.
So the spine isn't the only place where conversion makes animation much easier.
Here's another example.
Have you ever tried to animate a tail swishing around and interact with the ground or a wall?
It can really be time consuming to do this well.
Well, here you can see that the tail's just flowing around in FK, and it's going through the wall, and it's going through the floor.
And here I'm proving that it's in FK, because apparently I didn't trust myself.
And then I'm converting it into the aim space that I used for the spine.
So now I can just grab those points, and I can translate them.
And the cool thing here is, I can take anything that's translating below the ground, grab those translation curves, and zero them out.
And now the tail slides along the ground.
And let's say that I don't want it going through the wall.
I find out the distance of the wall.
I make sure that those translations never go past that wall.
And now I've taken the FK rotations and turned it into interacting with the wall on the ground.
And I can even offset it in time.
And the cool thing about offsetting translations through space is when you offset rotation, you can cause wobble at the end of your chain.
But when you're offsetting translations in space and it's updating your rotations for you, you never get that wobble.
So manually animating your overlap and your detail, it can be a lot of fun for an animator.
That's one of the animator's favorite things to do.
But it also takes time.
And depending on the skill of the animator, it may or may not look consistent.
So leveraging physics to supplement this area is a huge time saver.
And it allows for more realism, if done correctly, and maybe even more consistency.
So this is how I work with physics on Quill's spine.
So you can see here, I started out with just a core posing of her waving.
And I'm just going to send things to auto tangent real quick.
And then I'm just going to clean up her up and down on her cog, just to give her some flow to it.
And that's all I did to get to this point.
And now I'm going to convert her spine into that aim space.
And then I'm just going to prove that it's an aim space.
And then I'm just going to run physics on the different spine components.
And I can run different levels of physics, and I can run them multiple times to give me the proper offsets that I want.
And now when I press Play, free overlap.
I don't have to do this work anymore.
It's done for me.
And I can even offset the middle of the spine in time because I'm in this AIM space setup.
And by offsetting it in time, I'm not screwing up any of the other bones or any other bits of animation.
I'm just creating even more overlap.
And it's because I'm in this type of a setup right here.
So finally, I've got one more cool trick to show you guys.
This is actually how I animate Quill's ears.
So I start out with just basic core posing, where the ears just hit a few poses.
And then I press the Animate This Shit button.
OK, now they're animated.
And that's just off of a few basic poses and what her head was already doing.
I could manually animate Quill's ears if I chose to.
And there are certain high action clips that simply require an animator's hand to accomplish, like when she's rolling on the ground and stuff.
But the point here is, it's an optional tool, not my replacement.
As an animator, I'm able to make plug and play scripts for common animation needs myself.
And this flexibility comes from a simple and open sandbox set of tools.
When we started Moss, I was actually animating all of this stuff by hand.
And I was only able to produce maybe three or four seconds of quality animation in a day.
But after I wrote all these little optional tools and little workflow tricks, I'm actually able to produce up to 15 seconds of quality footage a day for Quill.
And that really made the difference for trying to get this game done on time.
So rethinking the animation pipeline and the need for Maya files, preventing animation rigs from becoming overly complex and constraining, allowing for space switching, which in turn enables us to do outright automation of the animation process.
This is how I was able to accomplish the one-man team approach and how we were able to work iteratively without animation ever becoming a blocking issue for design or engineering.
And unlike characters from film, TV, and books, Quill isn't just a character you witness.
With those formats, you follow a character on their adventure.
You're a passive observer.
Quill is not just a character you control in a game.
With a traditional game format, gameplay mechanics and story events, they take center stage as the character becomes your avatar into that world.
Quill is a living, breathing character who you'll interact with in meaningful ways while you guide her on your shared adventure.
And that.
is why this girl's crying.
So this session is one of two talks that Polyer is doing this year.
If you missed it yesterday, Brendan Walker did a great talk on a lot of the back end and how we hooked up a lot of the information for Quill.
And it was a good companion talk to this.
And he also talks a lot about how to wipe tears from headsets.
So questions?
Hello. This is more of a design question, but with this sort of strong relationship you've established with Quill and the players for future games or places where she might show up, have you thought about persistence? Like if I play a game later with Quill, will she recognize me from our experiences in the first game?
That's a great idea. That really is.
It's something that we've actually discussed in the past.
So the question was, are we going to maintain persistence with Quill in future products?
And the answer essentially is yes.
And we're trying to figure out what that means at this point.
This is amazing.
I'm sure we'll all understand if the answer is no, but are you making any of your rigging scripts available publicly?
I don't think you'd want them.
I'm such a poor scripter and rigger that you guys would laugh your butts off if you saw my scripts.
But I do know that some people on the internet have seen my Vimeo tutorials and have made marketable scripts off of those videos.
Yuri Monturi, I don't know, he's a Spanish dude.
I'm not saying his name properly, I'm sure.
But you could look him up.
And the physics script for the ears and the body is available through him.
Thank you.
Wow, looks like we're wrapping it up pretty quickly here.
All right, well, I guess we'll call it.
So if you have any questions that you don't want to say out loud on the mic, come see me.
And thanks for being here, everybody.
It's been my pleasure to be here.
