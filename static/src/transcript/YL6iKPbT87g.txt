My name is Chris Evans. I'm one of the lead technical animators at Epic. I kind of specialize in special projects.
which may make me an odd choice to be giving this talk.
So over the years, since my university days, I've been a bit of a Michelangelo-phile.
I've been told that I have more books on Michelangelo than they have in the Stadel Art Library in Frankfurt.
This is actually a plaster cast of David's face that I have at home.
I've studied Michelangelo for a long time, and I'm a big fan and big geek and part of that was, you know, 1999 I was at SIGGRAPH because I'm old and I saw some of the work that was done scanning the David and I've always been interested in that. So I thought it would be an interesting thing to take a look at and that's what we'll be talking about today.
At Epic we really like complex challenges so we have this kind of 20% time where we can go and do our own little endeavors. I was the games chair at SIGGRAPH last year and we thought, hey, this might be a really cool thing to do if we can work it out with the Italian government and all the parties involved.
In the end when it comes to complex challenges, I didn't just want to get the statue in but I wanted to recreate the tribute and allow you to ride a scaffold to any part of the statue and really look at it up close. So to give you kind of a background.
on the scanning of the David. The David was scanned in 1998 by Mark Lavoy and his team from Stanford. It was a huge undertaking. It took them 30 nights of hard work. So basically every night the museum would close and then they'd wheel in their like laser scanner.
and set up these trusses and work on scanning the statue piece by piece by piece up until like 4 a.m. and then they'd pack their stuff away and go to sleep and the museum would open again in the morning.
But it was a true labor of love.
It's interesting.
They scanned so much data that it wasn't really pieced together until about 2009.
So that was a different paper. But that kind of tells you how high fidelity the data set is. It's down to pretty much the quarter millimeter the scans are.
It's been presented over the years at SIGGRAPH multiple times.
You may have seen many geometry processing papers that come out use the David data set or at least attempt to use one of the versions of the data set because it's very hard to do anything with the billion point data set.
It is a billion points. When I was in university, they released something called scan view.
Stanford released something called scan view. What scan view allowed you to do was tumble around like a 300 polygon representation of the David or of one of Michelangelo's other statues that they scanned. And then it would freeze. And when you stopped tumbling it, it would clear their server and send you back like a high res JPEG of what you're looking at.
So I was there in my dorm room eating all my bandwidth, looking at every nook and cranny of the David.
You could see the chisel marks, you could see the unfinished top of the head.
It was really, really cool.
I think back then, it was I think 1999 or 2000, back then I even tried to set up a little photogrammetry rig, but they could detect if you were taking the same view and just rotating a little bit or something.
But I just thought it was so cool.
I really wanted to see that data.
I was really into sculpture back then.
And even back then, he was just one of my heroes.
So the data itself, it has no UVs.
It was basically laser scanned.
There's no UVs.
It's all range data.
There's multiple representations of that data.
There's a 7 million representation.
There's a 50 million.
And there's a billion representation, which is the one that took about 10 years to piece together.
So these different representations, they don't really match each other. They're warped in different ways. So being able to use one and then reach up into another one is very complicated because the data sets aren't aligned. You can't just bake the billion down onto the 50 million or vice versa. A billion points is a lot of data.
I am one of my close friends, Mark Galant, we were having a discussion just about data sets this size and he was one of the original architects of Houdini and he was saying, oh, we're going to work on some geometry processing stuff, maybe we can talk about this.
And he's like, I just thought about it.
A billion points, that's like a gigabyte.
just if they're bulls. And he's like, let's make them vertices. Okay, now it's 32 gigs of RAM.
And he's like, connectivity? Yeah, I think you're on your own. But it's definitely interesting that data that was captured in the 90s, today we still just don't have a way to visualize. It's just so much data.
So I kind of talked about how the different representations that people had pieced together over the years didn't really match. We weren't going to, so it started as myself and then I slowly started building this ragtag bunch inside of Epic to take a look at this stuff.
one of our awesome character modelers on special projects. You may have seen him talking just the other day about the Andy Serkis work we did with 3Lateral or the Siren demo. I kind of roped him into this. What sculptor wouldn't want to see the data at such high res?
We knew from early on we're not going to be piecing together the billion point set. I could load individual voxelized chunks of it. We needed to figure out how we're going to cross back and forth between these different data sets.
There are lots of scanning artifacts. So the lower res datasets, people had already written test algorithms and stuff to fill a lot of the holes, to figure out a lot of the artifacts. As you got more, as you got higher and higher res, there were some pretty gnarly scanning artifacts and stuff that I also didn't want to start cleaning up by hand and I'm not, you know, I'm a technical enemy.
I'm not going to write an algorithm to help me with that.
Just to give you an example on the 50 million data set, this is how many issues one of the geometry processing programs found with that.
Some of the holes are larger than others.
Some of them are very small.
But it's all a bunch of issues that normal geometry processing pipelines that we use in games would definitely have a problem with.
So I'm going to talk a little bit about the pipeline that we went through to get to kind of shoehorn this into Unreal Engine.
So we started with one of the lowest approximations.
Well, I think, so this one came from the 50 million, like a down res to 10.
And then we took that into ZBrush.
From there, in ZBrush, we did a UDIM layout in ZBrush, so we tried to give it UVs.
And again, all of this work we're giving back to the Digital Michelangelo Project at Stanford.
So, whereas it had this representation without these things before, now Stanford's going to have a nice ZBrush model with UDIM layout and stuff for future people if they want to mess with the data set.
So the ZBrush sub D3 was 2.7 million but it was pretty much indistinguishable from the 50 because we had vector displacement.
Like with ZBrush, as you go down under the hood, it retains a lot of those details.
And for me, because it was a very difficult rigid mesh alignment problem. I wanted to get into that 50 billion data set. We had to really align the meshes somehow. The ways we did that was by creating a lower res cage with C brush but also loading the 50 million data set.
It's just like magic. Very rarely does a paper come along and the authors decide, hey, we're going to make an application and hey, that application works better than most standard software packages that I have to use every day.
But instant meshes allows you to artistically paint.
Well, first off, it picks a beautiful edge flow.
This is the sample edge flow that it generated for the David.
And you can see, by default, clicking one button, it's already realized that along the sling it's going to put the edges here, along the face.
It's done a pretty good job.
You can go and you can artistically draw exactly what edge flow you want where.
And then it kicks out an example of exactly what it's going to build with quads and placed pole polygons.
So.
This is kind of an example of down resing the 50 into something that I wanted to go and then use to wrap. And so for wrapping, I originally tried and then we finally got to work.
RAP3, which I don't know if you guys have used RAP3.
It's a really great software for wrapping one mesh to another.
It's for kind of non-rigid mesh alignment.
You pick the different correspondences.
We use this all the time to cast different head scans into different facial topologies.
You can just pick a couple correspondence points on the two meshes, and then it will warp one mesh into the other.
Um, so it's really made for taking a low res mesh and wrapping another. Uh, the first time I fed it, uh, different kind of decimated versions of the David, it was just like, no, not, not gonna work. Uh, and I, I contacted them and we walked through some of the issues and it's just, um, it's, it's very, very difficult to non-rigidly align one scan to another scan. Um, So in the end what we did is as I was mentioning on the previous slide, we ended up wrapping the Z brush, the Z brush sub D3 itself and then once that was wrapped we bring that back into Z brush and reapply the vector displacement and then we had kind of a version that matched all of the I don't want to say they're warped.
It's just the way it's scanned, they can be slightly off, just very, very slightly.
But when you're talking about a 20-foot high statue, I mean, slightly is the toes five inches off at the base by the time you get there, because there are a bunch of different scans all the way up the length of the statue.
So it's a game development transcript.
This was the UV layout here. We used UDIMS and UE4 to kind of maximize the UV space. So you can see here how it's broken up. We gave a lot of UV space to the face. And then we The body and you can see the legs and the base down there is like just a little bit of UV space compared to the face.
But that was because we really wanted to scan at least the face, get the representation of kind of the head and shoulder area in UE4 down to the quarter millimeter.
So we were able to get the statue represented in UE4 baked to a millimeter.
But when it came to the face, we looked up into the billion data set and we now had our data set, our model in ZBrush matching the billion model. So here we are allowed to go in and now I can piece together all these chunks that make up the face and then I can bake those across.
But to bake those across, I just couldn't find a program that could load up, what is this, load up just 20 million polygons just for the face and decimate that down into something that we could ingest into ZBrush.
So one of my friends, Kevin, a guy on our team, Kevin Vassey, she was like, hey, so why don't we try Blender?
Because Blender's awesome.
And I was like, look, if you know.
If the standard mesh tools like mesh lab and all these things that people that write these papers use can't do it, Blender is not going to be able to do it.
I remember one weekend we had one software package going all the way over the weekend and then it crashed like three days into the down resing of the face.
And then in 12 minutes Blender just popped out of mesh.
It was indistinguishable.
I think it was just a couple million.
It was significantly lower, low enough that we could load it into ZBrush and use it to transfer the details across to our mesh. But that was amazing. I mean, you can see here, the meshes are indistinguishable, but on one side I think it's like 20 million, on the other side it's like three. So...
We had this high res face, we had everything aligned, but my topology was from instant, or no, it wasn't from instant mesh at that point, it was just from the ZBrush down res.
But to recapture all the details in ZBrush, we wanted the even on the cage mesh, we wanted the face to be higher resolution.
And around this time, we were also working on the digital Mike Seymour, and that's our in-house face topology.
So we used RAP3, and we just wrapped Mike Seymour's face onto the David.
So we were kind of running out of time.
And that worked really, really well.
So Adam was able to bake the details across.
And as you can see here, at the quarter millimeter resolution mesh, in the experience, you can ride the scaffold right up to David's face.
And you can see the chisel marks from the rasp chisel that he used here above the brow where he left it a bit unfinished.
I mean, he was told that the statue was going to be seen on the top of.
Unfortunately he did such a good job, it's a curse of competence, you know, that they wanted it down where everybody could see it really up close.
This is a screenshot from the experience showing Peter Sumaseni, he did a really good job setting up lighting.
So I had modeled the inside of the Tribune, which is where it's stored in Florence.
And he did a great job lighting it to where it looks almost exactly like it does there in Florence.
Here you can see the rasp chisel here.
There are so many details of the statue that come across. It's kind of an annotated experience.
So you can click different, you enter a mode and then there's little spheres you can click on and then you hear my voice kind of droning on about the statue. But little things like the cracked arm, like in 1527 they were riding and some people entered the Palazzo Vecchio.
Like some dude threw a bench out a window just because he was angry. Well the bench just like lopped off the David's arm outside. Imagine being that dude that's like...
Oh, I'm about to hit the dusty trail.
So yeah, you can see all of those fixes that were made over time.
It's really interesting.
Even that's with the body at the one millimeter resolution.
This was the final topology that we ended up using.
And then this is the Tribune here.
So.
We tried doing an irradiance bake from the skylight itself.
Pete set up, I don't know how many lights.
As soon as I unhit his layer in Unreal, my world just became wire frames of lights pointing everywhere.
But it's using the dome itself to bake a lot of the illumination.
And then there's a bunch of little spots that they have set up in Florence that just highlight the statue itself.
So looking at a couple things that could probably be better.
The experience we use of Vive and we were It was really awesome that they gave us this extension box.
So basically, the extension box would come out into the middle, and then we had full five chord length around the middle so someone can walk around the entire statue.
But the experience is not a teleport around experience.
It's very much you're on a wooden scaffold, and you can completely walk around the entire statue in 3D at life size and ride a scaffold up to any.
to anywhere you want, even way above his head and look down. We only really had time to look up into the billion dataset to do a quarter millimeter bake on the face. But, um, we Like I said, we're going to give this stuff back to Stanford.
And if in the future someone wants to do that kind of bake on the entire body, that would be a pretty interesting science project.
It would be nice to see some of the unfinished marks on the tree behind his leg and different areas at that resolution.
It would be great to see some more statues. The team when they were there, they scanned Dusk and Dawn and some of the Medici chapel statues and there were some artifacts and stuff. There's three or four statues you can look at on scan view even to this day. But it would be cool to see some more of the statues in VR. And I could envision in the future With other data, like scan the world, a bunch of guys go out and try to scan statues around the world. It would be cool to have a sculpture loader that would load it at the exact size that it's supposed to be viewed as the sculptor originally intended.
I just wanted to thank the people that were able to help me create the experience. It was like a learning process. It was the first level I ever made in UE4 so that was good. It got me out of Maya for a couple of days.
That's about it. If you guys have any questions, be sure to fill out the little email they send you.
I think it would be really great to see more people talking about kind of working to get sculpture or fresco or any ways to experience things in VR.
At a high quality that you can't really experience without traveling there.
I mean, it's pretty interesting to be able to also see things from different views.
Maybe the different views the way the sculptor originally intended it.
We can put the David up on the Duomo and you can check him out from there.
He changed a lot of the proportions just to be seen from that angle originally.
But, sure. Questions?
more about people's feedback, like artists, sculptors, sculptors, when they see David in VR?
It was, I almost feel bad telling you how awesome it is when, originally we had planned to have it here, but it didn't really work out. There were some, I never thought, I mean I'm a Michelangelo geek, so I thought like, oh, this is pretty cool, maybe some people think it's cool. There were some people that took the goggles off and they were like crying, and I was like, wow.
It's pretty crazy because you just don't expect it to be so large when you put the goggles on and you're there on a scaffold and you kind of look up at the statue.
And you don't expect the detail and kind of the hair and areas around the face.
And a lot of people didn't realize certain things he did with the eyes to cast shadows and stuff.
Some people say, oh, it's little hearts or something.
But it's just really interesting to be able to see the statue up close.
What did you do for materials? Did the Stanford data have any color?
And did you try to capture any of the, or make any of the subsurface stuff?
Yeah, so it had a whole bunch of color. I think there were like 40,000 photographs of color that we had to use as reference because they were all, I think it was film stuff, and we had access to it, but we just didn't have time.
So Pete went in and he lit it.
And then we used the images that we had.
So that's probably the thing that is the most, everything is super ground truth when it comes to the mesh.
But when it came to, if I can go back to my first slide, when it comes to the lighting or the surfacing, we really had to kind of try as light-handedly as possible to add something.
One of the, most of the stuff that we used as reference was pre-cleaning. So you saw like, you can really see the chisel marks because it's a bit gritty. They have since cleaned him with like a fancy process and he doesn't really look like that anymore but that was the data set we had so we just tried to keep, kind of adhere to that. Right, so it was just photographs, it wasn't like registered to the points or anything. No, no. I couldn't even get a Macbeth chart.
So I wanted to just white balance everything. Anybody else? You can either walk over there or yell and I can repeat your question into the microphone.
I'm just wondering if Stanford or other people have continued to do this with more statues and sort of what the time frame is now to scan versus the 30 days that it took to do data.
So, Mark Lavoie and his team scanned this stuff in the 90s. He's now at Google. He is the licensor of the data in the US if someone wants to do something with it.
He told me, he's been in the experience and he took the goggles off and he was like, holy crap.
He's like, I sure am glad I replied to that email from the animator at Epic.
But he was blown away and they did a lot of scanning of different statues and it's in the archive but there's not really anyone using those at the moment for anything like this. There are other statues like St. Anthony and there's other statues that are more life size that are interesting to walk around but there's no plans that I know of unless people's enthusiasm in this has kicked some people into gear.
I was wondering what are your thoughts on larger scale virtual tourism like the Pyramids of Giza or the Great Wall of China or something like that.
Would it be possible in today's or future technology to have that kind of level of detail for those kind of super large projects?
I've been taking stereo photos for like 20 years. I love stereo 3D. I do feel that, especially just like stereo photography, I mean you feel the parallax. It feels like, you know, almost like you're there just visually. So I do think that could be possible. I think you'll always get some kind of resistance from the people.
that sell all of the mouse pads and tourist stuff at those places. I don't think it will ever supplant the actual experience of going there but some people just can't.
It's really interesting. This also raises a lot of questions in terms of just IP. Because it's stuff that's public domain. If you go into a museum and you take a picture of something that is in the public domain, it's a slavish reproduction. You could then go print mouse pads.
It was really, really great for the game development transcript.
for Mark and for the team back in Italy to allow us to show the demo. But it had the caveats of everything being encrypted and it was really super on lockdown. So it's with data sets like that, I mean, to be able to capture some of the experiences to the level to replicate them means you're going to have to live there for 30 days. And the people that own those things aren't going to let people come in.
Mark had said that it was an amazing thing that they let him and his team go in and scan it because it was at the time, even newspapers in Florence, it was like, America is just going to make a digital David and nobody is going to visit.
You know, it was, he definitely said it was kind of just an amazing thing that they let that happen and they've had to be very respectful of the data and of the Digital Michelangelo project itself, has to be very respectful of that stuff and keep it on lockdown.
Hi, so I'm wondering if this project is actually available publicly for people to experience with or...
That goes right into the next question.
No.
Because I'm actually with Stanford, I work for Stanford University, and I'm wondering also like, do you know which department that Mark was in when he...
process of the scan data.
I should have put a link.
If you just Google Digital Michelangelo Project, there's the whole web page as it was in 99.
It's still there.
And you can get a lot of information.
As you guys know, as soon as something hits a video card, I mean, with DirectX RIP or anything, it's so easy to rip meshes out of stuff.
No, we would never be able to.
to have it, when we want to show it as an experience at a single place like at a conference, we have to ask for, to be granted the right to show it and then it has to be kind of on lockdown with encryption and it's pretty, yeah, unfortunately it would never work putting it online. The Sistine ceiling, however.
Yes, absent that.
Yeah, the Sistine Ceiling, there's enough slavish reproductions of images out there that someone could piece together Sistine Ceiling and its public domain by themselves, maybe in their bedroom.
But, yeah, in this case, the data is only, you know, you're only going to find the data in that data set, and that's not something that's shared.
But for researchers, he grants the data all the time to be used in papers and things.
Thank you.
Yep.
I see that the data was taken in 98 and nothing was done until 2009.
When did you complete the project?
I worked on it for about six months with Adam and Peter and Shane helped me with some of the learning the engine and the UI.
But it was about six months from start to finish.
But just in spare time, like we call it, like a 20% time project.
I mean, when you... in 2009, you mean it was completed in 2009?
There was a paper at SIGGRAPH, I had mentioned the author of the paper, where it was a new mesh alignment algorithm, and they said, and to show how well this works, we're going to piece together the whole billion point data set for the first time.
I'm wondering if you had the choice to do your own scans, would you, do you think if you had the choice to do photogrammetry, would you be able to get the same results?
No, no way, not even close.
Well, I mean, who knows.
With photogrammetry, you're just limited by how many cameras and what sensors you're dealing with.
The data is really, really high res.
Yeah, I don't, I love photogrammetry and I do a lot of photogrammetry, but the details that are captured in this data set, yeah, it's just hard to compare.
Even in VR, because in VR it's really, you really, the quality, you don't get the same quality that obviously the scans are able to capture.
with doing the scan.
You mean like normal maps in stereo view or which, like baking it to normal maps versus parallax occlusion or?
No, I mean getting the final, putting the final in VR using photogrammetry you would be able still to tell the difference between the scans and photogrammetry.
If you, like I said, it depends on your budget. I guess I can totally picture a photogrammetry rig that could do as good of a job with a lot of macro lenses and a big structure. I do a lot of photogrammetry and I think it could get there.
It's just the laser scan picked up even the smallest detail, but it's a bunch of swatches.
You go like, brrr, and then you have a swatch.
So for many years at SIGGRAPH it was, how do we align all these swatches?
Because that's how cyberware laser scanners worked back then.
It didn't just auto-align it like photogrammetry where it just figures out all the views and then does a pretty good job of making one mesh out of them.
It's a whole bunch of little pieces.
Yep.
Thank you.
Yep.
All right. I think we're out of time. Thank you guys very much. Thanks for coming out.
