Today's talk is data-driven or data-blinded, the uses and abuses of analytics in games.
And I'm very excited to see this many people here, very tired on a Friday, to hear me rant about statistical significance.
We're going to get to that eventually.
But just to start with, I want to say that data is awesome.
It is the backbone of science and progress and most of the large companies, Google, Amazon, Facebook, and most scientific undertaking.
But the thing that I want to focus on today is that data is also a hot mess.
Data is a tool, and like any tool, it has uses and abuses.
And it also is something just.
that where it's very easy to make straight up errors.
How many conflicting health studies have you seen in the last 20 years?
I mean, I'm at the point where if I see, whatever I see from a nutrition test study in a journal, I'm like, whatever, I'm just gonna eat green vegetables and whole grains, and I'm sure that's the right answer.
And that's because of some of the problems with data.
Congregate is a company that uses a lot of data, and we have from our start.
But I never am completely sure whether we are this ship that is using data to chart a course off to success and treasure, or we're this ship and we're steering into a pile of rocks and disaster.
And the reason is because both have happened, and both have happened a lot.
So who am I to talk?
I'm Emily Greer, and I am the co-founder and CEO of Kongregate.
And I am a shameless data geek.
I even put it on my Twitter profile.
I spend four, five, six hours a day in meetings, but I always reserve some time to go into dashboards and read PM reports, and the data analysts at Kongregate know that they are always going to get questions from me on anything interesting that they write.
So I want to talk a little bit about the sources of our data, because that's going to be relevant to this talk as I'm using examples throughout.
Kongregate started as an open platform for browser games, kongregate.com.
And we have averaged more than 1,000 games uploaded to Kongregate per month for the last decade plus.
So we are closing in on 120,000 games, and that is a lot of data.
Starting back in 2013, we launched a program where we do third-party publishing on mobile.
You may be familiar with some of our titles, particularly Adventure Capitalist or Animation Throwdown.
But we've published more than 40 titles on mobile, and in the last year started publishing on Steam consoles and doing premium titles as well.
And you may have seen the announcement that two weeks ago, that coming this summer we are going to be launching a new PC platform called Cartridge, but that's not really the source of data or focus of this talk.
So I want to talk a little bit about how I got here.
And it's not because I studied econ and math and other things like that in school.
I actually studied Russian and Eastern European studies.
So if you want to know about medieval Transylvania or the Ottoman invasion of Hungary, I'm your woman.
But as you might imagine, this was maybe the most useless knowledge that I could have on the job market.
And so I immediately did something else.
I went into book publishing, but I didn't like that very much.
And eventually I stumbled into data in the catalog business.
Now, what I was doing was called direct marketing, but really it was data science by the way that we know it.
And I just fell in love with being able to test things and look at numbers and find the story in it.
At that point, because I love data so much, I taught myself SQL because I hated waiting for IT to get me data.
And then I started taking classes at community college and at UC Berkeley here on probability and statistics and a little extra math so that I would understand the theory of what I was doing a little more than just doing it in practice.
After a while, I got a little bored, and I almost went to grad school in econ, but then ended up co-founding Congregate instead, thinking that I was gonna be doing something completely different, but then.
wow, we have all the data.
And I actually do more data and data science.
And what I did before became really, really, really relevant.
And the reason why I wanted to talk about that is because I wanted to make my first point, which is don't be intimidated.
You don't need an advanced degree in statistics to get data right.
And just because you have one doesn't mean that you won't get it wrong.
I'm not saying that it doesn't help.
It does help.
For an organization to do data right, it can't be just something that's thrown over the wall from quants.
It's something that needs to be, that you need to have an intimate knowledge of the game and how games are developed.
And it needs to be something that everybody works on and everybody looks at and everybody participates in for a company to do it right.
The other first point I wanna make, or I guess second point I wanna make, is that you should get paranoid.
And that is because under an apparently solid surface, data is usually a pile of shit.
Sometimes the issues are obvious.
When we first started launching games on mobile, we had an endless runner that wasn't verifying receipts from Apple.
And so all sorts of fraudulent purchases were coming through.
And so the average purchase was looking like $500.
And that was immediately obvious, like this is wrong.
And we were able to isolate it and get rid of it.
But a lot of times, data issues are much more subtle.
different things not firing in the tutorial sequence, or weird client timestamps that are changing dates, and other little subtle things.
So one of the things that I think is really important is that you should never rely on a single analytic system, and you should never rely on an analytic system where you can't get in and inspect the individual records.
Because oftentimes, you need to see the individual little items to find problems and figure out what they are.
Even when your data is good and when the data is correct, it can be misleading.
This looks like four separate pictures photoshopped together to create an appealing color grid, right?
Wrong.
And data is like this.
A lot of times, something appears one way, very straightforward when you look at it in one direction.
But if you change the angle, you can see that it's really something completely different.
Here's an example of that happening from congregate.com data.
So these are two games in the same genre.
They have similar day one and day seven retention.
Game one, slightly better.
They have similar lifetime buyer percentage.
Game two, it's slightly higher.
You can see from this chart that their daily ARPU, average revenue per paying user, is also very similar.
Since ARPU, average revenue per user, is just.
Buyer conversion times ARPU, they should have the same numbers, right?
But of course you know that I'm setting you up.
So game one has an ARPU of $2.27, and game two has an ARPU of only $0.84, even though those other numbers looked really similar.
And the reason is because people are playing game one much longer than game two.
and it has many more transactions per buyer.
So D30 is higher, transactions per buyer, and its lifetime ARPAPU is much higher.
And if you only looked at daily statistics, you wouldn't realize quite how different these games are.
So one way to think of yourself is as a detective.
The witnesses may be lying or confused.
The crime scene may have been tampered with.
You can't trust any one piece of evidence and you need to constantly cross-check them against each other to validate what you're finding out.
So we do a lot of triangulating in truth, and we do it by taking every source of data that we have and comparing against each other almost all the time.
So we've got client data from the Congregate SDK.
We compare it to Adjust, which is our mobile attribution provider that's also firing in our client.
We compare it to our server data.
We compare it to data from the platforms, and then we compare it to benchmarks that we've built up over the years.
Because there's always some variance between sources of data.
Sometimes it's 1%, some things could be as much as 5%, and that could be normal and tolerable.
But you need to understand what that is.
So we track everything all the time.
This is from some dashboards that we have where we're comparing four different pieces of data next to each other and tracking that.
And then we track the variance over time so that we have a ceiling and a floor that's acceptable.
And anything beyond that, we look beyond.
We check into it and make sure that it's correct.
And you're not just a detective.
You're a CSI.
Your goal should be to create a three-dimensional view of your players and how they're moving through your game.
Games are very complex and time-based systems.
And a flat view is never going to be enough.
Another way to think about this, and this comes from, I should really not press against my chest.
My favorite, while I majored in Russian Eastern European Studies, my favorite class in college was actually astrophysics for non-science majors.
And I think there's a lot to learn from astrophysics to help us think about how we should be building up data for games.
So if you think about the universe, We're sitting in one spot on planet Earth.
And that's a really limited view.
And we have really limited tools to understand what this enormous thing is doing.
And for years, millennia, we didn't really understand it.
But gradually, through first our visual observations with our eyes, and then by building more and more tools, and taking more and more observations from different places, and then comparing those observations against each other.
we've been able to build up a really beautiful and impressive model of the universe, a 3D model, and how everything works together.
Is everything accurate yet?
It isn't.
It absolutely isn't.
But when you think about 2,000 years ago when we thought that the sun revolved around the Earth, it's really amazing how far we've been able to come.
I want to talk about a lot of mistakes.
There's a series of classic data mistakes that almost everybody makes.
I make them, even though I fight against them.
So I wanted to take everybody through some examples of how they happen and what to watch out for.
And the first one and the first thing that you should always be thinking about whenever you look at numbers is audience mix.
So we tend to think of our player bases as monolithic, one individual millions of times.
But that's not really the way to think about it.
It's really an aggregation of all kinds of subgroups of people from different demographics, who've been in the game different amount of times, who came from different sources, who are playing on different devices.
And how those subgroups mingle and how they mix have a huge effect on our KPIs.
Usually when KPIs for a game change, it's more often because the mix of the audience has changed than that the game itself has actually changed.
Here's an example from one of our games.
It's the last 90 days, or 180 days, 180 days of retention for the same game.
You notice that it's mostly pretty steady.
But at the beginning, there's a spike pretty high on D1 retention.
This is because we were experimenting with how chart position affects organics.
So we did, just for a couple of days, a really intense user acquisition burst.
only on US, only on iOS.
And these are the highest value customers and the highest value players that we could bring in.
And so it really spiked D1 retention.
And if you look over time, you can see that really valuable cohort move through the system.
So D7, D30, D90, it's immediately obvious where that one cohort is.
On the other hand, you can see down here, there's a really big dip.
And that's because we got a very nice feature from Google Play that included a lot of countries where the game wasn't localized, where the game wasn't culturally appropriate, and the retention for those players was really bad.
And again, you can see them progress through the game, D7, D30, D90, affecting the metrics.
Now this is a really dramatic example of high quality players versus low quality players.
And most audience mix issues are much more subtle.
But they're there, and they're happening all the time.
Another example of this happening has to do with audience age.
This is an ARPDAU chart from another game of ours, Pocket Politics, where you can see this hugely dramatic ARPDAU decline.
since Christmas.
It was 75% decline from around Christmas through the end of January, and then a slight recovery thereafter.
And it would be very easy to look at this chart and think, oh my god, something disastrous has happened in the game.
Why aren't people buying anymore?
But that's not really what's happening.
And you can see it from this chart.
This game has been out for two years, and we've been doing a very low level of user acquisition for the last year or so, because of fatigue related to the election.
But we were testing new creatives, and we got a particularly awesome video creative that has a Trump-like character pressing a nuclear button again and again and again.
And apparently, people found that really funny, and it drastically reduced the CPI that we needed to spend.
to bring users in, and we were able to scale up our user acquisition really high.
So given the last slide, you're going to think, OK, so all of these new players are lower quality than the ones that were coming into the game before, right?
But if you break the ARPDAU chart out by D0, D1, and look at it over the same time period, you don't see that drastic decline.
There's some ups and downs, but it's relatively flat and in the same range.
Similarly, if you look at D2 to D7, there's a small decline, but it's not huge, much less than 75%.
D8 through D30, it's a little more dramatic.
You can kind of see what's going on.
But D30, again, there's not much of a dramatic change.
So why was there this 75% drop in ARPDAU?
And that was a player mix issue.
So you can see that the D30 plus ARPDAU is $0.20 to $0.25, $0.30 at any given time.
But the new player ARPDAU, anybody younger, it tends to be around $0.10.
So when The elder players were the majority of the game.
The ARPDAU was much higher.
As we brought a lot of good quality new players in, even though they were equally as good as the ones coming in before, the overall average for the game changed.
And overall, we're making more money, and it's great.
But that's something that you should look for a lot.
It's very common to see these kinds of patterns with any game where the average revenue per daily active user increases over time.
And that's most games that have a lot of depth and monetization.
Usually the way you experience it is you have a big rush of players coming in when you launch the game from features, from user acquisition, and then gradually they churn out and only a small core base is left.
And over that time, you'll see your ARPDAU rising, and people will think, oh, we're killing it on live operations.
We're doing really good.
But actually, you may not have made any improvements.
The player base for your game has changed.
Another point I want to make is that averages are average.
When dominant subgroups, good numbers, bad numbers, for a dominant subgroup can mask what's happening with smaller groups.
Here's a breakout of tutorial completion for a game that had a really bad, I think it was an iPod crash.
And when we looked at the top line numbers, everything looked fine because it was only 5% of the total.
But when we broke it out by device, you could see that there was something really dramatic going on with the iPod, and then we were able to identify it, fix it, and everything went back to the range of everything else.
Another thing to keep in mind, though, is that players are not averaged.
As you saw, that very different art out between older players in pocket politics and newer players.
And there's a lot of things that you can do that have a very different impact on older players versus new players.
So this data is the PVP win rates in a collectible card game that we have where there's a very dramatic advantage for being the aggressor, for being the person who plays first.
You see that little bump there?
That's when we made a change to try and mitigate how much of an advantage that you got from playing your cards first.
And it looks like it didn't change win rates much.
But if you break it out where you look at the max level players, in this case that's 35, versus everybody else, you can see that the change that we made was really advantageous to the highest level players and much less advantageous to the lower level players.
So even though the average looked fine, what was actually happening beneath that was not what we wanted to happen at all.
I've just spent 10 minutes talking about how important looking at subgroups and looking at different player bases are, but there are problems with that too.
Specifically, the more that you slice and dice your base, especially if you have a smaller game, you can get into situations where you're looking at data where the sample size is small, and that data might as well be garbage.
It's so erratic and misleading.
So looking at Android Australia, or just your Linux users, or some other combination may not make sense, so there's always this balancing act between.
slicing and dicing your players enough so that you're really seeing all the subgroups and you're seeing what's happening underneath the surface versus getting down to sample sizes where what you're looking at is useless.
So you just need to keep an eye on it.
The folks at Kongregate know that I am always going to ask them, if they don't put sample size on a cohort chart or something else, then it just means that I'm going to ask them as soon as I see it.
Another thing I want to talk about is the normal curve, because this, from my perspective, is a lie.
Nothing is normal.
It's not quite true.
There are some things that fall to a normal curve.
But almost everything that's important in games revenue, sessions, battles, anything else, is a power curve distribution.
Especially in free-to-play games, but really all games, are driven by intense activity from outliers.
And their presence or absence in any piece of data that you look at has dramatic impacts on what you're doing.
So here's an example from one of our games, which is the lifetime spend of everybody who's spent within the last 28 days.
So.
It's happened over a year, but it's people who spent recently.
And you can see that the top spenders for this game have spent $40,000, $45,000, $30,000.
And you can imagine, especially in smaller sample sizes, if somebody is present or somebody is absent, that has a huge, huge impact.
And one of the things to take from this is that any metric that you look at that includes something where one side of it is a power curve like revenue, like sessions, that the averages from those kinds of metrics are going to be much less stable and much more up and down.
than something like retention or tutorial completion, which are based on binary yes-no metrics.
And you can look at those with smaller samples.
So in test markets, where we nearly always have small sample sizes, I always look first at early retention and buyer conversion, because those are more reliable than things like ARPAPOO.
Sorry.
Another thing I wanna talk about is cherry picking.
This is something that sometimes we do consciously because we wanna make a point with a boss or someone else, but we also tend to do it unconsciously.
Whenever I look at a group of cohorts or I look at a data, the extreme positive values and the extreme negative values always make a bigger impression on me than anything else.
So I'm always sort of fighting to make myself like, check the average, check the median, check the mean to keep that in check.
Another example of where this could bite you is say you've set a personal goal or an individual goal of I want this thing to hit this metric so say you think the retention of a game should be at least 50%.
If you're looking at the data and it hits 50% once, you may remember, oh, that 50% and then ignore the 48 and the 47 and the 46 that come after that and think of your game as having 50% D1 retention.
And that's just a human bias that you have to fight and have to mentally fight.
And along with that is, in a similar vein, is confirmation bias.
Part of doing analytics, building worlds, thinking through your games, is having theories about how your players are behaving, how things are working.
A lot of times you'll go into data and analytics looking to confirm a particular theory.
If you're looking to confirm a particular theory, you're probably gonna find the data that confirms it, and you may ignore the data that contradicts it or muddies it, especially if you only look at data when you have a theory or a particular type of data when you have a theory, you're likely to...
Intentionally, unintentionally, buy it to yourself when you're looking at it.
Another thing to keep an eye on is the visualization of data.
Because the visualization of data has a huge impact on how we perceive it.
Here's an example of a couple of charts that our marketing team did for a partner that we were working with.
This is a game where ad revenue is very important, and we keep very careful track of.
what percentage of the daily active users are engaging with ads.
So you can see here that there was a decline in the percentage of users who were engaging with ads and the team was concerned about it.
and put big red arrows on the analysis to show what the rate of decline was.
But notice that these charts start at like 40 and 52 and are really, really zoomed in on that data.
If you look at the exact same data but start at zero, you can see that the amount of change in the engagement was actually pretty small.
it probably wasn't that big a deal, and it may have been just from audience mix.
I don't actually know what the answer is here, but this shows you how you zoom in, how you create your access, how you perceive the data, sorry, how you present the data is gonna have a big impact on how you react to it.
The other thing I want to talk about is correlation and causation.
Now we've all heard a thousand times that correlation and causation are not the same thing.
We keep hearing it because we keep needing to hear it.
It should be like a personal mantra.
every single day.
You know, an example of correlation and causation not being the same would be, you know, ice cream and drowning deaths.
They both go up in hot weather, but it's because both happen more in hot weather, and they're not correlated.
The ice cream kills headline would be a really stupid headline, but you could look at the data and come to that conclusion.
And the other thing, the reason why it's so hard for us is because as humans, we are wired to look for meaning and we're wired to look for cause and that's why we didn't eat the poison berries the second time.
It's a superpower, but the thing to keep in mind is that it's also a curse.
In particular, in games, the curse that we need to watch out for is that absolutely everything correlates with engagement.
And sometimes that's meaningful, but usually, It isn't.
A classic example that I've seen is people saying, players who connect to Facebook are twice as valuable as everybody else, so we should incentivize Facebook connections because then they'll be better.
Maybe that's true, but more likely it's that engaged players were the ones who were willing to take the risk that they're gonna spam their friends and make the connection.
So the true way to separate causation and correlation, or correlation causation, figure out what's really happening is A-B tests, and those are really powerful.
It's not a magic bullet because nothing in data is that easy.
Testing has real costs.
Engineering time, overhead, complexity, confusing players, divisions between your players.
And the more tests that you're running, the worse that gets.
In general, the test traps within A-B tests are a lot of the things that I've been talking about, like sample sizes, audience mix, cherry picking, confirmation bias, power distributions.
But there are a few additional test traps that you need to watch out for, mostly having to do with assignment.
So one thing when you're doing test assignments to keep in mind is that it's important to make sure.
that you split your test on what you're actually testing on.
So example here is a test in the store.
One way to assign people to the test is whenever they show up in the app, they go in group A or group B. And then later, maybe, they will engage with the store, and you'll see the results.
The problem with that is that you could very easily end up with a unequal distribution of people who interact with the store.
And therefore, your test is biased by something that has nothing to do with the thing you're actually testing.
The other thing is that if you're looking at multiple metrics, like retention or anything like that, then the noise from the people involved in the test may be drowned out by the larger sample.
So always split once people have engaged with whatever you're testing.
The even bigger assignment problem in tests has to do with time and anything we do to adjust when people are assigned to tests.
So revenue is a power distribution, so is engagement.
your best players always show up first, they always show up earliest, they show up oftenest.
And so if you only run a test for one day, then your sample is going to be heavily biased towards your best players.
If you run it for longer, then you'll get a lot of less engaged players who come back, and you'll get a more normal sample.
in terms of what your player base looks like.
Similarly, if you have heard of people doing, you know, tests, assigning people based on one day, next day, one day, next day, again, you're going to get a very biased sample that way.
Now, there's cases where you may want, you may be forced into it from, like if you want to test a store on Steam or some other place that doesn't give you tools, but even there you want to be very careful.
because people who show up on Tuesday may not be the same people who show up on Saturday.
So you want to do things that create an equal sample and an equal type of person interacting with the test.
Another thing to keep in mind is downstream impacts.
A lot of commercial A-B test products will make you pick just one.
one KPI that you're testing and that they're gonna, you're supposed to just judge the test on that particular metric and decide which is the winner.
They do that because they want to stop you from fishing for a particular result and cherry picking, oh, this was good, so A, and I wanted B to win, and this is good on B, so B is the winner.
And that kind of fishing is bad, but what we've seen is that there's so many unintended consequences to different tests.
that it's always better to look at the full picture and look at everything that happened and then make a decision based on the full picture as to whether the test was a success or not.
So an example, the screenshots you saw earlier of the store test were a test that we ran recently on one of our games that is a very complicated store with a lot of items.
And we had a theory that the number of items and how complicated it was was confusing players and reducing a number of people who were actually buying items.
So we did a simpler version of the store that focused on just a few items.
and tested them against each other.
So the main thing that we were looking for was to see an increase in conversion.
And great, we increased it.
We increased the conversion with the simplified store by 9%.
The problem is that by removing some of the items, we removed some of the items that the most engaged players were spending on.
So even though we increased conversion, The average revenue per paying user dropped drastically, and overall revenue was down 11%.
So conversion up, revenue down, oops, that was maybe not the greatest thing to have done.
So we reverted to the control and are continuing to experiment.
Along with not looking, another flaw that you can get into with A-B tests is looking at results too early.
So here's another example from our game Office Space where we did some, this is from a little while back where we did, we were testing kind of a flash sale for individual items.
They popped in, and certain players, they would pop in, great deal, short-term discount.
And you would expect it to increase conversion and reduce the ARPAPU and hopefully increase revenue.
And when we looked at the test after running it for 10 days, that was exactly what we saw.
Big increase in conversion, small decrease in ARPAPU, 8% increase in average revenue per user.
Yay.
But since these are all the bad cases, when we let the test run for 30 days and looked at the data again.
The basic pattern of increased conversion and lower ARPAPU was still there, but everything had moved downwards to the point where we were now at a negative on revenue.
Revenue had decreased by 2% because the long-term effect on average revenue per paying user was so much larger than on conversion.
And part of the reason why that's important is because of statistical significance and the question of what's a big enough sample size?
I have a particular point of view on this, which is that you don't want just the minimum number of players in a particular test.
You don't want just enough to pass the statistical significance test.
You want really big sample sizes.
And this comes from, I think, from a misinterpretation that I think most people have on how statistical significance works.
And I'm going to stumble over statistical significance a bunch of times because it's really hard to say.
And it's really hard to say fast.
So you'll hear things, people will say things like, we had a statistically significant 5% lift from this test.
But that's not how statistical significance works.
So the way statistical significance works is it assumes that there's a true underlying lift between the two populations.
And that if you run the test again and again and again, the results will come back in a normal curve of results where the true mean is the underlying lift.
So if you run a test, it passes the statistical significance, it's 5%.
It could be from around the mean.
It's probably a 40, 45% chance that it's in that range.
But it also could come from the other part of the curve.
In this case, 5%, the underlying lift would be something like 10%.
Or it could come from the far upper end of the curve, in which case there might only be a 1% or 2% lift.
And that's important in terms of how you think about what statistical significance means when you're looking at results.
And this is why I really like large sample sizes, because if you have sort of double the sample size that you need, then it's like you've run the test twice and averaged the results.
And over time, the results there are going to come closer and closer to that mean.
So big test sizes whenever possible.
But there are costs to that, so there's always trade-offs.
Now, you know that I think this is a lie.
So I want to talk about abnormal testing, because a lot of what we're looking at are engagement-related metrics or revenue-related metrics that are power distributions, and those need to be looked at differently.
So this is our old friend, the power curve of revenue with buyers who spent $45,000, $50,000.
If you think about what that means, here's a sample of a potential distribution in a game.
It's not uncommon in free-to-play games to have 70 or even 80% of revenue come from a very small percent of buyers who spent more than $500.
These two A-B tests have exactly the same distribution, except for one has a $1,000 buyer and one has a $500 buyer.
And the net impact of that is, in this small sample, a negative 40% on the average revenue per user, even though the distribution isn't very significant.
That still happens when you have a larger test, and that's something that you need to...
really need to look out for and watch out for, because a few big spenders can have a very big impact.
How do you deal with that?
It's really important to look at medians and any kind of distribution of how revenue and other things are coming in.
So that previous example, the average was drastically different, but if you look at the medians, they're both 15.
And that helps show you as an analyst that, the differences are happening just way at the top and not through the whole distribution.
Another made-up example is this one where there's actually a big difference in the distribution.
Everything is doubled on the second one, but has a slightly lower top buyer, and so the averages are very similar.
But if you look at the median, it's double the other one, and that shows that probably that's the better outcome of the test.
versus the other.
So that simple store test that I talked about before, where we were looking at revenue, and that was what we were judging on, this chart helped us figure out what was going on, so this chart breaks out all the different purchase sizes and does it by day.
And when we looked at the same chart for the simple store version versus the control.
there was much less of that bright pink $100 purchases.
So we were able to sort of confirm by looking at the distribution what had really happened.
The other thing is that when you're looking at something like revenue or some kind of economy sink, instead of using a normal P or Z test, you should use something called a Wilcoxon Rank Sum Test, which compares the medians rather than the means.
This isn't something that's easy to run by hand, but most statistical packages like R include it as an option, and that's what we use.
Now I want to talk about testing's dirty secret, which is that most tests don't show a significant change.
And this can be really frustrating for a team, because you've put a lot of work into creating two different versions of the game, and you want to have the work that you're doing have meaning.
You want to have...
You want to be improving things.
You want to have numbers go up.
But one of the things to remember is that a test that doesn't show a result is still meaningful and still interesting.
Here's an example from Butterscotch Shenanigans, who made the game Crashlands.
They were kind enough to share this particular example.
They wrote for the game a huge, long, very detailed description.
were wondering how much of an impact is that having on conversion?
So they decided using Google's A-B testing tools and Android to test the most extreme possible version of that, which was no description at all, just their four or five accolades that were talking about how awesome a game it is.
So they ran it for four months, and it shows absolutely zero difference between having this, like, thousand-word description.
and zero.
Now, on the one hand, that could be frustrating, all that work wasted on writing description, but in all game companies, in all companies really, a time and effort is really valuable.
So anything that shows you what is not worth doing and what is not making a big difference is something that you can reduce your focus on.
This result, where description has very little impact, is exactly what we've seen in terms of our testing using the A-B tool set that Google gives us, which is really nice.
At the high end, icon has a big impact.
Video has some impact.
Screenshots, a little less.
And then description, not too much.
And overall, this kind of testing has been really useful for us.
We used to spend a lot of time arguing about game names, but then we did a lot of A-B testing of game names against each other, never saw a difference in result, and now we're much, much chiller about choosing a game name.
And yeah, so we can let some of those arguments die a little faster because we've had data that tells us this isn't that important.
But.
When you're testing, it's also very important not to extrapolate too much from the result.
As you've heard, we're big fans of Google's A-B testing store tools on Android.
were such big fans, they actually did a case study on our use of it and came to our offices and took a video of all of our bunch of employees talking about particular tests.
Here's our senior producer, Peter Eichmann, talking about an A-B icon test we did for a game called Global Assault.
And we had been using this girl with a gun icon as our main one and tested a helicopter against it and got a 92% increase in conversions.
92% is a really big amount and had an impact of hundreds of thousands, probably millions of installs on the game.
So really exciting, great, yay testing.
Except for there's a funny thing.
The reason that we were using that girl with a gun icon testing is because we had tested various different potential icons on Kongregate.com and that's the one that won.
So the helicopter...
So the girl with the gun had a 0.28% conversion rate on congregate and a 0.19%.
The helicopter had a 0.19%.
So the results were almost exactly flipped.
And that's why you need to be really careful when you're testing to think about context and make sure that you repeat any test and that not everything is going to work the same way in the same place, in different places.
Another thing to keep in mind is that not everything is testable.
Specifically late game stuff, late game and late game players.
It can be very difficult to test.
This is a really nice quote from Daniel Cook of Spryfox and Lost Garden that I saw on Twitter recently.
He was talking about YouTube and how the algorithm changes the audience.
But I think that it's a really good way of thinking.
of thinking about things, particularly that the game also changes the audience.
The longer somebody is in the game, you're setting their expectations.
You're training them to behave in a certain way.
And the later you get in a game, they're not going to react to something the same way they might have on day one or day four and day eight.
Players talk to each other.
That makes it harder to test as well.
So there's a lot of things happening at the end of a game that make it difficult to test.
So I've created, because I like frameworks, my hierarchy of testing from critical to difficult.
And it basically follows the funnel of players into a game.
So starting with advertising, there it's really critical.
And there's tons and tons of tools.
Almost any ad network that you use is going to expect you to do A-B testing.
And this is important because the same creatives that work on congregate.com versus Facebook versus a video ad network, people are not going to respond to them the same way.
So you want to have a ton of creatives that are constantly testing, and this is very important to get advertising to work.
It's also really important wherever players are converting to actually play your game, whether it's a landing page or a store page on a platform.
The problem with this is that outside of Android, there's not really great test tools.
But you can use things like StoreMaven and other types of services to sort of fake test those.
And whenever you have a platform that does allow you to test, make sure that you do it.
It's still pretty easy to test on the initial experience of a player, you know, getting through tutorials, getting through the first day.
There's no real big gotchas in that.
But there is one thing that's important, which is that testing on the first day generally doesn't have huge, show very strong results.
Usually what you're looking for there is how long do players retain?
How deep do they get into the game?
How much do they play?
And in the end, how fun your core loop is, is going to determine those things.
And when you're testing the initial experience, you're really just testing.
that presentation. And the presentation of something has some impact, but doesn't usually have as much impact as the game itself. Where we've seen more meaningful testing on our side is in the first few weeks that somebody is in the game.
This is where we've seen good luck with tests, where we're testing out how we unlock features and good results on store testing and other things.
This is a sort of a sweet spot for testing in a way that's meaningful in a game, because players are still new enough to a game that you haven't kind of wired them into a particular type of behavior.
But it's deep enough that it can be meaningful, and you can test really substantial features.
But then once you get to late game, it's pretty difficult.
It's not impossible, but you should tread carefully.
There's a lot of problems with payers talking to each other.
There's a lot of problems with sample sizes, audience mix, et cetera.
So it's not impossible, but given how much effort testing is, I wouldn't recommend focusing on that.
So besides that there's a hierarchy of testing, I also like to think that there's a lifecycle for data in games.
And it kind of follows the stages of the game creation and the lifecycle of creation of the game.
And that can be loosely split into two sides.
One is the creation, concepting, pre-production, production.
versus optimization.
And in my view, the optimization phase, where you're trying to answer the questions of what's working, what's not working, what's breaking, it's really crucial for all games, free to play, premium, whatever you're doing.
Every game can be made better by looking at data.
For any game as a service, that LiveOps looking at data, it's absolutely critical.
We'll only make games better as long as you don't fall into a lot of the classic blunders.
Where things get more interesting, from my perspective, is the game.
perspective and where I don't think I have it figured out at all is on the creation side.
Now, production, I don't think there's really a role for data in it.
You just need to make the game. You're not really answering questions.
You're just trying to get to the fun.
But there's a really big, when you're in concepting and pre-production, you're asking a really big question, which is, what should we make?
And this is the question always for game companies.
This is deciding the fate of your company again and again.
You're committing.
enormous amounts of resources, you're potentially committing years to a project, it's what's defining your brand, it's setting you up for both success and failure, and this is the most important question that you can ask. We've done a lot of, you know, being a data-driven company, we've tried to pull data into some of that creation process, and we've had luck with a particular series of games of collectible card games that we've made, starting with Tyrant Unleashed, which is a very hardcore, gritty, sci-fi themed collectible card game that had really great monetization and long-term retention, but very high CPIs.
It was very hard to market.
So we looked at that game and said, we want to make it more marketable.
So we reworked the game.
The team Synapse in Chicago is the one who makes it.
And they did a brightly colored fantasy, cartoony themed version of it to make it more accessible.
And that did lower the CPIs.
It didn't really make the game more successful, but it did make progress along those lines.
Then we were able to license from Fox a bunch of animated IPs, and were able to create Animation Throwdown, fundamentally the same game, but we worked a lot on the accessibility of the early part, and these characters made the same gameplay.
very, very accessible.
So that was a big success for us.
So then we went to, we were like, okay, let's test art before we even start making the game.
So again, with Synapse, they were making a game that was gonna be an idle game, so we tested a bunch of different creatives against each other.
Eon Heroes, Relic Raiders, and Castaway Cove.
And the most casual one, Castaway Cove, had a really good click-through rate, and we decided to go with that.
But the funny thing happened is that as we got to test markets, that data didn't hold up particularly well.
We went through three rounds of testing of Castaway Cove, in each case, really good click-through rates, as you can see here.
But once we got to test markets and a broader sample, they really took a nosedive.
And then on top of that, we finally were able to see the conversion on the store page.
We usually target 30% or higher.
And this one, it was really disappointing.
It was 22%.
And overall, it's not that the CPIs for this game are bad, but they're pretty average.
And we're really trying to make something better than average.
Part of it, I think, is that the gameplay and the creative didn't really work together.
And that conversion piece, which we weren't able to test earlier, was important. I also think it's just some of the limitations of testing.
There's probably another five or ten things that we did wrong, but it shows that you can't just necessarily use science to figure out what you should make.
The other thing to keep in mind when using data to figure out what to make is that data maps the known world.
This is a map from 1457 made by a Genoese map maker.
And at the time, it was the best map that had been made in several thousand years.
It was the first map in a long time that had the relative proportions of the continents correct.
And if you look at the Mediterranean and anything near Genoa, it's actually a pretty good map.
But the farther you get out into Africa, into Asia, into the Baltic, it gets more and more crappy.
And our view of the world is a little bit like that.
And even more, there was a huge amount of the world that was completely unknown to that Genoese map maker.
And that's really important.
And games and data are like that.
If you're using data to decide what you're going to make, that data is always going to tell you to make a slightly better version of something that was already a success.
So Madden every year for 20 or 30 years.
But it's not going to tell you to make something like Journey, which is very different from anything that's been made before and is a wonderful experience.
I don't think there's a particularly right answer.
Again, I like framework.
So what I've put together is kind of from the left side, familiarity, to the right side, you have innovation.
And from the bottom, you have unknown newcomer.
to at the top, established incumbent.
Most game developer, oh sorry, EA Sports is a good example of what you would find in the top left because it's a huge company making a familiar game of a familiar game again and again and again.
The other end would be IndieCade where it's, you know, unknown teams making tremendously innovative work.
Most game developers are going to fall into this yellow band.
And that's because the bigger you are, the more established you are, your problem, there's more at risk.
Usually your budgets are huge.
You've got big expectations from your investors, from your shareholders, even from your existing player base.
So every decision that you make is high risk.
So you're probably going to put your investments towards something that's more familiar and safe.
and stay around your familiar territory.
When you're an unknown developer, your biggest risk is standing out in the crowd.
So innovation becomes less risky to you.
And in fact, it can be an advantage because it'll help you stand out.
But that doesn't mean that successful games are necessarily going to come from this yellow band.
Example would be Stardew Valley, which is a very polished new version of Harvest Moon from a totally unknown developer.
And that's been a huge success.
And certain incumbents, with the right process, are more risk tolerant.
So Supercell has a process where they're very comfortable with larger risks and going farther.
And because they have a lot of profits, they can do it.
And so they were able to release something like Kasha Royale, which is truly an innovative game, even though it came from a very established incumbent.
So, final words is that whether, you know, you're a detective, a CSI, an astronomer, a cartographer, or explorer, keep your eyes and minds open.
stay a little paranoid, and try and find your world to recreate.
So, thank you.
I don't know that there's very much time left, but in the five minutes, hopefully I can take some questions.
Special thanks to a bunch of people who helped me with data in the talk, and also for hiring.
Any questions?
Nope.
Oh, okay.
Hi, so I was particularly interested in your list of things where you found that the testing was useful, because when we were running a bunch of A-B tests, we often found, as you found that, you know.
eight or nine times out of ten, it wouldn't make a difference at all or be a difference in the subpopulation.
And so, could you go into a little bit more detail about the tests that you found did work?
So you mentioned store, feature unlocking, were there any common features between things that tended to get statistically significant results?
I think they were features that were large enough to have an impact.
A lot of testing, a lot of times people will test things that are really too small to have an impact.
So the kinds of things that have worked are significant testing on major features.
something that significantly changes the experience.
There's a pretty broad range of things that have been significant for us.
On Animation Throwdown, for example, we did a test on how long research timers should be.
So how long you wait until you get to unlock the combo that you're discovering.
That was a very significant test and had a big impact on retention.
So does that answer the question?
I think a lot of things can have an impact on your game, and it's hard to say exactly what.
I think testing the color of two buttons is not a very big question, and you're not going to get a very big answer.
So you have to have a very substantial question before you're going to get a significant answer.
Right.
So the timer, especially economy-based timers that can really dig into your economy in that game.
So that's a perfect example.
OK.
I had a question.
Sure.
A lot of the industry is changing with more and more games depending on ad revenue, which is something Congregate really sets the bar on.
And you were showing us how the map of the known world has changed a lot.
And something that perhaps AppVanity and SensorTower don't cover yet is the ad revenue market.
I was wondering how Congregate analyzes and views the market within that context.
So.
One thing that's tricky and that is trickier about ad revenue versus other revenue is that it's much harder to pull into game an attribute at a player level.
So what we tend to do is look at proxies like percent engagement with ads.
That we can fire on our side, and then we can make a good guess at what kind of revenue we're going to get by Geo based on that.
One thing to understand with ads is that there is our old friend the power curve in terms of revenue.
The first ad that somebody's shown is much more valuable than the ninth or tenth.
So ad engagement is a good proxy for ad revenue, even though you don't have it there exactly.
Does that answer your question?
Sure, thank you.
Hi, so if say you're a data analyst at a large company, and you're saying that it's hard to convince people to make new games using data because data only maps a known world, what tips might you give to data analysts at large companies who might want to encourage executives to make those new games?
Example, I think fake things, oh, this is apparently my last, the last question I can answer.
If they want data, think about what their motivation is and think about and use that to try and find things that will satisfy their motivation.
So it might be comps of different types of things or...
looking at something that's radically different or yes. Something like that. The other thing I would say is that the culture of a company and the culture of risk in their executives is something that's extremely hard to change. And so if you really want to do something innovative and different then you may need to go to a different company.
Fair enough.
I can see you're representing the same show.
Yeah.
The larger the company, the harder it is to influence.
Thank you.
Okay.
Cool.
Thank you, everybody.
