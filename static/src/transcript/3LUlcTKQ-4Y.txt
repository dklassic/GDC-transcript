Hi, everyone.
I'm so excited to be at GDC with you today to present Lessons from Duolingo, How to Turn Good Intentions into Good Numbers.
This talk will go over how to leverage a data-driven culture to further your business metrics while still staying true to your values.
I'll start by doing a quick introduction.
My name is Karen Tsai.
I have a BSc in computer science and a minor in neuroscience from Princeton University.
So I've always been interested in the intersection of education and technology.
Currently, I'm an engineering director at Duolingo, the most popular way to learn a foreign language online.
That's Duo, our green owl mascot that you'll see a lot in this presentation today.
I had the privilege of joining Duolingo in 2012, three months after Duolingo's public launch.
And I was able to witness the company grow from about 15 people to over 300.
I currently lead engineering in the learning area, the biggest area at Dolingo.
This area focuses on how to teach our learners better.
And of course, I'm particularly excited to be here because I've always loved games.
My current obsession is team fight tactics, though I was playing Animal Crossing pretty obsessively earlier this year.
I'm excited to share our approach about what we've learned at Duolingo in the past eight years because it's worked really well for us.
We're currently the most downloaded education app with over 30 million monthly active users.
We think this popularity is due to Duolingo being free and because it's designed to make language learning feel more like a game.
Since I joined, there were two main pillars of company culture.
The first was our mission.
Our purpose was to equalize access to quality education.
And our second pillar was that we would rely on data and A-B testing for all of our decisions.
When it came time to mature and become a profitable and sustainable company, we had to think a lot about how to do that while still being the company we wanted to be.
We also had to figure out how to prioritize things that were hard to measure, like how well we taught.
So despite all of the A-B tests we've run, it still wasn't capturing everything we wanted it to.
Last year, we gave a talk on how to make learning hard things easy, providing a lot of examples of surprising results we found through over 3,000 A-B tests on how to make an engaging product.
In the Q&A part of that talk, there were a lot of great questions on best practices for using data and what to do when data wasn't readily available or didn't quite capture what was desired.
So we thought it might be useful to dive a little deeper into those questions this year.
So let's start with some background to frame the problem at hand.
The question is this, how can we encode good intentions and values into day-to-day decision making while still listening to your metrics and numbers?
How do you make difficult decisions when it comes to trade-offs?
And there are a lot of trade-offs.
In games, you encounter these decisions pretty frequently.
For example, do you want to make a super accessible free game that's widely used?
Or do you want to be profitable sooner, but maybe with a paywall and get less usage?
Do you want to do constant patches and ship new features quickly?
Or do you want to have a very high-quality product off the bat?
Do you want to make your game challenging and appealing to niche users?
Or do you want your game to be accessible enough that it's hyper-casual and appeals to a wider audience?
There are a bunch of other sensitive gray zones, like how much moderation you should do, whether or not to take political stances in your product, how you approach user privacy, representation, diversity, inclusivity, et cetera.
And this talk is not about what to do or what to stand for, but it will be about how to develop a process and a culture that lets you stay true to the values you choose while maximizing metrics.
At Duolingo, we encounter these sorts of trade-offs every day We constantly have to decide how we can be sustainable and earn revenue while still being accessible to as many users as possible.
We also have to balance the trade off between making our app approachable and easy and increasing engagement, but also trying to teach effectively by also teaching harder content as well at the right times.
Also, one thing on my mind constantly is how we measure learning outcomes, which is a traditionally slow-moving metric and difficult to capture in day-to-day metrics.
By the end of the talk, I'd like you all to feel confident that you can develop a process and a culture that will allow you to be both idealistic in staying true to your values, but also pragmatic and intentional in maximizing business metrics.
So we'll go over each of the steps in detail, but here's a preview of what we'll go over today.
First, the first step is to document your operating principles and values.
And I'll explain in the next section what these are.
Second is to set up and encourage a data-driven culture.
Specifically, if you want to have good numbers, you need to be able to track them.
The bulk of the talk will actually be around a concept called guardrail metrics, which allows you to tie these two together.
And finally, we'll talk about how to account for things that are a little harder to measure.
Our approach is fairly simple.
but we found quite effective.
So the first step is to establish operating principles.
Operating principles are a way to put culture and values into practice.
A commonly heard principle in tech is to move fast and break things.
Amazon has a principle around frugality.
Some companies might say company needs should come before user needs, whereas others might say user needs should come before company needs.
What's so interesting about operating principles is that good operating principles have very valid counter-arguments.
Things that are obvious don't need to be written down.
But when the company makes a choice on what to prioritize or value that may not be entirely intuitive, operating principles are very helpful.
These are a way to encode your uncompromisables, things that you aren't willing to make exceptions for and that you want to preserve.
Once you've come up with some operating principles, it's usually a good idea to list out examples of how your company has demonstrated these in the past or hypothetical or aspirational situations where you'd want people to make the right decision.
So here are a few examples from Duolingo to illustrate these in practice.
One of our first principles is learners first, and that means we never charge for learning content.
As you can imagine, this makes it tricky to monetize efficiently, but since it's encoded in our values, we've been able to preserve it.
Another principle is to take the long view, meaning we'll often turn down short-term significant wins if they have a risk of having long-term detrimental effects.
Another example is all for one and one for all, which values cross-team trust and collaboration.
Some companies are very successful by having a more competitive environment, but we encoded here that every team should be working with the consideration of other teams.
So let's say you now have this lovely list of operating principles.
The first thing to do is to stress test it with past and hypothetical future decisions.
Given these principles, would an average person at your company make the same decision you would?
Would they prioritize the right things? And would they feel confident they're making the right decision?
From there, you can begin to evangelize your operating principles and establish a culture around them.
And then comes the hard but fun part, figuring out how to align your principles to business objectives by identifying what key metrics will further those business objectives.
So having values in a goal without strategy runs a large risk of failure.
Being data-driven, even with the simplest of A-B testing tools, allows you to be more confident in the direction you're taking and signals you to course correct when necessary.
By obsessing over our metrics and only launching things that win, we guarantee that our numbers increase over the years in all of our key metrics.
And since we run thousands of experiments, that adds up.
So I'll quickly go over what A-B testing is here, because a lot of you are probably familiar.
The idea is that you can test the impact of a change by giving it only to a fraction of your learners.
So for example here, you could divide your users in half and only give half the new feature. We generally internally call these two conditions the control condition and the experiment condition, where experiment generally represents the change that you're trying to test.
You then look at metrics such as retention or activity for both populations and see how they compare.
One tip is that when analyzing these metrics, only look at users who were or would have been affected by the change, and don't include those that weren't.
So for example, if you make a change to level 3 of a dungeon, you should only look at users who reach level 3.
Users who are only in levels 1 or 2 don't even see level 3 and wouldn't experience any change, and that also dilutes your data.
This simple concept really changed our ability to run experiments quickly.
Until we made this change, many experiments would be neutral or take several weeks or months to run. But after this change, most A-B tests run for only about two weeks with significant results.
As mentioned, Doolinga was run over 3,000 A-B tests with around 150 to 200 running at any given time. This is possible because everyone is bought into the culture of experimentation.
Anyone, not just engineers and product managers, are empowered to create experiments after going through a short training on an online course.
There's a quiz at the end to test understanding.
And if you pass that quiz, you get the permission to create experiments.
As a result, tons more experiments were created after this training was launched.
And this number continues to grow at a rapid rate.
We also try to make analysis as accessible as possible.
A key part of our experiment framework is an auto-generated report that updates daily, doing all the statistical work necessary to determine if the effects are statistically significant. This means almost anyone in the company is able to provide informed recommendations on whether to launch or kill experiments based on the metrics provided in the report.
And the final step, after experiments are created, run, and analyzed, is to share the findings.
This is very common for A-B test results, whether positive or negative, to be shared at our weekly all-hands meetings.
Here's a picture of our all-hands meeting before COVID hit.
Hopefully, we'll all be back in person soon.
All experiments are also sent to an open mailing list of experiment write-ups.
Through these two ways of sharing experiment results, Everyone gets pretty familiar with how we look at metrics, how we make decisions on whether to launch or kill features, and what seems to work or not work when it comes to moving these metrics.
If you're just getting started with getting more data driven, there are plenty of third-party providers that will provide a solid A-B testing framework for you and tools that will allow you to determine whether or not those results are statistically significant.
It took us many years to build a sophisticated framework.
But even in the very early days, we were able to benefit a lot from very simple testing tools.
What's more important than the actual tools you use is fostering a culture that loves experimentation.
But at Duolingo, teams are generally organized around these metrics.
We have a retention team, teams that focus on revenue, et cetera.
But sometimes that may cause issues when two metrics collide.
What if there's a metric you care about, but no team is in charge of it?
That's where guardrail metrics come in.
A guardrail metric is a business-critical metric displayed on all of our experiment reports that should never go down significantly.
Otherwise, the experiment shouldn't launch unless there's a very good reason to do so.
Remember those cool automated reports that I showed you earlier?
They contain around 20 key metrics on them.
many of which were created by teams to prevent other teams from accidentally negatively impacting their metrics.
You can see in this word cloud here, I've displayed the ones that we tend to display on our metric reports.
They include things like app crashes, which you may not always think of, feedback email taps for people asking for help, retention, subscriptions, et cetera.
By having these guardrail metrics on all reports, it aligns everyone on what the company values and gives a holistic picture of the impact of a feature.
Without them, teams might tunnel vision on the metric they sought to increase without realizing the effect that it has on other metrics outside of the team's purview.
Teams can also rest easy knowing that generally, if the analysis report looks good and no guardrail metrics are negative, they can be confident.
in making a decision aligned with company interests.
If something is negative or concerning, they also know which team to reach out to in order to come to a conclusion about whether or not the experiment should launch.
So I have three examples for you today around the usage of guardrails.
The first was a situation where we identified the need for a new guardrail.
Then I'll share a case where guardrails helped us make a difficult decision.
And finally, I'll share an example where guardrails revealed something positive and unanticipated.
So our first example centers around an experiment around our health mechanic.
Similar to many games, learners would lose a life or health from mistakes that they made in the app.
And if they ran out of health, that was a signal that maybe they were learning a bit too fast and would need to practice or wait to earn it back.
Health refills were an in-app purchase that we added.
So this feature actually made us quite a bit of revenue.
But it also seemed to help D1 and D7 retention without hurting daily active users.
However, what we saw in the metrics was not what we saw in the public forums and our channels.
Our App Store ratings really suffered a lot with learners unhappy with this change.
We had to manage a lot of community outrage, and we suffered for a while with a much lower app rating than usual.
We discovered this was mainly impacting existing users, since it's always painful to have something taken away or changed that you're used to.
New users who started Duolingo with this feature didn't seem to mind it.
So we let this feature run only for new users for a while and rolled it out to existing users very slowly.
But because of this experience, we added a new guardrail, which is App Store Ratings.
we can see if a feature significantly changes our apps for reading.
And this metric allows us to prepare a community response ahead of time so we can be thoughtful about how we execute it and also stay ahead of any community concerns.
A second example around guardrails involves an experiment we ran on our subscription purchase page that changed the copy or text of the main button.
In the control condition.
The one that users already saw, it said, try seven days for free.
We wanted to test a new copy that said, start my free week.
It turns out that the experimental copy made us a lot of money.
We saw a $6,000 per day increase in in-app purchase revenue in the experiment condition.
However, looking at guardrails, we saw that new user D1 retention dropped by about 1%, which seemed to translate into also a small drop in daily active users.
We hypothesized that the new copy may have been misleading. New users might have assumed that after a week Duolingo would cost money, which was not true. Due to these guardrails, we decided to leave the money on the table, kill the experiment.
and iterate on the copy further to minimize misunderstanding.
Our monetization team has long had a stance that we never want to trick people into giving us money.
It's powerful and telling that this decision came from the monetization team itself and was not enforced top down.
Because of our operating principles about taking the long view and all for one and one for all, teams are aligned to make good decisions for the company, even if it requires a sacrifice from the team.
Our last example is a serendipitous one.
One of our teams made a change that updated the look and feel of our language courses by replacing a button that said Test Out of and Skills with what we called Checkpoints, which when clicked on would also allow you to test out of skills but also become milestones and indicators of progress once a user went past them.
Unintentionally, we discovered that this made the Test Out feature less prominent.
Our hypothesis was that because learners weren't testing out as much, which is a shortcut, they were running out of help faster because they had to learn the material more thoroughly.
Free trial starts and subscription revenue increased substantially.
Now at first, we were worried that if learners weren't using TestOut, they might have been seeing content that was too easy and they would be bored and leave.
However, since all other guardrails, like retention, DAUs, and time spent learning were neutral, it implied that the learner experience actually wasn't negatively impacted by this change.
And now we know that test out is a lever that does impact revenue, something we would have not thought of in the first place.
So now that we know what guardrails are about, what do we do next?
Well, the first step is to identify the metrics that are most important to the success of your product.
For games, this might look like getting as many users as possible or to be as profitable as soon as possible.
You could also look at how engaged your users are by checking things like how long they play your game per day or how often.
You might want to look at latency and crashes if stability is very important to your game.
And if your game relies on network effects, You may even want to measure more specific things, like the average number of social interactions per unique user.
And finally, you can track more subjective but still quantitative metrics, like App Store ratings and social media engagement.
But some things are just really hard to measure and put a number to.
And how do you account for those?
For example, it may be that one small bug is insignificant and not very high ROI, as we call it, to fix.
But what is the impact on the user if they see a lot of these small bugs?
Similarly, how do you measure the impact on your reputation when it either comes to bugs or features?
Similarly, how do people perceive your product and your company?
How do you measure creative and design quality?
And one that I think about daily is how do we measure long-term learning outcomes?
The hard truth is in a data-driven culture, if it's hard to measure, it's hard to prioritize.
That means that if there are important things that you care about creating guardrails around that are hard to measure, you need to find a way to prioritize them anyway.
We have four approaches to this that we use for different situations.
The first solution is to simply care about it, even in the absence of measurement.
Brand design consistency falls into this bucket for us.
The approach usually requires an additional process for enforcement, or else people may not be motivated or reminded to care about these principles.
We enforce design guidelines during development, design review, and quality assurance.
but otherwise can't easily measure the consistency of our branding.
But for example, we updated all of our logos to our new brand logo, even if it didn't impact metrics. Operating principles are also very useful when it comes to encoding that something is important.
Another solution is to reframe what success looks like for an experiment. For some objectives in the learning area, for example, We consider a feature a win if it's entirely neutral for metrics, but research or a learning expert backs up that that feature is better for learning.
Again, guardrails are critical for this.
We can be confident that we aren't hurting by launching a feature we believe in.
A third solution is to actually measure slow-moving metrics periodically.
Some things are not impossible to measure, but may just take a long time to do so.
For example, metrics like brand awareness tend to move relatively slowly, so we do an annual study.
We also do an annual study on employee happiness.
And when it comes to learning, we conduct periodic efficacy studies that look at learners who have been around for a long time.
It's good to predict things that you'll want to measure in the future so you can set up an infrastructure ahead of time before you miss a window and have to wait a long time before you can do it again.
And finally, the solution that's my personal favorite and generally works the best is to find what we call proxy or good enough metrics.
We realized sometimes it's better to just be able to move forward even if the direction is not perfect, so long as you're generally moving in the right direction.
One example in the learning area is that one of our teams is working on making things harder on Duolingo without decreasing guardrail metrics.
which is actually quite hard because if you were at the talk from last year, you know that any increase in difficulty actually has a huge impact on retention and DAUs.
In this case, we're using make stuff harder as a North star.
That's a proxy for if people are doing harder things, they're probably learning more and practicing more of the language they're trying to learn. Even then, we had to have a proxy for what it even means for something to be harder.
we looked at the types of exercises users do on Duolingo and had a general sense of which ones are hard versus which ones are easy.
For example, translating something into the language you're learning is much harder than translating something from the language you're learning into your native language.
So the team will consider it a win if they release a change that can increase the ratio of challenges that make you answer in the language you're learning without harming guardrails.
This approach also pairs really well with longer term studies because then you can then check how well your proxy or good enough metrics helped you move the needle on the actual metric you care about.
So to summarize the four approaches to measuring immeasurable things, you can commit to caring anyway and create a process to enforce it.
You can declare experiments a success as long as they're neutral but are supported by some other measures such as research or subject matter expert.
You can measure slow moving metrics periodically and try to anticipate how to do so.
Or you could find proxy or good enough metrics to cure decision paralysis and move forward anyway.
So what has this approach of establishing operating principles, obsessing over experimentation, and leveraging guardrail metrics gotten us?
Well, internally, we've benefited a lot. Teams feel collaborative instead of competitive.
Through guardrails and a system of accountability, there's a lot of trust between teams to make good decisions that don't hurt other teams. We also now have minimal debate about whether or not to launch experiments.
This allows teams to make decisions quickly and ship things faster.
It also feels more fair and more consistent.
Finally, sharing both wins and failures not only offers a level of transparency and accountability into all teams experiments, but also helps the company get collectively smarter about the product and what works when it comes to moving our metrics.
With Gargoyle Metrics, we have a checks and balances system that allows us to be confident that we're moving forward in increasing all the metrics we care about without taking steps back on any of them.
We went from essentially no revenue to over a hundred million dollars in revenue within 3.5 years.
And best of all, our users are more engaged than ever.
We have five extra D1 retention since launch while also working towards becoming a sustainable company.
From a business standpoint, this approach has been really effective.
By having a free app, we've been able to attract a huge user base, meaning we can run experiments and improve our product very quickly.
We've also been able to become profitable, despite not charging for learning content.
Though this was probably the harder path than just starting to charge for our app, and we had to be more creative and careful on how we generated revenue.
It was the path that ended up being the best for business as well.
And finally, preserving our values is observed and felt by both our user base and internally as a company.
We tend to get benefit of the doubt, even when we make mistakes.
And as you all know, a loyal user base is absolutely invaluable.
So let me summarize quickly what we've gone over today, and then we'll jump into questions.
The approach we took started with encoding your core beliefs and values in operating principles.
Remember, it's not just useful to list out things that are obvious, but rather things that you want to take a stance on as a company.
Then set up a data-driven culture that loves experimentation.
It's much easier to drive up key metrics when you can measure them and get a sense of what impacts them.
Then identify and enforce some guardrail metrics.
You'll likely find more and more over time, but start with the critical ones that are critical to the success of your product.
And finally, do your best to make immeasurable things measurable.
Whether through process, proxy metrics, or longer term studies, being able to capture the things that you care about is really important in being able to encode good intentions into your product.
In summary, with some intentionality, you can drive a successful business and successful product without compromising your core values.
In fact, you'll likely find that you and your fellow employees are more motivated and successful aligning your core business objectives to your core values.
Games have been such an important part of my life, and I truly believe that they do change the world for the better.
I hope you'll join me in being idealistic and create games that we're proud to be a part of.
Thank you for joining me today, and now I'm looking forward to all of your questions.
