Welcome to our presentation on using AI to create digital avatars for AR and VR.
My name is Kevin He.
I started my career in gaming 10 years ago at Blizzard Entertainment as an engine developer for World of Warcraft.
After that, I took technical roles in Roblox at Disney ever since I enjoyed working on entertainment technology.
So five years ago, we started this company called Deep Motion.
We focus on working AI for animation.
Today, I'm going to introduce to you the technologies we have been working on and what we can apply AI and simulation to interactive avatars.
So my talk will be divided into four modules.
The first one on simulated motion.
That's our first phase engine to use physics simulation to create the characters.
The second one is on motion intelligence.
That is an extension of our simulated motion to provide you more diversity in the character movement.
The third one is about motion perception.
That will add additional ability to the AI to understand how people, how animals move around us and apply that knowledge to enhance our motion brain.
And in the last, I'll give a quick overview of the pipeline we're working on to help you to create interactive content for AR and VR.
So let's start on the first module, which is simulated motion.
First, I want to talk about what's the difference between keyframe animation and simulated motion.
As we all know, keyframe is the standard tag for film and for game entertainment.
But in general, keyframe animation, they are static.
Every time you play, they behave the same, so that's repetitive.
And keyframe take a lot of hand crafting to create.
It's also not reactive to user input and the user influence, which is the quality you are looking for in AR and VR applications.
On the right hand side, the simulated motion, we are looking at physics simulated movement.
That's procedural.
Every time you run a simulation, it generates a unique experience.
It's interactive, it's reactive.
When you engage the AR or VR characters created off simulation, you can see the immediate feedback, how the animals, how the character react to your input.
So that's something we find very interesting.
For that reason, we did a bunch of experiments in simulated motion.
This is the following slides.
We're going through the path we went through to achieve simulated motion for digital avatars.
In order to have a fully physics-similar character, we first work on building a very robust physics engine for joint.
for motors.
As you can see in these slides, most of these demos, they are cars and vehicles, machines, right?
So we started from here.
We make a really robust, multi-joint physics simulation of machines.
So that will lead us to the later stage, where we can add more intelligence on top of machine.
Then the step two we went through is to apply.
Apply the physics simulation on human.
So first we need to create a physics-based character model.
So this model is basically created with multiple rigid body representation.
And all the rigid bodies are connected with the joints, just to simulate the biomechanical model of human.
With such a physical model of human, The next step is to apply our physics simulation on this skeletal model, then want to see what you get.
So the result is a ragdoll.
As we know very well in gaming, it's a passive animation effect to simulate a character falling to the ground.
So we do have some physics simulation.
We have a ragdoll character.
It's modeled with physics, but not so interesting yet.
Then, step four, we need to apply muscle torque.
We need to give the ragdoll some power to activate the ragdoll so it can stand up, so it can maintain its own balance.
So, in this step, we basically apply a very simple muscle model at the joints of the skeleton.
For those of you who are interested in the details, we apply a PD controller on every joint to drive.
Imagine each joint is like a motor in a crane, right?
So the PD controller will drive every joint of the virtual human being to match the target trajectory you want the robot to achieve.
So with this very simple PD-based muscular model, we now can.
can power the ragdoll.
So the ragdoll will tighten up his muscle, and we do physics simulation.
So you'll get someone who can hold his posture, but still not standing there.
The remaining problem is balancing.
So in the fifth step of our simulated motion, we want to add a balancing to the digital avatar.
And it's not actually.
It's not very hard.
If we study the human balance theory, when our center of mass is under gravity, when the gravity pulls us forward, we will flip forward.
The problem is the projection center of mass is go outside of your support polygon, as shown by the red square on the ground.
So all you need to do is just tighten the muscle.
across your skeleton properly.
So you can generate a ground reaction force, as shown by the green arrows.
The aggregate effect of the ground reaction force will push your body back to a balanced posture.
So after implementing this basic robotic balancing theory, we can get this.
We can get a digital robot standing in place.
performing some simple actions, such as rotating his waist, do a kick, or just do squatting without falling to the ground.
So you can think of this like a virtual world analogy of Boston Dynamics robot in the physical world.
Basically, this phase, we employed robotics balancing algorithm into our digital agent.
We can get a standing in place balanced agent.
And the final step, we want to add locomotion to this digital agent.
So, it's not that hard.
Basically, we can balance in space.
We just need to lift one foot at a time and alternate between the two feet.
And then you want to give it some forward push, you know, implement a foot planting logic to place your foot strategically as you move forward.
Thanks very much.
we implemented a very simple locomotion controller that can power the digital character physically and walk around while not falling to the ground.
So, so far we implemented a physically simulated character that can do self-balancing and walking.
Now, the question is, what can we do with this digital agent?
So I want to show you a few applications in VR, which is our main interest area of today.
The first application of simulated motion to VR is full-body motion tracking.
Before going to the demos, I want to talk a little bit, why do we need full-body avatar in VR?
We all saw that Facebook released this.
awesome Facebook Horizon platform for social VR.
But as you can see, in social VR scenario, if you only see the half body, the upper torso of your friends or fellow, that's not as believable as you hope it to be.
So in multiplayer or social VR setup, you do want to see the full-body presence of your friends, you know, of your teammates in the virtual space.
However, commercial VR device only offer you three point of tracking.
Basically, you have one 6DOF tracker in your HMD device.
You typically have two other 6DOF or 3DOF trackers.
on the hand controllers you are holding.
So in short, you usually have three-point tracking.
You can utilize that to model the upper body movement.
And then the key question is, how do you generate or synthesize the lower body movement?
So together, you get a full body representation in virtual space.
And that's the first application we are trying to apply simulation to.
And we did receive questions from users, why don't we just use full-body mocap equipment?
We know we can do that, right?
We can apply the traditional optical-based motion capture or inertia-based motion capture.
There are a lot of options.
But in general, to apply a professional motion capture equipment to achieve full-body tracking, that's expensive.
You have to set up a studio to do that.
You need to purchase expensive hardware and software to do that.
The post-processing is quite complicated.
It takes a lot of time.
And also, there's no single standard for motion capture.
So it's very tailored from project to project.
In short, you cannot really achieve it with consumer-level VR device, right?
It doesn't provide enough sensors all over your body to do full-body motion capture.
So, let's come back to how can we solve the problem in the context of consumer-level VR hardware.
let's say you only have three tracking points, or four, or two.
It's not so much different.
The traditional technique to solve VR 3-point tracking is use IK.
So IK is a shorthand for inverse kinematic.
It's basically a mathematical solver to solve partial body or full body motion based on where your hands are, where your head is.
It's an inverse algorithm.
But IK is only looking at geometric constraint of human body.
It doesn't really look at physics, dynamics, like mass properties, Newton laws.
So with only geometric relationship considered, IK often have ambiguous solution.
You have to tune IK to make it work right.
We often have flip elbow, a lot of complex problem using IK.
And also, IK only solves the upper body.
It still cannot predict where the lower body are.
So a standard technique to add lower body is use a keyframe.
You can just keyframe a few fixed locomotion pattern and slap together with upper body IK driven.
And then it works in some scenario, but it's hard to match the lower body and the upper body because keyframe-based lower body are just fixed animation, right?
You often have foot gliding problem, self-penetration issue.
It's sometimes really weird because it's just keyframe.
It's just fixed solution.
So although the traditional technique of IK plus keyframe...
is a solution for full-body avatar tracking, but we think there can be a better way to do it.
using inverse dynamic, using physics simulation.
So in this example, we are experimenting, how can we use a simulated character to achieve three-point tracking?
Basically, all you need to do is to tell the physics system where the hands are, where is the head, and let the physics to figure out, you know, the body's motion state, the lower body's posture in order to maintain a full body balance.
So we want to maximize.
maximize the knowledge.
in physics equation to give us as much prior as possible to drive a full body movement.
So with such simulation-driven character, we can generate full body motion more naturally.
And also, because it's simulation, so it can automatically interact with the environment.
If you push the character, it will react to it.
If the character run into a wall, it will react to it.
So it's more interactive to the environment.
So that's how we are doing simulated motion for 3-point.
So in these slides, we are showing two flavors of that 3-point tracking using simulation.
The left-hand side uses Oculus Quest as the testing hardware.
The right-hand side uses HTC Vive.
There are no essential differences, but it's just different settings.
In general, We found a simulated motion.
We can run on mobile or tethered environment pretty smoothly.
As you can see, even with three tracking points, we can synthesize some reasonable lower body movement.
As the character is leaning, we're trying to predict the torso leaning from the head angle and hands positions.
So with such technology, we can create a reasonable full-body representation of yourself in virtual space.
So with such a virtual body, you can engage multiplayer experience better.
You can socialize with friends in VR more intuitively.
Yeah, another benefit of that is you can also interact with the environment more easily, right?
Like, we can see the right-hand side, the Vive demo.
The actor is moving around, and she can sit on the ground, right, because there's collision with the ground, so the sitting motion generates automatically.
In this case, she's sitting on a virtual box.
And the collision will generate the leg movement.
You have the leg need to rest on the box.
So that's three-point tracking.
The next demo, I'm going to show a different flavor of multi-point tracking.
This is four-point tracking.
There is additional fourth tracker on the back of the actor.
So the tracker on the back will track rotation only.
In other words, you could use a cheap inertia sensor for the fourth tracker and just attach it on the performer's back.
It should still work.
Why do we do the four-point tracking?
It's because, you know, even three-point tracking can track some basic leaning, but the fidelity of the torso curvature is still not ideal.
With additional points, as you can see, the character can do the leaning, you know, in a much more realistic fashion.
For some shooter games, for some location-based VR that need, like the shooting demo on the right-hand side, the shooter really need to hide behind an obstacle and leaning out his body in order to do the action.
Now we can do it with four-point tracking.
So still, the four-point are pretty lightweight.
It only need upper body standard VR plus additional sensor.
And with this setup, we can get a pretty good full-body representation.
Like the left-hand side, the performer is doing some dancing kind of movement.
It can track the hip and the torso reasonably.
And if we look at the lower body, the lower body's leg movement is not one-to-one mapped to the real leg movement.
But because we employed the robotics balancing theory, as I illustrated early, when the center of mass of the actor move outside the supporting area of your feet, the AI knows automatically swing the foot to the right direction.
So it roughly match.
It's most of the time reasonable.
And in a summary, here I just want to recap what's the difference between three-point tracking and four-point tracking.
As we can see, three-point tracking on the left, it track a basic torso leaning, but you need to lean very clearly for the VR to pick up.
The right-hand side with the fourth tracker on the back, you know, we can track more details in the torso.
You can do a little bit curvature movement.
you know, in the back, we can still track it.
So there's such quality improvement as you put more tracker into the system.
But I also want to stress, this shows the flexibility of a physics-based tracking system.
You can just add, you know, if you only have one tracker, you can put it on the head.
We'll figure out the hands and everything else.
If it has two tracker or three tracker, you put it on more, you put them on more body parts, and it will track more detail.
And we can incrementally increase the tracking quality by adding more sensors.
But the system can synthesize everything else.
And the last demo, I just want to show the same technology you can apply in a two-player environment.
So because both avatars are physically simulated, they can collide with each other.
You can sort of touch each other in virtual space.
We can do some dancing or fighting or people hugging.
So that sort of very intimate social experience will be enabled by this physically simulated avatar tracking technology.
We've seen a lot of demos about tracking the avatar, which is your own motion.
But what about NPC, non-player characters?
In this case, we also use simulated motion to simulate the dog movement.
The dog is a very simple one.
It's just trying to do a balancing, just trying to stand on his feet and remain balanced.
But here, the player is using his virtual hand being tracked by leap motion to engage with the dog.
Even though we didn't add any keyframe animation to the dog, but just because the AI dog trying to balance in place, you'll see a lot of richer experience being rendered, being generated in real time as the player trying to touch the dog, trying to pinch, or doing some random movement.
So this shows the potential of simulated motion in creating non-player character as well.
I think by combining avatar, which is a player character, and a simulated non-player character, we can build a full procedural world that's a lot more interactive.
because the motion itself now is generated by simulation and AI instead of just key frame can experience.
So we talk about simulated motion as the first phase of the technology.
And the second phase, we experimented to upgrade simulated motion to the motion intelligence phase.
And what is motion intelligence?
I like to quote.
a definition from DeepMind blog.
So the agility and flexibility of a monkey swing through the trees, where a football player dodging opponents and scoring goal can be breathtaking.
So how to master the AI to coordinate your full body muscle structure to achieve this kind of motor maneuver?
That's the hallmark of motion intelligence.
That's very exciting area in AI.
And that's the kind of technology we are moving towards as well.
To its core, we just want to build a digital cerebellum to do balancing, to do motor skills, like a little baby learn from observing.
For those of you who are familiar with reinforced learning, the setup fundamentally is a reinforced learning.
You have a digital agent.
It will...
It will take observations from the virtual environment.
In this case, that's the VR or AR environment.
It will send instructions to the agent and generate a reward.
By collecting reward or penalty from thousands, thousands of iterations of experiments, you learn from your failures.
You learn from your success.
And actually, that's very familiar to us because that's how humans learn our experience as well.
Applying that to a digital world, build a deep reinforced learning pipeline, you can actually teach a digital agent to learn to walk, learn to stand as well.
So this example, this is the input to the machine learning algorithm.
It's just running, right?
One second of running animation.
So this is the reinforcement learning result after 100 iterations.
Basically, you do 100 times experiments.
Every time you try to tighten your muscle, in a way, you can mimic the input motion.
Of course, you will fall. You will run into the wall.
A lot of bizarre.
scenarios but when you collect the feedback, use the feedback to do gradient descent to correct your you know your motion, you can eventually get something that's kind of working. But if you keep doing this after 1000 iterations, you will generate AI that can do running, running while keeping balance pretty smoothly.
And because of the simulation, it can also endure your disturbance.
As the digital agent running, you can use a mouse tracker to track him around.
You can apply a virtual force.
He will just react, right?
So that's the beauty of simulated motion.
And then in the nest, we can naturally will teach multiple behavior to that digital brain.
In this case, there are three behavior, like running, sharp turning, and get up.
by combining three behavior to the guy, now you have an agent a little bit more smart.
It can turn now.
And if you pull him to the ground, he can pick up himself.
This whole thing will be generated by a simulation.
It's not keyframe.
So every time you play it in VR, it could be different.
It is then, you can keep building on, you can keep expanding the motion brain to have six behavior, eight behavior.
In this case, that's a six behavior agent.
It can do basic indoor mobility maneuver, such as climbing up stairs, such as sitting in a sofa, yada, yada.
As you can see, we can keep building that.
And this one, we want to add a more upper body variance to the MPC, so we add 20 behavior to a pedestrian motion brain.
And in this case, the guys are walking while making a call, reading notes.
So just add a lot of diversity to the scenario.
So we could use this to simulate a crowd, crowd simulation, or paddy string case.
We can actually use this kind of motion brain in non-gaming environment, such as if you're making autopilot car.
If you make a training program involving full-body physical intelligence, we can use simulation and AI to help.
This example shows a more sophisticated motion brain.
modeled of basketball player.
So each demonstration in this slide had three behaviors, three styles of passing.
As you can see, the AI can stitch them together.
And based on high-level commands, like where to move towards, AI will automatically pick the right behavior to transition into and blend them together.
And who says the technology can only apply to humans?
Actually, it's the exact same methodology.
Physic-simulated character and driven by AI.
This you can use on animals as well.
Actually, quadruped is easier to do than human because quadruped has four legs.
It hardly can fall to the ground.
So you can apply to an animal, different form factors of creatures.
And we talk about motion intelligence.
So now I want to just show a few example how we can apply motion intelligence to create an AR character.
So the left-hand side are just a boy character spawned on a kitchen top in AR.
And you can mess around with the boy and see it's dancing, it's being pushed off.
The right-hand side is an AR dog.
You can spawn the dog on a carpet and throw different toys at the dog and do some kind of engagement interaction.
This is the, you're probably familiar with this video already, because we've shown similar six behavior characters in the earlier slide.
Now if we apply the exact same brain in the AR application, you can create a little.
indoor mobility demo with a house, right?
This is like a two-story house with a staircase.
You can dynamically change the digital environment, and that character will respond to it.
It will try to climb up.
If you change where the staircase is, the character may fall to the ground, may succeed, just like in real world.
So basically everything we did in simulated motion and in motion intelligence can be automatically applied to AR so that you can create a highly interactive character.
You can create a highly interactive environment.
We think this level of interaction is necessary to bring the AR world to life.
You can experience it.
You can really feel the presence of this digital being.
And then the next module, I want to talk about motion perception.
We talk about motion intelligence, very cool, to synthesize simulated motion.
But what about the vision?
What about the input part?
for our motion brain, right?
As a human, we can understand the external world very well by observing how people move around.
So this last piece, we are extending the motion intelligence to cover the perception path.
Basically, we can do the following thing.
five steps in order to achieve some basic motion perception.
The first, as we know, you can apply some post-estimation algorithm, such as the OpenPose from CMU.
Welcome to check it out.
It's an open source project, very popular.
You can use an algorithm like OpenPose to extract your joint position.
of human from just an image, from just a camera feed, camera input, right?
And the second step, you can retarget this skeletal joint position, retarget that posture to a digital avatar, to a digital model.
And the third step, you can apply...
physics simulation on that character to give it physicality, give it collision, give it a weight and mass, all these nice things.
And then your avatar become more alive.
It's not only tracking the input motion, it can also collide with the environment.
And also by applying this simulation technique, you can make the tracking quality higher because we can automatically fix some artifact, like a self-penetration, like a joint limits violation, things like that.
So by adding this motion perception to your pipeline, you can further improve the pipeline's capability to model human motion, to capture human motion, and convert them to a digital character in real time or offline.
So this last piece also demonstrate just from single RGB camera, what you can track from a 2D video or from a camera input.
And in the last slide, I just want to quickly talk about the pipeline.
So we talk about a lot how to create simulated character in AR and VR, but exactly what's the workflow for artists and content creators to achieve this effect.
So here I want to summarize this workflow at a high level.
The first, you can upload or ingest your own FBX file, your 3D Max or Maya digital assets into this AI system, whatever it is.
And the second, you want to add a physics layer on top of the artistic model.
By that, I mean you will add the.
the collider, the mass, the weight, the joint, the muscle strength, all those physics and the controller parameters to the artistic model.
After this second step and the third step, you need to decide what are the set of behavior I want my motion brain to master.
And you will pick those subsets of behaviors.
If you can have reference motion provided to the AI, that's good.
If you do not have reference motion, you just need to describe it procedurally or mathematically in some fashion.
After you select your behavior, you can start a training process.
The training process, for very simple behavior, you don't need to run machine learning.
For example, as I showed in earlier slides, for the standing in balancing behavior, That's a very simple heuristic.
If center of mass is moved outside your supporting area of your feet, then try to tighten the muscle to move the center back.
That kind of simple heuristic, you can just write it from a few lines of script.
So.
However, if the behavior is quite complicated, like some of our more advanced demo for the basketball player, dribbling the basketball without losing it, without losing balance, that's kind of hard to manually generate a few lines of script to control the muscles, things like that.
For those, you can run through a neural network, just like machine learning, to do thousands of tries and errors to figure it out.
So this way or another, you will train a control logic.
and then you download this control logic as an assets file.
This assets file imported to your game engine, your own environment, and then finally, during the runtime, you need the physics engine to be paired up to the controller assets, and you will get your controlled simulated character in place.
So that is an overview of how this pipeline could be constructed.
I mean, at D-Motion, we provide solutions for various motion intelligence.
Tech, as I described, we create AR, VR, body tracking solution.
We also help our partners to train interactive characters and provide the real-time motion perception and customize motion brain, et cetera.
So if you have any questions for the slides, or if you need additional help on motion brain training or digital avatar creation, feel free to reach out to us.
So, yeah, that concludes my presentation, and I'm going to take questions if you have.
Thank you.
OK, go ahead.
Hi.
So it's really impressive.
It's very impressive.
So the thing, though, that strikes me is, do you think There is a way that you can also include the next level, which is personality.
Is there any way at all that even if you train 10 people, 10 characters to walk, is there any way you can think of to inject, for the artist to inject different kinds of personalities in the different walks, procedural as they may be?
Thank you. So the question is, is there any way to inject personality in the trained AI, such as 10 styles of walking? How can you represent emotion or personality?
Very great question. So the answer is yes.
Although the motor brain...
I showed in the demo are for primary motor skill, mostly like walking and dancing.
But if you train 10 behavior that share the same pattern, like walking, with 10 different walking or running, what you can do, you can easily include action label in that training.
So you can label that action.
You can label the mood as happy.
tired, angry, with some reference motion to the AI.
So as you are training, you're basically creating a latent space to tell the AI, you know, what kind of motion you consider happy, what kind of sad, and the AI will create an interplay, the intermediate result representing the level of happiness.
So in short answer, yeah, you can define more parameters to train the AI to hopefully demonstrate some personality.
Thank you.
I thought the tech looked amazing.
It reminded me very much of Natural Motion's Euphoria SDK back in the day, but my understanding around that SDK was it was incredibly complex to add new behaviors and how easy is it to kind of train within, you know, how are developers finding using your tech, I guess, because I think that was almost the reason why Euphoria didn't become bigger was it became too problematic for developers to use.
How are you finding that?
Great, thank you.
The question is, natural motion euphoria produced very impressive physics-based animation toolkit in the past, but it's hard to use, and how the motion or how the new solution can help in that space.
So...
We were also inspired by natural motion.
So it's great technology.
But really, the difficulty in deployment is how much time to create a new behavior.
For example, using the last generation tech without deep learning, without this new simulation, AI.
it probably take a month to add a new, let's say, dancing movement to your physics-based character, because it's not just dancing, right?
You need to think about the muscle, pulling sequence, the balancing requirement, all the physical constraint.
The mathematics behind that can be daunting.
So I think the new tech is exactly trying to address that concern, because the beauty of deep learning and AI is that you can learn from data.
So we just need to throw more reference motions from motion capture, from high quality key frame to the trainer.
And AI will do that thousands and thousands of iteration and figure out some, you know, some pretty complex neural nets that represent some implicit math.
know, to control that. So, I'll give you an example now, in order to add a new running or dancing sequence to our motion brain, it probably takes an hour for a simple case.
For a very complicated case, yeah, it could take days, but still it's like machine time days. It's not like our mechanical engineers spend months to tune every motor control. Thank you. Sure, thank you.
Any more questions?
Okay, thank you.
So if you like the talk, welcome to give us a good rating in the review.
Thanks.
