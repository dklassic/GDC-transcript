So thank you all for coming.
Welcome to HoloLens and Beyond.
My name is Bart Schneidlowski, and I'll be discussing augmented reality game design challenges and open problems.
So just a little bit about me.
I'm not actually a game developer, nor am I an augmented reality developer, but I've been spending a lot of time with it lately.
Just out of personal interest, you can find out more about me by visiting my website.
Augmented reality is set to explode.
I think everybody recognizes that at this point.
And it's an exciting time.
Right now, I think you can divide devices up into two categories, mobile and headset.
Mobile is pretty ubiquitous.
I think almost everyone in this room probably has a device that can do AR.
But how many people here, just a show of hands, have used an AR headset?
Wow, so actually quite a lot, more than I expected.
So right now the HoloLens is the leading edge device, despite being two years old.
And that's what I've been using to do my AR game development experiments.
Just an overview of the capabilities for those who might not know.
It's a fully self-contained computer.
It's untethered, no connection to Wi-Fi necessary.
You can walk around the room, do whatever you'd like.
It has a depth camera for acquiring the geometry of the environment that you're in, as well as detecting hand gestures.
Now gesture support is very limited, it's not like a Leap Motion device, that's why you won't really see a lot of hand stuff going on in my demos.
It has excellent spatial audio capability that's often overlooked, it can do 3D positional audio very convincingly, and it has voice recognition.
So I've been doing a lot of just little prototypes and games in my spare time.
Two of them are up on the HoloLens app store.
And I'll just kind of go through those really quickly to show you kind of what I've been working on.
So this one is just a straightforward helicopter action game.
controlled by an Xbox 360 pad.
And it was inspired by Electronic Arts' Strike series, if everybody remembers Jungle Strike or Desert Strike back in the early 90s, that was my inspiration for this.
The next game that I've released is very different.
It's an educational game, it teaches you how to ask for and understand spoken Japanese directions.
You play a little bear, you have to guide him around a town, delivering honey and asking for the location of the next customer.
And there are segments throughout the game that will introduce progressively more vocabulary, so you actually look away and take like a little mini lesson.
Go straight.
Turn left at the second street.
Go straight.
Turn left at the second street.
So that's the the gist of those 2 games. Let's talk about game flow first of all a typical AR game involves scanning the environment first the device can acquire the 3D geometry of the level and then that is retained.
Placement comes next. This can be done automatically you can place units based on the geometry you have or in some cases scanning and placement are actually the same thing. In Doko Desu ka the user places things as the depth cameras turned on and there's no actual pre-scanning phase.
So once you're done placing it's time to play and you can optionally continue to refine the scan and use the depth camera during playtime.
Challenges in AR game design I think can really be broken down into two big broad areas and that's challenges with the unpredictable environment and challenges with the players unfamiliarity using an AR headset.
The environment is obviously unpredictable, you have no idea where someone's going to be playing the game.
It could be here.
in the bathroom for whatever reason or on an airplane.
You have absolutely no idea, but your game will be expected to work in any environment.
The placement and level design challenge, I would say, is mostly a technical challenge.
And I like to think of it as topology and context, the two big factors here.
What I mean by that.
Topology, I mean, does the object actually fit within the topographic area that you have physically, without actually asking whether it belongs there.
So for example, why are these crystals on my toilet?
I have no idea.
They probably don't belong there.
Context would answer that question, and that's a matter of detecting objects.
So in this photograph, the character was actually placed after the couch was detected.
Another design challenge that I found personally is the issue of scale.
So in a traditional game, you have complete control over the scale of the levels, the user's perspective and so forth.
That's not the case in AR.
And I found that you want to have your holograms be relatively large to be kind of compelling.
and also the hardware doesn't really have the fidelity to support, you know, micro-machine-sized vehicles running around.
But that also introduces the challenge of, you know, how do you scale, for example, physical buildings if you want to have them realistic compared to your objects.
So it's an artistic issue.
In this screenshot, it's hard to see, but the helicopter is way too big relative to the building, and it's something that I never really thought about, but I think as an artist, or as artists, you might want to consider that before embarking on an AR game.
The way placement works in the helicopter game.
I want to start off by having, you know, I want the maximum distance between structures in the game.
So there's a helipad in a building.
And ironically, this screenshot shows an erroneous placement where they're placed right next to each other.
That sometimes happens, the constraint solver fails.
I do this by generating multiple different, you know, sizes of objects and different separation distances.
I do a bunch of trial placements and I pick the one that scores best.
Vehicles are much more straightforward.
I distribute them between elevated surfaces and the floor.
Oftentimes the scan will not have any elevated surfaces, in which case they just fall back on the floor.
Now there's a big opportunity for people that want to kind of move the field forward to, I think, to develop better constraint and placement solvers.
The options right now, there's a few of them.
They allow you to kind of specify some basic constraints, but they're not very flexible.
I think there's really a huge amount of opportunity to really do something much better.
Dynamic navigation planning is a big thing, being able to incorporate.
you know new areas are scanned dynamically into like a much more robust and have mesh type system I think that doesn't really exist off the shelf yet and the big one is object recognition segmentation to provide context so that allow you to do more interesting and relevant placements around the room or whatever area you're in and of course facilitate character interactions with the environment and on that note you know what we do with object recognition when I when I took this picture I immediately thought I don't know how many people remember Duke Nukem the iconic alien scene, right?
But this is just kind of silly, but obviously you could imagine doing much more powerful interactions.
And in the way that Pokemon Go kind of inspired people to go out and explore, just get out of the house, you can imagine in the future having games that encourage people to just go around and explore how different objects interact with their characters or their game, like what abilities are unlocked and what could possibly happen.
I think that's a hugely powerful aspect of AR that I expect to see really explode the next couple of years.
This is just a wish list for placement solvers.
I'm going to just skip over this and leave this for people who can, you know, want to download the talk later.
While I was developing the helicopter game, one challenge that I encountered was the AI helicopter had a lot of difficulty navigating around.
The room rooms are actually very cluttered in this case, you can see it bumping into my what the hole and sees my office chair. So I I tried, you know, basically just ray casting and just a kind of a heuristic approach.
I you know, it's inside don't have control of the level. I thought that I can kind of get away with this. Well, it turns out it doesn't really work very well. So I had to essentially create like a din you know, a dynamic aerial nav mesh of sorts.
Now you can see the helicopter is able to navigate around the office chair without any difficulty.
So it's an interesting problem to have to consider.
You want to consider where the empty space is as well as where actual items are.
To accomplish this, I just basically sampled the space ahead of time after the scan is complete.
Here you see what are basically the center points of volumes and there are edges between.
So this forms a graph.
The yellow color coding indicates the largest island in the graph.
So white nodes are not accessible from the yellow nodes and vice versa.
And green is a particular pathway that you can take.
And you have to build this essentially dynamically when the level is first scanned.
Because you don't have control over the level, spawning vehicles in the middle of the game is a challenge, a design challenge I would say.
And I got around, the way I do it in my game is I have these hidden tunnels.
So it's kind of a fun effect, but a little bit gimmicky at the same time.
You don't want to overuse it.
For aerial vehicles, I use flyovers, which makes excellent use of the spatial audio capabilities.
You can really hear the helicopters coming up from behind and over you.
Let's see.
And this problem with static scenery, so.
the room is not going to change.
The player is going to be playing a particular physical location, you can't change it.
And that actually deprives us of a key reward mechanism.
And that is just seeing the environment change.
So for example, in a traditional game like here's Sonic, the player's ability stay the same throughout the game.
But a big motivator for going forward is seeing what the next level actually looks like.
And I never really realized how much games actually depend on this reward mechanic.
And it's totally absent in augmented reality, unfortunately.
So you kind of have to think about ways to work around that limitation.
I think one solution that's very clear is to empower the player to alter their own reality.
I've been told that Microsoft's acquisition of Minecraft was actually largely driven by their mixed reality plans going forward, so this is kind of a concept image of what Minecraft might look like.
But even without doing a sandbox, totally open-ended game, I think you can adapt a game like say a platformer, while introducing a level design and alteration game mechanic that gives people the freedom to alter their environment.
So I developed this.
prototype as part of a kind of short competition that I entered last year.
And you'll see Mega Man running around and collecting power-ups that allow him to extrude different kinds of platforms and the objective is kind of get to the next power-up.
So this is all controlled in the Xbox controller.
In my original idea, you can imagine taking this to a much greater extent, not just platforms, but having all kinds of deformations to the level, you know, say pushing.
things into walls, carving out tunnels, you know, bridges.
By the end of the game I would imagine that the entire room would be kind of transformed with all kinds of interesting elements.
And I think that this game mechanic would work for other kinds of games, like say a Bomberman type game or some sort of a competitive game.
It's something that I haven't really seen done, and I think it's an idea that I would personally like to revisit once more AR headsets become available and kind of commonplace.
So I had a lot of fun making it, and people that have tried it out have had a lot of fun playing it.
Another big design challenge is the fact that the player controls the camera.
The player's head is the camera, much like VR.
You can't really force a player to gaze in a particular location.
Unlike VR, however, at least home VR, you have near unlimited ability to walk around.
You can walk around the room without fear of tripping over things because you are still in your real space.
And that should be leveraged, I think.
but the narrative challenge still remains and I think you can kiss cinematic sequences goodbye for the most part.
But interestingly on mobile, they're still perfectly doable.
You can switch between a augmented and non-augmented mode.
So that's one key difference between the platforms, I would say.
Something that I encounter a lot when I demo the HoloLens to friends is what I call the freeze.
So here my coworker kind of forgets that he can walk around until I point out that, hey, you can actually move around and get a better view of things.
It's surprisingly common, and I think that since AR devices are going to be a novelty for several years and you want to kind of keep in mind demoing these things, you have to come up with strategies to draw the player out and make them realize that, hey, you actually can walk around.
You don't have to stand still.
It's not going to disappear if you move around.
A big part of why that happens though is the limited field of view.
Something that the HoloLens has been criticized for and I know upcoming AR headsets kind of have the same limitation.
The videos that are recorded have holograms present throughout the entire frame.
In reality there's kind of a clipping window that's quite small and it makes it easy, especially in the helicopter game, to lose track of things.
I use guidance arrows to point out where people should be looking, but I found that the simple guidance arrow that most people are familiar with if they're gamers, which uses the shortest angle distance, is very confusing.
What happens is people look down sometimes if there's something behind them, and then they don't see that thing there, and they're like, well I don't know what's going on, and they totally ignore the guidance arrows and they get confused.
What I found works much more effectively is just simply limiting it to horizontal movement.
And then once something's within like a range of an angle sweep, then do an up and down guidance.
And also just adding that look text made a huge difference when I started demoing it to people.
And I think there's room to do things like seamless confusion detection, which is something that I've kind of worked on.
I haven't gotten great results yet.
But the idea is knowing all the stuff that's going on in a game, where the player's looking, whether they're looking around rapidly.
You can figure out whether someone's confused and perhaps offer guidance.
The problem is you can also incorporate verbal cues, like if people will often verbalize their confusion.
The challenge of course is limiting false positives.
Many people might, you know, there's lots of times when you might want to look away intentionally, like say you're talking to somebody and you don't want to start flashing indicators in front of the user.
So far my ideas that I've kind of shown are not necessarily AR native, they're just adaptations of existing games.
But one thing that came to mind is that you have a lot of information about where the player's head is at any given frame, so why not use that as an input device?
So I came up with this little game concept that I think is closer to something that's really AR native.
It makes use of the environment interactions and that kind of room scale experience.
And it's something that really can only be done in AR.
And people that have tried it have had a lot of fun.
It's a very kinetic experience.
And again, something that I think I'd like to revisit further if I had time.
Ergonomics.
An issue that came up in Doko desu ka?
The experience, like many, takes place on the floor.
and it's very uncomfortable to look at the floor for extended periods of time unless you happen to be Gaston from Beauty and the Beast.
Most people don't have that kind of neck strength, so you want to think about that.
In Doco Descartes, the solution was proposed by a co-worker.
It's really simple. You just allow the map to be elevated and it works great.
Simple solution, but definitely something to keep in mind.
It's more comfortable to have your head looking up than it is down.
Now, To really take advantage of augmented reality or mixed reality, I think you want to kind of get away from the design language of 2D screens. Unfortunately, I know I'm guilty of this myself.
A lot of applications have 2D floating menus.
But what's really a lot more effective than pasting something in front of someone's face is to just make it incorporate your UI elements as part of the environment.
So for example, these buttons are 3D objects in the level itself.
Also, what you can do is map.
different kind of logical things that happen in the game to different spatial areas in the room.
So in Doko Desu Ka, the original idea was to have an instructor that stands off to the side.
I ended up with a blackboard, but the idea is that you look away from the game when you wanna learn or review vocabulary, and you kind of do that mental context switch.
The game knows you're not looking at it, and then when you're ready, you go back to the game.
And I think that's kind of a very natural feeling, very intuitive way to leverage space in UX design.
So also a key differentiator between AR and VR, of course, that you can do this kind of like room scale stuff much more easily.
So in summary, what have I learned about making things fun?
I think what I've learned is that you want to keep a casual audience in mind.
It'll be a while before these things become widely accessible.
You'll have a lot of, you know, non-hardcore gamers using it, and you want to be attentive to their needs in kind of drawing them into these experiences.
You can adapt many existing game mechanics, but I think the real payoff is in developing AR-native content, stuff that can only work on these platforms.
A big part of that is making use of the environment.
It adds that wow factor, and really, that's what makes things really AR-native.
So there's a ton of opportunity to do creative things here.
And just keep in mind that people enjoy creating and sharing things, and the ability to change their reality feels magical to them.
And so you should really keep that in mind, I think, when designing games going forward for this kind of medium.
I think there's a lot to look forward to in coming years, obviously improved headsets, and also the AR cloud.
So for those who don't know, this idea of having a digital soft copy, so to speak, of the world, everything kind of mapped out and accessible to facilitate multi-user experiences, you know, persistence.
And I think this will open up the door to what I would call like, you know, world-scale location-based gaming.
And I wanted to leave you guys with this little prototype that I made, a little demo, using a major motion picture franchise for the art of what I think gaming might look like in 2021.
Agent Nicole, do you read me?
Copy that. I've got your video and sensors.
So this is the place?
Yes.
Doesn't look like much.
I'll bet that's just the way they want it.
All right, I'm going in.
I'm picking up signs of motion. Be careful.
This doesn't look good.
Get out of there.
Head for the exit.
So is it actually arriving in 2021? Well, I think that depends on people in this room.
I hope that it becomes a reality and if you guys don't make it, I think maybe I will.
So that's pretty much it. I think we have 10 minutes now for questions.
If anybody would like to ask any questions, feel free. Thank you.
Nicole Lizaro from Z Design.
I was curious, what do you think is the largest unsolved problem?
What would you like us to do next for creating good experiences for AR games?
What is the largest unsolved problem?
I think that's a very good question.
I think, gosh, there's a lot of stuff, but I think that if I had to pick one thing that I certainly want to investigate going forward is the object recognition segmentation and making that much more easy to deal with.
I think that would really facilitate a lot.
That and of course, just improvements in sensing and so forth, but I think object recognition, if I had to list one thing, that I think could make things really magical.
So you know that it would be AI, a difference between a coffee table and a couch, because your character would do different things if it was on each side.
Exactly, yeah, being able to detect that quickly and having a way of robustly categorizing what the play space looks like, I think would be just super amazing.
I want to play a game where it knows that the best place to hide the body is in my fridge.
Exactly, right, right, right.
Thank you.
Thank you.
For developers, what other tools are available for geolocated-based gaming other than the new Google Maps feature and Mapbox?
In terms of geolocation, I'm actually not sure, unfortunately.
I wish I knew.
I know that Google recently released that API.
On the HoloLens right now, for example, there's no real built-in geolocation capability.
There's no GPS.
You'd have to do it through Wi-Fi or something.
I really don't know, and that's, I think, an area that's being, I think there are lots of small startups working on things, but unfortunately I don't have any understanding of that.
When you were scanning the room for the game, what's the time frame for the calibration of the room between scanning it and starting the game?
That's a really good question.
The question was, you know, how long does it take from scanning the room to starting a game?
And the room scan itself can take quite a while.
I use the Spatial Understanding Library, which post-processes the raw spatial mesh even further.
So it can take a bit of time, depending on the play space to scan the space.
So you're looking at like, you could spend like 30 seconds to a minute to two minutes scanning, if you want like a, you know, the full area scanned.
And then on top of that, depending on how you implement your placement solver after that.
It can take anywhere from like 10 seconds in my room to place the objects to I've seen some places where it gets confused and it spends like almost a minute.
Now that's partly because I didn't use the most efficient way of placing things and invoking the constraint solver.
That I think could be easily improved.
But you're definitely looking at sucking up about a minute of time.
And so in my game, once you've scanned a space, I make sure to retain that.
So if you start a level over again, it asks you do you wanna just reuse the previous scan so you don't have to keep going through and doing it.
It is slow though.
How long before it's not exact real time, but near real time where you could like walk into the bedroom and delay you for a couple seconds or something and then kind of pop up there?
That's a great question.
I think it's really just a year or two away.
So the AR cloud stuff that I mentioned, that's kind of that idea.
The idea is that they'll be able to kind of crowdsource scans and so when you walk into a place that's already been scanned, it recognizes that and loads the room data.
I'm not on one of these teams that develops the hardware, but from what I've kind of heard leaked to me.
It sounds to me like this problem will be alleviated a lot within the next year.
So I think we're not very far off.
So certainly within two years when this stuff is more mainstream, I think you'll see the scanning phase be done much more rapidly.
Cool.
Great talk.
Thanks for sharing.
Thank you.
Thank you.
I noticed that in some of your examples, you used a game pad for controlling the...
the game and in some of them it was gestures.
What do you think are the unique challenges as far as control space between, like the differences between those?
That's a good question.
You know, challenges with regards to control.
I think what I've noticed when people first use these devices is they start reaching out at stuff.
So obviously having hand gestures is super, super important.
Now, the problem is though, I feel that once you get beyond that initial stage, you always have to have that to be intuitive.
I feel that beyond that though, hands are not very intuitive.
And in fact, if you think about what hands are used for, they're used for manipulating tools, not like pointing and waving at things.
And I think that people that are very zealous about using only hand gestures are missing a key part of the human anatomy and why we have hands.
The Xbox 360 controller, I think, is unfortunately not something Microsoft is gonna move forward with in terms of a control device, but I find it to be extremely useful.
I personally prefer it to motion controller type controls.
I think a combination of something like that and hand controls would be great.
But if you really want fine, persistent control, when you're trying to design something, for example, something like a controller actually works really well.
And I like the fact that it's one single thing, so I can take one hand away and start waving around, potentially, and then return to my controller.
So I think realistically, though, it's gonna be a combination of just intuitive hand controls as much as possible, picking up voice, and then something like a motion controller.
I think it seems to be where the industry's heading.
Thanks.
Thank you.
I had a question about redressing or reconfiguring the entire scan.
So it seems like you have a lot of depth information already.
And for example, with the military game, how come you can't cover up the bed with, let's say, procedural texturing or?
kit bed, like modeling, modular modeling that could actually cover up the entire bed.
That's a really good question.
So he's asking about why not generate more procedural geometry to kind of cover objects.
And you absolutely can.
Two things though.
Number one, since I'm not an artist and just working on my own, it's been really hard to do stuff like that.
But a bigger challenge on the HoloLens in particular is that it's very fill rate limited.
So it's very hard to get things to run at 60 frames per second, which is what you want.
And if you were to generate a bunch of geometry over the spatial mesh.
So in fact, what I actually do is I actually cull parts of the spatial mesh.
I didn't show that slide, but I actually remove parts of the spatial mesh that are not absolutely necessary for occlusion.
I retain them for collision just to kind of lower the fill rate requirements.
You absolutely could with more powerful hardware.
I think we're all looking forward to better GPUs on these things.
And I think it's a fantastic idea.
One potential design issue, though, is that when you try to render something really big like that, that small FO, that field of view limitation can make it look kind of weird.
Like, you know, you'll look at the bed and you'll see an area that has geometry, but then the bed is still the bed out here.
So I guess that's kind of an artistic challenge.
To summarize that, one big issue with the HoloLens right now is the performance.
It's got a very weak GPU, but you could absolutely do that, and I think that's an area of further research that I think would be really fruitful to try.
Have you come across anybody that tried?
Not really.
The closest I've seen is just people kind of placing, I think, pre-made maps or maybe kind of procedurally generated but kind of flat maps on top of tables, things like that.
I haven't seen something where you have a conformant scene, like where you have, say, something conforming to all sides of a bed, for example, or a coffee table.
I think it'd be really awesome and I would absolutely love to see it.
Cool. Thank you.
I guess that's it.
I'll be hanging around for a little while probably outside feel free to you know if you see me any time during the expo to to stop and say hello.
If if you're lucky I might have my whole ends on me some of the days here and thanks a lot for attending the talk and I hope you guys do some awesome stuff and I'd love to see more you know more air gaming in the next year.
