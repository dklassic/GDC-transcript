Hello, my name is Jafar Soltani.
I'm lead software engineer working for Rare Games Studio.
We are part of Microsoft.
And today I'm going to talk about Rare's journey in adopting continuous delivery for our latest title called Sea of Thieves.
Is there anybody here who doesn't work in games industry?
Just a couple.
So this slide should be familiar to most of you.
I'll quickly go through that.
Games are developed as monolithic applications in C++.
And we use three main phases.
And we use waterfall process to develop games.
We have a pre-production or prototyping phase when the size of the team is quite small.
And the purpose is to come up with the idea about the game, come up about the story, define what the game is about, prove other technology, and come up with the roadmap for the game.
And once we get the funding and green light, we go into production phase.
That's where the size of the team increases significantly.
And the main purpose here is to develop as many features as possible that we defined earlier on.
That can last sometimes between one to two years.
And once that's finished, we go into bug fixing phase.
That's where we're getting close to the release date, and we can't really move that.
And at the same time, we face a mountain of bugs, maybe about 10,000 bugs that we have to now go and fix for that release date.
And at this point, everybody crunches.
Everybody work long hours.
It's quite stressful.
People make a lot of sacrifices to make sure a game is out on time at high quality.
And at this stage, we heavily rely on manual testing to prove that the game works and catch bugs for us.
And usually, for AAA games, we have one big release followed by a handful of updates.
And the main team moves on to the next project.
And they don't have to worry about maintainability of the code base.
Could you raise your hand if you're familiar with Sea of Thieves or you play Sea of Thieves?
Good.
So Sea of Thieves is a multiplayer cooperative adventure game that we are working on at Rare.
And the idea is that there is no predefined story.
There are emergent stories that develop as a result of simple mechanics in the game.
And you go and form a crew with your friends or with strangers, and you go on an adventure.
For this game, we decided to change our business model and develop this game as a service.
The difference between developing this game as a service compared to traditional process that I just explained is, as opposed to working for three years and then release the game at the end, we're supposed to release the game much earlier.
And by involving our community and our players, we evolve the game based on their feedback that hopefully leads to a better game that is more fun for our players.
So far, we've done over 150 releases of Sea of Thieves to our technical alpha players.
And that's by far the most number of releases that any title have done on Xbox platform.
And our game is live this week.
It came out earlier this week.
And this is the amazing team at Rare who worked on the project.
Now I'm going to talk about why we decided to adopt continuous delivery.
Because we changed our business model, it meant that we wanted to sustainably deliver new features to our players over a long period of time.
And we know that every time we wanted to release, we used to crunch.
So if we didn't change anything, we would have had continuous crunch as opposed to continuous delivery.
So it was quite important that we reduce the crunch as much as possible and therefore have happier developers.
We wanted to get fast feedback from our players so then we can deliver a better quality game that is more fun.
And on top of that, we wanted to reduce the cost of having large manual testing.
I did not even rely on them to prove the game works.
The problem with relying on testing to tell us if the game works or not is what we found is the moment we don't have that manual testing, our developers were not feeling confident to go and make any changes to the code base because they were not sure if they're going to introduce any new issues or not.
I wanted to clarify a distinction between doing continuous delivery and doing frequent releases.
There are probably some games out there that release regularly, but that doesn't necessarily mean that they are doing continuous delivery.
The difference is, for those companies, they still use the traditional process to develop their features.
They've reduced the time.
So as opposed to spending three years, they spent, let's say, three months.
And they plan these features that overlap with each other.
So then they can release them, let's say, one month from each other.
And on the other hand, when we are doing continuous delivery, we might still choose to release once every month.
But we have other candidates that if we choose to, we can still release.
When you're doing continuous delivery, you can easily go from, let's say, releasing once a month to releasing once every two weeks, because you have many candidates to choose from.
Whereas in the traditional process, you don't really have any other candidates.
So you can't easily go from releasing once a month to releasing once every two weeks.
Now I'm going to talk about how we are adopting continuous delivery.
Our developers are responsible for quality of their work and prove that a feature works.
We found that it's very inefficient to manually verify the game.
It can take days until we get feedback whether the game works or not.
So therefore, our developers write automated tests.
We have about 40,000 automated tests, and we run about 4 million tests a day.
90% of our tests are unit tests, written in C++.
And the remaining 10% are larger tests that test bigger systems.
Like these are end-to-end tests.
Some of them are performance tests and memory tests.
Take longer to run.
And when our developers fix bugs, they usually try to add some tests to prove that they fixed the bug.
We work hard to make sure our game is always shippable.
We prioritize fixing bugs and broken tests over working on new features.
If our build is broken or we have an incident, which means we can't ship the game, we lock our depot.
We don't let other people to add changes on top of that.
I'm not sure if you have experienced this or not, but we were seeing that a lot of times if somebody's trying to investigate when the build is broken and somebody else adds more changes on top of that, it makes that investigation more difficult.
So we wanted to avoid that.
So therefore, we locked the depot.
And what we do is we try to identify the change that caused the problem and back that out.
Again, what we found is it's a lot less stressful for our developers if they come up with a fix in their own time, as opposed to trying to fix the build while everybody's blocked.
And we have build lights and TV screens across the studio to notify everybody when the build is broken.
So you can't really miss that.
And as a result of this, the quality of the game looks something like this green line.
Compare that to traditional process where the quality is low generally for most of the project.
And it only gets to the high level towards the end of the project when you're fixing bugs.
And that's one of the main reasons that we can't really ship the game when we are at the state for most of the project because the quality is very low compared to the green line there.
We tried to make sure our builds, as much as possible, have a small number of changes in them.
Because what we found is when something goes wrong, and there's only, let's say, a handful of changes in there, it's a lot easier to know which change broke the build as opposed to when we have 500 changes in there, or let's say even more.
On top of that, when we have so many changes, they might interact badly with each other.
And so what we found is actually the risk grows exponentially the more changes we have.
And on top of that, our developers try to break down their work into smaller chunks so they can ideally submit once a day.
Now I'm going to talk about what traditionally we used to set up our branches.
We used to break the game into different features and form teams around each feature.
And each feature team used to go to their own branch and work on the feature.
And once that was completed, they would merge it back into main or trunk.
And at some point before the release, we used to create a QA branch, fix the bugs, and then create a release branch and release from there.
So there were some problems with this approach.
We could get really painful merge conflicts, especially if you're working, let's say, on a feature branch for a month or two months, and now we want to merge all of that back.
And on top of that, because we are using centralized source control systems like Perforce, merge conflicts are not easy to resolve.
Some people argue that if you use git, it helps with merge conflicts.
But the problem is that about 95% of the content of the game is binary files.
And as I'm sure you're aware, it's very difficult to measure these binary files.
So if two people end up changing the same binary files in different branches, when we try to measure them back, we have to overwrite one or the other one.
On top of that, we might get semantic conflicts.
Semantic conflicts means if somebody goes, let's say, try to refactor a function and split it into two because it was too long, everything might merge fine.
But we're not going to know that that functionality is not complete until later on when it gets into QA branch.
And again, because we don't really get any feedback until changes are in QA branch, the feedback loop is extremely long.
We're not going to know if our feature works well or not until maybe weeks.
And on top of that, if we identify critical issues that requires fixes, now we have to go and integrate that change into many, many branches.
And this is kind of the idea behind continuous integration.
as well, where it says that if something is painful, let's say like marriage conflicts, our natural reaction is to do them as few times as possible.
Let's say you do a marriage and it's really painful.
You don't want to do this again for another three months.
But actually what happens is the next time you're going to do this is even more difficult.
So what we should do instead is we should resist that temptation and try to do that integration as often as possible, ideally daily.
So therefore, we use the trunk-based development, where all the features are developed simultaneously on main.
And we release from main.
We don't create release branches.
So everything is work on main and released from there.
And what allows us to work this way is using feature toggles.
We use two types of toggles.
The first one is compile-time toggle.
which we define the toggles that we want to switch on at compile time.
And then when the game is compiled, we would compile out the feature that we don't want to ship to players.
This allows us to have in progress features and hide them when we release.
We also use dynamic toggle, where all the features are still compiled in, and we ship them to our players.
But then we choose.
to switch that feature on or off based on some criteria.
Let's say we want to test a feature that we're just developing, and we want to give it to a small number of players to learn something quickly or prove that it works and build confidence.
So we use dynamic toggles for that purpose.
And here is an example of implementation of compile toggle.
We use JSON files to define our features.
And in our C++ code, it's a simple if statement.
So it checks to see if it's enabled, do this, otherwise do that.
With dynamic toggle, it's slightly more complicated.
We still use the same JSON file to define these features.
And the way it works is when our server starts, it talks to our services to get the list of features that are toggled on.
And when the client starts, it talks to the same set of services to get the list as well.
And when clients try to join the server, they send the list to the server.
And server compares the two lists.
If they match, it allows the client to join the server.
Otherwise, it will reject the client.
That was quite intentional, because we didn't want different clients with different set of toggles to be playing with each other.
We also tried to continuously improve our cycle time.
Cycle time is the time it takes from somebody deciding to make a change, analyze their work, implement their work, get it reviewed and submitted to our source control system.
Our CI server to verify the change, and pass the manual verification, and finally is released to retail or production.
And here are some of the benefits of having a short cycle time.
By having short cycle time, we can get fast feedback.
which we can find out if we can release, let's say, make a change and give it to our players.
We can soon find out if it works or not, which hopefully leads to better quality game.
It allows us to work in small batches, which leads to lower risk.
Because we don't need to worry about piling a lot of change.
Because we are going to release regularly, because it's so easy to do that.
It also reduces handovers.
If we want to have fast cycle time, we cannot afford to pass the work to different teams and hand it over to them, which means we have to restructure our teams.
Therefore, they are cross-functional.
And the team is responsible for the end-to-end time and the whole process.
We're working on the feature until we give it to players and get feedback from that.
And in addition, if you find a problem, we find a critical bug, we can easily respond and release a patch.
Deployment pipeline is the implementation of our process for working and delivering these features.
And for this project, for Sea of Thieves, We formed a new team of mainly generalist engineers to implement this deployment pipeline.
And that's the team that I lead at Rare.
And here are some of the examples of the things we do.
We've developed our test framework.
We are responsible for our workflow, for our developers, for our artists to make changes and get them into source control system.
We look after our infrastructure and implement our pipeline.
And here are some specific things we've done to improve the cycle time.
We've developed a system to identify flaky tests.
Flaky tests are tests that you run them once, they fail, we run them again on the same set of change, and this time they pass.
So you're not really sure.
They're not deterministic.
I'll talk about them in later slides.
We implemented the feature toggle in our code base.
We regularly try to improve our build time and our cook time.
And we have tried to parallelize running our test across many machines.
Next practice we do is tackling technical debt.
If on the traditional process, when we don't address technical debt, the velocity looks like this red line here.
And velocity is the rate that we can deliver features to our players.
At the beginning, the velocity might be high.
We can work on features easily.
But what we find is after a while, as the code base grows, it takes longer and longer to work on features and develop them.
On the other hand, when we're dealing with technical debt, the velocity looks more like this green line, where maybe at the beginning, the velocity is lower.
But over time, as the code base grows and more people work on the project, the velocity doesn't drop.
It actually might even increase.
And the important thing here is that there's a point in time when these two lines crosses.
And the people who argue against tackling technical debt, they think that their project would be done before these two lines crosses.
But what we found is, unless the project's going to last maybe three to six months, you're definitely going to cross this point.
So it's well worth the effort to tackle technical debt.
But at the same time, we don't have zero technical debt all the time.
Sometimes we intentionally accumulate some technical debt because let's say we want to prioritize delivering a feature to our players quickly to learn something from that.
But what we do is, once that's done, we go back and pay that technical debt.
I would argue that continuous improvement is probably our most important practice, because we're trying to build a learning organization to learn from mistakes, problems, and failures, and get better every day.
Our teams do regular retrospectives, usually every two weeks, which they sit down and talk about what went well, what went wrong, and what can they do to improve.
If something happens, which means that we can't ship the game, like we have an incident, We usually do blameless postmortems, when the purpose isn't there to blame somebody who caused the issue, but to understand why we ended up in this situation and how we can avoid this so we're not in the same situation again in future.
And more importantly, we try to allocate time to complete the actions that we identify.
in these meetings, because what we found is if we just sit down and talk about these problems, but we don't allocate time, then people will stop coming to these meetings and they will lose faith in it.
And these sort of practices that I just explained here.
We didn't have these when we started the project.
We didn't sit down and say, these are the things we want to do.
What we did instead was we read what other people have done, talked to them, especially from other industries, and applied them to our own context, and kept the practices that made sense and helped us solve our problems.
And I fully expect, probably in a year's time, we would maybe be adopting some new practices, because we want to keep adapting to our situation.
Now I'm going to talk about challenges we've had in adopting continuous delivery.
One of our first challenges was changing our developer's mindset to adopt testing mindset.
We're coming from a place where some of our developers would argue that they know their area of code base so well that they can easily make changes without introducing new bugs.
But what we wanted to do was promote shared code ownership.
We wanted everybody to feel confident that they can go and make changes to different areas of the code base.
So therefore, we had to have some tests to catch issues if they're not familiar with our code base.
So what we did was we started with a small core team who all believed in this idea.
And we gradually added more people to the team.
And everybody who joined the team went through an onboarding process, including some test training.
We also had a separate prototype team using different code base and even different game engine.
Because what we found is, in the past, that prototype code could end up being the production code.
And by actually having separate code base, our developers had to go and re-implement that feature using a different level of code.
We also do mandatory code reviews for every check-ins.
And as part of that code review, we always check to make sure we have adequate testing when people try to make changes.
Commit stage.
Having a fast commit stage is another challenge for us.
Commit stage is the first stage of our pipeline.
And its job is to reject changes.
that would invalidate our release candidates as quickly as possible.
And here is an example of the things we do in commit stage.
We compile code.
We sync from source control system.
We compile code and cook assets and create our build artifacts.
And we run a small set of tests to prove that game largely still works.
And then we use these artifacts that are generated for later stages of our pipeline.
You have to bear in mind that just because commit stage pass doesn't mean that we can ship the game, because we have other tests, like as I mentioned, like performance tests that we run later on, that give us better indication.
And our developers wait for when they commit their change for the commit stage to pass before they start the new work, go home, go to meeting, or go to lunch, because we expect them to deal with it if anything goes wrong.
And here are some of the lessons we've learned in trying to deal with having a fast commit stage.
We need to continuously improve.
We made a mistake in this project to begin with, where we spent some time trying to improve the commit stage, and it reached an acceptable level.
Then we stopped.
We moved on to do some other work.
But what we found was, after a couple of months, we were back to where we were before.
So we tried to continuously improve commit stage.
Because as people add more code, add more tests, and more content, everything takes longer.
Sometimes, some of our optimization can lead to having more complicated systems.
So every time we try to look at an optimization in a solution, we assess it to see does it improve the complexity or reduces the complexity of our system.
And generally, we go in favor of the solutions that reduce the complexity of our system.
And we want to monitor the stability and confidence in the build as we optimize it, because we don't want to have a fast commit stage that people don't really trust, because we stopped running tests because they were taking too long to run.
And we try to identify and stop doing unnecessary work, as opposed to trying to do those unnecessary work quickly.
And here are some specific things that make commit stage long.
As I'm sure many of you have experienced, compiling large C++ codebase takes a very long time.
So we use distributed build systems where it distributes all the compiled tasks on different machines and then bring the results back and link on the machine.
We work really hard to make sure we can reliably use incremental build, and this goes back to what I was saying earlier about stop doing unnecessary work.
Let's say in a build, if you only change 5% of our codebase, We don't need to go and compile that 95% of the code if they haven't changed.
We should be able to rely on incremental build and only compile that 5% of our code that's changed.
We have a, relatively speaking, huge build farm, about 150 physical PCs that help with this.
They have two CPUs with 64 to 128 gigabytes of memory with NVMe drives.
As much as we like, we cannot afford to run all of those 40,000 tests that we have.
So we need to make sure we identify and prioritize the tests that are most valuable for us.
And to begin with, what we did was we prioritized running tests that were failing more often in our commit stage and stopped running tests that were not failing.
And that was a good start, but then there were some problems with this.
Let's say an area of code base has been stapled and nobody's been changing it.
So we stopped running those tests.
But as soon as somebody went and made some changes to that area, now they didn't have any coverage in our commit stage.
And they would find out later on when we run the test at later stages.
So the feedback loop was too long.
So instead, what we did is we developed another system that runs each test and creates a map to see which code files it covers.
And then when we try to run the commit stage on a build, it looks at the changes in that build.
and see which files are there, look at this map, and find the test that covers those code files and only runs those tests.
We found that that gives us much better confidence in the tests that we run.
Game packages are quite big these days, between 100 to 150 gigabytes.
And we may not need the whole game package to run some of our unit tests, but still there are tests that we run in our pipeline that requires the whole game package.
So having just fast network connection to transfer this helps.
Our build agents have local cache to store these build artifacts.
Let's say an agent downloads a build artifact to run some tests.
At one stage of our pipeline, and later on, it tries to run another set of tests on the same build.
As opposed to have to go and download the build again, they can use their local cache.
And our build agents can serve artifacts between each other, which helps from all the agents going to the server to download the build.
Flaky tests has been another big problem for us, especially when we started.
we were not sure how to write good tests.
So a lot of tests that we wrote at the beginning end up being flaky.
And here is the problem with having flaky tests.
Because when we have flaky tests, they infect our automated tests.
And when they fail, you're not sure if it's because of a change, a break you introduced, or because of flakiness.
And what's even worse, it can also fail other people's build.
Which means that then developers lose confidence in our system.
And therefore, they are not feeling comfortable when making changes, which means that eventually, they are slower to develop new features.
And here are some of the lessons we've learned in dealing with flaky tests.
Actually, having fewer deterministic tests is much better than having lots of tests that are flaky.
And we try to work hard to make sure these flaky tests don't infect our pipeline.
And we try to quarantine them.
And we either try to fix them and move them out of quarantine, or we just go and delete them if they're not valuable and they don't provide coverage for us.
Here are some of the common causes of having flaky tests for us.
Using random weights, let's say sleep for five seconds.
As an example, let's say the service starts as a test.
It's waiting for a client to launch.
So it waits for five seconds.
Everything might be fine at the beginning.
But as you add more content, the client takes longer to boot, which means that when it gets close to that five seconds, sometimes you might fail or pass.
Because sometimes it might just go over five seconds.
And sometimes it's not.
And tests that are not isolated.
They can fail depending on what tests ran before them.
Let's say if a test relies on some state to be set, but it doesn't set them, depending on what tests ran before, those other tests might be overriding those states.
So these tests might randomly fail or pass depending on what other tests ran before them.
And for some of our tests that we rely on external dependencies, like some remote services, they can just fail if that remote service goes down without actually us introducing any issue.
So another challenge here is how much time we should allocate for improvement.
We have three types of work.
The first category is planned work or the work that we do, that we plan to do, like, let's say develop features.
And that's the kind of work that, let's say, business and production wants us to spend most of our time.
But the reality is that we also have to do some unplanned work.
Let's say fix bugs.
If a game crashes, there are serious problems that we have to go and deal with that prevents us from actually developing feature.
The third category is the improvement work.
And these are the work that would address the root cause of those bugs or the incidents that we've had.
And there is no rule about how much time we should spend for each category, but there is a very good feedback mechanism there.
If we notice that we spend too much time doing unplanned work, it means that we are probably not spending enough time doing improvement work, which indicates that we need to spend more time doing improvement work.
And here are some of the challenges we've had in releasing weekly with confidence.
As I mentioned, game packages are quite big.
So we want to make sure every time we release, our patch sizes are as small as possible.
To make our life simpler, we only allow one version of the game that everybody use at any given time.
We don't allow different versions of the game to be live.
Which means that every time somebody wants to play and we have released an update, they have to go and update.
And if that update is, let's say, 20 gigabytes, people would soon stop playing our game.
The other challenge is delivering new features regularly.
Because traditionally, like let's say designers, they want to only release the feature to players when the whole experience is complete.
But that might take, let's say, three to six months sometimes for a big feature.
So it requires a change in mindset to say, how can I break this experience that might take three months into smaller chunks that we can still release, ideally weekly, to our players, but still is the full experience when it's completed.
As much as we like to, we cannot prevent issues going out to our players, because games are complex systems.
Therefore, we try to minimize the impact of something when something goes wrong.
As an example, let's say one of our services is down, which means players cannot buy or sell items in the game.
We don't want the game to crash every time somebody goes into a shop to, let's say, buy or sell items.
It's much better if game detects that and, let's say, puts a sign on the shop to say shopkeeper is away, come back later.
And it removes that sign when the service is back online.
And the other thing is, we try to identify these issues as quickly as possible, so then we can, thanks to having fast cycle time, we can respond quickly to them.
And finally, the certification process.
Platform holders like Microsoft, they have a certification process and certification team.
Traditionally for us, that process could take one week, sometimes up to two weeks.
So our release frequency is bound by that.
So for this project, because we wanted to release regularly, we started a lot of conversation with the certification team.
And we did a great collaboration with them, where they evolved their process.
And as opposed to, again, trying to prevent these issues from going out.
they trusted us that we can do it ourselves.
And instead, they agreed to monitor some key metrics for us in retail.
And as long as those key metrics, let's say crash rate as an example, was below a certain level, they trusted us to keep releasing.
And the agreement was if they find a problem, suddenly let's say crash rate increases significantly, Then they can jump in and have a conversation to understand what has gone wrong.
And we got to a point right now, from this point, that we have a build.
Until it's available for our players to play, it's about six hours.
That's much improvement from what we had before.
That was a week, sometimes two weeks.
And here are some of the takeaways from my talk.
We try to work in small batches to reduce the risk.
of releasing regularly, because we want to make sure we can release regularly and safely so we can learn fast, and therefore have better quality game that is more fun for our players.
As much as we like to, we cannot prevent issues.
So we are prepared for something to go wrong, but instead, we work really hard to make sure we can respond really quickly to these incidents.
And we try to continuously improve.
So every day we are better than the day before.
And we are not done with continuous delivery by no means.
We've been on this journey for four or five years, and there are still room for much improvements.
So we're going to carry on improving.
Before I go, I wanted to talk about two books that have been very influential on me and my friends at Rare.
Continuous Delivery book by James Humble and Dave Farley.
It's a great book that goes through much more detail about the principles that I talked about here.
And The Goal by Goldratt, which talks about theory of constraint.
It's an amazing book if you're working on improving your cycle time and optimizing your pipeline by reducing bottlenecks.
The authors of Continuous Delivery Book have a test, which they say, if you can ship your product to your audience by push of a button when you are on the beach having a cocktail.
It's a good indication that you're doing a good job.
Well, we released the game this week.
I'm not at the beach having a cocktail, but I'm here at the conference giving you a talk.
So I don't think we could have done this if you were not practicing these continuous delivery practices.
So thank you for your time and listening to me.
Does anybody have any questions?
Hi.
You mentioned about bringing your small core team on board and starting bringing more people.
How long did that process take in terms of timeline, and what was the size of the team you started with and growing?
I think it was about four or five developers when we started, and then we gradually added more.
I cannot remember exactly how long it took, but maybe after the first month, then we brought new developers, but it was gradual.
It wasn't sudden like sudden job.
Hi, I've got a question about the automated testing that you're doing.
Were you doing test-driven development where you're writing the tests first, or were you retrospectively adding the tests to new features?
And a subsequent question, for legacy systems, how did you implement tests for them?
Did you ensure coverage for them, or did you just leave them as is?
So I think very few people did test-driven development.
Not everybody did that, but the important thing was when they were trying to check in their changes, we made sure there was some tests there.
The important thing for us was that when somebody added some change, there was some accompanying tests for that.
There are some other benefits with doing test-driven development, which leads to more modular code, smaller functions that are testable.
So it has other benefits there, but we didn't mandate that.
We left them up to developers, and I think very few did that.
And your second question, could you repeat that again?
About legacy systems, were you testing legacy systems in the same way as new features?
No.
So for the area especially that we were not touching, we didn't bother adding any tests because we couldn't.
And we were using Unreal, which is a huge legacy code.
So we didn't try to go and add a lot of tests for Unreal code.
Thank you.
Hey, yeah, fabulous talk. Thank you. I could ask a hundred questions, but I'm specifically and you mentioned You've done a lot of work to make sure that you could trust incremental builds That's right. You could talk a little bit about what work. Did you do?
because one of the problems we had because we were had about 150 agents and The state of the bill wasn't the same because if an agent wasn't used for a while then the cash could have been quite out of date or Because our developers do personal builds, which they upload their changes by our CI system to be tested before they come in.
So some of those arch files might be newer and doesn't match the code that exists there.
So we did a lot of work to make sure we're not in this situation where somebody can do a personal build, affect some arch files, and we end up using that arch file that doesn't match the source code in our game.
That was the main thing.
Did a lot of cleanup work to make sure they work reliably.
AUDIENCE MEMBER 2. Cool.
Thank you.
So when you ship a version, I suppose any time that happens, you still have features, some other features still in progress, which you turn off with your toggles.
Do you do anything to prevent shipping content belonging to those half-done features?
So for content, if they are not referenced by the game, because if it's at the very early stage that is just being prototyped.
They're not referenced by the games, therefore they're not being shipped.
But if they start to be referenced in the game, we would ship them.
So that's one of the things that we can improve.
Because essentially, yes, we are shipping some content that are not being referenced by the game yet, or used by the game.
So do you have some mechanism, if you turn a feature off, then it's no longer referenced?
Do you detect?
No, no, we don't have that yet, no.
Right.
But that's something that we can definitely do.
OK.
Thank you.
You're welcome.
Hi.
Hello.
How do you deal with version and content mismatches between client and server when you have a bunch of content creators and a bunch of client developers, a bunch of server developers all trying to work on their own boxes with their own features?
Do you have a shared environment that everyone is hitting?
Or is everyone setting up the services on their own boxes locally?
So everybody work, like we don't keep multiple versions, because everybody work on the trunk or main, we just have the same version.
So if we, let's say, change an asset format, everybody who update will get that version.
Does that answer your question?
No, like, say I'm making that change, right?
Sure.
And I want to manually test it, just like quick, smoke test it without having to like figure out exactly how I'm going to like automated test it.
Is there a way I can set up the game and services end to end so I can do that?
Yes, so we have we developed some tools that allow to improve the local workflow.
that you can just press a button that compiles this client, the server, set up the services, and cook the assets.
So then, yes, you can then ideally even deploy to your Xbox and test it on your Xbox or your PC.
Yes, we developed a tool to allow us to do that.
Because manually doing it was quite difficult.
One quick question.
How many people are currently working in this one branch depot?
I think probably about 150 people.
Wow.
How do you deal with the problem of the build's been broken and now it's unlocked and everybody just dumps all their stuff in?
So what we found is the longer it's locked, the more the chance of people dumping a lot of things.
So that's why we try to back out the change.
Because if somebody tried to come up with a fix, it would take a lot longer.
So we try to usually deal with the build break within 10 minutes or so.
So we don't keep the depot locked for a long period of time.
Great, thanks.
Thank you.
Hey, I wonder when do make code review before integrating commit to trunk or after so yes It is before before we do that so we do the code review if it passes then you're allowed to check in your changes to your truck How do you deal with the Multiplicative like complexity of different feature toggles so say there's five features that could be on and off are you testing all the different configurations of those?
Or do you just, you don't know the test pass for it until a month later when you do finally turn it on?
That's a good question.
So what we tend to do, first of all, we try to keep that set as small as possible.
So if a feature is done, we try to remove the toggle so it's always on.
But also we treat the whole set as one set, because it makes the testing a lot easier.
So if, let's say.
We make a bill and there's a problem with one of the features that we toggled on.
You just switch off all of the features.
Because otherwise testing them would be very difficult.
The combination could be.
I mean, the test matrix would be very complicated very soon.
So yes, we just to again to make our lives easier.
We just treat them as a set that we switch all of them off.
I see. Cool. Thank you.
I thank you for the informative talk.
Thank you.
I had a question on the unit testing.
Can you elaborate further as far as, are your developers bringing to the table initially with the start of a project their own set of unit tests?
Are they sharing or propagating unit tests across the team, especially if there's one where it's found to be a key metric or a key set of tasks that they're evaluating that could be shared?
I guess I'm looking for more information as far as the continuous development improvements and process improvements.
If that didn't make sense.
I'll try to rephrase myself, I apologize.
It's okay.
Basically, your developers, especially if they're newer ones on the team, they're focusing on unit testing.
Are they bringing to the table their own set of unit tests and can those be shared?
Or are they developing them on the fly?
Yes, they develop them as they work on a code, yes.
OK.
But as I said, we had this problem with flaky tests.
And the main reason was because when we started, we didn't know how to write good tests.
So a lot of these unit tests that we wrote were flaky.
But people learned from that and improved.
Therefore, the tests that they added later on were much more deterministic.
OK, thank you.
You're welcome.
Hi, good talk.
Because you have 150 developers on head, When you're refactoring for technical debt, a ubiquitous feature that's ubiquitous across the code base, touching lots of files, interacting with lots of other systems, did you run into any issues or have any tips on dealing with changes that need to be made to reduce technical debt that actually affect a lot of developers because it affects a lot of systems?
Yeah, so that was one of the problems we had as well.
Fortunately, not many people had to do that, so the number of people had to deal with this was small.
The trick was, again, using feature toggles.
As opposed to, let's say, editing.
tens or hundreds of files and submit them together, how they can ideally try to use feature toggles to hide some of their stuff.
So they don't suddenly have to commit all of them together.
It goes back to what I was saying about breaking down the work into smaller increments so they can make a few changes, submit that, then make another change, submit that, and touch a smaller number of files as opposed to going quite big.
But sometimes you can't avoid that.
Sometimes you have to do that, and it's very painful, but we try to minimize that as much as possible.
Well, sorry, I don't have any specific kind of hint, other than try to minimize that as much as you can.
Thank you.
Hello?
Hi.
Where does Data Worker fit into all that?
Sorry?
Like, how do you manage your data?
Because this is very code-oriented.
Yes, sorry.
So that's in the same trunk in main.
So all our data, everything is there as well.
But your, for example, your, uh, I don't know, your audio designer, uh, they can't really code themselves automatic testing. So who does it for them?
No, they write some tests.
Really?
Yes.
Our Sun Sound guys, they write some tests as well.
But there's a difference, because you have the source sound and then this being converted that we use in the game.
They don't need to write tests for the source sounds.
It's only for the when it, they can easily work on the source file as much as they like without having to add any tests.
But when they try to hook it into the game, that's when they need to put some tests to it.
OK, so for example, for your level design and your, I don't know, VFX, your customization.
So they all write their own tests to make sure that nothing's broken?
Yeah.
But obviously, the tests that they write is at a different level to what programmers write.
And sometimes the programmers have to go and help these guys to write tests, or ask programmers to write a test for them.
But we encourage our artists to write some tests if they can.
And also, they try to do code reviews as well.
It's quite fun for artists.
Thank you.
You're welcome.
Hi.
When you said that you were toggling these switches on and off to enable code, did you mention if that is its own, like you have a pull request and a code review on that change?
Or is there like an out-of-band way that you update those switches?
No, it's part of the same change.
Let's say you want to toggle a feature on, but it needs some small pieces left.
You complete it, and then it allows you to toggle the feature on.
So you include that change along with the toggle along with your change, your last change.
And you submit both of them together and it gets reviewed.
Because sometimes these toggles, when you want to turn them on, you usually have to check with other people from other disciplines as well and make sure they sign off.
Because like potentially you releasing this feature now to your players. So you want to make sure that this a production signed off on that as well. But usually be encouraged to like the last change that enables us now releases toggle feature to a players to 10 to talk along along with that change together to make that change atomic awesome. If anything goes wrong, then we can back out and the talk all this office. Oh yeah, great thanks.
hey, for your targeted testing, so you said you had 40,000 tests, but you don't run the entire test suite, when you're doing your targeted testing, was that based on code coverage and was that a hand curated map or was that, you used a code coverage tool to then map that in?
So we used to use a code coverage tool.
And when we stopped using that, so we came up, we wrote our own tool, which, as I said, it runs each test.
Basically, once a day, it runs all the tests, each test, and then tracks which code files it touches.
So it's all automated.
We don't do that manually, because it's going to take a long time to create that map and maintain that.
And that was all internally developed by you guys?
Yes, that's right.
You guys are way too fun.
Thank you.
So what if you wanted to upgrade the Unreal Engine?
Could you do that?
Did you already do that?
Yes, we did a couple of times.
But then at some point we stopped because it was taking far too long to integrate a new version of Engine.
But because we had these tests, again, they were helping us, because otherwise it would have been very difficult to integrate a new version and try to release.
We got to a point where in the same week that we integrated a new version of Engine, we also released the game a couple of days later to our internal audience.
But it was thanks to these tests.
Otherwise, it would have been very difficult to test them.
Right.
So your test kind of covered most of that?
It covered the area that we were modifying and we were changing.
I mean, it was quite common that we take a new version and then something would be broken.
But again, we were trying to detect them as early as possible and trying to respond to those issues.
All right.
So you feel it's possible to do that, that's doable?
Yes.
OK, cool.
Hey, great talk. Two questions. Firstly, does this involve both the client and the whole back end part of the game? Okay, cool. And secondly, do the same teams, is there one team that implements on both the client and the back end? So they write tests for everything and integrate? Cool, thanks. Hi, when a test gets broken, who fixes it? I'm not sure. I'm not the tests that are broken, the person who caused the break.
So as I said, we try to keep the number of changes in a build as small as possible.
So it should be really easy to identify if you look at a couple of changes in there.
you should be able to easily identify who broke it and then you ask the person.
Okay, and do you have any dedicated people to basically take care of the automated tests, the framework, is there a dedicated team for that or is it the responsibility?
For the test framework.
Well, the framework and the tests themselves, do you have any dedicated?
So tests, the test framework, tools and everything, they are in the same depot, in the same main branch.
And the deployment pipeline team are responsible for writing the test framework.
But at the same time, we don't write the tests.
Our developers write the tests because they want to prove that their feature works.
Otherwise, I could do it manually, and they don't want to do that manually.
OK.
So there's no one that's actually dedicated to working exclusively only on writing tests?
On fixing tests, no.
Or writing tests, no.
OK, thanks.
You're welcome.
Hey, at one point you had said that prototyping is critical to effective testing.
Could you elaborate a little bit more on what you mean by prototyping?
I'm not sure you recall what I mean.
So I don't recall the specific instance, but there is something that came that came up where you said there was prototyping was important. Might have been when you were bringing team members on.
Let me see, I think you probably.
Let me see is it.
You mean the third point there, which having a separate prototype. Yeah, yeah, right there so.
So what as I mentioned so the what we found in the past was when we are working on prototype the quality level is quite different for production.
Code.
But you don't have a right test if you put a lot of things because you want to quickly test stuff and I see if that feels right and that kind of stuff so we didn't want them to kind of put that level of quality for prototype so having separate code base and using even different game engine.
It allowed them to.
work on that and kind of be isolated.
But at the same time, what we found in the past was that could easily end up being the version we use in production.
Because it might be difficult to justify, because somebody might say, oh, but this works, kind of works, why don't we use that?
So having this separation, they had no choice.
They had to go and re-implement that feature that they just prototyped in the production code with tests and everything.
Great, that makes sense. Thanks.
Does that answer your question? Yeah. Cool.
That was a great talk. Thank you.
I was thinking in terms of the commits themselves, presumably each commit is from a single developer?
That's right.
So how much, how weighty is each commit?
Because frequently you'd use feature branches for working on something for maybe a couple of days.
Do you see individual commits having that much work in or are they smaller?
No, they are quite small, because we encourage our developers to do check-ins every day.
Not everybody does it.
Some people do, but some people don't.
On average, I think we do about three to four check-ins each.
Like our developers do three to four check-ins a week, which means if they're not doing it every day, they do it every other day.
And therefore, then the number of changes, like files they have, is relatively small.
And they don't use another feature branch or anything.
Everybody work on trunk.
Right, and when you code review on one of those commits, presumably that's obviously in line with development.
So I would do a commit, someone is there ready to review it pretty much straight away.
So they do the review before they commit.
And again, one thing we found is the smaller the change, easier for people to review.
Because if you try to review and it's small, it's a lot easier for people to reason about and review that.
Because we had often, sometimes we had this problem where If the change was too big, the person reviewing it will send it back and ask the person to break it into smaller chunks.
So they review different pieces, as opposed to having this one big change that they want to review.
Right.
So does the review process have much latency to that commit process?
Yeah.
Right.
So that's one problem we had.
And one way to tackle that was to encourage developers to have smaller changes.
Because more people are willing to review smaller changes than bigger changes.
So it helped, definitely helped.
Great, thank you.
You're welcome.
First, thanks for the talk.
Great talk and excellent discussion from the group here.
I'm going to be selfish.
I have two questions.
Sure, we have four or five minutes.
OK.
First, for the delivery pipeline team, Is it a single team supporting all your programs and your entire studio?
Yes, that's right.
Okay, excellent.
And then, second question.
When you and the team at Rare first decided to stand up this team and kind of restructure your approach to development, how did you, the engineering team and leadership, kind of build a business case to get buy-in from the entire studio.
So our software director also was quite supportive in this, and the leadership team.
But at the same time, because we were changing the business model, and we knew that we have to sustainably deliver new features for a long period of time, we couldn't really rely on manual testing.
The type of game that we have, we can't really manually test everything.
So, these were the main reasons that we had for adopting testing specifically, because we couldn't really rely, otherwise we had to I don't know hire how many testers for 4 years, which we didn't want to do.
Excellent, thank you.
You're welcome.
Hello?
Hi, great talk.
Thank you.
I have a quick question that's sort of tangential to the talk here.
You mentioned briefly that you would release toggles to the world with a small set of players having them turned on.
Yes.
Can you discuss a little bit of detail about how friends playing together deal with these different toggles?
So people with different toggles, we don't let them to play together, because the server will reject the clients that have different versions.
And the way we implemented it is we actually used this concept of title IDs for Xbox games.
And actually, we had two title IDs.
One title ID for these features that we wanted to give to a small number of players.
And then we used the other title ID that goes to a wider audience, which meant that actually they couldn't play with each other as well.
So in that case, end users know already that they're playing two different versions of the game.
It's not actually like an A-B test to evaluate necessarily.
So it's not like kind of transparent or anything like that.
It is actually, you need to know that you are playing this kind of almost bleeding age version or this kind of more stable version.
But again, I mentioned there are room to improvement.
That's one of the things we try to do, how we can avoid that so we don't need to keep two title IDs.
How we can do the same thing with just using one title ID.
So these are the things that we are going to work on.
So in that sense, it's a relatively limited test realm in that sort of situation?
OK, cool.
Thank you.
You're welcome.
Thank you for your time.
