OK, let the record show that I'm the very wide one.
Thanks, Mike.
Yeah, a couple more minutes.
Last session of the day.
We have kind of gotten into the pattern over the recent years of ending one of the days with a panel just to kind of, because let's face it, we've gotten an earful so far during the day.
But it isn't just a panel.
We always try to make sure that we're giving it something that makes you walk out of here thinking, OK, yeah, inspirational or something you can kind of wrap your head around and say, this is something I can take back to my team.
That isn't necessarily technical.
So that's kind of where we tend to head with this.
So this came up on the guild list of something we wanted to talk about.
I won't blow all the secrets here, but that's why we're here.
Anyway, who was here for Dave Churchill's thing in the last bit?
Complexity classes kneel before him.
So where is he?
I want to see if he's blushing.
He's probably going, oh, thank God I'm done.
Oh, there you are.
So good first day?
All right.
I do miss Jorge.
I have to admit.
That was like we have one regular who's not here this year.
There is something that he was the most enthusiastic AI summit attendee ever and he definitely could bring the decibels. That's for sure, but I'll have to check on him. Hope he's alright.
So we got quite the slate tomorrow too. So you know, hopefully we're not up against any serious competition from the other summits and tutorial. Oh no, there are no other summits and tutorials. This is the only game in town, right?
That doesn't work as it.
So, OK, well, we're still getting some people scanned in here.
So we've got about a minute.
Go ahead and, because we do have some people still coming in, if you have some empty seats in the middle of your section, go ahead and scoot in if you could, please.
Yeah, thank you.
And once again, my near ubiquitous reminder of, please, make sure you're scanned into the sessions, but make sure you fill out those evals.
Even if you just score us, that's great.
But more importantly, please write some comments, some feedback for the speakers on the sum as a whole.
And we have been pretty shameless in past years about saying, writing things like the AI summit is the reason that I come to GDC, don't lie.
But if this is really, really important to you, write that in there, because then the GDC people read that and go, oh, wow, yeah, we've got to keep this AI summit coming back.
Not that we're at a risk, I don't think.
But please put that kind of stuff in there, feedback for all of us.
It's really important for us to hear that.
All right, I think we're good.
This door is closing.
So we're going to go ahead and get things kicked off.
Again, my name is Dave Mark, your summit advisor, and more importantly for right now, the moderator of this panel.
My job in general is going to be direct traffic up here.
because the real talent is on either side of me.
A lot of experience on this subject.
And again, this kind of came from a discussion on the guild list that as game behaviors get more complex, it's a real pain in the butt to test them.
And so the consensus was, yeah, we should have a panel on this.
I could speak to that, I could speak to that, etc.
So this is who we had ending up in here.
I will let them introduce themselves.
First, Sergio Ocio.
Where'd you go?
Sergio is right here.
So, go ahead.
Yep.
So, I'm a lead AI engineer at Hangar 13.
I joined the team when we were finishing Mafia 3.
Yes, I've been in the industry for a long time, so, you know, it's very interesting talking about what we have today.
You've tested a lot of things, have you?
Yes.
Emil Johansen, how are you?
I'm good.
I'm good.
I'm good.
I'm good.
I'm good.
I'm good.
I'm good.
I'm good.
I'm good.
I'm good.
I'm good.
I'm good.
I'm good.
I'm good.
I'm good.
I'm good.
I'm good.
Yeah, I'm a contractor, so I do other people's games.
And most of that time, it's specialized in AI.
And for the past nine years, I've been doing my own middleware on that.
OK.
Mike.
Mike Lewis.
I'm the lead gameplay programmer on Guild Wars 2 at ArenaNet.
And I've been doing AI for most of my game programming career in some form or another.
And Mike Robbins.
I'm Mike Robbins.
I'm the senior software engineer at Holospark.
I've worked on games like Planetary Annihilation, Supreme Commander 2.
I got interested in AI as a modder in the community and then transitioned my way into the industry.
Which is like the coolest way to get into the, oh, it's just modding stuff.
What, you want to hire me?
Cool, right on.
Pretty much my reaction.
And again, my name is Dave Mark.
I'm the president and lead designer of Intrinsic Algorithm.
I do AI consulting for studios all over the place.
including I worked with Mike at ArenaNet on Guild Wars 2, as well as EverQuest Next.
What else have I done?
The Pixel Mage Hero Song here recently.
So I've done a number of different things.
But that is not important right now.
What's important is what we're covering here.
Briefly, to give an overview, there are a number of common problems that we have with testing.
We're going to be talking about some of those.
as well as then best practices of how to approach doing our programming, doing our testing, et cetera.
And then also tool development.
You know, what are some best practices for having tools available to make all of this easier?
So we're going to touch on some common problem things first, just to kind of set the stage of what it is that we're trying to address.
And I'm going to let each of the panelists kind of, you know, chat about what they view and believe as being the important things on each of these.
And so the first one is that just identifying problems to start with.
You know, is this broken or not?
And so what is a way of kind of encapsulating, you know, a mindset of how you have, you know, just even realize this?
What how big of a problem is just this one thing?
Well, I think a lot of it depends on the actual goal that you're trying to hit with your AI, because in some cases, you do have a specific thing that you want to have happen and play out in a certain order, but in a lot of cases, it really doesn't matter.
Like, you can totally have an agent do his own thing and come up with its own interesting plan for how to do it, and then you get this emergent result that the player thinks is pretty kick-ass, and you know, you're done.
That's how AI is successful.
when you have those cases that are absolutely black and white, you have to know in advance that we know we really needed that guy to go through the door right then because otherwise the whole plot of the game doesn't unfold or whatever. So sometimes the issue is easy and sometimes it's a little bit more nebulous and hand wavy. So okay. What are some other issues surrounding the whole idea of identifying problems?
determine whether it's...
I suppose it's what he said, like, it depends on the problem that you're trying to solve, but many times you don't really know if something's broken or not, and you know, it's very hard, it's easy for you if you're working on the thing, but it's very hard for other people to know, like QA, to know if the thing is broken or not.
So just trying to come up with some rules or some way to tell people what means, what broken means for your game, or what, that's something that is important.
As he was saying, in games like Mafia 3, where narrative is very important, things have to happen in a very specific way sometimes, but then you have other games like, I don't know, like maybe GTA or something like that, where anything can happen and as long as it looks okay or it looks plausible and not too crazy and people understand what's going on, I think it's fine.
I think one of the things that was mentioned earlier today might actually apply to this too is, I can't remember who it was, it might have been Brian, who talked about how because AI is so central, sometimes something is broken, but is it the AI?
Or is it an animation bug, or a scripting error, or a data error, or something in the environment, or whatever, I mean, so even just determining.
Is it AI that is broken is sometimes part of our job too, rather than is it some other system related.
Anything else on this or should we?
I mean just establishing expectations and having common language to discuss this.
So yeah, we'll touch on a lot of these things as we go here.
Mike, did you have anything?
I think we've covered it.
Well, the next thing then on our common problems that we kind of came up with as a group is identifying the causes.
Of course, and I just touched on this briefly myself, why is this broken?
How would you rate this as a challenge to address?
Yeah, it can be broken because, as you said, like it can be broken because the code is broken, or it can be broken because something else in the setup is broken.
Those things are harder to find, I suppose, sometimes.
You can have problems with things that are scripted, and you don't know if it's the AI running or if it's something that is scripted.
So you always have all these types of problems, I guess.
Which lends to a lot of finger pointing, doesn't it?
So anybody else?
Like Sergio said, it's trying to determine whether or not it's broken because the AI is broken, or because some other system is influencing the AI and causing it to behave.
wrong is difficult, especially when your AI is swapping back and forth between doing the AI-based behavior or doing something scripted.
It's like you're trying to determine whether or not it's the script broken or is it the AI behavior that's broken.
Or even just some of the stimuli in the world.
The AI believes that it is perfectly doing the thing it's supposed to do based on the inputs, but we just kind of sit there and go, whoa, all of the inputs are always correct, right?
So it couldn't possibly be that.
Okay.
The third common problem is the scope of the testing.
As games get bigger, this is kind of a thing.
What are some of the scope issues that you've run into, just from a problem standpoint?
Let's see, we had on the order of several hundred unique combinations of archetypes for characters.
So everything from your forest wildlife to...
pirates to ninjas, whatever else might be roaming around the game world, and then each one doing on the order of dozens of decisions that they could choose with on the order of dozens of things that they're thinking about for each potential decision.
The number of things that you have to test is ludicrous.
To put this into context, this is in Guild Wars 2.
This is in Guild Wars 2, yeah.
So you're in an MMO with hundreds of different types of characters.
Right.
At which span, it isn't like, well, I have lots of characters, but they're all human, or something like that.
So we just have a variation in all of the different base behavior, brain behavior, how they act.
So that type of scope.
What's another scope issue?
I mean, there are massive pitfalls in over and under testing both manually and automated testing.
Like, again, having a good analysis of what are the key areas and what's...
what's at stake.
I mean, that's most important for manual testing, for having that agreement.
And for automated testing, making it a useful task while still having sufficient coverage, so you don't go overboard in automated testing.
So prioritization combined with, at some point, enough is enough.
Yeah, yeah.
Otherwise, it becomes this massive big task, and people start.
ignoring it a bit or you know it becomes a hassle right.
Well yes it is a psychological factor internally in the team of you know OK I got to test absolutely everything and does this really matter.
Yeah OK.
Yeah I mean personally I think that you never really finish testing stuff like even when the game is released after release you keep playing the game and you can see all these problems and hopefully in a patch you can fix them or something but normally you don't have that.
Ability to keep releasing like fixes for your fans. So I don't know it's right, you know I I coined a phrase by accident during a GDC week when a discussion with with a colleague where I said something to the effect of asymptotically approaching infinity and He paused and he looked at me. He goes Did you really mean to say that but it that's actually in this case? It's one of those things is yes We're getting ever closer to something that's constantly moving away from us.
So you can't win.
At some point you have to just say we're done.
Well let's get on to the solutions then.
As far as the best practices for just testing to begin with, of approaching, like we talked about the common problem of is something broken, what would you say that your general approach is to testing AI?
Either you or if you were QA or as a designer.
So just a general broad approach of how do you, come at the problem.
So to me, the most important thing when you have to do when you test AI is that you are not trying to test that the thing that you worked on works, but you're trying to see if it can break.
So you are all the time trying to break your stuff, basically.
Because you basically just want to avoid having problems that people are going to put on YouTube, and your work for years is going to be in this one video that is like.
Showing bad stuff about your AI.
So, you know, trying to avoid those things is always good.
Nobody ever YouTubes the really cool shit that, you know, hey, the AI is doing exactly what it's supposed to.
I'm going to put that on YouTube.
That's not fun.
Normally, people say that when the AI in a game is good is because it's when people don't talk, doesn't talk about it.
Right.
Yeah.
We don't notice it because it looks natural.
So what are the general approach to it?
I like.
testing iteratively, as you finish a feature, test the feature, start another feature, test that feature, to lower the scope of what possibly went wrong.
Oh, OK, so I've done this, and if I test it now, if something radically changed, it's probably what I just did.
Rather than, here, I just added 18 different things.
I think it's time to test.
Oh, shit.
I have no idea where this broke because I've been working for three weeks on this.
But I was also breaking in before that I have done this, and it's not done before.
You've already started testing a lot, like integrating testing as part of the development process, like using it as an extra intelligence-like tool or an important part of your productivity setup.
You leverage the tools you have for testing while you're developing it before you mark it done.
So yeah.
There's probably a case to be made for it's not done until you've tested it.
Or at least you don't know if it is done, is a different way of looking at it.
Or Mike, did you have something?
Just to riff on that a little bit, I think the real solid approach is to start by assuming that everything you build can break.
and try to find the way to do that as early as possible and continue doing that until as late as possible.
If that's after ship and you still can ship patches, you should still be testing your stuff even if you think it's bulletproof.
Okay.
That's a good approach.
Now, as far as the, okay, I've written my code and I'm running the game.
What is in your head at that moment?
So you've decided to iteratively test, stuff like that, using some of your tools early on and everything.
What's in your head?
Okay, go.
What are you looking for?
I mean, that sounds kind of lame, doesn't it?
Well, I'm looking for shit that's wrong.
Yeah, that's the easy answer.
But you know, well, you touched a little bit on, you know, I think the first step would be, is it doing what it's supposed to?
Yes.
Oh, thank God, yes.
Yeah, but normally you tend to work in small iterations, I guess.
So when you work on something new, it's like.
this tiny thing so testing that tiny thing takes just like you know a few minutes or something but then like testing like all the possible side effects of that tiny thing or how that new thing interacts with other things and what else is broken based on that and how how can you find like edge cases to to break your new things like it's well that's the challenge in and of itself is what are the edge cases because a lot of times they're not obvious Yeah, and it all depends on what thing you're working on, I guess.
But I guess that's something that you learn from experience, like, based on, you know, once you've been working on something for so long, you know what to look for, I guess.
So always talking with other people that have experience or, you know, trying to learn that thing from other people. Like, I used to have a lead, Martin Walsh, at Toronto that was super good at breaking things.
And it's like I learned all that from him.
So every time I tried to test something, I just remember handing my stuff over, like handing the controller over to him and seeing him break my stuff.
So it's just keep learning from other people all the time.
Cool.
Yeah, I mean, as I've been iteratively developing or testing during the production of whatever bit of functionality.
That's kind of been happening in some sort of lab clean room kind of scenario.
So before adding the done label, trying to kind of force myself to put it into as weird, unexpected scenarios as possible to kind of challenge the environment it lives in, to try and wrestle out some unexpected behavior, some complicated environmental interactions, stuff like that.
OK.
Do you have anything else?
Sure.
Usually when I go through, I'm trying to think of, OK, well, if the AI does this, how is it going to change the game state?
And then how is the AI going to react to that?
Because you could put it in a new feature that breaks something else somewhere else because it's changing game state now that you weren't prepared for.
So at that point, that's, again, where the iterative thing comes in, it seems, is that, yes, I added the one feature because we're doing this iteratively.
And I can test that one feature.
But.
we're also not just trying to break that one feature, we're saying all the other stuff that I thought was perfectly bulletproof, is it now being broken just because of this one thing where you have that give and take back and forth between them.
So yeah, that's just, so as we keep adding features, it seems like it's a combinatorial explosion of shit that could go wrong.
Which is where we get back to, oh, and there's hundreds of different agents to test.
How does this whole mentality change with doing your own testing rather than other people testing your stuff or you testing other people's stuff?
Is there a different approach that you would have to have, a mentality, when you bring in other people into this process?
I think the approach fundamentally stays the same.
the benefit of bringing other people into the testing situation is that they're going to have different blind spots than you do.
Like they will think of different things that they could try.
They will think of different combinations of things to put together and see what happens.
They will have a perspective that you don't by nature of being a different person.
And that gives them a lot of power to find the weaknesses in the design that you're so used to looking at that you can't see.
Like oh hey there's this obvious shortcoming right here.
I think as AI designers and AI programmers, we tend to baby our AIs a little bit.
We don't want to see them breaking or broken or dying.
So bringing other people in that don't do that and really put the hammer to the AI is very useful.
Is there, because those are advantages to bringing in other people, what are disadvantages to having someone else testing something that you've completed, for example?
I think the biggest problem I've had is the QA team not knowing what's done, what's not. So they find a broken behavior and the answer is, well, I haven't programmed that yet. So yeah, of course it's going to break and it's not going to do what it's supposed to do.
Have you ever run into the situation where you...
just almost subconsciously know, well, this is supposed to exhibit this way, that way, this way. And so you kind of, it feels right to you, but to somebody else, they don't even know what they're looking for on a more subtle level. How do we solve that?
Well, I think it was brought up earlier, like having this shared language of, what are we actually discussing here because I could look at something and see an AI bug that someone else wouldn't notice and vice versa because we have different opinions of what we're looking at and what we're actually expecting to see in those situations.
So knowing just that you're on the same page as far as, hey, yeah, this thing is supposed to act like a predator or act like a pirate, those baseline expectations are super useful for knowing that.
When you do start picking apart the details of what's going on on screen, you're talking about the same set of issues and not something completely divergent like, you know, the animation was broken and, well, I know the animation's broken.
It kind of comes from just what is our game? What are these characters?
And we can have all sorts of great discussions about, well, who's responsible for that vision?
But it's almost a chicken-egg thing is that if it's not communicated to you from...
whoever, a designer, a creative director, whatever, of this is the way a character is supposed to be, then you're possibly going to be programming it wrong anyway.
Or if it's a data-driven system, then designers might be putting it, you know.
So now you've run into this whole, well, no, it's wrong, but I can't tell you why.
I can't explain it.
Well, this is what you told me to do.
And so, I mean, we've all been there where there's this, okay, we need to back it up, nevermind the game character.
What is?
this supposed to be? What are we trying to emulate and everything? So that's that common language that I think you've mentioned. There's also an interesting balance between making sure that like managing expectations but also trying to not go too far. So you add, because you risk basically exporting your own blinders and blind spots to the other people and you've like basically share people missing the same kind of things and striking that balance is interesting.
interesting very political. I think it's also important to have a multidisciplinary team where QA is embedded into your team and they are not just someone that it's not just a person that goes and tests your stuff it's a person that is helping you build that thing so that way it's not that you have to just write a document saying like this is what's expected from the AI, it's something that you can talk with them about it every day and if you implement something new you can go and speak with this person and say like you know I've changed this thing so you can go and have a look at these potential things that I think can happen but I'm not sure I'm sure that you'll find more than I do and we can continue talking about it so it's I think as Roxanne said this this morning like the sort of team structure they had in Watch Dogs for the civilian AI.
It's the sort of team structure that everyone should have for AI, like having designers and QA and programmers and everyone, like animators, everyone, part of the same thing.
We are all in this together, building something together.
I think just having the embedded QA in different teams, depending on what the different disciplines are at the game.
if it cuts down the number of steps where you know just somebody can say you know hey Sergio is this supposed to be happening and they're right there rather than you know having Am I even terrified of walking into the QA area or vice versa, them coming over?
It's so much easier when you sit right next to each other because you can just go and talk rather than having to go to the other side of the office or something.
And it almost inherently reduces their responsibility.
They're not testing everything in the game.
They're working with, Roxanne said, here's the civilian team versus this type of team.
or people who are building new characters for the game compared to level designers that have their own QA.
Yeah, you basically need experts for what you're building.
Yeah, so that's good.
Anybody else had experiences with that?
On the QA thing, I mean, you know, having somebody embedded into the team that you can discuss things with and letting them know beforehand, like, hey, this is what I added, this is what we're expecting, like, this is the behavior that should happen, and if this is not happening, then, you know, let me know.
You know, setting expectations for the QA team is huge.
rather than, hey, there's a new build, see if anything's screwed up.
Right.
Great.
We did AI stuff.
So at the risk of starting, well, there's two potential religious wars in this panel here.
How do you go about approaching, we touched on this a little bit, scripted AI versus emergent AI.
What are the different challenges and how would you approach it differently?
in those situations.
We mentioned a little bit of this earlier in our problem section.
How would you approach those two different sort of things?
I think the main goal is to have something that is, you know, plausible, something that looks like it could happen that way.
So either for scripted AI or for emerging AI, in the end it's kind of like the same thing.
Everything has to look like they are intelligent and that sort of thing.
What they tend to do is, yes, it's a lot of smoke and mirrors all the time, right?
So it has to be like, yeah, it looks like they know what's going on, they know what they're doing.
So to me, You have to know how to break both things, and in the end you end up using the same tricks.
The other thing, maybe, with scripted AI is that, as we were talking about before, that you don't know when things are scripted sometimes.
It's like knowing if it's actually the code that is broken, or if it's some script that is not.
it's not built properly or it's not showing behaviors that are coherent with the rest of the AI or that sort of thing.
A little bit of a retrospect for, some of you may remember we had a panel here a few years ago where the concept of the green dot, which was famous in the Guild Circles and after that lecture, where they actually, during debug stuff, they would put a green dot over the head of characters that were under scripted control.
And the first thing that that did was immediately route any complaints of, well, this is acting weird.
It's not right.
Well, did it have the green dot over it?
If it is, it was scripted.
Talk to that level designer who wrote that script for that particular situation.
And so even just that helps to mitigate that, well, is it scripted or is it emergent?
Just chopping that problem space in half of what system might be responsible for it is huge.
I very much recommend going back on the vault and watching that.
I don't even remember what the name of the panel was, but it was a few years back.
Yeah, building on Sergio's stuff, I mean, the point where this splits between scripted and emergent is in the resolution step.
Like, identifying the problem, it's the same approach.
And then, OK, now there's a problem, we need to fix it.
OK, now we can figure out it's scripted, so we diverge it over here.
And of course, in automated testing.
But other than that, the general approach should be the same.
Mike, you've done a lot of work with systems where you had both scripted and emergent AI.
And what was your, you know, was there a mentality shift of how to approach that with you?
Well, I think the hard question is really, in the moment, is this thing scripted or not?
And that's easy to solve with tricks like the green dot, which are super useful.
The emergent end of the spectrum, I think, is actually where it gets tricky, because you have to ask your designer or whoever's owning the vision of your AI, like how attached are you to this exact playbook?
Because if my AI doesn't do A and then B and then C, are we gonna have a problem, or is it cool that he decided to do some random arbitrary thing that just was flashy in the moment?
Knowing, again, it comes back to the expectations thing, knowing what you're looking at and what counts as broken is really important.
But then having that, again, comes back to the key of am I scripted right now?
You have to know which direction to go down that fork of am I taking the solution that's going to be kind of hard-coded and scripted, or am I taking a solution that has to be more systemic in the actual AI implementation?
Just dealing only with emergent AI, how do you tell if it's broken?
I mean, that's a highly subjective question by nature, I think.
The best thing you can do is say, does this look plausible?
Does this look like something that is in character for the agent in question?
And if it is, then, I mean, really, who are we to judge our agents for being themselves and being free?
Well, that got kind of weird real quick.
Power to the AI.
You know, I don't...
And Mike, you actually, you had some of the same, you know, situations when you're working with the planetary annihilation where you were like, well, I didn't tell it to do exactly that, but dammit, it looked cool and everyone was fine with it.
Is it similar sort of?
If the designers are happy with the behavior and the behavior looks reasonable and not broken, then it's not broken.
As long as it's not doing something that's completely derp stupid.
Right. If he's not running in circles or running into walls, I mean, it's fine.
If he decided to do some random off-the-wall thing, but it looked awesome, and the designers are fine with it, then run with it.
I think one way I've phrased it in the past is if the player says, Ooh, I did not expect that.
I can see why he did that.
It was a cool move, but man, I wasn't expecting it.
That's a cool moment.
So OK.
But yeah, so that's where you get into the subjectivity of is it broken or not.
And that's a difficult space.
All right, moving on.
Best practices of actually then writing code.
What can we as programmers do to write code that's that's easier to test, easier to debug, et cetera, like that.
What's, oh boy, Emil's like, I'm ready for this one.
Separation of concerns.
And do not allow any crossing of that boundary.
What do you mean by separation of concerns?
Like clearly separating out sensor systems, like data storage systems, blackboards, that sort of thing, from the various.
Decision logic code and action code, that sort of thing, have that clearly separated in a modular form.
So you can easily mock that and basically run unit tests on every level.
So you're not just left with some massive integration test.
And yeah.
Helps you to be able to compartmentalize.
Yeah, harsh down what could be wrong.
And yeah, I mean, mocking stuff up, like quickly replacing the sensory inputs and saying, OK, so this scenario right here, given this world state, but with these tweaks, let's see what the AI does and make sure it adheres to its personality, stays in character.
What else?
I think.
I think you should try to keep solutions as simple as possible.
Have code that is readable and that people don't try to do like very fancy stuff.
Because we are engineers and we look like fancy, you know, like super architected stuff.
I don't think that's the right way to go for AI, for character AI at least.
It's probably better to have a simpler solution that then if you have problems with it's going to be easy to understand, easy to debug.
And not only by you, but by anyone else in the team.
As soon as you start using a lot of templates and stuff like that, it's like, who knows?
It becomes more complicated.
And try to keep it simple, I would say.
Behaviors are complex enough that don't make the situation worse by writing all sorts of crazy stuff.
And especially modularity is part of that too.
And when you're writing that for testing, you just gain so much from what you can reuse in the development of it afterwards.
Cool.
Mike, do you have anything?
Either Mike?
Clear ownership, I think, is important.
What module owns what?
What module is responsible for the lifetime of these objects?
Because if you're passing them around, I mean, it should be clearly.
in the code like okay I have a platoon manager it creates platoons those platoons go out and do behaviors the platoon manager owns the platoons it doesn't matter if something else needs a pointer to that or whatever they can have a weak pointer it doesn't matter the lifetime has to be owned by a single object. When you first said you know clear ownership my thing jumped in my head too is clear ownership from a programmer standpoint you know because there's a lot of times there's you know we're working finally at the point where a lot of teams have more than one AI programmer.
And or but even you know gameplay server programmers and stuff like that of just saying this is the stuff that I'm responsible for.
Yeah you can tap into it and here's your interfaces and stuff like that.
But that was first thing that jumped into my head.
But yeah I liked what you're talking about about the ownership of different systems you know in the AI.
I've never had another AI program so it's always been my fault.
For the record it's always been Mike Robbins fault.
So.
So I mean, I listened to this conversation, and I can't help but go back to what kind of my roots in computer engineering and think, you know, what would someone who has been doing this outside of games for 20 years think of the conversations we're having now in the game industry about how to write testable code?
I think it's kind of interesting, we'll go with that word, because there's not a lot of.
There's not a lot of awareness, I think, in the game industry, or at least people act like they don't have time to go do all these things that make your code testable.
And this is stuff that's been like common wisdom, body of knowledge in the industry mainstream for over like two decades.
And we're just now starting to get up to this idea that, hey, maybe if we can test our code, it'll suck less.
We're growing up.
That's an excellent point.
Moving on to the next one, here's our second potential religious war.
What architectures of the primary types, like your behavior tree, state machines, utility, more of the black boxy neural network type stuff, what architectures lend themselves easier to testing and debugging, and maybe a little bit of why, some of the pros and cons of things you would run into with that?
Anything that's not a black box.
Yeah, well, that's a good start.
Yeah.
Why is this broken?
I don't know.
To me it would be anything that you can visualize.
So behavior trees, you can always build tools to be able to see like what the tree is doing or steam machines you can know like we are in this state now or with, you know, that sort of thing.
Anything that you can build tools for and then kind of like have a more visual representation of what's happening inside.
Well, it's just more easy to wrap your head around the fact that it is in this state, or it is in this branch, even without visualization, you can mentally know.
And so therefore, this is the only stuff that should be available.
Yes.
Yeah, okay.
Non-planner stuff.
Non-planner stuff. Why?
Because, I mean, stuff where you have easy, clear expectations, like a state machine or a behavior tree, or, yeah.
Oh, and this kind of goes back to the scripted versus emergent thing too, is if it, you know, who was it, it was in the last session was talking about, well, you can back flip and throw the grenade off of four walls, and yeah, hey, it figured out all that stuff.
Got there.
Yeah, but you're going, was that supposed to happen?
So, but of course you can trump that with, but it looked cool, so therefore it's fine.
So now we're back into this whole, well, which is right and which is wrong?
So what else, any other?
I think it really depends an awful lot on the people you're working with, much more than the technology you're working with, because certain teams have reflections of how they think in their projects.
And I think every team does that to some extent.
And the more you have one mentality about how your AI is supposed to function, the harder it is to get away from that and decouple from that and look at your own blind spots and say, OK, now we've got to go find out what we forgot.
You know, there's a lot of impact between the way you organize your company and your team and even the sub-teams within that group and the actual technology that you produce.
And so if you're building something that's fundamentally incompatible with the way your team thinks, you're just gonna have a bad time.
Like there's no two ways around it.
It's gonna end up being awkward and uncomfortable.
So if you can identify like just paradigms general modes of thought that your team spends a lot of time with.
That can be incredibly useful for going back and saying, okay now we're going to we're going to try this architecture that happens to align very well with the way that we think.
And so that's an interesting balance consideration in between choosing the architecture that matches the needs of the game.
You know, so yeah, there can be a little bit of friction there.
Well, you guys tend to think like this, however the game...
system kind of requires us to have an AI like that.
This goes beyond testing.
This is like, well, shoot, so you have to have some bit of a transition and retraining and stuff like that of getting the team to think.
And I think ultimately, if our goal is to serve the game, serve the purposes of the game, we might have to change the way we approach things, whether it be testing or just the architecture to begin with.
So yeah, that's an interesting, you never think about, well, I think in terms of behavior trees or scoring everything in my life as utility or something.
Not many of us think in terms of planners, I don't believe.
So yeah, that's an interesting consideration there.
Anything else on just in general on best practices for coding and programming and our work disciplines as programmers?
Well, we brought up modularizing the AI as much as possible.
That also lends itself to easier time testing and easier time debugging because if your problem is in one part of AI, well, most likely your problem is in that specific module, not somewhere in this entire mess of AI code.
Is there a way that you would then be able to firewall off, okay, well I'm going to just continue to work under the premise that these modules are perfectly okay, I can trust everything coming out of them so that I can focus.
on something in this particular module?
How would you go about doing that just in a general sense?
Well, we've already mentioned iteratively testing as you're putting things together, so that you can already say, well, I've already tested this particular subset of code.
So we know that that's good.
And the inputs, outputs, all of that stuff is what I expect and hasn't changed in the last three months.
So it's probably not that.
So using your example, like the tactical manager, for example, and let's say it's giving orders to individual units.
You know the tactical manager is spitting out, you know, it's formulating good plans for tactics, and it's sending good, but you see units running all over the damn place in places you're not expecting.
You could assume, no, the plan, the tactical plan and the orders coming out of it is fine.
So it must be something in the execution, maybe something in your navigation system got horked up, or in passing the tokens of here's your command.
Yeah, or the information that's being passed in.
Maybe your influence map is wrong.
Maybe that code is wrong.
Maybe something like you were talking about earlier.
Oh, on the other side of the tactical manager.
OK, yeah.
So it's just doing, well, this is the knowledge that I know.
It's doing the right thing, but just using the wrong thing.
So at that point, and I hate to use the term because we've already maligned it, you've created a black box that you know you're comfortable with what's happening in that based on the inputs and the outputs.
And you've kind of sealed that off as this is OK.
So.
Is there a danger to that?
Well, yeah, because the bug could be there.
And if you assume that it's not there, you could spend hours trying to track it down somewhere else, only to find out that, oh, well, I mean, this code was not as perfect as it ought to be.
After all it was, yeah.
OK, excellent.
Anybody else?
Just general principles of being an AI programmer to make it easier on yourself or others?
Ultimately, you are not alone.
So you are in a team with other people and things like.
getting code reviews and have other people look at your code before you check in or if you have two possible options and you're not sure about one or the other, you can go to other people in your team and try to get like ideas for other people.
You know what I found is interesting about that is it doesn't have to be a lead or somebody above you or somebody with more experience. A lot of times even just getting another pair of eyeballs, even if it's a junior engineer or something.
Makes it makes a huge difference. So OK yeah, no everyone like even if you are super senior You can always learn something from from anyone in the team like even from Someone that just you in your team, and it's super junior. You can still learn a lot from him so as a practical example for Behavior tree setup I would generally unit test actions and integration test trees until the point where a certain subtree is reused in a lot of different systems and a lot of different other trees, then I would have a set of unit tests for that as well.
So it's kind of cornered off as a safe thing, and it becomes another...
Safe with an asterisk.
It's assumed safe until it's not.
For various values of safe.
It becomes a blind spot.
Okay.
Well, let's move on then.
What tools could we...
provide ourselves as programmers, for right now, just for us.
You kind of touched a little bit on some of them here, but let's put a finer point on it.
What are some things that you use, either for yourself or that are off the shelf?
I mean, one of my primary priorities is to make sure that the authoring environment and the debug inside environment is as much as possible the same.
So you're looking, when you're looking at the execution or the state of a running AI, you're using the same visualization and the same window, whatever, as your authoring environment.
So it's an easy one-to-one switch over there.
In some cases that works.
In other cases that may actually be difficult.
You need to have it in game rather than in the authoring environment.
Well, you did a lot of work, obviously, with Unity where some of those things are a lot easier because you can have stuff in the inspector window right there and being able to write your custom things in the inspector.
So it's finding the granularity and the visualization.
It's an interesting challenge. Sure. Sure. What else? What are some tools?
To me, the most important thing is debug graphics.
That's having debug graphics for everything that you work on.
Like every time you add something new, you just add debug graphics to it that can help you understand what your system is doing internally.
And you can also...
show it to other people and other people can understand what you were trying to do with it.
And also, you know, tools is always...
getting tool support is complicated sometimes, because you know it's not only tools for AI, it's tools for mostly content creators, so if you need a tools team to be able to work on tools for you maybe you won't have a lot of... you won't get a lot of support, and if you want to do it yourself.
We are AI programmers, not tool programmers sometimes.
So if you have to build this thing that interacts with the rest of the pipeline or whatever, it's probably too complicated for us, or it's going to take us too long to do it.
And it's amazing how something as simple as drawing a line of what's my line of sight, who's my current target.
And if you have no familiarity with the rendering engine, where's the code to draw a yellow line on the screen, please?
Whereas somebody who's in the graphics engine people, the rendering people, oh yeah, that's right here.
Oh, thank you.
What do you mean you're not gonna do it for me?
So what you're saying is rather than, hey, I've coded a bunch of stuff, and it's almost doing it iteratively along with that is, I wrote a feature that, I'm now paying, doing the pathfinding, just something simple.
I want to be able to draw it in the world to make sure that that path is legit rather than, and is he walking where he's supposed to be walking?
I want to draw it in the world.
So I'll give you an example.
So we had a sniper in Splinter Cell Blacklist.
And basically the sniper was going to use his sniper rifle if you were far enough.
And if you got closer to him, he would switch to a pistol and become like a regular guy.
And then if you got away again, like he will switch back to the thing, right?
Oh, I see where this is headed.
So just drawing those little like circles around him, so people understood like why he was changing the weapon, that was super helpful.
And please tell me that that wasn't the exact same range, that you could sit there and just do this back and forth and he would keep switching weapons, right?
No, yeah, so things like that you would catch by doing this, right?
Yeah, but if you can see the two ranges, now you understand, oh, once he gets outside of that or inside of this other one, yeah, okay.
And then let's say that you add something else on top of the range, right?
Like it's not, when he has the pistol, it's not only you are far, but it's also that he's not engaged with you in combat or something.
So you can add something else like a little ball on top of him that says like now I'm in combat with you, so I'm fighting with you.
So this is where we get to the iterative thing where and now that we've added this extra rule, I need a way to be able to see that.
Yeah, so you know that it's clearly, you know, the radius is showing me that I'm far enough, so why is he not switching the weapon?
But now I know that he has something else on his head, so maybe that's why.
So adding a lot of that is very helpful.
I think my favorite trick for debugging code in general is just a state capture.
I want a way to be able to at a moment in time just dump out everything that the program has in it in terms of what it believes about the world.
And you can learn a lot about your code by comparing a state capture and your expectations with the way that the debugger actually steps through the code.
You'll find that it takes different branches than you might have expected it to, or loops a little longer than you thought it should or whatever.
And this is a great way to discover your own blind spots and kind of do some of that iterative testing as you're going.
And you just dump them to like a log file, for example?
I mean, you can get as fancy as you want.
It really doesn't matter, I think.
What you have available at your studio is probably fine.
And if you can stomach using it, then just do it.
And if not, go write something better.
Anybody else good for tools for us?
Anything?
I think my favorite trick that I stole from our navigation engineer.
was the ability to tag an agent so that you could set a breakpoint in code so that when that specific agent entered that code path, the breakpoint would hit.
Oh, yeah, because if you're in a world where you've got dozens or hundreds of characters, and it's like, okay, so break when this value is greater than two, not him, not him, not that one either, not, you're sitting there just looking at one box.
If wait until that GUID ends in 4832, Oh, yeah.
So I like that, because then that narrows.
So how do you tag a character?
Is it in code, or can you click on somebody?
Usually it's a key bind.
In our game, it was a key bind.
You selected an agent.
You would hit a key that would set a flag on that agent.
And then basically all you're doing is you're setting a breakpoint inside of an if statement.
It's like, hey, if the agent has this tag set to true, I'll spin break.
I love that so much.
It's really simple.
Dude, now you've burned it, so you can't use it in the simplest trick in the book thing next year.
That's fantastic. Hey, did you have any...
I usually have a debug gun.
So I find a character and shoot it with that gun, and that then starts the session recording on that character.
And one thing on top of the breakpoint setup, I also really like to have the ability to overwrite values in the debugger.
So we're...
Like saying this is the current game environment we're playing and I've just added this new behavior, but I haven't actually implement the action yet, so it will just go to the default handler, but I just want to test it out, so I'm just going to set it to always return this value or just basically augmenting the reality of the agent and going even though.
You have like full ammo or whatever.
I want to see how you respond to not having any ammo at this point.
And so this beats the heck out of going and commenting out the real code and saying, nope, in this case ammo equals four always.
And because then you always forget to go back and change it, right?
Fun time.
Yeah, fun time.
You know, you got a weird idea of fun.
Okay, so then turning it around, what tools can we provide, you know, designers in QA?
And some of them actually apply things for ourselves like just the visualization.
Or the debug gun, which I'd love to see that in action.
I'd love to use that on real people, actually.
So what are some of the things that are good to provide to designers, to QA, to other non-programmer types?
I think that it's more, as you said, it's also very useful for us, but a lot of in-game things like.
being able to, as they were saying, select the guy and maybe have a command to kill everyone else in the level except that guy, or being able to spawn some people around him to try and generate a specific scenario where you need, like, you know, you need a sniper and something else.
So you can go and, by using debug commands, just spawn those guys in there without having to go and create a whole map for that.
And you can have a lot of.
camera controls like flying flying camera so you can like look at things from different angles or you can move the player somewhere else or you can make the turn the player invisible for people and all those things that you want to you want to kind of fake to try and stress test your your AI while you're playing. That was a lot of them. Anybody writing these down. That's that's cool. Yeah. Just basically I want to be able to completely screw with the environment in any way.
in a non, you know, something that doesn't violate the AI, but so I can set up my own scenarios, visualize, view these scenarios to just recreate a very specific situation is what you're getting at.
There was a cool feature in Slender Cell Blacklist.
So, well, for those of you who have played the game, basically the LKP, the last known position thing, where It was basically being drawn and people knew that you had been there or whatever.
So they thought you were there.
I saw you.
Yeah.
So to stress this, that thing, I just remember we had very quickly, if you pressed both, um, sticks in the controller, you started.
you were able to kind of like fly around.
So if you started doing that and kind of like flying on the other side and then flying on the other side and then flying on the other side, kind of like messing with them all the time to see if they were updating the LKB properly, if they were, what they were doing when they were not seeing you in there anymore.
So kind of like the invisibility thing, like toggling it like on and off, or like doing all those things like it's, you know, it's a lot of tricks.
You know, yeah, anything to be able to speed up the process of getting to the thing that you're trying to test is what you're getting at.
Cool. Anybody else have...
What are some tools for designers in QA?
I don't know if this is applicable for every game, but for Planetary Annihilation, we had the ability of just basically spawning AI on AI games and just having them run as fast as possible, no visuals, no nothing, just for the purpose of finding crashes.
That's an easy way where you can find those weird stack errors, finding overflow problems, finding null pointers where they're not supposed to be.
finding that sort of stuff.
And that's also a common thing for doing balance stuff, too.
In things that inherently have, well, and maybe even things that don't necessarily have AI players, but you have created AIs that will play the game.
So strategy games, and I know Civilization would do this quite a bit, where they would just crank up the sim speed and say, OK, well, this dude keeps winning.
Something's up with his balance stuff.
And it's not AI and everything, but the point still, Kind of like what Sergio was talking about, I want to quickly get to test this situation.
You're going the other extreme of, I want to quickly test all situations.
Yep.
We did the same thing for navigation.
Sure.
OK, cool.
So like selective session recording, kind of inspired by how the new consoles have that share button.
They can hit the share button, and you share a couple minutes before and after the situation you currently are in.
Having the.
the designers and the QA people give them basically a panic button, like, something's wrong on the screen right now.
And the session is just on a loop, recording the last whatever minutes and some remaining time afterwards.
And then you go, at that point, inject an event, a marker, this is where something, the button was pressed, and this is the context I have to look at.
Rather than, here's like...
20 minutes a session and around the time where this guy is on the screen and yeah.
Yeah, it's very similar to the rolling 30 second buffer and fraps for example, but.
What you're saying is now I also have a whole lot of game context that's been saved with that instead of just video.
I mean, I like having it as just a custom stream so you can also go, oh, I'm going to combine this with the video stream or some other debug systems and just go, here's the same section of gameplay.
And so that's great for the designers in QA, but then now you have a deliverable that they hear.
This is, something went wrong, I viewed this, and here's a shitload of data.
So, okay, cool, yeah.
Anybody else have anything real quick?
Yeah, something we do really extensively in Guild Wars 2 is just little cheat commands that you literally type into the chat box, and they range from things like.
boost me to max level to kill everything in a 5,000 mile radius around me, or whatever.
5,000 miles?
Wow.
You can type whatever number you want in there.
It's pretty open-ended.
Open-ended, like.
The cool thing about it is that those are available to anyone on our development branches.
So anyone in QA or design can go at any time, spin up their own map, and go say, okay, what happens if I just arbitrarily kill these three guards, but not that one?
and then they can play it and find out what happens.
And then they can go back and pick another guard to not kill the next time, or whatever they want to do.
And it's all done through these just little, literal text strings that you type into the chat window, and you can put parameters on them and get real fancy if you're so inclined.
But the basic power of changing the game state to do whatever you want is all there.
Yeah, it's exactly that mentality of, like, you should be able to do that at any time.
All right, well, we've got about three or four minutes left.
And I told the guys, I said, if you were to just evangelize one or two sentences, what's the, if you could tell your team, tell every team on the planet, this is one thing to remember, what would it be?
And so they thought about it.
We talked about this yesterday afternoon.
And so I wanted to see what you guys came up with.
Who has something?
That this is the one thing I want everyone to know about making life easier for testing.
Who wants to go first?
The developer who breaks separation of concerns gets to sit in the corner.
Built-in punishment too.
Wasn't expecting that.
Then that's that separation of concerns.
You said right, okay.
Yeah, so it gets to sit in the corner.
What if I like sitting in the corner?
Go ahead.
Anybody who's got one?
you know, just keeping in contact with your QA team.
I found that to be incredibly helpful, you know, keeping them apprised, letting them know what expectations there are.
And if you have any ideas on, you know, what could possibly break the AI, I mean, share it with them.
You know, if you don't have time to test something because you're working on a feature, let them know.
So just talk to the relevant people and listen to them.
Brian Schwab mentioned this earlier in one of the sessions, just listening to what, you know, yeah, okay, cool.
Mike.
Yeah, I think the main thing that I would advertise is just, you know, you need to make a tough call about how attached you are to the specific set of outcomes that you have in your imagination.
Because as we've talked about here, you know, a lot of times you'll get into a situation where things don't go as planned, but they still look okay.
And it's probably time for a lot of, you know, designers and AI programmers alike to come to terms with the fact that If the player's having fun, we've done our job.
And we don't need to be holding ourselves to any other metric besides that.
OK, cool.
Yeah, I like that one.
Sergio?
I'm just going to repeat myself.
But basically, testing that your stuff works is pretty simple.
But testing if it can break or not is more complicated.
So that's what you have to do.
Try and break it.
And when you think you can't break it anymore, go and find someone else to break it for you.
Like Martin Moss.
Yeah, I can't wait to see him again. You break things. I heard you're really good at this so excellent. I appreciate that it is 629. We don't have time for questions and I don't know short of debugging your games for you, which I don't think we could do in a minute, but please help me. Thank my panelists. There's a lot of good information there. Some of it seems obvious, but we always forget it, and so that's why I really enjoyed having you know having you guys up here. So please help me. Thank Mike and Mike and Mo and Sergio.
