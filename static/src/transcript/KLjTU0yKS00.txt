I am Alexander Boryzniak of Ubisoft, and today I'm going to talk about the tech I developed at Ubisoft that I've named the IK rig.
I was asked to start with some info on the speaker, and I did a little research.
I have 13 years in the industry, worked on a bunch of projects in a bunch of studios.
Past three years, I'm technical art director of Ubisoft Toronto.
Now, some of you have seen the original reveal talk from Vienna conference last year.
Some of you have not.
So I will do a short recap before I proceed to the new cool stuff.
I started working at Ubisoft on a title that I can't disclose because we just didn't announce it yet.
Then I moved to another title, which is under NDA for now.
And to troll you guys even more, I won't tell you about my two current titles.
And if you think that's a very boring folio, just wait till the announcement dates for these guys because it won't be as boring when you see what's behind the black boxes.
So for now, we're going to have a friend DA give you hints.
You speculate.
We wait.
To help you speculate, I just want to remind you what Ubisoft has recently shipped or announced just to give you some itty bitty ideas.
Some disclaimers before we begin.
So I'm talking about the tech that's quite fresh.
I came up with this idea about a year ago.
And it was for one of the unannounced projects.
And right now, it's sort of a can of worms spreading through several projects.
Ike Rika's best proof of concept made it to a runtime implementation.
Now, Now we're looking where else to apply it.
I'm not going to disclose anything that's secret, which is like 90% of what we do.
And I will not mention any project names or details.
I can't show you the footage from unannounced stuff.
So today's examples are not related to specific titles.
I'm not here to announce one game.
I'm here to talk about the tech that's going to be working in a number of games.
So today we're talking generic.
And I've prepared a bunch of examples where actual assets are hidden or replaced by placeholders.
So today we talk about the principles, use cases, effects on design, effects on the pipeline.
And we talk about how you guys can try this at home.
Disclaimer number two.
You will see a number of examples which are intentionally exaggerated.
So, for example, when I say we can take human motion and turn it into an octopus riding a bicycle, I do it, I show it, it illustrates a certain solution.
Doesn't mean that whenever you need to have a creature like this moving, you have to use the IKRig.
No, you're welcome to go on and mocap a T-Rex doing just that.
That's up to you.
I use these examples to talk about possible use cases, to demonstrate possible applications, dos and don'ts.
So rabbits will run, and octopuses will ride.
What is the IKRig?
IKRig is a systemic animation approach.
It's a principle, a tech, developed to let us do more with animations.
And the first thing is unbinding animation from rigs.
So a unified language sort of a thing, a system that will bring all animation data of similar characters to one shared form.
Secondly, we move away from animating the key frames.
We start animating the animations.
and we control the way our characters behave at run time in the engine.
We create behavioral rules that can be mixed together to produce a wide variety of animations.
All in all, it's retargeting 2.0, done on the fly with direct animated reaction to changes in environment, character stats, whatever happens at this moment in the game.
This is the most important slide.
Incidentally, it's the most boring slide.
You guys will want me to rewind to this couple minutes from now, I promise you.
So iKeyRig is a text solution for converting animation of.
any rig into a set of IK chains, application of context-aware modifications to those chains, and application of the result to any other rig, whether runtime or offline.
So you have your source, your motion capture rig.
You map it to the IK rig unified structure.
Using this mapping, all the animation can be converted without any loss of data.
Within the IKEA rig, it's easy to modify the parameters of what happens with limbs and character as a whole.
We create sets of rules to explain what to do at any particular situation.
And then we can mix and match.
We can apply these changes on top of other changes.
So for example, if one of the rules teaches the walking character to crouch, another rule teaches the character to aim a gun, third rule transforms male walk into female walk.
The combination will take male walk into a crouching aiming female.
This is actually elementary rocket science, and I love it not because it's complicated, but I love it because it's simple.
Let's take a look at something simple.
The leg.
So our usual leg, made of three bones, plus all the small secondary things.
Some of the problems we face here are, first of all, secondary animations for muscles and stuff like this.
If we bake them, they will not really be working at the game during the run time, if we have the ragdoll, for example.
Secondly, we have a bunch of transforms, but they have little meaning.
We can blend them, but those position rotation of each of the bones in the chain don't really tell us much about what's going on.
Finally, most importantly, our rigs and animations are bound together.
You can't change the rig because the number of bones, their names, their hierarchy, their sizes, they must be content for this animation to play properly.
If you really want to change the rig, you can of course retarget and re-import.
This way the update involves the offline loop, plus you need to maintain consistency between all results of all of your retargets.
Simple fact is we just need a couple things to move this leg, assuming it's planar.
We need the start position, we need the target position rotation, and we need the direction of the knee.
You understand these can be easily received from mocap.
You just need to map the bones to nodes.
That's it.
That's basically targeting.
Extra bones such as twists, they don't have to be animated offline.
When we use the bones which have soft skin geometry, geometry soft skin to them, and we can animate them using constraints as we play.
So the IK rig definition is mapping of bone rig to the IK rig directly in the engine.
Once it's mapped, all the animations we throw to this bone rig will be playable on all the rigs which have been mapped to the unified structure.
So in this example, the bull leg is the default config.
Everything else is running exactly the same animation, but in different rigs.
This is possible because each of those rigs has been mapped.
What constitutes a casual rig?
So we have active bones for body data, hands, feet.
We have twist bones.
We have small bones for fingers and face, secondary constraint muscle bones, different nodes, pivots for root capsule, prop attachment, and so on.
In case of IKEA rig, bones can be the same, but we're not working with bones after we have mapped them.
Okay, we're working with IKEA rig nodes.
So there are two chains, IKEA chains for hands, two chains for legs, one for spine, one for head.
That's it.
So what we do is we create one IK rig per character type, as in biped, quadruped, bird, fish.
We create mesh with skin to bones just like we would normally do.
But now we don't fit the model to the rig, as we often do.
Instead, we fit each rig to each model.
So the look and proportions of each character are driven by the concept and by the character artist.
not by shared rig.
When we export-import in the engine, we have the IKRig definition tool, which we use.
We map the bones to the full-body IK, to IK chains.
Everything else, all the small bones, are constrained and run directly in the engine at run time.
All the extra bones, they don't have to be shared.
We can have different bone setups for different characters.
We can add or remove them as we please.
And we don't need to rebake any data.
Let me just briefly go over the most common constraints because they will come in handy later on.
First of all, we have the position constraint, which is super useful when there's multiple parents.
For example, cowboy, one position parent can be his moving hand from the animation, another one is his holster.
Now if I constrain something between them, I can, at any moment, move the hand to the holster position.
Orientation constraint, much the same way.
If I want to add a tail, I can take the rotations of upper thighs and the pelvis to rotate the starting part of the tail.
So far, so good.
We'll look at, of course, what we usually do to aim our guns.
But I also like to use it to aim the head in a conversation.
And again, we need to do this at runtime.
We don't need to bake this.
Spring and damping for little flapping pieces.
We are using it right now as isolated example.
I like to use this within Nike RIG to add the secondary motion.
Trembling curve to throw in arbitrary curves that we want or maybe take a sign of time.
And driver driven.
So for example, one transform of one bone affects different type of transform of different bones.
I want to note that for most cases, very few constraints are actually required.
So over constrained case like this, there's only 5% of what we see animated is the actual bones.
Everything else is constrained.
We don't usually need this in our games.
Such cases would really never happen.
But it's good to keep in mind, we can LOD up and down as much as we want.
And again, we don't need to re-import, re-export anything if we want to add new stuff moving to the character.
But we want to do more than simply share the animations.
We want to change them.
So to the naked eye, it may appear that it's the same animation.
And it is.
The key difference here is that the left character is running 35 bones.
And the right character is running the same animation, but it's the IK rig format.
They're not different.
They should not be different.
By default, without any changes that we've introduced, we should have exactly full quality that the animator expected to have.
That was invested in the start in there.
Now we've defined this rig as IK rig, right?
So we can start changing the node behaviors as they play the animations.
The incoming animations can be received from just the key frames in your 3D software, in the offline case, or which is more fun, we can take the result that the blend machine spits out and modify that right as we play.
So now we have this IK rig definition, we can have some fun with it.
Our original base animation is always there.
So any change we have is just a rule that I threw in.
I explained that I want different behavior from start and nodes of the thing.
And by controlling the behavior of these targets for hands, spine, feet, I can do whatever I want.
I can modify the existing style of motion.
I can create a new style for a specific case.
I have control over footsteps, over hand arcs.
More importantly, yes, we can throw in new nodes and start animating to those nodes to create some new behavior.
I call such sets of modifications the IKRig rules.
There may be a rule teaching a character to crouch.
Maybe a rule teaching him to aim a gun.
Maybe a rule teaching him to cover his head.
Whatever else.
The cool part is, when we throw in, especially with the prop works, when we throw in the weight of the prop, and based on that, we change the way that the prop is being carried by the character.
Or we can also factor in things like character strength or tiredness.
One of the goals set for DaikiRig initially was to have one rig per one character.
So you have 50 characters in a game, you have 50 rigs in a game.
But keep it simple, share their animations and allow for modifications which are easy to manage.
So in case of character class rules, I just create a rule that's...
indented for this specific character forever.
So, a hero character will always carry his shoulder a little backwards.
Female, for example, has elbows bent inwards and a little more wiggling hips.
Zombie case in here, extremely bent spine.
I could add the tremble if I wanted.
And it's going to apply on whatever this character plays.
And I can mix and match them.
I can have a zombie hero or, I don't know, brute female if I want to.
Prop operations, dealing with weight.
We factor in the weight of the prop, play a different rule.
It looks like the artists prefer to have two boundary cases to create two boundary rules, super light, super heavy, and then work between them, or possibly even have a middle case.
So, depending on the weight of the prop change, we would be somewhere in between of these states.
We can also, as I said, factor in how tired the character is.
We can factor in his strength, so big guys will have easier time carrying rocket launchers and your grandma will not.
We can use the same approach for interaction with static props, like doorknobs, for example.
Characters of different structure and size will still reach for the same place in the world to turn the doorknob.
Imagine we have a shooting game with multiple weapons.
Conventional solution.
We create several animation sets for different weapons.
So we have 50 weapons, but we know every weapon we need to run, crouch, reload, shoot, too many things to mocap.
So we say our AK and our M16 will have the same placement of parts, so we can use the same animation to play them.
Now adding a weapon into the game would become a complicated thing, because now we need to create carrying this weapon with crouch, without crouch, aiming, shooting, whatever.
In conventional setup, we would also have to propagate every change that we want to do to every rig that we're using, because maybe we have male and female rig.
And now we change the way the AK behaves.
We need to modify it on both rigs.
In IK rig setting, though, every weapon, prop operation, IK rig rule.
Simple as that.
So I can just go in and modify the rule to create a new type of behavior with this weapon.
and aiming is separate from the motion itself.
And this will propagate to all the locomotions and basically to all animations of the character if I want to.
At the same time, grandmother and me will carry the rocket launcher in different way because the prop weight is factored in.
Should we want to add a new weapon, we would make a new rule for it.
Propagate is set to all characters if we want to.
Should we have several aiming modes, create a new aiming mode for hero, for zombie, for female, it will work.
The same thing.
One more new thing here is IKRig rule for collision avoidance.
I like to have the hard collisions as we do, but I also like to have the soft collisions, because we humans are smart things.
We apply the contradictory force before we hit ourselves with something.
So that creates a more realistic and believable motion for prop operations like this.
So speaking about prop interaction, now that the characters touch the props using the IK, which we have full control over, we can start scaling the characters and scaling animations.
And now you're thinking, Alex, we know how to scale the characters, and we know how to scale the animations.
That's not the thing.
Yes, we do. We do.
And every time we uniform scale the rig, we also scale the animations, we end up with lots of issues.
Speed of character changes for once, and this is not what we always want.
To illustrate this better, I made some fan art for Star Wars just to show you.
So it's the same animation for both of these guys, but drastic speed difference happens because of drastic scale difference.
In extreme case like this, that's OK.
But more often than not, speed must be dictated by the design, not by the conversion result.
Usually to fix that, we'd start scaling animation timing up and down.
But if we do it more than 10%, 15% speeding up or slowing down animation, we start losing the data about velocity, about the mass of the object.
And that's just not too good.
So here's the fun trick.
In the IK rig, I scale characters independently from the animations.
So animation is the same for these guys, but the red goes faster.
Why?
Well, what's done here is the scaling just of the footsteps.
Notice that all the animations are still in complete sync.
However, I control the amount of footstep scaling.
And now that I have this complete control of this speed, I don't need to adjust animation speed or timing.
I don't need to introduce the foot sliding or do any other wicked thing.
And degree to which I can speed up or slow down is immense.
There is 400% difference between the slowest and the fastest guy.
And to illustrate this magnitude better, it's pretty much that.
where we just scale the character up or down, we wouldn't necessarily wanna scale the speed of his motion.
We can just keep it, we can set it as much as we want.
So the change here happens reversely, right?
The characters were scaled, the animation remained the same.
Footsteps still have scale of one.
And I want to emphasize the importance of using different scales.
Yep.
And yep.
And why?
We know why.
We, uh, thing is that our games usually just have one character, really.
I mean, we can do different texture variation thingies, we can push the geometry on the models, but it's the same rig.
And yeah, that's, that's gonna look this way.
Even, um, like.
It's funniest when we share the rig between male and female, like Barbarian and Amazon.
But OK, let's say we added the female rig.
Now Barbarian shares the rig with a necromancer.
Amazon shares the rig with her grandmother.
It's not good for any of those characters.
And adding more rigs is, of course, expensive.
Thousands of mocap takes for several actors, feeding all of this into a monstrous blend tree.
Controlling it somehow, discovering now that adding an umbrella to the game is a huge overkill, because you need to mocap all of this.
And it's not something we want.
It's something that we just endure.
And it's easier for the guys in the movie industry because they can have any scale they want for their Shrek or for their Hulk and Spider-Man and Captain America.
For us it's hard, so what we try to do is we try to frame the guys so they look different even though we can't scale them, we can try to fake it.
We basically try to make them look something like this.
We're not building Star Wars games in Ubisoft, OK?
I just made it for you guys.
Just disclaimer.
And I scaled them all to their actual Wikipedia height.
And I've compensated the scale in animation, so they still move in perfect sync.
and we can speed them up, and we can slow them down, we can give them props, and they will look fun.
And the moment you set the scale is different, like this, something magical happens, something that you can't really unsee.
Because going back now, is very, very noticeable.
And OK, so we know how to do the scaling.
Now we know what to do with it next.
And we can still keep the speed of the change scale if we want.
But we just don't have to.
And just by the way, these guys, as every character that I showed so far, are still playing the same incoming male walking animation.
Uniform scaling is actually boring.
It's not fun.
Come on, every human has his own subtle proportion differences.
Here we have the bass dude.
He's feeling just fine, but.
I swap his left hand and right leg to much longer, bigger versions.
Now I call this guy Quasimodo for easier reference.
This is the base layer of the IK rig.
Notice the animation is exactly the same.
It's not enough.
We need to explain him how to move.
So I make a list of commandments for Quasimodo.
I tell them, you know, if your hand is heavy, you should lean.
If your center of mass has shifted, compensate for it with your spine.
Sorry.
And if the hand gets heavy, don't wiggle it around.
Drag it.
And the fun part is that the scaling, sorry, it's just really warm in here, the scaling can happen at runtime.
And that's super good news for all the monsters, and mutants, and robots.
Because now that the Quasimodo knows how to work with normal hand and super big hand.
He can do it with any size of the hand.
And just because we told him how to do it.
Detour a little.
So I often say I tell the characters what to do, and they do it.
And that's a little misleading, because people visualize the process like this.
I tell them to do this and that, and they do.
In fact, it's more of this and this.
Because I control the action by creating those rules, by plugging them together.
But it's more of a.
playing with the Lego kit without actual schematics.
So it's fun job, still a job.
End of Deter.
So yes, everybody is different.
And since we already captured the motions, we inherently capture the motion styles.
And I'm talking just about the mocap right now.
So this is the prototype feature, but it's worth mentioning nonetheless.
I call it Motion DNA.
And the idea is that we just dress a guy up in a suit.
We let them move around the mocap studio for a while.
And we're not capturing the exact loops of what he does.
No.
What we capture is instead, how does he carry his spine?
What is the width of his footsteps?
How much hand swinging is there?
And these things we can parameterize.
And there is about 20 parameters that so far I came up with.
And if I take those things, I can store them as IK regroup.
And I can apply it to the animation of a completely different guy.
It's not going to make them exactly believable, but it's going to be way closer to this actor who just moved around for a couple of minutes in my mocap studio.
So, just to oversell it a little, maybe sometime in the future you work before your Kinect and we pick up how you do this and then apply it to Cthulhu dancing on the screen even if you just have the controller.
It looks plausible.
So let's recap for a minute.
We can take one animation, we can change its style.
We can scale our characters, we can scale our animations, we can change the proportions on per character basis, while we still interact with props consistently and we can even control the speed.
Long story short, we know how to turn our walking male into crouching, aiming female.
What's missing?
Oh.
clicks. So what is missing from all of this? Then even terrain navigation, of course. But before we go into Unreal Terrain Navigation, there's this thing that's something we've done for so long we don't really notice it anymore. And I love to kick this thing, and I will kick it again now. What I mean is the term locomotion modes.
So we do so many things, weapons aiming, reloading, shooting, opening doors, that we limit the type of locomotions.
We let the characters walk or run, sometimes jog, and there are brilliant people trying to blend those states together.
We let the characters walk or crouch, sometimes crawl, and there are other brilliant people trying to blend these things together.
But the sophisticated blends are just masking the problem, but they're not solving the problem.
The problem is our navigation states.
In games, are integers.
And in life, they are not integers.
And this is inherited from the idea that we will want to do different things as we navigate differently.
So we can't afford to mocap everything in all the interim states.
So what's wrong with state A or state B switch?
But honestly, there's nothing wrong with this switch.
It works.
For past 30 fucking years.
And we keep doing this.
And what's worse is we build our levels with this in mind.
Your doors must be fixed width or the character won't walk through them plausibly.
Your obstacles must be fixed height or he won't jump over them plausibly.
The ceiling must be fixed height, because otherwise he won't crawl under them plausibly.
And every now and then there's those little just missing inches that the animation suggests a different height.
But we just have two states, and that's it.
What if this collision is only halfway down?
What if it's moving?
What if it's a little lower or higher?
Does this character remember what he just did?
Does he have a history?
Is he limited to these states?
If we start ray tracing forward, not in time, but just in space, we can start predicting what happens.
And we can change just the right amount.
We don't have to go all the way down if we just need to nod our head.
But if we have to, we will do, and we will, our motion will suggest the physical appearance of the world around us.
And it's gonna make more sense to us.
Humans, we don't collide by our own choice.
So we don't clip like this.
We avoid, we're smart beings.
And it's not hard to mimic if we scrap the idea of integer states.
And it's not just integers that are obsolete.
The whole concept of metrics is insulting, and it's ridiculous.
And I know the questions that we get at this moment.
Metrics are at the core of our pipelines.
Without metrics, how do we even design or model?
How do we build levels?
How do we build the game with arbitrary scales, like the real one?
like the real world.
We, it's scary, we can't pull it off, we're not gonna risk it.
And first of all, let me call these old ways what they are, limitations.
And you know, a guy breaks a leg, spends couple weeks in the hospital.
Then he has this casket for several months.
Then the casket is gone, his leg is healed, he still keeps limping, because he got used to that.
And we got used to that.
Maybe we don't have to do that.
You see I've thrown in some uneven terrain in here just for kicks.
So without further ado, let's take a look at different ways to navigate uneven.
Common solution, write trace down and be happy.
Well, the sad part is there is no history in this.
Walking down, walking up looks the same.
And of course, we can't have stairs in here because if we just shift this height between two frames too drastically, we're going to have the foot popping.
We can't use the stairs.
OK.
And.
Again, if we can ray trace forward, ray cast forward, we can start anticipating, say hi to anticipation, say hi to follow through.
And we can propagate this change to the whole body, not just the legs, because we're not splitting between upper body, lower body anymore.
We can act.
And it's fun.
And our slope can be arbitrary.
We're not picking animations from the pool and blending between them.
We generate the proper motion for this specific case, for this specific guy, with his specific scale.
No root sliding, no feet clipping, no metrics.
What do we do here after we fire the level designer?
Normally, we just say, OK, it's going to be a flat plane, and we're going to clip, and that's fun, because it's a video game.
OK, cut us some slack.
That's fine, yes.
No YK solution can handle stuff like this.
Usually, we take it and be happy, but again, you know, what we can do instead is we can switch into Nostradamus mode, or what I call Nostradamus mode.
Basically what I do, we throw in animation into the engine, and then we analyze our feet, and we just store a couple floats per frame.
For each foot, we remember, how long ago did I get lifted up?
How soon am I gonna be planted down?
Where?
will I be doing this?
So now, per frame, I know that the foot, for example, initiated the lift-up thing.
And since I know where it's going to land and when, I can modify the whole curve and arrive to the proper location.
Yes, some weight of the body is lost in here.
That would be fixed on the polish.
But at least.
I planned correctly.
In fact, I'd slow down the actor here by twice because human wouldn't really risk that fast, moving that fast, but that's fine.
In famous case of discontinuous terrain, the stairs, I love it.
So, okay, if we have stairs in our game, all the stairs gonna be the same because, you guys know, we mo-capped for these specific stairs, going up, going down.
We can't have stairs of different height.
What happens if he walks diagonally?
His animation's not gonna play anymore.
What happens if he crouches?
Do we mocap crouch up, crouch down, and maybe in diagonals?
What happens if he wanted to limp?
What happens if I have a female rig?
Now I need to propagate all of this.
So, again, conventionally, I would guess, root sliding and some foot clipping, and it's fine.
But remember, we have this Nostradamus mode.
Right?
So, we're tracing the footsteps, and when the foot gets lifted up, we can figure out where it's gonna land, and we can figure out if we wanna land there, or maybe a little different place.
So now we can have arbitrary steps.
staircases, I mean.
And we can have arbitrary scale of the character, and he will still walk.
I mean, it's just depending on his height, he will be missing the stairs, the steps, or maybe he will step on each of them, just like we do, because we have different scales, and that's how we behave.
And since we generate this animation on the fly, I mean, we can have any type of steps that we want.
for any character scale that we want.
Yes, we need to polish this, but this is already better than what we normally have with just clipping through the stuff.
Final test of discontinuous and even terrain.
Conventionally, we would have a mini game for this because you can't really mocap, unless you mocap for just this specific set of obstacles and just play this one animation.
But again, I mean, if in real life a five-year-old would have fun doing this, then our hero should be able to walk through this as well, just by pressing the button without no mini-games.
So I know it's a lot to take in.
Let me show you the combined result of these rules.
Please welcome the marching band.
This operates directly from the guy with the controller.
All right?
Then we added the IK rig.
We have different proportions and scale.
Remember the Quasimodo guy, right?
Even though they're all scaled, they walk in perfect sync.
Remember, we have the footstep scaling.
Remember our IK rig rule for crouching?
Notice that the tall guys and short guys crouch differently because they need to go to different height as a result. I made some of them male, some of them female because it was easy.
And every single one of them carries his or her own musical instrument and plays it to some tune that I will expose later on.
And one more fun part.
There's this waving ground in there. You see that? We're going to go there.
Because the watery, like, I call it the Jesus test.
It's the ultimate thing. I mean, if I can walk on this, I can walk on anything.
Oh, plus an Astrodamus mode for the steps.
So this is one source animation.
Everything else generated per frame right here, controlled separately.
This is a nice visualization for the curves that I threw in for them to play.
So each of those characters uses one of these curves to play his or her musical instrument.
So there's the drum, the dishes, the tambourine, the violin.
And you know what?
For all I know, I could just throw in an actual musical piece and take the curves from different musical instruments and feed them here and see the result.
Not sure it would be super cool from the start, but I can try.
And yeah, I scaled them all differently because, well, because I could.
Human height, usually like 1.7 meters male, 1.6 female, plus minus 10-15 percent.
We're all different.
And I choose to respect that because it's a very small and cheap way to add realism to this.
So, marching band.
Marching band is one single actor turned into nine characters, some male, some female, nine different scales and proportions, nine animated props synced to the tune, controlled separately.
They can crouch.
They can climb.
Time to make all of this.
I had the rules ready, remember?
One day.
Took me several months to realize one more thing.
So crouching is actually just a subset of a bigger thing.
avoiding. And if we can avoid, we can attract as well.
So if we start raycasting into sides, we can have a lot of fun with characters just walking around simply by the level.
And the coolest part is I've created these rules for avoidance and for traction. And I can apply them to all my crouching, zombie, aiming, female, carrying, whatever.
And I can tell the different characters will do different things.
Maybe somebody will reach further.
Maybe somebody will be raycasting into other characters and touching other characters.
Maybe somebody will be touching him as he goes on.
I don't know.
So let's have some more fun.
The big section, the quadrupeds.
First thing, of course, I have the horsey.
I turn the horsey into IK rig horsey because I can't work with bones.
So once I turn the horsey into IK rig horsey, I just have those nodes.
Now I can play with it, but the thing is there's less to do with quadrupeds.
Conceivably, they don't operate with props.
But what they do operate with is another character.
So I put an IK rig in an IK rig.
And we can tell this rider what to do.
We can give him props and so on, as much as we want.
As for the horsey, we don't usually distinguish between with or without rider animation.
We never really mocap for with rider, without rider.
It doesn't seem like we need to.
question of course, would there be a need to modify the horse animation in here? Well, does the player need visual feedback that the horse is overloading the ride, right? Maybe there is an extreme case where we want to tell him that this horse is not a happy horse. And it's nice to be able to do this fun stuff on the fly. Of course I exaggerate it, but you know what I mean, guys. And if you don't have the horse with the heavy rider mocap.
you're still fine. In fact, I didn't have horsey mocap at all and I'm still fine because I had Jerry. And Jerry has been my champion for the past year. So, you know, if you're still feeling bad for the green horsey, don't. Because none of them exist. But Jerry does.
And through my test, Cherry has been several breeds of dogs, cat, mountain lion, a bear, horse, elephant, and even more bizarre things like this.
So in all honesty, he deserves an Oscar.
And yes, I was naughty.
I scaled this animation, made it slower, made sense in this case.
So at least as far as the locomotions go, yes, I can share, but in a certain extent.
Of course, the elephant would still piss with his leg lifted.
We wouldn't do that.
But happily, we have talented animators.
And of course, all of this is only possible because we can scale the footsteps.
Quadrupeds are super different in their proportions as we go from breed to breed.
What else is good about IK Rig for quadrupeds, I guess?
Maybe this.
Maybe then even terrain for horse, for the rider.
Slope, speed, any direction as we please.
And not only that, you remember we did this Nostradamus thing, right, for foot placement?
You probably know it's coming.
Another thing that you will not unsee.
At least I didn't see it anywhere.
And as I said, GRE is amazing.
Let me talk about main reasons to introduce the systemic animation solution to the pipeline, be it iKey, Rig, or any other one.
First of all, it's about character design.
Character design that is driven by the character artist and concept artist, not by the rig that we share.
And we want to add versatility to sizes.
We want to add versatility to proportions of our characters.
We want to add different behaviors as they move around to show that they move differently.
And we want to provide visual clues when they're working with props to show it's heavy, that he's limping, he's hurt.
we can show it in animation.
Content on demand, that means generating animations right as we play, right as we need them.
So for this specific guy in this specific case in this specific level, right now, I need this.
And we make it.
So hence the term systemic animation that I like so much.
We don't pick from the pool of stuff we pre-created.
We make it on the fly.
Fast iterations and, more important, fast prototypes.
Anytime you want to add umbrellas to your game, you just throw in a prop and you create the accurate rule.
Now I'm not saying this is gonna be the perfect umbrella-carrying animation.
I'm saying you can do it within one day and see the result and play with it and figure out what else do you want.
You don't need to mo-cap shoot this offline loop of retargeting for your female rigs carrying umbrellas and stuff like that.
Import-export, no, right there, right now, fast and furious.
Final reason, I first called this saving money.
But I know you guys, we get $10 million envelope, we spend $20, we ask for more.
So whatever we save here, we would still waste in some other place.
So I'm calling it cost allocation.
Let's save some time, let's save some money, let's invest this time and money into maybe generating more content for the game.
Maybe investing it into the rendering, maybe investing it into the sound and whatnot.
We will figure it out, I know.
What are the pipeline changes?
So characters, yes, they're created based on concept art and common sense, of course, not based on the rig.
Basically, we're cutting away a part of the pipeline where your character artist has just made this wonderful model and now technical animator comes and says, now you have to adjust it to this rig and he's crying.
Rigs can be unique.
But for the most human cases, it's like, you know, with any boxing or MMA match, they have this screen at the start where they say that height, weight, reach changes.
Those are within centimeters.
They're super important for the fight and for the fighters, but they're not too far away.
So normally I would say character artists can just modify the rig on his own, if you trust him enough, just to scale things up and down as he pleases.
And in Engine, riggers and tech guys map this incoming rig to the unified IK rig structure.
And they define which parts of the bones belong to which chain in the full-body IK.
And yeah, they also set up the constraints for secondary things like muscles and whatnot.
Animators, they work with IK rig rules.
There are two very important things here.
First of all, to work with this stuff, you need the real-time preview.
So if you build something like this, make sure your animator sees the changes and can experiment with them.
Second of all, there is a need, and we're actually discussing it right now, a need to connect the animation engine of your game engine.
with MotionBuilder, or any other software of choice for the animator.
So he can go back and forth, and set the pose for this particular trumpet, if he wants to, that's the simplest thing.
And because he would be doing this a lot, with trumpets, there is a need to have this process simple and fast.
But the largest, largest difference of all, is actually for creative directors, for game designers.
Because...
The holy shit moment happens when you realize you can spam these marching bands on the fly as much as you want.
That you can ride different animals.
That you can telegraph the character's state and health through the animation without this extra cost of mocap.
That you can have cover system, destructible covers, and characters of different scale will use those covers differently as they get destroyed.
That you can have some very interesting fun with AI and...
I will talk about this maybe next time.
And yeah, that you can have characters of different sizes and have this versatility in your virtual city or whatnot.
And that all of these things can be connected to your game design.
Because now you're showing way more.
Maybe you want the player to be noticing this for a particular reason.
when to introduce systemic animation solution to the pipeline.
As early as possible, I would say, if you had your first mocap shoot, probably it's too late.
Because there's a lot of things to consider.
First of all, you need to decide what is it specifically that you want to do.
Do you just want to have different proportions of humans?
Or do you want to be riding quadrupeds?
Or maybe you have a game with focus on prop interaction, and you want this interaction to be realistic.
So you need to decide how much of it you need to do.
Because there's no need to build the full shebang from the start.
We need to build something simple, see how it works.
Also, there are things in pipeline which will be cut out.
And a lot of this is about iterations and a lot of this is about...
motion capture for every possible crouch with every possible weapon, because probably you don't really need this anymore.
You focus your animation on things that matter, on cutscenes, on the character of main characters, but not on making endless variations for male and female rig for several different props.
Yeah, also you need to think about your levels, because if you start...
discarding the metrics of the levels.
That changes things a little.
You can start thinking about dynamics and their proper collisions with dynamics.
It's interesting.
But whenever you do this, I suggest you guys do it soon, because a couple years from now, systemic animation stuff will start shipping.
And it's gonna be hard to keep up with this.
It's all good.
But I also want to be clear when not to use anything like this.
And first example is the obsessive compulsive game mechanics like Mortal Kombat, for example.
They have a number of characters, yes.
But every single animation, every single key frame of every single animation has been visited and revisited a million times because it's super important.
The reach of the Sub-Zero's punch, the speed of this punch.
This is the game mechanic.
You can't allow this to be controlled by some arbitrary system.
You want this to be specific.
So in this case, you don't need any solution like that, especially because in Mortal Kombat, you're not really working with props or navigating uneven terrain as much.
Massive strategy games, conceivably, I mean the games where you have 10,000 characters on the screen.
even if you had the fastest full-body IK solution ever.
If the character size is 10 pixels on the screen, there's no reason to go too deep with fine-tuning the foot placement of him.
So of course we want to keep it smart.
And solo games, meaning games where you only have one character or the amount of character animation is low.
In such cases, maybe you want to invest in some sort of systemic animation solution, but only if that's the key part of your game.
And there are games where there's one character, but dynamically interacting with environment.
It's very smart.
Unless that's the case, of course, there's no need to invest in a humongous system if the total time of animated character on screen is 30 minutes.
So let me answer the three main questions you guys probably have.
What stage are we at with IK rig?
Alex, nice videos.
How usable is it?
So I came up with this idea about a year ago, as I said.
I spent some time bouncing it around in my head and with my friends at Ubisoft.
And back in the day, we were only interested in about 10% of what I'm showing today.
Because frankly, we didn't know all of this is possible.
I built the first prototype in MaxScript.
That was a proof of concept stage.
We were playing with it.
We were seeing what can happen.
Dogs to cats.
I remember the challenge from Christian.
And that was fun.
But then my friend Michael Butner took interest in it, and he built the runtime part of it.
And he keeps building it as we speak, like right now in Toronto.
And yeah, we keep developing it further.
To be honest, the list of complete features is now 10% off.
the new 10 times increase of the Dream.
And I don't know how much do we need right now, but we'll see.
And the key parts are there right now, namely the optimized full-body IK and mechanics for creating and sharing IK rig rules between different characters and seeing it in the engine.
Main load, yeah, resource-heavy, expensive.
Main load, the expense, goes on your full-body IK solution.
All the IK rig rules, they have very noticeable results.
But in fact, they are very short and simple lines of math.
This is what I like a lot.
I mean, it's simple mathematics.
They're independent of the game engine or the programming language.
It's very simple.
Move, rotate, constrain.
You may think of the IK regrules as motion shaders, but with way less instructions and way more output.
So after this math is done, then the full-body IK starts working.
And you can use any full-body IK you want, and that is your cost.
The good part is that full-body IK can be optimized very far in case you wanted to.
It depends, of course, on the rig complexity, so it can be LOD'd.
Things like ray tests, you understand how to control those, so up to you.
Finally, where can I download it?
The reaction I got a lot from the moment that I did the original IK rig reveal speech was this.
And let me ask you, where do you download the normal maps?
You don't.
You can introduce the feature to your engine, yes.
Or you can purchase an engine that supports normal maps.
But the reason I keep calling this is just one of the possible systemic animation solutions is that it's not a software piece.
It's a concept.
And it's in development at Ubisoft, yes.
But you don't have to wait and wonder if we release it and when.
If I did a good job explaining the idea, you know how to build it with Blackjack and whatever you want.
In the recent years, we had very many advancements in rendering, in AI, in amount of content on the screen and in the game.
We got amazing evolution of dynamics.
We evolved the streaming of data and a bunch of other things.
So it feels like now is the time that the animation is taking this step to the next gen as well.
And there are amazing developments out there.
There's cool stuff.
There's HumanIK by Autodesk.
There's Euphoria by Natural Motion, FinalIK by RootMotion, Runtime by Akinima.
I like to think that if we share.
the ideas and developments, we can contribute to the industry as a whole.
And that would be cool.
Credit you, this smiling person is Michael Butner, tech lead of animation and physics of Ubisoft Toronto.
He's the mind, the father of motion fields tech, motion matching tech.
And he's also the father of runtime implementation of the IK rig.
So, whatever runs now and whatever will keep running.
It would not be possible without Michael.
And of course, my gratitude to Ubisoft, because this is the craziest family I've had for an employer ever.
And we are shipping kick-ass games.
And you know it's coming.
Yes, to ship kick-ass games, we give money to smart people.
So if you want to work on epic stuff, and if the dev force is strong with you, there's a team that could welcome you.
There's Ubisoft booth on the premises, second floor, like right outside here.
Go talk to them.
Figure out, maybe it's interesting for you.
Maybe it's not.
But don't be shy.
Just give it a try.
It's fun.
Or catch me up.
I can give you a shortcut to them.
Why not?
Two things I want to suggest you guys to do.
First thing, tomorrow, 11.30, there's a talk about motion matching by Simon Clavier.
Go there and listen to him.
Listen to what he has to say.
If you don't have time, make time.
Because this is one of the coolest things in animation I've seen so far, and I've seen a lot.
Second thing, we will have an extended Q&A session tomorrow.
I'm going to keep this slide for a while, so you can take a picture so you don't forget.
West Hall, second floor, tomorrow, 3 to 4 PM.
Come over.
We can chat.
We can have some laughs.
Somebody may try to recruit you.
I'm sorry.
It's going to be fun.
So thank you.
That's it.
Who wants to ask something?
Guys, microphone, don't be shy.
Questions, questions.
What middleware products are you using to achieve this?
You mentioned a few full-body IK solutions.
Are you using one in particular?
Built our own.
With blackjack and...
First, thanks for sharing the idea.
I think we need more of this in game development.
Glad you like the fancy videos.
Yeah, fancy videos also.
But ideas is more important.
What I want to ask, how much time and effort does it take to configure one character with all the parameters that you mentioned, like how it moves and which depends from which part?
Okay, so how much time does it take to basically define a motion capture rig into these IK rig chains?
Is that what you're asking?
Yeah.
Like, how much time does it take to create a new character using already ready the mockup?
I would say like way less than a day.
There is a couple of crazy characters like you saw in one of the videos, this guy with multiple legs and hands sticking out of him.
That took me more time because, well, most of this time I was trying to figure out where to stick an extra hand.
But if we're talking about bipeds, yeah, that's fast and simple, actually.
Because, yes, I have different bones coming in, but I know where my legs are.
I might even have a naming convention and streamline it, like just script it.
And all those definitions, do you define them in the code, or you made some kind of UI which is usable by artists?
So right now it's not as friendly, but of course we need the UI.
We're thinking a node-based system.
And yeah, at the moment, it's lines of code with some nodes in place.
But we're not complete yet.
We're not shipping it tomorrow.
But yeah, whatever we do, we have to make sure that people working with this, some of them are nerdy technical artists, yes.
But a lot of them are animators.
And animators need to have nice interface, ideally resembling something that they have experienced working with before.
Hence, I mentioned the MotionBuilder, obvious choice.
OK, thanks.
Thank you.
This is very cool. You showed some examples where you said that, like, this is good initially, but you would want to polish it.
Can you talk about how you would polish these sorts of things?
Like, if an animator looks at something and says, oh, that's really good when the character crouches, but it should look a little bit more like this.
Is there some keyframing-like way to tweak it and polish it?
So, uh, yes, it should.
That's why I was talking about the motion builder connection like directly because animators like to tweak things.
But to be honest, this thing, you're not supposed to modify this particular pose.
If it's broken, you need to figure out why.
Because at the end of the day, we're generating the content as you play.
You never know what's going to be happening.
So if all of a sudden, your leg got overstretched, That means you need to introduce a rule to keep the leg within the limits.
You know how to do this.
So ideally, I would say you try to fix by that.
Now if my animator comes to me and says, you know what?
This female walks looks like crap.
My first intention is, OK, I need to open a video of female walk that he likes.
And I want to try to parametrize what's happening.
Because when I was keyframe animating stuff, I was doing pretty much the same thing.
I mean, in the mind of animator, you open the reference and you see what happens.
Now I'm not just going frame by frame, adjusting to the background image.
I'm trying to figure out what exactly is happening.
Maybe there is a curve.
Maybe every time the leg is lifted, there is an extra x rotation in local of the hips.
I'm going to do that and see this.
Hope that it works.
Thank you.
Thank you.
When you're doing your Nostradamus footfall determination and you're under control of let's say the player who's constantly changing with the joystick which way they're going, are you recalculating the footfalls like as you are moving your foot in the arc or do you land that foot and then just do your calculation off of the next footfall and account for that?
So if the foot is in the air, my incoming stuff just from the blend machine spit out.
It already has the idea of the position.
And I also have my Nostradamus nodes in there, telling me I'm like 50% in the air.
Now, if I start moving to a different direction, I have to modify my target vending position.
But again, if the direction I'm moving in is controlled by the blend machine, not by some artificial leaning, these new animations will have this target position relative to them.
So as my animation changes, my target position also is grabbed from the new animations that I just walked into.
Am I making sense?
Yes.
OK.
OK.
Thank you.
Hi, I heard another talk from Ubisoft about motion matching.
Is that related to IK rig?
So these two texts are different.
I started working on this.
taking the input from motion fields, motion matching, there was a rename, but it's the same thing.
And that's why I advertise Simonstock tomorrow so much.
I think everybody should do this.
But these two techs, they do different things.
So motion matching is all about the natural human motion with very effective use of mocap time.
So you create the motion.
iQuery is about modification.
iQuery is not about gesture.
This is the animators, the mocap.
iQuery is about modification of walk into crouch, simplest example, or carrying something, right?
So these things, they sort of work one after another, and they contemplate, they add up.
Thanks.
Thank you.
Your encoding of the foot animation looked like it was distance from hip to ankle with the angle of the knee or something like that.
I was curious if you ran into an animation where, let's say it's like a Dick Van Dyke jumping up in the air and clicking his heels together type animation, where the distance between the heels was critical.
Does that require you to modify your animation rig so you now have an encoding of the distance between the heels?
And have you run into cases where that happens a lot based on different content going into the game?
And is that important to your pipeline, being able to kind of change the definition on the fly?
Or is this very specific to, like, basic locomotion?
So any type...
I will try to turn this into a bigger question, and you let me know if I got it right.
Any time our character...
has self-collision of any sort, or a collision with a prop of any sort, how do we compensate for it in this system, right?
Yes, that's good.
So first of all, of course, we have the physical representation of the character.
I mean, we have the full-body AK, but we still keep our ragdoll and stuff.
We need this.
What I really like, and what I didn't have the time to do at the moment, but what I really wanna do, is if we mo-cap the guy, Going over a box, for example, an example of the prop interaction, right?
What I do is, in this mocap file, I place the actual box and I detect where there was a collision.
Now, if the box is scaled, my animation knows that here's the place where I collide, but this place is inside something I can find the new placement for the leg.
Basically, if the guy was scaled, but the box remained the same, if I know where it's going to happen, I can try to get it out.
It's not going to work out of the box instantly.
When the character self-collides...
If I scale him just uniformly, it's going to be the same, you understand.
But if I do the proportion scale, I of course need to be smart about what's happening.
I need to make sure that I don't let him cross the new bounding volume.
So yes, there's physics in it, but as I was talking about the soft collisions and hard collisions, I also like to slow down before I hit.
So I'm actually...
I need to keep this in mind.
I don't have this feature fully developed, so I'm talking from the top of my head.
But I hope this idea makes sense.
Thanks.
Thank you.
Is that all?
All right.
Thank you so much.
Have a good BBC.
