Oh, okay. There we go. Integrating 2D UI with VR environments. You probably already read that on the program. Who am I? My name is Brad Davis. I've spent over 20 years doing software development.
I've been experimenting with VR since the kick starter days, the original dev kit one. I built the first Oculus, sorry, the first Java bindings for the Oculus SDK. I've co-authored a book on developing for the Oculus SDK called Oculus Rift in Action. For the past couple of years I've been at high fidelity.
we're working on creating an open source framework for an actual metaverse. Distributed servers, anybody can build things, nobody's in control. And among other things there I've worked heavily on integrating the 2D UI systems into the 3D world. So that covers really both of these points. So why 2D user interfaces?
VR is this massive shift, why do we still need 2D for anything, right? Well, 2D UI is everywhere and it's familiar to everyone.
Everyone who's already using a computer. So that's a good argument.
Also, VR, I think, as we evolve, we're going to see VR metaphor replace a desktop metaphor where a VR environment becomes your place to work in. We're already seeing moves towards that with things like big screen and virtual desktops. So we have to understand how we're going to integrate 2D UI into that 3D environment.
Also, many things don't benefit from any sort of 3D UI. In fact, very few things that are inherently 3D, like a topographical map or a body scan. These things don't benefit from just being flashy and in 3D. While 3D representation in a VR headset can give you more information about an object, like depth, it can also obscure information.
object that's in 3D, part of it is always going to be facing away from you. So with the 2D UI, that's not the case. Also the technology isn't necessarily there for translating some things in the real world that we have sort of either full 3D or semi 3D interfaces for. Take, for example, a light switch on a wall. A light switch on a wall and a check box on a 2D dialogue serve the same purpose. They have one of two states. They indicate to you which state they're in. To the extent that a wall switch has.
a 3D interface. That's useful because I can reach over and touch a light switch and determine what state it's in or change its state without looking at it. And as nice as the Vive controllers and the Oculus touch controllers are, I don't think there's any amount of little haptic buzzing that's going to replicate that. So I don't think the technology is necessarily there for translating existing 3D stuff into 3D.
the VR world yet. And finally, we need to walk before we run.
I've actually heard this in a couple of other talks. But the idea that, you know, when film curse came about, we had to learn the new vernacular of film, the concepts you could translate in film that wouldn't work on a stage. How to interpret stage directions from Shakespeare.
between a Baz Lerman production of Romeo and Juliet versus an original stage production. They're vastly different and it took time to learn the new techniques and it's going to take time to learn what new techniques we can apply from VR in 2D UI to enhance it, perhaps make it 3D, but that's going to take time.
I like to break down the concept of input styles into three different categories that are essentially layers. The first kind of style that you have is the console style. And this dates all the way back to the original Nintendo entertainment system where you have up, down, left, right, enter and back. And maybe you have a meta button for accessing the OS. And we've seen this style.
mirrored all the way up into current consoles. And for current console OSs, you still use these basic six functions, up, down, left, right, enter, back, to navigate the OS, to navigate menus and games, and it's very simple and easily conceptualized. It enables navigation. Above that, you have sort of the mobile style, and I call it mobile style because it's best exemplified by cell phones and tablets where in addition to simple navigation, you also have a point, a touch point on the UI surface, an XY coordinate that means the user is interacting at this point on the surface.
whereas console style lets you enter text by left, left, left, left, left, left, now I'm on the Q key and I can enter Q. Touch surfaces allow you to much more rapidly enter text. I still wouldn't want to write a research paper in it, but it's there.
And then finally, the top of the food chain, so to speak, is the full PC style input where you have a keyboard and a mouse. And this enables, in addition to, whereas the mobile enables dragging and drawing operations, the PC style enables massive amounts of text input and manipulation and also enables enhanced dragging and drawing operations. If you think about 3D manipulation programs often.
touching and dragging with one key press control will rotate or with shift will zoom. So it enables further kinds of input interactions. I also want to kind of divide up the kinds of UI that you can talk about in terms of internal and external.
External UI is something that is UI that's part of the scene.
screen on an ATM in an in world object is external UI. Whereas something like a browser window is something that's internal UI.
It's private to you. In a multi user or social VR experience, external UI is something that's going to be visible to everyone.
Internal UI is going to be visible only to you.
Most of the stuff I'm going to be talking about, most of the interesting problems I think have to do with internal UI, presenting the customizability interface to a user. I'm trying not to zoom through this but I'm also trying to make sure I have time for questions. So external UI.
external UI is conceptually easy in terms of rendering. You don't have to think about how am I going to position this relative to the user. It has a position in a scene. If you're rendering a cockpit and there's a UI element in the cockpit that has a particular place it goes and just as you're rendering the rest of the cockpit, you're going to render that UI element. Also, there's no conflict between the depth of the scene and the size of the scene.
that the object is an occlusion. I'm going to cover this in a little bit more detail later, but essentially if there's a UI element there and there's something in the scene that's closer to you than the UI element, then you just have to deal with that. That object obscures the UI element and you have to figure out how to get it out of the way. Maybe that's part of the game play. Maybe that's just where the user stuck their head.
As compared to internal UI, though, external UI is going to be harder for picking. If you have an arbitrary point in space and a ray projecting from that point, both for internal UI and external UI, you're going to want to be able to say where on the XY UI surface does that point intersect. Now, for internal UI, which I'll cover in a moment, you're directly in control of that. For external UI, the position of that UI surface is completely relative to where did the user stick their head and where is that in the scene. And you have to have some sort of picking logic for it.
Hopefully whatever rendering system, whatever rendering engine you use lets you say, okay I need to do the triangle intersection, I need to know exactly what point on this rectangle I'm hitting and you can translate that into xy coordinates.
Also for external UI, if you have multiple elements of external UI, you need to be able to find some way to manage where is the focus.
If I provide a user input, does it go to this element or does it go to that element?
Elite dangerous is a good example of gaze focus. It has multiple UI surfaces that you can activate by turning your head and focus on that and when you have that thing focused, that's where the input goes. You also have the same issue if you have both external UI and internal UI that are available at the same time.
One thing I wanted to note is when you're building external UI in particular, you have to be aware of making your users uncomfortable. The Vive controllers in particular, when you're doing picking, you have to be careful about picking. You have to be careful about tend to shoot the picking rays out of the top of the Vive controller. So if you're holding like this, basically the rays are kind of going along the axis of your thumb. But if you tried to use a touch screen with your thumb that's below you, that's going to be really painful.
You want to avoid anything that's going to bring the elbow up above the hand in order to do that.
Also, if you're just using sort of gaze activation, again, anything that's kind of below a few degrees of the horizon, it's going to put strain on the neck.
as you have to look down and especially if you are doing detailed gaze focus, you don't want to have to make your users have to make fine motions with their head as they're looking down. You want to kind of avoid having them crane their neck down if at all possible. So for internal UI, I'm going to you have to decide whether or not, this may be not your decision, but something that's just dictated by your application or game, is your UI going to be transient or persistent? A transient UI would be a menu you bring up to say, okay, I want to load this other level, I want to do this other thing. But a persistent UI might be you're in a social VR environment and you want to have a browser window that you can refer to while you're talking with this other person.
So is it going to be something where interacting with the 3D scene and interacting with the UI need to be essentially almost concurrent or is it going to be something where it's sequential? You're doing one or the other. When you're dealing with internal UI, you're going to have to be careful about you're directly in control of how that UI is presented to the user. And you may have a desktop metaphor with lots of different elements that the user can move around. You may have lots of individual elements that are each rendered individually or something in between. But however you have it, you're going to have to figure out a way of how do I inject these into the scene.
OpenVR and Oculus both have support for overlays or layers that they will inject into the compositing scene for you, into the compositing process for you.
But they're fairly limited in scope in terms of how you can use them.
they only, sorry, Oculus will only allow you to render to a flat planar surface. Open VR will mostly only let you render to a flat planar surface and then composite that in. It will also let you render one curved surface but you don't have direct control of the amount of curvature. And most of the feedback I've seen, most of the...
most of the talks I've seen when they're talking about UI, really recommend that you want to have a curved surface. That's ‑‑ sorry, getting off track. When you have these curved surfaces, internal UIs that are projected into your scene but are not part of the scene, you have the potential for occlusion or depth disparity, one or the other. Generally you do not want to render a UI closer to the user than at arm's length. It can become tiresome because the depth cues between accommodation and convergence will disagree. So you want to have the UI large and far away.
but when you have a UI that's large and far away, that also means that elements in the scene might be poking through the UI.
And you can either have that be a situation where the elements poking through the UI then occlude the UI, limiting its usefulness, or you can have the UI simply overlay the 3D scene.
The problem with the latter is that if you have the UI overlay the 3D scene, then you get depth cues. If there's an object that's poking out below it and one of your depth cues is telling you that object should be closer and your other depth cues are telling you no, this overlay is closer because it's obscuring the other object and that can cause some discomfort.
If you're creating a desktop metaphor where you're actually creating an entire windowing system that requires you to do a lot of work right now, there are UI libraries out there that will render UI for you. But generally they won't do a windowing system for you. At most you will be able to render a bunch of individual elements and you'll have to create the concept of window decoration or Z order, things like that.
and then finally on internal UI, don't ever pin the origin of the UI directly to the viewpoint. If you want to make text more readable or you want to make various parts of the UI more accessible, it's important that a user be able to lean their head.
left and right or lean in and get closer to the UI to gain greater clarity on it. You can also get about an order of magnitude more readability out of text even if you can just move your head in almost tiny amounts and see the text change you can make out the text shapes better. So compositing and projection. So that's my thought on that. Has anyone seen one of So when you have internal UI, you need to composite it into your scene and you need to choose a projection. Like I said, the existing SDKs have support for overlays but they're very limited. They will only allow you to do compositing over the scene. They will not allow you to do compositing into the scene where the UI can be included. Whatever projection that you choose, is going to lead to some kind of distortion of the objects. And when you're doing picking against a projection, the more complex the projection that you use, the more complex that mapping from a ray, an origin in a ray direction to an XY coordinate on the UI is going to be.
Open VR does help you in that, in that they actually have an API that says given this ray and direction, what is the XY coordinate on this overlay service that I'm hitting? So for the issue of occlusion versus depth cues, there isn't really a general purpose solution.
If you're using a transient UI, you can optionally fade the 3D environment to a mono environment while you have the UI open, which eliminates any potential for competing depth cues. But the fade effect itself, whether it's immediate or over a small amount of time, can be discomforting.
The other option is you can call the problematic objects, but calling objects can result in like bisected avatars or important items missing from the scene. For our purposes at the moment, what we just do is live with the competing depth cues.
integration details. What are the requirements for a UI library that you want to use to integrate into your 3D application? It has to work on your target platform.
This may seem like a no-brainer, but you have to think about the platforms you're using now and the platforms you're going to be using in the future if you're eventually thinking, well, we're going to port this to Android. You don't want to have to use something, you don't want to be using something where you're going to have to rip out all of your UI and rewrite it.
you have to use something that has capturable output. Ideally you want a piece of software that will render a UI directly to a GPU surface, a texture, either a direct 3D texture, a Vulkan image or an OpenGL texture. Less ideally, you could use something that will render to a...
image in the CPU memory and do the transfer every time the UI changes, but that's going to be consuming a scarce resource, namely CPU, GPU transfer and GPU processing power.
And finally, your library has to be able to accept programmatic input. Most UI systems just accept native events from the OS or from whatever framework you're using. You want to be able to tell the UI library that you're using, I don't care where the OS says the mouse is, I don't care what the OS said the keyboard input is, I'm handing it to you. Because you're going to need to be able to do translation.
desirable for a U.I. library. You want it to map to a set of U.I. controls that are familiar to everyone. The check box, the button, the slider, the scroll bar, whatever. You don't want to have an in-house designer who comes up with a U.I. design and then discover that it's going to be a gigantic pain to implement because your U.I. library doesn't really support all of these common features.
you want to have good tooling support. The ability to have lots of easy to use tools to build your UI. Things like Android provide that. Most UI systems have some sort of UI mockup mechanism. If it's something that provides good HTML rendering, then that's automatically going to hit the first two points.
and you want it to provide a rich focus model. In particular, it's valuable to have knowledge of when the UI is focused on a text element in order to provide in world on screen keyboard elements. So the UI libraries that I've worked with, the I've mostly worked with QML in the past couple of years. That's part of the QT framework. It hits all of these notes. QML will only render directly to an OpenGL tech... I'm sorry. QML is a general purpose library. You can build a native application with it and it will render to the screen. In terms of rendering to an off screen surface, it only supports OpenGL.
Crazy Eddie's GUI and lib rocket are both more special purpose libraries. They're targeted at game makers who want to build some sort of UI for their application. They're a little bit more limited in terms of their features, but they will both render directly to open GL and to direct 3D. So, I'm going When you're dealing with input, most UI libraries are typically built around the concept of keyboard events and mouse events. So if you have controllers, the RIF controller, the Vive controller, the Oculus remote, you're going to have to have a translation layer there.
sorry. There we go. Keyboard events are fairly simple.
Usually there's a straight one to one correlation between whatever keyboard event your framework supports and whatever keyboard event the UI library will support. Mouse events are a little bit more tricky. You usually need remapping of some kind. But naive remapping can also be tricky.
One of the issues is in VR, you might have a very large off-screen UI surface that's acting as a kind of virtual desktop.
So if you have a 4,000 by 2,000 UI surface and you have an on-screen window that's 100 by 100 or something more reasonable, the mouse movements are not gonna map one-to-one.
So it's better to keep a sort of virtual mouse position specifically for the off screen UI and use delta mouse events to move that around rather than try and do an absolute mapping.
Hand controller picking into the UI surface is tricky. As I said, open VR has helpers for converting.
If you use a simple projection on which to put your 3D, on which to put your 2D surfaces, then you should be able to create a simple mapping between 3D rays and the offscreen surface.
So for output.
You've got two levels of rendering. You have one process, you've got one, you have your UI scene which you need to render, your UI that you need to render and you have your scene that you need to render and you need a step to composite them together if you're not relying on the SDK to do that compositing.
You want to make sure that you use a rotating buffer of output textures.
You don't want to just use a single texture for rendering your UI to the texture and then compositing it into the scene and then waiting and doing that again.
Because you need one version of the texture which is being used for reading, one is being used for writing, and one which is in flight.
So typically you want a rotating buffer of at least three textures to hold your UI.
Ideally you want a distinct rendering context for your UI.
You don't want the UI rendering to be blocking the main rendering. You don't want the UI rendering to be changing the state of the open GL state machine or the direct 3D state machine while it's doing that.
and then synchronization can be a pain. If you're familiar with working with fences and passing textures between multiple open GL contexts, you have to be very careful about fences and flushes and making sure that even though you've completed all the commands to render the UI, the GPU hasn't necessarily finished the work, so you need to use synchronization primitives to protect against rendering corruption.
So tips for designing UI for VR.
You want to go big with your elements.
I recommend designing as though you had a fairly low resolution screen compared to current screens.
You want to avoid controls that require fine motor skills.
One of the recommendations I have is that if you have a touch screen available, design your UI and then practice using it at arm's length.
with the touch screen. That's going to reasonably accurately simulate the kind of accuracy you're going to have when you're using hand controllers and a pointer. And be cautious with transparency.
It might be very tempting to say, okay, I've got this 3D scene, I want to make sure that even though the user has a browser window up, they can see the 3D scene partially behind it. This ends up becoming a distorted mess. You can't really make out the text and you can't really make out the 3D scene. You don't really have any control over what's going to be behind it.
necessarily. So avoid transparency particularly in elements that are going to be containing text or might contain text. It's fine if you want to use partially transparent elements for window decorations. But Also, in terms of avoiding fine movements, you know, compare a number field that has little spin box controls that are wedged in there. You want to avoid that if you want something that moves a number value up and down. Try and lean towards a giant dial like you would see on a tuner.
Yeah. Okay. Also, more tips. You want to optimize for multiple kinds of inputs. And in particular, you want to look at the inputs that the user is actually using at the moment. I found a number of applications when we first started working with Oculus touch.
It worked with Steam VR applications but a lot of the Steam VR applications were working from the assumption that you had touch pads, not joysticks for the analog controls, which meant some of the tools were just much harder to use than they needed to be because they expected you to be able to really finally control the XY coordinate of the joystick in a way that's easy with a touch pad, not easy with the joystick.
And also, I can touch type. So, you know, if you've got a text field, don't force me to use the on-screen keyboard. Just try and be aware of, like, different people are going to use different inputs and try and put in the extra effort to customize the functionality of your application for whatever kind of input they're using.
And then finally, you want to minimize friction that they have to encounter when they're switching focus between the UI and the world and vice versa. I don't want to have to hit a button or jump through a bunch of hoops in order to be able to look at a browser or look at the person I'm talking to. I want to be able to just turn my head. So...
that's pretty much it. High Fidelity is hiring. You can contact me on Twitter or e-mail with any questions you might have. Also, we do have a little bit of time for questions. So if anybody has anything they wanted to ask, please step up to the mic.
of things translating over to augmented reality. It seems like a lot of the lessons are sort of one for one. Do you see any big differences? For augmented reality, I kind of feel like Sorry, I've got something caught in my throat.
The question is how do I see them translating into augmented reality?
I kind of feel like in augmented reality, you expect to see even more UI elements, but they're going to be sort of somewhere on that spectrum of internal UI to external UI.
They're kind of gonna be attached to things.
That's what you would expect from augmented reality.
I see a person walking down the street and they've got a tag floating off of them with.
you know, hey, I'm this person, you know, here's my family's names and here's how I should greet you and here's how long I've known you. So I would expect a lot of the same information to apply. I don't, I'm not sure if I'm going to I kind of feel like augmented reality is providing rendered information to you but doesn't necessarily expect you to provide a lot of feedback into it. Certainly you'll want to have some sort of UI for overlays and there will probably be a lot of the same...
same information you'd want, sorry, the same topics would apply to augmented reality in terms of like the integrated UI.
It's like, oh, I'm going to activate this menu and I'm going to bring up this overlay on the real world. So I'd be mostly focused on the things I was talking about in terms of internal UI.
Thank you.
Okay, well, thank you.
