Hey guys, my name's Ben.
I'm the lead AI developer at Havoc.
Hi, and my name's Alin, and I'm a PhD student at the University of Pennsylvania.
Alin is graduating in a few months, so if you like what you hear today, then you should totally hire her.
So first, let's talk about why people don't use machine learning, because there's been a lot of resistance to this in game AI.
People say it's too slow to execute.
You can't really afford it in the context of a real-time game.
They say it's too opaque.
You can't figure out what it's doing when it does something wrong, so you can't correct the weaknesses.
And of course, it's too unreliable.
It doesn't get the right answer often enough.
Is machine learning slow?
Well, this is an autonomous helicopter.
It is simultaneously flipping in place and pirouetting.
It's a move known as the chaos.
Accomplishing this crazy difficult feat requires adjusting four separate control settings 50 times a second.
What about opaque?
These are the probability models being used by a simple handwriting recognizer.
You may notice they basically just look like letters.
There is nothing opaque about this model.
If you want to know why C and E are being classified poorly with respect to each other, well, that's because their probability models look a lot alike.
As for unreliable, this is the Lifeline Auto.
It's an automatic defibrillator similar to those positioned throughout Moscone Hall.
This device uses a ML model of cardiac rhythms to diagnose ventricular fibrillation.
This box literally makes life and death decisions based on ML.
It does a pretty good job.
Here's a cup of cold coffee.
The reason game AI people have had such a bad experience with ML is not that ML is inherently bad.
It's that we as programmers don't automatically know how to do good ML.
For most of us, our exposure to ML in school was basically limited to neural networks and genetic algorithms.
These things are easy to understand, they're cool, and you can program them without knowing much math.
Statisticians in school learn about clustering and component analysis and decision trees and boosting and bagging because they have the background for them.
The lesson is, if you really want to try out ML, you'll need to step outside your comfort zone and learn some unfamiliar concepts.
It's potentially very rewarding.
ML is at its best when solving problems that you have no idea how to code up because ML is all about understanding the underlying process for you.
ML can help you refine your AI automatically, rather than by endless knob tweaking and testing.
It'll help you build NPCs with varied personalities, based on captured gameplay data.
And a whole lot more.
Like a lot of tools, until you really know how to use ML, you can't get a full appreciation of all the things you can use it for.
All right.
So before we continue on, let's go over some terminology.
So first off, the primary goal of any machine learning algorithm is generalizability.
As Ben mentioned previously, if we understood the process well enough to just code it up directly, that's what we would do.
We would just code it up.
So what we're trying to do instead is based on examples we have already, we're trying to learn a model which allows us to predict, classify, or cluster based on new examples that we've never seen before.
And so how do we do this?
Well, in general, there are two steps to any machine learning algorithm.
The first step is the training phase, where given the examples that we have, we try to fit the model so that it explains those examples as well as possible.
And then once we have that, we can then see how well our model works on new examples that it's never seen before.
So what do we mean by models?
So model is a very generic term, and it's just the representation of the underlying process.
And its job is just to encode how the input should relate to the output.
This, under the umbrella of machine learning, there are many, many, many different models that are tools that you might use.
So for example, decision trees and k-nearest neighbors are two techniques that Ben's gonna talk about in a little bit, but it also includes things like linear regression, neural nets, Naive Bayes, support vector machines, and many, many, many, many more.
So let's just talk briefly about features.
So the machine learning inputs are often called features.
And a lot of times, you're going to have many different features that you're then going to collect together in a big feature vector.
And so again, features are very domain specific.
They're going to look really different based on the applications that you're working on.
So to give an example, if you're working with image data, you have here a 32 by 32 pixel image.
You might flatten that into a 1 by 1024 feature vector.
Alternatively, suppose you're working with animation data and you have motion features.
So one thing you might do is just put all the key frames, one after each other, in a big feature vector.
And then each key frame is itself just a list of joint rotation values.
So that's just a big vector of real numbers.
Suppose you're working with text.
A feature vector you might use is a series of word counts.
So for example, each element of your feature vector will be the number of times a certain word appeared in the document that you're analyzing.
And then in addition.
just a notational thing, once we have everything in these big feature vectors, it's very convenient to then put them into big matrices.
And these matrices might represent a training set, or a test set, or new data we've never seen before.
And just the idea here is each feature vector is just stacked one on top of each other, so that each row is an example, and then it just follows that each column is a feature.
And then briefly, just to talk about the term labels, the output of your machine learning algorithm are often referred to as labels, particularly for classification.
And so some examples, you might see your gesture type is fraudulent or is spam.
So the types of things that your machine learning algorithm's trying to predict or classify.
And similarly to your feature vectors, we might want to combine all these things together into a convenient matrix form.
So here, each row is the outcome for a particular corresponding example.
And that sums up some terminology.
So now I'd like to cover a small number of techniques that I think are particularly worth knowing.
This is nowhere near comprehensive.
It's not all there is to ML, but it'll be a good start to understanding the various tools that Now there's three major areas that machine learning tackles.
Supervised learning, where you train the system to answer questions.
Unsupervised learning, where you uncover patterns and relationships in the data.
And finally, reinforcement learning, where the system learns how to behave optimally in a particular task.
I'm going to focus specifically on supervised learning, which I think is the most widely useful area.
Most of the problems you have in ML will turn out to be supervised learning problems.
First of all, decision trees.
Decision trees are one of my favorite ML techniques, specifically for game AI stuff.
They're beautifully simple in their operation.
There's almost nothing non-obvious about them.
Here's a decision tree that an NPC might use to determine whether to attack or retreat.
It starts at the root node, and at each branch, it goes up or down depending on the answer to the test in the node.
So first it checks whether its hit points are above 50.
Above 50, it goes up.
Below 50, it goes down.
If it goes down, then it checks whether the player is stunned.
If it's stunned, he attacks.
Otherwise, he retreats.
And if the hit points are above 50, he always attacks.
So nothing weird here.
This is just two if statements.
The real power here is in having the computer build the decision tree for you.
Say you've got this big table of example play situations that your designers or your testers came up with.
In this situation, with these hit points and that hair color and this and that and the other thing, the correct decision was to attack.
Looking at this table, it's not easy to see which factors are important and which never matter and which are only important in certain special situations.
But a decision tree learning algorithm will take this data set and spit out a decision tree like this one.
Decision trees are what's known as a white box algorithm.
You can look at a neural network's weights all day and not have any idea why it's making some particular classification that you think it shouldn't be making.
It just isn't questionable.
But everything a decision tree does is clearly exposed.
If you want to know what it was thinking when it gave you a particular result, you simply follow the branches it took from the root to the leaf, seeing what factors were important, seeing what it branched on.
And at each node, you can even see what training examples were relevant to splitting the tree in such and such a way.
And although machine learning people wouldn't normally do this, you can even manually tweak an automatically built decision tree.
If you think some branch is stupid and it just came out from a weird quirk in your training data, you can just go ahead and snip it out.
The algorithm most commonly used for decision trees is called ID3, and it's really easy to wrap your head around.
It starts with the whole training set, and you decide which question you can ask to separate the different output labels as cleanly as possible.
In the attack or retreat case, hit points were a much better predictor than, say, hair color.
Knowing just the hit points didn't always tell you what you should do, but it did give you a pretty big hint.
Then you separate the examples based on that feature.
Then you make those subsets children of the decision node with that test in it.
If all the labels in a subset match, as you can see on top, all the labels are attack, that becomes a leaf in your decision tree.
Otherwise, with the remaining examples, you look for another feature, and you keep dividing.
For continuous thresholds like hit points, you need a way to choose a split threshold, like the point at which things get divided.
A really simple approach is to try splitting at a few random different thresholds and see which one works better.
There are more clever approaches you can use, but it gets into some fairly deep stats.
Decision tree learning isn't perfect.
It can be difficult to tune the complexity of the trees.
Overcomplicated ones will fixate on irrelevant features once they get down near the leaves, while oversimplified ones will ignore genuine special cases entirely.
And they're not very good at discovering relationships between continuous features, like if you should retreat when your hit points get below the player's attack strength.
Nevertheless, the simplicity and the transparency here can be a huge benefit.
If you only remember one technique from today, let it be the ID3 algorithm for building decision trees.
On to the nearest neighbor algorithm.
With this one, there's no real training process.
The model is the training data.
When it needs to make a classification, it just determines, say, what's the most similar training example it's seen before based on the numbers in that example being close to the input numbers according to some measure of distance, which I'll talk a bit about later.
And it just uses that example's label.
It's dead simple.
nearest neighbor by itself is really prone to overfitting.
If there's any problems in your training set, any examples that maybe should have been labeled differently, that's going to show up in your model as classification errors.
This test point should probably be yellow, but it happens to be closest to that red dot.
So what's always done instead is called k-nearest neighbors, where you take the closest, say, five examples and let them vote on what the label should be, maybe giving more weight to closer examples.
What just happened here?
Oh, I think I went to the last slide.
The higher the K is, the more it smooths over errors in your training set.
But, it makes it harder to respect genuine special cases where the classification should differ from other nearby examples.
KNN has some problems of its own.
The big one is dimensionality.
If your data set has only a few dimensions, you can do things really efficiently with techniques like KD trees.
But in higher dimensions, these things don't work very well.
You end up basically doing a linear scan through all your data.
And that can get really slow.
There's also a subtler problem where in really high dimensions, a few training examples more or less take over the entire data set.
A little go for dimensionality later on.
The other thing is, you need to decide what distance means.
If your features aren't all in the same units, and generally they're not, then you need to scale them appropriately.
Lynn, again, will talk about that later.
And distance gets even more difficult to work with when your examples aren't continuous, but rather categorical data.
What should the distance between orc and goblin be?
Which one is closer to horse?
It's a weird question to ask, but you need an answer for it if you're going to do k-nearest neighbors.
So KNN can give you really high quality results, but it's limited to low dimensional feature spaces, and you'll want your training data to be really clean.
Genetic algorithms, which is really a whole spectrum of techniques, but they all have a few things in common.
The basic idea is to find a good solution over time by trying out a bunch of potential solutions.
They get tested out against some black box fitness function, which decides how awesome each one is.
The ones that do well get to pass on their genes to the next generation of solutions.
Usually the genes are randomly changed a little bit, and you keep doing that until you breed a race of super solutions.
This isn't exactly an ML technique specifically because it's not necessarily about learning from data.
But if you're trying to build a really good model, you can use your test set as the black box and have the genetic algorithm mess around with the parameters of your model to make it as good as possible.
There's a couple of decisions to be made when developing a genetic algorithm.
The most important one is coming up with the rules of how genes get passed down to the next generation.
A really common approach is called roulette wheel selection, where you randomly pick genes directly weighted by fitness score.
So, if model A scored twice as well as model B, then each gene is twice as likely to come from A than from B.
Roulette wheel selection relies on a well-behaved fitness score.
If it's possible for one contestant to rack up a really high score compared to slightly inferior contestants, then those other contestants' genes are just going to disappear in the next generation, even if some of them could have been useful when combined.
So an alternative is rank selection, where instead of using the score directly, you weight the selection based on what place each contestant came in.
So, say, half the genes come from the first place solution, and a quarter come from the second place solution, and so on, regardless of what the raw scores were.
This avoids that crowding out effect, particularly in the earlier generations, when things are pretty random, and some people just happen to be cooler.
It does, however, tend to slow things down.
It takes more generations to converge to an optimum.
Genetic algorithms also aren't always the way to go.
The big thing is they don't really know anything about the problem they're trying to solve.
They're just banging at it.
If there's an optimization method which is specific to the model, like ID3 for decision trees, it'll usually be the genetic algorithm in terms of speed and precision.
The other problem with GAs is they're a whole extra panel of knobs and buttons to mess with, which isn't necessarily what you want.
It can be really time-consuming to go through the results of a genetic algorithm to try to figure out why it did what it did.
So tweaking it is usually a try-and-see sort of thing, which is obviously not great.
Genetic algorithms, though, are really flexible, and it can be tempting to think of them as the solution to every problem.
But I think they're best used as a backup plan once you've done everything you could with more model-specific techniques.
OK, so now let's talk about all the different things that can go wrong, starting with the wrong features.
Because having good features can really help your model perform better.
And it's not a cut and dry process.
So here's a situation you might see.
You've tried a lot of different models, but you keep getting disappointing results.
So what should you do?
So the first thing you should do is look at your data and do something called exploratory data analysis or EDA.
And what this is gonna do is let you check all your assumptions.
So you can basically see which of your input variables actually seem to have a relationship with the outcomes that you're interested in.
And also you'll be able to see what the shape and distributions of the various features you're interested in are.
Then once you've done that, you can now boil down your data.
So you can remove irrelevancies such as input features that have nothing to do with the outcome.
Or you might even target some of your features.
For example, cropping images so that the model can just focus on the aspects of the features that are probably most relevant.
And then once you've done that, you can even look at your data some more and check whether transformations of your data help.
So for example, you might see whether linearizing your data or normalizing your data can actually help.
help your model work with your features.
So here, you know, we can apply a log transform and now we have something that looks like a normal distribution.
Or conversely, if we're working with images, we could desaturate the image so we work with single intensity values instead of a triplet of RGB numbers.
And furthermore, we might even try blurring it so that the changes between pixels are gradual.
And another thing we might do, just in the spirit of transformation, so this is something that's come up a couple times in this session, is make sure all your features have comparable scale.
And it's really easy to have an intuition about why this needs to be true, particularly if your algorithm is based on distance metrics.
Here's an example. So like suppose you have a feature, and it's got three, uh, feature vector, you've got three features in it.
Weapon power, player level, and gold amount. And suppose gold amount has a range that's much, much greater than all the other features.
Any distance metric that's based on this feature vector is going to be completely dominated by gold amount.
So what should you do? You should scale down gold amount so that it has basically the same range as the other features.
In fact, you might want to renormalize all these features so that they lie between 0 and 1.
So here's another thing you might see.
You're feeding in 50,000 features, and your classifier sucks.
Furthermore, it worked better when it was only 100 features.
So what might be going on?
Well, it's very likely that you've come across the curse of dimensionality.
And so the issue here is When your features become very, very, very highly dimensional, everything becomes far apart.
Like, your intuition about distances don't apply the same way they do in three and four and small dimensions.
And the other thing that's a problem is, as your feature space grows, you need more and more examples to understand it.
So like, for the same size training set, you get more bang for your buck when you have fewer features than when you have a lot.
So what can you do about this?
Well, you can try reducing the dimensionality of your data.
For example, you might use a technique like principal component analysis.
And so what PCA will do for you is it can take a big vector of possibly correlated features and then reduce them down into a small number of features that are uncorrelated.
And so here's an example based on walking data.
So.
Classes of motion work great with PCA, because a lot of the joint movements tend to be correlated, like the arms and legs move together.
So here, we started with a feature vector that had 540 features in it, and PCA reduces it to 29 features.
So here's another situation you might encounter.
You have insanely good accuracy in the test set, but the model is still terrible in practice.
So here, one possible problem is contamination.
Just some of your test data snuck into your training set, and you should just fix, it's a bug, you should just check and fix your code.
Another thing that might be happening, which is a little more subtle, is data leakage.
And here what's going on is you're using a feature for prediction that's essentially cheating.
Like a feature that you wouldn't normally have available for prediction.
And so this is something that can particularly happen if you're working with log file data, for example.
And here, you know, suppose you're trying to predict score from weapon power and player level.
If score is a function of kills, so that the more kills you get, the higher your score, you don't want to use the number of kills for prediction because that's essentially cheating.
You don't have that data.
And the next possibility is sampling bias.
And in this case, the training data is just not similar enough to the real world.
And basically, decisions of how, what, and when you log data can really matter.
And just to kind of give some intuition about why this can happen or why this is true, just think about the behaviors of players.
Like, the players who log in every day likely behave really differently than the players who log in once a week.
So now let's talk about when things go wrong with the wrong model.
So here's the situation. You've tried a lot of different features, but you have disappointing results.
So...
What's a good solution? Well, trying a different model.
Or actually, you can try lots and lots of models.
And there are tools like Weka, which can come to the rescue.
And so in particular, Weka will let you, like, input a big matrix of features and then run it on lots and lots of different models and give you the results.
And it even has tools for clustering and preprocessing, too.
The other thing you might try is using an ensemble of models.
And some names for different ensemble techniques are boosting, stacking, bagging.
And the basic idea of all of these is a bunch of weak learners working together can often outperform a more sophisticated learner.
And to give an example, large ensemble models were the best performers in the Netflix prize a couple years ago.
And so now let's talk about overfitting, which is another thing that's been mentioned a lot in this session.
So here the situation is your classifier has amazing accuracy with the training set, but performs poorly on data it's never seen before.
So what's happening?
Basically what's happening is your model is memorizing the training set and has lost its ability to generalize.
And this is just part of a broader challenge when you're developing and designing your models, where you need to balance having a model that's.
too simple where the data can't capture the patterns that you want to capture.
Like, so for example, having a decision tree that doesn't have enough depth, using a linear model when it's not appropriate, or using k-nearest neighbor with too many neighbors, versus the other extreme where you have a model that's way too complex, it has too many parameters you can fit, too many bells and whistles.
and you get these schizo fits with no ability to generalize.
And so this can happen when you have decision trees with arbitrary depth or using only one nearest neighbor or using a two high dimensional spline to fit your data points.
So some overly fitty algorithms, again, things that have been mentioned today, k-nearest neighbors with low k, artificial neural networks with lots of neurons, decision trees with arbitrary depth and ensemble models.
So how do you protect yourself against this?
Well, using cross-validation is great.
So what this lets you do is estimate how well your model performs on new data.
And how does it do that?
Well, by holding out, it lets you hold out subsets of your training data, which you can then use for testing.
And while you're doing this, you can try different model parameters to determine the balance between, a good balance between simplicity and power.
So just let me quickly show you how it works.
It's very simple.
You split your examples into training and test sets.
So here we have 90, 10 split.
Train your model.
Then you evaluate your model just on your test data.
And then you redo that with totally new training and test data.
And then you do the, and then the average over multiple test sets is your actual estimate of how you're going to do on new data you've never seen before.
So now to sum up.
If you've fallen asleep during this talk, please wake up and internalize the following points.
Contrary to what may have been your experience in the past, ML can be real time.
It can be transparent.
It can be totally reliable for your needs.
It can be the best way for you to do your job.
But in order to get good results out of it, you need to step outside your comfort zone.
You need to take off your programmer hat.
You need to move beyond the very small and limiting number of algorithms you've learned for machine learning, such as artificial neural networks and genetic algorithms.
You need to understand at least some of the theoretical underpinnings of features and models.
And you need to be aware of things like overfitting that really bite us all in the butt when we're doing this stuff.
If you want more information about this, there is a free online course on machine learning offered by Stanford University and a couple of other good reads in this area.
Thank you for your time.
