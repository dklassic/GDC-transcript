Thank you for coming today.
My name's Camille Chiu.
I'm the art animation lead at Monolith Productions.
And with me today is Sean Peel, our advanced animator.
We're going to talk about the process of creating the multi-character combat animations in Middle Earth Shadow of War, or what could also be known as the pursuit of murdering a lot of orcs.
So if you have any adversity to violence, there is a little bit in this show.
So feel free to vamooska if it offends you.
So our goals today are just to give you a little insight behind the curtain and what we do in the monolith pipeline and share knowledge.
So hopefully you come away with a couple of nuggets you can use.
I'll start off with talking about what Shadow of War was and what the multi-character combat animations were to us, our goals in achieving those animations and the challenges they presented with us.
And then we'll talk about a plan of attack for overcoming those challenges.
And then I'll pass it off to John to do an animation tools process and deep dive.
So Shadow of War was an action RPG released last year by Monolith, and it was the sequel to Shadow of Mordor.
The player controls two characters set in Tolkien's landscape, Talion and Celebrimbor, in their conquest for Middle-earth and their fight against Sauron.
And so in case anybody is unfamiliar with the content, we created a quick video to familiarize yourself.
Yay!
There was a lot of animation in that video, but what are the multi-character combat animations in there?
So, particularly what we're going to talk about today is what Monolith calls our Sync Actions, which is any action in the game in which we snap the two players together in order to perform two animations in perfect alignment and synchronicity.
We use this a lot for motions where we want the characters to grapple with each other or get a particular weapon into a certain part of the body part.
And they're very different from the non-sync actions you can see on the left.
Talion would look perfectly great performing that action all by himself or if there is an orc in the vicinity.
Whereas he's going to look really funny if he performs the one on the right by himself.
So to understand why they were important to us, I just want to talk about some of the goals that we had in creating those sync actions.
The two of them that were really important to us was that we wanted to take the player into the heart of darkness.
We wanted them to be able to become the monster to the monsters.
And the sync actions allowed us to explore some really brutal and grotesque performances.
So it allowed us to really take them to that monstrous area.
And then in the consumer values area, we wanted the player to really experience visceral, lethal combat and predator gameplay.
And so when I first heard that, I thought I understood what it meant, but it really is because of the tone that people would say visceral as.
I need the combat to be visceral, and I need the animation to be more visceral, Camille.
And so around the 50th time that I heard that, I realized that I should go to the internet and find out exactly what that means.
And so it means coming from strong emotions and not from logic or reason.
We wanted to evoke strong emotions from the player when playing this game.
And we had a lot of birds in flight in order to do that.
Some people might have heard about the nemesis system.
Chris Hoagy is going to be giving a talk later this week.
It's awesome.
He's going to deep dive into it.
But in a nutshell, it's a.
A system designed to provoke strong player emotions through these long-term interactions with the orcs.
Player commitment and time will allow them to interact and interface with these orcs in special ways and evoke those strong emotions.
But it was a very logical system.
And we want to be able to drop the player in the game and basically get to see any of these faces on them within the first 10 minutes of playing combat.
So how are we going to do that?
So we took a lot of inspiration from the Arkham formula.
And here's a video, just basic combat, player hitting X to attack, Y to counter, B to stun, A to dodge, and this felt really great.
It was really fun, but it was missing that spike, that moment to really bring the player into the moment.
And so here's a comparative video with the addition, same combat, but with the addition of the sync actions, the execution pool. So you can see it really added this moment to break up the rhythm of combat and had that crescendo moment to dive that camera in there.
and really allow that player to experience that brutal, monstrous, visceral moment in combat.
And we knew that we had hit that moment of success when we were in a company play-through and a new sync action came on the projector and everybody in the room goes, ugh.
And so we felt like we had found a pretty good recipe to success and the sync actions were definitely going to be a key ingredient, but they presented some challenges.
Multi-character combat sync actions are expensive and time-consuming to create.
From a content creation standpoint, it's understandably going to take a lot longer to animate those two characters struggling against each other than that basic swing.
From an integration standpoint, it's going to be a lot more intense on everybody down the pipe, too.
The designers to integrate it, the camera team to present that animation, the audio team to really accentuate that moment, and the effects team to make all that custom effects.
Additionally, it's going to be very hard to, it's going to have a lot of things that can go wrong with it.
What if the player is against a ledge or one of the characters start on a box or the terrain is uneven?
There's a lot that can go wrong with those sync actions.
And so we had to dedicate quite a bit of time and make sure that they worked.
I think it was probably one of our lead engineers last three months of the pipeline, just troubleshooting and making sure those things look sweet in game.
Additionally, our team is not the biggest, we're not small, but we were pretty small in comparison to some of the games that we were trying to compete against.
And we're going to need a lot of them.
We shipped with around 1,200 between just the player and the orc alone.
And we had to support a lot of different entry conditions.
Our presentation team established early that they didn't want to use camera cuts because it was too jarring to the player to have that camera ripped away from them.
So if we animate the player grabbing the orc by the front and stabbing him in the belly, but the player attacked him from behind, it's going to look really jarring if we whip that.
that work around for that sink action. So we're going to need to support sink actions from the front, from the back, if they're on all fours on the ground, the left, right, front, back.
We're going to be able to attack them from above and below. There's going to be a lot of entry conditions that we have to support.
Additionally, we're going to need a lot of variety.
We're creating these animations with the intent of being memorable.
So they're going to get really repetitive if we don't have a huge amount of variety.
The player can trigger these off pretty rapidly in gameplay.
So a lot of variety in order to avoid the repetition.
And we need to support all aspects of design.
Design doesn't just want this in the combat pool.
They're going to want it in range, and they're going to want it in stealth.
They're going to want the player's abilities to have these as well as they enter through progressions.
They can feel more powerful as they play.
And the player's going to have lots of different weapon types, and the orcs are going to have lots of different weapon types.
So we're going to have to support all of those.
Overall, with 1,200 animations between the player and the orc alone, if they take around three days to produce, the animation team is going to be doing nothing but sync actions between the orcs and the player alone, and obviously we need to make the rest of the game within three-year period.
So we need a plan of attack.
How are we going to make this happen on a production standpoint, a scope standpoint?
And our first plan of attack was motion capture.
I know there's probably a contingent of animators that motion capture is not the most exciting plan of attack because I was one of them many years ago.
But I'm here to say no motion capture really is awesome.
When I first came to Monolith and I saw how they were utilizing the motion capture systems, I was really in awe and I became a convert.
A single strike animation or a single run animation was always very precious to me.
And to make that beautiful is beautiful and it's a work of art.
But what's also beautiful is to make that run look really good in game, all the players jamming left and right on the controller and diving that player up and down a hill and juking left and right.
At any point, our player run animation can be blending between eight to 12 other animations in order to make the player run smoothly.
We couldn't have done this without motion capture.
The motion capture allowed us to support the movement.
The player movement was over 2,956 animations alone.
So a single animation is not your art.
The game is your art.
Having that animation actually play well and support the player experience, look and feel good, that's amazing.
And the motion capture is just another tool for creating your art.
So.
I'm not here to convince you about motion capture though, we're here to tell you about what we did with motion capture that helped us achieve our goals.
And so one of the things that we did that was unique at Monolith is that our motion capture studio was on site.
And this made all the difference.
Benefits are exponential. We didn't need to book any shoots in advance if anybody's ever worked with an off-site Studio, you know that you have to book months weeks in advance and usually you have to book at least a day Maybe you're half day if you're lucky So we could shoot and capture at any time We could have a meeting with our designers in the morning and then the animator could animate and capture later that afternoon.
They might have the data the following day and have that animation over to the designer for integration later in that 48-hour cycle.
So we took that concept to integration time and really were able to compress it, which meant we got the animations in-game faster and we got to iterate more, which is better.
Additionally, we got to do smaller, more focused shoots.
This really alleviated some of the pressure on the actors.
Even if you hire a professional actor or professional athlete, around hour three, four, they're going to be really tired if they're performing some really hefty combat actions.
So because we could do smaller shoots and we could break them up consecutively into days, our actors' performances were better.
And additionally, it minimizes the time-consuming pre-planning required for off-site shoots.
If you have an off-site shoot, you'll probably have the actors, maybe some expensive actors in the room, and you're going to have the directors.
Then you're going to have the mo-cap team, and then maybe you might have some stuntmen, and then you might have your combat choreographer, and some safety personnel and caterers.
So it quickly becomes very expensive to be in that room.
So you're going to spend a lot of time outside of that room making sure that you have everything down.
because in that room is not the time for you to be figuring out your choreography and your plan.
Because we had our shoot on site, we could do small shoots.
They were very focused and our pre-planning process was very short.
And sometimes we even shot and pre-planned in the motion capture room because everybody that we needed for information was on site.
So if we forgot, if the sword ended in Talian's hand or if he was supposed to be sheathed at the end of it, we could just stick our heads out the room and ask the designer that was down the hall.
So the other thing that was super unique about Monolith motion capture pipeline is that our animators are the actors.
So I know what you're thinking.
And you wouldn't be wrong.
Usually normally a studio would hire professional martial artists to perform their combat animations because if you put an amateur in a suit, the data is going to look amateur.
But luckily Monolith somehow collected.
giant suite of personnel, animators that are as talented at animating as they are at combat performance and acting. Right here is Patrick Wigi. He's got over two decades of martial arts and weapon training. He brought a lot of the combat choreography into Shadow of Mordor and Shadow of War, and he's a real-life Talion. Some of those with the war one.
Marc Cosio is another one of our mainstays.
He comes from a military background.
John Van Dusen is this big, giant, hulking animator, and we get him to do our Balrog and our Olag stuff and the big guy stuff.
Our new recruit, Jay Vargas, really rounds us out.
And additionally, we got guys on the presentation team that are fantastic actors.
So basically, everyone on the animation team has been in the suit at one point or another.
If they're not in the suit, then they're probably on the sideline directing the shoots.
The animators are the actors and the directors.
These have so many benefits.
Nobody knows what the source material for the motion capture should be better than the person who's actually using it, the animator.
And our animators talk directly to the designers and they make the specs and the...
the fine details that that animation is going to need in order to be best in the game.
So the animators are able to direct the motion capture and use it as a tool to make the animations the best that they can be for the game.
They know where to cut corners. They might say, you know, I only really need that front movement and the end because I know that there's already an animation in game that I made that can do that middle and I don't really need the weapon to be captured because I'll just hand key that. So we really know how to do it the most efficiently.
So here's an example of Josh Paraventi directing a shoot of Talian killing a Drake.
So there he is, he's jumping up on the Drake back and now the Drake's rolling fire and he's going to cut off his head.
Oh, nice job.
So let's see it in game.
Or in Maya.
So Josh really used the motion capture to maximize his use of time within that task.
He utilized the mocap to capture Talion's portion, and then he could focus on doing the hand key of the Drake with his time remaining, because we haven't figured out how to motion capture a Drake faster than we can hand key it.
So success, right? We've figured out a pipeline which we can take animations that usually took us around three to four days and compressed it down to an average of about a day and a day and a half. So we're sleek animating machine. So that's enough, right?
Not really, almost.
We ended up shipping with close to 2,500 sync actions in total for the game.
So even at 1.5 days, we're still doing nothing but sync actions for almost two years.
Even if we add more animators into the pit until we can't fit into our cubicles anymore, which we did, it's still not enough.
So we need another plan of attack.
We need another one.
It's going to be outsourcing.
We had two avenues to try and maximize the outsourcing, and we were gonna try them both at once.
And one of them was we were gonna just take our process, because we loved it and it was so successful with us, and just find very good sets that we could send to the outsourcers and own and take as a whole.
We hoped that this would be useful, but it ended up taking a little bit more time to manage than it would have taken us to do it in-house.
So we abandoned that process.
Instead, we tried our plan B, which is where we mocapped the source in-house and then we sent it to the outsourcers to complete from a whole.
So here's an example of some raw mocap that we would have sent out to play.
There we go.
So you can see there's...
You know, this is just the raw mocap.
No choices on timing or posing has really been done.
We expect the outsourcers to be able to make all those calls.
We'll write a little summary, but overall, we're just going to send them an example of 60 insta-kills that we've already done ourselves, and ask them to match the style.
And this came out pretty good.
We got really good results from this one, but the animators on intaking were still presenting, having to do.
a couple of hours on the back end in order to intake it, just to get it to match that style, that timing perfectly.
And so some of the animators started playing around with just doing a quick timescale pass.
They would use the timescale tool, which Jean-Pierre will talk about in a little bit.
But within that hour, that quick hour, they would be able to establish some really important timing and posing to give to those outsourcers.
And with this process, we found we got really good results.
And actually is very reminiscent of a process that a lot of us are probably familiar with, the classic 2D pipeline. Your senior animators lay the key poses and they do the timing pass and then they hand it off to the more junior animators, perhaps, to do the cleanup and the finish. And so that's actually how we ended up utilizing our outsourcers. Our internal animators would do the concept, the timing, the posing, and then send it to the outsourcers for finish.
We got really good results that way.
So as a recap, we had some pretty ambitious goals.
The multi-character combat sync actions were an important component for us achieving those goals.
And they presented a lot of challenges in terms of scope.
But the monolith motion capture pipeline supported us in the rapid creation of those concepts and source materials.
And the additional outsourcing lent us huge relief.
And finally, we felt like we had a a pathway to success that we could support our game, not just the sync actions, but everything else.
But I've left out a huge part of the important part of the pipeline, which is plan of attack number three, which is having excellent animation so that the animation is being created, can be created to the highest quality and to the highest efficiency.
So this, thank you very much.
I'll pass it on to John Peel, who will take us through the animation pipeline.
Thanks, Camille.
So I've been in Monolith for a little over 15 years now.
And I'm really, really proud to have been part of the team on projects like Matrix Online, Fear 2, Condemned, Shadow of Mordor, Shadow of War.
And today, we're going to look at sort of a day in the life of one of these combat sync actions from an authoring process.
I've chosen a sample, one of our senior animators, Mark Casio.
You saw him doing a lot of the falling and playing the.
and some of that mocap footage.
And sort of an overview of how this part of the talk is going to proceed.
Going to talk first about something that's unique to Monolith, how we use our game data as the source.
We don't save Maya scenes.
We don't back up Maya files or build rigs up front.
We pull data right in from the game data and build our scenes from scratch.
The Monolith Tab panel and our tool set that allows us to have this two-way information flow between our working space and our game data.
Our Maya tool set and some of the neat tricks and sort of our philosophy of how we deal with animation polish.
and using motion capture.
The motion encoding process for synchronized actions or sync actions, just sort of the nuts and bolts, digesting this data for the engine, getting it ready to go, key strings, and game data markup.
The point at which the confluence of all the different departments work, sort of joins with the animation data and makes something better than the sum of its parts.
And finally, look at the results and a runtime presentation of this sample sync action.
So we'll go back to the MoCap studio here and take a look at the shot we're going to focus on.
Marc Cosio is playing the Orc and Patrick Uecci is playing Talion.
And you can see this is probably the third or fourth take of this one motion.
They will often do a couple takes where they're sort of wrapped.
up in proximity with each other.
But often it's better to separate out, especially in this case, this is going to be a severing sink action.
So his legs are going to get cut off at the knees and Patrick's able to hit these big poses and make these big swings without worrying about hitting Mark or having to avoid what won't be there in the final product.
So going back to monoliths using our game data as source, just to talk a little more in depth about it.
Our animation polish, our data right out of the MoCap studio, is in the game ready format.
It's called a model anim file.
We store all the takes of a MoCap session within a single file.
And it's viewable with our custom tools.
The model edit tool will allow us to preview that motion.
We'll take a look at that in just a moment.
As a byproduct of this, our animators stay up to date with the state of that animation in the engine.
So if any changes get made, if we hand it off early and combat designers do a timing pass or trim part of the animation, any of those early editorial processes, especially if we haven't moved on to the polish phase, we're up to date with any of the work that's been done by any other department that coincides with that action.
Yeah, as I said, all departments have instant access to the animations once we've handed them off.
So if the audio department needs to get in there and start marking up the sounds, or most often the combat designers need to get in there and make sure that everything, all the damage key strings and windows are happening where they need to happen.
And this is a look at one of our proprietary tools, ModelEdit.
It's been around since probably the first Fear game.
And it's really sort of the confluence of all of the developers' work on any animation.
We're looking at motion capture data in this, but we'll look at some finished game data in this format later on.
Yeah, and any developer on the team who installs our tool set has this tool.
They don't need to have Maya to look at animations, and we can preview multiple takes or a pretty dense set of animation data to take a look and decide what we're going to use in a lot of cases.
Another sort of happy byproduct of moving our source material to this game-ready data is that if we need to make a big change, and we actually did about a third of the way through this project, we went and revisited the structure of the layout of our characters' finger bones.
And so our character artists did a new sculpt on Talion's hands and the orc's hands.
We share a skeleton.
And our Very talented technical artist, Frank Schorsch, went in and reconfigured the layout of those fingers, get better posing, better weighting, and almost overnight, I think it was over a weekend, but we took all those changes, batched them through, did the model geometry updates and had everyone back on the same page with pretty substantial change to our base biped skeleton layout without anyone missing a beat.
Sorry, I should move to that hand slide a little earlier.
So a little bit in association with the tab panel and the way we work with our source data is the philosophy of the tools that we've adopted at Monolith.
On this project and on many other projects, since motion capture is sort of our common approach, we've optimized our workflow for motion capture.
This dense data, we don't try to strip it away, we don't try to take the noise out of it.
We want to preserve what's great about the motion capture, which is that sort of natural noise.
So as a result, we don't start with an up-front rig or a rig that's trying to do everything right out of the gate.
We take a more modular approach or an on-demand rigging approach.
If you need IK, you build IK right onto the core skeleton, work with that, bake out from under it.
Our focus really is on speed, control, and what I like to call direct interactions. I don't like to bury the result of the animation or the pose that you're getting under multiple attributes on multiple controls and things wrestling for the state of control.
Really, if you're going to move into using IK, you're in IK mode.
If you need to go back to IK, bake it off. Go back to working at FK.
We accommodate a bring your best tools approach since really we start and end with a skeleton and Data on the bones rotation and translation data on the bones Then if an animator comes in or if we see a new tool or we see a new rig or some new feature in Maya we bring that right into the mix because we can just incorporate it into an as-needed approach to applying the best tool for the job And a byproduct of this, of working with MoCap, and of working with this applying and removing tools process is that you have to be OK with baking your keys on and off all the time.
So you can't get too precious about your curve handles and your sparse keying.
But there are, you'll see.
In a step, don't worry.
We really push into posing, we really push into timing.
It's just a different methodology.
They call it a destructive workflow, it always kind of hurts my feelings, but.
Yeah, we'll get into it.
And we are not particularly geared towards from scratch animations, although we've done well with it, or quadrupeds.
But really, our animation team is so senior and so talented.
I mean, you saw with the Drake earlier, they can.
You can throw any challenge at them, and they'll do whatever it takes to get results.
And they're sort of used to, if they need to do it and they need to drill down to selecting things right out of the hypergraph, they'll get in there and they'll make it work.
So this is a little bit of a sizzle reel of some of the novel ways of abstracting the data that we've developed as part of our tool set.
So really no one is forced to work with any of these, and most of these things we're looking at are not even going to be covered as the aspect of Mark's work on cleaning up this sink action.
But we have things like mirroring, and we have things like abstracting the root of the character's motion to a center of mass.
and IK on the knees, and just all kinds of novel approaches that when you need them, they're great to have.
Most of the time, you don't need them, or sometimes you forget you have them.
But it really is sort of like a utility belt, or just a toolbox of different ways of manipulating the data, putting things on as you need them, baking them right back off, and getting into a control scheme that you're more familiar with, or that's easier to achieve the stretch of the motion that you need to work with.
This last one's a little, oh, sorry, there are a few more to go.
I kind of rushed through that.
But yeah, we work with cross fading, animation blending, loading multiple characters into scenes and even things like baked ragdoll sim.
It sort of leveraged the NVIDIA physics system to do that kind of stuff.
It's fun throwing Caragor ragdolls into the air.
So now we'll take a look at actually using these great monolith tools in the tab panel to construct a scene from scratch with the game data.
So we're going to do a real-time look at bringing a scene into being, importing the characters, weapons, materials, and animations that we're going to need to start working on this motion capture data.
So on the left is a tool that queries our game database for the characters that are basic scale in the game and the weapon that they'll typically hold. And yeah, I know we're watching a progress bar, but it gets better. Talion gets pulled into the scene with all of his materials and he's ready, he's ready to go. He's skinned, he's got a skeleton. We build center out for our bounding boxes, so I'm going to build a dim dimensions box that's indicative of that.
I've added a tool to build proxy shapes under the bone shape data so that essentially those geometry boxes are the same thing as your bones in the scene.
So you can directly select them, manipulate them, and band select them in your scene.
And it just gives better runtime performance.
And next I'll pull in our second participant in the scene, which is the orc.
And I think we missed it a little bit, our namespacing approach is a little bit.
It's strange, but it's efficient.
We underscore, as in putting an underscore at the prefix of the character that is in our primary character in the scene.
So if we have two characters, one is going to be null and everything in his genome.
geometry, when it's going to be underscore null, and everything that follows in his hierarchy, and we can switch the focus between our primary characters just with a button press.
So we don't, our import export tools don't really handle namespacing all that well, and namespacing was really tough to deal with until a couple years ago, so that is our efficient means of swapping between several characters in the scene, and everyone has an easy time working that way and developing that mindset.
So now through the actual tab panel, we've been using its functionality, but that window that we opened is going to pull in the animation data from our motion capture shoot. First loading onto the Ork. That took a while because the Ork was brought in at scale. Our native scale for an Ork infantry in the game is 110% of Talion's height.
So all of our biped characters share a skeleton, and they share a common scale.
Orcs are Italians' height at their native state.
But in the game and in our scenes, we'll often scale them up to 110% because at sort of their generic height, they're taller.
And when we have a scaled character in the scene, we have to re-encode their motion encoding to make sure that we're pulling in data at a native scale, baking it to a bigger character, and when it goes back out, it's getting baked back to a native scale.
We'll look a little bit more at that later.
So back to Mark C. Now we've assembled the Maya scene from all the game data and we're ready to roll with the mocap.
His first step is going to be trimming and positioning that rough take in the mocap.
This is where he isolates the key stretch in the motion and is going to move the roots of the characters to block in those key points of interaction.
You're going to see a lot of foot sliding from here.
So on the left is the motion as it was shot in the motion capture space.
On the right is the rough alignment, really bringing those key points of the action in.
But there's, let me try to loop this again.
All right, well, we'll hope the next one loops.
So the timescale tool, Camille talked about it a little bit earlier, and it's very similar to, well, it actually predates the Maya scene timescale by several years.
It was first brought up in our studio as a way of cleaning up noise from our mocap data by moving the actual keyed animation to physical splines in the scene that we could then run smooth curve operations on.
But here we take those splines and we use them to drive a unifying timescale attribute on a character selection.
So it can be an entire character, part of the character.
We can run multiple timescale attributes within a scene.
And this is where we really start to assert our vision as animators.
As Camille mentioned earlier, this timing pass is super important for establishing rhythm, the weight, the power, the impact of that raw motion.
And it precedes any focus on posing layers or futzing with IK contact points or foot locking or anything.
We're staying pretty loose and we're staying free to push poses, well in this case really push timing.
We're not even dealing with posing.
Here's a look at actually working with that time scale attributes.
So that tangled mess of splines in the center are actually the NURBS splines that are driving locators that are representative of every animation track on the character.
Every one of them represents either translation or rotation track.
And we've slaved a single attribute to drive all those locators along the path.
And we can mark those key points in the animation and then work with the spacing on the keys, work with the curve handles, just any way you want to manipulate the data.
It is.
Packages have started to incorporate more and better ways to deal with scaling motion or scaling playback speed on animations.
But our in-house solution, again, gives us the flexibility we need, and it actually has quite a bit of the key string data that lives on the root of our characters that needs to ride along with this re-spacing.
So it's the best way we've found to manage that.
And we'll look at where Mark has arrived after his timescale pass on the sync action.
So already you can see that the, uh, darn, I really am going to need to get this to loop.
Sorry.
Success.
All right.
So we'll let this play through a few times and sort of let it register that beginning motion is as great as the acting was in the studio.
And really, our actors, our animators have got a flair for the theatricality of these attack motions.
But really, that punching up of the timing is where it starts to solidify the power and impact and weight and speed of these things.
And rhythm, really.
So next we would typically move on to layer and blocking in those key poses.
Still not worrying about locking down IK handles or applying rigs or doing anything to interfere with this very direct expression of where we want the key aspects of that motion to read the most important part of where we're making decisions as animators for how the final presentation of this motion is going to happen.
So determine the key selling points of the action.
Continue nudging those characters, further refining the proximity of the two to each other, still allowing for foot sliding.
Maintaining that motion capture as a base layer, again, that's a very important part of the consistency of our style on the team.
If we start clear-cutting motion capture noise or significantly altering things by cutting things out of the motion, we may start to get deviations in the read.
And finally, if there are key poses that we need to blend to, or animated states, looping states, or branching sync actions, this is where we start to blend in those important poses.
So here's a look at Mark's scene before and after the pose layer adjustment. Not as extreme a change as that timing adjustment, but really starting to make the key line of sight and those points of contact really read.
So we'll take a little bit of a detour into now this modular rigging approach.
There's several flavors of IK that we'll use now that we are happy with the posing and happy with the timing.
We're going to start locking things down relative to the world and relative to the characters relative to each other.
So the first and most common approach will be to bake an IK rig onto the character.
So this first look at it, we'll see.
It's a pretty vanilla mocap set up.
I build an IK handle to the ankle.
There's a pole vector.
Since we're preserving the motion, we don't ever get rid of any of it.
underlying motion. We do maintain an attribute on the pull vector handle that tracks the upper and lower X rotation if there is any on the limb because it's very important sometimes we need to break or twist those characters along their limbs and we don't want to pave over that motion just because we're using IK.
And then I added a way to move the IK handle down to the ball of the foot so it becomes a little more manageable at the toe and you can get some more complex interaction.
But again, it's nothing new.
The thing that is sort of novel about it is that it doesn't have to be there from the get-go.
We can pull in a raw character into the scene and he can have IK whenever we decide he needs it and bake it off when we're done.
Looking at a little more of a special case interaction in this scene, I've applied IK to just the right arm that has the sword.
And this is one of the nice features of our suite of tools is that it's very easy to change the object space of our controls.
So in this case, I've selected a vertex at the tip of the sword where he's driving it into the ground.
It's going to be the parent of the IK handle's motion on his right arm.
So if I plant a stretch of that, it's going to plant the sword.
in the ground, we can blend into and out of that state, and then we can key any sort of wiggle off of that, bake right out from under it, and we're back to a garden variety IK setup.
Really, a lot of our tricks are shifting things off to locators, locking portions of the character out to world space, or orient locking them to an object space, or in this case, moving IK handle pivots to either other parts of other characters, or other portions of the character's extremities in the scene.
It's also on-demand IK.
One of the things that sort of breaks my heart anytime someone comes into a rigless environment is they'll grab the end of a limb, pull it off into space, and sort of detach the hand.
It's a little scary to me.
And if you've worked with the Character Studio or those kind of fully featured rigs, it's something you really enjoy having.
So we added that through Script Jobs to our tool set.
So through the Picker window, you can build an IK handle that doesn't, bake a full-time dedicated IK layer onto them. It just gives you a temporary way of controlling and adjusting the pose with an end effector, with a pole vector, and even the ability to move the parent relative to the end space of that rig.
And again, it's just a great way of cheating these poses on layers or pushing poses on layers. And yeah, part of the bring your own best approach to the situation philosophy we've adopted.
And finally, there's a human IK rig.
This is one of those things where I see the things that human IK can do.
I know I'm not going to be able to build something that amazing and complex and comprehensive, so why not just incorporate it into the available functions of our tool set?
So it really was a simple matter of just writing a script to characterize our standard biped skeleton.
Bake it to Maya's version of the human IK rig.
And here, our principal animator, Patrick Wagee, is demoing pushing a pose on a layer using human IK.
So using all of the features, such as the space locking, full body IK, all of the really, again, the nice, fully featured, comprehensive aspects of a rigging system like that.
But again, we don't want to be a slave to a rig like that.
We don't want.
to have one paradigm for working with data.
Because a lot of times, you have a tougher time drilling down to the way you want to control the pose with a fully featured rig.
It intervenes in ways you don't want it to.
So great for some purposes, not for all purposes.
But we do roll it into the way we work.
So to recap, I know it went off on a tangent about IK.
We've seen the trimming and positioning of the raw mocap, the timescale pass, altering posing on layers, pushing those key moments in the animation.
Now we've seen the IK cleanup on the hands and feet.
And let's see where Mark arrived after that.
So moonwalking talion is no more.
And these key parts of the action are really, sorry, I'm going to need to figure out how to get this to loop again.
All right, let's give it a shot.
One moment.
And we're back.
So Talion's sliding, he's really grounded now.
His feet are locked, that stab through the head is sinking in there and staying planted and he's pulling it out.
Mark has even made a major adjustment to.
I went, oh sorry, next slide we'll see back to the primary motion capture clean up here. We're seeing the difference between pre-IK, post-IK.
Just a little bit of work that remains and that is things like finger animation, further polish, additional smaller details. Working general to specific really, it's like sketching. Good, it's looping. So I've gone all the way back to the original mocap here so you can see how far we've come.
Excuse the interruption.
Getting good at this.
So really we started with great data, but we've enhanced it every step of the way to something that really is paying off in both the speed, the lethal feeling and just the ferocity of this final read.
And it helps when Mark dresses up his scene with flying limbs.
That's not a requirement of authoring this in the game.
We will actually be spawning those giblets, but it's great as a preview for sure.
So onto some of the less animation-driven things, but more the practical aspects of bringing this into the game data format.
We'll look at motion encoding these.
So an important part of making sync actions function is that each character tracks the location of their counterpart in the synchronized action.
So each character carries something called an attach node, and that is used to track the world space position of their counterpart in the sync action.
If one of them runs into trouble, grinding up against a wall, getting pushed against a ledge, or moving into a space, is dangerous for them or is going to have them looking like they don't belong, they can actually push their counterpart out away from the obstacle.
So back to our model edit tool to look at how motion encoding looks in the format of our exported animation.
That first stretch is watching the animation play in space.
I click the key that allows us to see the motion encoding in action, and now the orc is moving as he would in the game and watching his bounding box, tracking his position in the world.
I'm going to unhide the skeleton in this tool so we can actually track what that attach node is doing.
It'll show up as that yellow point on the screen.
That's representative of where Talion is going to be during the entirety of this synchronized combat action.
So you can see the facing and positioning of where the player character is going to be and how he's going to get latched onto the orc and how they're going to be paired in playback during runtime.
I love talking about this target character scale in the game.
I talked a little earlier about orcs and how they're 110% of Italian scale, but they really use a shared skeleton.
So everyone gets authored Italian size.
But at runtime, not only do we have standard orc scales, but we have a range of variety within those orc scales.
We can have an orc infantry that's 105% of human size or 115% or if he's a captain, 120%.
We have orcs of every shape and size.
They all share a skeleton.
And they all have to align during sync actions.
So at runtime, we force the scale of the character during sync actions.
And we've got a nice illustration of it here.
Three archers in the world, those numbers that are floating over their center are representative of their native scale as newly spawned AI.
One guy was at 90%.
One guy was at 92%.
Another guy's at 97%.
But if you watch as these sync actions are happening, attack that he's doing, you see that scale quickly scale up and lock during the entirety of that interaction. And here where Talion seizes the orc to brand him, he gets locked at that 1.1 scale, but as he's released slowly scales back to his original 0.9 size. We hide at both ends of the scale locking where we very quickly scale to that desired sync action size, and then after we release them we allow them to slowly drift back to the scale that they should be in the game.
And this is where the other departments start to get involved with the data.
Now that we've exported it out to Model Edit, we have things called key strings.
They're markup in the game data that our combat designers will put in, again, about attack windows or exit windows or when damage is applied or when hip colliders are enabled.
Our effects department, things like blood trails, sword swooshes, all the gore, even the severed limbs.
Our audio key strings, again, without sound, these things really aren't going to pay off.
And boy, our audio guys really make these things sound intense.
Our combat face key strings, the animation team will come back in, and we'll define these preset poses for the faces.
animated looping states for characters.
And we try to sell the extremes, winces, grimaces, screams, yells, anger, you know, that slide with all the emotions, the intensity of these emotions.
And then presentation.
Our realization department are the guys who really make these things at runtime pay off as a cinematic experience.
They'll bring in slow motion, they'll bring in camera movement, and they'll really do their best to sell these as really special moments in the game.
So.
Here's a look at key strings in the context again of our model edit tool and Model edit I should say I'm spending a lot of time bouncing back to this We do have better tools that show our characters textured and actually with the game data It just happens that this as such a simple tool Really becomes the the confluence of a lot of the different departments work on these markups all the little green ticks if you can see at the bottom of the screen a representative of some comment or some action that a department has specified or even a multitude of on a single frame semicolon separated scripts that tell the engine something about how this is to play back All right, finally, let's see this amazing sync action in context in the game.
We'll see it from quite a few angles.
Yeah!
Give it a sec.
Let me exit out of here.
Boy, I don't want to steal the thunder in that one.
Hold on.
Ha ha ha.
Bear with me please, and let's give that another try.
Sorry Mark.
Sorry about that. Let me go right to the folder and let's try playing that sucker.
If I can just find that... You know what? I'm not even gonna...
Let's just play it.
And we'll see this from a number of camera angles.
Same thing, actually, but it looks new every time I see it.
All right.
Thanks.
And let me get my mouse back and let's get back to the presentation so we can finish up these slides.
Come on back.
There we go.
I know you guys have got it on your screen.
I don't have it on mine yet.
Thanks for the applause break. I guess that really is the end of our talk about the tools. And I really don't wanna miss this slide though, because I'm gonna tell you about the other talks that Monolith is doing.
Some really good ones. Be sure not to miss the ones you see listed on the screen. I got to see a few of them as part of practice, and I learned something new from all of them. So they're really informative and well worth attending.
Monolith Games is starting to hire, so yeah, we're starting pre-production on something new.
So, yeah, hit us up.
Check out our website.
That is our contact information.
Thank you so much for attending.
This was a lot of fun.
And I guess we'll start the Q&A section now.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Hey, I've got a couple questions.
Sure.
You mentioned using HumanIK for some of your control rigs.
Sure.
Did you also use that to import your mocap files or your mocap data onto your skeletons?
Oh, so no.
Once we move out of MotionBuilder, which we actually do use, I guess, HumanIK in its MotionBuilder format to process our motion analysis skeleton onto our character skeleton.
But really, once we move it out of there as an FBX file, we have a batch process that actually retranslates that data to our model anim format.
So it's really just bone animation data.
So we start with just rotations and translations on the character skeleton.
If we want HumanIK back in the Maya context, that's our business to layer it back on when we need it.
Awesome.
And then the other one that I had is camera angles.
Did you guys animate multiple cameras, or is that all handled engine side?
Yes.
Our realization department, when they focus on one of the sync actions, they will author multiple.
angles of attack for them to play back in the context of the game.
Because if there are visual obstructions or parts of the character they can't swing around to when they need to, they have several fallback options, falling even all the way back to just the game camera if they can't find a cinematic camera that operates in the space that it needs to.
But yeah, they'll author two or three different takes and have those all available at runtime.
Awesome.
Thanks for the question.
Yeah.
Hi.
Hi.
It's great.
I finished the game.
It's a great game.
So I have a question about the mocap and the size of the skeleton.
So you said all the skeleton could be shared, but there's a lot of different size of the orc.
So is it just one skeleton for all?
Yes.
And it drives our character artists a little crazy sometimes to have to conform to this shared biped.
But really, from our skinniest, scrawniest orc Portly Orcs, they all have Talion's skeleton.
And again, Frank Shores is a wiz at skin weighting, so he makes it all work on the singular format.
We did do a trick with female characters.
Our El Tariel DLC, and even her character in the game, is able to share a lot of Talion's animations.
Our engineers, our gameplay engineers, gave us this great tool that allowed us to scale out her shoulders, or scale in her shoulders.
So when she's idling, when she's moving around, she has a more feminine silhouette.
And when she needs to be talion and reach what talion can reach, climbing or doing some of the sink actions, we let her shoulders be really broad.
And again, like the orc scaling, no one seems to give us a hard time about it.
So, yeah, everyone's happy.
All right, thank you.
Thank you.
Hey, really cool talk, thanks.
So, really interested in the on-demand rigging.
I've been trying to push that for a while, but the two main pushbacks I'm getting is one, we're not riggers, we're animators.
Sure, yeah.
And the second one is we don't want to work with baked animation.
Oh yeah, that really is sort of the deal with the devil that you have to make with this approach to the tools is that you can't be precious about your key spacing.
So you know, once...
Once you embrace that layers give you the kind of control you typically want to focus on the key aspects of the motion, the way that you don't have to worry about clearing other things out of the way to do, I think once it sort of becomes a philosophy of the way you manage the data, it...
becomes a much easier mindset to embrace.
Yeah, it's...
That's one of those things where you have to prove it in order to be able to show them as to...
Yeah, and the aspect of they don't want to be riggers, it really...
It's a button press to put IK onto whatever limb you want or multiple limbs.
So, they aren't fussing with worrying about, you know, building handles or controls.
It's there when they want it and baked right back off when they want to be back in the FK space.
So that brings me on to a second question, sorry.
When there's a single button press, I'm assuming there's relatively straightforward metadata within the skeleton to be able to mark up to say, okay, well, it should be like this and so forth.
Yes.
Yeah.
So with the shared skeleton, we have names that are consistent throughout our bipedal characters.
So I leverage that a lot.
I'm kind of a lazy scripter where if I can reference something by name and I don't need to abstract it, I will.
But we do, we mirror the behavior of the bones in our skeleton.
So we flip those axes where we want to.
And again, helps with the direct control and not worrying about what is it like to manage.
raising both arms when you don't have a nice rake.
But it also lets us differentiate left and right behavior very easily.
Cool.
Thank you.
Thank you.
Hi, my question is regarding to creatures. Oh sure. I know that you can make a mocap with creatures So how do you manage the specific creatures animation and sync animation with this?
Oh sure. Do you want to talk a little more about animating creatures Camille?
Camille has worked with and has given me a lot of great feedback on what it's like to animate with the Drake in particular She did some pretty intense capture. I don't know if that works.
We do live a lot of the same things.
Oh, yeah, sorry.
We utilize a lot of the same tools, tool set, except we're not using Bakesy animation as our source.
So we're still using that rip-on-off tool set.
We'll bring in our quadruped, you know, Kragor and throw on whatever tool set, IK, feet.
We can have the pivots and stuff like that.
We can change them on the fly and animate very much at a classic sense for the quadrupeds.
Do you animate like a hand key animation?
Yep, mm-hmm.
Yeah, all key frame.
We did try to mocap the caracore, but we didn't quite get there.
Thank you.
Thank you.
Hi.
Oh, are we done?
You're out of time.
Oh, sorry.
We're out of time.
Yeah, we'll be here if you want to come up.
But thanks so much, everybody, for hanging around.
