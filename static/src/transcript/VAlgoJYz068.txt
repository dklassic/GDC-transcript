inform game design.
My name is Leroy Atanasoff.
I'm the game director on Rainbow Six Siege.
And to give you a bit of context, because maybe not everyone knows about this game, Siege is a five versus five competitive first person shooter.
And it's an asymmetrical game with attacker and defender.
And one of the distinct elements of it is that there is a presence of what we call operator.
You know, that's our character that come into the battlefield with a specific ability.
So just to wrap up, think about like a MOBA that meets a shooter.
And this is a bit what is Rainbow Six Siege.
And today I have a special guest with me, that is Geoffroy Mouret.
Hey guys, so I'm Geoffroy Mouret, I'm a data scientist part of the analytics team on Rainbow Six Siege.
So historically what we guys do, we just make reports and try to get people from the production team know we exist.
But the goal of the presentation today is to show that we actually have some very useful collaboration when we communicate with the people we're working with.
Yeah, so let's go right away into the plan.
So we're going to talk about the philosophies that mean, what does it mean for us, like a data-informed game designer, and how do we have this relationship, and how we work with analysts, and why we are working with them, and why we found like a.
super interesting stuff working that way.
Then the methodology on how, at least on Rainbow Six Siege, we apply it, okay?
I think it's really dependent for your context, but we are going to show you how we do it in Rainbow Six Siege.
And then a quick conclusion.
And all of this, even though we believe that intelligence design can be applied to any kind of feature in a game, we are going for this presentation, focus on the operator balancing, because it's more meaningful for this presentation, because operators are a big part of...
of this game.
So let's start right away with the philosophy.
So Quickly, intelligence design is how you share the design ownership as designer with an analyst.
And we really mean design ownership.
That means designer and analyst are sitting close together.
They work hand in hand on the design of a feature, from the conception to the release on live of this feature.
And they take decision together about this feature, either in conception or either in real.
They are just one team working together on design.
And why is that?
It's because the backbone of the intelligence design is data.
And it's all about how we use data to make better design decisions.
And why data, you can ask.
It's because it's the best tool that will help your intuition.
Especially as game designers, everyone has intuition.
But as game designers, often we are in this position of after a prototype, after an iteration, everyone is turning in front of you and they are waiting for validation or consent.
Is it good?
Is it OK?
Are we moving forward?
Stuff like that.
And you have to make a quick decision, most of the time based on the PlayStation.
Sometimes it's one or two-hour PlayStation that you just add.
And the thing is that.
to help to confirm your gut feelings, you have something like it's sometimes a tool and sometimes an enemy.
This is a cognitive bias codex.
It's normal if you can't read anything.
It's all the mechanisms that are in your brain that alter your perception.
There is really a lot of them, so that way you can't read them, like loss aversion, spacing effect.
That means you have all those mechanisms in your brain that alter your perception and when it comes to make a decision It's it's it's hard for human being in general But especially designer because we ask them to make this decision to understand what really happened versus what you think really happened And data can be a useful tool to help you to build this, to make those decisions, and to iterate and make a better design.
Quick example, for example, we always had this moment in a game where we are like, and even it's stuff that happened during PlayStation, like, ah, this hero is too strong.
You know, ah, I'm always dying against this operator.
Or you know, ah, this map, we're always losing on this map.
And we had this kind of story on Rainbow Six Siege.
We are playing with analysts after work.
We have kind of a team.
And there were always this map.
Every time we are playing on this map, we are like, ah, this map is so bad.
You know, we are always losing on this map.
Ah, we hate this map.
You know, why are we always losing?
Ah, we're so bad, so bad.
Oh no, it's this map.
Okay, free lose.
And stuff like that.
And the cool thing is that because we are working on Rainbow Six Siege, we have access to our own data.
So with this guy we can fact check everything we are saying, like every misconception that we might have.
So at some point Geoffroy was like, okay let's look at our win rate on this map.
And actually we were winning more than we were losing on this map.
And still if you have asked any one of us in this team, everyone has a perception that we are always losing.
It's because it's one of the cognitive bias theory that you tend to remember always things that confirm your first intuition.
So every time we were losing, we were remembering, you see, I'm losing on this map.
And every time you win, you don't remember that much.
At this point, I think cognitive bias is something that when you are a modern designer, you know all of this.
You need to be very careful when you take decisions, when you work after a play session, and stuff like that.
But there is another element for what I think data is the backbone of the intelligent design.
It's because you are just one human, or as a designer team, you are just one team of people.
And you only see the context that you are in.
For example, you only play maybe on PS4, or PC, or Xbox One.
And you also have what we call your skill rate.
Like you're a copper player or you're a diamond player.
That means you have access to one specific context of the game.
That means even if you're good with your perception, like you have an Excel sheet and you write everything that happens to be sure that you're not biased, you will only see a slice of the reality of this game.
For example, a copper player will always say, there is so much Caveira in my game.
Caveira is an operator.
She's a girl on the right.
And the diamond player on the other side will say, Caveira doesn't exist in this game.
But there is so much bondi, guys.
There is some issue with bondi.
And if those two people talk at the coffee machine the morning, like, hey, I had a game yesterday night.
Hey, me too.
Caveira, such an issue.
And the other, like, huh?
Caveira doesn't exist in the game.
So you only see one aspect of your game when you play, when you are engaged with.
So that's why you need to have data to see everything, basically, to have a 360-degree vision of your game.
So OK, data is important.
Fine.
We know it's kind of a buzzword.
And OK, we have analysts anywhere.
They handle data, and they send us reports.
And I'm sure we all receive email after a milestone, after a playtest, after when we ship a season, or something like that.
And it's 100 pages with a lot of boards and graphs.
And I'm sure everyone is reading them, like missing nothing.
And they are sure, like, yeah, I get it.
I get it.
I know exactly what happened in my game.
I read all those emails.
Are you really sure that you read them well and there is no issue?
Well, you know, like, as analysts, most of the time, we have, or we used to have, game designers coming to us saying, OK, we want a report with that data.
We want to follow the operator's win rate through the seasons and everything.
So you make that report, because you know they ask you something so you're happy to have them.
But sometimes the report is not what they try to, it's not answering the question they want an answer for.
A good example of that is Frost.
So Frost was our first DLC operator.
She came out like three months after the game released.
And at the time, we had basic reports for the game design team.
And people were not happy because they were saying, like, Frost is overpowered.
Like, she's way too strong.
And when we looked at the report, it was the case.
Like, Frost's win ratio was the highest of all the defending operators.
And so your basic intuition might be to say, well, the winner ratio is actually a good representation of the power level of an operator.
But I'd like to show you something when we digged as analysts a bit more into that data.
So what we wanted to check was whether this operator was good for new players versus advanced players.
So we took the data that we had before.
We split the population into four different categories.
And so we had beginner players.
For which we saw that, OK, Frost is not the best operator because she wins less than Rook.
So if you play the game, you know that Rook is an operator that just drops a bag of armor plates at the beginning of the round.
And Frost is laying down traps around the map.
So you know that when you're playing Frost, you need to have good map knowledge.
You need to know where to put these traps and everything.
Whereas when you're playing Rook, you just drop your bag and you're done.
You've done your job, basically.
So we're like, OK, it makes sense.
So we are going to look at advanced players, so a player who spent a bit more time in the game.
And it was the case as well.
Like for these players, Rook was winning more than Frost.
So we're like, OK, so obviously Frost is an operator for advanced players.
But the fact is that when you split your data across your whole population, this is what you get.
meaning that for all levels of play, Rook was winning more than Frost.
And this is the exact same data split into four different subpopulations.
So if you're used to work with data, this is something that is known as the Simpson's paradox.
So you can look it up.
But the explanation behind that is actually not that hard.
Because if you're looking at the win rates for the different populations, you can see that advanced players win more than beginner players.
Like overall, whether it's with Frost or Rook.
And the thing is that Frost is a DLC operator.
So to get her, you need to have a season pass or to have spent a lot of time in the game grinding some soft currency to get that operator.
So when we looked at pick rates for these populations for the two operators, you could see that Frost was picked a lot more by advanced players rather than new players.
So what explained these win rates was not the fact that Frost was better than Rook.
It was the fact that Frost was played by better players, meaning that this metric is basically useless in terms of power balancing.
It just shows you if an operator is more played by good players or not.
So this is where we started to show to game designers that, OK, you guys, you need to be careful when you're asking for data, when you're reading your data.
So this is one of the many examples that we have about misinterpreting data or getting the wrong metrics for an issue.
So when they show us the Simpson paradox, as designers, we're like, Oh, wow.
So OK, so maybe we are not like super mathematical people.
We didn't come from a school with statistics and stuff like that.
So maybe we should let analytic people do the balancing, look at the report, and maybe take decision, while us designers, we focus on new stuff, like making new feature, new aspect of the game.
And this question came into the table as a legitimate one.
And to answer this one, I feel like it was a concept of being data-driven versus being data-informed.
When you are data-driven, you might have the tendency to focus on decisions that will enforce, like for example, the perfect balancing.
So stall and simplify your design in order to achieve this perfect balancing.
Let me give you an example.
Imagine this system being your game.
So you have three marble and one plate.
So in terms of maintaining the balance, it's quite easy.
And every time you want to add a new marble, if you're data-driven, you might say, I'm not sure you should add a new marble on this system because it might create imbalance.
So let me give you a concrete example of what I'm talking about.
So this is a graph that represents a win rate between attacker and defender in Rainbow Six Siege.
And since we are an asymmetrical game, it was really important for us to try to maintain as much as possible this balance, because at the beginning of a run, we want to have each player having a 50-50 chance to win the run.
And at some point, around August, you see that we are quite close and quite happy in terms of balancing.
But at that point, we had another issue in the game.
That was the shotgun.
The shotgun were overwhelmingly good at everything, from long range to mid range to short range, good at destruction.
And everyone was using shotgun.
And they were, in terms of diversity, it was like a creating shadow for all the other weapons.
So we decided to change them and to nerf them.
And because of the nature of the game, defenders were using them the most.
So we anticipated kind of an imbalance on the defense side.
And it happened, and it happened like super badly.
When you look at this, it was one of the worst balance situation we ever had in the history of the game at that point.
So here, for example, if you're data driven, you might say, maybe we should reverse this change.
Maybe it affects too much the balancing of the game.
Maybe we should find another way.
Maybe we should change something.
But just we are more focusing on what we call the iterative balance.
And when we were talking with pro players, with people that were playing.
with a Reddit forum and pro Reddit play session, pro player coming to the studio.
And even us as designers were like, the game is better.
In terms of experience, the fact that now you have multiple strategies, multiple viable weapons, it's less like a shotgun festival.
And it's more interesting.
So in terms of pure experience, yes, the game is better.
Still, we have this balancing issue.
So instead of coming back to and revert, we decided to introduce another change.
That we decided to.
decrease the timing of a round down to three minutes.
And again, because it's an asymmetrical game, when the timer hits zero, the defender wins by default.
The attack team has to reach and breach the objective in order to win.
So decreasing the timer to three minutes were favorizing more the defense.
So it has an impact on the balance on the attack side.
But again, the game, we had PlayStation, we had playtest, and stuff like that.
And people were like, yeah, it's more interesting because it creates more tension, more meaningful choices for the attacker team.
They have to now take decisions in time, and it matters.
And even for the defensing team, it was now about not only killing the opponent, but they can play the timer, they can try to slow down the push, delay the action, so open for more variety and more diversity.
And as you can see.
In the end, we came back like we were super lucky.
Even though we anticipated that, you never can tell.
If you wish, you will come back to normal.
But we come back really close to where we were before.
So just to recap this example, to me, this is data-driven.
And what we want with DataInform is that we want to continue to add as many marble as possible, as many plates as possible, working with analysts.
to ensure the balancing.
So yes, designers should mostly focus on the diversity.
I think, especially when you're on a prototype, you're working on conception and stuff like that, a lot of times there is discussions that are crippled by power balancing.
How many times after a play session, 90% of the feedback is, I think it's too good, it was always killing me, and stuff like that.
Where actually, you should focus about, OK, is this interesting?
Does it create a new gameplay mechanic?
Are we creating a new player experience?
How are we adding a new layer of strategy?
Are we creating new situation?
Rather than focusing directly on the power balancing system.
Because analysts are the best to ensure that, to ensure the fairness, being sure it's not too strong or it's not too weak.
That when it's live, as they have the data, they have the report, they can help you to fine tune and tweak your value.
And even when you're in conception, they can help you thinking of future too.
OK, don't care about the power balance for now.
Just care about adding a mechanic and, you know, like a value.
Then we will be able to tweak when we'll be live to be sure that it becomes balanced at some point.
And we believe that this is this collaboration of those two people that make great feature and that one of the reasons that make, we believe, Rainbow Six Siege a good game.
So now, Geoffroy, methodology.
Yes, because what do you do in practice when you have guys like Leroy and his team playing around with lots of marbles and plates, just adding them in the game?
And so because we have lots and lots of data, and sometimes it's hard to know what to look, because you cannot monitor everything in the game.
So what we try to do is try to define a way of finding issues more quickly.
And then once we have that, try to dig deeper into the data to find exactly what's wrong.
So the process is actually the following.
We start by trying to define problems.
And once we have that, we try to find the causes, like making hypotheses with a community, with a design team and everything, and then using data to validate these causes, being, OK, is this the actual problem that we have in the game?
And once you have that, you can start designing solutions and then implement them in the game.
What we'd like to do is to start with focusing a bit more on the defining problem part, because it's more data heavy, and it involves a lot of collaboration and discussion between our teams.
And for that, we need three steps.
You need to define the context, to find the metrics you want to look at, and then set priority to find what problems you want to tackle first.
So choosing context.
As you've seen before.
we don't have a single game in Rainbow Six Siege.
We have lots of them.
If you're a Cooper player and you love to play Secure Area or you're a Diamond player playing, like, Bomb on different maps and everything on different platforms, this is not the same game.
So trying to balance for everyone at once is not feasible.
And so what we do is that we are looking mostly at people in terms of skill.
Because we have this distribution of skill in Rainbow Six Siege.
So we have lots of silver and gold players in the middle of your distribution, a few copper players that are less good than them, and a few very good diamond players.
And there are multiple approaches to this.
You could decide that, well, I want the experience to be fun for these low-level players because they're having a hard time at the game.
So I want to make sure that they're having a nicer time.
You can try to balance around the cover of your population for the silver and gold players.
Be like, OK, these are where we have the most players.
We need to ensure that it's balanced for them.
But what we actually do on Rainbow Six Siege is we try to balance for these top players for multiple reasons.
The first one being that we're looking at ranked matches.
So we want to have an environment that is as competitive as we can, because these people are playing to win.
They're trying to optimize their strategies.
They're trying to figure out ways of exploiting the game mechanics weaknesses and everything.
So these people are the most involved in your game balancing.
And we have that.
And you also have pro players, streamers, like people who have an impact on how your population, how your players see the game played at a higher level.
And what we see is that usually strategies that are played for the.
at the top of the ladder, usually makes its way towards lower skill levels.
So what you see today in diamond, you see tomorrow in platinum, and then a few days or weeks later in gold and silver.
So by focusing on these players, you're trying to find the biggest issues that you have with your game.
And when this metagame makes its way to lower levels, you make sure that it's more balanced and it's more enjoyable for your population.
So you have your population.
We have lots and lots of data to look at, from the gadgets to the weapons to the maps, the objectives, everything we can look at.
So this is going to sound like we're just looking at two metrics, which we are not.
Every time we are trying to put as much context as we can.
But with that being said, we are still looking to try to get a broad overview of what's happening in the game.
And for that, we are focusing on two metrics, basically win rate and pick rate.
to then map the operators on this graph to see where the metagame is at at the moment.
So we have pickrate, which is an indication of whether operators are over-picked or under-picked.
Are they attractive to players?
Are they not fun to play?
People don't want to play them?
So we usually split them, but try to have more blurry lines.
We don't say if the pick rate is above that value, then it's an issue.
We're just trying to have it as centered as we can.
And the same goes with the win rate.
If you have a very high win rate, you're overpowered.
So it's not a nice experience for the players because the game is imbalanced, and you don't want that.
So we tried the same way to bring operators more towards a more interesting place.
And once you have that, you have a quick overview of your metagame.
So these are the defenders in Rainbow Six Siege in last year's season three.
And you can see that we have two huge issues.
We had two huge issues at the time, the first one being ELA.
So if you're looking at the top right-hand corner, you would see that this new character that was released was winning way more than other people.
This is the win delta.
So the.
increase in win ratio when you have the operator on your team.
And you can see that people realized that this operator was way too strong.
Like, her pick rate was insane.
It was more than 90%.
She was everywhere, like in Pro League, even in Copper, in Diamond, like everywhere.
So you know that you have to do something about it.
And on the other hand of the spectrum, we have Tachanka.
So I don't know if you've played the game before, but this has been a big issue.
Like, some things are easier to balance because in terms of power levels, so we're going to talk about that.
But sometimes it's a core mechanic that you need to change, and it takes more time.
You can adjust both this operator because he's standing still in a game where knowing someone's position is crucial in terms of intelligence and everything.
So, yeah, you need to have bigger production processes to deal with that.
So once you've seen that, you can then set priorities.
What we try to do, we give a higher priority in terms of balancing to operators that are overpicked.
Mostly because when you are picked 100% of the time, when you have five options for your team, it means that you are taking one of these options instead of having lots of different situations that could emerge.
you are fixing your metagame into one situation.
A good example of that is Ella.
So we recently nerfed Ella.
And as you can see, so she is the top pick rate.
And so she went down to the middle of the pack in terms of pick rate with the latest season.
And sometimes you could have another operator taking someone's place and becoming the next superstar.
But this time, what we saw is that by bringing Ela more in line with the other operators, we gave room for the meta to grow.
Because you could see all the other operators shifted in terms of pick rates.
And so this challenges the metagame.
This pushes it forward.
So they're trying to take care of that.
Another good thing that is not an absolute truth, but a good rule of thumb, is that usually it's easy to fix operators when their win rates are correlated with the pick rates.
Because it means that either they're good and people know it.
So they're playing them, or they're bad, and people know it, and they don't play them.
So by nerfing top pick operators that are winning a lot more, you're decreasing their attractivity, and you're bringing them more in line instead of balancing, and vice versa.
Because if you have an operator that's never picked, but you buff this one to increase his pick rates, you just get an operator that is way too strong, people will notice, and then go straight to the top right-hand corner.
So by having parameters that you can tweak in terms of weapons, in terms of gadgets, you can most of the time do that easily.
So we saw that with LR, which was easier to fix.
But touch and key is another issue.
So the conclusion is that you want to grab those quick wins.
If something is problematic, you see it every time.
People know it's broken.
You have to take care of that as soon as you can.
But don't forget that sometimes you can also give some love to operators that have fallen out of meta to just like increase their attractivity mixed with the nerfs to like challenge the meta and make sure that everything is becoming interesting again.
And we have a great example of that.
Blackbeard.
So if you've played the game, Blackbeard was released during season two, so three months after Frost.
And it was a huge issue when it was released.
So this is where it was at.
So if you remember that, it was the exact same spot.
You're winning all the time, and people know it.
And when you have that, so we had the data, we had the players feedback, we had everything.
We're like, OK, we need to address Blackbeard.
So cool, thank you.
The data was showing us we needed to act on Blackbeard.
So it was about now to try to find the causes and validate the causes of what was the issue with Blackbeard.
So to this, we have many processes, the first one being the qualitative feedback.
As I mentioned, we have a pro player session that mean people that come to the studio, they play, and we have focus group.
And since we are focusing on Blackbeard, because we knew it with data that we need to focus on him, So it allows us to converge faster on those topics.
We have a pro subreddit.
We even look at Reddit sometimes.
And we have also designer playstations.
We gather together.
We play specifically centered around Blackbeard.
We try to recreate situations and see what was the problem.
And if you know a bit of the game and you know Blackbeard, it was quite easy to make an hypothesis about what was the problem.
meet the offender, the shield.
So just to give you a bit of context, Rainbow Six Siege has a high lethality in terms of gameplay.
That means one bullet from any weapon in your head, in the head of an enemy, eliminate the enemy for the duration of the round.
The ability of Blackbeard was to have a ballistic shield on the top of his rifle that was protecting his head.
So it was making a huge difference in terms of gameplay.
So quickly enough, we were like, probably the shield might be the issue on this one.
So it was just a hypothesis.
Then we go back to Geoffroy and ask him, OK, can you look at the data and see if there is a correlation between the shield being an issue and something in the data.
And they come back with a lot of data and it was quickly clear that just with a kill-death ratio, for example, that Blackbeard was killing more than he was dying.
So clearly, on operators, that was too safe.
So, okay, and I just say, go work on the shield.
So now, I will explain you this story, because it's really interesting, because you will see that it's a two-step solution.
Because at that time, we knew that we had the problem, and we knew we had to work on the shield.
But at that time, we didn't have the kind of process that we have right now.
That means the analysts were still on one side of the team, like far away.
They send us info about Blackbeard, about the shield.
But then we came back a designer, on our room of designers, working on the solution and the implementation.
So I'm like, OK, guys, let's act on the shield.
And we make a change.
The shield was destructible.
At the beginning, he had 800 HP.
So we decided to change that to make it 150 HP and add a second shield.
So you could have a second shield.
That means overall 300 HP.
but 150 HP per shield.
And it creates a nice mechanism of resource management.
And since it was the only ability, we felt a bit bad to just nerf him down to this point.
And so we make that.
We believe it's interesting.
We make some PlayStation.
We discuss with pro and stuff like that.
We play it.
And we're like, that's good.
That's good.
We were even like, pfff.
Maybe it's too big as a nerf.
Like maybe we might have put him in an underwhelming position.
But you know, at that point, it was the biggest issue we ever had in the game.
So we were like, we need to strike hard to show that we care about the meta.
And there is no overpowered character under our reign.
So confident enough, we push those changes.
And we look at the data.
And nothing changed.
Like, literally, nothing changed.
It was way above the other operator, and we were like, oh, wow.
So, wow.
So we came back to our analyst friend, and we were like, okay, so we validated that it was a problem.
We validated that it was a shield issue.
We nerved down the shield like, hey, insane.
We even took some clash from the community and stuff like that for doing so.
And the guy is still like super overpowered.
So guys, we need your help on this point because we don't really understand what is going on.
Yeah, and so at the time, we were super happy, because having designers coming to your desk being like, oh my god, maybe he loves me or something.
And so we were like, but we can actually help you, because we have lots of data about Blackbeard.
Like Kirill said, we had information about kill, death, but more especially about the shield.
Like we knew how much damage the shields would receive, how many times it would be destroyed and everything.
So just by looking at the data, we saw that with the new shield that was supposed to have a low amount of HP.
it would break like one round out of 20.
So it's like, OK, basically it's not working.
It makes perfect sense that nothing changes, because, well, it's still heavily protected.
And so what it is that, well, actually we have the data about the damage distribution.
So we can look at how much damage the shield would receive.
And if you want to set a percentage of.
the times the shield should be destroyed, we can do it.
So what we did is that we asked them.
They wanted to have about a third of the time the shield to be destroyed.
So we just did a basic model trying to add two distributions for the two shields, depending on whether they were used or not, to see if we put this threshold of damage at this level, then this percentage of shields would break.
This is what we did.
We came back to the design team saying, hey, we've solved it.
And so you should put it down to 60.
And Lira's reaction at the time was like, no way.
This is just insane.
You could just put a piece of paper on that trifle, and that would do the same thing.
That's two to three bullets at most.
But we're like, guys, we do math.
We are pretty confident about this.
And so luckily enough, this guy didn't.
So they're like, OK, I guess maybe you're right.
And so they trusted us on this.
And then they patched it.
And the community's reaction was exactly the same.
Like, everyone was like, this is the biggest nerf in the history of Siege.
Like, this character is useless.
Like, nobody will play Nerf Beard ever again.
And so we had like memes for months and months after that.
Except that we did the change and actually it finally brought Blackbeard back in line with the other operators.
He was still good.
Like, he was still one of the top faggots of the game, which is like what he's supposed to do, because that's what his gadget does.
But like, he was actually a good operator.
And it took months for the community to figure that out.
So the pick rate went down to 2% or 3%, and slowly went back up to the point where today it's now played in Pro League at the top level.
He's even considered one of the best operators in the game.
And except for maybe one change on one of these here, nothing changed since we nerfed it for the first time.
So just by trying to get your data more carefully and trying to work together, trying to find solutions and how to fix them using data smartly, you can do great things for your game.
Even if it's counterintuitive, even if you're playing the game, you can get some information that is really useful.
And we meet Capitao later.
Let's go back.
Bye-bye, Capitao.
And just to finish this on this example, you have to keep in mind that here we are joking about it.
But at that time, imagine we were working on Anbu 6 Siege.
We were making some design.
So design documentation, prototype test, the two shields, some new mechanics, like you have a resource management.
We send those documents to programmers, development team, production team.
They make the change.
They spend time and money working on it.
Then we put it into the game.
We look, and nothing happens.
Nothing happens.
It's pure waste of money.
It's like if we have put money on the fireplace.
The same thing.
So this is where we realized, OK, guys, we need to change, and we need to do the things differently because we're wasting so much time.
And just by talking with you, having you close to us, it makes so much difference in terms of impact into the game.
So we should never, ever leave again what we had with Blackbird in terms of production waste.
And we should always try to have you and that with us in any kind of discussion and anything related to design.
So Blackbird was kind of the turning point of our game.
relationship, I would say.
So now I will give you another example with Capitao, a faster one, just to recap, to show you, yes, Blackbird was a big one, but this process worked super fine and super fast, also for smaller adjustments that we might encounter into the game.
So let's go back with the process, you know, like define problems.
You look at the capital, it's a bit of the charts.
So mostly the data that is detecting that.
Then we try to find the causes, so we discuss with Pro, community feedback.
So the causes from this guy went, you know, like, yeah, he has good utilities, the ability is okay, but the weapon are underwhelming.
And since Rainbow Six Siege is a shooter, we believe that the weapon should need a little help and a little buff.
We validate the causes, we look at the data, yes, the capital weapon was performing lower than the other weapon, and the Geoffroy team validates that.
So we decided to push a solution where we were changing and making some variation and some buff of the weapon, sorry, using again this time the intelligence of those people.
And this is exactly like the patch note that we were communicating, because the cool thing also is that, When you have this kind of process, and when you're working with those people, everything makes sense.
You can explain at every step what you are doing and why you are doing it.
And so it doesn't mean that you will always succeed.
It doesn't mean that every time you hit the spot.
But at least there is a reasoning, and there is a math behind it, I would say.
And even when you talk with community, it's easier for them also to understand what you are doing and why you are doing it that way.
So it's really valuable.
It allows us to have those kind of patch notes that are super clear of.
what we are doing.
So quick recap, so we saw all the steps.
Define problem that imply a lot of data.
Find causes where the qualitative is super important.
Never underestimate the focus groups, the propagation, the survey, and stuff like that.
And what is important is that at the end, when you do your change, you need to follow up your changes.
This is what happened with Blackbird.
This is what happened with Capitao.
Every time you push something live, you need to follow up and do the same exercise to see like, okay, is everything balanced?
So in terms of conclusion, if there is one thing that we want you guys to take away today, it's not our process and stuff like that because it's really specificity of how we work on Rainbow Six Siege, and I don't believe things can be applied right away in any game or any other project.
But at least keep in mind that features should be owned by both game designer and analyst, at least when you're working on a live project.
And we really believe that this is part of the success and why we are...
we are working like that on Rebooting Siege, is really like sharing everything with them.
And I know it's hard because as game designers, we used to have kind of the last call on design topic, but you really need to learn to share with those people because they can really like add value and just make you, take better design decision, being data-informed, and make better design solution.
And for now, there is sadly only people that do analyst study, that come from school with math and statistics, designer on the other side, doing design school and prototyping and stuff like that.
And maybe in the future, there will be people that will have both skill sets, but until then, this is how we work on Rainbow Six Siege, one team having analyst and design working together on the design.
If you have questions, guys, thank you.
OK, question?
Thanks very much for the talk.
It was really interesting to hear you talk through perception versus reality as it pertains to the game.
I've played a lot.
A lot of your anecdotes about.
Blackbeard in particular there reminds me kind of, you know, thinking about the most recent update, where there's a lot of conversation around a character called Lion that you introduced in March.
I'm sort of wondering, I'm sure it's very early, but how are you accepting information around this character where in the most recent Pro League, we're seeing very high pick rates for Lion in Rainbow Six Siege?
So what is the information telling you and how are you processing it around this character in particular?
Balancing so quick.
So thanks a lot for that question.
So the question was, with the recent release of an operator that is working, he's been working really well, especially in Pro League, so for our eSports scene.
So what do we do to deal with that?
So most of the time, before you release, like Leroy said, trying to fine tune the balance in the power level of an operator is extremely hard.
Because the context that you have, you have PlayStation with designers that have different levels, playing on a specific case with a lot of variance.
So usually for that, the pro players are your best allies in terms of position.
Because they can help you see what's going to go wrong, try to tune the values before you set it up.
And what's important is that you make sure that you can fix it once it's live.
So you need to have founders that you can tweak.
And so the issue about what you asked us was that once it's released, what do you do?
Because the point is that.
It's kind of hard because it's really early.
Sometimes you don't know if the operator is extremely strong.
If people haven't learned yet to play with or against, like in some situations, we've seen the win rates decrease for weeks after the release of an operator.
So it's not always easy to have that.
So sometimes you have clearing issues.
You would see that the win rates of a team would spike and stay there for at least one or two weeks.
But why we want to?
avoid is making a decision based on like one or two days on data.
So we usually wait at least one or two weeks to let the metagame to settle, to see what's happening, and then once we have that we use these parameters to tweak the issues that we have.
Actually, it's quite similar to what you just said.
Looking at the data for Blackbeard, it's in that when you push the update, it took some time for the community to adapt to the new meta and find its place.
So you won't see the result right away.
So how do you approach that?
What do you know if the change actually get the results we're expecting?
How long are you waiting?
Do you track a number of games?
Do you know that kind of information?
Are you based on competition, e-sports scene?
When do you know, oh, you know what, we need to revisit that character again, because clearly it's still not enough.
Again, it's balancing cell process.
You might talk, you know, like.
Yeah, so we have that balancing cell.
So what we do is that we release the content.
So two weeks after its release, we are checking whether it's working or not.
The thing about using product data, this is something that we've looked at, but it's extremely tricky.
Because you have a very small sample of data, because we don't have that many matches.
And let's say you have one very good player, the best player of one's team, who's decided to start playing a new operator.
And then.
this guy could win a lot, but he would win with someone else as well.
And you could get a situation where you would have this very tight match where they would play the operator for eleven rounds and sometimes some complete stomps and the operator would be able to play it for five rounds.
So sometimes some matches would...
way more heavily than others.
So usually we try to focus on the metrics for top players and qualitative feedback from pro players to know how they feel, what's curving the issue, what they think make them lose or win with a specific provider.
And two weeks is mostly the time that we wait.
Like after we release the changings, we wait two weeks.
And then after two weeks, we look at the data, the player, and we act if we need to act.
If I may, just the two weeks, was it because just you found that usually it was like enough match were played, and so that you get good information?
It depends on the amount of data you get in two weeks, or was it just you pick a time in two weeks?
We said two weeks because it represents a number of games in our game.
But yes, it's kind of a threshold in terms of quantity.
The game tends to stabilize after two weeks, most of the time.
Thanks.
Hi, my company also got analysis and game designers, but it's kind of hard to put them to work together.
It's like sometimes the game designer thing, and they don't understand the games, and it takes time for the analyst to tell how to figure the data out.
Like, because my company is in China, and the market is very competitive, they got no time to listen to the analysis, and tell them the mathematical things about the data.
So my question is like, in your team, how could you figure this out?
This question comes a lot.
So it's about how do we make an analyst working with designer, or at least most of the time, designer listen to analyst.
When we had this example of Blackbeard, when he came back saying, guys, it should be 60 HP, obviously the first effect was to not listen to those people that are not designers, that don't know how we are making games.
The thing is, as I mentioned at the beginning of the presentation, we had a team of Rainbow Six Siege, and we were playing all together.
And this is, to me, the key part is that those guys, they were playing the game even more than us designers.
They were so embedded into the game that at some point, to me, it was like, OK, so clearly you speak the language of the game.
When we are talking about what happened in the Pro League last night, or what are the new tendencies on Reddit, those guys were like, hey, I saw the match, me too.
So they were so connected to the game.
that there were no gap between us.
And that's why we are always saying, when you're working on a game, if you're an analyst, if you're a programmer, if you want to talk to a designer or people that are handling the content.
And even sometimes, designers should play their own game.
But yes, you need to spend time.
playing the game, because this will give you the language of the game.
And then you will be able to speak the kind of set.
Because we both have two different jobs.
Sometimes he talks to me about the Simpson paradox, data, SQL, basis, whatever.
I'm like, woohoo.
And if I start to talk into design, prototyping, and stuff like that, he's like, OK, guys, I don't know what you're talking about.
But the things that we are in common.
eat the game.
The language of the game is the fact that we share the same passion and love for this game.
So to us, at least, this is how it works and how we notice each other.
It's because we share the same love of the game.
So play the game and show to the designers that, yes, you are as much involved as them into the game.
Yeah, and the same way you have like, it's dangerous to have designers using data without knowing statistics, it's extremely dangerous to have analysts using data without knowing the game.
Like sometimes you have a bias that could be easily explained by the fact that you cannot have like this operator with this one because it's a bad interaction.
And if you don't know that, you could take the wrong conclusions and it could like shift all your conclusions way off.
Hello. How do you choose and manage the data you want to track?
Do you plan in advance what kind of data you want to have?
Do you choose to track new one when you discover there is a problem?
Yeah, that's a good question.
So before we had this process, it was like when you're working on a AAA, you have to make some decisions.
You reach a point where you need to ship, and you reach a point where you need to cut some stuff.
And what were the things that we were cutting every time at the highest level?
the data.
People are, hey, we need to add this tracker and this stuff.
Hey, we don't have time.
You need to ship and stuff like that.
Now, since we are working that way, since we have this process, the cool things of having the designer being embedded at the beginning of the conception of a feature is that when we define the spec of the feature, now we have directly what they need.
OK.
For us, in terms of process, gate, and validation, we are not going through a gate if we are not data ready in terms of code, in terms of feature.
You see what I mean?
Because now this is totally part of how we reach the goal, or we reach the beta of a feature.
And since it's a since it's a part of the design, you know, like exactly like, oh, if there is not this ability or this cool down, we don't ship the thing.
And now there is like, OK, but if there is no data, we don't ship the thing.
And in terms of spec, maybe you want to talk about how you decide what kind of data you are looking for.
Usually try to, there's a lot of discussion involved with the designers to try to figure out the mechanisms they want to put in place and make sure how they can be used.
Like, let's say you have something that has an impact, a debuff on someone else's.
You want to check whether the people who get killed who are affected by this debuff.
You want to see the number of times it triggers.
You want to see, like, everything you can think of.
Because once you have an issue, if you don't have tracking, if you cannot verify that, well, this effect is way too strong, then you'll just go with a black belt again.
You're like, OK, well, I think we just tune the value and see what's up.
So this data list you want to track came from the analysts themselves?
Or it's like a mutual process?
So the analysts put it directly in the game specifications.
When you're doing that, you should send us an event saying, OK, I've done that.
Exactly, as we are bad to ask for reporting, because we ask for the wrong reporting.
We are also bad to ask for the tracking.
We think, OK, it should be good to track this kind of stuff.
So what do you want to know exactly about this guy?
But we want to know that.
Yes, but you should try more of that and that and that.
Thank you very much.
Hi.
Before I ask my question, I just want to compliment you guys and your team for one of the biggest turnaround stories in the history of this industry.
Thank you.
It's absolutely incredible.
My question is, you guys have been looking at pick rate and win rate for a long time.
Is there any big insights in those data that you guys have found in looking those numbers that come up and down, like stories that you're like, if we only would have.
looked at win rate and pick rate through this angle or through this question, it really would have saved us a lot of headaches early on.
Yeah, so like I said, most of the time people would be like, OK, you're just looking at pick rates and win rates and call it a day.
What we do, what we've been trying to do a lot more is try to add more context into that.
Because sometimes, like.
No metrics is perfect.
Like, you could have something that would win a lot, but that could be very situational.
So let's say if you would use that operator in a different context, it wouldn't work.
So what we're trying to do is just to add that situationality to the different operators to put more context.
Because every time we want to define a problem, say, OK, this operator, there's a problem with it.
We want to understand why the pick rate or the win rate is where it is.
So we try to add as much context as we can.
So usually it's hard because you would release some data to the community and they'd be like, well, you should have thought about that.
And we're like, we did.
We're just going to send you a 10 pages report on why it should be that way.
Yeah, you should be extremely careful.
And that's why you need a list to do that.
Hey, so your slide earlier had a couple of outliers, uh, Ayla and Tachanka. Um, you nerfed Ayla and you kind of gave Tachanka a buff with that shield. Uh, do you feel like Lord Tachanka is in a good place right now? And what sort of design challenges does he pre- present?
So the thing with Tachanka is that there is, so the design issue is that, as we said, this game was made in a context where, when they were working on it, we didn't know how Siege was going to be played.
And as Geoffrey said, the core ability of Tachanka, deploying a turret that means...
trading his movement ability to firepower, it's not something that works in Rainbow Six Siege.
Because of the destruction, because of all the Intel cam, not being able to move is not something that works.
So the thing is that if it was not Lord Tachanka, we would have revamped the operator and maybe totally changed it, like maybe remove the turret, do something else.
But at some point, it becomes kind of, I would say, God.
for a lot of people.
So we're in this situation of if we really want to address it, we need to change it in a way that it won't be Tashanka anymore.
See what I mean?
It will be a new operator.
And it's kind of a difficult decision to take because it's not only program design at this point.
It's also what the community, the stories that they are making with it.
So it's like, to me, since As The Lord is above us, I would say he's a nice place right now.
Thank you.
Back when you were talking about Blackbeard and the analysts put forth the solution that was at least according to the data, the correct one, and then the designers are like, oh my god, that's way too low, we'll never do that.
How do you reconcile what the data suggests and is probably right versus the opinions of the designers and their more like human reactions into what they believe the game should be?
That's the thing.
To me, it came like a...
It's because those guys were playing the same game as us.
So if those guys were confident, like having we put the patches, and then they are going to play those Blackbird in matches, and they are fine saying, like, yeah, 60 HP is enough.
Like, it still creates this interesting gameplay at the window and stuff like that.
But they have as much as good feelings than us, plus because they know the same game, they are playing a lot with us.
Plus they have the math aspect of it.
So I mean, why not listen to them?
And yes, it was a bet.
But if you look at what we did us before, yes, it was a bet in terms of community action.
But their solution was super simple.
We just changed the value of a shield.
So in terms of cost versus reward, if it works.
It was kind of easy to try it.
The thing was like, one of the things that we were I think a bit afraid was like, if it doesn't work again, it's like we really look like stupid people.
You know what I mean?
But to me, I'm very humble when people talk about mathematics and stuff like that.
So I tend to believe them when they do the job.
Why am I speaking in this mic?
Okay, so Siege has a very strong community and that's because of a lot of the effort that you guys put forward to, you know, listening to them and making them feel like they have an input in the game.
But how do you decide how much weight to give them at the table and, you know, how do you try to avoid situations like, you know, the infamous Bartlett University?
That's the thing, that's why we call it qualitative data.
That means we have process for that.
That means exactly as you are tracking data, like it's super serious, a programmer putting some hook in the code.
gathering qualitative data, that means coming from the community, coming from the pro.
It's also a process that is super serious, and you have where you have metrics, you have a process on how you gather that.
It's not like, oh, this morning I went on Reddit, they're talking about this guy, I think he's bad, we should do something.
You see what I mean?
It's more like we say we have pro players that come, we have focus groups, we have dedicated people that know how to...
manage and handle those focus groups with animated them, ask questions.
We prepare some kind of survey, and we make a correlation between data and what they are saying.
Most of the time, as Geoffroy said, we have a lot of data in this game, a lot, a lot of data.
And mostly, what qualitative is doing, it's help us to focus and look at some aspect of the game.
OK, there is an issue with this guy.
People seem to complain about this aspect of the game.
Let's put the light on the data and see if there is a correlation.
Thank you.
I was wondering, in your experience, is there a sample size where you thought that this relationship just accelerated between game design and the data?
Where previously maybe it wasn't so highly correlated?
on the data with the players, and then after, we just a certain threshold of whatever, that you really started getting a lot of info out of this.
You mean sample size, statistically speaking?
Of the player base.
I mean, if you went from 500,000 players to like a million, or if you have a player base of 100,000.
Is this relationship, could this relationship still continue between game design and...
Oh, okay, yeah.
You need to have reliable data, because otherwise you would just make assumptions, or you couldn't be as precise as these guys did.
So we are lucky enough to have a big player base, so we can focus on a smaller group of top players.
So basically what we do is that we adjust the...
the size of the context to have enough information to provide them with what they need.
Right, so sorry, so just to reiterate, was there a point where you felt like when the player base went from 500,000 to 1 million or something like that, or accelerated I think with 20 million or whatever, was there a certain point that you really felt like there was a strong connection between the game design and data, or where previously it wasn't such a strong connection?
No, we always had a...
enough data.
We always had lots of data.
We never went down to 10,000 players a day.
Even just considering PC only, we have enough players to be tweaking.
We never reach that point, so it's really hard to tell.
And I never see, even with the game growing, I never see a correlation of increasing of the quality of the version.
I think we were always above the cool threshold.
All right, thank you.
Hi. First of all, thanks for sharing your methodology and the way that you're approaching some of these challenges. My question has to do with in the, what you've shared today or even beyond it, are there particular topics or parts of this methodology that are difficult for you guys to share with players? And if so, what are the challenges to being more open or more thorough in explaining why you do what you do?
One thing that I find quite challenging is sharing data, because usually when you send that to the payers, you would get either people who don't know statistics, and you would get like very.
like very very wrong interpretation and you'd be like but no you didn't understand and you would get people who do know statistics and be like but have you thought about this and this or this effect and you're like well we have but we didn't want to just give you too much information because people would not understand and it would take too much time. So like pushing information and see the community reacts like in two ways.
that sometimes can be frustrating.
So trying to just focus and make it as simple and clear as possible, I think it can help.
Do you think that narrowing, sorry, a follow up, but do you think that narrowing the conversations to buckets of very experienced people who are discussing this versus parts of your community that are less experienced, do you think that would have an effect?
Or is that something you have any desire to do in the future?
Hard to tell.
I think at some point we had like a, we did like a, we released some raw data to the full community.
Like we let the full, the big community, like Reddit and all of those people, play with the raw data.
But the thing that there were one bias is that we spent a lot of time trying to communicate with the community, as Geoffroy said, like trying to explain a simple and clear concept about how to work with the data.
And we were surprised to see like the, I mean, the way they were handling it, they were like, every time someone like, well, not every time, but there were some posts where people think like, okay, look, I saw that in the data, you know, if I do this and this, it shows that this guy is broken.
And there were often some people saying, hey, but wait, remember, it depends on the context and stuff like that, so you know, they start to, personally, I really believe that, and because it's maybe I'm like an idealist, I really believe that the way you're talking to people depends on the way they will talk you back.
They will talk you back.
So if you think people are smart and they can understand, the more you explain them and the more you take time to, I don't want to say educate them because I don't think they have the same sense in English.
But you really try to be transparent, and it takes time.
But honestly, I think we saw some.
Some interesting, uh.
People are smart.
Like, there are smart people in the gaming community that can help you monitor this.
It's only an issue when you're not able to respond to these people.
Because I want to talk to them.
Because these are passionate, like very interesting topics.
And you want to share that.
You want to say, yeah, we thought about that.
And we saw that this was an issue as well.
But we don't have time.
But I think.
Releasing data to the community is something that we've been working on to have a more stable clean process.
And I think it will help in these discussions.
Yeah.
Thank you so much.
Thanks.
I think we have time for one last question.
Am I?
Yeah.
Because we need to clear the room.
OK, sorry.
But I want to ask you a question.
My question is kind of out of the session, but I was curious about how the culture was made to hire an analyst in your project.
Because in Japan, on mobile, we're really related to it.
Because for.
the purpose to monetize, we have analysts, but on like in PCs and consoles, we don't have analysts and we don't take budgets for to hire the analysts. So how was the culture made in your company?
It's complicated.
It's kind of started with the lab.
Yeah, it came.
It started at Ubisoft with a user test.
So we had these small focus groups during the development of the game.
And we had analysts at the time, so making sure that we could get some tracking information in the builds, and then see what were the players actually doing.
And so since we had that, we had people who were like, well, Once you've launched your game, we can also help you see what people, what players are doing live, and then send you some reports about what's happening.
So it started like this.
And so what we did on Rainbow Six Siege is going further than this and making sure that we are communicating and making the best use of that data.
And to give you a quick last anecdote story, it's like when I was, one of the first things that I was doing on Cities that I was doing on, at some point, I was asked to work on the kind of experience, monetization, you know, a lot of numbers and balancing and stuff like that.
and I knew that was not super good in math.
So I was like, okay, who is the smartest guy on this floor?
And this guy, I was like, yeah, I know math.
I said, okay, come at my desk and I start.
And this is how we met together.
And he said, I mean, you're really good at math actually.
You know, like it's interesting what you're doing and blah, blah, blah, blah, blah.
And then we.
Yeah.
Thank you guys very much.
Thank you.
