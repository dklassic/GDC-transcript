Hey guys, thank you so much for coming to our VFX presentation.
Ford and I are looking forward to showing you guys some of the recent work that we have been doing over at ILMxLAB.
A little bit about myself.
I'm Martin Lalan Romero.
I have been working in VFX over at ILMxLAB for about two years.
Mostly in VR projects.
It has been interesting and rewarding for me to be able to bring in my experience from console and mobile games to this new media.
And so I'm glad that I get to share with you guys some of our experiences developing these VR projects.
Before I go any further, I want to thank Mark Teer and our GDC team who made it possible for us to be here today.
So please, let's put our hands together for them.
Thank you guys.
We at ILMxLAB are very fortunate to have worked on this new kind of experience in collaboration with the VOID. And so I'm glad that I get to share with you guys a video with some of the things that we have worked on lately.
So random.
At ILMxLAB, we focus and pioneer real-time rendered interactive immersive stories on a variety of platforms, including virtual reality, mixed reality, and more.
ILMxLAB is best known for a few major immersive projects today, including Carne y Arena, directed by Alejandro Gonzalez-Se√±orito, in partnership with Legendary Entertainment.
Reflections, the real-time render, retro cinematic demonstration created by Epic Games in collaboration with ILMxLAB and NVIDIA presented here at GDC last year.
Star Wars, Secrets of the Empire, and Ralph Breaks VR are critical acclaimed hyper-reality projects in collaboration with Devoid, which we're here to talk about today.
And also Vader Immortal along with other projects.
So now, what is a hyperreality experience?
How is this different than any other VR experience?
Excuse me.
Let's start with a brief explanation of the experience for those of you who haven't tried it yet.
Which, by the way, how many people here have gone through any of the Void experiences?
All right, quite a few.
A group of four guests put on a VR helmet with built-in audio and communications.
and a haptic vest with a backpack with a laptop computer.
What each guest sees is generated 90 frames per second stereo on their backpacks.
They walk around a compact stage with walls they can touch, but these walls are then displayed in virtual reality, looking like the Star Wars Secrets of the Empire and Ralph Breaks VR environments.
The boy uses various tricks to make them feel like they're moving through a much larger space by using by adding things like multi-sensory elements.
What is that?
Things like heat, wind, and environment sense throughout the journey so that the user really gets a truly immersive experience.
As an artist, I highly recommend any of you to go ahead and check it out.
I'm not just saying that because I worked on these projects, but it's an awesome experience that speaks to the innovation possible within virtual reality.
Now, for those of you who haven't seen the Rocket Ralph Breaks VR trailer, here you go.
Over here!
Over here!
Over here!
Over here!
Over here!
Over here!
Over here!
Over here!
Over here!
Over here!
Over here!
Over here!
Over here!
Over here!
Aren't they precious?
Surprise!
We're taking you into the internet.
There it is, Gamer Street.
Welcome to Dunder Dome.
Just start pushing everything you can see.
Hurry, guys!
Don't panic.
Hey, anybody?
Virus located.
Ah! Creepy eyeball!
Ah!
Ralph!
Ah! Get off me, you stupid kitty buddy!
Prepare for immediate termination.
Uh-oh! Security drones!
We gotta go help him!
Let's go!
So I'm gonna talk a little bit about production tips and some of the workflow that we utilize in some of these projects.
The first example has to do with proceduralism, repetitive tasks, and being more productive.
What do I mean by that?
Well, we use a production visual effects software to do things like generating 2D and 3D assets, look dev, and for all kinds of visual effects prototypes.
The reason why is because of its nature, which allows us to work faster by reusing non-destructive systems or tools over and over, yet being able to add more content to these tools whenever needed.
Obviously, it takes time to write a tool, but once you create it, it will save you hours of repetitive tasks.
And the experience.
We engage players on a level by having them press a series of buttons that surround.
these circuit lines around the buttons.
Of course, there's a progression where there are different states of success visually displayed by how much the circuits light up to advance in the experience.
These circuit lines were an essential part of gameplay by ramping from zero to 100% at a choreographed sequence of events.
We then use a material inside of the Unreal Engine with a parameter that will be controlled via code.
As you can see, we're dealing with multiple circuit lines.
We were dealing with about between eight to 10 per button.
So there was a lot of states that we needed to call on each one of those buttons.
The design team came out with a concept that at a glance was straightforward to do this pretty much in any 3D package.
And it could have been done something like this.
You trace some curves, you strew some geometry out of those curves.
You modify the UVs for each one of those depending on what the progression was going to be like.
You select each one of the pieces of geometry, rename them, export them all out, and you are done.
But the problem with this is that that will only cover one set of circuit lines.
As you can see, we're dealing with 11, and we had 11 on one side, 11 on the other side.
And we were trying different things, so the design was changing.
At this point, the design evolved, as it often does, and this simple task will have become extremely repetitive and laborious.
That's when we turned to proceduralism.
We started out by creating the core elements that would generate the circuit lines, keeping in mind the variable time that was required for each one of the buttons, so the necessary parameters were created.
As you can see in the images, we took our spaghetti nodes and put them into a neat bundle.
This kept things clean, and it allows us to input curves, generate then geometry with custom IDVs, control the position, the scale, the taper, and whatever parameters we needed.
If we wanted to manipulate curves, circuit lines individually, we could do that too.
We have full control over the tool.
Here we can see the network that makes up all of the generated circuit lines.
We can easily drive multiple nodes that share the same attribute with one controller.
So at this point the task has been pretty much simplified.
So if the design theme was to change, the only thing that we will have to do was to redraw the curves, but everything else will be generated, which was the theme that we were looking for.
I was inspired to bring this into the presentation today based on one of the topics that emerged from our panel discussion last year, Pathways to Real-Time Visual Effects, moderated by Fred Hooper.
One of the topics was about how VFX artists are able to problem solve and do a little bit of everything and the job to get our tasks done.
So the challenges presented by the circuit lines seem to be a valuable share for this discussion.
All right, let's continue with the slides.
So now we see both sides, the left and the right side of all the circuit lines with the buttons all completed.
So all the geometry can be generated in no time, thanks to our little tool.
Now let's talk about the exporting part.
When I said that each one of those lines was going to be called through code, I mean each one of those circuit lines.
You couldn't just select a whole bundle of pieces and export them out and try to control those in Unreal.
That wouldn't just happen.
Unless you can probably get a hell of the ID number for the polygons, but that was too crazy.
So what we did is we used a Python node right inside of our workflow nodes that will export all the shapes with the proper names needed to be plugged in inside of the Unreal Engine, which was super cool.
We just click the execute button in the Python node and it will export all of the shapes as FBX to a specific directory.
What was good about this process?
One, we needed to provide the necessary shapes to the design team so they could test them right away.
Doing this manually will have taken a long time and it will have wasted a lot of development time, so we ended up just creating a tool for it.
Two.
For each design iteration, not much of the laborious parts were automated, so this left us to redraw in the curves.
Oddly enough, it was kind of relaxing.
You would just have to retrace the curve, and you were done.
Because it would keep the UV position, the taper, scale, everything else would just be the same.
Three.
Now we have a tool that generates shapes based on curves, but also has an iExporter function built in, which is the main reason why we spend the time working on it.
The fun of being in the feds is that there is always new, tricky tasks in production, and it's interesting to see how some of these things get accomplished.
All right, let's move on to talking about Star Wars, Secrets of the Empire, and some of the VFX that we did there, so let's go ahead and watch the trailer.
This is Mon Mothma.
I have an assignment.
The Rebellion needs you and we don't have much time.
You must work together.
You must not fail.
In disguise, your team's mission is to recover Imperial intelligence critical to our survival.
You're with me.
The rest of you, get on that skiff.
Do your best to act Imperial.
No!
What the?
Looks like you'll have to fight your way out.
What is that?
This could be a threat to our entire existence.
For the next few slides, I wanna cover this technique that has been used for a couple of years now, vertex animation, specifically the fluid and the soft bodies type.
As we all know, the vertex animation technique is a pretty good technique to use, but it can get really expensive, specifically while working on VR.
At ILMxLAB, we have a pipeline in place that catches data that...
that we can then bring inside of the Unreal Engine.
Something similar to what the Houdini game development tools team from SideFX has created.
The first example has to do with the lava flea monster and particles.
We wanted to have many of these interactions between objects, animated objects, and particles and effects.
This was a great opportunity for us to use the vertex animation technique.
In this case, we're using the fluid type.
Which, by the way, if you really wanna know more about the different techniques of, the different types that go inside of vertex animation, I would recommend looking at Louis Kroll.
He has a lot of videos on Vimeo and also on YouTube.
Here we see the same interaction of the lava flea and the particles, but now the particles have been converted to measure.
So we get the physics working correctly.
We have the weight and the size of all the particles and everything.
So we're ready to export this out.
Which by the way.
Particles you can also bring in particles, which is another vertex animation technique but in this case we needed the measures because we wanted it that you know, the trunkier pieces to behave and to have the look of Something like lava is splashing around And here we see this is the final image of the lava flea interacting with the particles and it's all real-time at this point We're playing this back And we can trigger the vertex animation via Blueprint or Sequencer.
Here we see a couple of examples of deformation and destruction elements where we utilize the soft bodies technique.
If you have vehicles, you can create this beautiful like crumpling kind of types of effects.
In this case, we have this platform and we're just...
destroying them.
Here's another example of the technique. However, you can go crazy with this type of technique like anything else.
If you single this out, if you go into your development folder and then work and create a bunch of the stuff, it's gonna work just fine.
But I would suggest testing these things with the assets that you're gonna utilize in a full-blown level.
That's when you can start counting how many polys, how many milliseconds this is taking.
We're working in VR, so we get about 11 milliseconds all the time, so.
Getting one of these is, you know, you need to know exactly how many frames you have, how many polys that mesh is going to have to keep everything in control, otherwise you won't be able to get that in your experience.
Why vertex animation?
Because you get a nice interaction behavior of elements without pain.
for something like collision, which is the case of the lava flea monster and the particles.
We didn't want to pay for that collision interaction inside of Unreal.
Doing that with every one of the characters would have been crazy.
It just would never happened.
There is also the fact that you can bake complex visual behaviors and import them into a real-time engine that otherwise would be impossible to do, at least in a production setting.
Now for the next few slides, I wanna cover how we did the netizens and the vehicle density inside of Ralph Breaks VR.
For the close-up vehicles, we utilized 3D models as well as vertex animation.
And for the vehicles, far away we relied on particle billboards and also imposters.
In the slides, you can see the different layers of vehicles.
We had a lot of, there was a lot of vehicles in the label, right, and the internet area, we couldn't just rely on one type of technique to sell the high volume of traffic and netizens as seen in the film.
So, like I said before, we utilized different techniques to be able to have something close to what the movie had, but also a good performance.
For the vertex animation technique of the net essence material, we added controllers so that we could mask out the different parts of the body to be able to randomize and change the color of the skin, hair, skirt, pants, by using dynamic parameters inside of Cascade.
This gave us a good amount of randomization, as shown in the videos, and we were able to get a pretty close look to the movie version.
So for the high density of vehicles and the netizens, we realized early on that we needed different techniques to be able to sell this moment.
We learned that vertex animation, excuse me.
We learned that vertex animation and impostors are really good techniques to use, but the problem with that is you get only so much control on these objects.
To create new objects that you could manipulate or send through path and things like that, we needed to pretty much come up with a new system to do that.
We were able to mimic the density shown in the film by using all the techniques mentioned before.
At the end of the day, it's all smoke and mirrors.
As VFX artists, we do what we have to do to get our job done.
These projects that I worked on were really fun, and I'm so excited to be part of the team.
And I cannot wait to show you guys the projects that we're working on right now.
Thank you so much.
That's it for my part of the talk.
Any questions?
Hey, thank you for the talk.
I want to ask, can you trigger a different part of your bait vertex animation based on your gameplay interaction?
Yes, you can.
Yeah.
You can set it up in the material.
The material that comes, actually, if you look at the material that the guys from the game development tools has created, there's a portion at the end where it has something like the frame.
You can actually modify that parameter, and you can tell it which frame you want it to start at.
So you can trigger it at different frames.
Thank you.
So my team and I, we were recently using imposters in VR.
We were using imposters because we wanted to bring in Houdini effects into our game.
The problem that we found with VR was that because it's two different cameras, we had to get the sprite sheet to be like 8K.
So it ended up taking a massive amount of computing in order to get it so that it displayed two different images to the eyes.
What did you do in order in like?
for in order to use the imposters, did you end up using a massive sprite sheet or did you find another alternative?
We actually didn't have that problem because the vehicles were really far, so we didn't really encounter that.
We didn't have to do anything, actually.
It was pretty much straight out of the box.
We just followed the instructions.
Unreal, the Unreal documentation for impostors, we generated the impostors right inside of the Unreal Engine 4 and didn't really deal with that problem.
So I don't know.
Thank you.
Yeah.
Anybody else?
All right, then.
Thank you, guys.
