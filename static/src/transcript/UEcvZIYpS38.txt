Let's do it.
There we go.
All right.
We're going to get going, guys.
Hi.
Thanks for coming out.
So we are here to talk about making mixed reality trailers and live streams.
So before we dive into the meat of this, we'll just give you a real quick background on who we are and why we're up in front of you talking today.
So I am from the middle of nowhere in Canada, called Winnipeg, and Colin is from a wonderful, beautiful city called Vancouver.
And Colin spent about five years sort of traveling the world and making games and doing wonderful things and exploring awesome places.
On his travels he made a game called Incredipede, which is really cool.
And way back before that, he made the 2D version of Fantastic Contraption.
And then decided to come back to Vancouver.
And VR was amazing.
And then decided to make the virtual reality version of Fantastic Contraption, which we're going to talk about today.
For myself, I spent about 8 years in the visual effects industry working on about 25 or so B to C grade Hollywood movies.
They were all pretty bad except for maybe Chum Scrubber. That one was kind of okay.
But basically, you know, I decided to try to get into something different.
And to make a really long story short, I now make indie game trailers full-time, which is like crazy.
And thanks! Yay!
Which led me to the creation of these two trailers.
Now, just due to the time, we're just only going to be talking about Contraption today.
But basically what ended up happening is calling in about January of last year before the vibe was launched. He's like hey Kurt. You want to try to make in this virtual reality fantastic attraction trip. Yes, I am so on board with that and so Devin Reimer who worked with alchemy was just outside of Winnipeg at the time before he moved down to Austin and I knew they were working on jobs him.
And what we decided to do was shoot both of these trailers at the exact same time.
And when I said yes to this, I actually had no idea that they were going to be the two launch trailers, or two of the three launch games for the Vive 2, so yeah, pressure was on, so to speak.
So, before we look at the Contraption trailer and what we did, I just want to talk really briefly about VR, and it has this like awful...
communications for a problem right now it's really really hard to explain what it's like to be inside a virtual reality and obviously everyone remembers the cover of time it was endlessly mocked and these are actual stock images that you can pay money for you can you you want that in your PowerPoint you can you can get that so but yeah it's like you're in this 360 world where you can touch things and move around interact with stuff and it's absolutely amazing and it's so not whatever that is right So we were like, how do we communicate this awesomeness of VR?
So at the time, we kind of looked at what everybody else was doing.
And when the Oculus first came out, everyone was doing these dual fisheye views, because that was the raw output you got.
And it was really just the easiest thing to do.
But it is awful.
Why would you release that?
That makes everything you're doing look so bad.
There's chromatic aberration on the edges.
Everything is fisheye distorted.
There's two views of the same thing.
Why would you even do that?
It doesn't make any sense.
So that's just really, really bad.
So the next thing people started doing was just doing the raw output of the head mount display.
And we're like, hey, we'll take that and we'll move it around and people can see what the person's looking at and all this.
But the problem with that is your head is like literally the worst camera in the world.
It's super jittery, you're not focusing on anything for any length of time, and your eyes are kind of darting around.
And your brain and eyes kind of cancel out, like all these little micro adjustments that your head is doing.
And when you look at the 2D footage, it's like this shaky mess.
And when you just see this, you can't really concentrate on anything.
It's really hard to look at.
And personally, I get motion sick watching footage like this, especially when it's at 60 frames a second.
It's really, really gross to look at.
So the next thing people started doing was this picture-on-picture thing.
It's like, well, this will solve it.
We'll show the guy in his dirty room going like this and trying to interact with stuff.
And you can see what he's doing.
But the two images don't link up in any discernible way.
It's just two completely separate things that are going on.
So we knew that was bad too.
So at the time, Sony started promoting the Morpheus, which turned into PSVR, obviously, and we're like, okay, here's a company with bazillions of dollars, they'll know what to do, right?
And so their solution, which I kind of like in one way, and we'll talk about a little bit later in another little section here.
They do these virtual head-mount displays, and what they do is they kind of put the person in, and, you know, they film them on a green screen or whatever, and they put tracking markers on their face, and then they remove those and pose, they 3D track the head, and they put this virtual thing on their face.
And it's great because you can see the person's eyes, and you can sense the emotion that's happening that's usually hidden behind the head-mount display.
and that's really nice but the problem is all the shots are completely fake.
They're just reaction shots cut into first-person footage and you can tell that they're fake because they're just going like Whoa! Yeah! Awesome!
You know, and it's not like linking up to the game in any way at all.
It's just like these things that are just spliced in.
So that kind of sucks too.
So when it came time to do the trailer for fantastic contraption, we were like, okay, we don't want to do any of this stuff. We want to do something new. And we wanted to try this mixed reality thing. So let's take a look at the trailer here. And you can see what we did.
Fantastic contraption is a game about building and using your creativity.
The challenge of the game is simple. You have to get the pink ball to the pink goal.
And to do that You have to build a fantastic contraption.
All right.
So that's what we came up with.
And I really think that works really, really well.
So why would you want to shoot a mixed reality trailer?
Well, I just think it's the best way to convey what it's actually like to be in virtual reality on a 2D screen.
And I'm not the only person that thinks this.
Even Valve agrees.
When they launched the Vive, they did their own sort of mixed reality trailer that mimicked ours in a lot of ways.
They had a bunch of people kind of sitting on the couch to kind of show us like the social thing.
They had a whole bunch of different.
You can see the trailer for Tilt Brush.
The way they did theirs is a little different than ours.
It was more of a visual effects post-production workflow than what we did, but the end result is basically the same.
Now that it's been a year since people have started doing this, people are doing it for crazy stuff.
This is like a collaboration Marvel did for the movie Doctor Strange.
They got all these like tilt brush artists to like come in and do all this like super crazy stuff inside a tilt brush.
And they did a mixed reality trailer to kind of promote that.
And now even HoloLens is getting in on this action.
So the five of you out there with HoloLens can try this out.
And you know you get like this crazy DSLR rig with like an extra HoloLens on it.
And you can do mixed reality now with a HoloLens.
And people are doing this live too and Colin's going to kind of talk.
But more about this, but this was a Tokyo game show a couple months back now.
And you've got this woman, she's all dressed up, she's in character.
You've got this crazy, like, camera on a jib, so you're getting these nice, like, smooth cinematic camera moves.
And if you look at the results that are, like, on the top there, you can see it looks like she's part of the game world.
She is like an actor.
She is performing inside of this world.
And that's one of the really main things I want you to take away from this, is that when you're doing a mixed reality video or trailer or livestream or whatever it is, you are performing that game.
And to sort of like hit that home, not to pick on these guys, but this is like an earlier trailer from this game Blasters of the Universe.
This is like when they were just in Alpha, and we can see like all the pieces are there.
Yes, this is mixed reality, but this game is like super fun to play, but it looks like he's like kind of bored, right?
He's kind of just like shooting. He's not like moving around very much. He's doing his thing.
The compositing is not super good. You're always looking at his butt, you know, because he's like in that way in the green screen.
So there's all these weird things that you need to kind of think about now when you're making a mixed reality video that just doesn't normally apply.
And I had to throw this in there because it's great.
Someone did this like, Tilt Brush Bob Ross, The Joy of Painting.
It's it's like 5 minutes you just got to check it out. It's like super good.
So getting back to fantastic contraption. So what we did we use 3 different techniques to kind of convey what it's like to be inside of VR. We composited the player into the virtual environment. That's all the stuff we should on green screen. Then we brought some of the virtual objects from the game into the real world and the 3rd one here. I'm going to talk about for a little bit is we shot an in game avatar with a camera with a physical camera move that was created in the real world.
And this could be literally its own talk on its own, and maybe we'll do it next year or whatever.
But it's like, this kind of technology of virtual cinematography and virtual filmmaking has been used on high-end movies for years and years.
Like, they shot a lot of Avatar this way with a rig like that, but it cost like hundreds of thousands of dollars.
And now you can literally do it with a $150 Vive controller and a $5 Steadicam.
So for the updated Fantastic Contraption trailer, this is actually what we did.
In my basement, I'm just shooting my friend Vince who's playing the game, but we're shooting an avatar rather than a live-action person.
And this has a number of advantages.
It's like way cheaper to do this because you don't have to do the green screen studio and the setup and yadda yadda yadda.
And it works incredibly well.
You still get the sense that there's a player actually playing the game inside of this environment, but you don't have all the overhead involved of actually worrying about a live-action person.
and like an actor and all the things that kind of come along with that.
So we also did this for the Space Pirate Trainer trailer.
And this kind of took it up like another level where the developer for this game had this fully rigged like inverse-kinematic crazy avatar that was being driven by the head and hand positions.
And we kind of shot him mostly from the waist up just to avoid the problems with the feet.
But this works super well.
Like there's no way we could have built a costume that looked this good and filmed them in mixed reality.
So doing it as an in-game avatar was like really advantageous in this situation.
And literally to do this, all you need is this.
It's me in my basement with my friend Vince, playing like this, and I've got my phone as a little remote viewfinder, and you have Vive Control on a Steadicam.
It's a pretty interesting solution.
So I don't wanna talk about this really in depth because it's not exactly mixed reality, but it's something to keep in mind that your game, depending on what it is, or game or product or whatever it is, might be more suited towards that than mixed reality.
There's also this, which is kind of a weird problem with Mixed Reality.
How do you fix that there's a giant box on the person's face, right?
And honestly, aside from what Sony was doing, I didn't really think that there would be a solution to this.
But just like Google fixed it.
They came up with this amazing solution where they get this rough 3D geometry of a person's face and capture their eye movement all across the screen.
Then they add pupil tracking inside of the Vive, put a QR code on the front of the Vive, and then in post they're able to map that new generated 3D face and composite it on top of the head mount display.
And it's kind of janky and a little bit creepy, but it's surprisingly okay.
And it solves that communications problem, it's like you want to connect with the person's eyes.
So it's this really amazing little thing here that hopefully we'll see where this goes in the future.
So to back all the way up to the beginning here, so we'll take you through this entire process.
This all started when Colin and Sarah Northaway were both contacted by Unity, and I guess they sent out this film crew called Breakwater Studios to do this little developer diary on Fantastic Attraction, because it was being built in Unity.
And Colin came up with a brilliant idea of just like, hey, let's just strap a third Vive controller on top of a DSLR.
We'll make that third Vive controller a virtual camera.
We'll align the positions in the virtual space.
And we'll overlay the footage on top, and we'll see what it looks like.
And it looked amazing.
I remember the day that this came out.
And I saw the video, and I'm tweeting at Colin.
I'm like, holy crap, dude.
This is amazing.
We've got to figure this out.
This is super cool.
And so then you know a couple weeks went by January came around. He's like hey, do you want to do this and then we started figuring out?
OK, how are we going to do this for real and Colin kind of looked at the live streaming approach of doing it live and I kind of approached it more from a post production kind of standpoint of like doing trailers and doing it all in After Effects. So that's what I'm going to talk about and then call and we'll talk about the other stuff.
So basically the trick to that is, the troll trick to this is this.
You know, you take a third Vive controller, now you can get like the tracking puck, which is way better than just like elastic bands and gapping tape here.
But you align a virtual camera to your physical camera's location, and Colin gave me this little UI where you can offset the virtual camera in XYZ position, XYZ rotation, you change the field of view of the lens.
You know, I'm just sort of oversimplifying it, but that's basically it.
And then once everything is kind of lined up, you get magic.
And so this is the first test I sort of did in my basement.
It was casefully thrown together.
I'm just like, yeah, the pieces are kind of there.
It's working.
And I got super excited really fast.
And I'm like, I got to move the camera.
So I called my friend Vince over.
He's like, Vince, you got to come over so I can move the camera and have the display.
And so I'm like going through the process of figuring out what do we need to fix?
And what do we need to do to make this actually look really professional?
And so I very quickly realized I needed a better camera.
I needed a camera that was stabilized so you're not getting this jittery motion all over the place.
And the main thing that I started to realize is that you needed to choreograph this.
You need to choreograph the movement between the player and the cameraman.
And you need to know exactly what people are doing.
And you need to practice.
because when you're doing this stuff live, it's literally like you're acting it out.
It's not like you're just playing the gameplay. You can do it 50 times.
It's like when you're doing it 50 times with an actor, the time this is taking is exponentially longer.
So we need to make sure we had a shot list of all the things we needed to get and like practice this all and just go through it over and over again.
So in the end what we...
We decided to do a shoot with a Sony a7S II on a Movi Steadicam.
This is just the gear that we had laying around.
There's other cameras, there's other solutions that you can use.
This is just what we used.
And this is the final test that we shot in the basement where I was confident that this was going to work.
So this is me playing the game.
My friend Ray is working the camera with the stabilizer.
And this feels really meaty.
It feels like we got a nice wide field of view so we can capture the entire contraption.
The camera movement feels stable.
He's kind of working with me to figure out where we're going to shoot this.
And it felt really, really good.
So once we did this, I was like, OK, we can shoot this on the green screen and this is going to work great.
So then it actually came time to do it in production.
And I don't know if anybody here has been on a film set or anything, but there's dozens of people involved in making these things happen.
There's a lot of people waiting around.
And so when you're on the green screen, it's not the time to experiment.
It's the time to execute what you plan to do.
So we had this whole shot list.
And I just want to show you here, this is what a whole rough take looks like.
So this is the very first thing we shot on the green screen.
Camera is rolling.
Camera's rolling.
Gameplay is rolling.
Slate one.
Good.
Controller calibration.
You can't see the thing, right?
OK.
Controller is off.
And action.
So we had to go through that entire process you just saw there for every single take.
And we did 90 takes of various things.
And this was just kind of the first one just to kind of get in the flow of things.
But you can kind of see the compositing is really rough in this because it's just a temp comp.
But you can see the light stands get in the back, his hands are kind of going over that.
So that needs to be rotoscoped out in post-production.
And you know, we're just, you go through it and you just sort of go through the shots and you put together what works in the end to make an interesting video.
So, just to give a really high-level overview of what this actually looks like, Colin's going to go into this in more depth.
But what we did in order to capture this is we split the computer screen up into quads, and so we have four pieces of footage to work with.
We have a foreground view in the upper right, background the upper left, and then we basically sandwiched the green screen footage in between those two views.
And we also had a smooth head-mount display view in the lower right-hand corner just in case we needed to cut to first-person footage because, let's just say, the whole thing didn't work.
You know, or whatever. Just as a backup, we had that in case we needed it.
And we did use it in one or two little shots here and there.
So just so you can see this all kind of slapped together, this is just like a, you know, shows a compositing pass of all this stuff.
So here's the green screen footage we shot with the camera.
And then the, this is the foreground layer, which we then kind of just like keyed out to blue.
So this is everything in front of the head mount display.
Then we bring in everything behind the head mount display.
And Colin's kind of sandwiched in between there.
And then we do sort of like a compositing pass.
And we do color correction, light wrap, motion blur, and things like that to kind of smooth everything together.
And that's one of the advantages of doing this in post, is you're able to do things like that to make it actually look a lot better than you would if it was being composited live.
So that's kind of like the rough scope of it.
And then it's just a matter of doing this for, you know, days and after effects and like putting it all together.
And the actual process of like making an interesting trailer is its own talk on its own.
But, you know, we just wanted to make sure we had a variety of shots that showed off the game in as many possible ways as we could.
And then just put that all together in a really, really nice package.
So, yeah, that's basically how you do it in post.
And now Colin's going to kind of talk about the nuts and bolts of this and how you do it live.
Sorry, all right.
So like Kurt says, as he introduced me, I'm Colin Northway.
My wife and I are Northway Games.
And along with Radial Games and Ashelm Peradio, we made Fantastic Entraption.
This is actually the first ever live stream mixed reality video ever made, which I think is pretty cool.
So I'm going to talk more about how to integrate this stuff into your game.
I'm also going to talk a little more from a streamer's perspective, because we did a fair amount of streaming.
And Kurt covered the post, the trailer post up pretty well.
Alright, so mixed reality is a pretty simple idea, right?
You really have three planes.
You've got the plane behind the player, you've got the player separated from their reality via green screen, and you've got the plane in front of the player.
So you just need your game to output those three planes, and then you need to stick them together.
A quick note about the rear plane.
You notice in Kurt's footage, the rear plane actually held everything that was behind the player, but now we just, in the rear plane, put the entire game in there.
There's some smaller reasons to do that, like usually the ground is in the rear plane, so if you start popping objects out of that, their shadows disappear, which obviously doesn't look very good.
So there's a couple different ways to do those splitting up of screens.
There's kind of two broad approaches.
One of those is to do the compositing outside of the game, and one is to do the compositing inside of the game.
So we're going to talk about compositing outside of the game first, and there's kind of two broad ways to approach that.
There's compositing, there's moving objects between layers manually, and then there's using the camera clipping plane.
So I call this like object blipping because objects blip back and forth between the front camera and the rear camera.
So this is really flexible.
It's good for trailers.
It looks good.
But this takes some integration to get into your game.
So I'm going to talk from a Unity point of view, but all of this applies to Custom Engine or also if you're using Unreal.
Unreal is a little harder to do mixed reality because it's harder to have multiple cameras.
But in Unity, all you need to do here is have some objects on a foreground camera layer and your other objects on a background camera layer.
So the only reason that that might be tricky is because the Unity layer system is really overloaded and they use those same layers for collision detection.
So if you've already made a game and you've got thousands of objects and tons of prefabs already, and you have renderers and collision on the same game objects, then it's going to be really hard to go through and separate those two things so you can change which layer things are on.
Because if you have an existing game and you just start changing willy-nilly what layer things are on, it's going to totally screw up your collision system.
So this can take some integration work to get in and might not be easy to do for everybody, but I think it's a really good way to do things if you can fit it into your game.
A technique that is super easy to get into your game uses the clipping plane.
So this is like conceptually by far the easiest way to do mixed reality.
It's really simple to get into your game.
You literally have background camera, it's fine, normal camera just like the previous one.
The foreground camera, you just set the far clipping plane equal to the distance from the camera to the player, right?
And then it just doesn't render anything behind.
A couple of problems with this technique, it doesn't look quite as good as the object splitting between layers technique.
You get that seam between the objects, which is hard to get rid of.
Obviously in the previous technique there is some visual difference between objects moving between layers, but it's much subtler.
And also there's the toes problem, because the game, you can see Sarah's toe disappears.
whenever it's in front of her headset, because the game doesn't know that the ground is a special object.
If you're blipping objects between layers, you can put in some custom logic, make sure the ground is always on the background layer.
So, like I say, the compositing for this step is all done outside of your game.
And so the streamer, if you're, so there's two options to do that.
If you're a streamer, then you'll have to download a program to do that.
Lots of streamers use this software called open broadcasting software.
They're really familiar with it.
It does everything you need.
If you're doing a trailer, then you just like make Kurt do it.
So set up is it's not super easy and there's no way to take it there's no way to do it for the user of us doesn't have the idea of like you can't make like configurations and and send send them to people or anything so you can read a good tutorial but it's going to add a lot of friction to the player that's doing it they have to take your game footage and thence.
Like literally type in numbers to split up which which quarter is for which screen and then they have to blow it all up and they do the green screen key in OBS and then smack it all together and so it's a fair amount of work for the player and there's also a really big problem with this technique of doing the compositing outside of the game is getting to 1080 P footage right so.
You need to get 2 screens out of your game, and the simplest way to do that is just to render them next to each other.
So literally, that would be the output for Fantastic Contraption.
We split the screen into quads, and then I have the foreground and the background separately.
But obviously, if you have a 1080 monitor, then those are half 1080 resolution.
And when you blow it up, it looks a lot worse than it would if it was full 1080.
So a couple solutions for that is the most obvious one is you can go out and buy a 4K screen, and then all of your quads are 1080, but then you're adding, for the potential streamer out there, you're adding like a big monetary investment for them to be able to do this.
Another possibility is you can do like output to multiple screens, but then the setup is going to be even more confusing for the streamer.
So a technique to avoid all of these downsides is to do the compositing inside the game.
So doing the compositing inside the game has a couple good advantages.
Big one is it looks really good.
So it doesn't have, there's no seams or objects aren't moving around, they're always rendered in place with the proper background where they're supposed to be in the game.
And then also a big advantage of this is for streamers, it's very low friction.
They just like start your game up, they plug in their camera, and then maybe they do some configuring of green screens, they set up their camera position, and they're good.
So you'd be like, well, OK, perfect.
We'll just all use this.
We don't have to think about the external compositing.
Except for two problems with that, of course.
One, this only works with cameras that Unity can recognize.
And that is only webcams.
So Unity can't read capture cards, HDMI capture cards.
And those are how you're going to get a really good camera into your system.
So a lot of streamers do use webcams, but when they're using a webcam, they're really like, sitting about a meter away from it, like, you know, next to their monitor.
And that's what webcams are designed to do, so they're good at it. But when you're doing mixed reality, now you're more like three or four meters away from the webcam, and they're really bad at resolving images that far away.
So, like, one of the many reasons that my footage looks so much worse than Kurt's footage is because this is using a good webcam, but it's just a webcam instead of a really good camera.
Definitely the number one thing streamers can do to improve the look of their stream is just dump money into a camera.
So if you do the in-game system, they can't do that.
There's kind of some tools on the market that are starting to pop up to let you put an HDMI camera into a USB and be readable by Unity.
So that would be a good solution. A better solution would be is if anybody in this room works for Unity, when they get home, they can make Unity read catcher cards, which would be awesome.
A potential problem with this is it has the same toes problem as the clipping plane technique, but since you are doing logic in your game, oh yeah, well I guess I'll talk a little bit about how we actually do it.
So this is one way to do it.
This is the kind of simplest, conceptually really simple.
We literally just take a billboard, we put it in the game.
To where the player is so from the player the word is like surrounding me and then we just draw the webcam on top of that. We the still here, but where you see Sarah with their arms out that would be webcam the moving webcam image.
And then we move that to wherever the player is, we move the billboard.
And then you just resize it to take up the entire of the camera frustum.
And so this has the same toes problem that the clipping plane has, but you can solve it by where your camera frustum overlaps with the ground.
You can just draw yourself a quad there, and then project the camera on that as well.
Which looks really good.
I really like how this looks.
So I'm going to talk a little bit about if you don't want to write all these tools into the game yourself, there are some tools that can help you do it.
And those, they're kind of popping up now.
They're kind of just starting to get, people are kind of just starting to start release tools that are usable.
This is probably built into your game already if you are working in Unity.
This, um, Kurt showed a little bit of footage of the Steam trailer for the Vive that was on Mixed Reality.
And so Valve actually wrote this tool into SteamVR specifically to make that trailer.
So, it's still in SteamVR from like a year ago when they wrote it.
So you can use that. That's what streamers are using kind of today to do stuff.
But it's really limiting. It was written for one very specific usage.
It was written just to do their trailer and it did exactly what they needed it to do and 0% more.
It's actually a particularly bad tool for streaming because when you do streaming output from your game...
You can capture the alpha channel from your window, but the Valve tool nukes that alpha information, so OBS can't read it.
Valve solves that problem by outputting grayscale alpha information in one of the corners, but there is no streaming tool that can make use of that.
So the best you get, and they use the clipping plane technique that we talked about earlier.
Of course, it's easy to make a tool that uses the clipping plane technique that we talked about, so it makes sense.
The big problem here is the alpha stuff.
You can see there, when the clipping plane overcomes objects that are partially transparent, it looks really terrible.
You can also see there's a line going up that has a black edge around it because they just, you literally have to key out black.
So it's a really, it's not a very good tool.
It was never really designed to do this.
So this is like where we're looking to improve upon from now.
There's a company in Vancouver that is working on MixCast VR.
I really like these guys.
We did a lot of streaming out of their studio, actually, before they were working on this tool.
So their priorities and our priorities overlap really closely.
They're out now.
They just released last week.
And they have actually three ways of doing compositing in MixCast.
So this is the in-game technique, like we talked about earlier.
And then there's also, they have, okay, I'm going to talk a little bit about camera delay.
So another downside of the in-game technique is if you watch Sarah, the game is a little bit ahead of her.
You'll see the wheel bounces a little faster than she does.
So whenever you have, however you get a camera from the real world into the game, that real world image is always delayed a little bit and the game is always, you know, is much closer to real time.
So you always get this weird look of the game knowing what the future is, and the person and the player being behind in time.
So if you're compositing externally with OBS, you've got a good solution for that.
Literally take your game, every game frame, just buffer it and add a delay.
So you can solve this delay problem perfectly.
But if you do the in-game technique, then it's not obvious how you solve the delay because the player is inside the game already, so you can't delay it because they're also in it.
So, um, MixCast is an interesting solution to this.
Uh, they can't solve it entirely, so they give you two options.
Uh, they give you this in-game option if you have a low lag camera, or if you are playing a game that doesn't have, like, really fast hand motions, then you can cheat it and get away with it.
Or they also have an in-game clipping plane technique.
So...
In-game, they render the background, they render the clipping ground foreground, they put your player in, in-game, with their own green screen tech, and then they delay the game.
So you still have all the ease of setup, and you still have, but you get that delayed footage.
They also have an output, they also have the split-screen quad output, so if you want to do compositing outside of it, you can do that.
So I think this is a really good solution.
As you can see, I implemented MixCast into Fantastic Entraption.
It literally took me under a minute.
I dragged in two prefabs and he'd go and like, this is the footage coming out of it, which I think is pretty cool.
Something I kind of glossed over earlier is when...
For any of this to work, you need a real-life camera and you need an in-game camera, and they have to be in the same place, in the same position in space, and also the same rotation in space.
So in that Valve tool that they made specifically for their trailer, the way you set that is, there's a configuration file.
You've got like, you know, a camera here.
And like a controller glued to it here.
And then you start it up.
You realize that, oh, the in-game controller thinks it's facing straight up.
And it's also off to the right because it's where it's taped.
And so you go in the configuration file.
And you take random guesses about where that might be.
And then you start the game.
And you see that it's still off.
And you close the game.
And you make the changes.
And then you start the game again.
And then you have to actually restart the game every time you make a change.
Hugely painful so in contraption we have usually 2 people to do it. We've got one person.
Do we think these radio somebody with controllers in the real world and in the game world right because you know your controllers in the real world and in the game world. So you have one person making these like T poses and kind of like.
hinting to the person standing at the computer who is using like arrow keys and WASD to fly this real-life camera into the place and sync it up perfectly.
Mixcast has this really nice solution where they use the same general idea, but they put it all inside VR.
So I put on the headset and then I grab the virtual camera and I put it over close to where the real camera is.
I see the footage of me, and I can see the controllers in-game and in real life, and then they have these nice nudging buttons so I can get it to just where I want it, so I can get it looking perfect.
There's also, you have to set the camera field of view, which adds this weird complication because you're never totally sure whether your camera is too far forward or too far back or if the field of view is too wide or too shallow.
So their solution to that is...
They actually just have a database of field of views of cameras.
So when you plug your camera in, hit a button, it goes up to their database and you're like, okay, well, field of view for that is 45, you don't have to worry about it.
Which actually is, surprises, simplifies things surprisingly enough.
They also have a really nice built-in green screen system where you just like flop to the green screen and then like hover over the green canvas and fade it all out.
So I really like MixCast. I think it's really worth looking at.
One more advantage is this is all done in an external tool launched from Steam rather than in your game.
So if players set it once, they never have to set it again.
So for streamers, that's really low friction, which I think is what you're looking for for streamers.
The last system we're going to look at is the kind of like crazy future tech left field job sim mixed reality, which is like the coolest.
They use a depth camera.
uh... and so in all of these systems the player is actually two-dimensional right so you have one position for where the player is and that's like where the headset is and that's where they are in space so if you try to reach over something your hand is not going to go over it like if i try to reach over the podium my hand would actually clip behind it but with the alchemy system I'll get back there you'll watch me grab the grab the donut off the tray with the alchemy system they use the zed depth camera This guy, they use this Z-depth camera to actually get the depth of where your body is in real life.
So your arm is in front of your body and they know roughly where that is.
And so they can use that to composite you actually three-dimensionally into the game itself.
And so.
Um, it can't, right, so ideally that, you can even do that without a green screen because it's separating you perfectly from the world but depth cameras are not that good.
So you still need a green screen, this is like kind of a hybrid green screen, um, it's kind of a hybrid like green screen depth camera approach.
They're also, since this is similar to the, right, this is in the same family as the in-game rendering techniques.
And so since you're doing it in-game, you have, you can mess with the image in all these different ways.
And so they're like really taking advantage of that.
They do lighting effects.
And if you watch when I teleport out, oh yeah, yeah, these are, I always like these.
And if you watch when I teleport out, you'll see them like, push a blue, kind of some blue sparklies in front of me.
And so you can like mess with the player and what it looks like, since it's in the game.
They're also developing all of these, they're developing tools to make all of this like lighting and player effect stuff work better and integrate better into your game.
I remember there's that problem of thinking of where the cameras in real life and where it is in your game so how can he has this super interesting technique to solving a problem in so they actually 3D print this like little mount that marries the said camera to a 3rd steam controller and then you always if you know that transform is then you're good you don't have to like mess with anything so that's great if you have a third controller All of these systems work with 3 controllers. Kurt was talking about handheld, because handheld footage, especially in trailer world, looks a lot better than, like, fixed, like, footage.
So, all of the techniques I talked about before work super well with 3rd controller, or the Vive trackers that are coming out now.
It's, like, really obvious how to integrate all of that and set that up.
It's really just like a moving mount point instead of a fixed one, and then when you...
When you line up your in real life camera with your in game camera, that's just lining up the offset of the thing taped to it.
So I think that's a really cool solution to the offset problem that I'd like to see more of.
Hopefully that'll become like a standard mount in the future.
Those are all the techniques and the tools that I think are worth mentioning right now.
All of these have such clear upsides and downsides that I thought I'd just make some slides that are like simple pros and cons.
So if you want to take a picture of slides right now, this is like the, this is the rundown.
Like you'll know all the pros and cons simply if you catch these.
I don't want to talk about the webcam thing on that one.
The webcam thing on the last one there.
Yeah.
All right.
And that's all i have to talk about, about mixed reality integration.
Okay. Cool.
So just to kind of sum everything up, we got time for a video?
Yeah.
Oh, yeah, right.
Yeah.
So we got here.
Yeah.
No worries.
Yeah.
It kind of just goes over everything here.
We are working on the Job Simulator and Fantastic Contraption trailers right now.
They're the two of the three launch games that are coming out with the Vive.
So when they approached me I was really, really excited about it.
I mean, I just really wanted to be on board with this kind of thing.
My background is in visual effects and I used to work on a whole bunch of different Hollywood movies.
After I got out of the visual effects industry, I sort of transitioned into video games and started doing more motion graphics.
But these two trailers are the first ones that really merged those skills in a really, really nice way.
So generally speaking, I'm literally a one-man shop in my basement making game trailers, but for this, I knew we would need a green screen studio, we'd need a camera crew, we'd need people on set, we'd need actors.
And so all of a sudden, your needs are just growing exponentially.
So I called Handcraft and they were like, yes, let's do this.
And so then we started thinking, okay, what cameras are we going to shoot with?
What are we going to use to stabilize the camera?
We needed to figure out which camera would actually fit in the stabilizers, and how do we attach the third Vive controller to that, and will that throw the camera off, and blah blah blah blah blah blah, it just keeps mounting on top, right?
And we did that first test in the basement, and I put it on Twitter and it just exploded.
People were like, whoa, my God, this is crazy!
And so we knew we were onto something.
Today here we're recording VR trailers, which is kind of this new thing that people haven't had to think about before.
And we're doing that with a green screen and doing some mixed reality stuff, where we can kind of superimpose people into the actual game, with the hope of being able to explain to people what VR is like.
Actually showing people inside VR for the first time, that's really, really magical, and I think will kind of allow people to understand without trying it, just a little bit of what this stuff is going to be like.
Job Simulator is a game coming out for all three VR platforms, the Vive, the Oculus with the Touch, and PlayStation VR.
It's a game where robots have taken over all jobs.
It's the year 2050, and so they created the simulator to simulate what jobs used to be like, so these future humans could experience what it was like to job.
Fantastic Contraption is a video game for VR, where you use your hands and you walk around.
And you make a contraption, so you make a machine.
You use these simple tools, you use just wheels and sticks, and you basically try to get from over here to over there.
Basically, every single time we would do a take on set, there was like this checklist of things that we would need to go through.
So like the first thing we would do is make sure that the camera is rolling and then make sure that I'm capturing gameplay on the computer.
And we would have to get people to do a calibration of their hands with the controllers visible.
And the reason we needed to do that is we needed some way to align the footage from the gameplay and the camera in post-production.
So the screen would be split up into a foreground layer, a background layer, a composited layer, and then what the player was actually seeing through the head-mount display.
So we're capturing all four of these views live at the exact same time as they were playing.
This is awesome. Kurt's crazy.
As soon as we had our first Skype call with Kurt and he was telling us all his ideas, I was like, that's impossible.
And after we saw the first few of his screen tests, it was pretty mind-blowing.
People aren't going to realize how much work this was.
It's like, oh, they shot it on a green screen, it's all computer.
But no, what we actually did is we shot the gameplay live.
So the gameplay was actually being captured at the exact same time.
We didn't go in and Cinema 4D or Maya and rebuild all this stuff.
So literally it took two days of doing rough composites of every single one of these shots and just layering everything in, lining them all up exactly on the right frame and spitting that all out into this big collection of files.
Then once you have that, you can actually start looking at it and sifting through the footage.
It's really the best way to showcase virtual reality.
It's the best way to show that that person is in that environment and manipulating those objects.
And a lot of people just say you have to try it and really experience it.
And I think that these mixed reality videos are the best way that we can get that feeling across, you know, in a 2D format without having the headset on.
Being involved in the first of these kind of things is really, really fun.
It's fun to work through all the technical problems.
And it's fun to figure out how to shoot in VR and what things you need to do to make that look good.
So it was stressful, but it was really exciting at the same time.
All right, that's all we got.
Thanks, guys.
Thanks.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Oh, if you have a question, I think you have to go up to the mics in order to ask them there.
Hi. Can you guys talk about time and cost of the first two trailers that you guys did?
Well, it depends, right? It's all... The answer everyone will give you, like...
Like, for a... I can't give you exact numbers, but I mean, like, it's not cheap.
You can't do these for, like, a few grand, right?
You've got, like, the overhead of, like, a studio and production.
It's like, you're literally shooting a commercial with all the overhead that's involved because of all the people you need on set.
Because you need like, there's like food, you have to pay, you have to bring people, you have to bring food, you have to have people for lighting, you have to pay someone to work the lights, you have to pay someone to work the camera, you have to have like, I was trying to do like three jobs at once, you have to have someone doing slating, so it's really, it's expensive.
But it depends on what you need to do, like for Fantastic Contraption we did a couple, two days of shooting, one on the green screen set, we did another day in my house.
If your game needs four or five environments, your costs are going up by four or five times because you need to shoot more days and that's more overhead.
So it's really impossible to give a number until you know exactly what you want to do.
But yeah, it's not cheap.
But that being said, if you don't want the quality of a professional trailer, you could do it yourself.
Just do it in OBS and you can get some footage out there for fairly cheap.
You can rent a green screen and just put it up in your basement and get something going that looks all right for a couple grand, just all in.
Yeah.
Also, avatar footage looks, I mean if you're more, if you're like really more budget conscious, then I think filming in-game avatar stuff is, still looks really good and costs like Yeah, exactly. Like the in-game avatar stuff, I really, really like it.
And especially for right now, when VR is at an infancy, kind of, you know, and developers aren't making millions of dollars on their games, they can't throw five figures at a trailer, right?
So if you can do in-game avatar, all your cost is really is dev time in order to make that in-game avatar look good and fit within your world.
And so, yeah, it's definitely an approach that I can see a lot of people going with.
And it still looks great. It's really, really cool.
I love the movies that you do, they're really great.
But just a technical question, why are you using a Z camera when it's a passive optical depth camera rather than something like a Kinect which is an active IR one and seems more suited for the range?
Yeah, so the reason they're using a Zed camera is because the IR camera is screwed with the Vive tracking.
So yeah, it needs to be just purely depth-based rather than IR-based because, yeah, like I'm the alchemy guys.
If you find them walking around or whatever, they could probably answer it in more depth.
But that's the rough answer.
Yeah, it's the IR.
I followed your step-by-step with DSLR, but I was kind of transparent.
I was wondering, I think it's my green screen.
Do you guys recommend any green screen paint brands or anything?
Well, you know, the green screen is really the least of your problems.
It's the lighting.
Spend your money on lights.
You can key any color as long as it's even.
So if you can pay $600 to get the big friggin' light boxes and make them as big and diffuse as possible and get two of them so they're lighting your green screen evenly from each side, and then have a third light that's lighting your player from the front.
And if you do that, you have the one light behind here, you have the player in the middle, and then you have the two lights on your green screen.
That'll create the most even green that you possibly can.
And then that will key out easily, and then it doesn't matter what color it is.
Thank you, sir. Do you have a wattage for the lights?
Bright as possible without blowing out the power in your apartment or wherever you're setting it up.
Perfect, thank you.
G'day, I caught your talk at our Full Indie Summit last year.
It's good to see some new stuff in this one as well.
Cool, thanks.
Quick question about props with hand controllers.
Yeah.
I noticed in a lot of your trailers you keep it basically to just the controller to show.
Do you find any issues with actually putting the props in people's hands when you're trying to film things in mixed reality?
Well, the biggest problem with that is the camera delay problem, right?
If you solve the camera delay problem, then you're like 90% of the way there.
For Fantastic Attraction, it's actually really nice when people spend a lot of time grabbing these big wheels.
And that actually hides the delay a lot, because you can't really see where their hand is going.
So that works really well.
I think the biggest problems with having like literally if you try to put like a gun or an apple in someone's hand is that they're 2 dimensional right there hand is like going to be on top of the Apple underneath the Apple or the opposite of that and so it's going to be hard to make that look really good.
Yeah, like for Space Pirate Trainer, there's a number of reasons for, like, God, I could talk about the virtual cinematography stuff, that would be another talk, but you know, one of the reasons we went with in-game avatar over mixed reality for that is we wanted the player to look badass in that player, in that thing, and like, honestly, like, I can't find an actor that's going to look as cool as that avatar was.
But one of the other problems is, yeah, you've got this gun.
So in the game, the gun goes all around your hand.
But there's no easy way to composite live action footage where it's actually going in properly without doing a ton of rotoscoping in post to make that look right.
So we just solved that problem by just doing an avatar.
And I think if you're having problems like that, where you're holding giant objects, and your hand looks like it's swimming inside of it because you can't get that layering proper.
Yeah, that's an issue, right?
And some people will notice that and take issue with it more than others.
For me that worked in VFX, I see that kind of stuff and it makes me want to puke.
So it's just like, I need to fix that.
And so you have to sort of look at what's best for your game.
And maybe a different option would be best to fix the hand problem.
If you were using one gun, you could probably just actually make one.
You could make a prop, but then it's got to line up really, really close.
And you've got to get a really dead-on calibration.
And that's really tough, too, depending on the amount of time you have to throw at this.
So, yeah, there's no real easy solution to that problem right now.
Maybe you could put green gloves on or something and make your hands invisible.
I don't know. Or make the whole Vive controller green.
That might work, too.
Someone could experiment with that. Maybe. I don't know.
Thank you very much.
Yeah, no problem.
Hey guys, I was wondering when you're rendering for the trailers themselves and you're outputting all four images, are you doing it to a video capture card? Or is it just so yeah, like when we did the trailer, we didn't use a capture card.
Collins game is light enough that we were basically able to render the two views for the head mount display and the four quads.
I was rendering at 2560x1440 which gave me four 720p quads.
And his game is light enough that we were able to do that.
The head mount display was getting 90 frames a second.
I was getting 60 frames a second on the computer.
And I just used Bandicam to record the in-game footage just on the computer.
And it has like a video card GPU encoder on it.
And so you can hook up to it and it just worked.
It worked well.
But that being said, if your game is like raw data or something insane that's already struggling to hit 90 frames a second inside of the Vive, you're going to have problems because now you're rendering everything else.
You're rendering it again four times.
And if you're doing smooth head-mount display, now you're rendering another camera for the head-mount display.
So yeah, there's a lot of overhead involved there.
The least of your problems is going to be the overhead of capturing the footage.
Your main problem is going to be maintaining frame rate.
For the footage in general, right?
So a capture card, yeah, definitely can offload a little bit of that load.
But honestly, that's the least of most people's problems.
Just maintaining frame rate is the biggest issue.
So from a streamer's point of view, i think it's a little worse even.
It's a real downside of the quarter screen technique.
As Kurt says, you have to render your game at like 4K with multiple cameras.
And you do the capturing and the streaming all on the same machine.
And so the problem with using a capture card and offloading that to another machine, which a lot of streamers already do, is you lose the alpha information out of the game.
So OBS is, if you use game mode in OBS to capture a game, then it'll get the alpha information out of Unity.
And so that'll be your partial transparencies will be perfect.
But if you push it through a capture card, then you lose that, and you'll have to do a key on it, which doesn't look as good.
Thank you guys.
Thanks guys, great talk.
Do you have any strategies for non-Unity game engines?
No.
No.
Yeah.
Well, I mean, there are people that are doing it in Unreal.
I think they're modifying the source code to Unreal to actually make this work.
I remember I was talking to another developer and he was like, yeah, they said they were going to put mixed reality in Unity like a year ago and we're, you know, still not there.
So I don't know.
I think eventually they will do it because it's like the obvious thing to do.
I don't know why it's not on Unreal yet.
I mean, like they've got billions of dollars and tons of programmers.
It should take them a weekend or whatever.
Right? So maybe there's some fundamental flaw there in Unreal that's making it harder to do, but I don't know. There are games that have done it, though.
The Everest guys have a mixed reality trailer that was done in it, so it's possible there's just going to be more overhead involved.
Yeah, it seems like they're all custom solutions right now.
Yeah.
Hey guys, fantastic talk. This was a really, really great session.
I just had a question about when you're directing the actors, when they're wearing the helmet, what's your process for that?
Do you show them the gameplay on a computer first?
Yeah, so for the Contraption trailer, I mean, the vibe wasn't even out yet, right?
None of the VR headsets were out when we shot this thing.
And so none of them had even tried VR yet, right?
And so I purposely got friends to help that I knew would be enthusiastic about this, and that I knew would be comfortable in front of a camera, too, and that would also take direction.
And those are three really key things, and honestly, that's really hard to find in people.
People freeze up in front of cameras.
These were just friends that I had.
And, you know, like a lot of people just aren't comfortable taking direction.
They're not comfortable feeling comfortable in front of a camera when there's people watching, even when they got a thing on, right?
So yeah, we rehearsed.
Like I brought them over to my house, you know, we spent an afternoon.
Each of them played the game for like an hour or two, you know, to get comfortable in there, to understand what we were doing.
I showed them the shot list.
Here's the things that you will be doing when we go on set.
We rehearsed them, we showed them, and just because you need to be comfortable doing that, We were like, okay, do this thing, do this thing live.
It's like, no, that's not working, do it like this.
That creates pressure on the person when they're on set.
You need to make them feel comfortable.
And this is like a directing thing.
It's like like it's filmmaking basically at this point.
So there's a lot to think about.
And dealing with actors is a very, very big headache, which I shouldn't say headache.
If you find the right people, which we did fortunately, it Worked out very, very well.
But if you don't have the right people, it can turn into a big Which is sort of another sort of point in the direction towards in-game avatars.
Because then it's just you and one person not with the pressures of being on a green screen set and having a dozen other people around waiting for you.
And you're screwing up and yada yada yada, right?
So, yeah, that's a can of worms. Thanks.
I actually think performance stuff and streaming is a super interesting question.
And there's like a galaxy of tools that are eventually, and like techniques that are going to be built for that.
So like in Contraption...
Like one of the basic tools is being able to read your Twitch stream.
Like if you're a Twitch streamer you have to be able to interact with your audience there.
And then we also have multiple cameras that you can switch to and direct your own experience.
And I think that's all going to end up being really important.
It kind of shows how much work was just done, like the very toddler beginning of all this.
Now we have to figure out how to do it basically, but then the real questions and the real tools are going to be how do you get expressive performance and...
Yeah. And one of the interesting things, like Alchemy just had a new blog post about the latest and greatest with their Alchemy VR stuff, and one of the things they've added is actually like a live in-game composite...
Of you in VR that you can see while you are in the head-mount display.
So what it is, it's like a screen, floating screen, that is like the final composite of what the streamer is seeing, but you can see it in VR. So you can see how you're fitting into that in-game world.
And that's really key too, because like you're blind when you're in there, right?
You don't know what the viewer is seeing, but now they've got that, so they're kind of piping the output back into Unity somehow with not a lot of lag out.
It's like magic, right?
Yeah, and Mixcast has the same tool.
I think that's really valuable.
Yeah, that's super valuable because you need to see how you look to your audience, especially if you're a streamer.
Do you know how many people are using the Mixed Reality for streaming?
Are most people just using straight eye output with your game, or are a lot of people doing Mixed Reality?
So there are very few mixed reality streamers right now.
There's not that many VR streamers.
I don't understand the streaming ecosystem as well as many people do.
But I think a lot of that is because there aren't that many VR headsets out yet.
And people that watch games are people that usually play games, rather than watching games that they've never seen before.
Right now people are doing interesting stuff.
Oz at Upload VR is making really interesting videos.
They do a lot of Tilt Brush stuff.
And that's almost entirely done with the Steam tool.
And so they end up doing a lot more post videos than actually streaming, because the Steam tool is much better as a post tool than it is as a live tool.
The one thing I'll say, though, it's like I know when Contraption came out, like, and it launched, like, all I saw were people streaming fantastic Contraption videos because it was this new thing.
Right? They were able to get on green screen and they were able to do this, and it's something new that your audience, like, as a streamer, can see that no other streamer is really doing, right?
So it was this really neat way for them to bring in viewers, and they're like, some of the videos had like, like, bazillions of views because they were the first ones to really get it up and working.
And I remember getting emails from, like, streamers like, And then you do the calibration, and I'm having trouble, blah, blah, blah.
And people are all trying to figure it out at the same time, right?
So that's kind of died down a little bit.
Now the tools are starting to gel, and the techniques are starting to get a little bit easier.
But yeah, initially, it seemed like there was a lot of interest for streamers jumping on board.
Yeah, and we did our like second stream.
We did our first stream and someone at Twitch was like, what the fuck?
And then they were like, what are you doing this again?
And we're like, I don't know, next Tuesday.
And so they like put us on the front page and we had like 3,000 people in the stream.
And it was very surprising to us.
But that tapered off pretty quickly.
And I think the reason is we're just not good streamers.
Like I think for a stream of VR to be really compelling, you need...
Not a webcam, you need like a real camera, and you need, yeah, good lighting and a good key.
And then you just need to, you know, be the, like, YouTuber, streamer person who's like really performative.
Yeah, and that's like an extra layer, right?
It's like...
That's their job.
That's their job.
Like, their job...
When you're a YouTube streamer, or like a Twitch streamer, you're a performer.
Yeah.
But now you're like a full-body performer.
It's not just what's coming out of your mouth.
It's your whole body and your acting, and that's just another level of...
So it's really the entire point is to give the tools, after we get past the basics, is to give the tools for people to give good performance.
Yeah.
Hey guys, great talk.
Just a question.
This might be one of those areas where you go just go with the avatar.
But have you guys ever looked at trying to put in-game lighting onto the mixed reality avatar?
Is that too hard?
Well, that's what Alchemy's tool does.
So they have magic, basically, that takes that z-depth information that they were getting from the zed camera.
And turns that into normal information or something like that.
And then basically, they're able to do in-game interactive lighting that way.
So if you check out their latest post that they had a day or two ago, the first post they did was like, yeah, it looks all right.
But the new one is like, holy crap.
The lighting is really dead on.
So I don't know what they fixed or what they improved.
But they've got this raver set up where they've got two balls like this, and they're going like this, and they've got flames, and they've got stuff.
In-game lighting is amazing.
And that's one of the big advantages, actually, of doing it in-game.
Doing that in post is a major pain in the ass.
And what they did actually for the Tilt Brush trailer is they had a guy with like an LED ball on like a boom that was like, you know, trying to like go like this and matching it and they just rotoscoped it out in post to get the in-game like lighting matching the live action person.
And you can do that in a post with After Effects and things like that too, but it's never going to look as good as the lighting in game.
But the problem is, is with the Z camera, you're using these crappy little webcams essentially as like your image, as opposed to like a full frame sensor on a DSLR.
So you're not getting as good of an image, but you're getting that really good interactive lighting, right?
So there's like all these trade-offs with each of these methods, and there's no one clear winner depending on...
It all depends on what you want to do.
Yeah, and that Tilt Brush video was done with a crazy technique, right?
Yeah.
Where they actually tracked in space where the camera was.
Yeah, the Tilt Brush, I could talk for like half an hour on how they did the Tilt Brush trailer, but it's not like what we did.
They did like, it was more of a post-visual effects approach.
What they, like, the short version, they tracked the camera and then brought the camera information into Unity and then like replayed out of Unity and it's like, it's this insane setup, so yeah.
I mean, I think that's a really good point is like, there's still tons of room for creative problem solving.
Yeah.
Like, these are the techniques that people are kind of using right now.
But I mean, I'm sure people in this room are like, well, I don't know why they didn't just do it like this and like that.
And that's probably like, maybe that's the future of how we're all going to do it.
Maybe that's the thing.
It's still pretty early days.
I hope someone makes it better and easier, because that would just make it better for everyone.
Yeah, definitely.
Definitely.
Yeah.
OK.
I think that's it.
Thanks, guys.
Thanks.
Thanks.
