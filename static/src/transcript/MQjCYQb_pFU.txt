As the slide states, I am Brian Vinisky, senior technical animator at Avalanche Studios.
I'm located in the New York City office right in the middle of Manhattan.
Our other two locations are actually both in Sweden, there's our OG studio in Stockholm, and then our newest studio, which officially opened last year, is in Malmo.
Our most recent games that you may know about are the latest installments in the Just Cause series, with Just Cause 4 having been released this past December, Mad Max, Rage 2, which comes out on May 14th, and notable self-published games, such as The Hunter Called Wild, as well as Generation Zero, which actually comes out this coming Tuesday, March 26th.
This is actually my third time speaking now, so hopefully I've settled in and I can give you a good experience with this presentation.
Today, I'll actually be talking about the content workflows that we've employed for both the gameplay and cinematic teams that began from the ground up during Just Cause 3 and evolved over the production of Just Cause 4.
I actually wanted to call this talk Maximizing Animation and Cinematic Content Workflows for a AAA Project that utilized two different DCC software packages with a team size smaller than a typical AAA team, but the full title wouldn't quite fit on the schedule board, so I had to make do.
In all seriousness, the reason why I wanted to talk about the work that we've been doing over the last few years is because I truly believe that what we've been able to do with the scope of our projects, as well as the team size we've had, is quite remarkable.
It's not only a testament to the tools and processes that have been employed, but by the fantastic teams that I've had the pleasure of working with.
I'm gonna set the expectations going into this to better help you understand what I'm going to talk about.
Consumers will look at your product.
They're going to see shiny things.
They're going to rate your graphics, how you compare to other games, so on and so forth.
At the core of it all, though, everyone here is producing content in very similar ways, using workflows and methods underneath that shiny finish.
A large focus of mine over the years has involved a lot of process.
And with that said, I'm not going to give you a groundbreaking new piece of tech during this talk, as it is, in fact, about using and manipulating things that do already exist.
While I love sharing my thoughts and process, I'm not saying this is the definitive way to do anything, but it did work well for us.
Now, while a good portion of this talk is focused on cutscene work, it's not a narrative talk, so I won't be going into detail about that side of cinematics, although certain parts of our pipeline were created to allow for the extra work that came into narrative changes.
It's also not a talk about the overall look of anything or the in-game tech behind it.
And it's not a talk that discusses creating high-quality, high-quality content, or how to hit a certain level of quality.
It is about managing large quantities of content and making the lives of the animators a little easier through decisions and work done to improve the process of content creation during the production of Just Cause 4.
And I wanted to make sure that no matter the position that you are in, whether it be a small studio, large studio, or maybe just yourself, that you can take back a few things with you and without the need of a programmer or some special tech, do anything that I presented to you as you don't always have the luxury of programming support or an army of tech artists or tech animators by your side.
I also wanted to make sure that this talk was accessible for any level of knowledge.
Because of this, I'll probably go over things that some of you may already know or have done yourselves in order to better help those who may not know about those particular topics.
This is even more prominent when I get into motion builder specifics because of the greater lack of support that you find out there compared to, say, Maya.
Slight heads up, though.
This talk is about to be a bit of a hodgepodge because I have some odds and ends that I do want to share among some of the bigger tasks, so hopefully you don't mind a bit of random tips or tricks mixed in with pipeline and workflow.
In an effort to keep this talk as accessible as possible, I want to make sure I define some terms because I found that in practicing this talk, a few different disciplines across the studio weren't quite sure what they were.
I had animators asking me what JSON was and programmers telling me that I don't need to explain what JSON is.
I'm just going to lay this all out here for all of you.
So DCC refers to Digital Content Creation Application, and for this talk, the DCCs that I will be mentioning are Maya and MotionBuilder.
Mobu is shorthand for motion builder.
JSON stands for JavaScript Object Notation, but simply put, it's a language-independent data format that lets you store data to be accessed in a very straightforward and logical manner.
If you're aware of what a dictionary is in coding, JSON looks and behaves very similarly on the surface.
Headless refers to running Maya in standalone mode, which means that you're using full-on Maya in all of its features, but it runs in the background without actually opening up its visual component.
Baking and plotting, as animators and tech animators should already know, is the process of converting animation data to key frames on every frame.
Since this term is different between applications, I just want to make sure that everyone knows that baking is the term for this in Maya, and plotting is the term for that in MotionBuilder.
TA in this case is short for technical animator.
Mocap is short for motion capture.
And when I say JC3 or JC4, I mean Just Cause 3 or Just Cause 4, respectively.
Now that you know those key terms, here's a brief overview of what's to come over the course of this talk.
I'm going to examine the two main DCC packages that we used, Maya and MotionBuilder.
This includes what we were working with from the JC3 pipeline, and what we did differently for JC4.
Additions made to that pipeline, and the tasks that we tried to automate as much as possible.
And speaking of automation, I think it's worth looking at the batching process that was set up at the studio.
A standalone UI was created that allowed us to quickly set up customized batch scripts for any tool or process we had in Maya.
Finally, I'll end up by going over our cutscene tools and how we set up the content for running the pipeline, as well as get a little in-depth on how the actual transferring of content between Maya and MotionBuilder was achieved.
Now in order to better understand a bit about my job at Avalanche Studios, it's important to define the role of a technical animator at the studio, as every studio within the entire game development world seems to have a bit of a different definition for what a tech animator or even a tech artist should be.
To illustrate this better, I've thrown some buzzwords for you here up on the screen.
And the tech animators at our studio, all at the very least, should know a little bit of everything and have proven the ability to pick up new unknowns and very quickly adapt and learn.
For myself alone, I've dug into everything involved with the animation process to some degree.
I've built a ton of Python-based tools and scripts, supported both the gameplay and cutscene teams, dug into state machines, in-editor content, and have worked with each and every department in some way, from the vehicle team, AI designers, graphics programmers, almost everyone.
Because of this, a TA at our studio needs to be ready to roll with the punches at any moment.
They also need to be able to solve the overall major problem, which is how will we maintain so much content when we are limited in our time and personnel?
Another aspect of being a TA at Avalanche is that each project utilizes DCCs and workflows in a bit different way.
Now this leads me to the question for JC4 specifically, Maya or MotionBuilder.
While we were mainly using Maya for Just Cause 3, we did have some tools created during development that allowed for animators to push an animation from Maya into Mobu and also take an animation from Mobu and bring that back into Maya.
However, this was not extensive enough to have MotionBuilder be anything other than a supplemental tool as opposed to a standalone.
As a Maya guy, it did take me a while to really appreciate what MotionBuilder could do, but now I do realize that the things it does well compared to Maya, it does really, really well.
I've also seen firsthand some passionate debates between folks on which one is the superior product.
You'd actually be amazed at how heated they actually get.
Just kind of an anecdote, I made a Twitter poll recently while prepping for this talk to kind of get a gauge on what people use, and the results were pretty staggering at how much Maya is loved.
But if you actually go back and check this thread, there's some really, really heavy MotionBuilder lovers there, especially if you see Brad Clark.
He's always up on MotionBuilder.
You should check out this thread if you wanna see kind of the debate that goes on.
So, when asked by our MotionBuilder, Why not both?
It did almost seem like a death sentence to say yes to building up a pipeline that could truly support two DCCs, considering at the time there was only one tech animator, myself, and six animators to support in two pretty different pieces of software.
But our team was pretty split up on preference and expertise, so I decided it was more beneficial to allow them to all work the way that they wanted to and were comfortable with, considering the lack of time that we actually had to complete the project.
especially since we were dealing with so many different factors that worked better in one or the other.
Since many other studios out there do, in fact, use both MotionBuilder and Maya, we knew that building up this pipeline wasn't a crazy idea, but the challenge would be to develop something that was robust enough with our lack of people power.
Now, in order to try and explain how overloaded the team actually was in working with a AAA open world title such as Just Cause 4, I do want to start off with a few quick facts.
On the gameplay side of animation, we had six animators to support.
And no, the one and a half tech animators isn't like a King Solomon cut the baby in half thing.
That's just me quantifying that I was the only tech animator for the first few years of the project.
And we brought on TA help a little bit later on.
About a year, year and a half left in the project.
In the end, we had around 7,200 unique animations, and among six animators, that's quite daunting to manage.
It equates to about 1,200 animations per animator for the entire project, very roughly 2.7 animations per working day over the course of roughly two years, which is kind of the amount of actual production time we had.
Now factor that in with the amount of time necessary for things like polish, XNs capturing, throwaway R&D work, and meetings that everyone loves to be part of.
Now, as far as preference for those six animators, we had one person who was 100% MotionBuilder, never touched Maya, one person who was Maya, never touched MotionBuilder, and the other four were sort of a mix in between.
As far as the pipeline goes, we did have to follow this structure, which I know that many studios who utilize both MotionBuilder and Maya actually do follow.
You start with MotionBuilder to handle any sort of mocap data using the tools it has, such as animation stitching, pinning, and the take system for organizing content.
You then move that content over to Maya from animation, polish, and finaling.
And then from Maya, you export to your game.
In our case, since we did have an animator that only used MotionBuilder, he would stay in MotionBuilder until he was ready to export to the game.
And from there, I had a process set up that would allow him to pass his content through Maya when exporting.
And we needed to do this to make sure that all of the correctives and the custom rig features got picked up before they got exported to the game.
Now, keep in mind that we were not using any sort of runtime animation or rigging, so it was extra important to make sure that we stream everything through Maya on export to ensure that quality control.
All right, so let us dig into Maya.
I need to think about what we already had from JC3, what we needed to do for JC4 that would get us to where we needed to go.
We already had many basic tools that you probably expect to have in a AAA game pipeline, but after working with them for an entire project cycle, we were able to pinpoint the pros and cons, figuring out what to improve on.
Matching parent spaces and IK, FK limbs, as well as setting up constraints have been very standard for years, but how could we improve them or even use them a little differently than before?
Did the tools we have overcomplicate things?
In the case of foot planting, animators on JC3 had to manually tag the content when this was occurring.
How could we automate that?
How about all those scripts and random tools that animators find online, install or use, and then they just end up breaking Maya for all the tech animators and it's a joy to fix?
There could have been a better way to handle that and essentially animator-proof things.
There were also attachment tools that would help set up weapons and props for animators, but they ended up being so cumbersome with too many buttons and options that we actually just got rid of them.
Animators tended to have their own workflows and set up attachments on their own ways, and a lot of this was in part to outside tools that they had found or just processes that they were so used to that I didn't really have to worry about that.
We also had a file browser for the animators where they could find and open Maya files from within Maya, as well as multi-select files and folders for batch exporting animation.
Finally, a tool from Maya to MotionBuilder and back to Maya did exist, but it was very limited, and we didn't really have a good system to transfer multiple takes back and forth.
Now just visualizing some of this right now, we had a lot of those processes broken down into one button clicks for JC3, thinking that it would be easy and awesome for hotkeying, but in many places this really wasn't efficient.
This did allow for a few things like space and IKFK matching, to have the ability to be used as hotkeys, but the physical UI was sort of a nightmare and a hassle to navigate.
All of our tools also had their own dedicated UI window, which was a pain to keep track of as well as organize visually.
In addition to these tools, it was sort of the Wild West when animators started to download these third party tools and install them themselves, or create random shelf buttons for Mel and Python scripts.
Going back a slide, we did have animator-organized third-party tools, but they were completely unsupported by tech animation.
Essentially, we gave the authority to one animator who oversaw this process.
He used the network folder to add scripts to, and then called them in a custom user setup file that any animator could grab.
It worked well enough, but it wasn't really something that I could oversee or easily quality check.
So because of this, all of our supported as well as unsupported tools.
came together like that shoebox full of cables and wall chargers that you keep in the closet just in case you might need them someday.
Now, for Just Cause 4, instead of trying to add more, the focus was to see where we could scale back.
Like those constraint tools, what else didn't we actually need?
What needed a restructure?
What tools needed some serious code refactoring?
When it came down to it, we almost started from scratch on many things.
There was also a lot of refactoring and some decisions made to either completely get rid of things that were barely if ever used, as well as to push to automate as many things as possible.
This led to the main goal.
Instead of trying to solve every little issue, we would give more power to the animators.
We realized that we didn't have to try and make every little thing a button or try to solve every single problem.
We also realized that the animators in the end didn't want to work that way.
So simplicity became the goal.
Now, in order to reach our goal of simplicity, we started focusing on efficiency, speed, user preference, and customization.
The first thing we did was tie all of our tools into one main toolbar that can be set and customized as the animators themselves want.
After watching other GDC talks or checking out online resources, I realized that many other studios tie their tools into one specific animation toolbar.
So I wanted to set out to do that first.
but also give it a bit of something extra.
If the animation tools are open on Maya close, they open when you reopen Maya.
They can be docked or undocked.
You can pop out tools if you only need one, but don't want the entire UI in your way.
If you like the UI, but you don't really want to use certain tools, you can completely hide them from the tool set.
You can also rearrange the order of the tools if you use one more than others and prefer it at the top of the stack.
Instead of having animators click a ton of buttons or use convoluted interfaces to set attachments, for JC4 we created a simple one-button attach-detach system that could easily set up specific attachments, as you see here, with a parachute and wingsuit.
Underneath the hood of this process, it detects which rig is active, and then when the attach function is run, just sets up the active rig accordingly.
Further, thanks to the power of space switching, once attached, you can quickly set specific as to how you wanted things like handles or buckles to follow the character.
Now, IKFK and space matching are pretty standard these days.
So what we set out to do, instead of reinventing all of this, was to evaluate and refactor the way that we ran these tools to create quicker times for both running IKFK matching and space switching.
Single frame matching almost happened instantaneously now.
And if you wanted to match for the entire timeline, it really only took a matter of seconds.
Now something that was introduced back on JC3 to our rigs, the further enhanced quicker speeds of space, and further enhanced by the quicker speeds of space matching, was what we called the driver root control.
You can see it here being that giant brownish colored control above the character's head, when it's not selected, that is controlling the entire character.
Now say you want to animate the entire character here dangling, with both arms and legs in IK.
You can easily place the driver root to the position you want to rotate them, set the space for hands, feet, and the cog control to this driver root, animate it, say, tilting left and right, and then space match all of these controls at once back to world space, where you can then polish each one individually.
I kind of like to call this a locator on steroids, because it functions the same way that many animators do like to use locators to bake and transfer animation.
But having it built into the rig and ready to work with the space matching gives you immediate out-of-the-box functionality the second you reference a character into your scene.
We also introduced rotation order matching.
The rigs themselves were set up with best practice rotation orders on all controls, but adding the ability for an animator to change the rotate orders of the controls on the fly gave them even more control.
We also gave the animators the ability to create custom selection sets on the fly that were also simple to make hot keyable.
Multi-select the controls you'd like to group together, click Create Selection Set, and that's it.
Now you've got a custom group of controls that you can grab instantly.
Even more power was granted to the animators to be able to set up their own custom pick walking for anything.
On JC3, we did have pick walking on the character rigs, but that's it.
In order to set those settings, we would have to hard code it into the rig.
This time around, the character rigs did have default pick walk settings, but the animators could customize this any way that they would like, and also add pick walking to whatever else they may choose.
We also added a custom scripts tab to the tool set.
That tangled mess of third-party scripts and snippets were turned into official, unofficial scripts.
The TAs can add third-party tools officially and per force for deployment, and animators can locally add anything that they want to their Maya scripts folder, and then magically populate that tool nice and neatly here.
There's no need to override or create a custom user setup file.
It's all here for you to pretty much use out of the box.
All those random Mel and Python scripts that were now nice and neatly organized were also labeled in a way that anyone could see and understand.
All custom scripts have an auto-create shelf button.
The ability to hide and show what you would want just like our official tools.
And a customized help pop-up that includes a unique command, which through the use of metadata, create custom runtime commands that, when sourced, to allow for the animators to easily apply to hotkeys and marking menus.
Now, I want to show you one specific example of workflow improvements that actually took away the power from the animators, but it was not power that they really wanted.
So here's one of those.
On JC3, any time we wanted to grab footstep data from a file, we needed to have an animator go and manually tag any time a footstep occurred.
They had a tool that easily allowed them to scrub through an animation and click left plant, right plant, left plant, right plant, so on and so forth.
You can see here that the red and blue foot placement markers that are getting set for all the frames where a foot will hit the ground.
While this is a pretty simple and easy process to do, it's really monotonous and it takes up a lot of time.
That's time that takes away from, say, actually animating.
On JC4, I wanted to take this out of their hands and automate that process completely.
All it took was a pretty small script.
Now, don't worry about trying to read this.
I'm gonna break it down for you.
I just wanted to kind of give you an idea of how small that script actually was.
The first thing that we need to do is find the distance between the ankle joints within consecutive frames.
For example, let's say you're looking at a run cycle and the left leg of a character is swinging back down to hit the ground.
On frame seven, you grab the position of the left ankle joint in world space, then you grab the world space position of the left ankle joint on frame eight.
You can then calculate the distance apart that the ankle joint has traveled from frame to frame.
Here's a simple function that can return the difference in distance of two points.
Here, we're just saying that we want to evaluate the left and right foot joints, and then depending on an animation being a walk or a run, there is a predetermined threshold that is slightly different based on the fact that the distance each foot moves, depending on the gait of the character locomotion, is pretty different.
If you look at a run cycle, the ankle joint will have a further distance between frames than a walk cycle.
So what did this threshold actually do?
We know that when the foot is planted, its position doesn't change a whole lot from frame to frame.
This value simply says, do not bother even looking at anything greater than this.
It was sort of a hacky but efficient way to filter out any non-obvious, any obvious non-planted values, as sometimes those can otherwise mistakenly get tagged by the character's leg movement when not planted.
Now here's the bulk of the script that takes these factors into account.
We analyze twice here, one time for each foot.
The analyzation goes through the timeline, finds the current value of the world space position of the foot on each frame, and compares that value with the world space position of the foot from the previous frame.
If a distance value is less than the frame before, as well as less than the frame after that, we tag the frame the foot is planted.
By tagging the smallest value of movement in a particular range of values, we assume that the script has found a planted foot.
For walks, we also added a filter that made sure a foot wasn't too drastically rotated up and down or twisted, since the slower nature of walks meant that the distances of the position of the feet weren't as drastic from frame to frame.
So surveying the pitch and roll of the foot gave us another good indication that a foot was actually planted and flat on the ground.
Another really nice thing that we could do from here is take the footstep data, and depending on if an animation is a walk or a run, determine when you'd hear a cloth sound.
The character almost always hits the point where their legs cross each other after a step.
For runs, this was two frames after a step.
And for walks, it was five.
Thanks to this consistent behavior, we can automatically gather cloth sound triggers.
And not only are we gathering footstep data, but cloth sound data, this made the audio team extra happy, as they do not have to worry about manually tagging all of this in the game.
They were already benefiting from the automatically tagged footsteps, as was VFX as well.
So hey, we just made the animators, the sound team, and the VFX team pretty damn happy.
To the surprise of the animation, VFX, audio teams, and myself, this method had a really good success rate.
I'd say about 85% to 90% of footsteps and cloth were tagged correctly just because of this.
The rest was manually fixed, as sometimes extra footstep tags snuck their way in, or a tag here or there was missed.
But the amount of work to do that small bit of cleanup was a lot less than actually going through every single animation and tagging those footsteps by hand.
Now let's see this in action.
Here's a run.
And by quickly scrubbing, we can see where the foot is planted.
The right foot there is planted on about eight or nine, the left on about 17 or 18, right on 28, so on and so forth.
Now, we run the script, and it cycles through the animation as it evaluates each foot.
When it is done evaluating, we get our auto-generated footstep tags.
Let's actually check and see how accurate this really is.
We got a right footstep on eight or nine, a left footstep on about 18 or 19, a right on 28 or 29, a left on 37, 38, right on 47, 48, and a left on 57.
So it's pretty accurate.
And just for fun, let's evaluate four feet and run this script on a llama, which is the newest of the animal heroes in the Just Cause series.
All right, here's our auto-generated data now.
There's a right rear footstep about six or seven there.
A left rear about eight or nine.
Right front about 11 and left front is around 12.
All right, that's pretty good.
Ooh, quick moving over to Mobu now.
Here's what we had custom on JC3.
Yeah, nothing custom.
While we did have a way to send Maya content to MotionBuilder and then MotionBuilder to Maya, that entire process on the MotionBuilder side of things only involved making sure that you save your MotionBuilder file with the take that you want to transfer over to Maya saved as the active one.
The only Maya-specific tools that we really had were just the Maya tools that grabbed FBX files and transferred animation onto our Maya rigs, or would bake down and export animation content in Maya out of FBX.
Our goal for MotionBuilder was to actually make some tools.
Now, we did actually have some animation tools that my counterparts in Sweden had developed over the years, but there was nothing that existed for project and workflow-specific needs for us on Just Cause 4.
So now we were developing actual MotionBuilder tools and allowing animators to use it as more of a standalone tool.
We wanted to extend the default HIK rigs to allow for more from within MotionBuilder in terms of things such as retaining root motion data, weapon attach data, and any other reference aside from the default HIK controls.
We also needed to drastically improve the process of sending content from MotionBuilder to Maya.
It became necessary to send multiple takes from within a MotionBuilder file over to Maya, which then handled animation content on a one-animation-per-file structure.
In addition to multiple takes, we also had to send full cinematic sequences from MotionBuilder to Maya, just as we had to do the same from Maya to MotionBuilder.
Now I'll detail that a bit more later when I actually talk about cutscenes.
What we had on JC3 was the basic default HIK rig with no additional features.
What we needed for JC4 was to add extensions to the HIK rigs so that we could pull that data, retain it, and modify if necessary.
We actually start off with Maya, we actually start off in Maya with the A pose character, T pose it, and characterize it.
Now this is our base to work on.
From here we can either build a Maya rig or an HIK rig from within Maya.
With the MotionBuilder rig, we actually took that and brought it into MotionBuilder as an FBX.
And that FBX that was generated, that was brought into MotionBuilder has sort of an auto build function that we run in MotionBuilder that creates the rig extensions to things like the root bone there that you see, it's a yellow arrow, and a bunch of key reference joints.
Possibly you can see here, I put a cursor on the right hand's plus sign cross-licking thing, there's a left side one, and there's one on the chest for aim target where animators could animate the...
motion data for where the character was pointed.
So now they actually had controls on these things to animate those in MotionBuilder, and it's easy to actually port that data over to Maya.
Now the team has a custom MotionBuilder rig that they're animating with.
Now that the team has a custom MotionBuilder rig that they're animating with, they need to be able to push that content over to Maya.
On JC3, animators would have to save their MotionBuilder file with the take that they want to transfer to Maya as the active take.
Since animators were utilizing Mobu much more this time around, and especially for the animator on our team that only used MotionBuilder, this really wasn't going to cut it.
Some files could contain as many takes such as this one.
So we needed to make a way to quickly get any and all content out of MotionBuilder.
We developed this UI that allows you to pick and choose whatever animations you want, which then get plotted down and completely stripped down so that the result is a simple skeleton that is very simply able to be quickly brought into Maya and transferred to the Maya rigs.
Now it may not be a ton, but these changes did make MotionBuilder much more accessible as a standalone tool for the team.
Also, I don't have it listed, I don't have it shown here, rather, but I have it listed.
There was a batch tool for the Xsens content to bring into MotionBuilder, but it's literally just a button that kinda asks what directory you wanna import, so for sake of visually not being too pleasing or interesting, I'm just listing it here to tell you that we did have a batch way to bring in Xsens data to our animation, or to MotionBuilder, rather.
Now, this section isn't quite as long as the others because it was at the core, but it was really at the core of everything.
I'm gonna restart that sentence.
This section isn't quite as long as the others, but it really at the core of everything was making everything quicker and more automated for both the tech animators and animators.
This batch processing tool became the central core to running heavy processes into and out of Maya.
Everything was streamed through here.
The way this works is that the user selects a batch process from a list of JSON files that we've made for them.
This could be an animation export or a Mobutu Maya transfer.
When they select a file, any options pertaining to that file show up on the right.
The user can tweak those settings as necessary.
When you load a script, any directories with files that pertain to the script also pop up on the left.
When you select a folder, the files within that directory show up for you to multi-select and run the batch on.
When you run a batch, a tab pops up with the UI that runs a Maya standalone thread and batches your content headless.
You are able to run different processes at multiple times at once without ever actually opening up Maya.
This is optimal for computer performance and speed of data processing.
It's also very optimal for being 100% animator-friendly, as they do not have to worry about anything from command line and could potentially customize these batch options themselves.
Now, quick look under the hood.
We have a very simple JSON dictionary where we state some rules and list out all of the options for the user.
In this case, we're looking at our Maya animation exporter.
We tell this file what file types to look for, which directory to look for them in, which batch process to actually run, the version of Maya standalone to use, and all the options that we wanna list here.
Now using as an example animation exporting, these options let you create things such as toggles for maybe turning on or off the generation of a debug animation file, or a line edit for customizing what you want to call the Perforce change list that all your animations get added to.
We set the type of UI widget, what to label it, the default behavior, and the command that we will use to pass the state of each option through the Python batch file when running the process.
which then magically populates your options here, as seen a minute ago, whenever you hit that select, and whenever you hit that select batch script option, you can choose your specific batch.
You actually see that here I have a few batch scripts that I've set up for myself.
Some of these are actually official scripts that were checked into Perforce.
Some really aren't.
There's like a cut scene AnimExport02 and 03 here that were just kind of local things that I quickly set up for testing.
But essentially you can either do anything for yourself locally or do something that you could check in and allow animators or any other person to use.
So in the end, thanks to this nifty batch tool, not only was I able to set up anything for myself really fast to start churning out lots of content through Headless Maya, it opened up a ton of time-saving possibilities for the animators, too, and not just for export animations.
In fact, even though our pipeline called for content to be passed through Maya before export to the game, our MotionBuilder-only animator used this batch tool in conjunction with the FBX takes batch exporting tool that we had within MotionBuilder, and he never once actually opened up Maya, not once.
We just had the batch script set up for him to select all his FBX exports here, and then the tool would go through each file, throw the animation onto the applicable rig, and export that to game.
And now just with the few months ago, MotionBuilder 19 coming out with a headless mode, I plan on actually upgrading this to adding headless MotionBuilder into the mix.
A world where we're pushing MotionBuilder content to and from Maya, exporting it out to the game, all without the need of having to open up any visual DCC is really a world that I want.
All right, on to cinematics.
We had two full-time team members as far as managing the actual animation and camera content goes.
One of these was myself, and the other was our cinematic artist.
In the end, we had about 56 unique sequences.
This number also doesn't include our single camera, single cut, in-game scenes that were used to quickly set up missions.
Our smallest sequence had about five shots that made it up, with our largest sequence having up to 60 cuts.
While all characters and cutscenes were bipedal humans, we had 12 unique rigs of various heights and sizes to manage and make sure that they were correctly used in shots.
Between all of these characters and sequences, we had about 65 minutes of actual cutscene footage.
Yes, sorry, that was a phrase in the question.
We did have 65 minutes of cutscene footage.
Now actual hands-on production for cut scenes didn't even start until about halfway through the project, and we literally didn't have actual content to work with until about six months before we went gold.
So that's, uh-oh.
Looking at the process, this may not seem too daunting when you look at what we were working with, because it's fairly straightforward on the surface.
In creating JC4, like any of our other AAA projects at Avalanche Studios, we work with outside vendors for all motion capture and content solving when it comes to cinematics.
To start the process, we have a narrative team at the studio, as well as a cinematic artist who worked on the story and dialogue, which then, those ideas were transferred into storyboards and animatics to try and lock down things like pacing and camera work.
Once this was nailed down and our actors signed on, we had a shoot.
The shoot involved both our Avalanche Studios cinematics team and our vendor working together with the actors to best replicate all the factors that our animatics had tried to prove out.
When the shoot was done, the vendor provided us with all the takes that were shot during the shoot, and our team at the studio went through and selected which content that they wanted to use and have the vendor solve for us.
After this, there was some back and forth between us and the vendor to make sure that our character rigs were set up nicely for things like good proportions based on the actor proportions, and then the content was solved on our rigs, polished, and delivered to us as baked-down skeletal animation in the FBX data format.
Each shot was delivered as a separate FBX file with camera, characters, vehicles, weapons, and props for that shot.
The content was then brought straight into Maya, transferred to our Maya rigs, and then exported to the game from there.
So we're done, right?
In a perfect world, yeah.
But making games is hardly a perfect process, right?
Going back to that gameplay pipeline we were already working with, we had this pipeline.
Once we started getting cinematic content from our vendor, we added that content in a similar fashion to what we had already done with the gameplay content.
This transfer is quite easy to do.
We set up a batch script that takes all the delivered baked down skeletal mocap data, and since the skeletons are a perfect one-to-one match with the Maya control rigs, we transfer and bake that animation over quite easily.
Now of course, since we're making games, random emergencies or unknown needs may come from out of nowhere.
Needs that with the luxury of time and a larger team can sometimes be met handedly.
But time and numbers were definitely not on our side, so, yeah, did not really please us to go this way.
But for the sake of brevity, I'm not gonna go into a post-mortem right now pertaining to this issue.
I will say that as a technical animator, my most important role is that of a problem solver.
When working out any issues, the first thing you do is assess the situation and come up with a viable solution based off of that.
So going back to our cinematic content integration process, we needed to be able to do this.
After we got our content into Maya on our game-ready rigs, we needed a way to get our cut scenes back into Motion Builder to allow for dealing with changes, tweaks, and critical fixes or updates.
Now some of you may be asking, why would you actually go backwards?
And I get that it's not ideal.
It's kind of why I paused a little earlier.
But simply put, our cinematic artist who handled the camera work, sequence edits, and any cut scene setup was 100% a MotionBuilder guy. 100%.
Just like that other animator.
This guy did give his best try to learn Maya.
He really did give his best effort, but the amount of time that it takes to make edits using the tools that MotionBuilder has, not only mention the great frame rate that it gives you for playback, Remember, animation caching in Maya just happened a few months ago.
We really couldn't do what we needed to do without being able to take our Maya files with that solved motion capture data from the vendor, put it back in a motion builder, and then send that back out to Maya for final approval and export to the game once any new edits and changes were made.
So in order to best explain how we tackled our cinematics workflow, I want to break down one of our cut scenes that we had put through its paces.
So let's start by watching the full cut scene.
Load up the truck! We're leaving in 60 seconds!
Did you organize this ambush?
Si.
Most of the stuff we recovered is standard black ammunitions.
But I also found some of these.
We're not sure what they do.
We didn't want to mess with them.
Um, there's a bunch more left in the truck.
Good. Fine, Sohento.
These could be useful.
Anyone have a screwdriver?
Are you adding that to your grappler? No jodas.
You have to make do with whatever you find.
What are you going to do with that?
Que chimba!
Black Hand are coming, Sargento!
Mierda! Listo, I'll drive the truck.
I've got a chopper nearby waiting to extract this.
Make sure you get there in one piece.
Gracias, parcero.
All right, so that cut scene shows our main hero, Rico, meeting with one of his supporting pals, Sargento, who gives him the part that he adds to his grappling device that allows the player to use the airlifter in-game to pretty much turn anything into a giant flying mass.
This particular cut scene has a good mix of characters, vehicles, weapons, and props, so we're managing a lot of different types of content.
To start, let's look at the process that we need to take in order to get our Maya content into MotionBuilder.
We have our Maya files shot by shot, but MotionBuilder doesn't really work this way.
We needed to take these individual shots and transfer them into MotionBuilder in one long sequence so that they can be set up properly for editing in story mode.
So I love JSON.
I use it all the time to store data settings for anything and everything I can.
It's everywhere in our pipelines.
It's probably in yours too, even if you don't know it.
It is the base at which our Cinematics content workflow is driven.
Using the power of JSON files to organize all of our cutscenes was the solution that not only bridged the gap between Maya and MotionBuilder, but it organized our content in a way that it's really easy to parse and locate with a simple call.
Every cutscene had a master JSON file for the cutscene data, which included the scene name, the campaign that it was part of, all characters, props, shots.
vehicles, and weapons that appear in the entire cutscene at any point.
Then we had a JSON file for each separate shot that told us the specifics for that particular camera cut.
We again have the campaign and scene names, the shot number, the characters that are actually in that cut, the props in that cut, vehicles in that cut, and weapons.
We also have the start and end frame numbers stored.
Now just to better illustrate what this means for us, let's just take a look at a few select shots within this cut scene.
In the first shot, we have just Rico in this dirt bike.
Since there's nothing else in the scene that isn't already part of the environment, only these two items are stored in the JSON file for this particular shot.
The second shot is only Rico, so he's the only thing that we store.
The third shot sets us back to a wide angle that reintroduces the dirt bike and brings Sargento into the scene with those two trucks.
I'm gonna skip over the fourth shot to the fifth one since this shot includes a little bit of everything.
All four characters are seen as our three vehicles.
You can actually see the dirt bike a little bit behind that tree.
That also includes a prop, which is the crate in the back of the truck.
And if you can't tell, the rebel there is holding a weapon in the middle.
Now in an effort to not completely force you to look at code, here's what this looks like in a more visually pleasing way.
Because we have all this finer detail stored the way that we do, I don't actually have to open up Maya to find out what is in any given shot, which I'll say is even far easier, quicker, and nicer than trying to find this stuff on a spreadsheet.
I can also run a cutscene export, headless, in our batch tool and filter it down to only, say, Sargento, because maybe we made a small rig update or added a secondary animation pass on his jacket.
This batch process that we set up will parse every JSON file for the scene, and instead of wasting time opening shots where Sargento doesn't exist to check that he's there, it will immediately go straight for only the files that he's in.
For a small team like ours, this time savings, when you think about the number of characters, scenes, and shots that we're working with was a huge, huge win.
Now, this also made it easier to shot by shot deal with our Maya to Motion Builder transfer.
Now, let's start our content passing process on the Maya side.
It's time to take our entire sequence that is cut into multiple files in Maya and stitch that all together into one giant sequence of content so that it is Mobu-friendly for a cinematic artist to use with the story mode and camera switcher.
A short look into this part of script that does it for us.
I promise I'll go quick.
When we run our process, we open a fresh Maya scene.
Let's call this the master file.
We immediately start at negative 15,000 for a frame.
This just ensures that each individual shot is having its content transferred over and it doesn't trip up the shot before it.
Before this actually though, the JSON file for the whole cut scene itself is red and all cameras, characters, et cetera that exist are referenced into the Maya scene.
And now that everything exists in the Maya file that we need, we loop through all the individual JSON file shots, referencing in the corresponding Maya file shot, and adding all the animation data for characters, weapons, et cetera, sort of in chunks.
So if you were to transfer that data over one by one, you would kind of do it with like shot one goes here, shot two goes here, shot three goes here.
And it's kind of stacked one after the other.
Once all the shots are fully transferred into this master Maya file, there we go.
Once all shots are fully transferred into this master Maya file, all keyframes are grabbed and moved up to frame 101, so we're not dealing with crazy negative space on the timeline.
You could easily just make zero your start frame, but I personally like to leave frames zero to 100 open in case any necessary padding might be needed.
Now, as you can see here, though, we have all rigs, cameras, and everything else kind of nice and neatly spliced into one.
Now, there's another script that actually bakes all this animation down, cleans up everything, and only makes sure that we have skin characters and baked down key frames on the bones.
Once this file is exported to FBX, it's ready for some story editing in Motion Builder.
So picture this, the Maya file you just saw was baked down, transferred into MotionBuilder, it's made its way to the cinematic artist, this person's made all the necessary edits to the cameras, character positions, whatever.
This person goes up to you and says, Brian, it's ready.
So now we're there.
I just wanted to skip the whole part of his editing.
Here's that MotionBuilder file.
Since we're ready to send it back to Maya, we need to split that up, we need to split up every single one of these individual clips that exist in Story.
And then we need to export them out into each individual file, into individual Maya files, as per our pipeline, in order for us to finish off the scene and get it in game.
In order to do this, we need to run some commands in Motion Builder that export out data that can be brought into Maya quickly and with little to no load times.
Using the Send to Maya command or opening an FBX straight up into Motion Builder means that you send everything all at once into Maya.
And it's not necessarily the cleanest result, so we don't really want that.
File sizes get really huge, as evidenced by the FBX scenes here on the right.
And these can take forever just to open up in Maya, or MotionBuilder even.
To avoid these long and arduous transfer times, we break down the MotionBuilder scenes for transfer, then fit them back together in Maya.
Not only do we have to break each of these clips down and export them separately, but we need to make sure we do so in a manner that doesn't export out large file sizes.
Since we're getting close to the end here, I'll spare you more long code explanations and just sort of briefly run through this process.
We first create a component list for everything in the scene, then gather all the start and end frame data in a list for each shot.
Then we plot all animation to the skeletons.
And once we've plotted, we grab the root joints of all skeletons using our component list to search from.
Then we iterate through all of that, making sure to grab the entire hierarchy of each individual skeleton as we create clips per character and export out a separate file per character per shot.
Our results look like this.
And include file sizes that range from mere kilobytes for cameras and anywhere from about a half a megabyte to 10 megabytes per character, depending on the complexity of the skeleton and the length of the shot.
Rico would happen to have the largest files with background characters having less, for example.
This is much, much more manageable now than those giant MotionBuilder files that you found.
These files truly contain Jeff skeletons with baked animation data, sort of like what you saw before when we were exporting our FBXs from MotionBuilder into Maya for the gameplay side.
They do look exactly the same as that.
And shot by shot, we can now open up a fresh Maya file, reference in only the rigs for the assets that actually exist in each specific shot.
And it only takes a matter of seconds to reference these small files in, which we then transfer the animation onto the Maya rigs from.
Which then kind of obviously takes a bit more than just mere seconds to do, but it's a lot easier to do.
It's all streamed through the batch tool.
The transfer script for taking those exported pieces from MotionBuilder does all the referencing, baking, and file cleanup in a snap.
We can process multiple shots, scenes, whatever, much more quickly than by using the default built-in transfer setups.
I did mention earlier that Maya was necessary for secondary animation policy, I believe, and here's why.
One awesome content tool that we had in Maya was the motion generation tool.
It could use a number of different solvers from Maya dynamics, such as spring or bounce, and then take in some user settings and process that based off of the root motion of the characters.
We could batch through entire cut scenes in Maya and run an auto secondary animation patch on all characters very quickly.
Now I think our first pass maybe took about a week to add secondary animation to all those cut scenes.
You can see here Rico's straps are not really moving, mirror's here, nope, now it's moving because we just applied the motion generation to it.
All right, the extra special thing about this tool is that we can run it through a PlayBlast script that we had in conjunction with Batch, and we would actually get a directory of MP4 video files for each Maya file shot to see the secondary animation after the fact.
This is super useful for going in and making sure that certain shots might need to be fixed up or certain shots were fine after a run.
To wrap this all up, what we were able to accomplish included better workflows.
more power and customization, a lot more automation and less manual work, a full 360-degree pipeline between Maya and Motion Builder, and all this helped us churn out a ton of content that we wouldn't have otherwise been possible unless we had a small army to help us.
Now, while there are other studios that have similar tools and workflows, I do hope that this talk at the very least gave a new perspective for those of you that are familiar, and for the rest of you, I hope you've enjoyed a bit of the behind-the-scenes and under-the-hood that I have presented.
Before I really conclude, I do want to extend a huge thank you to these three guys here who all helped me in some way.
Brad Clark, he's the co-founder of Ringing Dojo, he's there on the left.
He was always available to help answer any MotionBuilder specific questions that I had, as I was pretty new in setting up more advanced features in MotionBuilder.
He still does, he's really helpful, and you can probably ping him on Twitter, and he would be happy to talk about MotionBuilder with you.
He actually made a quick video demo for me just on how the camera switcher worked out of his own time.
It was like a 15 minute demo, and it was super helpful.
John Molaska, he's in the middle there.
He actually gave a talk this past Monday at the Animation Bootcamp about freelancing for animators, so check it out in the vault if you weren't there.
He came on board actually for a few months when I was cold, alone, and in desperate need of help.
He really made it, he was really an integral part of a lot of the motion builder setup that you saw, especially the exporting take system and kind of working with animators on what workflow worked.
Now finally, Cole there on the right, he was responsible for many Maya optimizations, including a bunch of that custom script stuff that you saw that's really, really cool, and that motion generation tool.
Also, they aren't listed or shown here, but my animation team's awesome.
Despite all their crazy, crazy demands, they truly make me a better TA.
so I really wanna thank them.
And also thank you to the conference associates in the back who have helped out.
If you see the red shirts, thank them.
This really wouldn't be possible without them, all this organization and stuff that none of us really would wanna do.
They do for us and it's super awesome.
And hey, thank you to you so much for coming to my talk today.
Looks like we have about...
six or seven minutes for questions if you wanna ask any questions, or you can just see me outside of the hall, I believe across the way or something.
There's a overflow room somewhere.
But yeah, follow me on Twitter.
I'm at TechAnimator.
My email, brian at techanimator.com.
Yes, I have all those.
So yeah, thank you so much.
I was wondering if you had a headless version of MotionBuilder as well?
We do now.
I mean, we're looking into moving to 2019, but it's literally headless MotionBuilder just started happening like a month ago.
And also, did you have something to generate all your JSON files for your cinematic stuff?
We got JSON files from the vendor at first.
But we actually did have to do quite a bit of just kind of error checking and manual work just to make sure that they kind of were set up properly.
But kind of once you set those up manually the first time, you're kind of good.
And it was also a really nice way to sort of catalog and understand what was in those scenes by hand.
Okay, and last one. What's the reason for having different files for all the shots in the cinematic?
For the JSON?
No, like in your engine you have like different...
Maya files for each shot?
And couldn't you have one big animation for the whole cut scene?
That was more done just for frame rate reasons in Maya.
Because when you have content that's like a five-minute cut scene, you really don't want to work with multiple characters and all that when you have a giant Maya sequence of, say, 8,000 frames.
So we just kind of broke those down into quick little files that you could kind of go in and update one.
The idea between that was that if you're in Maya, you're in the polish phase, and you're not changing your camera framing or anything like that, and you're literally just polishing content.
Okay.
Thanks.
Hi, I also talk, and I just want to ask the name of the extra tools you use in Maya from rigging?
From, sorry, you phrased that question?
The extra tools you use in Maya for rigging?
For rigging?
Yeah. Oh, we build our own.
What?
We build our own.
We have all auto rigging tools that we build in-house.
So everything is all Python scripts that I sort of started from Just Cause 3, just from scratch.
So it's all automated stuff.
We have a bunch of scripts in-house.
And we build Blueprint files using JSON as well, actually, where I just kind of state, this is the name of the control.
This is the size.
This is the color.
Sort of everything offsets all that.
And then we just run that Blueprint.
And it just automatically builds our rigs.
And by any chance, can you say me an equivalent?
in right and...
To build rigs?
Yeah.
Equivalents to build rigs right now, I know a lot of people use something called Mgear, which is pretty robust.
I can't think of anything else off the top of my head right now.
All right, thank you very much.
Yeah, thank you.
Anything else?
All right, that's it, thanks.
