Super happy to be here with the Tech Toolbox 2017.
Who made that?
It's awful.
Uh.
As you can see, things are going well for us here at the Tech Toolbox.
Now we're actually all ready to go.
I don't have a lot of things to say.
But thanks for joining.
If this is your first Tech Toolbox, let me give you a super quick introduction.
Something has been becoming more and more clear to me that to be a tool designer should actually be a hat that you wear if your browser crashes.
It should be a hat that you wear.
You know, even if you're in a small studio or a single developer, you wear many hats.
You're the engineer or the marketer or whatever, but you should also consider...
being a tool designer and dedicating some focus to it.
Because the tools, they often influence the final product, game design, but they're also at the same time invisible because they're made for the developers and not for the gamers or for the players.
And I think they're not talked about often enough, especially the ones that only live for one project but had a huge impact and were tailored specifically to solve problems within that project.
And that's the sort of stuff I hope we'll be talking about today.
I'm going to give it off right away to the first speaker, which is Michael Cook from the University of Falmouth.
Let's give him a big hand.
There we go.
Let's try that one.
Yeah.
Are we good?
Yeah.
Okay.
All right.
Hey everyone, my name is Michael Cook.
I'm a researcher in procedural generation, creativity, and AI at Falmouth University and the Metamakers Institute.
I also organize the procedural generation jam and in my spare time I make games.
So I'm really into procedural generation and I meet loads of people who either want to be or already are and over time lots of those experiences have led to me building this tool called Dynash.
That's enough slides.
So I'm gonna be demoing mostly today.
I really love to walk you through this tool.
which may end up being ill-advised, but I'll try and give you just a little flavor.
But the idea behind Dynesh is that it lets people look at procedural generators from a different angle, and it pulls in some ideas from research and some of my colleagues' research as well.
It's built in Unity, but it is completely open source, so if you don't use Unity, maybe you can pull some of these ideas out and they'll be relevant to you.
I don't want to talk too much about setup, but it's quite easy to connect a generator to Dhanesh.
It just requires one or two lines of code to tell Dhanesh where your generator is, what parameters you're interested in modifying, maybe how to visualize it on the screen.
And when it loads in, you see something like this.
So we're actually using a dungeon slash cave generator that I pulled down from a tutorial.
And the reason I did that is because often, lots of people tell me they load procedural generators that they found in tutorials, and it's often very hard to solve problems in them.
Maybe you've been handed this generator by a programmer and you're an artist who doesn't really want to dig around in code, but this generator has problems.
So maybe it generates dungeons that are okay most of the time, but occasionally there's like an island that isn't connected to the rest of the dungeon, like this one here.
So we want to solve this problem, and we could just sit here all day and change parameters, and Darnesh lets you do that.
The parameters are loaded in on the right-hand side, and just like in Unity itself, you can fiddle with them and generate more content, but that doesn't really tell you anything.
So the next thing you can do is load metrics into Dinesh.
So if you have an idea that you want to capture, like how dense the level is maybe, like what percentage of the tiles are solid in your map, you can write a few lines of code.
So usually these metrics are quite short.
And Dinesh can load them in, and every time you generate content, it'll give you some feedback.
So we've got a couple of metrics here that we're gonna look at.
One's density.
So on the left-hand side, you can see that this map here is 52% dense.
So 52% of the tiles are solid.
and it's .9 connected, which means that 90% of the open tiles are accessible in the largest chunk.
So what we want to do is we want to get to 100% connectedness.
So again, we could sit here all day and fiddle with parameters and things like that, but Dhanesh has much cooler tools, cooler ways of helping you look at these things.
And one of those is expressive range analysis.
Expressive range analysis was first proposed by Gillian Smith and Jim Whitehead.
Gillian's a professor at Northeastern now.
Jim works at UCSC here in California.
And expressive range analysis is a really cool way of looking at a procedural generator's output.
So you take two metrics that you have, two ways of thinking about your content, and Dinesh will go away and sample this generator hundreds of times, and each time it samples it, it'll write down those metrics, and it'll put a dot on a histogram.
And the more dots appear on one particular point, the whiter that dot becomes.
So on this histogram here, on the x-axis, we can see density.
And each dot represents a piece of content, or maybe 10 pieces of content, generated by this generator.
And the y-axis is connectedness.
So points at the top of this, and if we hover over, Dinesh actually shows us an example.
Points at the top of this histogram represent content that was completely connected when it was generated, or very close to completely connected.
And if we hover over points at the bottom, we can see that this one is broken up into loads of different islands.
And so what this histogram kind of shows us is the spread of expressivity and the range of possibilities.
How often does your generator cause these outliers and what shape do they take?
So now with this expressive range analysis, what we could do is we could sit here and change parameters every day and then we could look at the expressive range analysis and see whether we've managed to fix this problem.
Because kind of what we want to do here is push all of these points up.
so the average output is more connected.
But actually we can do better than that.
We can ask Dinesh to help us with it.
Because the thing with procedural generators is, and the thing that people often have trouble with, is that thinking about it in terms of parameters are like inputs to a black box.
So you can change a parameter, but you might not really know what impact it has on the output.
It would be much easier if we could talk about procedural generators in terms of the things that come out the other end.
And that's what metrics let us do.
So Dinesh also has this feature called auto-tuning.
But we can say to Dinesh, what we really want is output that is 100% connected, so we can say a connectedness of 1.0, and maybe 50% dense.
We don't want it to just be a completely empty room.
And then we can ask Dhanesh to start auto-tuning, and it'll go away and do all of that work for us.
It'll, maybe I'll risk it, I think I have enough time.
It'll use a number of algorithms that it has to search the parameter space, and each time it searches, it's looking at how the expressive range is changing.
That progress bar isn't in real time.
It occasionally terminates early if it finds a solution, and it should do in this sense, though this isn't too difficult.
So what this allows you to do is ask Dhanesh for a space of outputs rather than talking about your procedural generator in terms of its inputs.
And it's not just that you just have to trust it when it comes out the other end, you can run expressive range analyses to confirm the output, which we'll do when this ends, if it ends.
I don't have time to look into the other features that DynAsh has right now, but you can do things like look at the complete parameter space by doing a randomized expressive range analysis.
Or you can ask it to search through your code for parameters that you haven't considered as levers for your generator, but they could be.
So fortunately this did, and I have no idea what parameterization it's going to return, so let's see what kind of things it's generating.
Okay, well, they're definitely 100% connected.
This may not be exactly what you were imagining, so you can go back and ask Dinesh, you can implement more constraints, you can turn off parameters to stop it fiddling with certain things, but we can confirm that this did what we asked it to by running another expressive range analysis.
And if you remember what that last one looked like with a long tail of results coming all the way down the Y axis, hopefully, fingers crossed, when this expressive range analysis completes, we should see that all of those points have been shoved up towards the top of the histogram.
And you can see we've got a couple of outliers.
Let's hover over this one to see.
This one has sort of cut off diagonally, but most of the data points are actually so connected that they're right at the ceiling of this histogram.
To give you another idea of the kind of weird outputs that it can find, right now it's hard to tell Dhanesh to give you a number of options, but we're still developing this so there's new features like ways of exploring these through visual examples and automatic optimization and things like that.
So Danesh is still in development, and it's a research tool, which means it looks ugly.
It breaks things sometimes.
It uses meta programming to dive deep inside your game, which means that it might jumble things up and break things.
So there's lots of warnings everywhere to back up before you use it.
But what I really need is people who use procedural generation or people who want to use procedural generation but have never done it before to download Danesh, have a play with it, and let me know what they think.
So it's available online, there's papers and extra information at darnesh.procjam.com or alternatively you can talk to me on Twitter or maybe after this session or you can find me usually hanging out near the Indie Mega booth for the rest of GDC.
Thanks very much and I'll pass it on.
Thank you Michael.
Has it been used in any games that are out?
I'm working on a game called Rogue Process right now, and it actually came from hacked together tools that I'd used for that game, and I'm hoping to pull the finished thing back into Darnesh now to confirm the ideas.
So yeah.
Where's the name come from?
I didn't know if I'd have time, but Darnesh is the Farsi word for knowledge, and I'm learning a bit of Farsi, so yeah.
Next up we have Holden Link from Turbo Button, who's gonna talk to us about cross-platform VR tools whenever you're ready.
Let's give him a big hand.
All right.
Thank you.
Thanks, sir.
Thanks.
Thanks.
See you.
Cool, does that look all right up there?
Sweet.
Hi everybody, I'm Holden, and I'm the VR guy in this session.
So I'm from Turbo Button. We're a tiny studio in LA that just makes VR games.
We've released two so far, Adventure Time, Magic Man's Head Games, and Floor Plan.
We're currently working on our third VR game called Along Together.
So these games have released on four different VR platforms so far.
Gear VR, Oculus Rift, Vive, and Daydream.
Adventure Time was actually a launch title for all of the platforms that it's been released on.
So we somehow ended up doing two games across four newly launched platforms within a year with a team of three people, and also none of us had shipped games as programmers when we started.
So for a while our logo was a bit of a lie.
So how did we do it without going crazy?
I'd argue that we didn't, but I'll show you how we made the games work at least.
So we use Unity, Unity is cool.
And Unity didn't have native support for most of these platforms at launch.
They have varying levels of support now, but each platform also has its own SDK and set of plugins.
And to take full advantage of each platform, you have to use their SDK, and of course, some SDKs are only compatible with certain Unity versions and so on.
So uh before we were working in VR we were using things like end control and rewired to handle gamepad input in some some of our game jam projects and we wanted something kind of like that that could wrap all the different VR headsets and SDKs uh the same way that those those things wrap gamepads. So we made a thing that we call T-BUTT.
We call it TBUT because that's the obvious namespace abbreviation for Turbo Button and there's no other reason It's a wrapper for VR SDKs that supports Oculus Rift, SteamVR, PlayStation VR, Gear VR, Daydream and Cardboard out of a single Unity project so our games use TBUT functions for accessing stuff like the cameras and the controllers and Then TBUT turns it into SDK calls for whatever SDK that we're targeting A lot of this stuff that I'm going to show today is up on github now, and I'll share the link at the end So here's a little interface we made for switching between SDKs.
We're using Unity's built-in scripting defines to handle swapping between them.
And they all get toggled on and off one at a time so that they don't cause any conflicts with each other.
The other big reason for doing this is that we're not actually allowed to ship some competitor's SDKs on certain platforms.
So this sections off the SDKs from getting compiled and builds as well.
So we're making a game right now for Daydream, where you play as a kid's imaginary friend, and you follow them around on their adventures.
A big part of the game is looking around and grabbing things in the environment, but there's actually minimal support right now for testing Daydream games in the Unity editor.
It looks kinda like that.
So you have to hold the Alt key on the keyboard and use the mouse to rotate your view around.
And you can't see anything in any connected headsets.
So there, I used the platform switching tool, and that makes our Daydream game retarget the Oculus SDK instead of the Daydream SDK.
Because if you're developing for Rift or Vive in Unity, you can see your game directly in the headset straight out of the Unity editor.
So it does take a few seconds to recompile, but I didn't have to make any scene changes.
And when I hit play, our camera's now running through the Oculus SDK so we can test our daydream game and headset.
It's a much better workflow for us.
And it's one that we initially started out of necessity because we didn't have access to enough phones.
But we've stuck with it.
Here's what that camera wrapper looks like in the inspector.
So most of the VR platforms have their own camera rig prefab that you just drag into the scene and it's super easy to get started with them.
But when you're trying to swap between different headsets, swapping out those camera prefabs can be a bit of a pain, especially since they have to live in your scene for the most part.
So the camera rig we made is the only one that lives in our scenes and it just pulls in whatever camera rig it should be using for each platform and then applies the same settings, so you don't have to put your settings in one place, onto all of those different camera prefabs. So that's everything from like the standard Unity camera stuff to stuff like tracking volumes and sorting methods. For example, it's like more optimized to have the camera do sorting in a different way on Android compared to PC so.
we can set all of that stuff up to work for all the different platforms here.
We actually do the same thing with input as well.
So in our game code, if we want to check a button presses on a motion controller, for example, we have a button was pressed function that accepts a button type and then a left or right controller. So for example, we can say like main trigger on the right hand and it will return whatever that button means across PlayStation Move, Vive, or Oculus Touch.
New controllers are getting announced for VR platforms all the time.
Samsung actually announced a new one on Sunday.
So this actually makes it way easier for us to support all of those new controllers as they come in too because we can just add them to one place in this shared code base between our games and then it works across all of them.
So we made a little interface for the editor that lets us toggle a bunch of different controller settings for our games, like how many motion controllers they need, or if they work with game pads or touch pads.
We can also set up different hand prefabs and controller models for each different type of controller.
And that's really important to make sure that each controller feels right in your hands in VR, so they all have different scales, different offsets and sizes.
We also set up button mappings for each controller.
And we do this on a per-game basis, because what we found is that we couldn't really get a one-size-fits-all solution.
And when it makes sense to have two buttons be the same thing across Vive and Touch in one project, they might need to be different in another.
So we serialize our button mappings out to JSON, and then bring them in at runtime to create a lookup table for that script that I showed on the last slide.
So setting stuff up this way has been really helpful in our current projects, even just for development purposes.
So this is the Daydream controller.
It essentially has a subset of the functions of the Oculus Touch and Vive controllers.
It has a touchpad, it can track rotation, but it doesn't track position.
So for testing on PC, there's a controller emulator that is provided as part of the SDK.
And it lets you turn any Android phone into a Daydream controller.
But it's not exactly the same thing as the real one.
It doesn't have a clicky button.
You have to double tap on the touchpad to simulate that.
And sometimes it's kind of hard to tell where you're pressing on the phone screen.
So because we wrap all of our input, we can just switch over to the Oculus SDK and use touch controllers, or the SteamVR SDK and use Vive controllers.
And different people on our team are using all sorts of different controllers to test the game in the editor.
It's also really helpful to have this control option for playtesting so that we get a live mirror of what our players are seeing.
So the way this works is that when we're running on PC, we first search for that controller emulator, and then we fall back to Rift Revive controls if we don't find it.
When running on Android, it's not compiling any references to Rift Revive stuff, and it just goes straight to using the Daydream controller.
So we added a layer to our input wrapper to throw out the position data from the Vive and Touch controllers for that.
And we also have offsets that we can specify to make the data that we get out of those controllers match what we would get out of the Daydream controller.
So whenever we're developing for a mobile VR platform, our workflow is actually that we just have one person on the team make builds for the actual target device every couple weeks, and then everybody else is testing in the editor on Rift or Vive.
So we very rarely actually have to make builds and go straight onto the device.
It takes a week or two of work each time we add a new platform to TeeBut, but once we add those platforms, it means that all of our games work across them.
Wrapping code is not a revolutionary concept by any means, and a lot of VR developers are doing this themselves.
We wanted to go ahead and share how we're doing it, so we can get feedback about what we should be doing better, of course, from other developers, but also to help other people who are getting started get a start with it.
So the source for the platform switching tool, the camera rig wrapper, and a standalone version of the daydream wrapper are all up on GitHub there.
There's also a wiki with documentation for setting them up.
I hope you found this useful, but more than anything, I hope you write butt more often in your code.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you so much.
What a great resource to share with everybody else.
And good name too.
more but-named tools for just abbreviation reasons.
Next up, we have Ryan Williams from Spryfox.
You may know Spryfox from games like Alphabear.
Anyone?
Yeah, thought so.
You ready to go? Let's give him a big hand.
Hi, I'm Ryan Williams, and as was just announced, I work at SpryFox.
I want to tell you about a little library we made called Darkenfig.
It is a tool for improving iteration time by hot-loading text files directly into games.
As you know, faster iteration time is always better.
Faster speaking time always better too.
And hot loading assets and behavior is one of the key ways to get a shorter iteration cycle.
So there's lots of tools out there that do this.
This is not unique.
But they're just not really suited for our use case.
And I think that you might find that to be true as well.
So you might find Dark Config to be useful as well.
So Dark Config sits in kind of a sweet spot.
where it's very general purpose.
It helps you iterate quickly with its hot loading.
It's easy to integrate new configuration files with the game.
And you can express virtually anything in those files.
So you can use it for all sorts of things.
And so by doing that, it takes over some of the mental burden of developing, which is super important.
Because when you're working on something, you really need to be focused on your problem, your gameplay, and you don't want to be wrestling with like, oh, these files aren't loading.
That's great.
Let me show you what I mean with the classic live demo of fast iteration.
There's no way this can possibly go wrong.
So we use uniting, and that throws a lot of you.
This is the current title I'm working on.
It's called Steambirds Alliance, and it's a big game.
It's an MMO.
It's got a tremendous amount of content.
We put almost everything in Dark Config in this game.
We have close to 1,000 configuration files for this, which we have manually typed.
So ignore that error message.
That's because I'm offline.
The configuration file I want to show you here is describing the backgrounds here behind this area.
So I'm flying around and I've got this beautiful procedurally generated world behind us.
And on the left here is the configuration file that describes the parameters for that background.
So one of the things that's useful is being able to reuse and pastiche existing assets.
We're an indie, and so making new stuff is expensive, and we're often just trying experiments and then try something new.
So, we have this based on property here.
Like this is the section that describes this dungeon that I'm currently in.
And you can see it's only using a subset of all the fields that I've got available for all these other things.
So, we've got this based on, and that steals all the fields from a different area.
So, I can change this, and I'm gonna try to get like a cryogenics lab type appearance here.
So I can change that, and then boom.
Let me make this a little bigger.
It just changes every single asset on the screen.
And then I can refine that, right?
Like I can change, I remember that the highlands has kind of like a mechanistic background, so highlands.
So cool, we got some stuff down there.
Fog, let's do the, bring the fog up.
Okay, that wasn't enough, let's do more.
That's the right amount of fog.
Let's change the walls.
Oops.
Oh, I didn't know there was sound. Cool.
So, yeah, so we're kind of getting there towards the cryogenics lab.
The last thing I want to do is just turn off the snow.
So okay, cool.
That's different than what we started from.
And just to prove to you that it's not exactly like the Highlands, I can steal the rest of its fields.
And you can see, you know, the lighting makes a big difference, so cool.
So we wouldn't just stop there.
We would probably make new assets, but we wouldn't have to remake everything for that area.
And that's because Dark Config allows us to do this quick reloading and see that we've made something that's distinct enough.
So, okay.
Express anything.
Why is this important?
Well, it's.
really valuable to be able to use this general purpose tool for something that's an experiment.
And if you end up using that feature or functionality, then you can make a more specific purpose tool.
So let me show you some of the expressiveness of DarkConfig in some of these configuration files.
This is the configuration file that defines how the procedural generation layout of the dungeon works.
So this defines where the enemies are spawned, where the walls are.
And we use, as you can see, YAML.
YAML is a very human file format.
It doesn't have a lot of boilerplate that XML and JSON have, so it's a lot more fluid to type.
The other nice thing about it is it supports sort of different layouts for how you do things.
For a list data structure, you can do a dash at the beginning of the line.
You can see here's two list items here.
But that doesn't always work. It's a kind of cumbersome if you're just doing a point So this is another list and you can do that wherever you want, and it just looks the same in the in the code The other thing that we do that's fairly interesting and this is specific to dark config is that you can override the parsing in certain cases and do heterogeneous lists.
So this is a heterogeneous list here.
This is an enum.
This is another enum.
And this is a structure.
And there's just a little hook in the code that looks at that list and parses this out.
And it makes it so that when I'm authoring this file, I don't have to have a separate field for showing.
I can just say, OK, this room spawns at this area.
And it's right there.
You can also see that we have ASCII maps in here.
That's always nice.
It's almost visual, you know?
Just imagine.
So, yeah, it's not going to be the greatest at any particular thing, but it is pretty good at a lot of things.
And the last thing that I think is really important is that you actually make your game configurable.
I don't know if you're like me, where I work with a bunch of other folks, but those people really hate it when they can't change something about the game.
So I really think, I've really focused on making Dark Config easy to integrate into your game, and oftentimes it's just one line.
And I know that this is an important property because I've been that guy who hasn't made my game configurable and these guys have been all like it.
I don't want to learn C sharp to have to edit this game.
I want to be a designer or an artist.
So this is the line that integrates this Dungeons.bytes we were looking at before.
And this does two things at once.
It loads the file the first time.
And any time that file changes, it hot loads that and propagates those changes everywhere in the world, the game world.
One of the nice properties of Dart config is that it updates the objects in place.
So you see it blows this onto this dictionary.
And then I take objects out of that dictionary, and I'll just pass them around.
I pass that to the constructor of some other thing.
This object just goes all over the place.
And everywhere that uses it doesn't have to care that it came from a configuration file.
It just knows that when the configuration file hot loads, the properties in that object will change.
and this is the object, just so you can see it.
It's just some fields, and they'll just be updated when the file gets updated.
So that means that much more things are hot loaded, much more things are editable by other members of the team than if this had been a lot harder to integrate.
So that's a very important value for us.
And I want you to do this.
I want you to iterate fast, express lots of things in your games, and I want you to actually do it.
We're proud to announce that this is Firefox's first open source project.
It's up here on GitHub and you can check it out.
Try not to inundate me with pull requests while I'm at GDC, thank you.
And I want to drop in here that it's not just a Unity thing.
We use Unity but we've made DarkConfig so it supports any C-sharp project.
So you can expect to see it in all sorts of weird, wonderful places.
Thank you very much and happy developing.
Yeah, thank you so much, that's really interesting.
I love also how text is such a good lowest common denominator, you can build more stuff onto.
I think last year, two years ago, we had Omar Kornet talk about, dear Imgui, about writing C++ visual tools.
He's right there, I see him.
Talk about like C++, more visual tools that you can build on top of anything.
So you could go, you know, Unity to text file to like a custom level editor without having to make that level editor into Unity or compatible.
That's really, really cool.
Next up, we've got Shukya, also known as your boy, Sugar, who's here to talk about procedural mess deformation and how they use that in their game design.
Are you ready to go?
Are you on a different one?
Yeah, there we go.
Okay, yeah that's me.
Are you going to introduce yourself?
I will.
Really happy to have Sugar here and have some other South African developers as well.
I got lucky enough to go to Amaze Johannesburg a few years ago and met all these great, great people.
And it's a huge pleasure to be able to have them here on stage with me.
Alright, you ready to go?
Yeah, let's do it.
Let's give them a warm hand.
Thanks, thanks for that introduction.
What's up guys, I'm Shukya Kimani, but you guys can call me Sugar.
And I'll try my best to keep this short and sweet.
I'm the...
I'm the lead programmer down at Yamakop, a small game development studio in Johannesburg, South Africa.
And we've been working on our debut title, Semblance.
And while we started working on this project, I started developing a tool that creates procedural animations on dynamic 2D meshes.
It's a lot of words, okay?
But basically it's just to animate our character and make him feel as juicy as possible.
But like any of us programmers, it was a tool because I can't art, I can't animate, but I can write some pretty good code, right?
And most of us have our prototypes look like little rectangles jumping around on top of other rectangles, and I wanted to do something a bit more interesting.
And Mike Bethel has shown us that a rectangle can be pretty interesting.
So I started investigating what other people were doing with rectangles.
And games like Super Sex Soccer and Tennis use a rectangle and change the width and height of the rectangles.
And they start to feel, they start to come more alive.
And then around the same time, Zach Bell released Ink and he started tweaking the corners of the rectangle.
And that's when I started thinking.
What if I used Bezier curves?
What if we define our rectangle based on Bezier curves?
And for you who don't know what a Bezier curve is, it's pretty much the pen tool in Photoshop.
So you got two points, the anchor points, that are beginning and end of the Bezier curve.
And you have two other control points to change how the curve works.
And I did that.
So we got the rectangle based on Bezier curves.
And at this point, I wasn't too sure I was still working with a rectangle anymore.
Because you could change it into any shape that I really wanted.
And this was fun.
But creating the mesh was the first problem.
And using the Bezier curves, you can go all across the Bezier curves in different steps.
The more steps you use, the higher fidelity you get, the smoother curves you have.
And then from there, the next thing you need to do is create the triangles.
And triangulating every frame becomes very costly.
And so we developed a way of just putting one vertex in the middle of the rectangle and having all the tris go right to the middle.
And that way, we didn't have to re-triangulate every frame.
And the middle vertex is just an average of the four corners.
So it was able to move around, and you were still able to.
change the mesh the way we wanted. But now it's just a mesh. Why not just slap on a sprite?
then at this point you can have more complex shapes and still be able to not have to animate every frame and then you can just deform the mesh in the way you want it.
And this is how we started creating the key frames that you would animate to.
But procedural animation, we just use numeric springing.
And that way you can jump from different key frames into other ones and it would still look nice and springy and interesting.
And that's when we finally ended with this tool that helped either of us go in and change Squish's animations, but just by the key frames, a little tools to swap it, the mirrored, flip it, and then eventually just save it into the bank of animations.
and then internally in code, we could just trigger off those different animations.
And what's great about the key frames and the procedure animations that you could blend in between any of those key frames.
And that was fun.
And we still needed the square to look.
And so we added eyes.
And just make it a little springy.
And became a lot more interesting, a lot more fun to play with.
But at this point, the tech was being built, and it's just a collection of algorithms, right?
You have your Bezier curves, you have a physics simulation on the springs, and we started asking ourselves, but what if we just add it to the platforms as well?
And then that way, you have this interesting mechanic where you can start changing the world and spring it as much as possible.
And that's basically what the tool is.
And if you need to ask me any more questions, just holler at your boy.
Or if you're more into Twitter, just throw it on Twitter as well, man.
Thanks, guys.
Thank you.
Thank you.
Thank you so much.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
That was excellent.
Thanks, Sugar.
I hope a lot of people will hollow at your boy.
And definitely check out Semblance.
I'm really excited to hear more about it.
Next up, we have Chris Martens, who's going to talk about a tool called Scepter.
Are you ready to go?
Yeah, I think so.
Come up.
That's number one, yeah.
Let's give another big hand for Chris Martens.
All right, thank you.
Well, that was awesome.
I'm really excited and honored to be here among all of these people making amazing things.
So I made a thing called Scepter, which I'm calling a Tinker Tool, kind of if you know what a Tinker Toy is.
So it's a tool for tinkering with rule sets.
And what I was kind of inspired by was the idea of rapid prototyping.
I guess I should say I'm an academic.
I'm an assistant professor at North Carolina University, so a lot of what I do is sort of research motivated.
And I'm interested in rapid prototyping games and particularly I really like using index cards and paper prototyping sorts of things, especially when making.
analog prototypes and I really wanted something that could be used similarly in the digital space to basically get right to the heart of rule sets without having to muck around with graphics and input controls and things like that.
So this slide has a little tiny bit of Sceptre code on it that you can look at without understanding for now.
But it's effectively a very tiny programming language for prototyping system design.
And I'm gonna show you three examples to kind of demonstrate the breadth of what I like to do in Scepter.
It can be used for a number of things, but I wanna talk a little bit about using it for codifying resource economies in games, doing a little bit of procedural generation, and prototyping interactive fiction.
This next part is gonna be the most syntax heavy that we'll get, but the, so just to give you a sense, an overall sense for how this works, we use something called linear logic rules, and these are just rules where you have a left-hand side that when the rule is selected, that left-hand side gets replaced by a right-hand side.
So the only syntax-specific deceptor in this rule is.
the Lolly and the Tensor.
And Lolly just separates the left-hand side and the right-hand side, and Tensor connects together the pieces that go on either side of the rule.
So what are those pieces?
Those are, I'm being intentionally vague about what they are because they can literally be anything that describes some kind of segment or slice of the world state that you're trying to describe.
So without further ado, let's look at our first example.
So resource economies.
So what I mean by resource economies.
is the kind of thing that you often find as a central play element in games like Minecraft or really any kind of crafting game where you have recipes that can be put together from resources that you're collecting around the world.
And you can also typically have some kind of feedback loops in these systems where the recipes allow you to create new tools that allow you to uncover and obtain richer resources.
So in Scepter, you can write a system like that with rules that look kind of like this.
So the pieces of worlds that you represent are things that the player has in their inventory, which is like this have predicate.
And they can have, for example, a material M in some quantity, which is a quantity plus two.
So this is a kind of pattern matching, if you're used to that from another programming language.
So what this rule does is it basically says if you have a material M in a quantity, quantity plus two, you can replace that piece of world state with having a shield made out of that material and having M in a quantity that is two less than the quantity you started with.
The things in purple are variables and they can stand for arbitrary things in the world.
So for example, M here, if this were sort of a Minecraft world, could stand for any material like wood or stone.
You can also have a rule like a mining rule, which says if you have a pickaxe made of some material and there's some ore in the world of some other material, M prime, and M can break M prime, and you have a quantity of M prime equal to Q, then you can replace all of those things with having a quantity of M prime that's one more than that.
It should be noted that we don't actually want to get rid of these two pieces of our world state, so we have a little bit of syntactic sugar for keeping those things around.
and basically checking condition without deleting them.
So what can you do with a rule set like this?
So the author can select subsets of these rules that either run autonomously, so the system will pick randomly among all available rules that can fire, or you can run them interactively, the user picks.
So in the Minecraft example, this could be used, for example, to basically run this rule set at random and see what kinds of things in the world can be put together and interacted with to create the kind of feedback loops I mentioned, or you could interact with the system to try and debug it.
So the next example is procedural generation, which is basically, this is the kind of mode where you would always want to be running the rule sets autonomously.
simple procedural generation example.
So I have like a screenshot of Spelunky here to suggest maybe we want to generate some kind of level with starting with like an empty 2D array of tiles and then populating them with some random tiles including walls and ladders and spikes and things like that.
So because Scepter randomly picks between sets of rules that can fire, you can start out by saying everything in the world is empty at coordinates X and Y, and then you can at random replace those things with the fact that there's a wall there or the fact that there are spikes there.
You can also introduce some more interesting constraints, like for example, if you only wanna put a ladder in a location where there's an empty space above, then you can also add that condition.
Again, checking that condition without deleting it.
And you can, for example, if you only want, if you want to place the player in a random position, but you want to make sure you can only do that once, you can add a token to the rule that basically says, is the player free?
Like, has the player not been placed yet?
And then remove that token from the world so that that rule only fires once.
And finally, my third example is prototyping interactive fiction.
So, by interactive fiction, I'm thinking in kind of like, the parser interactive fiction sense, where you're typing commands like go north, take lamp, and you can describe these sorts of user actions and interactions with the world in a similar manner.
Starting with a character in a room, if you, that should have a, the indirection should have a dollar sign next to it.
So if the room is in a particular direction towards room two, then you can move in that direction and be in room two.
The interesting thing about this is that it's also fairly straightforward to start scripting more interesting types of interactions like character conversation.
So if you're in a room and a character is also in that room and you have some knowledge about a topic, then you can start talking about a topic.
And then maybe this rule interacts with another one that takes the talk about predicate and then the character can respond to it.
So these first two points are kind of in summary.
You can take rule sets and you can either run them as generators, run them as interactive prototypes, or kind of combine the two to switch between automated generation and interaction.
Another thing that you can do is run them to analyze causality between events.
So as one example, so in interactive fiction, we often have like these quest or progression structures talk about which pieces you need to advance to certain puzzles, and then what that permits as an outcome that will allow you to advance to new puzzles.
And you can, there's a visualization component to Scepter that allows you to see the causal structure between events that happen in the world.
So with that I will throw up the GitHub link and this is by the way like a crafty command-line tool So be prepared for academic software But you are welcome to ping me on Twitter or talk to me in the wrap-up room and I'd be happy to help you out Thanks Thanks Chris We're already at the last tool, tech toolbox talk.
That's crazy.
So next up we have Ines McKendrick from Hello Games.
If you guys ever heard of this game called No Mansky.
It's good to practice that, sorry.
Cool, I'm just gonna give it over to Ines McKendrick.
Let's give him a big hand.
Hi, so I'm Ines McKendrick and I'm a programmer at Hello Games.
And yeah, we worked on No Man's Sky, which is a sci-fi exploration game.
Today I'm going to talk a bit about the tools that we use for texturing, for procedural texturing, and making the most of our artist textures.
But a lot of the things I'm saying are really relevant across the board when we generate content.
They're just principles.
But there are some useful details, I hope, that you'll pick up as well.
So when it comes to content in No Man's Sky, we have a few aims that kind of guide our approach to creating that.
The first is that we have a really large space.
So we want to create a variety of interesting content within that.
But...
At the same time, we want to create enough content that we couldn't possibly save it all to disk or have it all in memory at runtime.
So we need to be generating it on the fly, which means we need generation approaches that are performant and we can do it at load time or runtime.
And the final guiding thing for us is that we're a really small team.
And I know that might not seem true to some solo indies out there.
That's maybe an unfair thing to say.
But compared to a AAA team with hundreds of artists, we have five.
And there's no way we can output the amount of content that those people can.
So every piece of art that our artists work on needs to count for a load more content in the games than a single texture normally would.
So in terms of how we texture, the basic overview is that our artists make parts of textures, and we just recolor and recombine them in different ways to get a much larger variety of textures.
And how that looks.
is something like this.
So we start with a base layer there, and then we add different layers over the top.
And really quickly, I think you can imagine the kind of variety that that can get.
For example, if I take that stripe texture and swap it out for spots, then suddenly we have a different creature.
If I take that sort of underbelly, that light color there, and swap it for some fur.
then we're gonna have another set of textures and exponentially we grow the space in which our textures exist.
And this is with a diffuse texture, but we do it matching up with our normal maps and our specular maps.
So we have a complete set of textures for any of the props in our game just by the artist creating a series of these texture parts.
And so in terms of how we author that.
It's really important to us that our artists use pieces of software that are already out there.
We don't want to make a tool that replaces Photoshop because that would be really dumb.
We're not, you know, we're a really small team and our artists like Photoshop.
So our artists work like this.
They work on a texture as a whole.
So this is a head for, I don't know, some rodent-y thing, I guess.
And they work on all the layers all together there.
So they can see the texture as a whole.
They can combine the layers themselves and look at how that appears and check they're in the correct order and things.
And then you can see all the little hashtags next to the layers there.
And that's because we have a script that simply takes all those textures and it exports them to a series of separate files and names our normal maps appropriately for us to load them in the game.
And to supplement that, we use metadata.
And what you're seeing here is our metadata editor, but the data itself is actually just XML.
We can edit that in text format as well.
And I think that's a really good thing that we use all the time with our generated content, both so that our artists can provide more information on what they've made, give it context, give it probabilities for showing up.
Also so that we can really easily debug and look through existing data.
You know, any time we can see what's been generated and we can check the input in a really clear and easy to store format and go back to it later.
So it's really handy.
So we've got our textures, we've combined them, and the other important step that we do is recoloring them.
So looking at the same texture from before, we're recoloring all the layers separately there.
And one of the pieces of information that our artists gave on the previous slide was what each of those layers is made of.
So.
If they say that a texture part is made of fur, that will determine the types of colors that that layer is allowed to be.
But when we say recoloring, that could mean a lot of different things.
We could be just hue shifting those textures.
So if we have a texture and a color input, we could combine those in a lot of different ways.
So for us specifically.
The most important thing that you can take away from this today, and I think this has been said at Tech Toolbox before, if you're working with textures, don't work in RGB.
Find a nice, friendly texture format that gives you some control over what you're doing.
We tend to work in HSV, but it doesn't have to be.
There are other nice color spaces, but just find something that gives you nicer control over your colors when you're working with them.
So then we have this transform.
And this is our super secret code bit.
You can kind of copy it wholesale if you want, but the important thing to think about here is what we're inputting is our texture, which is our input texture parts, a series of them, the average colors of those, and the color that we want them to become.
So in terms of hue, to give a brief overview, but there'll be more detail on the notes for this slide if you want to come back to it and actually look at how it works.
Our hue is just shifted so that the average color of our input texture becomes the hue that we want the texture overall to be.
And the importance from this is that when our artists are creating those textures, they're not creating them in a flat hue.
They're not creating, they're adding detail and we need to do our best to preserve that.
And that's why we hue shift in this way and do separate transforms for saturation and value.
So for our saturation, that line's simply saying that we will let our textures become.
less saturated to match the colors we put in, but not more saturated.
So we will let them fade out to whites and grays, but we won't boost the saturation just because it's really ugly.
And then that final line, which is a bit of a monster, is basically a curve that came from fiddling with things in GraphToy.
And GraphToy is my favorite thing whenever you need to deal with curves.
And basically all it says is that we wanna keep the dark parts of our input texture dark and the light parts light.
but in between we want to try and match the value of the color that we've chosen for it.
So then in terms of fitting all those things together, so that was, sorry, this was a bit of shader code and we do all these next steps in a shader in the game as we load into it.
So the basic steps to bring this all together are that our artists create all these bits of textures, they create these color palettes.
And in-game, we select the ones we want.
They give us probability data, and that lets us choose how rare individual parts should be.
So we choose a complete set for an individual model.
We load all that data in-game.
We load all those input textures.
We then combine and recolor them.
in a shader, and then the important part is that we then generate mipmaps and compress them and unload all that previous texture data.
A big thing when you're working with textures is that it's really easy to end up having a massive RGB data in your game, so you need a plan for how you're going to get that into a workable format.
And then the results themselves.
We have a bunch of creatures, plants.
We use this texturing technique right across the board.
And this shows a creature in context, but a really important thing when you're evaluating your content is that you're seeing things en masse.
So we have tools that let artists really quickly see vast amounts of content generated from their input work.
and they can quickly call out bugs if there's a problem.
They can see if all those creatures show up red without stripes or if something's showing up too much or if there's a texture that they're not able to get enough from, if it doesn't look different enough to their other approaches, if there's a space where they haven't made enough content.
So being able to evaluate content in a wide way is really important for us.
So I know there's not a whole lot of.
detail on a tool that you can take away, but the important thing I want you to go away with today is that anything your artists make, you can get so much more from.
You can take that content, you can boost it, you can manipulate it in ways, and that can be a really approachable and controllable way to look at procedural generation.
This is great because our artists have made all the things and they know what the output is going to be like.
So we have a fairly manageable space, but we're still doing so much more with the textures they've made than if we just used them directly.
We have a way broader amount of content.
If you have any questions about this stuff, then just shout me on Twitter.
I don't have such a cool email address, but you can email me, and we'll all be in a wrap-up room later, I think.
So give me a shout.
Thanks Ines and thanks all of the other speakers as well.
Look, I fixed a year.
Just to wrap this all up, thanks for listening to a bunch of super varied game developers talk about their games and their tools.
Take it, yeah, just take it seriously and think about the tools you're making and especially sharing them with each other because that's basically why we started doing the Tech Toolbox.
Thanks for coming.
Tell your friends, especially tell anybody you know who may not, out of their own, You know, who may not normally submit to these type of things, tell them to do it anyway, especially your diverse friends.
We really appreciate that, and it's hard for us to find them.
Watch the previous years in the vaults if you thought this was really interesting, and please fill out the evaluation, and definitely come up as soon as we clap one more time to the front of the stage and talk to anybody you have questions about their tool.
Thank you so much, and I hope to see you again next year.
