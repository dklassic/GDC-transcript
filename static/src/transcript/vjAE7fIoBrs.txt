Thanks for coming, it's an amazing turnout.
My name's Alex, I'm a co-founder at Creative AI, and in general we apply lots of AI techniques into creative industries.
One of those is deep learning, but that's more of a hobby of mine.
I apply algorithms on recent papers that I've read and put them onto GitHub, including this one that I did recently called Neural Enhance.
You can find the code at github.com.
That's all the neural network.
And another project I did, this one's published last year at the ICCC called Neural Doodle where you can just sketch cool things and the neural network will paint that out in the style of an artist.
So in this case, it's turning my doodles into a Renoir painting.
So that's also on GitHub.
And my journey with neural networks actually started a long time ago.
And 15 years ago, I published an article called, it was a book chapter called The Dark Art of Neural Networks.
And luckily, a lot of things have changed since then.
And now they actually work.
And we understand them much, much better.
The entire academic community is kind of converged around deep learning and adding ideas or taking the best ideas from the field, and we've got a much better theoretical understanding of how they work.
The downside, however, is that deep learning is still, and it may always be a dark art.
That part is not going to change.
You will never understand a deep learning model as well as we understand a rule-based system or a finite state machine.
And the reason for that is that modern deep learning models can be made up of up to a billion parameters.
That's a billion floating point numbers that somehow combine together, multiply, divide.
You have to somehow understand how this works.
And you will get a very different sense of understanding of how such a model works compared to a finite state machine with a few states.
So what I want to give you in this talk is an overview of all the different tools that you can use to understand deep learning.
And starting from the data side of things and understanding properties of the data sets and how they can help us understand how that data flows through these different models.
So covering the models and the computation.
And then finally, the more the learning side of things and the optimization.
And there are lots of different fields of maths that help understand deep learning.
And I'll give you sufficient references so you can dig into it at home.
Not too many equations, but a lot of homework.
So what is deep learning?
Well, ultimately, the simple answer is that you compute an output y based on an input x.
And deep learning is basically making a really heavy, memory intensive, overweight, bloated function f that will compute y from x.
But it can do things that no other machine learning algorithm can do.
But I like to think of deep learning as basically taking all the ideas of neural network that work well in practice and moving more towards the side of software engineering.
So it's a field that is typically called differentiable computing.
So not just solving problems that you'd expect with neural networks, but generally building large programs that just happen to be differentiable, which means we can train them using gradient descents or other optimization algorithms.
And this is particularly important because these two disciplines are going to merge.
Like software engineering and deep learning are becoming closer and closer thanks to libraries like TensorFlow.
And so it's very useful to understand how deep learning works a bit more formally because it's going to become such an important part of solving difficult problems in the future.
So when should you use deep learning?
And well, the simple answer to this is that it's the same as machine learning in general.
Whether you're doing a classification problem, you're predicting.
Outputs, classes, this is a cat, a dog, that's your output y.
Based on input x could be an image.
Or a regression problem, which is basically predicting continuous values y from the same input x.
Then as kind of a special case, generation is sort of predicting very large images based on very small inputs.
So that's kind of, deep learning is also shining in that department.
And it's one of my favorite applications of deep learning.
So what really justifies using deep learning is actually the data set itself.
So you need to have certain properties in your data set.
In fact, I think there are three big properties of your data that justifies using deep learning.
Otherwise, you might as well just use regular machine learning and linear classifiers.
These three properties are a very high dimensional input.
So if you're working with large images or videos or even a highly sampled audio signal, then deep learning is ideally suited to this.
And that's why all the progress has been made on image recognition and voice recognition and translation, because there's really a lot of data there.
And deep learning does it exceptionally well with it.
But it also requires you to have a sufficiently interesting problem.
So think of deep learning as trying to approximate a manifold in n-dimensional space.
So it's like a decision surface that separates true from false in your input space.
And neural networks are very good, or deep learning in general, is very good at finding these complex decision surfaces.
And if you just had a very simple decision surface, you wouldn't need a deep learning model for that.
The third thing is having a lot of examples.
And this is almost a cliche of deep learning, that you need a lot of data for it.
But Google's internal data set that they wrote about in some recent papers have over 100 million images, which is completely crazy.
And it allows you to be very lazy about how to approach things.
You just grab that much data, and you don't have to worry so much.
However, it does help if you have a statistical understanding of how the data is structured.
So if you have these images, it's good to have some sense of how the data is distributed.
So are there more cats than dogs?
And if so, we need to re-bias things.
Or if there are very few chairs, we may need to train the neural network a bit more on those variations.
to get it to be just as accurate at recognizing chairs than it is animals, for example.
So this is called data de-biasing and it's a, I would say, a relatively standard practice in machine learning and data science.
But what deep learning is sort of increasingly doing is also doing the same thing, but in the input space.
So it's a bit harder to do, because typically, these input spaces are very big.
Here we see a data set that's very imbalanced.
There's lots of samples on the right side, but we're missing information on the left side.
So we could gather more examples from the left and have a much better representative picture of this data set.
And this is more valuable than having a lot of examples that are condensed on the right side.
It's better to have a nice distribution of your input data across all the dimensions that you care about.
And this is a very active and important area of research.
And in fact, Google's data set of 100 million images is actually partly automatically labeled as well.
So they didn't go through all those images and manually assigning labels to them.
They just understand that based on proximity or the structure of the images that they have a certain tag.
So they don't have to go through that whole thing manually.
And that is kind of the essence of this field of interactive learning, being able to suggest questions to humans, say, what is this?
And then popping that up and getting a really valuable piece of data is better than having 10 times more data that is a very low value.
Here are some references if you want to go into more detail.
The slides will be available later.
You don't have to scribble it down.
So now we have this data, we can basically feed it into neural models or deep learning models.
I like to think of these models as computational graphs.
So I'm not going to use the words biologically inspired, or I might occasionally use the word neuron, but deep learning has progressed way beyond the biological inspiration.
that it originally started with.
Now, it's easier to think of these models as very large computation graphs that basically take some input and then produce some outputs with the steps of operations in between.
So on the left side, we have our input x.
And that is typically a tensor.
So that means it's a 2D matrix, for example.
If it's an image, if it's a batch of images, it might be a 3D matrix.
If it's a batch of videos, it might be a 4D matrix.
And then on the output, we have similarly, it's going to be also a tensor, which is going to be another matrix of, it could be Boolean values, that this is a cat, a dog, a chair, or probabilities of different classes.
So within this computation graph, we have even more tensors.
These are potentially smaller matrices of different sizes scattered throughout the graph.
And these make up the model, what we're going to be doing our computation with and what will be trained.
It becomes this function f.
And we have lots of little tensors scattered throughout.
Some of them will be relatively small.
Others will be much bigger.
And effectively, you can think of it as a way of expressing all these different matrices that build up one bigger matrix multiplication with lots of custom operations in there instead.
Another way to think of this is that when you specify these computation graphs, for example in TensorFlow, you'll specify what happens to your input x and the library might not immediately know what it is, so there's some abstraction there.
And then it will sort of reason about these different operations in this abstract way, like multiply this input times this other matrix and then perform a nonlinear function on that, add the results, split it into two.
And all these different steps are specified on abstract concepts.
And then TensorFlow, or Theano, or other deep learning libraries will compile that and run it on your GPU as fast as it can.
So there's two things to note about these computation graphs.
The first is that it's important to have nonlinear functions at regular intervals throughout the graph.
If you just have linear functions throughout your deep model, you'll end up with just another linear function.
So it's going to be a really expensive linear function, which is a complete waste of time.
So you want to put nonlinear functions into your graph to really give it the benefits of the depth.
And the deeper you go, the more benefits you get, as long as you add nonlinear functions.
So from left to right, these are some of the most common, the most popular ones.
On the left is the tanh function.
It's like a smooth step from minus 1 to plus 1.
Sigmoid looks a bit similar, except it's between 0 and 1.
And they basically use this very often for machine translation and sequence-to-sequence learning.
In the middle, we've got the ReLU.
It's called the Rectified Linear Unit.
which is basically defined as 0 on the negative infinity side.
Then it's the identity function on the positive infinity side.
And this is used extremely often in image recognition, image processing.
And then on the right side, we have the LU, exponential linear function, which is kind of a mix between the two.
It goes from minus 1 smoothly transitioning into a linear function.
And this is used more often in image segmentation.
And I like this one.
It's a good one for generative models.
So the second thing to note about the computation graphs, here we have lots of nonlinear functions throughout the graph, but it's also good to have these residual connections.
So this is a concept that's borrowed from signal processing, where you have a base signal, and then you add a bit more detail on top of that.
So in this case, we're going to have a separate branch that branches off, and then another computation is done, and then we add the result back in later.
And this is important because it helps the data flow a bit more smoothly.
If you can imagine a graph like this that has thousands of layers like this, this is the biggest depth that Microsoft has trained.
It went up to 1,500 deep models.
the data flows much more smoothly if you have these regular skip connections throughout the graph.
So that means that when you're going forwards and you're computing these functions, it means you can remove parts and it still kind of works, but also when you're going backwards, it means that the training procedure has an easier time updating all the values in the neural networks.
I think this is one of the biggest insights of the past couple years actually.
It made a big difference in all the competitions.
So the tricks we've seen so far are on the general side.
They're reusable concepts, you can apply them into whatever function you're trying to approximate, you can use these.
But they're also very specific tricks that apply to, for example, image processing.
And this can be thought of as a node inside the computation graph.
It's called a convolution filter.
You might have heard the word convolutional neural networks, and this is what's going on under the hood.
It's like, it's taking a really small kernel or a 3x3 matrix and scanning it over the input image and then producing another output as a result.
So you can think of this as like edge detection or pattern detection on the 3x3 scale and then producing the sort of the output grayscale image that shows how strong those edge detector were activated. So you can imagine stacking these detectors together, we have more 3x3 matrices and just ending up with more and more output channels and they're all detecting different kinds of patterns.
And those patterns then get turned into more patterns.
And as you make the neural network deeper, it learns patterns of patterns and building higher level abstractions.
So on the right, you see there are gaps between the blue cells.
That means we're sort of padding them with zero.
That means you can do things like up-sampling operations.
You can also skip certain cells as well.
You can do down-sampling operations.
And you can change the kernel size and make it a 5 by 5 matrix.
And so in practice, most image recognition neural networks use layers and layers of these operations just stacked together to basically form these deep computational graphs that recognize images.
So another common pattern that's used often in image, not image recognition, translation, is the LSTM cell.
So it's the long short term memory.
And you can think of it as like an electric circuit that's very specifically crafted to be assembled together.
So they put together thousands of these, and then they stack another layer of them on top of each other.
And they can learn sequence to sequence translations.
So they're very good at translating.
And that's what Google uses everywhere.
So here are more details about the models.
But so far.
In general, deep learning is difficult to get right when you're trying to figure out what you need to plug together.
It's a very intuition-driven process, and this is the part where the dark art comes in.
You need to have some sense of what's going to work, and you can do that by reading a lot of the papers in the field.
So every morning I try to read some PDFs from archive.org, which is a preprint server for the machine learning community.
sort of converged on.
So you can get papers that are related to what you're trying to solve, and then get a sense of what architecture they're using, and try it out.
But so in practice, if you have hundreds of thousands of GPUs available to you, if you're willing to spend the money on the cloud compute, then you can use more experimental testing approaches, trying out all the different variations of your architecture and seeing what comes out.
It can be a bit expensive, but that makes it more of a methodical and experimental science as opposed to we don't yet have a full analytical understanding of how these architectures work.
So in practice, what this means is that it puts more emphasis on the computation side of things and understanding how the data flows through your neural networks.
And that's important to understand the statistics of the data as it's progressing through the network.
So in traditional machine learning, you have to, well, it's recommended that you normalize your input data, which means you want the mean to be around zero, and you want the standard deviation.
to be around 1.
And so that has a lot of benefits.
All these nonlinear functions that we talked about, if you feed in a nice normal distribution into these functions, you get a mix of plus 1's and minus 1's, or a positive or a 0 in the case of the middle function.
So this means that the neural network or the deep learning model is actually making decisions.
It's actually deciding whether something should be true or false or a plus one and minus one.
Whereas if you weren't normalizing your data, it might end up in the case of this tanh function, all the values might end up being 1.0, in which case it's giving you no extra information about your input data.
You've just lost a lot of information.
And these were fundamentally the problems that the machine learning community wrestled with for about 20 years until they figured out what was happening So it sounds simple, but it took us a while to figure out how how things were Things were going on under the hood So as you take your data and you filter it through this computational graph, the deeper it gets, the harder it is to predict what shape the data's distribution is going to have, if it's going to drift off towards infinity or if it's going to converge towards zero and end up just vanishing completely.
So that will depend entirely on the kinds of operations that you're using.
It will depend on how you've initialized your data, how you've initialized the parameters.
And so it's generally a very difficult thing to predict these distributions.
So the trick, and this is what brought back the deep learning craze.
is correctly initializing the parameters.
This is one of the approaches such that your output distribution is basically also a normalized distribution.
So if you set the certain weights and biases, you can shift your data's distribution around and then make sure that it is a normal distribution again.
So there's some great papers on this.
This is a recent paper with a new approach to doing that called weight normalization.
It's a well-written paper and quite accessible.
The other approach, which is used almost everywhere, is called batch normalization.
So basically keeping track of the mean and the standard deviation of your data set as you're filtering it through the graph.
And then effectively, you can adjust it on the fly.
The more data you filter through it, then the better it will understand how to move the data set around.
This is a technique that allowed us to really get much better results in image recognition and it's made a significant difference.
This is one of the most important techniques of the last five years.
So as a result of applying these techniques, things like batch normalization, you get a nice distribution of positive and negative results by each of the channels inside your deep model, making decisions positive and negative, and you end up with a nice sparse result, as opposed to just having zeros everywhere.
So, and this is what made things like neural style possible, because each of these different photos, as you filter it through the network, will have a different fingerprint or a different unique identifier.
And it looks a bit like this.
So, you can train for this and try and encourage these results to emerge.
And I'm going to skip a bit quicker through that.
What this means is that you have certain channels of certain neurons that have certain activation patterns, and you can remove them, and the network will still mostly work.
So we don't want to put all of our eggs in one basket.
There's a certain amount of redundancy there, and the systems are relatively fault tolerant, thanks to these representations.
And yeah, you can train to encourage these representations to emerge.
And I'm going to skip this, because I'm running short on time.
But that leads into the section on training and gradient descent.
So this is where we take all these models that we've talked about and we actually train them to approximate the function that we're interested in learning.
And in general, this is also from the machine learning perspective, there are two functions that you can use to train a model.
The first is mean error, which is you calculate the difference between what you have and what you want, and then you use that as a signal to basically train the whole network.
If you have non-continuous values, if you have Booleans, like this is a cat, a dog, then you'll use another error function.
It's called categorical cross-entropy.
And so these are two, they're not specific to deep learning there.
You'll use them in regular machine learning as well.
And the way deep learning takes those signals and then trains the model is using an algorithm called backpropagation.
And backpropagation combines two things.
It combines the chain rule with the update rule.
The chain rule is there to take the error, which is calculated on the right side.
We calculate how far off we are from our target.
And then we sort of propagate that through the graph again.
And the goal is to calculate the gradient of the error.
So we want to know what direction we need to adjust the weights in to get a better result.
So there are derivations of how you can do this.
There are some great tutorials that links to this.
I'm not going to derive that now.
But as a result of applying the chain rule and going back, we get a sense of what the error function looks like locally.
And so we can step in the right direction to minimize that error.
That is done using an update rule.
And there's a collection of different rules that you can use.
There's the standard SGD, which is stochastic gradient descent.
And that's the classic.
It's been around for decades.
And there are others that are more recent, more modern.
And they have a lot of different.
benefits but all the bottom ones use a lot of extra memory to sort of maintain they maintain extra per parameter weights that tell it how quickly to step so you basically get a lot of acceleration for free but it costs you memory So of all these different algorithms, the ones that are used the most often are the plain old stochastic gradient descenders.
So the simplest thing you could possibly do, like just take one step in the direction of the gradient based on random data.
And the second most popular is probably Adam at the bottom, which is used very often.
Eve is the up and coming.
These academics and their names.
And RMSProp is also, it'll come back.
Most of the time, as a practitioner, you don't need to know about these.
They're completely encapsulated.
You just call this update rule, and it just works.
And you don't have to worry about the details.
One thing you might have to worry about is that deep optimization or optimization of these deep graphs is a non-convex problem.
So you have no guarantees that you will reach a global minima.
But as it turns out, and this is very recent research on the topic, it doesn't really matter that you can't prove that the global minima can't be reached because it's such a huge parameter space with over a billion different parameters, there are minimas everywhere and there's many symmetric versions of the solution and as it turns out, these algorithms, the update rules, will converge often to results that are, if not the global minima, they're pretty close and so you won't notice in practice.
So there are also some proofs that if you simplify deep learning down to some very basic primitives then it actually is convex.
So if you want what we can mathematically prove, it's not quite the state of the art, but the state of the art stuff, you don't have to worry about it.
It mostly just works.
The hard part is figuring out the architecture.
So here are some more great tutorials that go into more detail about the backpropagation process and I highly recommend those.
And again I'll post the slides online afterwards.
So to summarize, data is important, but it's more important to understand the statistics of your data, make sure it's well distributed across all your output categories and all your input space.
The models in deep learning have progressed way beyond neural networks and biologically inspired models.
They're general computation graphs that you could integrate into your software to solve problems based on data.
Making architectures and models for deep learning is difficult, but you can sort of get away with doing lots of simulations and seeing which ones work best.
It's a bit computationally expensive, but it's quite a reliable way forward.
And from the optimization perspective, you can't guarantee for sure that it will converge, but in practice it always does.
And modern deep learning libraries are very reliable, so you get very good results from them.
Great, that's it for me.
Thank you very much.
