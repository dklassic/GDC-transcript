I'm Bo, hi. So I got started in games in 2014, started at a company called Wabi Sabi Sound on a, or in the Blind Forest, The Witness, then moved on to Bungie, worked on Destiny 2, some of the DLCs. Then made my way to Naughty Dog, helped ship Lost Legacy, and then Last of Us Part II, the lovely, beautiful Last of Us Part II, and then now my PlayStation just shipped God of War Ragnarok, so.
Yeah, it's been a couple years of a lot.
Yeah, and I'm Jesse.
I started my career pretty much early on in commercials and then I came out here and worked for outsourcing companies like Formosa and Technicolor, things like that.
I actually got to work with Naughty Dog on Uncharted 4 as an outsourcer and then came to Naughty Dog back in 2018 to help ship Last of Us Part 2 and then most recently Part 1.
So I've gotten a little bit of both sides, being an outsourcer with companies and then being in-house with the company as well.
So in this talk, we're gonna be diving pretty deep into the breathing system we developed for The Last of Us Part II, which was later brought into The Last of Us Part I.
So early in the production of Part II, there were, you know, we were hearing about whispers and murmurings of this Bleeding Edge prototype, and many Bleeding Edge prototypes and systems from upstream departments, like animation, programming, props, rigging departments, that frankly made us a little nervous.
It was becoming very clear.
that the studio at Naughty Dog was gunning to release a technical beast with The Last of Us Part II, a title that would dazzle gamers with its interactive props, set pieces, and systems.
So the audio team understood early on that we needed to evolve with the studio and reinvent how we handled certain audio challenges, specifically for player enemies and buddy characters.
After some time spent in pre-production, our needs solidified into two key pillars.
The first pillar being a creative need.
The Last of Us aesthetic merited a robust breathing system.
The Last of Us is gritty, it's realistic, and insanely detailed.
It has a story with characters you can feel excitement with, laugh with, and even cry with.
The high dynamic range soundscape begged for a robust breathing system that really telegraphed the player's level of velocity, fear, pain, and exhaustion all through sound.
If there was a game that required a detailed breathing system, The Last of Us pretty much fit the bill.
The characters you play as are, they're vulnerable, they're broken, but most importantly, they are human.
The goal for sound is always to support the story.
We knew if we could hear their breathing reflecting and supporting these truths, that we could be achieving a higher level of detail that would support both of these goals.
The Last of Us features extensive stealth gameplay, which is a break from the exploring combat states because of the dynamic nature of the audio in the Thiele universe.
A breathing system would be an audible welcome addition for immersive storytelling and gameplay feedback.
And the second being a technical need.
So starting with The Last of Us Part II, the studio adopted this new technology, which was later added to Part II, and it's called motion matching.
I'm sure many of you have heard of it.
Motion matching is a system that programmatically checks the desired character trajectory based on inputs from the stick or from character pathing.
Every game frame, there are dozens, if not hundreds of animations being searched to find the right animation to play in a given context.
It's an extremely detailed and elegant solution to achieve a realistic moveset for a character.
And with this addition to our animation pipeline, audio now has a new beast to overcome in terms of how we traditionally tagged animations.
And just for some history, in The Last of Us for PS3, circa 2013, infected vocal efforts were hand-tagged to animation frames.
The animations played back in a less granular way and were more long-form, so this method was far more feasible and less error-prone for audio.
The vocal efforts were also streaming and not loaded into memory.
And the cons of using streaming sounds meant when a sound triggers, there was a possibility that it may be delayed based on how hard the I.O. was getting hit at the time of the audio triggering.
So, you know, if the game is trying to load a bunch of textures all over, so, you know, sound, streaming sounds are also trying to load, if the I.O. is getting hit, then that sound might get delayed a bit.
And that's kind of the last thing we want with a game like The Last of Us, where...
If a clicker looks at you really quickly, you want to feel like, oh my god, there it is.
And you hear a sound associated with that visual.
So looking forward to part two, we knew that the old method of tagging animations was a non-starter with this more granular spliced animation technology.
We felt that all characters needed two things.
The first being a looping sound concept leveraging game data.
So for this, it's sort of like a single looping sound event that plays on a character's jaw joint or face, right, where we can pipe in game data to our audio middleware, like player velocity, health, any game data that would be helpful to make this a more modular setup.
So we can align.
So we can play back modular bits of audio to align with the animation.
And the other need was utilizing in-memory sounds for frame-accurate triggering.
This is so we can ensure our sounds are completely married to the pixels.
So when a clicker does that, you know, whoop, what was that, right?
That head turn, that contort, when you hit that with a shriek, that, you know, that clicker inhale, that's the sound...
It glues to the animation.
It's merging the pixels and the sound, and it becomes this thing that's super heightened.
It just feels more next-gen.
It pushes the story in a more threatening, interesting way.
All right.
So we got to thinking, what kind of technology do we need to expand our audio systems into something that we can manage for the scope of this demanding title?
After much work and collaboration with dialogue, programming, and other disciplines, we landed on the beginnings of a system that would snowball into something expandable, flexible, and something that we were all really excited about.
It can be summed up into two core concepts, and we call those murmuration and heart rate.
Murmuration is a persistent loop sound on a character.
We call it murmuration because it's basically the sounds under the sounds.
For Joel, Ellie, and Abby, it's immersive breathing.
For infected, it's groaning, mumbling, clicking, shrieking, frenzying, things like that.
And for dogs, it's like mouth sounds, licking, things like that.
And we even used it on the baby, on baby JJ as well, to be able to play individual cooing or things like that that babies do.
It's kind of anything that we wanted it to be or needed it to be, and that's kind of the power behind it.
Let's watch a demo of this in action.
All right.
So murmuration is the lowest priority sound possible within our dialog system.
There's a big laundry list of vox priorities where the higher priorities will basically stomp the lower ones.
For example, a player leaping over maybe a short wall or like a vault, the player may play an effort.
This effort has a priority of what we call effort low.
This effort would stomp the murmuration loop, and once the effort has ended, the murmuration loop would seamlessly return.
Now that we had a functional looping sound event implemented into the dialogue system, we now have the unique challenge to create a game variable that would relay that theoretical heart rate of the character to this looping sound at runtime.
This illustrates their tiredness, pain, exhaustion, and fear, or really any emotion that encapsulates what we needed to telegraph to the player.
And not surprisingly, we called that heart rate.
Heart rate, yeah.
So ultimately we needed a base way to express the tension of the game, which manifested as the danger level for the player.
And we chose to express this value as a floating point, ranging from 0.0 to 1.0, with 0 being the most relaxed and 1 being the most intense.
The basic tensions of the last of the combat loop is what's called ambient explore, and this is where, you know, Joe and Ellie are having a light conversation.
Maybe there's some scavenging, maybe there's some narrative lines that are playing.
You know, pretty low energy.
And then there's stealth.
So stealth is where the player is, you know, sneaking around enemies, but the enemies do not yet have a full awareness of the player.
So there's obviously a lot more tension than ambient in that sense.
And then combat, of course, is when the enemies have full acquisition, they've acquired the player, and are now actively engaged in combat, actively trying to shoot, maul, whatever.
This is a brutal game.
Shoot, maul, whatever, you know.
So yeah, and with these three base AI concepts, we broke up the logic in that there were different heart rate states for pressing L1, so running or not running in each of the three base tensions.
So if you look at the, you know, this is kind of a simplified version of it, but essentially you see like there's ambient high and low, unaware low and high, combat low and high, and the purpose of that is high, It's basically, it's that AI state, but you are deciding to sprint. You're sprinting.
And then low is you're not sprinting, right? So you're either walking, idle.
Things like that, crouched.
So after we found a potential solution for the heart rate states, we worked with gameplay programming to create a way for us to interpolate or slew the values that heart rate was sending to the audio middleware.
This was so we can get a more hand-authored, realistic sense of growing exhaustion or the opposite, that recovery in the breath after having sprinted for a long period of time.
And so here's a video to demonstrate heart rate active with debug to actually see what's going on.
Let's see how.
So as you can hear, when Ellie started to sprint, her target heart rate immediately snapped to a destination value.
And the current heart rate then slews up over time to get to that destination target heart rate.
And so using current heart rate as a basis for driving all the sounds in the middleware, that gave us the ability to add what we call, you know, stages to the breathing.
So for example, we broke up the combat sprint into buckets of three stages, with stage one sounding the least exhausted and stage three sounding the most exhausted.
So, actually show of hands, who's played part one?
Like the recent part one, maybe? Or just...
Part two? Part two? Alright, alright.
So, I mean, maybe you get it, you get the whole, you know, combat, sprint, the AI, that's great.
So, um...
Well, now I'm like, off.
So yeah, so they were broken up into three buckets.
So yeah, as you're running, I'm hoping you've kind of noticed that there is that tension, you know, Ellie and Joel, when you're sprinting with them, you start to hear like, oh my gosh, they sound exhausted, they sound beat.
So that was our effort, right?
And hearing that really like glues you to the character.
You feel like you're in that headspace, you know, with the bullets whizzing by, the dogs mauling, whatever it is, right?
So these stages, and this is actually a really important point, was all of this was recorded with those principal actors, with the system in mind.
So with that, what you get is this, you know, very natural, seamless representation of a growingly exhausted or decreasingly relaxed breath in the character's breathing cycle.
And so here's just a bunch of text for you to squint at.
So on the left is an image of the heart rate states and the settings sound designers were able to modify.
The delta value is the difference between the target heart rate values.
So...
I'm going to do my best here.
But so this is at 1.0, combat high.
So this is when you are sprinting in combat.
And the delta for the growth duration just basically means if you're coming from combat low, aka you're not sprinting in combat, the delta would be 0.3.
So I threw in 18.
So over 18 seconds, it's going to increase and hit that target.
So yeah, so we were able to say, and what's really cool about this is we were able to say, like, you know, for a character like Ellie, maybe she's more light on her feet.
Maybe she's, you know, done cardio more.
So let's make it so that she sounds more tired after a longer period of time.
But for Joel, maybe, you know, maybe Joel hasn't been as strict with his cardio workouts, right?
Maybe he's drinking too much coffee.
So his sprint stages, you know, progress quicker, aka he sounds more tired quicker.
This was a great setup for us to not only get a feel for what sounded good, but also to tell you a little bit more about the character.
You know, it's a really interesting subtle way to add storytelling just to breathing, you know.
And to speak on the audio implementation side, the image on the right is from our proprietary PlayStation audio middleware, Scream.
So what you're seeing is a small slice of what the looping player murmuration is doing under the hood.
So the audio executes from top to bottom, and what happens first are what I call the initial checks.
So it's just checking a bunch of game data.
Yeah, so these checks are for variables like, is the player injured?
Is the current heart rate greater than or equal to 0.5?
Or 0.85?
And so with these checks, that's the way to route the logic to, oh, you are injured, so go here, and then play this bunch of logic that will pepper in the injured sound.
So, and that's what's amazing about Scream, is the SCR is an acronym, it's scriptable.
It's essentially a scripting...
middleware, which is really empowering when you get a hand on it, you know.
And lastly is the sound execution stage, where the middleware will randomly pick a breath asset to play.
And this matrix of logic on the audio middleware side is the core of how we dynamically change stuff like, you know, the volume, the frequency, what assets to play, and the overall mix of the breathing context, really.
And here's it in action.
You should have done more cardio.
You should have done more cardio.
So yeah, I really hoped, or wished, Troy was here so he could just sit here and watch himself, you know, breathing.
It is hard to say apologies, but I did try to superimpose that text.
But essentially what's happening is all these curves, all these values up here, this is all game data where we are able to smooth and adjust and basically alter the breathing loops and assets to where it just fits in the context of the game.
So using variables like velocity, current heart rate, we were able to do things like, one, decrease subtle delays between breaths as heart rate increases, which can suggest a more rapid breathing.
We were able to slowly creep up volume and pitch as heart rate increases, suggesting larger exertions.
And we set a flag based on velocity to seamlessly change the idle breath to the walking breath buckets.
And this.
I mentioned before this is a view of Scream, but this is specifically what's called the CC, or Continuous Controller view for Scream.
All right, everyone still with us?
Sorry, so now that we basically have solved all the technical blockers in our way, we now had a different challenge.
This brings us back to the main goal of the system.
If you remember, we said this will always be something to support the story and its characters.
So how can this system support the narrative the game was trying to tell?
This brings us to arguably one of the more important features of this system, which is the narrative impact.
So in a studio where story is absolutely king, everything that we do in the audio department always comes back to a single concept.
How does it support the story?
From small drawer opens all the way to, you know, characters breathing.
To achieve control over this newfound system, we added script functions to do things like Suppress the breathing loop for a period of time.
So there were times where suppressing murmuration helped the narrative and it kept combat tense.
We could be silencing stalkers during big infected fights.
We could be silencing Tommy when he's playing cat and mouse with Abby during the sniper showdown.
Spoilers.
There were key moments where silence basically just was the answer.
So in this next video example we're going to show, Ellie is entering a new location.
This is in Last of Us Part 1.
She's entering a new location where the player needs to feel like there basically are no more threats.
They've achieved their previous combat area, and they're moving on.
So to achieve this, the ambient infected in the distance, which you can't see, we silenced them until Ellie reaches a certain point.
The sound of the infected breathing loops would turn on right before her body language and her efforts change, basically to telegraph to the player that there actually is danger nearby.
Let's watch.
OK, so two things.
One, as you heard, when the infected get turned back on, Ellie also has a big like, right?
So right there, we also spiked the heart right there to give her that sense of like, oh, there's danger nearby.
At the very end, we had a unique kind of problem.
So another feature at the end that we had was when Ellie is closing the door, And she's invading the infected, right?
So in this section, once you close the door, this whole entire previous level would just completely unload due to memory issues.
Something that I'm sure a lot of us game developers are very used to.
So, because of that, if there were any infected still alive, you would basically hear them abruptly just cut off.
Right?
And that would completely break the suspension of disbelief.
So to fix this problem, we were able to take the current breathing loop on any infected and seamlessly transfer it to a spawner in the world where we were able to place wherever we'd like.
To basically to allow for like a seamless handoff to other objects, like if they would unload.
So we would take that, and in this specific situation I took those and I just like lerped them, those spawners, towards the door to basically build tension, almost like you're voluming, you know, raising them up.
And then as soon as she actually closes it, then I do the same thing and I alert them away from it, all the way far away so that they would fade away seamlessly, right before we would basically just completely suppress them for good.
So this allowed a good storytelling idea of like, oh, okay, you've closed the door, maybe the infected now have tried to run away, maybe they're going to try to find another way out.
But it allows them to fade out rather than to cut off.
And in that situation, as soon as the door closes, it's just those objects that are completely taking that sound and populating it onto a different spawner and then sending it away to be able to do that.
So another way we elevate the narrative is what we call spiking the heart rate, which Jesse mentioned.
So we created a script function which snapped the current heart rate, which basically was a knob for us to say, sound scared or tired right now.
And that leveraged the systemic breathing assets that worked procedurally in a way where it just worked wonderfully in very choice narrative gameplay beats.
So let's see an example of that.
This could be a tad loud.
Oh yeah.
Just a warning.
There's a bloater in it.
I want to get out of here.
Other than drinking a lot of coffee?
Uh, yeah, so, yeah, this is an interesting one, because, um, one of the cool details about spiking heart rate at the end there was, um, that tired breathing loop was acting as this, like, really awesome, interesting, connective tissue between those narrative lines that were, you know, directed to sound tired, like you just, you know, you just thought a bloater, right?
So it was a really interesting, uh, cool way to, to, to web it all together.
And then lastly, and this is one of my personal favorites, we were able to override the whole breathing loop.
So far, all we've talked about is the base systemic breathing loop that supports the tension.
So ambient, stealth, combat.
It's a lot of assets, a lot of complication, but you know...
What if we add a custom breathing loop to a very specific narrative moment that just plays a whole different loop?
And so yeah, so this let us play custom tailored assets and it allowed us to assemble a whole custom breathing set for key narrative moments, pushing the storytelling through sound.
So here are some examples from part two.
Where is that coming from?
So in the first example, the moment Ellie hears Joel getting tortured, the murmur loop changes completely to a custom breathing set.
And what's so impactful about this set is that not only is it emotionally charged, but uh...
A little technical, but it's kind of cool.
The whole Maya XYZ is exposed as a global variable for sound.
And so as you descend, go down the chalet, I was able to make it sound more high-tension, more like a climax getting to that door of fear and terror.
So, that was really cool.
And then the second video, yeah, so I'm overriding her default breathing loop there to custom breath assets that Ashley did for that cinematic.
So the breathing transitions seamlessly into the cinematic dialogue stem.
And that was really cool.
And that was just a collaboration with the cinematic team where, hey, do we have any Ashley breaths from the cinematic that I can take and build a breathing set with that so that transition can happen?
Because that moment's pretty quiet.
You just hear that tone of the music with breath.
So it's just another cool way to add that detail and that quality.
OK, awesome.
So.
We now have a hyper-realistic breathing simulator for an interactive player character.
So then we thought, how can we expand this system into something other departments could leverage?
So we began branching out with an interdepartmental mindset.
So why animate the player's facial expressions divorced from this robust and expansive technology that we've all created?
So our first collaborators were with animation.
So Keith Passiello, who was pioneering the emotional facial system with Mike Herhan, Christian Whirlwind, and others, they were incredibly excited at the pitch that we gave them basically in regarding to a breathing solution.
So he was able to provide us with a myriad of custom facial animations to play in tandem with whatever breathing set we wanted.
For Abby, basically Abby's vertigo, featured in part two, Troy Slough animated the shoulders so that you can clearly see a sense of Abby's body tension, like the terror of heights that she has.
So we're going to show a video for this, but for both examples, the duration of breathing assets dictated these animations.
The sound was driving the animations, basically ensuring perfect sync.
So we were, in a way, kind of animating, like in a sense, right?
So here's a video demo, and then we'll talk about it.
Let's see if we can achieve this.
It's funny you're showing that like yeah, it happened but like The amount of time it took to for me to move the camera lock it and do that and look at him like oh my god It works This is unreal So yeah, this is a very brief description of how we made it happen, but it's such an amazing collaboration.
So yeah, I mean, I did my best to overlay text to give some insight, but yeah, it's really cool stuff, you know, working with programming.
Like, just as an example...
The fade of that facial animation was, we could input values of how much does the heart rate actually affect that animation, because we can set it so, all right, as the heart rate gets higher, basically are you more exhausted, right?
That facial animation could look more intense, or her eyebrows are more down and she looks more beat.
Yeah, it's really, really amazing.
The next one.
Yeah.
Let's show the next one with Abby.
Mm-hmm.
Watch our shoulders, or not, in particular.
Huh?
Huh?
Huh?
Huh?
So as we know, we work on software, essentially, right?
So programming is the core of how games are made.
And their success, or sorry, geez, and their support on these projects was paramount for our success.
Everything we've shown so far wouldn't be possible without their knowledge, insights, and critical additions to the tools and technology.
So here's another thing you have to squint at, sorry.
So on Tilo 2, programmers Jonathan Lanier and Eli Omernik provided the audio team with features that radically altered how we implemented breathing assets.
And to go into a little bit more detail, in our audio middleware we had the ability to send integer values to the engine that dictated, one, which facial animation to play, of the ones that were supplied from animation, and two, the duration in milliseconds of the corresponding breath sound.
So in the image on the left, we can see our breathing sounds paired with what's called the plug-in command grains.
Essentially, a plug-in command grain is just sending game data back from the middleware to the game engine.
So the image at the top that you see there is the property view of this plugin command crane.
So you can see int1, integer 1 expresses which animation to play in the array or list defined in the image on the right.
This gave us the ability to actually cherry pick which facial animation went with which breath sound, enabling us to essentially define and polish the facial animation in tandem with our breathing work.
And yeah, so integer two expressed the time in milliseconds of the corresponding breath sound file.
And so I believe Eli was able to create a technology that either stretched or compressed the facial animation per sound file.
So I believe each breath animation, facial animation was 30 frames.
Everyone, I believe, was all 30 frames.
And so this logic would essentially go, oh, if the sound asset is longer than 30 frames, then it will stretch.
Or if it's shorter, it will compress each corresponding facial animation, in real time, of course.
Which is an amazing, amazing thing that worked really well.
So yeah, this feature working in concert with a breathing sound feature was the pinnacle of our efforts.
Yeah, so kind of in summary, you know, this was an amazing, amazing thing that was able to happen because of this team coming together and, you know, working on trying to, you know, push for something that we were really excited about.
It's such a gratifying experience having all these departments come together.
Okay, so a lot of information.
We're going to do a quick recap.
So basically with things that we've talked about, right?
So we said The Last of Us really required like a robust breathing system to support a gritty, realistic combat with vulnerable characters, right?
So to achieve that, we really ran into another greater need, which was the technical need, which was with our animation team adopting motion matching.
This birthed the creation of two main pillars in our system, which we called the breathing system.
And those were the core concepts that we called heart rate and memoration.
The breathing system was used to impact the narrative in ways that we were never able to do before.
And then we started collaborating with other departments within our studio to spearhead features like facial animation that was driven by the breathing system.
So at the end of the day, our number one priority was to honor the characters and immerse the player in the story and give them an interactive experience that would leave a lasting impression for the players.
There was a lot of pie-in-the-sky ideas for the system, most of which were able to be fulfilled, thanks really to the passion, the creativity, and the talent of the entire team.
So we'd like to give a little bit of special thanks for people that were able to work with us and help us on this.
None of this really would be possible without the passion and just the creativity of the co-collaborators within the company, within Naughty Dog, and be able to help with this.
So we just wanted to express our gratitude to some of those.
And that is it. Thank you so much for coming out.
Thank you.
Yeah, we wanted to leave a good amount of time for Q&A, because this is quite a technical talk.
You know, there's a lot of confusing, potentially confusing stuff, so please feel free to...
Yeah, ask any questions you have.
Yeah, it was an incredible talk.
Thank you so much.
I had a question.
You were talking about what some of the more specialized breathing loops.
I was wondering how you account for kind of like player freedom in those situations.
Like if someone in the chalet went down to the basement and then turned around and went back upstairs, would you maintain that momentum?
Would you kind of roll it back?
How did you deal with those situations?
Great question.
So in that case, yeah, it was just based on the Zed.
So the, The height.
So yeah, when you would go back up, it would revert back to the less intense, but that's a great point, right?
Like, maybe not. Maybe it shouldn't be like that.
You know, there's a lot of ways you can do that, but...
Awesome.
Yeah.
I was just wondering what you guys chose to do with it.
Yeah, it's a good way to think about it, because you can, you know, and in that way, that way of thinking, right, is...
is a story way of thinking, right?
Because you're going down and you're like, well, I've already heard those yells, so maybe I should stay, you know, really heightened.
Maybe I should stay that way.
So you can change it to say, no, I want the character to feel this way.
And that's the right way to think, because then you're thinking about the character and about how the player is perceiving it.
In that situation, yes, you know, most people did not turn around and go back.
But it's a good thought.
A lot of it comes down to scope.
Because you could do a thing where, well, if you stay at the door, maybe after 20 seconds, they have a whole new set of breathing for the...
The I'm at the door tension, you know, breadth assets.
Like, you know, so, you know, this was already a lot, obviously, because we had other, this was just incepted on part two.
Like we didn't, this was not our main task.
This was just kind of added on top.
And we got, you know, it snowballed, like it was mentioned.
So we were definitely trying to manage the scope of this, you know, so.
Awesome.
Thank you so much.
Yeah, thank you.
Thank you for the question.
Hi, thanks a lot. Awesome talk.
I'd be curious to know how far you kind of push things in terms of exposing game variables and having this kind of procedural approach come from the animation side, and there's a lot of other things happening. There's footsteps, kind of gear and pack, and ambience based on clothing, grabbing objects. Did you expose more things, or did you still put tags in animations for specific sounds?
Yeah, so basically, the core of the question is, were there animations that you would play systemically that would talk to the breathing?
Like picking up or...
Like, for example, footsteps.
Did you tag these in the animation data as before, or did you have, say, maybe a game variable which indicates when footsteps are occurring?
Oh, yeah.
So what's interesting about the murmuration, which is just the looping sound on the player, persistent.
And yeah, it was literally just based on, you know, velocity or are you sprinting or not sprinting.
We didn't consider, you know, footsteps, the rapidness of footsteps or anything like that.
Yeah, we had, I mean, I would say the things that we tagged that would cut off the breathing loop would be things like efforts.
So like swinging a machete that would play like maybe a specific type of effort set or specific type of efforts or...
Or narrative line of course.
Or narrative line going over or jumping over a vault or even death, right?
All of that would have different effects on the breathing loop and then bring the breathing loop back when needed.
But the cool thing about it is that if you were constantly running and you were constantly swinging your machete, you would get all those efforts which are very like high energy, but your heart rate would be going up as you did that.
So then your breathing loop would come back in at a higher...
Like at a higher set possibly, so it would complement the efforts, which was really one of the main goals.
Especially when we moved on to the remake of the first game, because we didn't have any of those assets, we had to re-record everything.
So that's why we had things like injured states and stuff like that, and we re-recorded Troy with all that.
But yeah, for, as in like, for tags and things like that, that's not something that we did, you know.
Yeah, that's cool. Yeah, I'd really love to see this kind of procedural approach applied to more things like, I don't know, how heavy your clothes are or you know how figure out what material they are, have all of these things feed in.
I mean, actually, what was not talked about was, on part two I was experimenting with a bunch of stuff, like when you vault, like spike the heart rate every time you vault, or spike the heart rate every time you get out of the water after swimming, but procedurally.
It was like the nudge value was the name of it.
And so we were experimenting with a lot of stuff, but in the end when the game shipped, all those were just zero, zero, zero.
Like, no delta was applied. Like, it just didn't really work great, you know.
And yeah, it's just about hearing and feeling.
And if it feels like, oh, that felt...
In that context, that felt weird.
But that's always going to happen, right?
And when you get out of water, I...
It's a weird sensation hearing that.
Especially, you know, you have to account for, oh, are you in combat when you're getting out of the water?
Are you in stealth when you're getting out of the water?
So we actually found, like, the more simple approach, the better.
We're going to have a whole suite of sounds for prone.
Like when you're prone, what does it sound like?
Maybe the mouth is close to the grass or the dirt.
Like, I don't know, scope, right?
Like, and the bang for the buck.
My big thing is bang for the buck.
Like, is there value in doing this?
Are we just doing it just to do it?
Or is it going to have, like, a, it's going to make, you know.
the player feels something, is it going to add something of value.
Right.
Thanks a lot.
Yeah, thank you.
Thanks.
Great talk, by the way.
Thank you.
I was wondering when you said that each heart rate had its own delta to move back down between states, if the player changes target heart rates in between thresholds, how do you decide what delta to use?
Um, yeah, well, it takes your current heart rate.
So yeah, if it was going up, slewing up from .7, and so it was at like .76, then at that moment when you go to a new destination, that's your new delta, .76 and .4, or whatever that is.
Oh, so you just... okay.
Because you talked about how if Joel has a little more hard time doing cardio.
Exactly.
So in that instance, we had different heart rate sets for them.
Specifically for that character, when we set those deltas, we could set them different for different characters.
So a good example of this in part one with Ellie, we had all of Ellie's data from being a player.
And she is a player at some point.
So what I had to do is I had to create two separate completely breathing sets.
And I created two separate breathing characteristics for her.
So I made her basically recover from certain types like combat when she's like we could, because she's younger.
So she could recover maybe a little quicker.
And we made Joel recover a little bit later.
Those are completely different breathing tables, breathing sets.
Their deltas are set for them specifically.
So that was nice, because I was able to get a toggle that said, oh, is she a buddy or is she a player?
And we could actually affect them differently.
So when she's a player, she also has way more detail in her breathing system.
She has catches, like so when she's really exhausted and she's trying to catch her breath.
Oops.
She's trying to catch her breath and things like that. There's way more there.
But we found that if I had that breathing set in while she was a buddy, it was very distracting.
Because at that moment, who are you? You're Joel.
So you're walking around and you have your breathing set, and you have little Miss Ellie running around all over the place like she shouldn't be, but that's just what she did.
And then she was exhausted.
So she was like, you know, breathing heavy and was like, what is going on?
So I had to set a specific breathing loop for that with completely different characteristics.
Even had a situation where she's laying on the operating table and she was breathing heavy because she's in combat.
So we had to suppress that for the rest of the game almost because you walk in as Joel and Ellie's unconscious going, huh, huh.
Very problematic.
So yeah.
Thank you.
Yeah, thank you.
That's the fun thing about game development.
Yeah, I think I'm part two.
Because animation will override the tension, because it's an easy way to like...
Right now, there's a combat set for animation.
So like, what was it?
It was like a pop-gun game with like Abby and...
What was that one in the aquarium?
Oh yeah, yeah, the like, Nerf gun thing.
Oh, was it a Nerf gun?
Yeah, so they had like, you know, it was set to combat or?
It was set to combat I think because they were technically in a combat state.
Yeah, so then the breathing would like follow that and it would work.
And it's just like, geez, sounds like she's gonna kill somebody.
She's just playing along with a Nerf gun.
We had that in part one too with the water gun.
And, you know, the water gun fight, they were running around and it really sounded like they were like, you know, really trying to fight.
But I had to make it more playful. So that was a whole separate breathing loop for that.
So then the programmer added this cool thing that was basically like a, like lock the AI tension.
So it could only adjust within like a boundary, which is really cool.
Yeah.
Sorry, go ahead.
No problem. It was an amazing talk. I learned so much from this.
My question actually was about the accessibility features that were built in for Part 2.
And factoring in the closer we get to more realistic audio, how are we compensating for that for people that are hard of hearing?
And as we increase the amount of sounds that are going to be surrounding the player, how do we still enable players with that lack of ability to hear distance?
And how did you guys calibrate that for Part 2?
So, specifically for the breathing stuff?
Just, yeah, the breathing and the heart rate changes, because they are a lot louder for the player.
I'm just curious if you guys had made any adjustments for the...
Because the accessibility features were huge in part two.
Yeah.
I know for the breathing system, it was a really good question, I can't say that I know of what we did for accessibility.
That was a different person who worked on accessibility part, but...
For breathing specifically?
For breathing specifically, yeah, for breathing specifically.
So, I'm not sure. I don't know the answer to that one.
I mean...
I don't know if you have...
Yeah, I mean, just a spitball.
Like, you know, captions, heavier breathing?
Like, is that kind of a suggestion?
So are they...
Or like, maybe...
It's a good idea, I mean...
I think my question might have been a little confusing.
I actually meant more of hard of seeing, where players are relying on so much sound in their environment where the louder we get around the player, now they're having more difficulty actually being able to understand where the sound is coming from.
So I'm curious to see how you guys adapt to that system and having to still be accessible.
No, that's great. Yeah, I mean on part two, definitely, you know, a lot of folks were talking about, and I agreed and I wish we added this, but an infected slider, or basically a toggle to kind of tame the infected, because, you know, the clicker frenzy was, is, many people, doesn't sit well with them.
Like it's painful, it physically hurts them to hear that kind of quality of sound, really high frequency piercing grating.
And yeah, like that was, I think that was tossed around, but I'm not sure, I don't think it happened unfortunately, but that's a great question, yeah.
Learning opportunity for everyone, honestly.
Yeah, exactly.
And it's, yeah, totally possible.
I could totally see, yeah, like a breathing cold, like maybe don't have it as maybe loud or as prominent or as a constant, I'm not sure, but.
Yeah, no, it's something to think about for sure. It's a good one.
Thanks.
That's what's cool about it. We are always thinking about it and there's, you know, coming off of God of War Ragnarok, like, we did a lot of, you know, looking at the industry and going, what can we do?
How can we be better? And I think every game, there's so many opportunities to be better, especially for sound.
Mm-hmm. You guys did an amazing job. It was a very impressive game, audio-wise, so.
Awesome. Thank you so much.
Thank you.
Hi.
When it comes to, you mentioned the compression and stretching of animations based on the length of the sound for the breathing, was that applied to other things or just the facial animations?
I was also applied, I mean, so it was applied to a lot of things.
Things meaning animations?
Yeah.
Driving animations based on...
Yeah, so it was applied to other types of animations, not just facial animations.
Like we showed the example with Abby with her shoulders, her entire shoulder set.
For the baby, which was very like, a last minute thing, because we were trying to figure out...
You know, depending on the type of content you have, babies can be really hard to put together.
So we actually got specific animations tied to the specific audio that we made.
Little things like, OK, if the baby laughs, we can have parcels where it'll lean back as Ellie's holding the baby and it'll throw its arm up like this, or maybe it'll throw both arms up or throw the left arm up.
So I had the exact same system being able to choose, okay, well, which animation are we going to play?
And then if the baby—I had sneezes in there as well, which is part of the breathing loop— the baby would be cooing or whichever, and then when the baby would sneeze, it would play that asset. The plug-in grain would then tell the game, like, oh, okay, play this one out of three sneezing animations, and it needs to be this long.
So it would literally animate the baby to sneeze.
And it would sneeze, or it would cough, you know, this like disgusting child cough.
And it would cough, you know.
So yeah, we were actually able to build custom animations based on the sound for that thing.
And the whole baby moving around and coughing and sneezing all in the right time is all based off of that system.
So it was cool because you can kind of do a lot of things with it other than just the facial animation.
You're making me think, though.
That sounds awesome. Thank you.
Yeah, thank you.
What other animations can we do?
Oh, we can do a lot.
Stretch, based on sound, speak my language.
Oh. How's it going?
Yeah, so you have your murmur system, which is just a loop that's going on the entire time, which I'm assuming isn't being synced with facial animations and breaths, or maybe it is, but I'm wondering, so like, you showed later that there were specific breaths that were...
that were informing the animation and they were specific lengths for those breadths.
I was wondering if there's anything that you guys need to do as audio guys, or if you just kind of send those variables over to animation and they just handle it.
Does that make sense?
So, the first part, I believe, of the question you were saying, like with the memorization loop is pretty interesting, like, you know, are there facial animations tied to all that?
So every individual breath that happens in any bucket can be, you know, hundreds of breaths has a corresponding plug-and-grain for facial animation.
That's crazy.
So every single one has one.
And we had to enter, actually, Grayson Stone is right there.
He painstakingly entered every individual millisecond number for every individual, which we are trying to streamline better, but that's how it was.
And that's how we did it for the next one.
But yeah, he helped out with basically choosing mouth open, mouth closed, and that.
So it is for every single one.
So it's not just like a loop, a straight up loop that you're just playing.
It's literally individual.
Yeah, one shot.
I don't think I mentioned that, but yeah, each breath asset is an exhale into an inhale.
Gotcha, okay.
Together in one.
And we found that that, because it's funny, I'm actually consulting a lot with other folks doing a breathing system, and there's so many ways to do it, you know?
And just personally, especially with the Naughty Dog engine and the dialogue, it kind of makes sense to do an exhale, then an inhale, because an exhale is almost like an effort or a line.
For sure, yeah.
When you're speaking or efforting, you're exhaling.
Giving your breath and then you're inhaling. Yeah, so when it we're stitching it all together It becomes more in line and becomes more seamless and cohesive Yeah, basically each individual state or each individual type of breath table within that one main loop that's always on, it's constantly checking variables, different states, heart rates, things like that.
And it's picking the inhale, the exhale, the inhale, the exhale from all different buckets.
That's why you get a lot of good variety on it.
And it's like, I mean, some of the individual, like one, like, just stealth breath or just combat breath may be like a...
350 line scream script that's bouncing back and forth between that and another 350 line scream script just choosing. It's messy.
I would think of like just the looping sound as just a nest of logic.
It's just a lot of deep logic instead of, yeah, because usually like traditionally, you know, me before doing all this work for part two, I would have a loop is a sound that has a zero crossing and it loops.
But with Scream, it's just nested structures of logic and it gets so dense.
The systemic stuff is just one loop.
We can play so many different variations of sounds based on all this data.
I don't want to take up too much time.
So you have random containers filled with...
this level of effort, and then that's the loop that's cycling between those.
So it plays whatever, waits for the voice to finish, then goes back up and does a bunch of checks.
And then it's like, oh, I'm injured, so I'm going to go down here, right?
And based on your velocity, possibly, of running or things like that, it will actually change a random delay grain to delay the breath between each other.
So if you're more exhausted, that delay can be less, so that breath, it's like you're, you know, rather than...
So that randomness of that delay based on the height of the heart rate will actually allow those like the space between the breaths to give a whole different set.
Yeah, that's so cool.
Really cool.
Thank you so much.
Yeah, thank you.
Thanks for the question.
Hi.
It's really cool what you guys built.
I had a question regarding the animations driven by sound.
One of the things that is kind of interesting animation system is priority based, like who's driving the animation and who takes precedence.
How did you guys resolve that with regards to the sounds that are driving facial animation versus the dialogue system potentially coming in or other systems coming in?
Same with any of the other kind of full body or shoulder.
Yeah, I mean, I think the whole murmuration loop was pretty bulletproof in that it will always wait for a voice and you will never hear overlapping breaths.
That was pretty much impossible from what I've seen.
And so there was no issue with the facial animations that were driven by the breathing stuff, but of course, yes, there's going to be the systemic lip flap for gameplay lines and all the narrative lines.
And.
I'm not exactly sure, but I assume that the dialogue system stopped the breathing loop and then that lip flap took over or that other animation took over.
So I think it all just kind of worked where things would trump in line with our VoxPriority stuff.
So the animations would also work in that way.
And a lot of times we'd have efforts with certain animations, like jumping over something or swinging, whatever I was saying before, and they would have their own, it's its own animation, its own facial animation, so that would take priority and that would play and then it would go back to our breathing loop with our animations.
And it was pretty seamless, the way that would go, but also it helps that it's more difficult to turn the camera around, it's not perfect.
So, you know, you're typically, the camera's kind of behind you, so it was like when that would switch, it would feel kind of seamless because you weren't necessarily looking right at the player's face.
So, you know, it's not, like I said, it's not perfect, but it did give that sense.
It switched, I think, between it pretty seamlessly.
Cool. Thank you.
Thank you.
All right, hey, I came in a little late to the talk, but I caught the end and it sounds very cool and it's like, wow, I didn't realize there was like a whole breathing system.
I myself am visually impaired and I have friends that are completely blind, so I want to first commemorate you and the rest of the audio team on like doing such a good job that a completely blind person can play the game and especially with the audio descriptive cut scenes in part one, like I've never seen any game do that. So that's super cool.
And I also wanted to know, like, was it easy to, how difficult was it to preview, like the, like, you know, preview and QA test, like the, like how well all the different breathing loops work, cause it sounds like such a, like a nitpicky thing to be like, okay, we have to go in this scene and like, like, you're like run the game and like, okay, we have to wait for it to run.
And, you know, like how, how complex was that?
I mean, it all started with Ashley Johnson's work on the 2018 E3 demo, where she was going through the forest and then you meet the Seraphites for the first time, the gutting, all that whole E3 demo.
She did basically a run of just that whole moment, like linearly.
You know, was in the booth and did a, like, breathing, exertions, efforts, through that whole thing.
And I was able to take all that content and a couple of additional stuff and just kind of figure it all out.
Like, alright, with what I have, how can I make this work? The whole breathing stuff, right? The whole system.
And then, with that...
We were able to go, alright, we now know what we need, like exactly.
So then we were able to go with Laura Bailey, alright, this is what we need.
Like no questions, we know because we already built it with Ashley's set, right?
So when recording Ashley, that was interesting because we just needed like a little bit more, you know, because we already had so much from her, we already built most of what you heard before doing a proper breathing session.
I think, does that answer your question?
I guess I'm coming from the point of, so I do sound for games and like a lot of times like when tying stuff to animations it can be very difficult when the engine, I know you guys got your own engine so I'm sure you sort of like easier, but like for example like with Unity it's like you gotta, it's like okay the animation event is here.
Now to actually see this, first I have to run the game, and then I have to do the animation, and then like, oh no, it's not off.
I pause it, like stop it, move it over.
Oh sorry, you're talking about like iteration process.
Yeah, so how difficult was that?
Not too bad.
Yeah, so screen in the proprietary middleware has a lot of really great features.
So you basically like restart script.
You can connect to the game, be able to adjust your logic, I believe you can adjust the sounds, and you can hot reload banks.
Yeah, it'll just immediately update in the game as you edit, whether it's CC sounds, things like that.
Also, the iteration process of choosing breaths takes a little bit of time because it's all about the feel of being able to get it right.
It does handle it pretty well, trying to iterate, the process of iterating.
Is that?
I'm not sure if that answers.
Yeah, that answers it.
Now I have to figure out how to recreate that in Y school.
Yeah.
Thank you.
Yeah, thank you.
Thank you so much.
We have time for one more.
Yeah.
Perfect.
Look at that.
It's me.
It's you.
Hey, so did you guys run into any issues with the breathing animation being restricted to 30 frames with the randomness that you were intending for the breath?
What frames?
30 frames.
30 frames.
30 frames.
Is it restricted to 30 frames?
I don't think so.
Oh, I thought that's what you said.
30 frames.
Oh, no, no, yeah, sorry, yeah.
Any restrictions with them being 30 frames long?
Oh, yeah.
I don't think we...
With the randomness, because I feel like, I mean, 30 frames was that...
Oh, I see what, yeah.
Right, with the amount of time.
What was interesting is, yeah, it was literally like one breath.
It wasn't like, oh, this is variation number two or four of this one breath for the animation.
It was one base animation.
And then, you know, after a while I asked Keith for, oh, can you give me like a different kind of exhaustive?
So there was sort of variations of the exhausted.
But I think for the most part, because of that whole setup that the programmers were able to do where it would, you know, compress or stretch or whatever, that 30-frame animation, it's just hard to see the sameness.
Like it just...
A lot of the audio felt very varied, like it was, you know, it felt varied.
So the facial just kind of fell in line and you didn't, I don't know, like the association, I didn't look at it and go, oh yeah, this feels samey or uninteresting or.
Because it seemed a little choppy and it wasn't, it wasn't like a, you know, a criticism.
Why there was kind of and hearing the 30 frames I The other questions it's definitely not Like it's it really helps else Her nose and out there her mouth Round or was able to see the player It really helped with that stuff if you I never noticed.
I don't know.
So it's like you're being yourself to the enemy.
So in that situation, the whole thing is to sell like.
So as soon as you and she'll be.
So there was certain tech created for that.
So I mean.
Her mouth will be in the correct position.
But it's a really good thing that we're trying to do.
Thank you.
Alright, one more. Can we do one more?
Very cool. Well, thank you all for coming out.
You can ask one of them.
We'll quickly answer them. It's quick.
Let's do it.
Then we'll go.
The last one, sorry.
I have a question.
If you have any problems with the transitions between the loop system and the individual efforts, like, I don't know, hittings, jumpings, and so on, because sometimes it sounds...
Each individual sound in-stream has an answer, and so I believe...
I think we set the release to be pretty...
You know, long-ish.
So, yeah, you would never get anything super jarring when a sound would stop.
And the only time the loop would actually aggressively get cut off was when you were sprinting or not sprinting, which is why this whole catch idea happened, where, like, right when you let go, without having, like, a...
Without having that, it felt really, like, oh, it sort of digitally just stopped, right?
It felt fake, so we had to, like, fake the realness by adding something, like, organic, like a catch the moment you let go, to kind of blend it all, but.
Yeah.
Thanks.
Thank you.
Thank you.
