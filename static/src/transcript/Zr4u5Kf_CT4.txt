Hey, welcome to playtesting God of War.
And to put it pretty simply, playtesting this game, it wouldn't be the same without playtesting.
Playtesting provided us and our game a huge advantage, and hopefully you'll see that by the end of this presentation.
And so we hope going through this journey of playtesting with you guys that we can either help inspire you or maybe even do something similar to us or even just learn about what it takes to playtest a big game like God of War.
And so just to hone in the focus a little bit, the playtesting I'm talking about is external playtesting, because there's a lot of different types of playtesting that you can do.
You can have internal reviews, you can do focus groups, you can do market research, but that's not what we're gonna talk about.
The playtesting I'm talking about is when you go in and you recruit people to come in and play your game when it's in development, and then they give you feedback.
And so the people I'm with here today is G.E. Shroff and Kevin Kieger.
Jeet's the Gameplay Director at Santa Monica Studio, and Jeet was heavily involved and helped drive the playtesting process from a creative leadership point of view.
Kevin Keeker was a Principal User Researcher at SIE, and still is, and he's right now focused on PlayStation exclusives.
His responsibilities were he planned our research strategy, he guided our data collection, and he helped us conduct analysis and reporting for all the God of War studies.
And me, my name's Ed Deerian.
I'm on the Production Team at Santa Monica Studio.
One of my primary responsibilities was to coordinate all of our external playtests, and I worked as the bridge between our team and Kevin's team as the user researchers.
And so when I'm finished with this presentation, we'll have a little bit of time for Q&A in case you guys have any questions.
And note to everyone, there is spoilers in this talk, so I hope you weren't expecting no spoilers.
And I do spoil the last boss fight, so getting that out of the way right now.
And so before I get into the mechanics of what we were doing in our playtesting process, I'd like to give you some quick backstory and what kind of prompted us to rethink our playtesting process to begin with.
And so it really all starts with this image.
This image represents God of War's new vision.
And it's not exactly what made it into the final game, but it embodies the spirit of what creative director Cory Barlog wanted to achieve.
And the vision for this game was very substantially different than the other past God of Wars.
It had a more intimate and grounded approach, and it was centered on the relationship of Kratos and his new son, Atreus.
And at the center of this vision, brought this idea of character growth into the franchise.
And surrounding character growth, you had the three game pillars, narrative, combat, and exploration.
And so it's an important point to make here that this game wasn't a remake or reboot.
It was a reimagining, and it was a continuation of Kratos' story.
which means that at its core, this was the same Kratos that players have spent over a decade with.
And I'm pointing this out because whether or not people had played the previous games, we knew that some people would come in with expectations, ones that were based on how these past games were.
And so the studio was faced with all these challenges with reimagining this game.
The first challenge being that it was just a brand new vision.
It had a new weapon.
It was in Norse mythology.
It had a no-cut camera.
And it also had a boy.
I was afraid my voice would crack when I did that, so I'm glad that worked out.
In addition, since this was not a new IP, we knew that there were gonna be new and old player expectations.
Ones from people who have played the previous games and ones who were new to the franchise.
And we also knew that this was just gonna be a difficult game to make, because this was a really big game with an extremely high quality bar.
This was the studio's first game on the PS4, which in and of itself required new technology and infrastructure.
And since this new vision was so vastly different than the previous games, it meant that a lot of extra effort would have to be spent early on just designing and prototyping.
And so all of these challenges basically made the early development for the game very difficult.
And all these things were kind of clashing and colliding with each other all the time.
And as a result, the early effort was spent on just kind of building these components up, just to get it playable in the first place.
And so this journey from pre-production to alpha that we had was very long and difficult.
And there was this huge mountain of doubt and frustration boiling up on the team.
And it culminated with a couple of pretty serious questions.
Is this even God of War anymore?
And then more importantly, is it any good?
And so we carried this burden all the way to late 2016, which is when our alpha was.
And this is when we'd set out to play the game from start to finish for the first time.
But it was still alpha.
We knew that not every feature would be completely in.
But this would be where the first time that we as a team would see the whole game being played from start to finish.
And so what was the result?
Well, to put it frankly, the game wasn't very fun.
And like many game alphas are, we, there, you know, that's just kind of how it is.
But there was still a lot of friction and frustrating moments in our game.
And it was very easy to tell that there were some things that were a lot further along than others.
And kind of an example at this time, the cinematics and the key narrative aspects of the game hadn't been fully realized.
And so there was a lot of gaps and holes in the game that had yet to be filled.
And so this huge and complicated puzzle that we were trying to piece together just wasn't really fitting.
And we kind of needed to speed up because we were at alpha and there wasn't a lot of time left for us.
And so the state of the game at this time was telling us that we needed to accelerate.
But not only accelerate, doubt was starting to sink in about the game, about the process.
And so we began to even question the mechanisms for how we were validating our work.
But we knew one thing.
What we were doing clearly wasn't enough, so we had to rethink our approach.
So this is when we kinda took a look at our current playtesting process.
Because we were conducting these playtests, and the studio had been doing playtests all the time, all the way since the original God of War.
But we noticed that these playtests weren't being very effective.
There were some problems with them.
And so I'll dig into them real quick.
The first problem was that playtests weren't really telling us any new information.
Sometimes it kind of felt like we were just looking for sand in a desert.
The feedback we had was pretty surface level, but most of the time it explained to us in a different way what our problems already were.
People would say things like it's hard to control the camera or that they were having a hard time to track the enemies.
And at the time we knew these were problems.
We weren't getting any feedback beyond that.
And our second problem was that we didn't have a global process.
Because like I said, the team was playtesting, but it was happening in isolation, and nothing was really looking at the whole game together.
So we'd get feedback on levels, we'd get feedback on combat, but nothing was really looking at them together.
And as a result of this, we were getting this conflicting feedback.
And this was very difficult to determine what the next steps forward would be.
Well, you know, because I think this beast was different.
Like this was a much different game than we were used to.
And it was a very different game, a bigger game.
And this new game had a new vision, it had more complexity, it had a deeper narrative.
And so playtesting the game, how we were doing it before, just wasn't really working.
And so some just started questioning playtests as a whole.
And this valley of doubt on the team grew even more.
Why were we even doing it?
We just keep getting the same feedback, it's a waste of time.
And so around this time, our sister studio, Gorilla Games, had finished up wrapping a game of their own.
Horizon Zero Dawn.
And they heard that we were looking at revamping our playtesting process, and they offered to give us some advice.
And so we jumped on a call with Matthias and Lambert to hear more about what they had to say.
And they too said that they were having similar struggles with getting a game just as big as ours stood up.
And they had similar experiences with the challenges associated with playtesting such a game.
And they told us about working with Kevin, and how this partnership with the Experience Lab helped them create a playtesting process that was very effective.
And so like we were experiencing, they also had doubt and they questioned how much they should even rely or invest in playtesting to begin with.
And ultimately what they learned and what they shared with us can be summed up in this one sentence.
You have to go all in.
And if you fully commit, it will be the difference and it will pay off.
And so this call with them inspired us.
We immediately contacted Kevin's team and we got to work.
And at this time, I would like to say that it would be a huge understatement if I didn't acknowledge how much of a direct impact Kevin and his team had on the game.
But at the same time, I know that not many studios don't have access to such a user research team.
And so I've tried my best to focus this presentation on the concepts which hopefully are universally relatable.
And so let's dig in and let's start out by outlining our playtesting process and what we did.
So I'll start with some broad strokes of why we playtest, what our testing goals were, and then dive in deeper into the different types of playtests and how we analyzed our data.
And so I'll start by asking this question, why are we even playtesting in the first place?
Well, the first part is we're trying to validate how well we've realized our vision.
And note I didn't say validate our vision.
The vision of the game was set.
We weren't looking for feedback to tell us that Kratos should have a mustache instead of a beard.
We wanted to see how well we had realized our vision through our implementation of it.
And so we needed to make sure that we were getting actionable data that we could use.
Because as I mentioned earlier, we were struggling to get this information beyond what we already knew.
And finally, we're doing all this in order to make the game more fun.
And maybe that's a bit obvious.
But I don't mean just going into a playtest asking, is the game fun?
For us, it was about extracting and understanding this feedback in order to make meaningful steps towards making the game better.
And thinking about it this way was very important to us because it set the tone that these playtests weren't just to tell the dev team if they were right or if they were wrong.
These playtests were helping us get data, and then it was up to the dev team to ultimately decide what to do with that.
And so we made sure that the dev team was getting the data that they needed in order to help make the best decisions.
And so this whole process began with defining good play test goals.
And so we made sure that the goals were clear and measurable because before this new process, we'd go into test just asking, is the UI intuitive, and then just kind of stopping.
But we learned that asking the UI is intuitive is pretty a good starting point, but we had to learn how to focus our feedback towards the crux of what we were actually looking for.
For example, we'd ask more details about the menu flow or how well people understood the tutorials.
And second, we began to realize how much overall work that this would require, and so we began to treat playtesting as a game feature in its own right.
This meant that we had to support it with a strong production backbone, because just like any other features, it needs advocates and it needs people to fight for it.
And so we created a playtest strike team.
This was led by creative director Cory Barlog and our director of product development, Yumi Yang.
Because it was important to us that the leadership was right there in the middle of it, weighing in and driving it forward.
and they were advocating for the process across the team the whole time.
Because you will run into problems along the way, and you'll need a mechanism for being able to adapt and react to these changes as they happen.
And so finally, we needed to be able to find a way to measure success, to see if these changes that we were making in the game were actually working.
And so we took it one play test at a time.
Because just doing one test wasn't gonna fix all of our problems.
And so we were using these play tests to make incremental steps forward.
And so we also closed out, we made sure that we closed out every test with a debrief, where the researchers would present us some top line summaries and some key takeaways.
And this gave us a good place to start targeting goals for the next test.
And so remember these things when you're defining your playtest goals, because it's important to start off playtest with a purpose.
What are you trying to achieve?
And this was a good way of kind of helping us springboard that way of thinking.
And so our next hurdle was defining how often we should test to begin with.
And so this is the playtest calendar that we originally proposed.
And I made this around when we started revamping our process around March 2017.
And this shows the playtest through November.
The blue tests are our usability tests, and the tan tests are big game playtests, which I'll go into those shortly.
And then like most calendars that producers make, as soon as we started doing things, it actually changed.
So it's not the exact calendar that we used.
But there's a few key takeaways from this that we think are pretty important to share.
The first is to reinforce to the team that there's always another playtest.
It's important that the team is confident, and that they feel like they can rely on playtests, which means that consistency is key.
Because if you keep canceling or moving your playtests, it creates instability, and then the process just loses credibility.
And so we made sure up front that when we confirmed a playtest, we stood behind it.
And we did everything in our power to ensure that it actually happened.
And for us, that meant that we had to be okay with taking risks, and balancing new feature integrations with build stability.
Because we were in development, and that's just kind of part of it.
And so it was important for us to make sure that the leadership was on the same page with this way of thinking.
And so this did lead to some heated back and forths about some last minute check-ins.
And sometimes we broke very significant parts of the game.
There was a time where we broke a trace in one of these playtests.
And after you killed a Draugr, he would just yell Draugr over and over again.
He'd be like, Draugr!
Draugr!
Seriously.
It was terrible.
But we had to push through this.
Because in almost all cases, it was better for us to playtest the new things that might break stuff than to leave it out and then miss out on the feedback.
And so this mindset helped kick us into high gear because every playtest was really important.
And lastly, it was important that we took incremental steps forward, each test building upon the last.
This meant that we used the different types of tests to our advantage.
So we'd look at big picture issues and we'd figure out where the pain points were and then we'd dive in deeper.
Because for example, you're not just going to nail the camera after one test.
We were fine-tuning it all the way to the end.
And so we had to make sure that we were constantly making that meaningful progress.
And so now let's take a look at the different types of tests we used and what types of feedback that they produced.
So we primarily used two different types of tests.
We used unguided playthroughs and usability tests.
And both of these tests have very specific goals that they achieve.
And using them strategically helped us find and focus in on the core issues.
And I'll start by talking about unguided playthroughs.
Put simply, unguided playthroughs try to find your pain points.
They look at the whole game and how well you've realized it.
And generally, they'll give you an idea of where people are having the most trouble and where you need the most attention to spend.
And the goal of these tests is to have as little interaction with the playtesters as possible, much like how they would be playing the game at home.
This meant that we would only help them if they specifically asked us for it, or if we knew that they were encountering a bug or an issue.
And so these tests helped us identify and validate the holistic components of the game.
An example of these are like measuring the overall pacing or the narrative comprehension, or the difficulty balancing, just to name a few.
And looking at it this way gave us a nice mental image of where the top issues were in our game.
And that's where the usability test came in.
We would take these pain points that we found in the unguided playthroughs and then we'd dive in deeper.
And these usability tests were meant to isolate the root cause of where this friction or pain was coming from.
And it lets you investigate alternate solutions in a quick and iterative way.
Because in a full play test, you'll get this noise that is created by other game systems, which can lead to generalized feedback.
For example, someone might say the combat isn't fun.
But then you use ability tests, and you find out that the real problem was in the camera sensitivity.
And so it was important for us, that since it was such a big game, that we had to have a proper mechanism to be able to filter out this noise and then hone in on our core issues.
And so these tests, these usability tests, were conducted in small one-on-one sessions where a user researcher would be paired up with a playtester.
And these playtesters were encouraged to have an open dialogue with the user researcher.
We'd ask targeted questions about what they were thinking moment to moment.
And even though we were mainly looking for constructive feedback in these usability tests, things that were wrong with the game, it was also helpful to hear feedback that was telling us that we were on the right track.
And so here's a list of the features that we did some kind of usability testing on.
And as you can see, many of these things are very core components of the game, and that's very intentional.
And since we only had a limited amount of time left, we needed to make sure that we were constantly moving these features in a right direction.
Which leads me to this next question.
How do you extract meaningful data from this process which leads to positive results?
And for both types of these tests, we learned that this depended on how we were asking for it.
And so we used two primary ways to our advantage.
Feedback that was volunteered to us, or feedback that we explicitly asked for.
And so we referred to this volunteered feedback as undirected feedback.
We got our undirected feedback from either in-person interviews that we conducted, or what Kevin called the red button.
Because at any point, a play tester could essentially just pause the game and then just type in a comment box about any epic, frustrating, or confusing moments that they were having.
And doing it this way, this undirected feedback gave us the moment-to-moment feedback.
This was all unprompted.
And this often led to follow-up questions that we would have from the dev team to dive in deeper.
And the directed feedback that we got refers to things that we specifically asked for, usually in the form of a survey.
And so we would always try to get this undirected feedback first, because we didn't want to bias the players towards thinking that there were problems with these game systems that we were asking about in these questions.
And so here's some of our real data filtered by undirected and directed feedback.
And as you can imagine, we got lots of data from these tests.
It was pretty daunting at times.
But Kevin and his team proposed a methodology of filtering this data using adding and subtracting.
So we'd ask these questions through the lens of whether something added, meaning enhanced, or subtracted, meaning reduced, from the game experience as a whole.
And we did this for both the directed and undirected feedback.
Asking the question through this lens helped us not overcomplicate things, and it gave us a nice sorting mechanism to be able to see the top issues.
And for us, this kept the feedback manageable, and we were able to move things into action very quickly.
And so here's a list of the things you just saw on the top undirected subtracts of one of our tests.
And this top example is that four people told us that the Valkyries were too difficult.
Yeah, I know, what a surprise.
And this list, this whole list, came from the red button comments in the interviews that we were conducting, and this was all unprompted.
And now looking at our directed feedback.
you can see that people were struggling the most in this test with the enemy difficulty.
45% of people were saying that.
And so overall, looking at it this way and seeing enemy difficulty on top was prompting us to look deeper into the feedback.
We would see this, and then so then we'd sometimes pull the playtesters out and ask them further questions.
Or we'd target more and more tests in the future to hone in on figuring out what that problem was.
And so overall, organizing the sheets like this between directed and undirected, adding and subtracting, helped us build this consensus.
Because they showed us the state of the entire game.
Because if you're on the team and your feature was just one of those things on the list, seeing everything kind of together in this way showed you the full perspective.
And this made it very clear where we needed to direct our focus.
And this was a huge win when we saw something go from heavily subtracting all the way up and then adding to the experience, and that was awesome to see.
And so as the needs of the game changed and evolved, the data that we were wanting to know and learn also was changing with it.
And so we were constantly iterating and changing these data sheets as the game became more polished.
Because it's an important point to make that each department's gonna be kind of looking for slightly different data.
Level design might care more about seeing the breakdown by the level, or the combat might want to see the breakdown by the player.
And so make sure that you spend a lot of time with your departments in order to understand what their needs are and what data that they need, because it's hard to generalize some of this stuff.
I love this boxed water.
It's great.
And that's a broad overview of what our process ended up looking like.
And to recap some of our takeaways, make sure you have clear and measurable goals.
Develop a good playtesting cadence and stick to it.
Use the strengths of the different types of tests to your advantage, and then make sure when you're getting the feedback that you're considering the whole team.
But a process is really only as good as the people who use it.
And part of our learning process was just doing it.
And so I'll give you some real examples that we went through, which taught us some pretty valuable lessons.
And so starting this off with a little context, when we're designing features, our team likes looking at it in terms of fantasy and function.
The fantasy kind of refers to the intended player experience.
The function refers to the more mechanical things, like the controls and the camera.
And so when we apply this usage that we have on our team of fantasy and function.
We can see that it maps pretty well to the tests that we were also conducting.
The unguided playthroughs were helping us validate this fantasy.
The usability tests were helping us validate the functional.
And so I'll use a boat as an example.
I was able to dig up a pretty early brainstorming document of the boat, describing the early vision.
And the boat was designed to allow the player to explore, to collect, to hear story, and to experience combat in that order of priority.
And if you played this God of War, which I hope all of you have, it stayed pretty close.
Except for the last one, combat, which didn't make it into the game.
So what happened?
The past God of Wars had combat in everything, literally everything.
Even Pegasus in God of War II, which was a heavily scripted moment, had combat.
I'm just gonna let that finish, because that's awesome.
Well, to put it frankly, we cut combat from the boat because it was overscoped.
The boat's core purpose was a way to explore and to hear more about the world.
And so when it came down to making this production decision, the boat combat being the last thing in order of priority, that was gonna be the first on the chopping block.
But that was a hard decision for the team to make.
God of War was combat.
And so after this decision was made, there was absolutely doubt on the team.
Because this kind of broke the classic formula.
But we continued on without boat combat.
And we got to alpha, and the boat wasn't in a very good state.
The controls and the steering hadn't really had a proper tuning pass yet, and the story and the narrative components just weren't even really in at all.
And the story and the narrative components referring to the back and forth banter that Kratos has with his son while on the boat.
And so we had ran some playtests recently, and the general consensus was that people didn't want boat of war.
It was too slow, it was too boring, they didn't like it.
It was terrible.
And so the feedback that we were getting around Alpha was constantly calling out the boat as a bad experience.
And so all of this negative feedback brought back up combat.
See, they hate it.
Put back in the combat.
And this was brought up many times in meetings, even in the form of an ultimatum.
Sometimes it would be like, either you put the combat back on the boat, or we'll just cut the boat all together.
Well, let's look at the feedback.
And summarizing it, you can kind of end up with these two statements.
Players were either saying there was too much boat, referring to the fantasy, or they said that it was too slow and difficult, referring to the functional.
Well, like I said earlier, the fantasy of the boat wasn't even in, and so it makes sense that the playtesters were saying that the boat was boring.
And in this moment, we kind of had to remind ourselves not to address this feedback that was talking about the boat's fantasy.
But we could address the feedback talking about the functional.
And so we asked ourselves, what can we do right now to start improving this experience?
While we wait for this fantasy elements to come in?
Well, we have these usability tests where we can dive into this exact type of thing.
And so we did.
And we made many great quality of life updates to the boat, like you see here, as a result of some of these usability tests.
And doing this helped us confirm and drive some of these things into the game, and that was really cool to see.
And then finally, once these narrative elements of the boat did come in, we were able to get feedback on the big picture.
And then you were able to hear about Kratos' amazing storytelling.
So, no any good stories to pass the time?
There was one that concerned a hare and a tortoise.
So what happens?
They wager on a race between them.
The hare is too confident to victory and foolish, while the tortoise is steady and disciplined.
The tortoise wins.
You haven't told a lot of stories, have you?
That's one of my favorite stories.
And also, like Kratos, I have a pretty good story about the boat.
So let me paint you a picture real quick.
We'd just come off a playtest where we had an issue with the docks.
You just couldn't use the docks or the beaches.
So once you got on the boat, we had to help the playtesters off the boat every single time they wanted to get off, because we had to do this debug flying.
It was terrible.
And so this whole test was a wash.
We couldn't get any feedback on the boat.
And so we started preparing for this next test, because we were putting in these boat stories for the first time, just like the one you saw.
And this was a big moment for the team, because it'd be the first time that we'd be seeing people react to this intended experience that we had for the boat.
And so we're about two hours into this play test, and the first player that got on the boat was rowing up next to a dock.
And then they just sat there.
They just sat next to the dock.
And there were people watching this back at the studio.
And they were like, it was kind of heartbreaking.
It was like, oh, I guess it's broken again.
Like, you know, what are we gonna do?
I guess we have to wait for the next play test.
But then we realized when the next person came in, the docks weren't broken.
They got off the boat just fine.
So what was this person doing?
And so we interviewed this person, and they stopped the boat right next to the dock.
because they were waiting for Kratos to finish his story.
And so for us, that was kind of the moment that we realized that this boat was fulfilling that intended experience.
And that people could enjoy this boat without having the combat.
And so what did we learn from this?
Is that, well, we could have just added back combat at any time.
This feedback was sometimes even telling us to directly add in combat.
People would write in the surveys, add boat combat.
But in these moments, the play testers didn't really know what our final intended boat was supposed to look like, because it wasn't fully implemented.
And so we learned not to prematurely react to feedback, especially since the design intent that we had for the boat wasn't fully realized.
All right, so for the next story, I'll be talking about the final boss.
So we're about to go into a play test where the first time, players would see the full ending of the game.
And we felt pretty good about it.
And to give this a little context, Kratos and Atreus met Freya pretty early in the game.
And Atreus immediately became attached to her because Atreus had recently just lost his mother.
So we began to see Freya as a maternal figure throughout this game.
Well, in this scene, Freya's son Baldur is about to take her life.
Yeah, it's complicated.
But Kratos steps in to save Freya by having to kill Balder.
So let's see what Freya had to say about that.
No, no, no, no, no, no, my boy.
My dear sweet boy.
Freya, he chose this.
I will rain down every agony, every violation imaginable upon you.
I will parade your cold body from every corner of every realm, and feed your soul to the vilest filth in hell!
That is my promise!
He saved your life!
He robbed me of everything!
It's everything.
Yep.
After watching the scene, playtesters were telling us directly in the feedback, they didn't like how Freya suddenly turned into a villain in the end.
They actually said that.
And we're like, what? Freya's not supposed to be a villain.
She had this impossible situation. What was she supposed to do?
Well, we looked at the feedback and we noticed that there was this divide.
Some playtesters were empathizing with Freya.
And some didn't at all.
And they were even calling her a villain.
And so we kind of asked ourselves, why would the playtesters think that Freya's a villain?
Maybe the problem was in that moment, they were identifying more with Atreus, not Freya.
And so we thought, why might that be?
And we thought that people identifying strongly with Atreus might not be parents, or they might not understand the impossible situation that Freya was in.
And so we added a new part immediately after the cinematic.
I don't understand.
I know saving her was the right thing, but she seemed all evil at the end.
Not evil.
You killed her son, lad.
Her son.
That death of a child is not something a parent gets over easily.
But he was gonna kill her!
She would have died to see him live.
Only a parent can understand.
So you'd let me kill you?
If it meant you would live.
Yes.
Look, there was no easy choice.
For anybody, brother.
But I think we can all agree you did the right thing.
The world's a better place with Freya in it.
Just...
Give her time, lads.
She'll come around.
And so in this new part, we had Atreus ask this question back to Kratos, as if he was one of the playtesters who was calling Freya a villain this moment.
So he'd let me kill you?
To have Kratos answer, yes, yes he would.
And that only a parent can truly understand what Freya was going through.
We noticed that after we added this bit of the game, it worked, we stopped seeing negative feedback talking about Freya.
It was pretty, pretty cool.
It was great.
So what do we learn from this?
Well, sometimes you might go into a playtest and you think that you've got everything figured out.
And then suddenly you might get feedback that you don't expect at all.
It might even be the complete opposite of what you were intending it to be.
And you know, everyone who plays games will have their own unique perspectives, and as a result, they could misinterpret something like this.
And so playtesting showed us what it really meant to listen, to hear people out.
Because we could have just said, oh, well, those players don't get it, possibly missing out on a legitimate issue that maybe was isolating a big chunk of our players.
And it's an important point also here that play testing isn't just about testing gameplay.
Because we found that it also can be a pretty useful tool for helping measure narrative comprehension.
You don't know what this means yet.
but I'll talk about our good friend, the olive pizza guy.
And so what if somebody comes in and they play your game and they just absolutely hate it?
So much that they write about it in very painstakingly specific detail in all of the feedback that he gives.
Well, lucky for us, this happened in one of our playtests.
And now before I read his comment, I'd like to say that we're very open about sharing the data that we're getting from these playtests with the team.
And we'd send out the data every night after every test with a basic daily report, kind of with the top findings.
And so now you saw it a second ago.
But let me read his comment to you.
And I quote, "'The puzzle solving is a bore.
"'What's worse, the amount of puzzle solving "'significantly outweighs the action.
"'And when the game is called God of War, "'that's just not acceptable.
"'Imagine you called Pizza Hut "'and you ordered a meat lover's pizza.
And when your pizza arrived, there was more olives on the pizza than meat.
That's how I feel about this section, and just about in every other section of this game.
I went in expecting lots and lots of action, lots of meat on my pizza.
But no.
I got lots of puzzle solving.
The pizza arrived stacked with olives.
Who the hell wants more olives than meat on their meat lover's pizza?
Nobody.
That's who.
Now who wants Kratos to spend the majority of his time as an imitation Laura Croft doing puzzles versus being a badass and killing everything that moves?
Also nobody.
More action, less stupid generic puzzles.
And oh, that's another thing.
The puzzles are so boring and generic.
Ring, ring, 1998's Legend of Zelda Ocarina of Time called.
It wants its basic, outdated, 20-year-old dungeon platforming puzzles back.
And so naturally, like I said, the dev team read this comment.
And so halfway through the test, the olive pizza guy was a celebrity on the team.
And it was very clear from his feedback that he was a fan of the past God of War games, because he kept referencing these games in all of his comments that he was making.
And so on one hand, we kind of saw the thing that we feared.
There was a passionate God of War fan that just didn't like the new game.
But on the other hand, the Olive Pizza guy's intensity clearly came from a place of love.
He was a self-proclaimed huge fan of Kratos in the franchise.
And when he wrote this comment, he boldly and directly challenged our new vision for the game, when he referred to the puzzles in the combat.
And so we learned a couple things from this.
The first being that not everyone would resonate with this new game.
We were making a reimagining of God of War, one which had more puzzles and combat, and that wasn't changing, and not everyone would like it, and that's okay.
And we also had plenty of evidence to support from our play tests that we were doing a pretty good job.
People loved the characters, the setting, and the narrative.
And we had made enough progress through iterating on these play tests that the problems weren't in the mechanics, they were in the balance.
Which leads me to the next learning, is how to strategically look at the feedback.
Because the Olive Pizza guy even told us that he hated puzzles in general.
So why would any of his feedback about our puzzles really help?
But he did like combat.
And so we looked at his combat feedback.
And in his own hilariously accurate, but mildly anger-inducing way, the olive pizza guy had a lot of great points.
Because he was trying to tell us how slow the combat pacing was during the beginning of the game, and that we needed to retune the pace, and that our decision was that we needed to retune the pacing so that someone like him early could get enough of a combat challenge that they were looking for, while still being able to engage with the puzzles, even if he didn't really like them.
And so this was really valuable feedback to us because it helped call out a legitimate issue.
But I'm not done.
I received a text the day after we got this feedback.
And it said, before he eats his normal lunch tomorrow, you should buy this guy a pizza.
But put olives on it.
And so I did.
And we made sure it had a lot of olives.
And for the record, just so you don't think we're complete jerks, he was smiling under this picture, I promise.
And we also bought him a meat lovers pizza also that he actually ate for lunch.
So.
And my final example is about our difficulty.
Our game has many different difficulty settings.
You can even switch the difficulties around at any point.
So let's say you start by choosing the balanced experience or normal, but then you thought the game wasn't enough of a challenge.
You could then bump it up to give me a challenge.
Well, the feature for switching this difficulties wasn't in yet.
So for a long time, we were relying on the balanced experience to give us a baseline.
And so in one of these play tests, we intentionally tuned everything way harder because we knew from the previous test kind of what too easy looked like.
But we were looking for this sweet spot.
We needed to find out where the hard ceiling was.
And so we tuned it way up, and we found out after that play test that that was too hard, so we tuned it back.
Well, the next play test, we were about to go into this play test after the hard play test, and we'd have this switching difficulty feature in.
And this made the combat designers very excited.
Because this would be the first time that they'd see how people were utilizing this feature.
especially how they would react to the harder difficulties.
And so there was a player in this test that we were having, and they were absolutely raving about the game.
Everything was perfect.
The game was perfect.
The difficulty was perfect.
10 out of 10s.
Well, our combat designers were very interested in this player, because they were making all of these comments about how well and balanced they felt like the difficulty was.
And based on his feedback, This player sounded like they were very proficient in the game.
He had detailed analysis of fights.
He was talking about strategies in the arenas.
Surely this player was a combat game expert.
Well, I've personally learned working on a game like God of War that combat designers treat the harder difficulties like the magnum opus.
Like this is why they wake up in the morning, for the hard difficulties.
And I don't think I've ever seen a more disappointed look in anybody's face when they realized after looking at the telemetry that this player was playing on easy.
Give me a story.
So we learned a couple things from this.
The first is get critical features in front of playtesters as soon as possible, because our game was designed from the beginning to be able to switch difficulties through the middle of the game.
And this actually changed how people engaged with our game.
And we saw a huge amount of engagement with people and people using it once we put it in.
And for a long time we were only testing balanced experience, the kind of middle normal difficulty.
And this made us, it made it very difficult for the combat designers to action on any feedback we were getting that was saying it was too easy or too hard.
Because we weren't really sure what the skill level actually was.
And the second takeaway from this is that we needed to look at feedback from all sides.
Because we assumed from this player's feedback that they were playing on a harder difficulty because of how descriptive and analytical they were.
And so this reminded us to make sure that we cross-checked all of our feedback with the videos, the telemetry, the difficulty settings, and create this player profile to make sure that we painted the most accurate picture of these people that were playing our game as possible.
And finally, difficulty does not equal enjoyment.
And just because they were playing on easy doesn't mean that they still couldn't have a good time and enjoy the game systems and still provide us valuable feedback on them.
And so if I can sum everything up, and hopefully you kind of started to see the reoccurring pattern, is that we weren't playtesting to question our vision for the game.
We were using it to see how well we had realized it.
And this is an important mindset that we had to shift to.
Because reading through this feedback can be frustrating.
But know that you can get valuable feedback that will help you make your game better through playtesting.
You just have to be in the right mindset to receive this feedback.
And not everyone's gonna like the game, and that's okay.
And so we told ourselves from the beginning that we'd go all in, but it took time, and it took practice to get into a rhythm.
And we saw this after every test, that we got better and we got more efficient at it.
And we started seeing what data was valuable and what data wasn't.
And so now I'll close this out with some final thoughts that hopefully you can take back to your teams to help improving your play test process.
Now this is with the understanding that all teams are different.
And so while these work for us, you'll have to be the one to determine if it will work for you.
The first thing that we think is team-wide accessibility really helped.
We streamed the playtest live to the entire dev floor, which anyone could watch.
And this was valuable to us in a couple ways.
It was valuable to see the content being played from a dev perspective, so you could reference back to the videos.
And it encouraged people to ask a lot of questions, which then they would relay to the researchers or anybody who was at the test for the next day to look into more.
And the second was that it reinforced the idea that these play tests were for the team and that we encouraged people to make their own assessments on what was going on and to bring those ideas to their leads.
Second, it was reinforced through the leadership.
Corey was highly involved in the playtest process, and he strongly believed in it.
And you could almost always hear somebody on the dev floor talking about the feedback from the last playtest or the upcoming playtest.
And this all starts with the leadership being on board and all in on this process.
And third, the relationship with QA is really critical.
because you will hit problems with builds right before a test.
And you might integrate something that breaks something down the line somewhere else.
And that's just a part of making games.
But what helped us a lot was that QA was in these triage meetings with us.
They were at the play tests, and they were part of the process for preparing the builds.
Because their perspective is very valuable, and I can't stress enough the importance of that.
And fourth and last but not least, reduce barriers for user research.
And for us, just getting a user research team was a critical component to helping make the game better.
But it doesn't stop there.
The key was to ensure that we kept the researchers as up to date as possible, whether that meant sending them updated build notes, or there even several times where during a play test we would call them up on the phone.
Because the more context that they have about the game and the systems, the better that they can help you.
And so if there's one thing that you take from this talk, it's this.
Just keep doing it.
Because we had a very similar experience to what Lambert and Mattias described when we first jumped on a call with Guerrilla.
Because there were times that it was very painful.
And we did have to move things around in the schedule a little bit to help accommodate.
But it also significantly impacted the game in a very tangible and positive way.
So that concludes the presentation part and we'll be sticking around for any questions.
We're also doing a lot of talks here at GDC, in case you're wondering.
And we're hiring.
All right.
Question time.
Hello.
Oh, sweet.
In addition to the external playtests with testers that you brought in, did you have any internal playtests as well?
Yeah, so prior to the external playtest, we did do a lot of internal playtests within the team on a sort of grouped levels like combat for example would do their own internal playtests, level design did some specific feature teams like the boat or like the axe. We were doing some small internal playtests around that as well.
And then it's not that we completely stopped that, but we made the sort of bigger leap and bigger transition into doing more, you know, longer-term external playtests given that the game got so big and it took literally sort of five days just to get through the whole game.
So you sort of used the internal playtests for rapid iteration and then the external was more for the fine-tuning of the features near the end?
No, I'd say probably that at a specific point, like kind of how Ed describes that alpha, we sort of made the full transition to just doing external play tests.
Thank you.
Hi, very good talk, by the way.
I'm a bit curious about the playtest strike team that you talked about that's led by Corey Barlog and Yumi Yang.
I was curious if you could go into why that was necessary, what their roles were.
Was it enforcing the consistency that you talked about?
Was it kind of choosing what topics were tested over?
Like, why did you think it was necessary to create them?
Well, I mean, it kind of went into it a second.
It was important to have, like, you know, production and the people kind of on top of everything, like being that driving force, because, you know, kind of our problem was that it was just kind of happening all over the place.
So it was kind of important to make sure that, and even G, you know, being our gameplay director, it was important for him to be there as well, because it was one of those, like, you gotta have the big players in there in order to help make sure that these things happen, right?
Gotcha.
So there's, oh.
I don't know.
Can I hop in on that for a sec?
The other thing is, it was a new process, or they were undergoing a new process.
So having a strike team that could essentially fan out throughout the entire studio and connect the studio with the research group and to make sure that everyone was heard was super important.
So having an organized and broad approach to that is really key.
And I think for us, I guess, like to Ed's point, I mean, like, I was one of the engineering leads on God of War.
And so for us on the strike team, it was really important to have the other leadership as part of that process to decide, because again, like, because it's such a big game, everyone is running.
And in a lot of ways, everyone is kind of running with their piece.
And so you can have everyone kind of going in a different direction to some degree.
And then playtesting was kind of a way for us to unify and centralize.
Like, hey, we all need to run in that direction right now.
And a big part of this, I think, is also just making sure that relationships are key in this process.
Like, even with user research, but even internally in your own studio.
the leadership team has to have a very unified and strong sort of relationship with production, with all the other devs, and with user research, because you really want to make sure again that like everyone has each other's back, given that things are going to be really bumpy, and everyone's going to want to look at like why that didn't work, or you know what was the problem there. So I think the Strike team helped us kind of get everybody on the same page, making sure everyone knew these were the goals, these were the areas, and then we could.
to help us prioritize how we were going to get to the best possible build for the next playtest.
Thank you very, very much.
Hi. Thank you for being here. It was a good talk.
So just when I'm playtesting stuff, if one person points out a problem, I'm like, okay.
But if a second person points out a problem, that's when I'll consider it.
But with you guys, I'm assuming you guys probably get a lot more playtesters.
So I was just wondering how many people have to point out a problem before you consider it like an issue.
Yeah, it's not so much a matter of how many people, to be honest, it's a matter of kind of understanding where that problem is coming from and how relevant it is and how important it is to the overall experience of the game.
So in our cases, the larger group playtests were 20 people at a time.
And as you can see from some of the data up there, the fact that four people out of that 20 had an issue with the Valkyries was enough to be taking a look at that particular balance item.
But...
At the same time, the olive pizza guy, in a certain sense, sort of single-handedly brought that issue to the fore.
It wasn't that we hadn't seen that before from other people, but oftentimes you'll see a similar issue just spoken about in very different ways across a wide variety of people.
So it's the job of all of us to try to put those things together.
Okay, and one last thing.
In that data, there was one point about Kratos kicking the chain down, and he goes down automatically, and that was a problem.
And I've seen online that that bothered a lot of other people, so did you guys not consider that because it was one person, or just not change it?
I'll take that one.
It was brought up and again, like to Kevin's point, it's about looking at that sort of pain and then kind of prioritizing it across all of the other pain that we're also receiving and then seeing like, okay, is this going to...
significantly subtract the experience compared to everything else that we have to prioritize and get to, and what does that impact have? So I guess it is great to kind of assess that feedback, make it very clear that we understand it, and then, you know, do the estimation of like, what will it take to actually address it? Okay, thank you. Great talk, thank you. About the Freya scene.
I might come across as a super villain, but I would not have gone into explanation myself like, oh, you know, let's understand she lost someone.
I would just leave it to the audience and I want to know how much of your...
design and work rotates around the sensibilities and sensitivities of your audience.
And would you be comfortable looking back at trusting your audience to actually just go at it and just have it a talk at Discord or just, you know, I don't know, Steam chat or something like that?
Can I start that one?
Yeah.
So that's one of the biggest reasons.
You're touching on one of the biggest reasons why we do have 20 people come in for a play test at a time.
It's the practical limit for us in the first place, but why do we want a wide number of people?
It's not so much that we have then lots of numbers of people saying they have an issue, but that we get a broad.
spectrum of the kinds of people who are going to play the game, honestly.
And one thing we noticed, just as a sort of a...
This doesn't actually answer your question, but it's an observation that might kind of get at it.
I'll have a follow-up if...
So, one of the things we did was we decided to ask people a little questionnaire at the start about how much they knew about Norse mythology.
so that we could find out, you know, are these people who are experts in knowing who Freya is, for example, or who Baldr is, for example. And we right away found out that people don't know a lot about Norse mythology.
And it has a big impact on potentially how you're trying to communicate things and how people will perceive things.
So it's those elements of perception are really important, I think, and trying to gauge and meet people halfway in cases where they don't necessarily have the right perspective or a full perspective to get the impact you wanna try and drive.
So you added that as a stretch to basically help people have empathy to situations like these?
Yeah, I mean, yeah, to kind of follow up on what Cameron was saying, I think for us ultimately kind of, you know, you were touching on sort of the design process in relation to kind of having to either explain or not explain, but I think we're a very vision-driven studio, and so we fall back to sort of the creative director's vision on kind of...
what does he or she determine that we want to communicate or not communicate?
And when we posed this question to Corey, when he kind of looked at that feedback on like, hey, you know, people are starting to perceive Freya as a villain now, you know, it was really up to him and the narrative team to go like...
do we want to take this on?
Do we feel like it's necessary to explain this point?
And from his vision's mindset, he was like, no, I want to explain this.
And so we're going to double down on that.
So I think for us, it becomes about sort of what is the vision we want to communicate?
And in that instance, I think the way Ed describes it, it's we hadn't executed that yet.
We hadn't fully realized that part because for him, it was super important that people empathize with Freya in that moment.
So that's kind of how we find our way, I guess.
Hey, hi, thank you very much for the game, it's amazing.
First of all, yeah, and my question would be about how did you get, how did you make decisions based on all this feedback, maybe sometimes arguable or contradicting that you got?
So whose role was that, who kept, for instance, team focused on not making the combat when it was like so many ideas and opinions that we should?
So how did you make decisions like that?
I think that really was everyone's job and that's part of why it's so important to have that strike force, why it's important to get everyone involved.
From my standpoint, I mean, a big part of my job is, at least especially at the start of the process, to try to help the team understand that we're working for them, we're working with them in terms of research and we're not trying to...
change the game, we're trying to make them realize their, help them realize their vision.
So basically, that's what's driving a lot of the prioritization of different issues that you see.
Trying to weigh it against, kind of, where's the game right now?
What is it that the team cares about right now?
What kind of impact are we seeing it have on players?
All of those things are gonna go into the decision making on which things you tackle and which things you don't.
So it was like a group decision making, it wasn't one person.
Well, exactly.
Yeah, well, I mean, I'll give you a kind of quick example.
We get a lot of data, like Kevin mentioned, they'd help prioritize it, give us a big debrief.
We take that information, have a playtest strike team meeting where we look at all the data.
Corey would ask each one of the leads to look at the data that directly impacted their team and come and talk about it in a very direct way of like, why is this happening, what do you think the next step should be, confer with your teammates and then represent your team as a lead.
From that we would look at all of that and then assess what would the impact of this change actually have in the entire game and then ultimately it would be up to Corey to decide if we're going to act on it or not. I see, okay, thank you, got it.
Great talk, awesome game, really appreciate it.
So we have access to a usability lab and a research lab.
And so for Kevin, when do you think people should come to your lab?
Is it pre-alpha?
And I guess for Kevin, do you wish you guys started sooner versus where you're in alpha, and you're like, where's the fun?
So that's my first question.
And then the second question was a small thing.
It was that on the chart for the unguided for the combat, it was a negative four.
You pointed that out, but it was also a positive four on the plus side.
So it was an even split on the eight testers.
So how do you kind of divide in the middle on that?
So.
Okay, first question about when to start.
And yeah, as early as possible, but don't do it for no reason, of course.
It's expensive, so spend your dollars at the right places.
But that said, we have worked with people in the concept phase, and especially if you're coming up with a brand new set of mechanics, or you have things that you think are particularly risky.
So like a vertical slice would be sometimes even doable?
Before vertical slice even.
For example, I worked on Gears of War with that team.
I shouldn't say anything about them.
Particularly, we did work on elements of taking cover, for example, and other things that were really crucial and new to that game.
And a similar thing happened here.
I'm guessing that in your case, that it happened more as a matter of internal testing and internal processes, but there's no reason why we couldn't have probably benefited from having consumers take a look at some of those things at the right time.
Cool.
Second question I forgot.
It was the positive four and the minus four on combat for the unguided.
Oh, plus what happens if you get something that people like and hate?
Yeah, part of the strategy for asking those questions is that we expect that these things that we're asking about are so big and so important to the game that people can love them and hate them at the same time.
So people aren't actually saying, I don't like this or I do like this.
They're saying, they're potentially saying things that there are things about it that they do like and things about it that they don't like.
The key is just to figure out what those things are within that.
For example, in the enemies, let's say that they love the trolls and they hate the Valkyries.
I don't think that's true, actually.
But you see what I mean?
Yeah, I see what you're saying.
Yeah.
Cool.
Thanks.
Thank you.
