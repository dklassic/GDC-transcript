This was a difficult game to make, but we are happy to share the stuff that we think is cool.
We're pretty proud of the audio.
We worked on this for a long time, and we put a lot of thought and hard work into it.
So this isn't going to focus on the hard parts and the challenges of development.
We just want to share a handful of things that we think are cool that would be useful to other people making open world games, or maybe wanted to take a risk on some of the things in their game.
The audio for this game was done by a coalition of developers, people from the publisher, which is me, and freelance people working out of their houses or studios.
My name's Christopher Melroth.
I'm the head of audio for publishing, and I was the audio director for this game.
I'm joined by Finishing Move, the Bryans, who did the music for the game.
And we're going to walk you through a whole bunch of details, so thank you.
Next up.
I'm going to hand it over to the Bryans and they're going to do the first half of the talk all about the music.
So without further ado, the Bryans from Finishing Move.
Hey there, I'm Brian Trefon.
This is... I'm Brian White.
And so we're going to talk about the music of Crackdown 3.
So the topics we're going to cover is the composing strategies for a large open world, staying organized across tons of assets.
And then the benefits of composers implementing their own music, we implemented our score for it.
So, essentially, the task at hand with Crackdown 3...
It was scoring the island of New Providence.
That's where it takes place.
The New Providence is controlled by the evil Terra Nova Corporation.
They've got their hierarchy of corporate gangsters that are essentially the bosses that control various zones and assets throughout the island.
And so we had to figure out, how do we approach music for this?
So these are the challenges that we faced.
The first is, it's an open world.
You can kind of do.
Literally things in any order that you want.
You can enter in a boss battle and then run away.
So we had to figure out what is the music based around?
How do we make it support the gameplay?
What are we trying to tell with it?
The next challenge is interactivity, and it's to figure out how much interactivity do we want?
Where do we want it?
How much can we get, given the limitations we have?
And then third, we have to consider the musical style, which is super important.
And so it's, what does this crackdown world of the island of New Providence sound like?
And with that consideration, do we need some licensed music to support that, or are we going to do all original score?
So I'm going to dig deeper into the musical style that we did for it.
So I think the main thing that we're trying to achieve with the music was to just get that feeling of like, you're Terry Cruz, you're jumping around on the top of buildings, you're blasting people.
Like the visceral, the fun aspect of that, the futuristic sort of nature of the world.
So stylistically, we came up with what we called cinematic cyber trap.
That was sort of the combination of influences.
And so what that ends up meaning is it's basically we took a lot from Like modern hip hop, trap rap music, so like big 808s, lots of like attitude and swagger and vibe.
Some of the cinematic and larger than life nature that you get from film trailer music.
And then like we were inspired by a lot of the, like just highly stylized musical sound design, electronic music and bass music.
Like people like Noisia, Amon Tobin, Two Fingers, that sort of stuff.
So with that music and that style, the way we approached it, and Brian White's going to talk a lot more about this, we themed a lot of it around the bosses and.
and their different zones and things like that.
And we'll get into the details of that.
But we really wanted to capture the unique and distinct personalities and quirks of these different bosses.
And so the way we approached that with this cinematic Cyber Trap style was essentially for each boss, we would write a linear piece of music, like a three or four minute piece, that has all of their themes and just...
totally captures their vibe.
And then we digest those into the interactive assets later.
So it's like creating a sketch that just totally covers the vibe, has the feeling, the melodies associated with those characters that we can use throughout.
And then break that up into the interactive assets and create more stuff based on those sketches.
With the music, we decided to do an all original score, so there's no licensed music.
It's a lot of music.
I think it's close to four hours that's in there.
And a big part of why we did that was we wanted to achieve coherency throughout the musical world in the game.
So we wanted the music to sound like what you might hear like on the radio or in the club in the future.
So to be futuristic, but also to have the cinematic and narrative elements that you can only get with the score of like light motifs and character motifs and being able to have the music react or foreshadow things like you get in a film or a very cinematic sort of game.
And we also wanted to avoid some of the risks that can be involved.
with licensing music.
Like one of the issues that's happened in the past is, if a game came out 10 years ago, and they didn't secure the rights in perpetuity for a song, because they never thought it would be released on Xbox One or come out on Game Pass or something, they'd have to go back.
And that can be a lot of headache.
It could be expensive.
Or sometimes it can be just actually impossible.
So we wanted to mitigate that with just all commissioned original score.
That's what we did.
So we're going to show you a little montage example of this cinematic cyber trap sound.
This is with no sound effects or dialogue.
It's just cut to the boss battle so you can get the flavor of the visual style and the musical style and how that goes together.
So how did we get all those sick beats to work in an open world game where, like Brian was saying, you can basically do anything you want in any order. So you can basically start the game and go after the final boss and you could be in that fight and you could run away. So you load once into this huge open world map and that's it. You can kind of go where you want, do what you want.
or sort of not follow any prescribed thing at all and just throw cars around at people.
So how do we sort of take and use music to support the narrative of the game, support the story sort of of direct players towards things?
And what Brian had mentioned is we sort of decided that we'd theme things around these bosses, these corporate gang bosses.
And so I say it's all about that boss base.
And so each boss controls certain assets in the world, in the island of Terra Nova.
or New Providence, and the idea is.
These assets can represent certain missions that the player can take on.
And if you complete enough of these missions, let's say you destroy enough turrets or mine machinery, you can actually draw that boss out into a final boss fight to defeat that boss and sort of work your way through the different bosses in the game.
And like I said, you can do that in any order.
You can actually take on the captains.
before the lieutenants, and it's pretty crazy.
You'll probably lose, but you totally have agency towards that.
And the reason we wanted to do that, so music is not persistent, because a lot of people play Crackdown where they don't actually want to go after the missions all the time.
They just want to go run crazy in the world, collect orbs, level up, kind of just do their thing, kind of do stunts in the world.
So you're not hearing music all the time.
And so we wanted to make music special in that it either informs you that, hey, this is something important, this is a mission, you can take this on and sort of move through the narrative of the game.
We also wanted to use music to reward the player or essentially say who won or who lost.
Did you die or did you complete the mission?
Did you level up?
And you hear sort of a little musical.
reward. And so we have sort of this boss-themed music as well as agency-themed music for when you level up and accomplish things.
So I've got a little example here of how a mission, how you would encounter a mission from sort of no audio, no music in the world to encountering a mission and then hearing how that progresses and completes.
Enforcer operation you bust up loosens their grip on the island.
Dismantle enough low-level troops, you'll force their top-dog lieutenant to come out and play.
And that's when you drop her.
Then the whole Enforcer Brigade goes bye-bye.
Things are getting dusty in Nova's Chemical HQ.
For a man who calls himself a genius.
that you're in a mission area, then combat music kicks in, and then you get a sort of reward themed to that boss, and you hear some of their themes kick in when you complete that mission.
And the breakdown of this, like I said, there's nine bosses.
that you can encounter in the game.
And there's quite a few assets for each boss.
So we started, like Brian said, with sort of a sketch that was two to four minutes that sort of defined their swagger, their flavor, their themes.
That helped us get buy-in from...
Chris, you know, make him happy, stoked on what that sounds like, and then we'd go and make almost like 20-25 minutes of music sort of abstracted from that as a palette for those bosses. So the ambient loops, there's combat loops, there's three sets of combat loops, there's low health, death, down but not out, loops and stingers.
There's also a whole bunch of just notification one-shots.
So it's not looping music.
It'll be things like when you enter a boss's area for the first time, you'll get kind of like a little thematic 10-second stinger.
When you see one of their assets show up in the world, you'll get a little eyes-on stinger that's sort of themed towards that boss.
So there's all these little sort of leitmotif moments where you're hearing the.
bosses theme kind of trickle in, and those all sort of culminate in this final boss battle.
So when you draw the boss out, you complete enough missions to draw the boss out.
That's when you get these sort of scripted boss battle moments where it's it's it's loops sort of train card, and we're sandwiching those between actual scored to linear runtime sequences.
And I have a little example of that.
And I'll annotate on the screen when it's actually scored to picture to the runtime sequence versus it's in the interactive music system and running the state system.
Oh, this place is seriously messed up.
Well, well, it has come to this. I can take the pressure. Question is, can you?
Gate is trying to redirect the pressure to release valves across the plant.
We need to hit those valves and purge the whole system.
Pressure we sent down the line should have opened up the valves, but they're locked tight.
All units, defend those control panels with your lives. Do not let the agent deactivate them.
That's it, agent. Hack that panel to free up the piston and blow that valve.
You need to hack the control terminal and expose the next valve, agent.
That's one down.
Re-routing pressure to auxiliary outlet.
Protect those valves, you fool!
Looks like another valve is vulnerable.
Pack the panels. Take it out.
I'll make this simple.
Kill the agents, or everybody dies.
You have no idea what you are doing!
We know exactly what we're doing. We're flashing you, you piece of shit!
Let off some steam, game duck.
The Kaida and the chemical plant blown to smithereens?
Terra Nova's industrial division is on the ropes.
The pipelines sending Chimera to Vargas have been cut off at the source.
It'll be a lot easier to reach her, Agent.
So the boss battle is really that sort of, when you get the full enchilada, you get the full beat and it's this really cool visual thing.
You actually hear it all kick in and you've been hearing these themes kind of throughout and that's kind of your reward as you're fighting this boss, this like super lit trap music.
There's some additional interactive systems I'll just go through really quickly.
There's some things called reinforcements based on a hate meter.
So if you're just causing trouble in the world, the captains can come after you.
And there's a musical system that sort of says like, oh, okay.
These guys are coming at you.
The kingpin, who is the lead boss, she can actually lock down the whole city.
So that's like sort of like a big retaliation reinforcement from her.
And those, you either win them or you die to get rid of those.
And then we've got some side missions in the game, propaganda towers, and these different races.
And those each have their own little interactive systems to support those.
Briefly talk about some of the diegetic world music.
So we created a whole bunch of stuff for, there's lots of businesses in the games and night clubs and things like that, and so we wanted a way to just, one, just fill out the world, and also to add a little bit of humor and just more color to the environment.
There's a bunch of these businesses that have holographic advertisements, and so we made non-looping jingles for them.
And then for nightclubs, and there's boom boxes that you can pick up throughout the world, we made cool, looping pieces of music.
We did over 50 unique little jingles and pieces for the diegetic music.
And by the way, in case of diegetic, that means it's coming out of something in the world.
So it's coming out of a speaker or a holographic advertisement.
So it's funny because our background is actually started in commercials.
So we were able to flex our muscle doing that.
So I'll show you an example.
This is for the Cash Cow loans business.
And in the actual game, it would be futzed.
But for just the sake of this example, this is just going to be just clear.
So that is.
So just, you know, low rent television commercial just to add like some more humor and color to the world.
So next, Brian White's going to talk about how we stayed organized across all of the tons of assets.
Yeah, so this game was like almost five years in development and we did four hours of music.
So just like lots of assets and then lots of time.
putting that together, design changes, and that kind of stuff.
So we really had to kind of stay organized.
And we used some typical things like spreadsheets.
But really, the most interesting thing I want to show you guys is something that Chris actually developed.
And it's these Visio.
flowcharts that really sort of take all the interactivity and they put it into this really nice visual so that everybody from us who are doing the implementation and writing the music to the coders, to even the designers, the level designers, they can sort of understand and either comment or know how the music system is supposed to work.
And this actually, These things got sent in emails numerous times.
This is how this is supposed to work.
But actually, they're super effective.
It would have been very difficult to just dive in and create a music system from scratch in Wwise without looking at these.
Yeah, oh, I just wanted to make sure that was on.
Wwise doesn't have a good way to visualize your interactive system, it's a bunch of menus and lists.
And so, for our brains to be wrapped around how this was all gonna come together, Visio is a great tool, there's other tools like that that I get, Visio for freaks, I work at Microsoft.
So, I use that one, and I wish that the interactive music systems would actually use a visual scripting system like this so that we could see how the music's going through as opposed to these.
sort of dumb lists that are impossible to debug once they're built.
But we debugged the music system before we built it this way, because I had to be completely honest about how many cues I was requesting.
So this is the entire interactive music system in the fully zoomed out view.
And then, so what we did is we mirrored this with an actual linear spreadsheet.
Oh, sorry, can you go back?
Anywhere it says campaign gameplay, no score is playing.
So instead of having like one big block where everything comes off of that, it's you're just basically understanding campaign gameplay is just a simulation of the world.
And then you can get in and out of any state by following the flow graph.
And this really helped with the transition matrix because there's like, you can literally go anywhere to anywhere.
So our transition list in Wwise is, it's a bit insane.
It takes a long time to load the interactive music system.
This is just a linear list so that we could sort of have a list of everything and then make notes on that and sort of we check that in and out of Perforce so that we could make either design notes or the coders could say how they wired it up or we could just keep track of how many minutes we were writing for the different cues and transitions.
And then this is just a quick example of just some wise organization.
We're very, very organized with such a big game.
And we had very specific naming conventions on everything that sort of lasted throughout the development.
There's even typos that Chris made that we just kept those naming conventions so that everything stayed true all the way to ship.
And I really like to use a lot of folders and all the labeling capabilities that Wwise has.
It's really useful in the interactive music system, especially for transitions, like transitions to and from folders, instead of just like having to make one for every individual asset. And naming your transitions, because we had, we probably had at least a few hundred, maybe like 500 plus transitions in there that needed to be accounted for.
And then finally, I'll just talk really briefly about composers as implementers.
So we built the whole music system in Wwise, so it wasn't a system that already existed that we just threw assets into.
I actually took those design documents and I built the entire system myself.
I knew exactly how it was supposed to work and how everything wired up, how the states worked, how the stingers worked, how the transitions worked. And really the benefit of this is just being able to provide instant feedback that instructs the composition process. So we could iterate ourselves just by testing either in SoundCaster or testing in a build to know like yep that's not going to work or that transition needs to be twice as long.
It needs to be half as long, you know, just those basics.
And I think that's, you hear that a lot, like, oh yeah, you know, composers should implement because then they can make better music.
And I think the biggest one that we ended up figuring out late in the development cycle was just being able to own the polish and find and fix bugs a lot faster.
So in the last two months of the game before ship, I was playing every day, probably three, four hours a day, just playing the build and then back and wise.
And I was catching bugs that testing was not catching, nobody else on the sound team was catching, just because I knew exactly how all the specific little systems worked.
And then I was able to go in and fix those bugs, or in the event where there might not be code support, just come up with creative solutions for making it work.
You know, like, oh, I can tweak this, I can make this, and we can make it work with what we have.
And I think that is pretty difficult if you're also relying on a sound designer who's implementing the music, and they've got a million things.
It was amazing for me as the audio director, because at the end I'm just chasing a million bugs, and especially perf on this game was a real problem.
And Brian White, like, just, hey, I fixed this today.
Okay. Sounds awesome, buddy.
It was far too big a game to play through on a daily basis.
Yeah, so and then just a couple of the challenges with that.
We were working on this coast and there was also teams in the UK.
So there's just your typical, you know, sync time differences.
Like I'd lose Perforce access in the middle of the day and they'd all be sleeping.
So.
Just those kind of things and keeping up with those changes.
But in the end, it was actually super valuable.
And it wasn't that hard to embed in the development process and really add a lot of value to the team.
You want to do this last one?
So we're going to hand it over to Chris.
Just the key takeaways, just in terms of composing for an open world, you have to have a unique approach and think about what is the music based around.
With, obviously, the organization, as Brian showed, it's like having different ways of communicating that information beyond just a single spreadsheet really helps keep everybody across teams on the same page.
You know, the composer's implementing their music, then you can react to things very fast and just get it going.
So now we're going to hand things over to Chris for his portion of the talk.
All right, time check.
I think you guys are right on schedule.
Yeah, you got 30 minutes.
Hang on.
All right.
I got 80 slides to do in 30 minutes.
I'm going to not micro-machines it, but I'm going to gloss over some of the details that are up here.
And you can always get the slide deck later and dig into the numbers if you would like to see more details.
All right.
I'm going to focus on some sound design systems and some mix stuff that we did.
When we get to the destruction section, James Nixon and Dan Krizliff right here in the front row actually made all these slides and they did all the hard work on this and all I had to do was copy and paste them in here and make it sound dumber for myself to present.
So thank you guys and they did an amazing job on this game.
We wanted kind of cool acoustics, and we needed to do it super cheap and super fast.
So I'm going to share how we did that, because I think it sounds all right.
If you don't know what Crackdown is, there's two modes.
There's campaign and multiplayer.
The multiplayer hosts 10 players in a fully destructible world.
You can take down entire buildings piece by piece, like literally piece by piece.
So in order to play the physics sounds for all that, it was an incredible technical challenge.
So I'm going to share how we do that.
We have an individual system that is also part of destruction, but turbulence.
So it's something that I personally have always wanted to get into a game, because I hate it when a physics object is flying through the air and it's just silent.
I want that movie whoosh as it goes by.
So I'm going to share how we did that.
Just a real quick short thing on LFE, because as being in the publisher side, we work on a bunch of different games, and I see a lot of inconsistency about how people treat LFE, and I'm a huge fan of LFE.
I'm going to share at least how we thought about LFE on this game.
And then I'll show you some, I'm going to try to get to the mix stuff as fast as possible because there's a lot of detail there.
And mix is on everyone's mind.
All right.
So the acoustic system.
I did some sessions with a blind gamer named Sightless Combat, and he is incredible.
He's an incredible resource.
And like, this game shouldn't be playable by someone blind, but I took it as a challenge of anything that would be good for Ben to be able to play this game would be great for everyone.
I mean, if he doesn't get a piece of audio information to understand where he is, where the bad guy is, what's happening.
then it's actually a failure on the audio director's part to not identify that for all users.
So he was generous, and he gave me some of his time when he was in Redmond.
And he played the game with headphones on.
And I would just ask him questions.
And one of the first things that came up was his ability to navigate the environment was pretty rough.
Because really, he just had to listen for either ambience or bad guys or traffic or something to try to get his orientation.
A big chunk of the game is platforming.
So the ability to know where a building is or some sort of traversable object was important.
And the best way to do that is echolocation.
So echolocation is really hard.
It's a complex physical interaction.
You've got a wave that propagates out into environment, bounces around, scatters, and comes back to you.
And your perception of that delay, and the loudness, and the texture of that sound that comes back is acoustics.
The whole game takes place in an urban environment, so we didn't have to worry about forests or caves or really anything like that.
It's pretty much all city street and rooftops.
It needed to be fully dynamic because I didn't want to build two systems.
I just wanted to build one system and have it work for both single-player and multiplayer.
And in multiplayer, you can blast through walls and take down buildings.
So the building that used to be there is no longer there.
So we couldn't pre-process the environment and make some sort of detail map of the acoustic layout.
We had to do something that was fully dynamic.
We didn't have any time to make a custom plugin for early reflections.
And we started on this right before Wise Reflect came out. So we basically built Wise Reflect, but inside of Wise without Wise Reflect.
It's an awesome plugin and that actually does more than what we were able to do, but for how cheap CPU-wise our system was, I think that it sounds pretty good.
We didn't have any time to do manual markup.
The city's too big.
So we couldn't go in and draw zones or make low-res structures that mirrored the game's geometry.
And then also, acoustics are just noise.
Reverb is noise.
And so the more of that you want to feature, the more noise you're adding to your game.
And there was a whole insane design ethos that I pushed on everyone.
No noise, no distortion.
This has got to be tight, clean, punchy, and I want to hear tone, and I want to hear power, but without using distortion and noise, which is, as we all know, it's impossible.
But the choice to push that as hard as we could meant that the game sounds nice and clean and I didn't want to muddy that up.
So when you decide to use distortion or noise, it's intentional.
You're not just using it on everything and then trying to figure out how to get the noise and distortion out of your game once it's all baked in.
So if we're going to add acoustics into the game.
We didn't want to undo all that hard work that we had done.
Cool.
All right, so the technology that we employ to get the data back and make a sense of the acoustics are pretty simple.
We ray cast in five directions.
We do five rays per frame.
That's it.
We do them on the corners, and we do one overhead.
And it's all listener relative, so it comes out from the listener, which in our game is the camera, because I think camera is the best listener.
I don't like to fudge with it and like move it in between or put it on a spline or put it on a character.
I just like camera because what I see is what I hear.
We then take the data output by the rays and we do two things with it.
We turn that data into RTBC parameters that we use to manipulate all the different sends and do some other trick stuff with.
But that data also under the hood determines how we interpolate between the delays.
So we go out to 50 meters, because that's kind of what you couldn't really afford more.
Every game-defined send and wise that you add is extra overhead per voice.
And given the scale of this game and how many bad guys we can throw at you and stuff, we're often running 900, 1,500 sends with this system.
So if you even just add one more, you're multiplying out, and it gets crazy.
So I would have loved to have gone out to 250 meters so that we could have done procedural echoes and everything.
But.
50 meters was like a good compromise.
Then we take both the actor mixers and the output of the game defined sends and we route those through some reverbs.
One of which is close and one of which is distant.
Both of those are using impulse responses that were recorded in city streets.
And I'll show you how we do that stuff.
So I'm gonna skip that.
I think you can barely see it cause it's a single pixel that's moving around.
A single ray cast is far too rigid, so what we do is we jitter them, and we take the output of the jitter and we average that over time.
Then I take the output of that and I also smooth that in Wwise just because it was still a little too twitchy.
And we stop ray casting at 50 meters, beyond that point where we don't have any delays to go to, so it's a slight optimization.
All right, so the RTPCs that we create are each direction.
We measure the distance between the ray from the listener to the object that it's striking.
We use that for attenuating the delays.
So we're interpolating to different delay lines so that farther away there's a longer delay before the slap comes back, and different longer time between the slaps.
And then we use that distance to attenuate it as though it's getting quieter because there's no object out there.
It's not actually in 3D.
It's just sending to one that's hard panned.
a bus that's hard panned here, here, in each direction, matching the orientation of the rays.
We also get, in order to further reduce noise, reverb sounds pretty bad if you have all these reflections and all this reverb, and then you're folding it down to stereo.
So we detect the endpoint that the person's listening on, and if they don't have those speakers, we just mute those and the reverb.
So I mute the bus.
Like, if there's no rears.
Rear reflections, gone.
Rear reverb, gone.
Overhead, same thing.
And that way, so you're not folding down reflections of things that you don't have speakers for, back and doubling them up.
You might even hear in a few of these captures, I forgot to toggle back to stereo because the video capture actually folds down internally.
And it does a terrible job at it.
So if you hear any weirdness with some flangey stuff, it's actually the delay is getting folded down, I can speak.
All right, and then we just take the rudimentary values that we're getting back from all those rays, and we create a super low-res version of the volume of the space that you're in.
And that's not meant for any detail other than big space or small space.
And between those ranges, up to a million cubic meters, we can determine how much we want to send to the close reverb and how much we want to send to the far reverb.
The reverbs are just stock-wise convolutional reverbs.
And then these are some of the parameters that we use to control them and shape them.
So if you're in a smaller space, it's got less spread and it's a little bit hyped in the loudness.
And as you go into a larger space, it opens up and it gets into the wider space that comes up in volume and the close one comes down.
Super simple, like nothing special about it, just trying to solve problems and be clever with what we had.
All right, so I've got a series of videos to show this off.
Hopefully it reads in here.
It's a big echoey room.
But this is the dry signal of the carbine in a test level.
All right, next one is early reflection soloed.
So if you hear something, that means that there's a reflection coming off that geometry.
Super simple.
It is filtered just a little bit, because we don't want to send so much low end and we don't want to send so much high end, so it's bandpassed.
And it gets more bandpassy the farther away it is from you.
This is only the distant reverb.
So as you can see, as I get away from that geometry, the space volume increases.
And that was hard to hear.
All right, and then this is just the acoustics, not the dry signal.
So up to this point now, we've got everything playing together, and hey, it sounds kind of city-like.
We didn't want to overhype the acoustics.
You know, we're not doing like battlefield-style gun tails.
This game is all sound design forward, so the sound design of the gun itself is key.
But we still wanted to glue everything together and give you that sense of being in an urban environment.
So this is with everything including the dry signal.
That's it. That's our acoustics.
The side benefit of spending some time on early reflections in that urban environment is that it sounds pretty good on voices.
It also helps you judge distance.
Just like a lot of people like that, the battlefield talk, they did everything like most of their system was player-centric.
Same with ours. Our AI is really stupid.
It doesn't really know much.
So what we would do is we would pass a token when you do something to the AI system and say like, hey, player has fired a gun, so someone should scream and that kind of stuff.
It's better than actually having the AI try to know what's going on.
That's expensive.
There's also propaganda, so this horrible corporation that is totally not Microsoft, this horrible corporation is in control of the whole island, and they blast you with propaganda, these huge holographic Blade Runner-style propagandas, and it sounds pretty good on that too.
A vision of brighter tomorrows.
A vision powered by human potential.
Together, we will forge a bold new future.
Together, we are Terra Nova.
All right.
So that's our acoustics in total.
All right, cloud destruction.
This is the hardest stuff that we had to tackle.
I mean, it really took, honest to God, it took a technological miracle for audio folks to figure out how to do this.
All of the physics for the multiplayer sessions are calculated on a server.
There is no data for any of that destruction.
None of the physics interactions, anything, exist on your game client.
So we rely on all those values to determine, hey, a piece of concrete was flying this fast and hit this other thing, and I need to play a sound for all of that.
And they're like, cool, well, I can give you the position of it, which doesn't help.
Technologically, it's quite difficult and sophisticated.
I'll just do the simple version, because that's how I understand it.
There's a cloud server that all the players connect to.
It's hosting an instance of the game where all of the physics are being done.
All of that data is then sent down to your local server.
Your local server speaks to your local client.
Those are both on the same machine.
And that then is rendered.
So here's basically what it looks like.
with no audio on.
This is the level of mayhem and chaos that we have to try to make sound decent and sound cool.
And hopefully, if we do our jobs, not fake, because it actually is all real physics.
All right, so just real quick on some of the definitions of how all this stuff works.
We've got the cloud server, we've got local server.
Some of the things are CPU processed and some of the things are only on the GPU.
Client-side prediction or CSP makes it so that things, you know, if you drop a packet or something, that something doesn't just stop and then update over here.
It'll smoothly interpolate.
And if it has to rewind and fix itself, it's less noticeable than like not updating per frame.
Then we've got some things that some really awesome, I, you know, it's really cool that they figure out how to do things only on the GPU.
Unfortunately, there is no data for us to tie into.
So we had to write all these new systems to, get, you know, basically query what's happening on the GPU and pass that to the CPU so we can do something with it.
There's a whole bunch of behaviors and that we need to cover for each of these pieces, you know, when the whole building's blowing up.
And then there's a tremendous scale, tens of thousands of objects are updating across the network.
They all have a different state, they have a position and orientation, and there's a huge range from, I think there's even smaller than 50 kilograms now, but 50 kilograms to like a billion or whatever is the max.
It's just.
we need to make its hard bank concrete sound huge.
So we have to do that all in real time.
All right, so there's physics chunks, GPU chunks, static geometry, which is like the ground that can't be destroyed.
And then we've got impacts, destroys, and we've got a value of impulse.
We do it all because of the nature of how the team is so distributed.
We try to do as much as possible and wise.
So.
Every physics sound in the entire game for impact is a single event call.
It's play physics impact.
And everything we do underneath that is driven by data that we query from the game and set a parameter to drive a switch.
So we hate the switch view.
Like with a fiery passion, we wish that Wwise would fix it because it's terrible.
It's extremely powerful.
And you can do a lot with it.
But actually dragging things and having to reorder things and the fact that you can't sort to actually see what you're doing is like nightmare inducing.
But the cool thing is you can do layers, and it'll make stuff on the fly for you.
It's got a lot of functionality built into it.
So we nest the switches.
So the first thing we do is the material, and then we do the mass.
So it'd be like physics impact is the action.
Then we do carbon dark as the material.
And then we do the size, carbon dark huge.
And then we go through all the impulse levels under that switch.
And then we define what sounds can play at each one of those.
So that as you can imagine, our switches, I should have done a zoomed out view, but our switch view is enormous.
Fortunately, adding materials is extremely expensive for every department, so they kept the number of materials down to a reasonable amount because there were so many actions and so many mass ranges that we had to cover.
Because we have to make actual sounds to represent the stuff we can't physically model or procedurally generate it, we have tried that in the past.
It just doesn't sound as good as a sound designer putting their creativity in and making it sound the way you want it to sound as opposed to how it should physically sound, like physically correct.
We take ranges of mass and we assign them to a single sound.
So there's tiny, small, medium, large, huge, and then we even added enormous because we're running out of words to describe the mass range.
But it's better than like zero, one, two, three, four, five.
So by taking a range of masses and saying, this is what we say is a medium sound, we know what sound to make for that size.
Then within the range, the min and max range of that mass, we track another parameter, which is the percentage, and we can use that to sort of pitch it up or down slightly to give it a little more gradient.
So it's not just like, you know, some of the ranges are actually quite large.
So being able to pitch it a little bit and give it a little more variety is much appreciated.
So we do the same thing with impulse.
There's a range.
We have to bucket it, because we have to make a finite amount of content.
Then we've got a bunch of behaviors.
It can be damage destroyed, it can move through the air, it can scrape on the ground, it can hit stuff, and then they clean it up, and we call that a de-res, just like Tron, because it looks like Tron.
I'm gonna skip all this stuff, because you know what ballistic damage is.
This is, so destroyed is not when it's actually deleted, that's a derez, destroyed is when any physics chunk breaks into two or more smaller chunks.
And so it's basically, if you can think of it as something fracturing, it's just called a destroy.
Turbulence is cool and I'm gonna get into more detail on that, but basically as Havoc bodies are moving through the air, you wanna hear a moosh.
And we do full contact sets for every single mass range and speed you can have.
An impact is if a physics chunk collides with either a physics chunk or static geometry, then play a sound.
That sound, that system is designed to be two hands clapping, so I've got this thing hit something and this thing was hit by something.
Now you get a sound.
That sound is not a single sound that we say like, oh, it's concrete versus glass at this speed and everything.
It's the sound of concrete hitting something and the sound of glass being hit by something.
And we play them both at the same place, same time.
So it's actually two.
Both objects register their sound that they should play.
Because it's same play, same time, you get an aggregate sound that sounds correct.
We have a bunch of different layers.
That's so we can get perspective on the sound and tune it differently.
Impact, resonance, which would be like if it's a piece of metal, it's the ringing of the metal.
Sweetener would be like, hey, it's a piece of concrete and some chunks fly off, so we want to get a little bit of dust and grit and debris on there.
And then LFE, we do for sizes above medium.
Medium and above?
Yeah.
And the huge ones we really just kick your room with a lot of air.
That's awesome.
If you have a 5.1 system, this is a great game to showcase it with, because we went crazy with LFE.
And LFE's fun.
Alright, scrape is...
So if a turbulence is in motion and not colliding with something, then a scrape is it's in motion and colliding with something.
Pretty simple.
Conditions.
It's really hard to tune.
That was actually our most problematic system, because...
on the server, getting data back from the server to say, hey, I've done like a bunch of collisions in a very small window.
Should I play a scrape?
Is still our most problematic system.
And it's just OK.
But it's kind of nice when something is hitting and sort of rotating and falling that you get the impact.
And then it's got a little bit of a scrape until it falls.
It's cool.
All right, so this is an example of, with all that working, just destruction audio.
So no weapons and no building level stuff.
The game designers were super nice and they decided that when anything should break, they should just play an explosion, which was really nice for audio.
And so that's why you're seeing so many explosions.
So that's just destruction audio only.
Here's with everything, weapons and everything all in one.
And this is all test level stuff.
In a normal match, there's 10 players all trying to kill each other and shooting stuff.
So the audio director in me gets obsessed over the details, and I'm like, there's stuff I didn't hear.
You have to kind of go, yeah, but.
You're never going to actually hear that that isn't playing, that one little thing that rotated and fell.
So we did try to get all that stuff.
Just sometimes we have to make some hard decisions about what's most important.
I'll get to that.
Oh, and then we just play it.
We use the server cleanup as a sound design opportunity.
All right, so buildings.
So those are all the destruction chunks.
That's all the little pieces.
That was probably the hardest thing to get.
Then we can query the state of a building.
A building is a collection of pieces fused together through constrained joints.
And once you damage it enough, it will enter into different states.
Those states are below.
We have sound designs for each one of those states.
Here's sort of the definitions of how those building states work.
It was really simple.
We were like, hey, at 33% and 66%, can we just start triggering some sounds?
And basically, where there's constraints, we can start to play creaks, and we randomize where they are.
So when you're in a building, you can tell that it's starting to take damage.
So that you don't hang out in there too long and get crushed to death is the real idea.
So there's critical.
The server can handle one building collapsing in its entirety at one point in time.
So if two buildings have been damaged enough to come down, one of the second one is queued.
And so it enters into a special state, says like, dude, I'm totally going to come down as soon as the other one's done.
And we have a system for that.
And then we have a building collapse.
So here's just building destruction.
All the physics sounds are muted in this one.
Cool, so we can't really cheat in this game, because you can go all over the place.
Like, we don't constrain the player at all, especially in multiplayer.
So we actually had to figure out a way to get all the sounds positioned on the structures and do everything correctly.
When the server reports that a building is collapsing, we have that shepherd tone that keeps coming in.
So it always feels like it's falling.
And then when the server says, hey, you've stopped, that's when you hear it fade out.
Just a simple little trick.
We used a lot of shepherd tone in.
in Crackdown 3.
Cool.
And then this is a full simulation, just a whole building coming down.
So not perfect, but I don't know, pretty awesome considering the technical challenges to get there.
And then the last part, I'll blast through this, performance tuning was super hard. So basically with tens of thousands of things happening, the first, when we first started this, we were like, can you just send everything to us and we'll work it out on the client? And they were like, sure, we'll turn that, we'll turn on audio and pass the parameters that you asked for.
And we completely took down the server. And so then they were like, man, you guys took down the server. You don't get any data.
And we were like, well, we need something.
So between some and none, or all and none, we needed to figure out what was the right amount of stuff to hit server performance and also give us enough so that we can do something convincing for the player.
So we do a whole bunch of relevance checks.
I'm gonna have to go through that quickly because I talk too long.
But the way that it's done is we take everything, And we look at, we start bucketing things, and we say, in this bucket you can have this many, and here's how we determine what's most important, and then it can graduate out, and we filter, filter, filter, filter, and down to what is most important to send to the player, because server bandwidth is precious in a multiplayer game.
Once it gets to the server, so one, oh, sorry, one of the ways that we do that is we merge collisions as they get farther away from you.
So you need a lot of precision, like I'm standing right there in one little chunk column, you know, falls and hits the ground.
I wanna hear it like right then, I don't wanna wait.
The more you merge, the more latency you're introducing to the sound, because the server has to accumulate a certain number of impacts and then say, hey, here's a merged impact for all this stuff.
So as it gets farther away, we increase the radius on those.
That's a really nice optimization.
We got a ton back from that.
And we also bias that towards the view frustum of the player.
which has helped a lot.
So if you, basically it was like, when it was just a sphere around the player, things behind me were totally taking precedence because they were huge or fast or something over the stuff that I wanted to see.
But that all had to be, all that logic had to be done on the server.
So on the client side, we do all that whole process again because even the stuff that can come to the client can swamp wise.
On the Y side, we do that again, but for voices.
So we still like, all right, only this can come from the server, only this can get to Y's, and then once Y's is there, it's like, I can't really play back all that stuff, so I'm gonna do my, you know, the most important stuff of what's left, I'm gonna play sounds for those.
And we aggressively limit and throw out sounds that we don't want.
We have a priority system for every single voice, and so it's a gradient between how heavy it is and how much impulse it is and what material it is.
If you want to dig into this later, these are the actual numbers we use for how we limit how many instances come through.
And then this is just to show you how extreme it can get.
So this is a building with all the physics stuff simulating and the player still interacting with it, like still smashing it up as it's collapsing.
All right, and you can do that at 30 Hertz on an Xbox One, which is the miracle part.
So it's pretty cool.
All right, so turbulence, if something's moving and it's not colliding with anything, it's turbulating.
This has a lot of physics in it.
Bad guys throw physics objects at you as weapons.
You can throw stuff.
You can blow things up into smaller parts, and they fly all over.
I'm going to quickly get to some examples.
So real quick, this is the scale of which every single little section you see here has props in it.
There was a million billion.
This is an example of the spreadsheet that James managed to track it.
I'm going to do the zoom out.
Yes, there's that.
And then, oh God, why did they make so many props?
And for most of those, we made unique content.
It was 700.
Yeah, somewhere around 700 unique props that had a full set of everything from impacts to turbulence.
Pretty cool.
And then that's outside of the destruction materials and all that kind of stuff.
And we dynamically load those banks, that's, you can imagine.
So here's what you get with turbulence.
Pretty cool.
Guys like Ben, Sightless Combat, can now tell when physics objects are flying around.
Here's throwing a pipe.
Here's a really cool one that shows off how we take the rotational velocity of an object and we modulate the sound.
Damn that last tip over, I wish we, I wish, the tip overs are the hardest cause it's in contact with something and so Havoc doesn't give us the right data.
And then, oh, this is probably the coolest sounding one, it's a piece of sheet metal.
And as you throw it, you throw it like a discus and so it spins and this shows off what the rotational velocity is about.
Pretty cool, it's super simple, it's actually not expensive, and we have all that content now, so it's good, because other people can use it.
This just shows that we also do it for destruction objects.
I'm out of time?
Yeah.
Okay, can I go five minutes over?
Okay, I wanna get to our mix stuff.
I'm gonna cheat.
LFE, we play some LFEs.
All right, so the mix stuff, we wanted, player feedback is essential, but we wanted the music to be nice and up in the mix.
That creates contentions.
You've also got narrators, as you heard, they just talk all the time.
We need to make sure that you hear everything they say.
And then we just wanted to reduce cacophony noise.
We use Weiss HDR.
I know some people are saying they don't like it.
I love it.
This game would not have been possible without it.
But it's not easy. It's really hard. You can't just grab a fader and be like, I want this to be quiet. You have to do all the math in your head and think of it in three dimensions.
It's horrible. But the good news is once you get it right, the game just stays clean sounding.
We do frequency domain processing, much like the Just Cause guys, where depending on what's happening, we'll carve out some frequencies.
So if you can think of, hey, I don't wanna duck the music when someone talks super, you know, like six to nine dB, like you would need to when you have the music nice and up in the mix, but it competes with the melody.
So I can bring the melody down, leave the perk on the bass, because there's no frequency contention there.
And then all I need to do is on the.
percussion, just really the hi-hat is competing with the audibility, like the sibilance, so I just use a notch.
Like I use the meter on the output of the speech to pull these things down.
So if we did our magic right, you wouldn't notice.
You can notice a little bit, but it's not as bad as some games I've worked on where we've had to just dump the music so you can hear the voice.
We also do some category to category things, like hey, if I'm shooting a gun and an explosion goes off, the gun can cut the lows on the.
The explosion can cut the lows on the gun, and the guns can cut the highs on the explosion.
Super simple, and they just trade off.
And the meters are really fast.
We own fast meters on those, peak meters, and they trade off very nicely.
HDR is great for sandboxes with chaos.
Every single sound in the game is 3D.
It was a decision that we made because I think 3D sounds are awesome, and we wanted to make something that could be a showcase for spatial audio, which we're still wrestling with some bugs on.
So if you play it today, you won't get that.
It's coming.
Yes, we use meters too.
Here's some information you can dig into later about just some of the PEQs and ducking that we do.
Performance was horrible.
You're not the only ones.
That's why this slide is here.
We aggressively throw sounds out.
If they're at max distance, they're a zero priority.
We don't care about them.
We just care about the stuff that you should hear.
And we limit all of our sounds to 50 max.
This game, with all the things that can happen, you only ever hear 50 maximum sounds.
And the way that we do that is through HDR for audibility and then priorities.
Believe me, you can't tell that we only have 50 voices playing.
It's crazy.
I was inspired, I was challenged by Bungie because not challenged by them, but challenged in knowing that they limit their voices to somewhere in the 40s for Destiny.
I'm like, how?
I played that game.
It's impossible.
So I just want to thank everyone.
If anyone here worked on Nimbus, that was the code name, or our friends overseas that worked very hard on this game, I just wanted to thank you.
It was a long journey, and there was a lot of dedication from the audio team on this.
I love you guys.
Love you all.
Thank you.
Thanks.
Thank you.
