Hello, everyone, and welcome to our talk.
The topic we will be presenting today is Simulating Tropical Weather in Far Cry 6.
Before we begin, let me introduce myself.
My name is Emily Zhou, and I have been a technical artist at Ubisoft Montreal for the past four years.
Colin will introduce himself a little later when he covers the second half of our presentation.
The game that our talk is centered around is Far Cry 6, the latest installment of the Far Cry franchise.
If you're not familiar with Far Cry, it is an open world first-person shooter series where each title takes place in a new environment.
In this case, we introduced the players to Yara, a fictional Caribbean island that was inspired by Cuba and other countries.
Here, players will take on the role of Danny Rojas, a local rebel fighting to topple the regime of dictator Anton Castillo and his son, Diego.
Far Cry's dynamic open worlds are a shining trait of the franchise.
Each world has a life of its own, with many systems constantly interacting with one another.
Now, to add a tropical island experience to the mix, we needed something that would be new to Far Cry, a complete dynamic weather system.
To give you an idea of what dynamic weather entails, here's a quick sneak peek at what you might see as a player in Far Cry 6.
Today, we will be presenting to you our weather system in its entirety.
As such, we have condensed many topics into the following categories.
To start, I will go over our inspiration and core controls.
I'll then explain how we tackled material wetness by covering each asset type.
Following that, Colin will step through the technical details for each rendering feature that supports weather.
And lastly, we will conclude with some final thoughts.
For our inspiration, we looked at the weather for various tropical locations, such as Cuba.
Island locations are typically home to very distinct and varied tropical weather.
At the start of the project, research was conducted on tropical scenery and weather so that we could give our players an authentic experience as they explored our world.
We soon found that we needed to incorporate both the iconic, sunny weather shown here, as well as the flip side of tropical weather, which includes heavy rain and thunderstorms.
We needed to emulate the way that weather could change back and forth drastically, sometimes within hours.
We then took these elements that we wanted to highlight and included them in our concept art.
Yarrow was pitched as an ideal tropical paradise.
It should serve as a convincing escape into our game's fantasy.
However, our art direction wanted to contrast the picture-perfect weather with foreboding thunderstorms.
And when the player gets caught in a storm, the rain and wetness should be felt and should be convincing.
And of course, Yara is an island.
So the ocean is a key part of the equation, even for weather.
With that in mind, let's translate the direction into goals for implementing weather.
We want to have a collection of different weather states.
We want the weather to be dynamic and varied.
We want a system where the transitions make sense.
For example, a storm should be preceded with darkening clouds and followed by remaining water puddles.
And of course, everything needs to be efficient and fit within our budgets.
We must emphasize here that this weather system will be for an open world game.
Our decisions are often influenced by data and performance budgets and the need to support all times of day and all locations in the world.
Let's move on to our implementation, starting with the core weather controls.
The weather manager is the core code for our weather system and is what essentially drives weather behind the scene.
It contains information used to define and control weather, some of which is exposed as settings in our weather database.
Consider the weather manager as the back end of the system and the weather database as the front end.
Let's go over the setup for our system, starting with the weather preset.
The collection of weather types is referred to on the project as weather presets.
These are defined in the weather database using the exposed parameters from the weather manager.
To get an idea of what we can achieve with these parameters, let's step through some of our final presets and compare a few of their values.
For the few clouds presets, we use low cloud coverage values shown in the bottom right.
In comparison, the Broken Clouds preset uses higher cloud coverage to give us big, fluffy clouds.
Next, we can see that the Mist preset has added light fog in the distance.
And in comparison to that, we have our Heavy Fog preset with fog values cranked much higher.
The next few presets will show the progression of rain intensity, starting with light rain, then moderate rain.
followed by heavy rain, at which point the rain intensity is maximized and the sky begins to darken, which leaves us with our most intense preset, the thunderstorm, which adds lightning.
Now that we have our weather preset building blocks, we need to create a cycle or a weather pattern.
In the very beginning, our initial idea was to collect and use real-world meteorological data from cities like Miami.
As shown in the image, we obtained the description of weather at every hour from a period of time in 2013.
Although it would have been nice to use real-world data to drive our forecast, we wanted more artistic control.
The method that we ended up going with was a similar text file, but we timed our chosen weather presets ourselves and designed up to five full day cycles per region.
This freedom also allowed us to showcase our weather presets where they look the best.
Now we need to take some time to discuss regions.
The world of Far Cry 6 has been subdivided into three main regions, West, Central, and East, each with its own visual identity.
The West region was the dry region, Central was the wetlands, and the East was the jungle region. To enhance these distinctions, we wanted our weather to differ between each region as well.
Note that our definition of regions also extends to smaller zones, such as interiors and caves, and anything else that artists might need to define.
For each region, we exposed min and max curves to limit certain weather properties.
This was how we altered weather based on where the player is moving around in the world.
An example usage case would be how we set the max fog curve to zero for the indoor zones to prevent fog from appearing inside.
The limitation to this is that if your indoor area has windows, you would see the fog disappear outside as soon as you walked in.
And this is why we assigned our zones very carefully.
Our weather manager then took in all the potentially overlapping regions around the player and interpolates the curves accordingly to output the adjusted weather.
The weather manager also needs to allow for overrides, such as for gameplay missions and pre-rendered cut scenes.
For example, the opening mission of the game has the player fleeing through the city streets in the middle of a nighttime thunderstorm.
Additionally, our game is available in multiplayer co-op, so weather needs to be replicated for all players.
This flowchart illustrates the final order of operations.
So to summarize, we first consult the forecast to get the weather preset at the current time of day.
Then based on where the player is, we apply regional adjustments.
This gives us the current weather state, which can be overridden for gameplay.
Finally, we use this weather state to update the wetness, rain, and lightning with time and output the final parameters.
These are variables like wetness factor, rain factor, et cetera, which we can now access to drive visuals, audio, gameplay, and so on.
Now that we've established the inner workings of our weather manager, we need our materials to respond.
In other words, how do we make our assets wet?
Wetness is a huge component of weather.
When it rains, we need to see the world change.
And this means that every asset now needs a wet state so that we can properly blend from dry to wet.
The risks associated with creating a wet version of every asset are, one, there are too many assets, so requiring any extra data will add a lot of production time.
And two, our art teams all have their own asset pipelines and shaders.
We could be risking a lack of cohesion, which would be very hard to control given our project's scale.
So our final solution was to work in parallel with asset creation.
To do this, it needed to be simple and unified as much as possible.
We also decided that it would be primarily tech artist driven and it should work out of the box.
This means that we should be able to drag and drop assets in the world and they should get wet properly in the rain.
Of course, some materials receive more detail and polish, but for the simplest props or legacy assets, wetness should work without having to revisit them.
So let's dive into our wetness implementation.
First and foremost, we split our solution into two parts by having two wetness types, static and dynamic.
As their names imply, static wetness is for objects that will never move, and dynamic is for objects that will move.
Static wetness applies mostly to our object bank assets, which includes props and structures.
Their wetness level is determined by using the wetness factor parameter provided by the weather manager.
We also use a wetness shadow map to mask out wetness where appropriate.
Static wetness will be the simplest visually and will be applied in the deferred lighting pass, which means it will only be applied in one spot.
The pros to static wetness is clearly its simplicity and the fact that everything can be tweaked at once.
But the con is that there is no flexibility at all.
Dynamic wetness is reserved primarily for weapons, vehicles, and characters.
Their wetness level is calculated by raycast to detect exposure to rain.
Their wetness also includes a bonus feature called local wetness, which handles submersion in water.
For dynamic wetness, the visual change will be applied in each individual shader, which means that we can tailor the look a lot more.
The con to this is that we now have to manage every shader that could be used for dynamic assets.
First, let's look at the static wetness type.
The main wetness calculation we need is the wetness shadow map, which includes objects that are, for example, under a balcony or indoors.
This is important because we use our props a lot, so their wetness should be accurate no matter where level artists place them.
Since our static wetness is applied in the deferred lighting pass, we store the wetness shadow in our deferred shadow pass.
We then simply multiply the sample shadow with the weather manager's wetness factor to get the final wetness.
Let's look at a wetness shadow example.
In this scene, there are lighter, dry concrete patches on the floor and on the wooden table as well.
But more importantly, let's look at the wetness shadow.
As you can see, it works quite well, but there are some precision limitations, particularly on vertical surfaces.
If you look closely at the highlighted areas, you will see some speckled details.
This is because the cutoff was originally a very harsh line, which we softened using dithering.
Now let's move on to how we apply the visual of static wetness.
We first referred to photo reference to analyze exactly what inputs we would need and what changes they should drive.
The two visual changes that we identified as necessary were darkening the albedo and increasing the smoothness.
Some materials like cardboard would darken, but wouldn't get too shiny.
And materials like tiles would not darken, but would get much shinier.
And some materials fall in between.
The key to all this was how absorbent the material was.
As a result, the input we chose was porosity.
Porosity is most directly related to albedo change.
For example, high porosity materials like dirt, fabric, and raw wood will become saturated and appear very dark.
On the other hand, low porosity materials like plastic, marble, and metal will have water pooling on the surface rather than getting absorbed, which means that the colors should be unchanged.
But now that we have our input, we could just use a porosity map and move on.
Except for one problem, not every material can afford an extra texture map.
That would exceed our budgets for not much reward.
So now we need a way to derive porosity whenever a porosity texture is not available.
And this is where we introduce porosity factors.
Since porosity and smoothness are properties that are conceptually connected, our solution was the following formula where the two porosity factor values are just floats that we could specify per material type.
This formula essentially generated a porosity map with all the details of the smoothness map, but with its values lying in the range that we specifically curated.
Luckily for us, our object bank shaders already had a dropdown of material presets with hard-coded PBR values.
All we had to do was insert our porosity factors in there.
This kept the artist workflows completely unchanged and prevented any setup bugs.
So now that we finally have the porosity input ready, we can look at the central apply wetness function that is called from the deferred lighting pass.
The code can be referred to later, but what we essentially did was darken the albedo according to porosity, except that the material is metallic, boost the smoothness, and note that we used porosity once more to get even more control, and finally, replace the specular reflectance value with that of water.
So for an approximation with very few inputs, it would be impossible to get every material response correct.
It was more important to judge the bigger picture rather than individual assets.
This helped us avoid an endless loop of tweaking values.
Here's a dry versus wet comparison for some of our materials.
Now let's look at a full scene of assets transitioning from dry to wet.
Now let's switch gears over to dynamic wetness used for our characters, weapons, and vehicles.
These assets all have a wetness component that keeps track of their wetness level.
As mentioned before, characters and weapons perform a ray cast every few frames to check their exposure to rain.
Since vehicles are larger in size, these actually need more than one ray cast.
When exposed to rain, dynamic assets will increase in wetness level slowly before they are fully wet.
And when no longer exposed, they will also dry slowly.
The per vertex local wetness calculation handled submersion.
So if a character wades halfway into a lake, for example, their lower half will appear wet.
Now, if we recall, the benefits of dynamic wetness was that we could customize the way we applied wetness.
To gather our ideas, we once again revisited real-world references, such as staring at parked cars outside in the rain.
For character clothing, we utilized the same approach as for our props.
We used an existing dropdown menu in our shaders, but also added an upper limit to the final smoothness.
This was just to give us even better results for wet fabric specifically.
Character hair was the simplest change we made.
We only used one set of porosity factors.
At the early stages of brainstorming, we really wanted a way for hair to actually clump together when wet, but it was too ambitious for our scope of work.
For skin, we wanted rain to be visible, but not in a way where scrolling could look unnatural or distracting, so we went with a subtle approach by using a static texture, which contained a droplets normal map and a wetness mask.
With these applied, the skin material wetness was quickly finalized.
On our project, weapons and vehicles are closely related.
They are both gameplay elements and are seen up close from the first person's perspective.
For this reason, we gave these assets animated rain effects, which were applied using animated textures updated per frame.
These effects were divided into two parts, streaks and droplets, which were applied on vertical and horizontal surfaces, respectively.
For the streaks, our input data was only one texture, which packed in a streak mask, a normal map, and a height map.
The scroll mask was what actually drove the vertical movement of the streaks, which we applied in local space UVs.
We made sure to only apply the streaks when the asset is oriented upright or upside down.
In the latter case, we reversed the scroll direction.
For the droplets, we supplied another texture containing a normal map, a height map, and an ID map.
To drive the animation here, we sampled the droplets two times to vary the pattern of droplets fading in and out with varying timing cycles, which was made possible with the ID.
One thing we were still missing, though, was the tiny static droplets that tend to build up over time on hard surfaces.
So what we did was squeeze them into the height map texture later on.
We also made sure to use manual MIPS on the droplets texture to reduce sparkling artifacts.
As we all know, adding a feature often results in unexpected issues and edge cases popping up.
One issue we faced was that wetness effects were showing up inside vehicles.
This was because our vehicles sometimes share materials between the exterior and interior to save on draw calls.
So the fix for this was simply to use the red vertex paint channel as an interior mask.
Once this was painted by artists, we could remove the interior rain effects, except, of course, in edge cases, such as when our convertible cars put down their tops.
Another problem came up when moving windshield wipers were implemented by our gameplay team, while rain streaks were added by our 3D team, resulting in no interaction between them.
What we did was pack in a windshield gradient map and set up the shader to mask the rain according to the current gradient value.
We then pass this value over to gameplay to hook everything up.
This is a very small feature that we never actually planned, but inconsistencies like this can really take a player out of an experience very quickly, so we were happy to add it.
For vegetation, we originally used the static wetness approach, but this led to two very big issues.
Firstly, trunks and plants near the jungle floor were actually getting shadowed from the rain, which was very strange to see in nature.
And secondly, we were getting bugs explaining that the trees were looking metallic.
And this was because the flat leaf cards were giving off a uniform reflection of the gray sky.
And the visual was so strong that it effectively reduced the realism of our trees by exposing where the cards were.
To fix this, we converted vegetation over to the dynamic wetness type, just so that we could tailor the wetness in the shaders independently.
Of course, this meant that there would now no longer be any shadowing.
But 99% of the time, our trees are not hand-placed indoors, so we went through with the swap.
In the end, we used the droplets texture on the leaves, which finally matched reality a lot better and fixed the reflection bugs.
Terrain was the final area to conquer.
Not only is it always in the player's point of view, but as we can see from these photo references, it also requires a lot more detail.
Let's first summarize very quickly what we need for our terrain.
The primary textures used include albedo, normal, smoothness, and now the new texture, porosity.
These properties are stored in a virtual texture atlas that we use for the entire world.
To see more about this system, you can refer to our past talk on terrain.
Note that roads and terrain decals are all baked into the virtual texture atlas.
So it really is one complete system that we needed to apply wetness to.
To make terrain wet, we actually just used the same apply wetness function shown earlier.
The key difference now for terrain is that the transition was changed.
We didn't want a linear fade, as we thought this was a bit too artificial for something with such a large surface area.
So the new transition we created was our raindrop splatter effect, as shown in the video.
To do this, a height field simulation was done on the GPU, which took in some simple parameters, such as drying and spreading speed for the splatters.
The resulting animated mask was used as our custom wetness factor.
Another thing to note was that we chose to temporarily increase the terrain porosity at this point just to accentuate the effect with darker splatters.
Now that terrain has its wetness applied, we have to turn our attention to the largest visual feature needed to present wetness, puddles.
Puddles are also a heavily desired feature for our direction because they would provide reflections, which ultimately makes the environment and lighting much more appealing.
The setup for puddles was very simple.
A puddle itself is a terrain decal, but all it contained was an alpha gradient and a checkbox to only write to terrain porosity.
The gradient served as a pseudo-sign distance field, which allowed puddles to build up from the center.
We only had a few puddle decal variations, which were scattered by our existing terrain and road recipe systems, similar to how cracks and potholes would be scattered.
We calculated the puddle wetness based on the terrain porosity and the puddle factor provided by the wetness manager.
Once we had the puddle wetness, we once again just altered the gbuffer values accordingly.
What we did was slightly blend the albedo to a muddy puddle color and blended this evenness to 1.
For the normals, however, we initially used a flat normal, but that now brings us to our next implementation, which was puddle effects.
The two types of puddle effects comprised of rain impact ripples, which are the circular ripples that simulate rain hitting water, and wind ripples, which are the small waves that occur when a gust of wind pushes the puddle surface.
It would have been perfectly acceptable to have flat, undisturbed puddles, but we really wanted to take it one step further and tie in our weather elements.
So we generated a tiling animated texture, updated every frame, to be used for all of our puddles.
Our input data was a single texture with two normal maps packed inside, one for the rain ripples and one for the wind ripples.
The output was a single animated normal texture.
So for the rain ripples, we sampled the normal texture twice to have two overlapping layers of flipbook animations.
These are faded in based on the weather manager's rain effects factor, which will ensure that these ripples only appear when it is actually raining.
For the wind ripples, we scroll the texture based on the current wind direction and intensity.
As a result, this effect can be present whether it is raining or not, such as when puddles are still on the ground just after a storm.
Finally, we blend these effects together to get the result that we use for our puddle models.
This concludes the collection of shader changes we needed to support material wetness.
And now, Colin will take you through the technical features that allowed us to render the weather.
Thank you, Emily.
Hi, my name is Colin Wyke.
I'm a 3D programmer at Ubisoft Toronto, where I've been for the past five years, and I worked on Far Cry 5 and Far Cry 6.
Now that Emily has shown us the intricacies of the weather manager and the wetness, the implementation of the wetness, let's take a look at the rendering features we needed to realize dynamic weather.
But first, let's take a look at the technical constraints that we had.
Our game shipped on nine platforms, spanning multiple console generations, as well as PC.
We were aiming for 4K 60 FPS on next gen and 30 FPS on the previous gen.
This was also our largest open world to date.
And as Emily mentioned, we have a full day night cycle in an open world, which features indoor and outdoor environments.
And now we need to do all of this with dynamic weather.
Let's take a look at a sample scene and see how these rendering features come together to complete the depiction of our weather states.
First, we have atmospheric scattering.
Then we have the clouds on the horizon and overhead.
Then we have volumetric fog obscuring regions and producing light shafts escaping from the clouds.
Next, we have cue maps and reflections.
Notice how the power lines are reflected on the road.
And finally, we have rain and lightning, which complete the scene of an intense thunderstorm.
Before we look at individual techniques, let's start with a primer on our lighting model.
We use physically based formulas and aim to be energy conserving as a goal.
New for Far Cry 6, we have a higher quality multiscattering diffused BRDF and a GGX specular BRDF with multiscattering lobe and support for area lights.
The chart on the right.
shows how we handle specific materials, but one surface type of note was translucency, which we use for vegetation. For translucency, we wrapped the diffuse lighting for subsurface scattering and added a second diffuse lobe to simulate light going through the surface.
You can refer to Steve McCauley's i3d talk from 2019 to learn more about these improvements.
For global illumination, we use a light probe system where probes are placed by artists throughout the world and baked daily to incorporate changes in the world.
GI data is stored in voxels.
We pack 13 frames of data, 11 of which are time of day increments to give coverage to the sky and moon.
There's one key frame for local lights, which is mostly used at night, and one key frame for sky occlusion.
But there's a problem.
These GI probes don't include clouds or any impact from the weather.
The solution that we had was to fade out indirect lighting when cloud coverage is high.
This could create a problem if local lights were in the key frames in all the key frames.
But since they're isolated to just one, we can avoid fading artificial lights during high cloud coverage.
Additionally, urban environments tested the limits of the system.
We leaned heavily into the sparse nature of the data.
and the variable probe size to increase precision near and within indoor environments.
We use the Brunton Sky model and Pretham Sun model with improvements made such as Ozone.
This setup involves lookup tables calculated offline.
And previously, this was generated at runtime, but that was very slow.
Artists create four skies, humidity 0 to 1, turbidity 0 to 1, and we blend between these at runtime to get variation. You can check out our GDC talk from 2015 about lighting in Far Cry 4 for early implementation of these concepts.
Here we show the effects of humidity and turbidity parameters on the optical depth lookup table and the blended in-scattering lookup table. We blend four pairs of data together to get our final result.
The green arrow points to the blended result for each frame, and the red arrow points to the lookup tables with the greatest weight for the frame's humidity and turbidity parameters.
This is what gave us our stunning sunrises and sunsets, which you see casting beautifully on the edges of the clouds.
That leads us to our next subject, volumetric clouds.
So why do we want volumetric clouds?
Simply put, a skybox is not sufficient.
It has poor results in motion and is difficult to blend between different weather states.
A skybox is also more of a backdrop where we wanted something more grounded and interacting with the world.
Additionally, we needed our clouds to respond to weather and varying cloud coverage.
Our eventual solution was built on prior work as listed here.
The most important part of cloud lighting is extinction.
As light travels through a volume, it loses energy due to the interaction with water particles that clouds are composed of.
This extinction is determined by the Beer-Lambert law, which we will henceforth refer to as transmittance.
Extinction consists of both absorption, which is actually quite low for clouds, and scattering.
The light that eventually hits the eye after being scattered is measured in terms of radiance.
Single scattering refers to when light enters a cloud and encounters one scattering event before traveling in the direction towards the observer.
Multi-scattering refers to when the light encounters a near limitless number of scattering events within the cloud before traveling in the direction towards the observer.
Here we show the effects of single scattering on the clouds.
Next, we isolate the effects of multi-scattering on the clouds.
And finally, we have the effect of both. Notice the greater sense of depth in the clouds.
All of these scattering events can be modeled by a phase function, which projects how much light and in what direction it will travel after it's scattered.
In this diagram, we have the oval shape representing the phase function approximation of light scattering after the scattering event at the blue dot.
On the right is a plot of the double-lobed Henye-Greenstein phase function we used in our implementation.
The real phase function for clouds would be much more complex and far too expensive to calculate in real time.
You can reference this and other lighting code in our bonus slides.
All of these scattering events can be modeled by a phase function.
Let's talk about how we authored cloud data, which represents the density of particles in the clouds to be used for scattering techniques.
We created base and detail noise textures containing mixes of noise types and frequencies, which we collapsed into a single channel.
These textures are then volumetrically sampled and blended together to create cloud shapes.
Next, we generated a weather map to be tiled in the world and scrolled in the wind direction from a weather manager.
This builds our XY formations of clouds and enables us to smoothly interpolate between levels of cloud coverage.
We also have a curl noise, which contains 3D vector data, which we use as an origin offset when shooting rays into the base in detail noise.
Finally, we have cirrus cloud texture, which we map hemispherically to the sky, and cirrus horizon texture, which we map in a cylinder around the camera.
Let's put all of these together.
Here we start with a clear sky and just our atmospheric scattering.
Then we add the cirrus hemisphere and horizon clouds.
Then we add the weather map, and you start to see the xy shape of the cloud formations.
Then we combine the gradient texture to define the shape of the clouds.
This cumulus gradient ended up being the only gradient we needed to ship, although we did experiment with other cloud types.
Then we use the base detail noise volume texture to get the distinctive cumulus cloud shapes.
Then we use the detail noise volume texture to erode more detail from those clouds.
And then finally, we add the curl noise texture to get the wispy details on the edges of the clouds.
Ray marching is the process we use to draw clouds efficiently.
It involves shooting a ray from the observer and evaluating at specified steps.
At each step, we integrate the density from that point back to the sun or moon.
We then convert the optical depth that is returned to transmittance, which can then be used to approximate single and multi-scattering.
We then integrate this transmittance along the segment and accumulate single and multi-scattering for the raymarch total.
We also apply the extinction to the transmittance, so that affects later ray steps.
This general process for each pixel is as follows.
Calculate the origin from the pixel ray march, sorry, from the pixel and ray march, the density transmittance, optical depth and scattering, both single and multi.
Calculate the cirrus clouds if there is non-zero transmittance.
Use the single and multi scattering values to calculate the radiance from the phase function.
And then finally calculate atmospheric scattering and blend the clouds in front using inverse transmittance as a mask.
It would be far too expensive to ray march at the full screen resolution, so we do so at half of the screen resolution.
We have a radiance and transmittance texture, as well as a history radiance and transmittance textures, which we temporally reproject from the last frame.
We fill these runtime textures using a checkerboard rendering process.
We do a single ray march for each pixel quad using a checkerboard offset combined with the curl noise offset to get the ray origin.
This ray march result is single scattering, multi-scattering, and transmittance.
We project the history into the current frame and then gather the neighborhood and reject based on heuristics listed.
We then use bilinear interpolation of the neighbors and spread the result.
of the ray march over this area, fading the history over time.
The result is stored as radiance and transmittance history that will be used for the next frame.
Particularly of note is the process we use to store the radiance.
We encode with blue noise to help mask the noisy artifacts inherent to ray marching at such a lower resolution.
Notice on the right, we do a 2x blur of the blue noise, and it converges to gray, while the white noise does not.
This looks pretty good, but there's still a problem.
We're missing shadows on the ground.
This makes it really hard to determine where the clouds actually are.
And in reality, they're directly overhead the island.
We tried to raymarch shadows from the G-buffer, but that was far too costly.
The solution we landed upon was projected cloud shadows.
It was a simple orthographic projection, nothing fancy.
And the resulting shadow texture will be used later on for volumetric fog to get light shafts from the clouds.
And here we have the clouds on the ground and also on the water.
Next, we have the volumetric fog, which is used for environmental height fog, as well as local fog from the weather presets.
Our implementation uses a frustum-aligned voxel grid presented in prior work.
We have local and global fog based on weather, as well as local fog used for different biomes.
With the volumetric fog, light shafts are created by shadows evaluated within the cells of the volume.
And here is where we have some problems.
Here's a brief overview of the volumetric fog process.
We prepare our scene depth information in the first three steps shown and call local fog volumes to screen tiles, and then we'll cover the remaining steps in the following slides.
We fill the cells with a radiance.
And we do so using the local and global densities of fog and fog particles, as well as lighting from the sun, atmosphere, indirect lighting, point and spotlights.
Note that we use temporal filtering on last gen instead of the upcoming bilateral blur and this would require a history buffer that is reprojected into the current frame and faded out over time.
This requires more memory and is unfortunately, has some ghosting artifacts, but it was a cheaper solution.
We also use the same phase function as the volumetric clouds.
Here, you can see us filling the volume.
After we fill the cells with irradiance, we calculate the continuous sum towards the back of the volume.
Now, we can sample any voxel for full irradiance accumulation from the eye to the destination.
And finally, on NextGen, we do a two-stage process of a bilateral blur in the x and y-axis.
Blurring in the x-axis outputs transposed, and blurring in the x-axis transposes back to the original orientation.
This optimizes reads and writes of the textures.
But there's still a problem.
On the right, you can see stair-stepping in the light shafts, which is an artifact of sampling the shadows.
We want this to be completely smooth, and the bilateral filtering helps, but it doesn't completely solve this.
The next approach we had was to blur the actual shadow maps.
And one benefit of this approach is that it can be reused in multiple places.
Here we see the process for exponential blurred shadows.
First, we convert and downsample three near-shadow slices from our cascade shadow map from the sun.
Then we blur the slices and downsample again.
As you can see, the result is significantly smaller than the original, which is more optimal for quality and performance when volumetrically sampling.
Let's take a look at a sample scene with no fog from our weather manager.
Here we have the fog on its own with some less than ideal artifacting.
This is the result of blurring the irradiance volume and screen space.
It's better, but there's still artifacts that suggest the shape of the voxels.
And finally, we enable the exponential blurred shadows.
To this point, we have not mentioned how we actually apply the clouds.
We apply atmospherics, clouds, and fog at the same time to save on bandwidth when writing to areas of the screen where both are visible.
You can see here that the cloud shadows give nice light shafts in the fog.
One final detail of our composite pass is that we apply the final value with blue noise at the final screen resolution.
This combined with the temporal AA post-process further eliminates ray marching artifacts visible in the clouds.
Now let's talk about reflections.
Reflections are crucial to completing the visual concept of wetness.
As Emily mentioned before, we used values from the weather system to determine what surfaces are wet.
Then we perform the process where rays are path traced using the G-buffer positions and normals to pixels and screen space to be reflected.
This does most of the work of reflecting objects in the scene, such as trees, sky, and clouds on the surface of the road, as well as the mountains shown on the water surface.
For more information about our SSLR, as well as our hybrid ray tracing solution, see my colleagues Stephanie and Ihor talk about performant reflective beauty in their talk.
But what happens when we don't have enough data for screen space reflections, such as when we view water at the angle shown on the right?
We need a fallback where there is no data to drive the screen space reflections.
Our fallback comes in the form of cubemaps, which need to show time of day changes, weather, clouds, as shown in the sunset reflection on the cloud, or the car on the right.
Our QMaps are generated at build time in positions determined by artists, and they need to be relit with our atmospheric scattering, clouds, and fog.
QMap data is baked and stored as albedo, normal, and smoothness, as well as a lower resolution depth map.
To achieve acceptable performance, we relight just one face of the cubemap each frame, except in special cases such as cinematic camera cuts.
The process starts with generating a sky-only cubemap containing only atmospheric scattering, clouds, and fog.
Then, the sky-only cubemap for our fallback reflection is used on our ocean.
Then we were like the scene using, sorry, the scene face using the baked cubemap data.
And finally, we composite the sky cubemap into the relit scene cubemap, giving the final result shown in the bottom right.
We progressively update a single face, each frame attempting to process cubemaps when streaming in new areas.
After the relighting process, cubemaps need to be filtered for blurrier reflections on rougher surfaces.
One large problem that we had was overcast lighting and weather.
Sometimes it was just, it just wasn't dark enough and it could appear flat.
We want the clouds to have contrast and we also want to have blue sky breaking through.
There were several reasons why this was happening.
One such example was our authored sky were too turbid and gray.
If you want blue in your scene, in your natural lighting, essentially, it comes from here.
But be careful.
There are otherwise, if you put too much blue, then your whole scene will get too blue.
We also had issues with the fog washing out the image.
And to fix this, we fade fog based on cloud coverage.
We had skylighting that was too bright because the clouds never reached the horizon in the cube map.
And there's also a blue band in our cube maps that contributes to this brightness.
Clouds of uniform thickness.
could also wash out the image, which is why we limit the cumulus coverage to 0.8 even in a storm.
So now let's move our attention to rain. Remember, the weather system keeps track of a rain factor that we use to drive our particle systems.
it was very apparent early on that our existing CPU particle system would not be able to produce the number of particles that we wanted for rain. Therefore, we set out to create a GPU particle system to render rain effects, and in doing so, we ended up with a generalized system that supported many other types of effects as well.
Here's a brief overview of our particle system you can refer to later.
In summary, we use compute shaders to emit, simulate, sort and render all of the particles on the GPU with minimal interaction with the CPU.
For any particle system, you need billboards or geometry rendered in the right order.
Our solution was a bitonic sort to order the particles all at once.
This algorithm is massively parallel.
And while it has a higher average cost than something like a radix sort, it was one of the lowest worst costs, which is important in game development to avoid frame time spikes.
Once we have a set of visible particles sorted and ready to render, we need to filter which particles need to be rendered in which paths.
We used a prefix sum algorithm to effectively eliminate the empty space in a buffer to a sorted list of visible particles that are just for a single pass.
This is also a highly parallelized algorithm perfect for the GPU.
Let's go through some of the particle effects that we had for rain.
The first particle effect we want to look at is the rain streaks.
We experimented with refraction, blur, and reflections, but we ultimately used transparent textures to achieve the look of rain in photography, which was part of our art direction.
We used two frames of variation and a normal texture to aid in the lighting.
We recycle rain streaks in a cylinder around the player to ensure the number of rain particles remains consistent, regardless of how fast the camera is whooping.
To add continuous variation to the particles throughout their lifetimes, we used a 3D noise texture mapped to world space as turbulence.
And finally, the emission rate and direction of the rain is determined by the wind values provided from the weather manager.
Since we had many indoor and outdoor environments, we needed to occlude rain where it was not plausible.
To do this, we created a rain shadow map.
that we can reference to determine if a rain shriek or splash was occluded.
This was a directional shadow set up to cover the playable area near the camera parallel to the rain direction.
Then we stored this in our regular sunshadow atlas shown on the right.
And the colored image on the left is used as a lookup table to convert UV space into the atlas texture.
The rain shadow map only contains static scene elements and is updated in sections each frame.
The next particle effect we want to talk about is splashes that appear when rain hits a surface near the camera.
Initially, this was driven by a GPU particle event system, which emits new particles when a parent particle hits a surface on the depth buffer, terrain, or water.
And this means we get splash particles on any surface which is opaque, including vehicles, characters, and weapons.
However, we didn't necessarily want these splashes to be on sides of buildings, so we incorporated a slope factor.
Another problem that we had was we weren't getting enough splashes because not every particle we checked for collision would generate a splash.
So therefore, we spawned more particles in a volume around the camera and snapped them to the depth buffer.
For both rain streaks and splashes, we wanted pixel lighting, especially at night.
However, this was far too expensive and vertex lighting just wasn't good enough.
What we did was create a system for generating spherical harmonic light probes for each vertex of the particle. Each of these probes calculates third-order SH coefficients and incorporates sun, point, and spotlights. This gives us better than vertex lighting, but is much cheaper than pixel lighting.
In addition, we can sample normal maps of particles for directional lighting and better specular highlights.
Here we see an example of a particle billboard of a sphere on the right, and it only has a diffuse and normal texture.
The next particle effect we needed was lightning bolts for our thunderstorms.
For this effect, we created a ribbon or trail system. We started by creating trails automatically left by individual particles, but extended this by generating particles and tessellating geometry between each particle. We call these linked emitters. Using the linked system, we emitted particles in the cylinder moving from top to bottom, increasing the turbulence in the center of the volume and fading out at each end. But this alone was not very realistic.
we needed to make the lightning effect the world around it.
First, we generated an omni light and placed it in the scene so that lightning would light the environment.
Then we had to address the clouds around the lightning bolt.
We applied lightning to the clouds in the upsample pass so that lightning doesn't end up in the history buffer and cause ghosting.
And in order to determine where in the clouds to add light, we use sun scattering factor that we saved in the green channel of the cloud ray marching pass.
And our implementation functions essentially like a column light.
The next rendering feature is the ocean.
Improvements we made to the ocean rendering were mostly to support the tropical environment.
However, ocean still needed to be affected by weather in two ways, Beaufort level and wind direction.
Our previous screen space tessellation had issues with shoreline waves whenever the ocean was parallel to the view direction.
It lacked a lot of detail in the distance.
And for more information on our previous implementation, you can see a previous GDC talk we had.
So we added a new type of tessellation to support sharp waves for weather and improve the distance visual and support shoreline waves. The tessellation we chose for our ocean was the i-sub-d tessellation scheme. This basic algorithm subdivides the ocean mesh into a from the single two triangle quad down to subdivided triangles you see near the camera.
It is a progressive refinement algorithm that will subdivide into smaller triangles, each frame as they get closer to the camera, and merge into larger ones as they get further away.
There is a performance limit on the number of operations that can be done every frame, so we fill a buffer with keys to perform these operations.
This led to a list of a lot of empty space.
This was an area of the sub-D algorithm that we found to be unsolved, and we needed a solution to this problem.
So we used the same prefix sum algorithm from the GPU particle filtering on a different type of data.
In our world, we have fresh water that meets the ocean, therefore we needed to blend the previous screen space tessellation to the sub-D tessellation.
To do this, we use the displacement and normals of the fresh water as input in the intersections of this tessellation type and blended in that range.
And then we use simple stencil testing to avoid redundant pixel shading of the water meshes.
The Beaufort scale is a empirical system that relates wind speed to its effect on bodies of water.
Here we show how Beaufort levels coming from the weather manager can impact the look of the water.
And this is going to be Beaufort level 0 to 4.
The Beaufort levels themselves are tuned with data such as wind speed, amplitude, and scale, as well as choppiness.
That was Beaufort level 1.
And here's the value.
Sorry, these values are used to drive the wave simulations, which we will show later.
The Beaufort level also includes settings for shoreline waves, such as amplitude, frequency, speed, steepness, and number of waves.
The Beaufort levels include settings for foam, which you can see accumulating alongside the increased wave size.
And this is Beaufort level 4.
Here we have the ocean wave buffers that we generate every frame in order to create motion in the waves.
These buffers can be mapped back and read on the CPU for physics calculations, ocean level, sorry, and ocean level, but we disabled this due to performance limits.
The world space FBM was created to get sharp waves up close near the camera.
Using the world space buffer, sorry, using a world space buffer gave us the ability to generate normal directly instead of from screen space like we did previously.
This gave us better details up close as well as in the distance with mipmapping.
Next, we used the FFT simulation for waves in the distance.
We adjusted our original implementation to respond to Beaufort level and wind parameters.
We also added the accumulation channel to the texture to create persistent white caps on the waves.
To prevent tiling in the distance, we used the FFT buffer as a cascade at a different scale as well as a Perlin noise wave texture.
Here we see the FFT buffer we started with.
Notice the tiling in the distance.
Then we have the FFT cascading, the FFT cascade eliminating some of the tiling.
And finally, we have the scrolling Perlin noise texture, which is removing the rest of the tiling.
It does a good job of simulating the effects of wind on large patches of water in the distance.
Related to the new ocean tech was the inclusion of shoreline waves, which we needed for our tropical beaches. We had far too many shorelines to cover, so we needed a procedural method, which led us to choose the Gershner wave formula.
We used multiple parameters such as amplitude, speed, and foam to control the visuals.
The last rendering feature we'd like to talk about is the tree bending, which works in conjunction with the wind direction and magnitude coming from the weather manager.
We improved the tree bending settings by adding noise and bending amplitudes.
Combined with the wind values, this gives us an exaggerated movement in the trees that reflects our stormy weather conditions.
These settings are controlled by artists when setting up tree trunk skeletons.
The movement could sometimes exceed the tree's bounding box, which could be problematic because they could suddenly be cold.
And ultimately, the feature was limited by being plausible when zooming into the distance.
To conclude, we'd like to share some of our final thoughts from both tech and from tech art and programming perspectives.
For me, things that really drove our success were limiting the complexity, studying references, reducing production dependencies wherever possible, and identifying the biggest wins so that we could focus on what sells the weather the most.
For future work, we could simplify the database further.
We could also set up an easier way to debug and review weather without overrides confusing us.
We could make more use of wetness effects and push our presets to be more extreme.
And lastly, we could always investigate procedural weather patterns as opposed to our hard-coded forecasts.
Thank you, Emily.
For rendering takeaways, as you saw, there was a lot of tech to maintain.
There were a lot of interlocking parts which needed to be polished both separately and together.
The biggest recommendation we have is to finalize these things early.
That means your data formats, data and processes.
You don't want any of these things changing too far into production.
For future work, there were several things listed, but one thing I find most interesting would be to use the same tessellation for both water and terrain and use the same atlas for virtual texturing.
We would quickly like to thank the 3D teams who worked on this feature and everyone who helped us along the way.
As you can see, it was a collective effort.
Here's a list of references for the talk.
And with that, we'd like to close out with a video of our final results.
All right, thank you for joining us today.
Are there any questions?
Thank you for a nice talk.
I'd like to ask about the QMAP relighting.
How did you handle shadow maps for relight QMAP?
I believe shadow map is outside and then shadow map is assigned to main view custom areas.
Yeah, so one of the things that we noted in our problem section with overcast lighting was that we don't have any shadowing in our cube maps.
So, yeah, we didn't have any shadowing.
That's something we'd like to add in the future. Thank you.
Hey, great talk and a lot of awesome information. Thanks for doing it.
I had a quick question. You mentioned you had some undesirable effects when in interior spaces with some of the weather and that that led to constraints on sort of how you were working with like the fog disappearing through windows and stuff.
What sort of solutions and constrictions did that put on like your concave spaces and interior spaces?
So for our zones specifically, we did run into a lot of issues, particularly for the indoor zones.
As we mentioned, the change happened immediately. So you will see these changes if you have windows.
What we did was if there's ever an open space or an area with windows, we made another zone, essentially, where we would solve these issues.
Hello, great talk by the way, it was very informative.
My question was related to performance.
It sounds like there was a lot of features that you had to add to the base shaders to get all this wetness and stuff.
Were there any challenges regarding optimization and keeping that running at all times to dynamically switch between things?
Yeah, so I would say optimization wise, everything had to be designed so that your cost would not increase when the weather turned on.
So like Emily talked about with dynamic wetness, we had to assume the cost with the trees to be.
wet dynamically as part of that cost.
So I guess the thing is we would have to optimize as much as possible, but we essentially have wetness as something that could be turned on at any time.
Okay, thank you.
Awesome, thanks for the talk.
I was very curious about your static wetness specifically because it sounded to me like you were doing that as a deferred rendering pass instead of complicating your materials.
And I just wanted to make sure that I understood that correctly.
Is that true?
Yes, so basically in every shader, we would be assigning the gbuffer properties.
And then in our deferred lighting passes where we would assemble all these properties and perform the lighting.
So what we did was just basically defer our wetness solution.
So instead of applying it in every shader, which would incur some more costs each time, we do it all at once.
Awesome.
I just had a quick question.
Thank you for the talk, by the way, really, really informative.
I had a quick question.
You render out a rain shadow map.
I was curious.
It doesn't seem like you reuse that for wetness.
No, that was specifically just for particles.
We tried to use it for other elements in the game like vehicles.
But as we said, it only included static information because it would be far too costly to update that every frame.
So it's really just to give us directional directional occlusion information for particles.
So essentially, you wouldn't see rain when there was an overhang that would block it.
whether it's indoor, outdoor, or both in and outdoor.
Thank you.
Thank you.
Thank you.
Thank you.
