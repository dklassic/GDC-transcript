Hey everyone, I'm Dave Hunt. I'm a technical artist, and this presentation is for the GDC 2020 Technical Arts Summit, which is virtual this year.
I do hope everyone is staying safe and healthy out there.
Here we're going to talk about control rigs.
Now this is the thing that helps animators create animation content.
As a technical artist in my career, I've spent a lot of time building these kind of control rigs for animators in game productions.
So, while the job of the control rig is to help it be easier and faster for animators to create content, it actually turns out, as we take a look at how this fits into production pipelines, that it actually introduces a lot of inefficiencies at times that we may not always be aware of because we're just so used to it working the way it usually does.
So let's take a look at how this is often done and how the control rigs are built inside of animation productions.
And let's pay attention to how it actually works when iteration happens, because we all know how important iteration is for creating fun gameplay.
So out of this, let's build a list of requirements for the control rig that ideally we would like to support so that we can keep a nice, good iteration cycle for our creative people on the team.
We do have a pretty good solution here that we've found and that I'd like to share today.
And I want to share how this works in general terms so that anyone could implement this in any tools context that they might be building control rigs in.
And then we'll show how we've implemented this in Unity.
And there's some cool new features available which I'll describe how they work.
For these examples, I'll be using the digital human character from The Heretic.
This is the real-time film made by Unity.
And just wanted to make sure to say that this content example in this presentation is not the film itself.
For that, please check out this link, and you can see it on the Unity website.
Alright, let's get started.
So, what do we mean when we're talking about Freeform animation workflows?
I'll just use an example of laying out a 3D animated scene.
So we might start out with a very simple environment that's blocked in with just basic cubes.
And then we can throw in some motion capture of our character walking.
And this lets us immediately start to make creative choices about the camera angles and even lighting and scene composition about how we want to tell the story.
So in the next iteration we might get more specific about the environment.
So here we've brought in more of a broken concrete wall to be the environment.
But notice our character is hitting his head.
And wouldn't that be nice if I could put on a control rig right there in the scene and make some keyframe modifications so that he ducks underneath that wall?
And so once that's done, it can be baked down to the skeleton and saved as a new animation clip.
Well, as it often happens in productions, they want to iterate.
So here we might have added some new concrete to the wall to break up that silhouette.
And now our character is hitting his head again.
So in this case we need to add a full body control rig.
And I can make some keyframe animation edits so the character reaches out and holds on to the wall while he crouches underneath.
Alright, so now that's looking how we want, we can save out this new custom animation clip, and that's ready to go.
In this example, all of these edits were done right here in the scene, where you have access to all the final rendering output, all the lighting and shading.
If there's anything like cloth simulation or effects, you can see it all right there, and it actually makes it really nice and direct to do your edits right there.
So why do we call it freeform animation? That's because you can go back and forth between this loop as much as you want, where we're transferring motion from a skeleton onto the control rig.
That's where it's easier to make these edits, and when you're done you can bake it back down to the skeleton, where it might be more an optimal playback format for runtime.
Of course, if you want to, you can leave those rig constraints active so they can do runtime rigging.
So I'll get into more detail about exactly how this works, but I wanted to give this quick definition up front.
Here's where you can learn more about this.
My good friend Rick Lico, my former colleague, has been a real pioneer of working this way, and he was innovating these kinds of techniques even before he came to work at Bungie with me more than 10 years ago.
And these days he's very active with training and education.
He really cares about helping animators find more efficient ways of working.
So in 2018 he did a great GDC talk.
Later that year he joined me at SIGGRAPH for a presentation where he went into deeper explanation of the technical pipelines and how they work.
And now these days he's opened his new online animation class called Animation Sherpa.
So here's where you can find out all about that.
I highly recommend it.
So let's take a look at the basic animation pipeline where we would normally set up character creation for any kind of 3D animation.
Usually it starts with some concept art and designing and then you do some modeling and then rigging comes along.
This is where we're going to focus our presentation today.
And rigging pretty much takes place in these two parts where...
First you need to define the deformations and the skeleton.
And then it's the control rig that gets built.
And this is where the animators base all their content.
Setting keyframes here, making poses, and creating animation.
So first off let's take a look at who will be using this.
In my experience, there's no two animators that are the same.
There's a lot of different backgrounds they come from.
And this results in very different working styles.
Where it's really great to have this variety because there's a lot of different advantages of different approaches whether you're more a 2D drawing or a technical animator or working with motion capture.
There's a lot of different kind of diversity that comes from this which is great for getting different kinds of motion that's needed for a game.
So we really want the control rig to be able to support all these different ways of working.
In an ideal world, it's going to give them the optimal...
way of interacting with that rig to get a fast and efficient creation of their creative vision.
And now let's take a look at what they will be creating.
For example in a game, an action game, you might have a player character that has to do all these different kinds of actions through the game.
And each one of these categories might represent dozens or even hundreds of animations that goes into creating the full motion set for the character to go through the game.
And they're all very different actually.
So if we zoom in and look at, for example, first person animation, where the camera's really close to the hands as they're contacting a prop, and it's very dynamic because it changes all the time.
Like what's controlling what, and how it needs to move, and it needs to be very nice and smooth and high fidelity when it's that close to the camera.
So that's a very specific set of needs.
Let's compare that, for example, to locomotion.
And here we can see the characters more at a distance, and the hands are moving free in this case.
It's more of a whole full-body mechanic that we're looking at, rather than a detailed close-up interaction.
So really the needs here are very different for what the Control Rig will need to be able to do for the animator to get a really nice and efficient way of working.
Here's yet another example, this environment world interaction that we talked about earlier.
So in this example...
Objects are introduced that we're not even there to begin with.
How could we possibly plan for all of this ahead of time?
And most importantly, the Control Rig really needs to avoid counter-animation in all of these different situations.
So counter-animation is a really important concept to understand here. So we'll take a look at this example.
Counter-animation is really any time an animator has to fight against the rig in order to get their desired behavior.
So for example with this prop, he's holding a gun, and in most cases both hands are going to follow that gun wherever it goes.
So here we want the control rig to act as if these hands are child of the gun.
And that's really great for the times when it's in that state.
But how about here when it's reloading?
He takes the hand away, and now it needs to move free.
But if I haven't changed it, it's going to still behave like a child.
So that makes this movement, every time I move the hand, I have to fight against how its parent is working.
And that is what we call counter animation.
In this case, if animators don't have a way of switching it, then every single frame, they need to correct it and make it look right.
And I've even seen people pull out a whiteboard erase marker and draw on their computer monitor to make sure it's a smooth arc.
And it's just very inefficient.
And after they're done with that, it's even harder to make changes.
So that's.
not what we want and the control rig ideally needs to help us avoid this situation.
So one of the ways people try and solve this often is by building what might be called an uber rig.
And this is a big monolithic file that attempts to plan for all possible situations and make all the switches that Animator can change in a given situation.
But it really isn't very possible to actually do this in my opinion.
For one, it's really complex, it's slow to work with, it's hard for animators to figure out exactly what will control what.
And it takes a very long time for rigging people to build too, and the longer it takes to build, the more you're blocking creative progress.
So we don't want that.
And I actually even doubt if it's really even possible to think of everything in advanced.
I think that in animation, you'll always be able to find some new situation.
where you need to have a new solution for the rig.
So let's just take a look at this and imagine how it works in a production.
So let's say a rigging person has taken a whole bunch of time to build this control rig by hand and save a file on disk that then gets copied out to all these hundreds of animations.
And, uh oh, what happens now when it's time to do iteration?
So.
As all the animations are playing out, it's real common to see okay, well, it doesn't look as good as we want in the given pose or maybe the game designers say we're not getting the desired read of the player experience, so we need to make some changes here.
In an often cases, we're simply fixing the deform rig, so we're adding new bones that are procedurally controlled to automatically follow other bones in the skeleton. And these might be called fix up bones, twist bones, post-correction. So what we'll do in that case is need to add them to the rig and then push out updates to all the files where the rig is used in these animations.
So let's plan for this in advance and make it a requirement that this control rig needs to be able to take updates.
Okay, and there's a lot of common ways this is solved, and one of them is file referencing.
That can be really nice and fast where all of these read from a source file.
that every time an animation is opened, it brings in the latest version of that new source file control rig.
But one of the problems with that is it locks down the hierarchy, so it's now read-only, and animators really can't change anything on the control rig at that point.
So they're very stuck now with whatever choices a rigging person has made.
There's other ways this can be done too. Instead of file referencing, you can import the rig and do a copy curves solution, which can work. I mean it does require some tools development to make a system like that. And there's even other solutions I've seen out there too.
Either way, these are something that's good to account for and build the systems to enable a nice efficient production.
Let's go farther up the chain now and imagine what happens when...
there's some changes at the 3D modeling stage.
So maybe the art director wants to get a little bit better proportions so that the poses look better.
In this case, we need to move some bones around on the skeleton.
And we'll have to adjust the skin weights.
And after that's done, this control rig now needs to get rebuilt.
That's a big problem if someone has spent time by hand in manually constructing this file.
So, and they would have to get it exactly the same naming and parenting and orientations and everything in order for animation to still work.
So, let's not let that stop us from iterating.
Let's take another requirement here that we need to be able to quickly rebuild this rig and get it exactly the same as it was before every time the skeleton has been redefined.
So a lot of ways this is solved is by building auto-rigging tools or having scripts that automatically assemble the rig based on a definition of some sort.
So this is great. It really speeds up iteration when you can automatically rebuild the rig without having to pull an all-nighter.
And great, now that we've also got a consistent rig since it was built through one of these systems, we should be able to get it onto the animations.
If we're lucky and the bone...
names and transform parenting orientations can be similar enough, then animation can still apply.
But if it's too different, then it will end up changing the way the animation looks.
So there needs to be some fix up here.
And Ideal World, we want to be able to preserve that animation whenever this new control rig is built on top of a changed skeleton.
So often there's tools to solve this, like Motion Builder, where you can do offline retargeting and make some creative choices about how to apply the old motion onto the new rig.
And maybe in some cases you want to favor the hands holding an object.
Other times if they're hanging free, you'll want to use a rotation-based transfer.
So there's a lot of choices that can be made there to get a quality motion retarget.
So.
Thinking about the rigs again, we're not just animating a single character.
Games mostly often include a whole library of different characters, and they'll look different too, so they'll have different rigs.
So, the rigging staff will need to be building control rigs for each of these one of different character shapes and sizes, including everything from different kinds of people, because people have very different proportions, if we want to make it look naturalistic and real.
There's also creatures, can have very different body proportion types and all different kinds of shapes.
Maybe even props, vehicles and weapons.
So in an ideal world we really want to have a consistent interface for the animators between all these different rigs.
And so how can that be done when there's so many different rigs to build and the advanced functionality that goes into each one of them like changing the states if you're still using the Uber Rig solution.
It can be very, a lot of work, and a lot of repetition, and a lot of duplicated work to get those rigs nice and consistent for the animators to work with.
So a really smart way of working is to make a modular rigging system.
And that way you can share this functionality and easily make a wide variety of different kinds of character rigs that feel consistent when the animators work with them.
So how this works...
is that you can build a library of rig parts, for example, arms and legs and heads and things like that, and then recombine them in whichever way you want to apply to these different skeleton proportions and shapes and sizes.
So this is a great way to go. It can be a lot faster, because once you define a part the first time, you can benefit from reusing that work in a lot of different ways.
And...
It does end up with a nice and consistent result that the animators get to learn to work with and even speed up their workflow more by having consistency.
This is highly recommended. It's a good way to go.
And there's a lot of different ways it can be done.
I mean, one way is through scripted rig development.
And another is through templates or prefabs.
And they each have their own advantages.
So we have a nice list of requirements now.
This is what's really going to help us keep a nice iteration on our projects.
Some of the best practices for how to achieve this.
Well, number one, we can rule out the monolithic uberrig solution.
As we've demonstrated, that's just not very practical for a lot of reasons, and it ends up putting in some bad limitations in the iteration process.
Instead, modular rigging is a great way to go.
And that frees up.
us to build all different shapes and sizes rigs very quickly and have a consistent result.
One way or another too we need it to be fast how rigging gets set up and we need to regenerate and rebuild that consistently any time the skeleton changes.
We also need to push updates out to the animations when either the skeleton or the rig are changed.
And how about this one?
We really would love to be able to preserve animation when the control rig changes, but we haven't talked about a solution to that yet.
Here's where we want to talk about freeform animation.
And to get more specific about the definition, we're talking about bidirectional motion transfer.
So, when a constraint like an IK has a solving function, What's happening there is the bones involved with the IK system are solving to a target location.
We're now introducing the concept of an inverse solve where the rig controls now are built in with the knowledge of how to follow the skeleton.
So when you have both of those, you can provide a two-way road where animators can choose to use this however they want in their production.
And like our name suggests, this really frees them up to experiment.
and come up with novel solutions for these interesting situations they'll get into with animation.
So here's a demonstration of how this works.
Here we have a three-bone chain of what might be a leg extending open and closed with very simple rotation animations based in local space.
So now, we're going to bake-inverse-solve this to an IK control rig.
And the IK control rig has two targets.
There's the end effector target and the hint, or what might be called a pole vector.
So the inverse solve function for this is custom, and it can be developed however it's needed for a given constraint.
For the target it's very simple.
We're just going to align the translation and rotation to the end bone's position.
But the hint is a little bit more custom.
We need to do some 3D math here to calculate the position out front of that knee where it's based on the triangle and extends through the center and to the length that feels comfortable for animator to interact with.
And if we do that each frame, we can determine the position that it would be nice for it to bake onto.
So here we have exactly that same animation as we saw before.
but transferred onto a control scheme that's now much easier to interact with as an animator.
Now the motion is transferred to world space translation, where it's very easy for me to do something like this, and interact with an external object like the ground.
So I even added some animation to the top bone to show that the foot can stay in contact very easily with the ground.
Now we're done making our edits, we can bake the animation back to the skeleton.
And now it's playing back, once again we'll see exactly the same animation we saw before, but the keyframes are now put into local space rotation.
In this case, it would be very hard to make an edit and keep the foot on the ground, because the final position of the foot is the result of all of the parents working together.
But in this format it's going to be easier for playback in runtime, because often that's the most optimal format for playing it back.
So where this is really useful is, for example, with something like editing motion capture.
So here in this example I've actually exaggerated the driftiness by just turning up keyframe reduction.
So that left foot, or the right foot on the ground there is drifting around a little bit.
If I want to clean that up, I can add a TubeBoneIK constraint and transfer motion to the constraints.
So here we have exactly the same motion.
I can zoom in on the foot, look at those keyframes, and start making some additions.
I'll set one keyframe in the beginning here to lock it down.
And then I'll paste that same keyframe here later where the foot is leaving the ground.
So now I can zoom in and delete the drifty keyframes in the middle.
and keep it solid on the ground.
So I still need to delete the rotation keyframes, that's why it's a little bit drifty.
And if you want to take some more time and polish this, you can make a really nice looking result.
That was a quick demonstration to show how easy it is to make some of these edits.
So what we need here in order to make a really workable solution is as close as possible to a lossless motion transfer.
Now if I'm using FK, which is Single Translation Rotation Control per joint, then I can get lossless motion transfer.
This is exactly the same as we saw before.
But that's not the case if we're going to a different kind of control rig, for example an IK constraint, where there's a lower degree of freedom.
If we're going down from these 5 or 6 bones to only 2 controls...
you can imagine what will happen.
It'll have a close approximation of that, but it's not possible to get exactly the same.
So, people can use this deliberately if they're working in an artistic authoring workflow to get a deliberate result.
Maybe you want to smooth that out or reduce some of the motion in the middle of that chain.
In that case, you can transfer to a different kind of control rig.
This is all part of the animation authoring process, which will be up to the artist to use how they want for a given situation.
Now, parent spaces is another really important thing to consider while working this way.
And here's how I'll demonstrate using the simple planetary system to show the importance of working in the correct parent space.
So, with this planetary system here just showing a moon rotating around a planet, If I'm going to describe this using a flat hierarchy, then I'll be using world space translation keys.
And that's what it looks like to make keyframes that describe this motion.
So as you can see there, it gets very, very detailed and precise for the moon's position.
And you could imagine what it'd be like as an artist to say, well, what if I want the moon to speed up over time?
Then it would be a lot of really detailed keyframing animation and a lot of counter animation.
to try and get that to look right.
And it's going to be very, very hard to take some time to get it to look just right.
Well that's where it makes a lot more sense to be working in a ordered hierarchy where there's a parent-child relationship here.
This way the keyframes are local space rotation and it's simply a keyframe at the start and then 360 degrees at the end, and you can loop it.
You're going to get a nice smooth motion.
and very simple to edit. If I want to make that speed up, I can modify the tangents or add additional keyframes and it's very easy to work with, very easy to iterate.
So let's take a look at this again on the example of a first-person animation or a third-person where you're contacting a weapon.
So two hands here start out by following the gun and that's great when he's holding onto it.
But in the example of our reload animation...
Halfway through he takes his hand away.
And now we really need to be free.
So what I'll do in a freeform animation workflow is change the parent.
I have a parent constraint, so here I'm going to give it a new target.
Right now it's following the gun.
Let's put it at the character root.
There we go.
And now I can transfer motion to those constraints.
Here we go on the parent constraint.
And let's do it again on the IK constraint.
Great. Now I have exactly that same animation, but it's going to be playing back in a different parent space.
And while I'm animating this part where it's free, I can move the gun and the hand remains unchanged.
So great, that's what we really want for this part of the animation.
I did want to mention too that there's other solutions to this that might make sense in different situations.
Like for example, you might want to do an animated switch where it remembers the state where it's in the beginning of the animation and halfway through it does a switch to being in a new.
That's totally possible here.
And we're not saying you have to work this way.
But it can be cumbersome in that animated switch example to manage this state switch where it's the aligned pose.
So it's really a choice of what's going to be faster and easier.
And for short animations like this, there's a lot of advantages to working this way because it's very very fast and direct.
It can allow you to keep a very simple control rig too.
Another one of the challenges working this way is working with baked keyframes.
And this is where our curve here has a keyframe every frame.
And that's what you need to get a high fidelity source quality that's exactly the same as the previous one.
But it's hard to work with as an animator sometimes.
So, a couple solutions to offer.
One of them is keyframe reduction.
You can apply a curve filter that automatically removes keys and modifies tangents to try and keep the curve looking the same.
But it's a tradeoff between fidelity and easy to work with.
So as you can see in the lower example where there's more keys removed, the curve actually looks different.
So we're missing some of the detail.
But this is a trade-off that might be worth making if someone wants to modify the curve and they want something that's easier to interact with.
Another solution is animation layers.
So you can easily make offsets from that baked keyframe.
And then just the delta animation curve is what's worked with by the keyframe animator.
There's a lot less keyframes there to work with.
So that can be faster and more direct.
Both of these have their time and place, and when you're working with these, you can choose which one makes the most sense at a given time.
So let's talk about how this is implemented in Unity.
In 2020.1, there are some new features in the Animation Rigging package.
So these are documented here on the Bidirectional Motion Transfer documentation page.
The new commands are accessible from the context menu shown here.
And this is right inside of the menu that is both at the constraint level and at the rig level.
You can transfer a group of constraints that are bundled together as a rig all at once from that one command.
We also added some new preferences, so animators can control the keyframe reduction in both directions when going to the constraint and to the skeleton.
So that's let you choose, for example, you might want to keep easier to work with keyframes going on to the control rig, but higher fidelity quality when going back to the skeleton to preserve exactly what you created.
And all these settings here can be changed at any time for your workflow that you want to have.
So inside of the AnimationRing package, all of the C-sharp is available in the package to open up and look through and make modifications if you want, or make custom constraints.
And if you're working with this and building it in your own system, one good thing to remember is that not all types of constraints can have an inverse solve, just by the nature of what they are.
For example, some constraints are...
passive, like a twist correction for shoulder, has no animatable controls so in that case it doesn't make sense to transfer motion to a constraint.
So it's good to keep this as an optional function that can be implemented for constraints where it would make sense.
Another example are physics-based constraints.
For example, the damped transform or the spring-chained constraint.
Once again, there are no animatable controls on those.
so it wouldn't make sense to have an inverse solve.
Here in the documentation, we've given a description of what you can expect with the motion transfer, transferring to the constraint.
But all of these constraints can transfer back to the skeleton because that is simply plotting the keyframes exactly to the bones.
OK, let's take a look at how this can be useful in a game production.
Two examples.
start out with looking at an object interaction.
Here we have an animation of a character pushing a button.
It's really common in games to use runtime rigging for things like this.
So here's what I'm doing with that yellow plus shape is the end effector of a 2 bone IK system.
And I'm animating the weight of that rig constraint from 0 to 1, as you can see in the keyframes, so that it blends on and it has this moment where.
right when it's contacting.
Oops.
Back up a little bit.
Now what I can do is move that Constraint Target and the hand will always keep contacting it exactly at the right place.
This is really useful because that way an animator doesn't have to keep creating new content every single time that button is in a slightly different position.
So this is great.
But what if we try and move it up really high like this?
It doesn't look so good. We're pushing it too far.
And sometimes runtime rigging has this effect where it feels a bit stiff or robotic.
Or we might just get some bad looking poses.
So in this case, we can use some freeform animation techniques to work with it.
The first thing I might do is blend in a single pose with a control rig like this.
And then, just like I animated the weight of the IK constraint, I can bake that.
to the skeleton.
And have a nice base to work with where the pose is going to be much more natural.
I've involved the shoulder, twisted the spine, I've rotated his head up to look at that, and I can even drop the hips a little bit or whatever feels right.
Next, I might bring back the runtime rigging IK constraint and bake animation from that little button press that he gets so that it actually feels like he's activating the button.
And finally, you can transfer back to the full body control rig.
and make some polished tweaks to the exact interpolations and in-betweens of how it looks when he's animating.
So now we're done, we can save out this final clip.
And that can be a new button press high animation, which if we want to we can apply some more runtime rigging and get some movement within that range.
This was an easy correction to make where it's already looking almost good enough and we just want to make a couple tweaks so it's nice and quick to even save out a new animation that way.
Here's another example where we might have an existing run animation that we got from a motion library somewhere but we want to transfer it onto our character and wherever he goes he's carrying this big heavy suitcase and that just doesn't look so good.
There's no weight at all, and it's going right through his leg.
So we need to make some edits.
So what I'm going to do is first add a Tubo on IK constraint.
Now he's holding on to this suitcase.
Let's set some keyframes and put it in a good position that's looking about right.
OK.
Now, I can play with the weight value of the IK constraint.
And a value right in the middle, something like a 0.6, is going to give a nice blended result.
So I'm inheriting a bit of the motion from the body.
And it's not locked in space like it was before.
So I'll transfer motion to the skeleton, and then transfer it back to the constraint.
Here I have the motion transferred onto the IK where I can start making some animation edits.
So I can go into the keyframes now.
And I might want to do something like offset everything in time.
So it moves at a slightly delayed rate, and it gets a little bit more sense of a bounce and follow-through.
OK, and I want to maybe go into individual curves, like here's the Translate Y for vertical motion, and I might exaggerate that a little bit, make it a bit larger and easier to notice.
Okay, and now I can go into something like the Translate Z, which is the forward and back motion.
And I can offset that just a little bit in space.
And now it's up to animation to make some creative choices and add some quality here so it looks believable.
Once I'm done with that single 2-Bone IK constraint, I can bake it back down to the skeleton.
And...
weight down the rig to zero.
So here I have it playing back on just the skeleton.
Next I might want to go in and add a full body control rig.
So I'll transfer motion to the constraints and start making some modifications.
Because when you're carrying a heavy object like this, it would affect the hips, the shoulders, the spine would lean over in a direction.
And you can make all of these edits right here by modifying the keyframes.
Great, now we've ended up with a better looking result.
And one thing I wanted to demonstrate next, but I ran out of time, was using physics.
So a really nice thing to do would be putting the Damped Transform constraint on that heavy suitcase.
And that would just give it a little bit more sense of momentum and more realistic follow through.
So you can play with the weight of the Damped Transform to get it to be softening up that motion just a little bit and making it feel even heavier and more realistic.
So all of these tools exist within this nice sandbox of motion editing tools which is really interesting to think about how to use in combination.
So now that we have the ability to do motion editing using these Freeform Animation workflows, we can make use of these other parts that exist within Unity.
So the Kinematica Motion Matching solution can be used to generate a really nice and natural looking motion controller as the character runs around through the game, which can be recorded and then baked back down to a single animation clip where we can go in and make some edits.
Like for example, if we're making a cinematic or a cutscene.
We can then make it look custom in a certain pose.
We can send that back to Kinematica for some more motion matching, and then out to the gameplay, where we can do runtime rigging and have it interact with the world.
This is all within this ecosystem of tools that can be interchanged in very creative ways.
And personally, I'm really excited to see how people will use this and get creative and do fast iteration to make their creative projects.
So let's take a look once again at these requirements we've put together.
I'm feeling pretty good that the Freeform Animation workflows really help speed up iteration and they address most of these issues pretty well.
I wouldn't say it's the solution for every single situation, but it's one of the tools that you can now use to find a solution.
And it really adds a lot of nice flexibility to these workflows.
So looking back at the animation pipeline, the old way, it was the rigging people's job to build a control rig and to have to think of all the choices for animators.
What we've done now is give that control to the animators and let them choose how to assemble control rigs and use them for their animation however they want.
This really frees up the iterations cycle and it's really great to be able to have that freedom.
And what we have now is much more like a sandbox instead of a pipeline.
Where we've opened things up for experimentation.
And what that lets animators do is take motion sources as input from all these different places.
You can start with hand keyframe animation.
You can go back and forth to your DCC applications with FBX exporters.
And you can record physics or gameplay.
Do some motion capture.
All of this can be edited using hand keyframing, which puts it into the artistic control of the animators where it belongs.
So I'm really excited to hear how people will work this way, and I was hoping we can have some questions and answers here, but we'll just have to take that to the Unity forums.
And here's the link where you can find that.
Thank you very much for joining us, and look forward to hearing how it goes.
