In this presentation, I will not dwell too long on the past, but instead I will share with you the current shift in mindset that we are undergoing with our online development here at Massive.
And by the end of this presentation, you'll understand how, by adopting a cloud-first mentality, you'll be better prepared than ever before for a smooth and stable launch of your game.
But first, some background.
And who am I?
And what perspective do I bring to this presentation?
I'm online focused. I've been at Massive for 15 years.
The company itself is over 20 years old.
Since we can trace our online roots back 15 years, We have this long legacy and I just want to state for the record that I think legacy is a good thing because it enables us to rest assured on the quality of some subsystems so that we can focus on improving others rather than having to reinvent the entire wheel for every game we do.
So let's take a look at a typical massive game.
So there's no spoilers here.
There's just considered a normal classic massive game, whichever it is.
running on your TV or console or PC.
This game is connected to an online infrastructure that we run in data centers around the world.
Contains a lot of microservices, over 20, that deal with things like your C gaming, your character and progression, and leaderboards and what have you.
And...
Furthermore, your game is also connected to a game server, and this game server is running the AI, it's running missions and controlling NPCs, and of course also multiplayer activities.
And then of course, this game server is also connected to the online services infrastructure.
And one thing that that this enables us to do is to, or that it forces us to do, rather, is that we take care of the save game.
Since the game state sits on the game server and not on the game client, the save game is synced between the game server and the online backend.
So, with that out of the way, let's talk a bit about what's cool about our online and the way we do this.
And as you can understand, it enables us to run larger simulations because since AI is offloaded to the server, we're not limited by the compute power on the console or the client, but rather limited by the compute power that we have access to in the cloud.
This enables us to have a bit of a different performance budget than all the titles.
Mostly we focus on better graphics and better UIs, or higher budgets for this, since there is less AI that needs to be computed on the client.
And also, we have multiple clients connected to the same game server, so since they're already there, it's easy for us to add seamless multiplayer.
And it's up to us whether or not we flip the switch if we oversimplify it, so to replicate the players so you see them.
This happens for example in the division when you can walk into a group of players and that's because they're already there.
So now then let's talk some consequences on our online strategy because of course there are choices and there are pros and cons with the way we build it.
And these consequences are, well, to start with, it's a complex mesh of online services.
You saw a very simplified picture of it earlier.
But it is quite complex, and there's a lot of interdependencies in there.
Furthermore, since the AI is running on the server, we cannot have an offline mode for the client.
The client has to be always online.
Otherwise, the game simply doesn't work.
This means that the online stability is absolutely critical, even for a single player.
So we try to think of our online infrastructure as critical infrastructure.
And furthermore, since all the teams, regardless of what you work on pretty much on the game, You're affected by online, so this means that you need to be careful so you don't break things for other people.
If you break the game server, all the players on that game server are kicked out, for example, even from their single player session, which we don't want to happen.
And we need to ensure that everyone is up to speed on the latest on anti-cheat technology, etc.
So here at Massive then we of course have all eyes on online and to deal with this we have a very efficient live ops organization here at Massive who take online seriously and the quality of the service to our gamers and adding more stuff to the game as we go on.
And we do lots of open betas etc for testing. For example the debate on the division was the biggest ever for a new game and that reached over 6.4 million players which at the time was a record.
But let's go back and talk a bit about development.
It's a story of unintended consequences because since we've been doing this for so long, and as time goes by, different teams may start to use different testing methodologies, for example, and different technologies that are best suited for their specific task at hand.
And then also different tests may use different stubs and shims.
And different teams may test on different hardware.
So there's a lot of special cases.
And what can happen then is that special cases become the norm rather than the exception, which increases an already complex system.
It results in lots of isolated testing, which is not always relevant for the shipped game.
And as we grow as an organization, fewer people understand the full online system.
So let's talk about some examples of unintended consequences.
Well, I mentioned the save game system earlier.
It's very important that it works.
So we have built it to be rock solid.
It's essentially a SAN.
However, this SAN came online very late in the project.
So a local stub had to be used in most cases.
during the years that the game was in development.
And the team size is not large enough because at the end of the the latest project we did a server could handle 1000 clients because of all the performance works that had gone into it. But we didn't have 1000 members on the team so on a playtest we couldn't actually fill a server. We had to rely on bots.
And bots don't necessarily load the servers like players do, so it complicates capacity planning for launch.
And some stubs and shims were tested more than the system and that were shipped.
So we were not always focusing on testing what we actually shipped.
So that was actually very much something we wanted to improve.
But furthermore, this is more than a technological issue for us.
For many developers, live is a special case.
Because it's not the case you see while you are working on the game.
And we have different processes, therefore it has evolved, different processes for when we develop the game.
We have built-in data managers handling ops with QC monitoring.
But then when the game goes live, we have our live ops organization that I mentioned earlier.
with a war room for monitoring.
So it's, it's quite different.
And, and of course, few developers actually see both sides of this fence.
So it's hard to anticipate the needs of the other disciplines.
So.
We needed to be better at testing what we ship.
And we have this mantra now that we test what we ship and we ship what we tested.
And this sounds very easy, but really, it's not.
Because how often do you test the client?
Well, you do pretty much every day, I'm sure.
The services, your account systems, and any other thing you depend on, I hope you test it a lot.
The game server, I also hope you all test it a lot.
Auto scaling, how often do you actually test that?
And do you also test scaling down, which you most likely need to do at night?
Do you test disaster recovery, or how often do you test that so you know it works?
And furthermore, how often do you actually test the war room?
So what happens here is that as a developer, As the impact on life increases, our ability to test decreases.
So the less something is tested, the more it relies on a few experts with specialized knowledge.
And these are people that can get sick.
So it's a potential liability and a risk that we don't want.
And then if you couple this with the way the industry is changing, the way it has been working for a long time is you spend a bunch of years in development, making your game and ship it live for a while.
And of course, as you all know, this is changing shorter time in production, longer time in life, and ideally no end of life.
So this new era where Where AAA games are live longer than they were in development is pretty interesting.
It has a couple of consequences.
For example, few developers stay on a project from the beginning to the end.
Simply because people have different interests.
Some people want to build the foundations of the game.
Some people want to maintain it, etc.
And what happens then is that processes, or what can happen, is that processes tend to be even more strict and formal to compensate the natural knowledge loss that you have.
And we really want to break this chain of complexity.
We want to focus our attention on one single unified workflow, where we define processes in the most strict and formal way we know how.
And that is with code instead of with people.
So if we take one workflow, what we really do is that we start looking at the ideal live workflow, and then we work backwards from there.
And we try to apply that everywhere.
And that means that we have a single code path for all features, a single way of debugging your game, a single way of monitoring your game.
Deploying your game and testing your game.
And this is all defined in code with automation by default.
So let's talk a bit about this infrastructure as code.
In practice, what we do is that we're using Terraform to script the complete infrastructure.
And we're using Google Cloud Build to build virtual machines.
When we do this, we needed to embrace infrastructure on demand, which was a big shift for us, since we relied completely on on-prem just a couple of years back.
So, start and discard is what we call the set of tools and principles that steer this attention to one workflow, where you start complete live-like environments, You use them whatever person you need, and you can start how many you want.
Every single team member can start their own complete LiveLike environment if they want to, and if they need that for testing.
And then, of course, you discard it once you're done with it.
So what do I mean then with a LiveLike environment?
Well, it is a copy of the real infrastructure.
It is defined in code, so it's easy to copy.
It's easy to instantiate.
And in the cloud, so you boot it up with the exact same auto scaling rules, exact same firewall rules, etc.
And the exact same way to deploy, debug and monitor your system.
As an example, if a gameplay programmer wants to add a dashboard to Grafana to monitor a gameplay feature, they would check in that feature and the dashboard.
at the same time in the code base.
And then at every start and discard environment thereafter, they're able to monitor in real time that specific metric that they wanted to track, regardless of if it's a playtest for a handful of people or going live.
So, Since this infrastructure then sits in code, it can of course be branched and tracked together with the code that uses it.
And it means that we can have 100% repeatable deployments, no special source on any of these servers, no special cases, and everyone can go in and see.
how it works. If a developer wants to add a new service that uses a new firewall, or a new TCP port, for example, or UDP port for that matter, you'd go into the infrastructure, open up the firewall for that port as needed, and check it in together with the code and have it submitted for review. And it's all done by a single person. Can be.
So, In order to do this, as you understand by now, we need to have a cloud native mentality.
And we use 100% Google Cloud currently here at Massive.
And I guess some of you will react with, oh, using 100% cloud of any type of cloud is very expensive.
And yes, it can be.
Typically it is if you move all of your on-prem and just boot it up into the cloud.
That's not exactly how we do it, which I'll cover later.
Because once we are inside Google Cloud, it makes sense to use some other on-demand services.
And in one case, for example, We replaced the subsystem that we had built ourself with on-demand service on Google Cloud, and that saved 90% of the operating costs for that particular system.
And furthermore, since this particular system then, and any others like it, or on demand, there is no overhead cost of actually letting the entire team boot up their own complete LiveLike environment, because you don't pay until you actually use it.
So that's a pretty nice benefit.
So we test what we ship, and we ship what we test.
We ship what we test.
And then what we did was that we moved Say Games to Google Cloud Storage.
I mentioned earlier in the past we had the SAN.
Just made more sense to put it in Google Cloud Storage and have it there.
Furthermore, we have smaller game servers so that we can fill them with human players at every single playtest.
And then of course with With design and technology inventions, we hide the fact from players that it's a smaller game service. It's not necessarily something that you would notice, but it means that we can have more relevant testing of everything every single time we do it.
Furthermore, it has a lot of nicer benefits.
It's easier and faster to scale down when you have fewer players to wait for, that leave the server, et cetera.
Every single playtest utilizes the same tech from deployment through monitoring and debugging as the live game does.
And the process, as I mentioned already, it devolves along with the code, and we can branch it, which I find pretty cool.
And knowledge transfer is improved.
With new hires, they can get the entire complete live experience in a safe environment.
We can break stuff and have them troubleshoot it.
Or they can break stuff just exploring how stuff works.
And then discard it if they want to start up a new one.
We start learn under proper live-like situations.
So every playtest is then an address rehearsal for being live, and sets the full path from build system, deployment, live monitoring, debugging.
But we can also do some pretty creative testing that we were not able to do in the past.
For example, we can simulate Australia.
Or rather, we can simulate how it feels for Australian players.
Normally, what you would do is that you add latency to the client and you play test and you see how Try to get an idea how it feels to play the game in Australia It's not really feasible to send devs across the globe just to test the game so that's normally how you do it but But a complex mesh of services all have different latencies So it can actually be quite complex to simulate Australia in a proper way.
And we can actually test better than that. So let's take a look at that.
Here we have a classic setup, core servers in Europe.
And there's some third-party services in the US and game servers in Australia.
And then the players are normally here in Australia.
We'll start on discard, which we have defined in code.
We can easily flip the table, put all the core servers in Australia and the game servers nearby our office, because then we get a true feeling for the Australian experience.
And since this is sitting completely inside Google Cloud, it's literally five minutes to change.
So it's very easy to test this out.
So, yeah, so maybe I actually went a bit ahead.
Nevermind the animations, that was that.
And then, of course, we have the same dashboards available as we had before, and now we can see the effect in these dashboards on top of playing the game and feeling the effect.
So, hopefully, the experience for Australians and others are improved.
But...
There will still always be some special cases.
You saw the third party services that I mentioned in this example.
We always have third party dependencies for a multitude of reasons.
It may be, for example, telemetry or tracking.
So the way we deal with this is that Where they have a dedicated staging and a dedicated prod environment.
We have a set of proxies that sit in between our start and discard environments and the internet, if you will, where we proxy their traffic through.
And this means that all of our start and discard environments have the same view outside, even though.
We cannot control the insights of the third party infrastructure.
But it's still, it's a nice compromise.
And it goes to show that we really try to minimize the special cases in our infrastructure as much as possible.
Yeah. So.
What would you do if you were to, if you want to embrace start and discard?
Well, you need to stay clear of special cases.
And you should use a single code pattern and so on.
But at a fundamental level, it's a shift in mentality that's supported by technology.
It's not purely a technical solution.
Cloud native and on demand is critical for the success of it, and in order to be at a cost parity or cheaper than on-prem hosting.
But remember that if a programmer needs a special case for feature x, then you're probably doing it wrong.
Hopefully, you can find another way.
You have a single code path, you do it the same way as when you're live, and then stick with that.
It's worth doing, to not have to maintain multiple parallel code paths.
And if you also plan to hand over to a live team when you're done, when you're shipping the game, then you're also probably doing it wrong.
Because as you see here, live is truly part of development.
We try to be live from day one of any new game.
But then, what's maybe next in store?
Or what's for you, my challenge to you guys?
Why stop here?
Because what you can do as a logical next step is that you can define all of the infrastructure in code.
Do you need a port opened?
You can do it yourself and submit the change for review.
So in a way, you could open source Office IT.
You could have lower upfront costs by using compute or lower costs by using compute where it happens to be cheapest at the moment.
Anywhere around the globe, there's spot pricing, et cetera, that is way cheaper than normal.
And you may use that for your entire build pipeline, for example.
Furthermore, we all know that Stadia works great for games.
So of course, you would be able to stream your level editor.
So you could have your workstation in the cloud and be able to upgrade it with more memory, etc. as you need.
And working from home would then or could then be the norm rather than the exception.
where you pretty much just need to fork the code in order to create a new dev team.
But that would be what I call Game Studio 2.0.
And that is the subject for another talk than this one.
That was my presentation about start and discard here at Ubisoft Massive. Thank you and stay safe.
