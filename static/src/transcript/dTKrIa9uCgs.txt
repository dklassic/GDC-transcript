I assume you're all gathered here today to worship Cat Thulu.
Welcome congregation.
Or perhaps you're here for the Sucker Punch facial performance capture pipeline, as seen on Infamous Second Son.
So, the Infamous series is...
The second sign is the third installment of the series, of the Infamous series, where basically you have an open world superhero game, or super villain, depending on how you play and the choices you make.
So, karma plays a big role in the game, not only in the choices you make and the gameplay, but it affects the story and the outcome and how your powers progress.
The Infamous series, with its comic book roots, has a very strong narrative, as all superheroes need a strong backstory.
And with the jump into new hardware, we really wanted to see about pushing the facial performances, trying to just push the quality on all fronts.
And as you are probably familiar, if you're here, with facial animation, the higher jump in quality you get.
all fronts. The more important it is to have solid facial animation, you know, because you'll quickly dive into the uncanny valley. So what we had before, the previous two infamous games, we had the real-time cut scenes, hand-keyed animations for both cut scene and in-game play, basically to be driven by joint-only facial rigs. So moving forward onto Second Son, we really We wanted to dive deeper into the story with new hardware.
We felt like we could kind of push in closer to the characters and have a more compelling story, bring some actors in, actually capture, try to capture the subtle nuances that will help carry, connects you with both the story and the characters and hence the gameplay.
Ultimately giving the gameplay more meaning, giving more motivation to the player.
and just give a richer, fuller, more cinematic experience to the game.
So we were exploring facial motion capture.
It was on the table right at the beginning.
But if we were going to go that route, we knew that we wanted to have bodies, faces, eyes, and audio, and multiple actors captured at the same time.
There are methods where you can get some facial capture that really require that you're in an isolated area.
Separate from your body capture, your face would be scanned and tracked and applied later.
So you end up having this disconnect between your body and your face, as well as a disconnect between your actors.
And lastly, it was still a requirement that this all runs in real time.
So here are some highlights of our results.
No spoilers.
The game is released on Friday, by the way.
We're all excited.
So there you go.
So we were looking into the various approaches for facial performances.
And we could have gone the route we went before with hand-keyed animations.
There's a 3D scan per frame performance capture, kind of like a, I guess I would call it like LA noir style, where you're captured in a booth.
I think that's a method where you can really get lots of detail directly from the actor, but there are, like with all these things, there are their own limitations, and we did want to capture the body and the face and the other actors all at the same time and on stage.
And using that sort of method, you're really kind of isolating, narrowing what kind of performances you can get.
They're supposed to kind of keep their heads still.
They have to use reference objects for where their eyes are tracking and it's kind of all pieced together.
So you don't want to go big with animations because that might not necessarily come together well.
And also there's a really consistent topology or UVs with that method without considerable work.
So retargeting to other characters is next to impossible.
And having animation on top of it and kind of fixing anything.
would basically just require a reshoot, because you really can't do that.
So there's other methods, muscle-based simulations, and things that have been done for feature film stuff that has relevance, and it was really more of an R&D project that we wouldn't really think was necessary, considering the last option that we did go with, which is the data-driven, example-based approach that is based on a lot of things that have already been.
proven out and has worked well in feature films in the past.
So basically, what the data-driven example-based solution is, is you have a bunch of data, in our case, a bunch of scans of your actor going through a range of motion to get to infer the physiology of the face.
All the little subtleties that happen with the muscle sliding over bone in fashion.
And scan, you know, when you get it from a range of scans, you can kind of get all that stuff for free.
The scans are roughly based on what's called a FACS system, which there was a PhD named Paul Ekman that completely unrelated to CG, he had described all the muscle movements in the face as independent movements that defined everything you could do with the face. Anything else was just a combination of those. So it ends up being a really good.
library for what kind of range of motion you can get out of the face that can be used for getting quality scans to really fill out that data set of poses that you're going to use to ultimately blend and match to what the actor is doing at any given frame of motion capture. So to make that happen you need a consistent facial marker at least the method that we had gone through there's a number of marker dots that are placed on the face.
It's aligned with muscle regions and directions that the face moves.
Per actor, you kind of align it in a way that it's following their muscle flow and kind of between the creases, so it's always exposed to the cameras.
I think of cameras, I'll go into that a little bit later, but we did use head rigs that are wireless.
Head rigs have...
The camera's pointing back at the actor that's recording all the markers on the face, and they're up close enough to get that high fidelity that you want for the subtle movement of the face.
So that same marker set is what it's used on a day-to-day basis for the motion capture itself.
So what we do is we actually get a vacuform mask, the default pose of the actor, and there's where the marker set is, we basically extrude little points on that model and have it sent off to an engineering firm that will print out a vacuum form mask and drill holes in each one of those spots so we can use that as a template, you know, onset at the beginning of any day of motion capture and they'll correspond directly to our facts data, all of our scans.
So as part of that, we need a method of taking that facial marker data and obviously coming up with a nice line of shapes that's reasonable.
That's where the solver comes in.
Basically just applies that data, in our case, to a facial rig, where it can also be used for animating on top of it.
You can replace animation or just animate on top to tweak or change the performance as needed.
And lastly, if your game character doesn't look exactly like your actor, you want to vary it up.
you'll have to retarget that somehow onto your game character.
So I'll go into that also in a little bit here.
So here's just a sky view of the pipeline in general.
The whole left column is basically about acquiring the shape.
So I'll describe that next.
We're just kind of floating over the points here.
We have our facial motion capture, a picture of Troy Baker, the actor wearing the head rig.
So all the scans are combined with our solver, which in our case is a post-based deformer.
And the motion capture is applied to the rig so that it's directly connected to this solver and just outputs a normalized number of shapes that matches the current configuration of motion capture.
And you can do additional things on top of that, additional deformers, simulations.
Kind of the sky's the limit with this method.
We actually have joints per facial marker.
There's nearly a couple hundred facial markers, and so we've got a pretty solid skin cluster as a basis where this all starts off.
And all the deformations that we have are pre-skin cluster to give you the vertex level changes that are inferred from all those scans that we got.
So...
I'll come back onto those details later.
Just jump ahead twice.
Okay, yeah, here we go.
This is the fact session of this giant chart here.
And basically we have a day, this is a one-time setup, you know, a lot of stuff where we'll scan the actor going through these series of...
fax poses and combination poses, and sometimes poses that are just unique signature looks that he's gonna have while in character.
So we're trying to get as much data that's gonna be repeat and useful.
And the method we're using, it doesn't have to be quite perfectly isolated shapes.
Like a lot of scan processes really require that you kind of isolate those motions so they kind of add up well.
But the problem with the blend shapes, if you're trying to make them add, and not keeping them.
normalize, you're going to end up with shapes that just don't work well together. So I've seen huge networks of fixer shapes that try to make combinations work together in order to kind of make up for where shapes don't work well together when actually they should be morphing together, you know, where they're not adding. They're all normalized so that you never have the weight of any one given shape over one. So, uh, just going with the process of how we get the consistent topology of all of our fact shapes.
Each of these scans are millions of points.
If you looked at it in wireframe view it would just look solid.
So obviously we're not going to play that in the engine.
We want decent topology, it has good flow, much lower resolution, a lot more resolution than we could have on previous games.
The actor...
doing these scans, you know, he can move around and he'll take breaks and come back and you know, they're seated. So there's a stabilization process that has to happen to make all these shapes relative to the default pose. As we take a low-res version of that face, based off the default pose, we make sure there's a tight correspondence between the markers from that default pose and all the other shapes on that low-res version that we make. It's basically extracted from our...
our default pose to start with.
So that's, once that's set up, we'll use a warp algorithm that'll take the marker points and basically warp the geometry and snap it to the surface of each one of those shapes.
There's a subdivide and then a snap where it just projects itself onto the surface to pull those extra details back into the shape.
And so, each of these shapes end up being combined with the back of the head.
We have eye socks and mouth socks.
nose socks and those basically are joined and kind of go for the ride as you run through each of those shapes almost like a as a blend shape initially just to have it pull the neck and mouth along for the ride so they that they're staying combined so you just kind of copy them off along the way You know ultimately you end up with your final topology so you have with this process as we're starting with the low-res You have a version. I have two versions of LOD of low-res and a high-res that can be applied.
With scans, there's always problems with the eyes and around the teeth, where there's a lot of reflectance.
You have eyelashes, and that causes interference.
You can get a lot of noise in those areas.
And the lips in particular, you have areas that are not always exposed, the inside of the lips.
And you'd be surprised how much that actually shows with really expressive emotional performances.
So we'll have a modeler go in and, based off of the scans, clean up that area around the mouth and make sure that they maintain their nice blendability across all the shapes.
That's one area that's neglected quite a bit, which when you get more realistic characters, it's really important to make sure that you're maintaining the thickness of the lips, that the volume changes as the lips kind of scrunch together.
And so you really need a modeler who understands anatomy.
And ultimately, you go through and you can kind of run them off as a bunch and see how they all blend together.
And here's more stats on our scanning process.
You have from 60 to 70 scans per actor.
As I described, we apply that onto consistent topology.
The hardware we used were two linked structured light scanners.
They can get a lot of detail out of the face.
We had them scripted to kick off in succession.
So there's four cameras, two per unit, which takes a couple seconds to a few seconds.
We had faster machines and everything, but it was the cost of getting that much detail on the chief, I guess.
But it does get a lot of detail, and you get results right away.
So you didn't have to...
if the actor moved a little bit at that time, you'd see it right away.
You can just have it taken again.
So that was the process we used.
Again, that's a scanning process that's done in a static volume, you know, they're seated in the chair.
It too has to be stabilized, you know, against the defaults that they all blend well.
That's quite a process for the fax poses.
So here's Troy Baker again.
Again, hopefully, Delson Rowe.
Series of his fax poses.
So quite a number of shapes, mouth open and closed, giving it a range of motion.
And because each of those markers has a joint there, and actually every joint has their own pose space deformer, each one of those shapes can contribute uniquely to any other part of the face.
So these are all triggering independently to give you basically an infinite combination of what the face can do.
Thank you.
Here's a view of that consistent topology remeshing that I was describing.
On the right, we have our low-res mesh.
Same dot configuration.
It gets warped and snapped to that dense one in the middle, which I think, like I said, I couldn't show that in wireframe, wouldn't be able to see anything.
Then that's combined with the back of the head, the mouth socket, and such.
So here's this little video of I'm just going to plan through some of those blend shapes.
I just made a blend shape out of the shapes so you can see them in succession.
You can see how they're working and decide if there's any editing that needs to happen.
It's really important.
This whole process, you really are following those marker points carefully through the entire pipeline because you want as much consistency as you can because those are going to be your drivers and you want them triggering the right shapes.
So I've already described that marker correspondence.
You can see what the custom vacuform mask looks like here on the lower left.
So it's got this series of holes, you're just going to go on there with a marker and place the dots.
Of course, you want to be careful doing this to make sure that the mouth is consistently in a nice default pose, the actor's not trying to talk or anything.
We actually had gotten some back where I think the marker set was a little off and I'll animation kind of look like.
He had a droopy mouth, but luckily we have just animation controls.
We can tweak that right back.
So I think I've run through all these points.
I can move ahead.
So now I'll be describing the motion capture process itself.
These wireless head rigs are really lightweight.
They're wireless.
You can just swap in cards.
a couple hours of motion capture if we actually would have to swap it and the batteries swap out and such as needed.
So some of the other requirements of these head rigs is that, well actually I'm jumping ahead of myself, I'll just follow through here and follow the slides.
It's already described that we want everything to be captured simultaneously.
So how do we keep all that stuff in sync?
Traditional film, you know, we just use a piece of hardware that just has cycles that's always ticking at the same moment.
And most cameras you would find on a motion capture set are going to allow all the equipment to capture at the same time.
So they're all in that same cycle and they're stamped with the time code.
When you're going back and looking at the data, you'll see that they're all lining up as needed.
Kind of a follow-up benefit of having those facial rigs is that you can get video right from any one of those cameras, or a combination and stitch them together to project onto a temporary piece of geometry, so you have basically an animated texture map of the actual performance, and you can have that delivered right away with the body motion capture, so that you can just continue setting up your scenes, setting up your camera.
for cut scenes and while the actual tracking is taking place that can take a little bit longer.
As far as these head rigs, there's different ones that are out there and depending on your own pipeline, you might have different needs.
We use the multi-camera setup so we could actually...
triangulate the positions of each of those markers in 3D. Other methods that have been used for tracking use things like optical flow and feature recognition to try to estimate the movement of the face.
But then you can have motion capture. Sometimes there's noise that you have to deal with and try to filter your data. Luckily when you have cameras up that close that's not as big of a deal.
The helmets can slip around, you know, they're on there pretty snug, but they're not, you know, bolted to their head, unfortunately.
They didn't agree to that.
So, they can slip around from day to day.
They're going to be aligned slightly different.
So, a stabilization process that can constrain those back to relative to a default is necessarily part of that process.
So...
You can also have missing or occluded data.
You have wrinkles and folds in brows and stuff that can just hide marker data.
So a way of dealing with the missing data.
And then with facial rigs, the lighting conditions are very important.
The stage that we were on had the entire ceiling was emitting light, and the walls and very nice diffuse lighting.
So pretty much from any angle, you're going to get a decent capture of the face.
Other units have.
lights that are on them projecting back.
It can be distracting to an actor.
But again, you know, depending on the conditions and the stages that you're on, you'll have different needs.
Which brings me to the retargeting phase.
So retargeting is can be done in a number of ways.
I don't think I'll go through describing all the ways we didn't do it, but basically for our method we retargeted both the shapes and the facial motion capture.
You know, so that the facial motion capture is basically moving in the same space as your game character version, so it's triggering your game character shapes.
And it's all applied into a rig so that since we're actually using the joints connected with that marker movement, it's all...
incorporated into a rig that works well using the actual game character.
So here's one of the actors you might have seen in the highlights there.
Her name's Karen, and we have her Native American counterpart in the game.
Thicker face, baggier eyes, wider nose, more ethnic look.
And using these two defaults, and all the shapes we got from the scans, we can generate all the other shapes.
So basically, it uses a proportional constraint to take every edge in the face.
And that relative difference between the two defaults, it does a solve and iterates, basically, to come up with where all those would settle as moved through each of those other shapes.
If Adrian stops by here, he can answer any questions about that specific solver that he had created.
He explored a number of different constraint types.
One nice thing is that with this method, it also was retargeting the motion data, which is very low res, relative to the really high res shapes.
The marker points actually line up exactly spot for spot, so we know we're triggering the right shapes with the right data.
So as far as, How we're applying those shapes, I mean you can sure have blend shapes, they're easy to animate.
Has this, you know, possibility of having the over-adding issues that I was describing, if they're not normalized.
Any shape-based system, if you have too few shapes, you can get cozy results.
The paths, as they're blending between each other, if you have too few shapes, can...
kind of take a wonky direction.
I don't know if you've seen that in animation where there's something just a little off, it just kind of takes a weird path between poses.
So that's a problem with any system if you don't have enough shapes.
But we do have a method that we can kind of correct for some of that just in case there is areas in the pose space that you're not actually getting data to avoid that wonky path that it could take while blending.
So ultimately as our solver, it was a pose space.
deformer. This is basically just deriving shape weights from your marker data.
At its heart, it uses a radial basis function, which you look at the references to go deep into the math on that if you're interested. But ultimately, what's really nice about it is that the weights that it outputs are normalized from the most relevant, the closest shape pose space from your, that matches your marker configuration on any one of your shapes will trigger in that section.
Remember, we're doing a different pose space per joint. So if you have a configuration that exactly matches the scan, you'll get that exact vertex data applied in that area. So and this is applied pre-skin cluster so that The heavy lifting is done by the joints and then the extra data is on top of that which Helps for some efficient Compression because we can stream that stuff into the game where the joints are just animation that's played back in an engine, but the vertex data is Just that extra level of deformation that can happen on top of that But you have the joints there for a level of detail if you want as well So as far as training this pose space deformer, you have all your shapes and you have your marker configurations.
And basically you need to fill in your deformer with all its information.
It's basically just a matrix that's set up of rows being shapes and columns being vector data, coefficients are the shape weights, and a process called a single...
a value decomposition is run on it, which gives you a transpose matrix that basically just says if you have a bunch of new marker data from an arbitrary pose from motion capture, you just have that lined up, you multiply it by the original matrix, and you're going to get pose weights as a result that can, within the deformer, it will blend those shapes together to give you an output.
It's by this deformer node, in our case in Maya, so you have a direct connection into the node and you have a direct connection out to your shape.
And it's just this blended result of all the most appropriate shapes.
So here's just kind of a way of visualizing.
Check up time, my timer did not work well, so I'm gonna try to speed up as I can because there's a lot to cover.
Here's just kind of a visualization of what a post space.
deformer would be if it's really simplified down to 2D. Let's say we picked a point exactly on one of those markers and there's a bunch of poses that where that vertices is moved around in a different spot and so that that would represent, you know, the pose space here and my little red circle with the fallout there is some arbitrary pose and from this, that radial basis function The beauty of it is that it kind of reaches out into a radius in that space and gives you a kind of a capture volume of relevant shapes.
And so the closer a shape is to the center, it's gonna be driven to a value of one while all the other ones fall off to zero.
Now it's kind of like a weighted average, but biasing towards the ones that are closer to the center.
So it kind of cancels out any other shape as it gets closer to an exact match of your pose.
So let's say you had in this scenario that single vertex that has a wide range of motion in y, but very little in x, you can do what's called a regularization of your parameters, which you basically just have an input width scale for each of your parameters that brings them down to very similar space, so they're similar values, that instead of an oblong circle trying to capture that really wide stretch in y, you really have it.
and normalize down in space to where you're using a circular radius, you know, the radial basis function.
So, one cool thing about your input width scales is that you can also, if you scale them all together, you basically are increasing the volume of your capture space for influence of shapes.
And so if you have holes in your post space, you have kind of an even distribution, a little farther apart, if you don't have maybe enough poses, you scale that up, it's going to reach out and capture more and get a nice blend of those shapes.
This is something that could actually be exposed as an animator control.
If you have a nice distribution everywhere, but you just have a few spots where there are holes in your pose space where you just didn't have a pose you could actually do a scan that was relevant for that pose.
marker set. You just scale that up until it's getting influenced by the nearing poses and you can either just animate it and use it like that or just copy off the shape after you scaled it so it's blending with those other poses and put that right back into the pose space deformer as a new shape. So here's this little test we've done all this training to get our pose space deformers where we iterated through our shapes we had surface locators placed at each marker and so what that would do is Go for they'd go for a ride as you're running through all those shapes And as Susan blend shapes and we're just copying off those values and it just gets fed into the deformer. And so We took those shapes and just randomly picked different shapes over a thousand, you know combinations of just picking random shapes so and blend them over over time to see that your interpolation of your pose spaces is working well and it should obviously look like your pose when it's at the same pose that your blend shape is at. So here we have the actor on the left, their shapes, the retargeted shapes on the far right. The other ones are moving joints, so it's actually a skin cluster, but on top of that the vertex level stuff per joint is kicking in to make it actually match what is happening on the right. So it isn't a blend shape, it's actually using that pose space to form it.
An example of some of those poses is if they were only a skin cluster, if they didn't have that extra level of vertex deformations from the post-space deformer itself.
So just kind of go back and forth. You can definitely see on the lips, you know, the amount of difference.
In the lower left there, you can see the volume of the lips is totally lost by just dragging around joints.
Obviously when you get more realistic, when you're aiming for more realistic...
animation, the lips are a particular area that could fall apart really quickly.
So this is a great way of just really pulling out the real physiological detail at a vertex level.
So we have key drivers out of those markers that we can divide up the face into.
regions and each of those regions use a different combination of these markers based off of distance and Offsets and then another layer on top of that one or you're down at the per joint level Uses those key markers and their own position So you can have more fine and control on top of that I'll describe that in a little bit more detail the eyes are a separate thing you might have noticed the eyes weren't blending in that one version in the middle it's because they They have different drivers, your eye rotation and your blanks, the direction you're looking, as well as the markers, so for squinting, as those distances change, they have their own set of drivers that are trained for those separately.
So kind of like I was mentioning, part of your process is fine-tuning your pose space deformer.
So you've got all these shapes in there, you're going through them and they're not quite hitting all the poses like you'd like them to.
You went through that maybe blend shape test and you see that.
Well, it's not really interpolating the way we want.
So we have a method that kind of goes through and regularizes all those parameters so that they're working in the same space as the starting point.
And you can kind of scale them up until you see that they've captured all the spaces you needed.
Again, like I described, there's a whole bunch of really nice distribution of poses.
There's just certain spots that don't have a decent pose.
Again, just scale them up at the pose until it finds the neighboring shapes and copy them off, put them back in the system or even, you know, with rigging you can use a lot of things with pose space, which is really awesome is that you can actually, we didn't do this, but you know, you certainly could go through and just say wherever there's a gap in pose space, let's train it to ramp up those scales on the fly, you know, so that it's normal default scales But then when it gets close to an empty posed region, it'll just automatically branch out and kind of encompass the neighboring shapes.
I can briefly mention weighted PSDs, but basically this separating of the joints into, or the pose-based deformers into different regions is basically a way of getting a similar effect for those that are familiar with the weighted PSD.
So here's our example of where we had a hole, and our post space is not quite reaching.
So you can imagine, if we just increased our scalars, we'd just increase that radius and encompass those shapes.
So we have our one-time training phase where it goes through with our surface markers going for a ride.
We have that matrix all set up.
Once it's set up, it's just in the rig.
The nice thing about it is that as you move your controls around on the rig, it's actually interpolating.
Those same shapes so that's One things is particularly about doing this layered approach of using key markers is that you have Individual controls on the face at some of those markers you can just move around I'll see the example of that a little bit later how that works, but I Mentioned that the efficient streaming here, that's that's Basically, the fact that you're having skin clusters doing the heavy lifting and you have basically small offsets are the difference for your deformer, which you can take advantage of GPU decompression to stream that in really fast.
We had about 12,000 vertices per actor.
I think we had up to three actors at a time.
We could do things like on some characters, like the Betty character, we actually did full sims that this would drive the face, but then on top of it we could do a simulation, loosen up the skin and just allow it to be dynamic and get some of that extra cool stuff out of it on top of it because the fact that we're just streaming it in afterwards.
So one thing about doing this per joint motion that is really cool is the fact that you can actually get smoother.
natural arts of motion in this way. I'll describe what I'm talking about here.
So again, we'll go back to like a vertice that's right on one of those markers.
On the lower left, there's a blue dot representing a pose. If you're right on that pose, you know, your face is going to match up perfectly with that pose.
Along the way to that other pose, along at the top, you're going to get a another perfect match of a pose. But when you're in between there, if you don't have poses specifically trained all along the whole way, it's going to be basically interpolating between the most relevant shapes nearby in space. So you can see it's influenced a little bit more where it's curving over to the right. And the shapes look good in themselves, but if you didn't have quite enough shapes, it's going to take a slightly different path while it's blending between those shapes. And so having these joints...
pulling back to the actual 3D tracked locators that you have, just basically warps the whole thing, gives it a nudge to it, so it's 100% accurate, if your track is accurate, at each of those vertices.
And so, everything in between just goes for a ride with a nice fall-off.
So, here's just an image of what I'm talking about.
Just to kind of go back and forth here, you can see that they're not quite aligned with the surface.
Here they are.
And so that's just, we actually have a control per joint that allows the animator to say, well, the track was really doing something weird here.
It's solving well, but I don't really want to follow the track there, but you know, the brows, I'm getting a lot more dynamic motion on that, so I'm just gonna have them attract to the markers and just get that more fluid dynamic.
You know, the face moves around very fluidly, and having that directly tied to the actor's motion is really important for realism.
So I'll just play this.
So this isn't in the camera view, so it was used in the cut scene, so you'll see a hand coming in here that kind of snaps into place awkwardly.
But you get the idea that you can see that those dots stay with the face, whereas this earlier version...
Hold on, did I click the right one? Yeah.
It's right here.
So with this version, you can see that as he tilts his head down, there's a gap between his face and the actual 3D position that represents it.
an inaccuracy. We didn't have enough poses to completely make it that tight, but that little nudge that we gave it, you know, makes it as accurate again as your track is.
So that brings me to the fact that we used a 3D track versus a 2D track.
The 3D track does require four cameras, you know, to get really good coverage we have found.
But the benefit of the 3D track is that it allows you to verify your data.
We have, you know, the actual track points, they're right next to the face.
And as your face is solving, those should be lined up at least very closely.
So you can, you know, tell right off if you're not solving very well.
And then, you know, the additional benefit of just being able to warp right to those marking points and give you just that extra level of a detailed fluid motion with just taking the normal natural arcs.
right from the actor.
Lastly, and this is pretty key, I think, is the fact that I mean, the lips are the most moving part of the face and it actually moves in Z.
You know, you can pull your lips out and your jaw forward and all this.
With the 3D track, you can really take advantage of that extra dimensionality in your drivers so that, there's me making some funny faces, but it kind of shows like, roughly, you can make...
the same pose from a front view and have it look very similar.
When I'm pushing my lips out, one of them really tightening them, but kind of keeping that same rounded shape.
Just moving your jaw forward even can do the same thing. And here I'm actually rolling my lip in and pushing my jaw forward and rolling it out.
From the front view there's hardly a difference noticeable, which means if these two different shapes are getting triggered by your same drivers because they look the same, and 2D space, you're going to get wonky interpolation in your shapes because it's applying shapes that that are irrelevant to the actual pose.
So that brings us on to the facial rig.
So I was mentioning that we have key markers. They're carefully picked as, all the drivers are carefully picked per region to use the most relevant either distances or vector directions of movement of those markers so that basically through motion studies of seeing where the face moves independently and some experimentation to find the best combination of drivers.
But once we have our key drivers, those points in the face that basically move the most, we can hook controls up to there and you can just move them around and you're actually going to get all that interpolation.
between the shapes as you pull them around.
I'll show you an example in a second.
Then of course the fine level of control for each individual joint, if you want to expose that.
We have the UI, as I described, that had a picture per scan.
It was one of the pictures that we actually taken right from the moment of the scan.
And we have a UI that exposes those with sliders.
You can click on any picture and it'll give you sliders.
And we can pick however many regions of the face we want to break it up into with sliders.
It's basically just moving the joints in that region.
And it moves them in a normalized way such that if you moved them, if you moved all the sliders to the right for that face, all the joints would exactly match the positions of those markers as they were in the scan.
So I'll show you what that looks like here in a second.
But again, if you're, because it's tied directly to this deformer, this pose-based deformer, and you go from pose A to pose B, I mean, it's a big distance of travel between those poses.
what you get for free along the way is any other relevant shape triggering, because it's all in there in the post space.
And so maybe you're doing something with your jaw open, it comes to a point where it's ready to compress.
If that shape information is in there, it's gonna wait to the right time before those lips touch and start to compress and you get that stuff for free, which is very cool.
So here's an example of the UI I was describing.
We're able to slide those regions around and kind of pick.
any part of the face from any of the picture, oh, I like what the eyes are doing here.
I want the left side of the mouth to do what this one's over here.
This gives a little, this gives a way of fine-tuning what you want if you're starting from scratch or if you're just tweaking motion capture.
So with this, you're, you're able to slide an attribute that says that you want to have it take over.
your motion capture so that basically as you dial in any shape it dials away the motion capture so you're not fighting with it or you can just set it to add on top.
So here I'm just pulling around some of those gross motion controls and you can kind of see how like moving the chin makes the lip compress up or I'll just play that again it goes pretty quickly there but I'm not rotating anything I'm just translating them around and just because if you pull it under the mouth it's going to.
you know, curl the lip in if you pull it over the front.
You can try to, you know, pull the jaw forward.
And it's just, yeah, it's fun to play with it when you actually see the shapes kicking in.
So I mentioned exposing some of those attributes.
I'll just jump into some of the ones that I haven't mentioned.
Sticky lips, eye drag.
You can kind of dial in how much eye drag that the controls have.
How are we doing on time?
Anybody?
37 minutes in?
OK.
So this brings me to our wrinkle or crease maps that basically we're using the compression and stretch independently from a subset of markers on the face.
that drive basically an alpha, a dynamic alpha.
So wherever there's compression, it's gonna light up red.
Wherever there's stretch, it's gonna light up green, and that will reveal a wrinkle or crease map or a stretch map.
So what this does is you can, with that subset of vertices that you're using, you basically are defining a low-res cage that you can art direct where you want things to.
to always trigger together.
If you used every vertex in the face, it might be noisy.
You'll have a red and green little dot.
So this is not only good for that, but it makes for faster calculations, smooth falloff.
The way we create our maps is really interesting is that if you look at streamed data from scans, if somebody's going through a range of facial motions and you look at it in UV space, where you're just seeing the color changing and changing and the geometry's not moving.
What's really interesting is that 95% of those wrinkles are just coming on in the same spot.
And this is why as you get older you start to get wrinkles, because it's the repeated wrinkling happening in the same spot.
So that means we can just take the aspects that we wanted from the various maps and just kind of make a single map out of a combination of where those wrinkles and creases are coming out.
And of course you can just go in and use a normal painting.
Map to kind of add any that you want in addition if you're trying to make them older So here's an example that you've seen in In our highlights video, this is a Reggie Rowe character, and he's You see a corner of his eyes and his forehead you know those wrinkles kind of kicking on dynamically there And this is what he looks like with just Lambert and just the skin cluster and post-space deformers.
So there's a little bit of that stuff there, but not that kind of detail coming out.
And this is what the vertex color being triggered by that compression and stretch looks like.
So when he squints the one eye, you can see that becomes red and the other one goes to green and gives just a nice dynamic way of having them trigger right where they need to be.
So which brings us to actually getting the stuff into the engine in real time.
Oh, and that wrinkle map is also applied as a plugin in Maya, so when you're moving the rig around, you see those kick in as well so that you know that they correspond to what's happening in the engine.
So as far as the real-time performance, mentioned that we have the joint animation, GPU decompression, one aspect that something like Maya and these other programs do.
for free is to recalculate the normals if you have deformations moving the vertices around.
If they didn't do that, you might not see the details actually come out because it's not reacting to light at the angles that it should.
So that's one of the things that had to be added in there.
Some seam fixing, we had a shader applying the parts that are streaming of the face.
And a different shader, very similar, but just wasn't streaming for the edge around it.
Kind of ran into a funny little snag that the...
round off that Maya was using for its skin weights wasn't matching what the round off in the GPU was so it was giving this little seam that once we corrected that went away.
Some additional considerations Tracking issues we really kind of learning experience from this round was that it's probably beneficial if we have a rig version that's of the actor before any retargeting that's, that you're, the tracking company can, if you're doing it outsourced, they can apply it immediately to the actor rig and get immediate results to see if there's anything wrong, maybe camera calibration or misalignment from the stabilization, so they can iterate on that before it goes through the rest of the pipeline, gets applied into a scene, and you know, gets animated on before we realize that it's really not behaving well.
Our target process, I think, you know, it has room for some improvement as well.
So, that's something that I think, you know, led to a little bit of hand animation tweaking in spots where we probably didn't need to.
We infer the jaw motion from a couple points on the face.
We basically are aiming a joint at the center point of those.
So, it's moving the jaw with it.
I mean, the skin does move no matter what you do.
So, it's doing a decent job of.
implying where the teeth are, but we just give an extra offset control so you can kind of fine tune and align those as needed afterwards if it looks like it's not aligned correctly.
Again, additional attention to the deformation point where it connects between your post-based deformers and maybe regular skin clusters and such.
Which brings me to the end.
I don't know if Adrian Bentley is here.
He's had his own talk.
this week. Oh, hey, Adrian. So he's going to be around, hopefully come up and help answer questions related to the deep engine programming and plugin writing that he implemented. So he's a co-contributor and a big shout out to him for all the work he did, as well as Josh Scherzold, excuse me, Josh Scherzold, who did additional plugin and programming work.
Shwe Liu and Steven White as well.
So thank you.
Cthulhu thanks you.
And special thanks to the GDC committee and RUGWE.
Feel free to contact me through the email or I just got Twitter on there.
So actually you guys are the type of people I would actually have an account for to respond back.
So feel free to contact me.
Thank you.
Thank you.
Connect with me that way. We do have the wrap-up room afterwards for additional questions, but we have time for questions now And just go ahead and step up to the microphone as you have a question Yeah, it looked fantastic. I was just wondering if you Simulated or considered simulating any of the muscles underneath you said you talked about simulating the skin. Yeah, that's that's something that It's certainly done in feature films that, like this example, they have this amazing tissue simulation that starts from muscle and bone and all these layers are simulated.
It's just we don't have a giant render farm to be kicking these things off and it's really a matter of time and iteration.
And animators will be able to tweak the performances without having to wait to see the results.
So we didn't go that far.
you could certainly infer that stuff after the fact and you had a farm to kind of go calculate that stuff.
You certainly could.
Yes?
How much work did the animators have to do after the facial motion capture?
Yes, the question was how much animation was required after the whole process was done.
We didn't save a lot of time in the end for facial tweaking.
There was definitely a good amount.
Like I said, we did have tracking and retargeting issues.
It's easier sometimes just to go through and tweak them and then to figure out.
We'll have to postmortem to check where some of those errors came from.
Surprisingly, we didn't have a ton of that that we had to do.
considering we did it on the cheap, basically, compared to a feature film approach.
But I can't tell you the amount of man hours that went into it, but it was a very tight schedule at the end, where animators were just going in shot for shot, just kind of tweaking and pumping them out.
So there was that to it, I guess.
What?
What facial capture system did you guys use?
Because it looks like it's doing a little bit of 3D and a little bit of 2D with the eyes or something.
Can you go over that?
The facial, the actual capture process?
The helmet.
The helmet.
The helmet is, again, it's a four camera setup where you actually have to use tracking software that looks through those cameras and tries to triangulate the position and per frame it.
It follows from the different views to figure out where those points are.
Awesome.
companies that specialize in that.
Certainly in the film industry, we use digital domain. It was our company, we used their stage and their helmets.
And so they did the tracking for us and would deliver us the marker points that we would apply into our rigs.
Yes.
Did you have two different runtime rigs for the cinematics versus the gameplay sections? And if so, roughly what was the joint count for the face for those rigs?
The marker joints themselves, if you don't count the eyes and additional helper joints on the neck and such, we had 168 markers.
We could certainly have implemented some of that within the game, but we really decided that you're really not up and close.
to really need to do all that additional streaming.
We have an open world game, very fast travel.
So we didn't really feel that was an area that we really had to take up computation power.
But since we do have the joint motion, that was perfect for just the LOD for the rest of the game.
But you could drive it with the same system.
Do you remember how many joints you had for the real-time character though?
Oh, the same, 168.
168 for the face, okay, thank you.
Yes?
Did you use certain markers to stabilize the face?
For the stabilization process We have the stabilization from just our fax Session that we need to get those aligned and then there's a stabilization process that digital domain did using the motion capture to try to get that relative to the default and They have their own proprietary method of doing that constrained spring-based system that tries to keep it aligned.
But I'm not privy to, you know, answer any of the details on that.
Yes?
Yeah, I was just wondering, as far as using this sort of system for instances where your characters are quite a bit different from the actor, like what are some of the challenges you see with that?
I know you've mentioned retargeting being an issue here where Troy Baker looks a lot like your character.
What if it was a creature or something of that nature?
Yeah, you could certainly take...
Is it one minute left?
All right, so you might have to pull other questions into the room after this, but so your question was, if you retarget really far away from what your actual game character was, we had a pretty close correspondence with our main character, but let's say we wanted to animate a giraffe, you know, you could, instead of retargeting all the motion capture and all the shapes, make a correspondence between your giraffe for each pose.
And even if it's a combination of other blends to give you that pose, that correlation is what can give you a path for an algorithm for retargeting those.
If you do have an exact pose, one for one, that nicely blends on the other end, the output weights that are from those deformers can be just directly fed into the other one.
And it would drive the face at the same time.
Thank you.
Well, thank you very much for your time.
