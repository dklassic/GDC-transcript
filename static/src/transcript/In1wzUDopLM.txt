Excellent. Wow. This is a much larger turnout than I would have ever expected when I drafted this. So how's everyone doing? Thank you all for coming to the talk today. I'd like to take a quick second to remind you all, please turn off your cell phones. It's very interrupting when all of a sudden you hear baby got back in the middle of one of your conference presentations. Also, please, too, please make sure to fill out the little surveys they give you guys. It's one of the only ways I know how bad I screwed up my talk during GDC.
And that's important for me to grow as a speaker.
For those of you filtering in, we've got a couple seats here in the middle.
And please look to the people in the yellow shirts, and they will direct you where to go.
So with that, let's go ahead and get going, because we've got a lot of stuff to do.
So hello, everybody.
My name is Colt McAnlis, and I'm a programmer at Bonfire Studios.
And when I'm not working too hard there, I'm also an adjunct professor at SMU Guildhall School for Video Game Development, where I teach various things like physics and multi-threading and all those fun things.
And before it was closed down in January of this year, I was a graphics programmer at Microsoft Ensemble Studios. Now, for those of you who had seen previous Ensemble talks, you know that there's usually one guy stuck in a basement for about three years whose entire job is to work on train and whatnot. Fortunately, when I was hired there, they said, hey, guess what? You're that new guy. Which isn't all that bad. It gave us a chance to work on some new things and we got a chance to bring up this nifty talk.
Before we get going today, I just kind of want to do a little survey of who's in the room so I kind of know if I should skip over things or whatnot. So let's start with artists. Please raise your hand. You get a prize after the show. I don't know what it is yet, but we'll get it.
Designers? He got your prize already. Sorry. Non-artist, non-programmer, non-designer. So media or administration. A couple of you guys. Excellent. So programmers then, everybody else I'm imagining.
All right, yeah.
Why did I not expect that?
So for those of you who are programmers, let's see your hands again if you've actually implemented a height field terrain before.
OK, interesting subset of that.
And for those of you who've implemented that, how many of you have actually made an RTS game?
OK, interesting.
Well, don't worry.
Like I said, Ensemblz is closed down, so we're not really competitors.
Don't tell anybody that.
So anyhow, let's get two things.
So we started prototyping the RTS on a console idea in around 2004, 2005.
And we finally got around to the fact that it was far enough along that in early 2006, we said, hey, let's just go ahead and scrap what we were working with and start a brand new engine from scratch on the Xbox 360.
Up until this point, we had our prototypes working on the H3 engine, working with the controller, had some different builds and whatnot.
But we really said we had to hunker down.
Now, anytime you start from scratch and you build some new technology, you kind of have to have some bullet points and some guidelines that says, here's what we're going for.
So when we created the engine, we said, first off, we need to make sure that we're taking control of the hardware to create new visuals.
We didn't want anyone to think that when this game shipped, that it was a port of any PC technology to the 360, or that later on they said, well, that looked OK, but you could do all that on the PC.
No.
We wanted every frame of our game to scream Xbox 360 at you.
And I guess.
you guys can tell us whether or not we succeeded at that, but at the time this is our goal. The second step was we wanted to make sure that we took advantage of the hardware on the performance side of things so that our simulation could take advantage of it. This meant going back and revisiting some things like our simulation to say, hey, you need to take path finding and put it on multiple cores or your animation update and put it on multiple cores.
And this was kind of different because this was actually the first game that Ensemble Studios did that had multiple processors. We had done a little bit of success with it in the past, but never to the degree that we were working with on the Xbox 360. And finally, we wanted to make sure that anything we were contributing to this new engine and this new technology actually was pushing the genre forward. So the RTS space is actually a pretty small niche of the gaming community, and we wanted to make sure that no one came back and said, oh, well, they could only do that because they were on the 360 and it was the Halo IP. No, no, no, no, no. We wanted to make sure that sooner or later someone down the line, they would go, well, hey, this was a really cool thing that no one had done before and they did it in Halo Wars, let's go do it again.
So I actually get this question from a lot of people.
And when we started the prototyping, I got this question from management a lot.
They said, why did you want to make new terrain?
I mean, honestly, for most video games out there, first-person shooters included, terrain is not really a huge part of your visual space.
But for RTSs, you have to understand that it's a massive part of your visual area, up to 70% to 98%, even 100% in a lot of times of your visual space.
For us, we had the decision early on.
We said, well, we can go innovate like most games do and focus on increasing our technology for rendering meshes, which a lot of RTSs will do that.
They'll have a very low resolution terrain, and they'll include a lot of technology for getting more meshes, more particles, better animation, and these things.
Or we can go for what's actually taking up most of the screen and innovate in that space and sort of see what that gives us and see where that moves us.
On top of that, we had some issues with the prior technology.
For those of you who had played Age of Empires III, you know that one of the big features of it was the cliffing system, which was effectively a way to hide texture stretching on our height fields.
The cool thing about it was that it gave an amazing look for horizontal displacement and got rid of all those problems that you see with texture stretching.
A downside was the content creation process.
Effectively, how we created these things was we took meshes from 3D Studio Max, which were high tessellated, and then we would apply a displacement texture to them in the vertex shader and then kind of snap them in the terrain at various places we wanted.
It was really an awkward process because you had artists creating these meshes in separate tools that then they would take back and sort of fuse into the terrain in our editor.
and it never really kind of synced up too well.
There was always these little things you had to work with because the textures didn't match and the resolution didn't match and we just didn't want to bring that over to the 360.
On top of that, although the cliffing system allowed the age maps to be random and to have a lot of user-generated content and things like that, that was not the way that Halo Wars wanted to go.
Our goal artistically was to create unique handcrafted maps.
From the get-go, the vision was that every map had to leave an impression in the end user's mind.
The problem with the cliffing system was that you kind of ended up with a lot of visuals that looked identical to each other.
You couldn't tell one map from the other map if it used the same cliff and all these other prefabs.
So we couldn't really get the unique handcrafted look with our prior tool set.
So just to give everyone an idea of where we were and where we're going, this is a wireframe overlay of the H3 train. And I'm going to try this here. Is that even showing up up there?
A little red dot? You guys can see that in the back? Fantastic. Okay. So let's see if this will work.
This could make me crash and burn.
So we've got, you can see this is a low resolution height field, OK?
And then you can see here that we've got our inserted cliffing mesh pieces.
Now, these mesh pieces themselves are about 8x the resolution of these pieces here.
And you can see like these sliver polygons there is where we've welded.
And you can also see here, I'm not sure how it shows up on the big screen, but you can see this little line.
right there and on the top here, where the texture that's applied to these cliffing pieces doesn't really match up with the texturing that's actually on the terrain.
And this is because that those texture pieces were made in a separate app, in a separate environment, and then sort of just welded in.
This meant that the artist kind of always had to keep up with the process to make sure that no one put a cliff in or that the textures weren't off-kilter.
So the new terrain specifications, now that we talked about why we wanted to create new terrain, what's our goal in doing so?
So first off, we wanted to remove the cliff pieces.
I think that was actually the only bullet point we had on the board at the time.
But to do that, we had to accomplish a number of things.
First off, we decided, well, if we're getting rid of the cliff pieces, we have to somehow match that resolution.
So let's just increase the resolution of all the terrain by 8x.
I mean, hell, we're on the Xbox 360.
Why can't we do that?
Because it matched the cliff resolution.
At the time we were like, well, it seems like a good idea.
We didn't know if it worked or not.
The second thing was that we still needed to get that horizontal displacement that the cliff system allowed you to get.
That's actually the big thing.
When you're dealing with a height field, you really can only deal with sort of a shell mesh, sort of the sheet dropped on a bunch of objects type thing.
But we needed to still give you that visual appeal of having that horizontal displacement.
So we implemented something called a vector field train, which I'll cover a little bit more here in a second.
But what it allowed us to do is actually get that horizontal displacement without having to create all these other pieces.
So again, here is our Age of Empires III train.
And in contrast, here is a screenshot from Halo Wars.
Now for those of you who've played the game, hopefully most of you have, and that's why you're here.
That makes me really happy.
This is actually a screenshot from our Blood Gulch multiplayer map.
Now, the immediate thing that stand out to me and that I kind of want to point out here is that the terrain looks much more organic in this environment.
It looks like it actually exists.
And the height difference from the bottom of the screen here to the top of the screen isn't the same as it is in the age three screenshot, which is more like a stair-stepping pattern.
So you could really only have these linear quantized heights.
uh... in contrast we allowed the the artist to go ahead in there and say you know hey well you can do whatever the hell you want to inspect the field trained go go scope crazy uh... and sue just kind of put things in perspective you can see our vertex density at this point and that's hot that's uh... a little bit more hand what we were uh...
what we had our previous technologies and of course what screenshot would be okay without having a a warthog in some crates right that have scaled so now for a lot of you had implemented high-field train And you're going to know everything I'm talking about, but let's just get our terminology on the same page.
So height field terrain, when I say that word, what I'm talking about is a uniform spacing of positions on a grid, where each position in space contains a scalar value, which represents vertex displacement along the y-axis at that position.
Now, one of the big problems with this, though, is that when you get adjacent, no, that's not going to show up, is it?
That was a great idea, Colt.
When you get adjacent vertices that have scalar values that are a little too different, there's not enough UV data.
There's not enough texel density in that area.
And what occurs is texture stretching.
So you get these big blotchy pixels in there.
Now, common height field approaches to fix this is you slap either a cliff piece on it, or you slap meshes on it, and whatnot.
But in reality, that's one of the bigger problems with height field terrain is you always have to make sure that your artists aren't making that difference between adjacent vertices too high.
Now vector field terrain, in contrast, contains three scalar values per vertex that allows displacement along the x, y, and z-axis.
The cool thing about this, besides the fact that we can now create the sort of overhang and cliffing structures that we got with our regular cliff system, is that we can now have the ability to kind of sweep vertices from the left over to the right when there's not enough UV density over there.
and uh... this screenshot here actually shows it off pretty well so this is actually a picture of uh... a sculpted terrain in our system actually made by one of our programmers uh... i said hey i'm doing this talk and he said hey i have a really cool idea of something to sculpt for it let me go make you a picture And so this took him about 10 minutes to sculpt in our environment.
So the first thing, obviously, again, this is not a mesh.
This wasn't made in 3D Studio Max.
He sat there with a brush and actually sculpted this whole thing out.
The first thing is you're getting overhangs, right?
Anyone in here who's made a height field terrain, the first thing your artists ask, or artists and designers ask for is, I need overhangs and I need bridges.
Well, ta-da.
They didn't really like it when I presented it that way.
I was like, I better go.
The second thing is, you really notice that, let's see if this one shows up, sweet, on the shadow part here, and even on the bottom here, there's really no UV stretching.
For the amount of displacement that we have in this environment, there's no UV stretching.
And how that's accomplished is he actually swept vertices from this low-lying area here that has low fidelity, not a lot of displacement, and he swept it up into this displaced area to sort of make a more uniform density.
Now, when we originally wrote our tool for this system, you kind of had to do that by hand.
But after a time, we said, oh, well, why don't we just automate that process?
And so one of the things we did during export was we would find areas that had low UV density and sort of search around it and say, hey, if we can sweep some vertices into this thing and sort of unify the distances to increase our texel density.
And here's another good screenshot.
This is actually, for those of you who played the game, this was actually the very first pass of Scenario 1's terrain sculpting.
And the reason that.
It's a little bit different now than what's in the game, right?
The reason I actually show this shot is this was one of the first things, this is one of the first screenshots that the producers actually went, oh my god, you guys aren't crazy, this whole terrain thing might actually work.
The artist took an image of these horizontal terrain sheets which are caused by massive glaciation.
When you get these huge glaciers that are moving through terrain, they cut these horizontal slabs into bedrock.
and you can kind of see that down in here.
And the artists were able to sculpt all of this horizontal displacement down here by hand in the editor very, very easily.
In fact, the whole sculpting for this map only took them about three hours to do, and this was early on in our development too.
So we hadn't put in all the fancy clip arting systems and everything else that we have today.
But it was a really good technology and it kind of showed off some subsurface scattering and some ambient occlusion there as well.
And yeah, we've come a long way from what we shipped with, right?
So.
Now that we have this concept of, hey, this is a vector field terrain and here's what we wanted to do and how we do it, it's kind of time to talk about the realities of innovating in these areas.
The fact is that when you take a terrain system that you have and you up the density by eight times, you kind of end up with some problems that you didn't expect.
And the rest of the talk is actually focused around how I solve what we, I solve them.
uh... how we solve these problems and sort of went through uh... my goal here is to talk about what didn't work and what did work and uh... the main thing is see this this bottom one here content creation burden I'm really going to try as hard as I can to get to that, because some of the really cool, innovative technology we came up with was actually in the content creation process.
So if I'm talking a little fast, I apologize.
I really want to get to that data.
And if you guys are OK with it, we might cut the Q&A session a little short to get there.
So let's go ahead forward.
So let's talk about memory first.
So the original terrain in our original demo, when you up the density 8x, you actually ended up with a 4096 by 4096 terrain.
So.
4k verts by 4k verts.
Well, the uncompressed result of this was around 400 megabytes.
That's 200 megabytes for position and 200 megabytes for normal.
Any of you who are Xbox developers know there's only 512 megs on the box.
At this point, our producers were popping antacid tablets and trying to tell us, hey, we're not just shipping a terrain demo.
And we said, don't worry, don't worry.
We're going to work on some compression technology to make this whole thing work.
Now, the first round of compression technology was actually fantastic.
It took our 400 megabytes of terrain and compressed it down into 32 megs, which, off the cuff, that's around a 12x decrease, or about 16 bits per pixel.
And the way I accomplished this was I took the terrain vector displacement information and I used a combination of wavelets to separate out low frequency data and data that just wasn't needed at all, and then stored the resulting information in DXT textures.
And then I would take the DXT textures and in the vertex shader, sample all these textures and reconstruct my terrain environment.
Well, you can't argue with the compression results there.
That's absolutely fantastic.
But the real problem with this is that it was not performance enough to be able to handle the resolution we were at.
There was actually seven textures when you separated all the data out.
And that meant that we were sampling seven textures in our vertex shader at 4K by 4K terrain.
and it just really wasn't performing.
So we needed another solution, to say the least.
Now, anyone in here who's actually worked on a shipping product knows that there's sort of always this give and take between design decisions and management decisions, and then what your technology is doing, and what the industry is doing.
And I kind of want to take a second and address this, because there were a couple of things that weren't technical that actually changed the way we had to approach our compression process.
Now, the first of that was that we walked in one day, and our artist said, there's just too much density.
There's too many vertices.
We just can't manage all of these things, which in respect was probably a result of bad tools, which I wrote the original tools, so that's my fault.
But we'll fix that later.
But anyhow, they said, well, hey, it's just too much data.
So we said, OK, well, how about we cut the density in half, but increase the tile spacing so that the train itself will occupy the same world space size uh... but won't have as many vertices so we went in and did this and i resized all the maps and i checked them back into the tree and the next day the artist came in and says hey we thought you were getting rid of the density we did get rid of the density which was kind of eye opening to us that there was so much density that our picky artist didn't know when half the density was missing So it was kind of actually a good thing we did that, because it showed us that we really didn't need 4K by 4K resolution, that 2K by 2K resolution was actually sufficient for us.
The next was that we kind of came to a concept that the content creation process for these 2K by 2K trains was actually too large.
It was taking too long for our designers to map out things.
It was taking too long for our artists to sculpt the data and make it look good.
And so we had to sort of settle in a middle ground and said, well, our maximum size is going to be 1280 by 1280 berts.
And this, again, was.
was not necessarily due to a technological limitation, because we've said we can address all these other problems.
It was more done due to the fact that our content throughput process.
So needless to say that now that we have a 1280 by 1280 vert terrain, it drastically changed our memory footprint.
Unfortunately, we didn't figure this out until we had made these other memory modifications.
But here we are anyway.
So the final result that we ended up shipping Halo Wars was a 14 megabyte footprint for a 1280 by 1280 terrain.
and that was accomplished by putting XYZ and normal displacement data into 32-bit textures at 11, 11, 10. And then we also added an AO and alpha texture per vertex that were stored in DXT 5A textures. Now for those of you who played the game, you also know that when you place down a UNSC command center, it'll kind of cut a hole in the train. You'll see the slide doors open.
and then something will come up from the bottom.
In order to do that, I also provided a DXT3A texture, which those of you who have worked on the xenon know that when you sample that, it comes out to one bit per vertex.
With a little bit of trickery, you can actually turn that XYZ one bit into actual horizontal movement.
So we actually used a quarter resolution with DXT3A texture to actually map one bit of alpha to map out those areas.
In general, this works.
Again.
you know, the bigger thing was there wasn't a technological revolution after those decisions were made, it was just like, well, let's find a good middle ground.
Now, I kind of have to put this in this talk.
This was actually the very first screenshot we had of the terrain system.
And this was actually sent around in early, or late 2005 when we prototyped the technology on the PC.
And this was our original density.
And you can really see how quickly we went from pretty cool wireframe to just sub-pixel triangles to complete solid.
And we were really impressed at that time.
Like, oh my god, we can really push that many vertices on the 360?
And this was back when we had the beta hardware, too, so we didn't have the final hardware.
So some of these numbers over here are a little skewed because it's a different hardware.
So that was our memory.
That was how we handled that.
And to kind of move along with the next big issue in terrain is always performance.
Now, when we created our original terrain document, we were still in RTS.
We were still dealing with an RTS camera and RTS angles.
But for some reason, almost overnight, all of a sudden, it turned into the fact that we needed to support FPS camera angles.
And we all kind of looked at each other like, well, wait a minute, what for?
The design doc called for cinematics and in-game wow moments and the ability to snap to and from these things on a trigger of a dime.
And we all kind of looked at each other like, well that wasn't in the original design doc, what the fuck are we doing? We didn't design for those things.
So, but necessarily we couldn't push back on it so we had to put in some support.
And you can remember...
Our density really didn't support a first-person angle.
I mean, there's a great picture of it right there.
So we had to put in some sort of LOD metric for the process.
And specifically, we had to put in something that wasn't a memory burden and wasn't a performance burden, because we kind of already had our technology prototyped out to this point, and we're moving forward with content generation.
So the first round of LOD solutions was actually geo-MIP mapping.
Now this is different than geo-clip mapping, which is some research by Hugh Hop over at Microsoft Research.
This is your standard vanilla geo-MIP mapping.
The basic process works by taking index buffers in changing the stride. So, our terrain was actually one gigantic texture and each quad node chunk was 64 vertices by 64 vertices. So the simplest way to do LOD was instead of an index buffer that samples 64 by 64, you have an index buffer that samples 32 by 32 and 16 by 16, etc, etc, etc.
And then depending on how far the chunk is from the camera, you determine which resolution it should be.
Now, in general, it was pretty quick, pretty easy to use.
But you can see one of the big problems right here is that this transitionary line was kind of right in the middle of where the user was looking.
And in motion, the difference between the 64 by 64 and the 32 by 32 was an immediate pop.
And so to fix that, we'd have to kind of push this line way up to the top of the screen.
And by that point, 90% of the screen was high resolution, and it just didn't work.
On top of that, geomip mapping really wasn't responsive to areas of low density, or at least where the artist really hadn't sculpted anything.
So it's like, well, wait a minute.
There's no displacement here.
Why is this still high resolution?
So the second thing I took a look at was restricted quadtree triangulation.
And you can find a good talk of this by the FlightSim team at Microsoft, who actually put a much larger and more robust version of this in Microsoft Flight Simulator 10.
Now, RQT works on a concept of taking sort of a three by three area of vertices and creating an average plane and then deciding distance from that plane.
If a vertice is not too far away, it says, well, you're not displaced enough.
Get out of here.
And then re-triangulates the process.
So to kind of describe what you're looking at here, here's a wireframe shot.
We've got a little bit of vertex density here, and we've got some cliffs there.
And we also have some sort of scars in the terrain and a little bit of an overhang area.
And here's what it looks like at lower resolution.
we pretty much got what we wanted out of this. It kept the areas that the artist had sculpted and got rid of the areas that the artist hadn't touched yet. And this was what we considered a win. The problem with this though is that we were now introducing that each quad node chunk had to have a unique index buffer to be able to describe how that area was represented in memory. And we just did not have the memory ability to be able to do that. So we're kind of like, well, it's great because it gave us what we wanted, but it wasn't memory efficient enough.
At this time we had our final hardware and we were messing around with different things which we didn't think we had the ability to rely on before. And then we decided to move to the hardware tessellator. So for those of you who have worked with the 360 or attended any of the DirectX 11 talks, you're hearing a lot about this new hardware tessellation thing that's going on.
So we were really blessed, though, because the gods of numerical accuracy like kind of opened a cloud and put down a god ray upon us, because we ended up in a lucky situation that our quad nodes were 64 vertices by 64 vertices.
Well, a hardware tessellated patch for the 360s max resolution is 16 vertices by 16 vertices, which without any change of me, we could just easily do four by four patches and kind of move on and say, hey, it's a day.
So how we would do this is I would compute the bounding box of the patches in world space at an export time, and then at runtime, project that bound box to screen space on the CPU and measure its projected area.
Now, you guys are probably reading some papers right now, especially in the Shader X7 book that just came out, that talks about the same process, but doing it on the GPU instead and using StreamOut to write out this information.
In our RTS, we are GPU bound more than we're CPU bound, and if you've got three cores and six threads to work with, it's much easier to offload this onto the thread pool and have that computation done there than it is to sort of have this readback process on the Xbox 360.
The cool thing about this was that water tightness was kind of automatic for us.
I mean just like with geo-mip mapping, we had the ability to just say, you know, here's a patch, go sample this large texture in your vertex shader, and as long as we kept the edges between two patches at the same density, we never had to worry about T-junctions or holes in our terrain, which, hey, that's perfect, that's less work that I have to do.
So here's a good picture.
This is actually the map that we got the whole project greenlit with.
And so we're kind of up in space looking down on top of it now.
And you can kind of see a nice canyon.
And you can see some walls working on that as well.
And you can see that these areas are low tessellated.
We move a little bit closer too and kind of focus around where the user's cursor is here.
Moving closer you can see that the terrain cliff walls immediately start getting more density because they were higher density to begin with and now their projected screen space area is much higher.
Around the focal point here you can see that this is now increased in resolution all the way to where we get into the max resolution and you get these cool little tread tracks left by a warthog who had just run over a grunt and threw a 7-11 and hopped on a grenade and did a backflip and all those cool things you see on YouTube.
In general, the hardware tessellator was fantastic.
And we shipped with it, and we loved every bit about it.
It had some downsides in the fact that it was a little bit of brain dead, in that it just kind of tessellated.
It would've been cool if we could do some other things with it.
But I guess that's a different talk for some different technology people.
One downside about it, though, was it sort of had the same problem that geomip mapping did, but at a much smaller scale.
You still had problems with boundaries between patches, especially if you had a high tessellated patch here and a low tessellated patch here, where you had to sort of find a middle ground of what the edge should be between those two patches.
And what this caused was you would get some slivers sometime, and I don't know if you can really pick it out.
Oh yeah, here. So if everyone can see right here, you kind of have a low tessellation patch here and a high tessellation patch here, and you get all these little sliver triangles in the middle.
The problem with that is that kind of creates anomalies in lighting.
And so our artist kind of always came back to me and said, hey, you know, the patches are screwing up our lighting, why aren't you doing your job?
And I would kind of point at the hardware and go, well, it's...
You have a brush to fix that, dude.
Just hit a button.
But it was easy enough to police, and the artist really didn't take too much time to fix it, because it wasn't everywhere.
But that's just something to be aware of, that when you're working with this, you can get these sliver polygons, and those sliver polygons can give you anomalies in your lighting.
So LOD performance, we can now support first person camera angles.
Our producers were happy.
They were throwing confetti and parties for everyone.
I hit the wrong button.
Sorry.
Nope, wrong button again.
Whoop.
Where am I going?
There we go.
Anyhow, moving to the sim.
So it's a great idea to say, hey, let's make a new terrain that takes advantage of the Xbox 360.
It's a different question to say, well, how does your game actually interact with this new technology?
So for most height fields, you have the.
You have a visual representation and a sim representation that are sort of coupled in one in the same.
And they're usually low resolution, because the sim needs to be able to do things like height field calculations, ray casting, and physics and whatnot.
Well, the problem with our vector field terrain was it was GPU specific, and it was compressed on top of that.
And also, we kind of had a problem saying, well, what do you do to define a ray cast into the vector field without resorting to going to an ABT or a BSP or anything like that?
and effectively just turning into polygon soup.
Well, the solution we decided to opt for was actually create a lower resolution terrain, which was height field only, and acted like these sort of legacy terrain systems that Age 3 and Age of Mythology had before it.
So that way, any systems that had come over from those prior systems just kind of worked automatically.
This actually, by the way, turned out to be one of the best decisions we made in the entire freaking project.
So to kind of cover this again.
The solution was to create a CPU side terrain, which we called the SimRep, just so we're cool on terminology, which was a much lower resolution than the VisRep, which was the GPU side solution, and it was organized and swizzled to take advantage of the CPU caches, so that we could do things like physics interaction and raycasting at much higher speeds, which was great because we could make decisions on how to organize that data for the CPUs, that didn't step on the feet of how to organize that data for the GPUs, and that was absolutely fantastic.
So how we would generate this process, which was actually a question a lot of people said.
They said, well, do the designers go in and block out the level, and then in the low resolution, and the artists come in, and do they spec it out, and yada, yada, yada.
And it was a big back and forth.
And finally, I said, tell you what, I'll just automatically generate it for you.
So to generate this process, effectively what I do is during export, I render the terrain from top down in the orthographic camera, and I render it to a render target, which the render target size is actually the resolution of the sim rep that we want to support.
Hypothetically, I'm rendering a 4K by 4K terrain to a 256 by 256 texture.
And when I rasterize the terrain, I actually write out the world heights at that position.
The cool thing about that is that I read it back on the CPU at that point, and I kind of have an automatic height field, which is kind of nifty.
That you can then take it and do various GPU or CPU massaging and hole-filling algorithms.
We'd also leverage this process to be able to go through this process and say, oh, this is too steep.
Make it impassable.
This isn't steep enough, et cetera, et cetera, et cetera.
And this was actually such a successful idea that we had four separate versions of these low-res representations.
We had one for the sim, which was what all the units used.
We had two super low-resolution versions for the flight rep and the camera rep, which eventually actually only ended up being 8 by 8 tiles altogether.
So that when your camera was swinging around the environment, it had a much more gradual height field to interact with, as opposed to our 4K by 4K train.
and then the scarab had his own representation because it's a freakin' scarab and it can do whatever the hell it wants.
And all these were at different densities and I think we had like two more for uh... our uh...
our decal representation and another one that I can't remember.
One other side point that designers decided to throw into our design document a little too late was that we need the ability to play on forerunner objects.
We need the ability to play on bridges, which, again, a lot of you have probably heard that same thing before.
And we said, well, hey, wait a minute.
We have this ability to sort of generate a sim rep on the GPU.
Why don't we leverage that technology to allow you to insert meshes into that process that could then modify the sim rep?
And that's exactly what we did.
So we allowed the designers and artists to flag meshes in the editor that could then be rendered during the sim rep generation process.
And the bonus for this was that it modified the sim rep with these objects without screwing with the vis rep, and then no one had to change any of these things.
Oh, that's what I said.
Sorry.
So here's a great picture of that.
So for those of you who've played the game, this is a picture of one of our multiplayer maps.
It's called Repository.
And you can see here's a massive, big, forerunner object.
It kind of drops down to infinity there.
Here is the terrain itself.
And this is not alphaed out.
This is actually sculpted using the vector field displacement, just making nice little curves.
Because when you swing the camera around, there still has to be density there.
And this is actually the sim rep that represents that same area.
Now you can notice that it's much lower resolution.
It picks up the fact that the terrain drops off right here.
And it also picks up the fact that you have this massive mesh in there.
And it actually does it to a fault, which can be a good or bad thing.
You can see that it's immediately picking up these bumpers right along here.
I don't know if you can see that with my weak laser pointer.
It picks up these bumpers, and it also picks up this little observatory tongue depressor looking thing, which I don't, I mean.
I don't mind that, you can't play on it, but it's really cool that it picked it up automatically for you.
An extension of this actually came to us very, very nicely in that our design document required you to play on the back of the Spirit of Fire, which is the main ship from our storyline.
And originally we had decided, well, we'll just zoom the camera in and change the textures to...
look like metallic and so you won't ever see the edges of the ship. We said, well, why would you do that?
We have the ability to insert meshes.
So we actually took the Spirit of Fire model that was given to us by Blur, who did our cinematics, and we had some artists actually remove polygons and make it acceptable for gameplay usage.
Then we went ahead and put it in the editor, flagged it to be included in the sim rep, and voila.
We immediately had a height field representation that the sim could play on, and we had a visual representation that was given to us that had nothing to do with terrain.
And in fact, for these scenarios that you play on the back of the Spirit of Fire, we actually turn the terrain off entirely.
So we get all of the memory footprint back for the terrain, and we don't have to do any of the processing.
And just sort of a side comment.
sort of a spoiler, I guess. You kind of get bombarded by some covenant artillery. You can see they raised the terrain way up in the corner there just to make sure that they have the proper height. I didn't realize that when I originally took the screenshot, but someone pointed it out while I was giving the presentation. They're like, hey, what is that over there?
It's kind of just funny to see how that looks. So we now have the ability to allow the sim to interact with this crazy new terrain that this bald graphics programmer kept ranting on about.
The next biggest problem, probably one of the bigger issues that we didn't realize would take so much problems is texturing.
So for standard height fields, we do a texturing process in the industry called splatting.
And this process is involved by re-rendering the geometric data to the frame buffer and applying combinations of alpha textures and I guess diffuse albedo normal textures and whatnot.
And you re-render the chunk, and you bind a texture, and you bind an alpha, and you keep re-rendering, and you do this multi-pass rendering.
And the result is that you get an alpha-blended terrain.
The problem with this is that we just increased our density by 8x, and we really couldn't render a chunk more than one time.
So that kind of caused some problems, because we were no longer able to do splatting.
Now, another way to actually do sort of the same terrain process.
is that you bind four alpha textures, and you bind four textures that you want to visualize, and you kind of sample them all at the same time, and you get one result, and you write that out instead.
Well, the problem with that was that early on, our artists had requested to increase the number of blends that we allow per vertex to 12, and then increase the number of textures to blend to five.
Now, a lot of you who've implemented terrain, you usually only have diffuse and normal.
Maybe you've got a single-channer gloss spec or something like that.
Well, we actually support diffuse, normal, three-channel specular, an HDR emissive, and a three-channel reflective mask.
And originally when they came to me and said, we need these channels, I said, you're high.
You don't need those channels.
It's terrain.
But then they came back with this information and said, well, hey, here's what it really looks like in the game.
And I said, well, sold.
So this was an ice map that we were working on.
And they went ahead and made some great use of some fake Fresnel in here, as well as some fake use of subsurface scattering using these five texture channels.
And I said, cool.
Hey, well, I guess it was a good idea.
Maybe you weren't high after all.
I'm glad that this all worked out.
Of course, at the end of the day, I'm walking out of the office.
And one of the artists said, hey, man, I just checked in a map that plays a little bit more with the emissive.
You know, go check it out tomorrow.
And I get in the next day, and I find this map sitting there.
And at this time I said, well, maybe I shouldn't have put all that information in.
And I took this screenshot back then saying, sooner or later, I'm going to have to show somebody this.
This is just fantastic.
But needless to say, this was actually kind of cool to see Terrain contributing to the emissive part of the rendering equation instead of, without just having to be sort of the high-pass specular capping, where you just sort of, you know, take anything above 0.8 or something like that.
They could actually control this.
And for the most part, this worked out really well, and we were happy with it.
Now the big problem with texturing here was that we allowed a 512 by 512 texture per terrain chunk, which meant that for our large terrain, for our 1280 by 1280, that resulted in 10k by 10k.
texture across the entire train.
When you start looking at it that way, it's actually easier to visualize the entire thing as a virtual texture, which a lot of you, I'm sure, have heard things like megatexturing or sparse virtual textures, depending on what paper you read, or unique texturing.
They're all buzzwords that effectively mean some huge freaking texture that has to be paged in and out of memory using smaller pages and whatnot.
Now the problem with our particular texturing system was that our train was actually, our data for texturing was actually too large to use unique texturing. Effectively in an FPS, for those of you who made first person shooter games, you kind of work on a rail system. I mean you're moving from portal to portal, from area to area.
You sort of have this predictive nature about what's coming up next.
And in which case you can say, oh, well, I know these portals are coming up next.
Go ahead and preload these things so that I can use them when I get there.
And RTS is very, very different in that fact that if you click on that mini map at the bottom of the screen, you can jump to anywhere you want as many times as you want.
In which case, you're looking at completely new terrain, completely new units all across the board.
Well, if we're going to be streaming unique texture data, we have to stream five channels for.
you know, the 15 chunks that are visible in a given view, as well as decompress and stream all of our mesh textures at the same time. Well, the compression technology that we had at the same time was fantastic. I mean, it was amazing, but it wasn't fast enough to handle the texture load from the terrain as well as the meshes. So the terrain being a little less complex and able to solve, we said, okay, well, we'll keep using the real time texture decompression for the meshes, but for the terrain we'll do something a little bit different.
So typically with a virtual texture page, you'll decompress a page into an uncompressed form and then use that in your rendering process.
So you kind of have this massive texture in either off disk or in a compressed form in memory, and then you kind of grab what you want and decompress it and put it into a cache and whatnot.
Well, instead, what we're going to do is instead of decompressing it, we're actually going to do the same splatting process, which is very common to Terrain.
But instead of doing it in the screen buffer, We're going to do it in 2D space, in effectively texture space.
And we're going to do it on the GPU.
So the cool thing about this is we just render some quads.
We create all of our data.
And then we apply that back to the chunk in the same UV mapping that you would normally expect to get.
After we would do this, we would then take those textures and we would put them in a cache.
Now, to make sure that we fit in memory, that cache actually had to be compressed, because otherwise we'd have 70 to 100 megs of cache space.
Because when you do a render target, you can't actually render directly to a DXTn or a DXT5 or something like that.
So we'd have to render to a DXT8 and then feed it back into the GPU and then compress it to using on the GPU.
We had some shaders that would actually do the compressing the DXT5 and whatnot.
And then each texture channel, Albedo, Diffuse.
normal, specular, and emissive, and everything, had its own cache.
You kind of look horizontally, you've got these stacks of caches where any cache page, vertically, all represented the same terrain chunk.
Now, the cool thing about, well, I guess not the cool thing about this, was because of the fact that you're jumping around in your environment so much, managing troops, attacking this guy, dropping a bomb over here, going and getting these resources, fighting on this front, you're jumping around a lot.
And again, an RTS is very, very different from an FPS.
In such, the standard sort of virtual texture cache management routines kind of fell on its face.
We needed a different solution because we just couldn't do that.
We couldn't call create texture, new texture, load texture.
We couldn't do that for every frame.
So instead, we made a custom system.
Basically, for the entire cache, we would just allocate one large physical chunk of memory on the 360.
And then, anytime we needed to update it with different size textures or composite into the cache, we would just use the XG set texture header to point to that area in memory.
So this allowed us all the benefits of sort of rendering to a texture atlas, but without all the problems with that.
It allowed us to say, here's your handle, here's this handle.
It allowed us a little bit more CPU side cache modification and usage.
We also found out that MRU and LRU weren't sufficient.
predictors for victim page elimination like they are in other sorts of systems.
And the research that I was able to put into that actually was published in Efficient Cash Replacement Using the Age and Cost Metrics in Game Programming Gen 7. If any of you are dealing with these systems where you're paging lots of data in and out that can be randomly moved at any given time, I highly recommend you check out the age and cost metrics because it gives a better predictor than LRU or MRU or NRUFS.
any one of those algorithms. So please check it out and if you have better ways to do it, please let me know.
I'm always interested in hearing more of those things.
Into, because I've had six slides without a picture, here's a picture.
Now, this, first off, this is obviously temporary artwork from 2006, so please don't go post this on your blog and be like, the early origins of Halo Wars!
This is between us and the slideshow.
On the top here is the actual cache itself.
Now, the reason is a lot of you people are saying, well, why are you using the XG set texture header and whatnot?
Well, if you look at the bottom right-hand area here, these are all low-resolution 64 by 64 textures.
And there's a ton of them that we have to move in and out, especially when we're swinging our camera around at such fast speeds in first-person shooter mode for cinematics and wow moments and everything like that.
you know, having to either have a greedy cache of 64x64s already allocated, or a greedy cache of 512x512s, or expanding those things over time, it's just too much burden on the CPU and the driver. So instead, we just move around these XG set texture headers, and it allows our 64x64s to occupy the same space that a 512x512 would. So this was actually a huge win for us, because it gave us all the benefits of a caching system without the burdens of actually having extra resource handles.
Now, I just want to talk a little bit about the compression system, because it's important to say that, hey, when you do a render to texture, and it does come out in uncompressed format, RGBA8 or something like that, that you then have to compress it in order to put it in our compressed cache.
And like I said before, we did have some shaders that would take in the uncompressed data and compress it and swizzle it to the 360 swizzled format, and then write that data into the cache itself.
And just for number comparisons here.
diffuse, three channel spec and three channel reflective all got their own DXT1 texture.
Normal was compressed to DXTN and then emissive which we use as a high dynamic range went to DXT5 and you can see their memory footprints there. In reality, the cool thing was that for a lot of maps that didn't use emissive or didn't use reflective maps, we could turn off those caches entirely and save us some memory. So if you were playing on a map with all dirt, you know, we wouldn't allocate six megs for a cache for that.
So now I would usually get into the content creation process, but I actually kind of want to talk about something else first.
One of the other big things that we wanted to fix with the Halo Wars terrain was we really wanted to fix the whole floating in space RTS look.
So for those of you who had played Age of Empires III, when you get to the edge of the map, it's just kind of black.
You get over there and it's just a black fall off.
Or if you're playing Age of Mythology, you get to the edge of the map and it looks cut and you kind of see the strategy.
the layers of the earth inside there.
We kind of wanted to get rid of that, as well as now that we have to support a first person camera angle, when you swing the camera into first person mode, if there's nothing out there, it's a little alarming for the user.
So we needed the ability to put geometry out there that made it look convincing, but didn't really have any impact in our content creation process, or additional impact on our memory footprint, or for that matter, our performance footprint.
Hence, what we call a terrain skirt.
So what we decided to do was take quad nodes, basically just empty quad nodes, and sort of populate them out.
around the end of the map out to sort of a preset distance.
And then each quad node actually pointed to a node on the playable map that it referenced.
The goal here was to actually make it appear as though the train itself were mirrored across the edges of the playable boundary area.
So this quad node over here on the end would actually say, well, I'm referencing quad node 715 or whatever like that.
And then when we actually get that quad node visible in the camera, we'd render its referenced quad node at the lowest resolution with the lowest texture resolution for that as well.
And here is how that sort of turned on.
So from the top down, you can see that the playable area is down here in the left-hand side.
You can see objects and whatnot.
And then you can see that along the boundaries here, you've got a mirrored version here, a mirrored version here, and a mirrored version here.
You can see that these terrain details like this is easily mirrored across that.
Basically, for those of you who understand it, this is just like the mirror texture mode in texture addressing and whatnot.
So for us, it was kind of a, well, let's just do what the textures do.
Now.
From top down, this looks kind of obvious and hackish.
But from a first person angle, this actually comes across very, very well.
Now this is an unaltered, undoctored screenshot.
An artist went in, he moved the camera, he put down some wraiths there, and he hit Make Screenshot.
And this is kind of what came out.
And to show you what this actually is doing in the background, the actual geometry is highlighted in green, the skirt is actually highlighted in red, and the sky dome is in blue.
Now, I'm going to switch kind of back and forth a couple of times here, and I want you to focus on a couple of things.
First off, this area back here normally would be approximated in a first-person shooter game as part of the sky dome.
But for us, it's actually part of the skirt.
So what that actually means is, since we're doing a mirror mode, the camera is sitting on this little plateau here, and those mountains are actually quad nodes that are behind us that are being flipped onto that side out there.
Oops, wrong way.
You guys didn't see that yet.
The other thing too is you can actually kind of see that where the objects stop right here that this is the line Sort of right here that that we're actually flipping and mirroring the train off and when you actually take a look at it You can see that these geometric features here are easily Reflected on the other side as well as this and this and and you know these little cliffs right here But if you didn't actually know that that was a skirt you wouldn't actually you know You wouldn't actually see it especially when you're flying through a canyon and seeing all these things Now, just to kind of bring up another point here, we kind of got a lot of slack when we finally released the demo that everyone was saying, oh my god, it doesn't look like the E3 2007 demo.
Well, the E3 2007 demo, where you're actually following a pelican through a canyon flying around, actually used this skirting technology extensively to show that sort of horizontal view and make it look like it's a much larger environment.
So it's not to say that it's not the same technology.
It is.
It's just a different usage of it for what we actually shipped with.
So let's talk about some takeaway points.
And I think I'm doing OK on time.
10, 5 minutes?
What do I got?
I got 12. 12.
OK, cool.
So good, good, good, good.
Then we can talk about content authoring.
So takeaway points.
So first off, I want to point out that height fields are no longer de facto.
If there's anything that you come out of this talk with, I want you to know that there's something else beside height fields and meshes being inserted with them.
In general, a vector field displacement is actually pretty easy to implement in its basic form.
Even if you have a height field terrain and then a separate channel that is the vector field displacement afterwards, there's a lot of different modifications that you can play with.
And there's really no reason to not empower your artist to be able to sculpt things like that overhang that a programmer did.
Secondly is that the tessellator hardware is really the future of terrain.
If you've read the article in Shader X7, and if you've attended any of the AMD talks, the DirectX 11 talks, it talks about the ability to do these tessellated quads and whatnot.
that fits perfect for terrain. There's really no reason to not use it. Things like ByLOD or SOAR or ROAM, really with the hardware being able to support those things, those are really all in the past at this point. The next thing is that the separation of the SIM data and the VIS data was a fantastic idea. Now, in the first person community, we see this a lot.
You have a visual representation and then you have a lower resolution physics or...
or collision representation, but in the RTS community, it's not really that prominent of a feature.
You usually have some coupling between tile data and pathing data and visual data, and no one, or at least that ensemble and the research that we did, no one had ever taken such a drastic step like we have to draw a hard line in the sand and say, no, no, no, these are sim concepts and these are visual concepts, and we will make decisions on each separate from each other.
That was by far one of the coolest things that came out of this research, especially when you look at scenarios 11 and 12, when you're playing on the back of the Spirit of Fire.
We really hadn't seen any visuals like that in any other RTSs out there, which if you remember the second slide, we said we want to advance the genre.
You really can't ask for anything more than that when you actually ship a product.
So since I have a captive room of people and you're all seeing the little bonfire studios at the bottom there, I figure I might as well talk about this a little bit.
So if you read the news, you know that Bonfire Studios is made up of ex-Ensemble Studios employees.
And we're actually mainly comprised of the original team, the original technology team, that created the renderer, created the engine, created the toolchain, and actually got the game greenlit.
It's the same team that's been there from the beginning of Halo Wars to the end of Halo Wars and got the whole thing shipped.
And we've got about 35 of us.
We're all from Ensemble so far.
And we're working on very new intellectual property.
And please check out bonfire-studios.com if you're interested in more information there.
And I'm still good on time, right?
So I'll go ahead and say thank you.
10 minutes, good.
So thank you very much, Andrew Foster, Rich Geldritsch, Sergio Ticone, Scott Winsett, and Ken Adams.
These guys have been crucial in getting this technology together over the past four or five years.
So now, this isn't my last slide, if you guys don't mind.
I'm going to kind of try to push the Q&A session back and talk about some density or some content creation process.
Is that OK?
Unless people don't.
Is that OK?
OK, cool.
All right, excellent.
Extra slides.
Sweet.
How do you actually edit a 4K by 4K freaking terrain?
Well, Ensemble Studios, this was the first time we had made a console terrain, or a console game.
So we kind of had to answer problems in a little different way than we were used to working with.
So we had our editor designed entirely on our PC, and we had a game on our 360.
And so we had a very, very robust streaming and content reloading pipeline.
that any changes that were picked up on the PC would immediately be reloaded, sent across the network, and shown on the 360.
This included any modifications to terrain.
Our editor was written entirely in C Sharp, which was a fantastic idea.
C Sharp is a great language.
It allows you a lot of abilities to add controls and visual elements that aren't as robust when you start talking about other sort of widget languages.
On top of that.
It allowed us to say, well, this is all in C Sharp, and so it can't be in C++, which actually led us to one of the worst decisions that I think I've ever made in my professional career, was that I actually duplicated the entire rendering system on Halo Wars on the PC in ManageDirectX, which was a horrible, horrible idea.
Don't ever do that ever in your career, please.
It was cool that we sort of had the same renderer on the PC.
in C sharp, but at the same time, you just didn't have the same hardware. And so we were constantly fighting with people's machines not being up to spec, or weird things with managed directX running out of memory for no friggin' reason.
So let's talk about vertex editing. Because when you increase your density to the type we did, standard height field brushes just don't work. I mean, height up, height down, I mean, that's really all you need is standard height field But for a vector field, you have to start looking at things and controls like ZBrush and Mudbox, things that allow you to rotate the camera and get in there and actually sculpt like an artist would actually sculpt.
So we actually sat down with the artist and looked at those programs and said, well, OK, here's a control, here's displacement along a camera angle, or here's displacement along a normal, or here's a unifier, here's a digger, and things like that.
And we actually implemented pretty much 90% of the functionality that these two tools had.
in our terrain system and allowed them to use that.
In addition to that, we allowed something like Adobe Photoshop masking situation, which allowed you to go in and mask out areas and apply different filters to them and also mask them out and say, well, you know, mask out this cliff area.
I'm going to do all this sculpting over here, and I don't want to modify this.
And that was actually a pretty cool process, too.
It allowed us to actually introduce the concepts of some vertex translation widgets, which are very familiar to you guys who work in 3D Studio Max, being able to select a bunch of vertices and move them around.
So here's our terrain in our editor in C sharp.
Here's some C sharp buttons.
And we have the areas selected in red here that are masked out.
And then you can see that we just kind of created a little widget that you can grab the widget and move it in displacement in any way you want.
And all of the data just kind of moves around since we're now dealing with a vector field.
These widgets were absolutely fantastic and allowed the artist to do mass properties at the same time.
uh... the next step though was they said well we need some other things we don't just need per vertex definition we actually need filters we need fbm we need perlin uh... we need sort of these mass generation things uh... the problem with that though is that perlin really doesn't look like terrain fbm really doesn't look like terrain if you've ever looked at terrain it's not a noise function it's really not Terrain is actually a modification of erosion properties.
It's the process of thousands of years of hydraulic and thermal erosion moving dirt through the environment.
And that's the look that we really needed to get.
So what I did is I probably spent probably about four months of pure, whole time during the development of Halo Wars, you know, kind of spread out between years, working on a hydraulic erosion process.
Now if you got Shader X7, you probably read an article in there by Eric Benez, I hope that's his name, I apologize, on how to do this whole thing in the GPU.
Well, I did it a different process.
I actually did the whole thing on the CPU using a process called path tracing.
Effectively, what I did was I dropped millions of raindrops on the terrain, and for each raindrop I would sort of figure out the next delta, pick up some dirt, move to that next delta, drop some dirt, add some water, etc., etc., etc.
You can find some various situations out there that kind of describe the same process.
The trick to this was actually getting it right.
Like, it's one thing to just say, I'm going to move to the next element, pick up some dirt, and then drop some dirt off.
Getting the control parameters actually correct when you're sort of dealing with these sloping pieces of data that can kind of accumulate and snowball and then channel.
And then do you hit sedentary rock?
Are you loose dirt?
Is it defined by your texture?
I mean, we had the concept of all these things in our editor system, so that if they put down some grass, it wouldn't get eroded like it would with the dirt and everything.
uh... and so i i know a little clamp on time to win a trustee to these really quick i'm sorry uh... on average you can erode the entire map took about twenty to thirty seconds and this was a back when you're still to pay by two k terrain i i fortunately didn't didn't time it on the cool thing was that actually figure out a way to make this whole process multi-threaded uh... so if you've ever looked at the talks by world machine or some of those other things they talk about doing uh... doing each node as a separate threaded process while we actually figure out a way to five minutes thank you We figured out a way to actually thread the drops of these water and moving it around, which allowed us to speed up.
So here is a picture of a chunk of terrain, and it's highlighted in red because we had the ability to say, just apply this filter to this selected terrain.
And then here is the eroded process.
Now granted, it's probably not the best looking erosion, but you can imagine that, hey, having the ability to just make it look eroded at all is better than a Perlin filter or anything along those lines.
And so here's another view of this, and this is actually something else that is really, really cool.
Is that we allow the ability to not only erode the terrain, but because we were doing the whole thing on the CPU, we could then generate masking information that we pass back to the artist, they can sort of put back into the information.
So you can see our C-sharp controller here, we had no masking, or generate an erosion with a mask, or just generate the mask itself.
And so here is the terrain eroded with a mask that defines the eroded area, and the intensity of the mask defines how much erosion actually occurred in that place.
And the result of that is allowed the artist to sort of apply this green texture everywhere.
And then when they did the erosion, they would be given a selection mask that they can apply a mass texture apply to the dirt texture.
And that kind of gave you these nice grooves in the terrain like that.
To our knowledge, no one's really implemented anything like this in any tool we were aware of.
So we were very, very impressed by the fact that this came out this way.
And again, terrain doesn't look like Berlin.
Terrain doesn't look like FBM.
Terrain looks like eroded.
uh... information and so this was this is actually the cornerstone for everything we're doing uh... texture editing actually work the same way of the the artist had request for increased out the resolution the concept of layers just like Adobe Photoshop, so they could take textures and swap them visually in a vertical fashion. They had masking. They also had extra data channels that they could add in that would be applied per texture information. And we really had to go back just like we did with Mudbox and ZBrush and sort of implement all of these other tools that exist in Adobe Photoshop. So at the end of the day, our terrain editor actually looked more like Mudbox and 3D Studio Max and Photoshop just kind of all stuffed into itself with 4K by 4K terrain.
And that's what we dealt with every day.
So again, thank you all very much for coming to my talk.
I appreciate you all being here.
This is more response than I could have ever imagined.
So thank you very much.
And so I think we have three minutes for anyone who does want to ask questions.
If you don't get your question asked, I'll be standing outside probably for the next three days with any other information.
So if you do have a question, please approach the mic.
Hey, how you doing?
Good, I'm good.
My name is Maddie.
I just wanted to leave you my card and continue to do a great job.
Oh, thank you very much, ma'am.
Yeah, I'm sure you've talked to a lot of people, so I don't want to like...
I'm having a headache right now, but do we have a conference?
Unfortunately, I don't.
Ensemble's sort of shut down and we're sort of bonfires in the process.
So let me come around here, actually.
Right, like if you have an oak cropping, it hangs up a roof.
Thank you. Thank you very much.
Appreciate it. Thank you.
So, yes, ma'am.
How long did it take before your screen editor was going to work on your film?
We went from initial concept to something that the artist can actually use in about three weeks.
That included the C-sharp tool set, that included the specifications and everything.
So we went from concept to here's something, you guys go play with it in about three weeks.
And you continue iterating on it?
Oh, yes, ma'am. In fact, I say we iterated on it to about, until about July of 2008, at which time we said, hey, listen, we got a ship-freaking game, we can't fix anymore.
So, yes, ma'am.
Thank you.
No problem, thank you.
Hopefully, quick question.
Okay, so...
With the, not the VisRep, but the...
The SimRep.
SimRep, right?
Okay, so if you're constructing it by just straight projecting down, and you had a scenario where, let's say the terrain was being pulled, so it outcropped over, like over a canyon?
Ah, yes, yes, yes, yes, yes.
How would you account for the fact that, you know, things should be able to move underneath?
So, our simulation didn't actually allow for objects to move under that, because to the Sim, we were still using a height field.
So it had no concept of pathing under bridges or pathing under objects.
The sim was still using, effectively, the same pathing engine and AI engine that Age of Empires III shipped with.
So we just had to make sure that our data supported that legacy code path.
We did have to do some tricks, though, for decals.
So when you do have these overhanging things and you move your cursor to something underneath it, you had to show the decal there as opposed to on top of it.
And to do that, we would render from the top, and then we would do a depth peel.
that would get rid of the front fragments that would give us sort of a bottom. And we would store that as a separate texture that had height and low data. And then when you would get the cursor position in the GPU, I would scale it and determine which one you should use. Should I use the high value or the low value and then draw our decal there.
Okay. So some of it's just sort of conditioning the artist to not do crazy stuff, right?
Well, we wanted them to.
Really weird things where you could almost like...
have the terrain wrap over itself or build a cave?
Correct. Utah overhangs and things like that.
Well, we actually wanted our artists to do those things.
Unfortunately, we didn't get enough of that in our shipped product.
We wanted them to do that.
The big thing was whether or not that that should have an impact on our sim representation.
So that was the big question.
And early on they said we don't have time to innovate on the pathing and all those other things, so we'll just use a height field.
Okay, that makes sense.
Excellent, thank you.
Yes, sir.
Great, thank you.
I'm from Japan.
Yes, sir.
I'm a man of...
No, thanks.
Maybe, could you give me a presentation later?
Yeah, you can actually download the presentation on the GDC website as soon as GDC is over, and I'll have a version at bonfirestudios.com as well.
Do I need to grab that, sir?
I'll just download it from the system.
Thank you.
So that should be available within the upcoming weeks.
Okay, thank you.
Thank you.
So this might be a really newbie-ish question.
No problem.
What would be a good place for me to look to find out about the algorithms for sort of automatically pushing the verts around to balance the UV density?
We used a spring mass tensor system, so it's actually something from the particle dynamics world.
So it's like a relaxation?
Yes, exactly.
Like a causing.
Okay.
Thank you.
Yeah, we actually implemented a sort of hacky version first, which said that if any two vertices are more than x distance apart, just snap them.
Then we sort of moved it to the particle dynamics things, and then our analyzation process kind of handled the details of saying, hey, these need to be moved over here, and here's sort of an interest point that everyone needs to move to, and that moved there.
I've only, I mean, this talk kind of blew me away.
