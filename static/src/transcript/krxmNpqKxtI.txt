So welcome to our talk.
Thanks for coming.
It's 5.30, which is late in the day, so I appreciate it.
So first off, my name's Michael Trautje.
I, prior to this, spent a bunch of years, 15, at BioWare on Knights of the Old Republic, an artist.
I was the level art lead through Massive X 1 through 3, and in the early days of what is now Anthem, I was a technical art director.
Hi, my name is Mikolaj Konik, and I'm an engineer at Phoenix Labs, and previously I worked with SideFX on Houdini and Houdini Engine Plugin for Unreal.
So today we're going to talk a little bit about Dauntless, the game we're working on right now.
We're going to talk about what we wanted to accomplish, where we're at, and To those that don't know, we're discussing a Houdini, Houdini engine pipeline.
So we need to cover a few basics on just some Houdini terms so that some of this makes sense to those with less experience with that.
And from there, we're going to describe our overall pipeline, what we've set up.
It's still a work in progress, but we're going to describe where we're at.
And we're going to dig into a few sort of useful pieces for anyone interested in sort of a Houdini engine pipeline of their own.
So that's those building blocks.
So Dauntless is a co-op action RPG.
It's a free to play title.
It's currently in early access.
You'll fight powerful behemoths and craft weapons and play with your friends.
So it all takes place on the Shattered Isles, which is pretty much the topic of our talk here.
So if you want to learn more about Dauntless as a game and everything else, I invite you to go check it out at playdauntless.com.
But.
The rest of this should hopefully become self-explanatory.
So yeah, the Shattered Isles.
They play a pretty critical role.
They're not just a passive backdrop.
We joined the team because working on this pretty exciting environment was...
a lot of fun personally, but also because we're doing this with a really, really small team, and that meant we couldn't go about this basically in the traditional way.
When I say small team, most of the world art you're going to be seeing here was created by world artists with Jen Morgan, technical art with Corey Lake, and art direction by Glenn Varnes.
So very few hands were involved in this.
So when I say, instead of talking about an art department, I'll probably say Jen or Corey, and think of them as synonymous.
We wanted to, we had an open-ended, you know, list of themes, biomes that we wanted to kind of take on.
We only had some concepts and some idea of the types of environments we wanted to take on when we started this, but we knew that there'd be more to come, and it'd be an evolving kind of thing, an evolving visual target, but also just, you know, we'd kind of adapt to new themes as they showed up.
So we didn't know the scope of everything that we'd be taking on when we started it, but visually we knew there'd be pretty wide range.
Technically, we couldn't just make do with any space.
We wanted fairly predictable topology for the player navigation gameplay.
But these are procedural spaces, and we'll get into some of what that means, but they're also directed spaces.
That means the...
level creators, I'll say designers, but it was sometimes gen, but everyone sort of contributed to that.
But the idea is that we wanted to kind of lay out what would happen, and that would be part of the input.
So that would mean that we land on the island, we start in this swamp, and we make our way up into this forest area over here, whatever, that'd be the type of input that someone might want to sort of begin creating some of these things with.
We haven't ruled out generating, just sort of new things and see how they go, but in general, these are a directed thing.
We needed to be compatible with Unreal.
We wanted Jen and Corey to continue working on an island as if it was created by hand.
So the end result of this, specifically that means maybe not a lot of custom or custom mesh we had to be careful with because we wanted her to resume painting landscape or plants or manipulating meshes and that kind of thing.
Her overall workflow couldn't be damaged as a result of us generating it.
That also meant we'd just need powerful controls to sort of tailor or alter the generated results.
So not so much just letting her manipulate after the fact as much as it was like sort of guide some of the generation stuff coming into it.
So here's where we're at now.
So note that these have received some art polish.
That's just part of our standard process, which I think I can hopefully explain a little bit more of, but it's just game captures of some of the islands.
Sorry, cartoonishly sped up to make the most of your time.
Yeah, so we've got initial couple biomes here.
We've got more in the works.
These all have a bunch of sub-area kind of types, sub-biomes, whatever, within that.
But we currently generate all the assets required to make a fully playable Dauntless level.
So there's no additional work required on the result to kind of hop in and continue, do full play experience on it.
Yeah, and this is where we'll describe a little bit of Houdini.
So this is our introductory slide. So what is Houdini?
Houdini to us is a visual programming tool, node-based.
You have four main types of geometry in Houdini, which are points, vertices, indices, primitives, and detail.
You can associate different attributes with those.
Then you can package your node networks into Assets, and you can bring that into Unreal.
This is very similar to Blueprint.
where you have parameters on individual nodes, and you basically promote them to your asset, and those show up in Unreal in the details panel.
Then you have two main scripting languages in Houdini, which is Python and VEX.
There's an SDK, which is the Houdini development kit, which we make use of extensively, and there's an engine.
And to us, Houdini engine is essentially a transport mechanism that brings the data from Houdini to Unreal and the other way as well.
So our technology right now, we use Houdini 16.
I believe we started on Houdini 14.
Houdini 16 was a pretty major update for us because it brought hide fields, which we make use of for landscape construction.
Our Houdini Engine Unreal plugin is a fork of Unreal Engine plugin provided by SideFX, which we split off about two years ago, and we maintain our own fork.
We'll do our best to point out differences between our plugin and the current thing.
Forgive us if we're wrong.
Obviously, take the official engine docs as gospel over top of any of our examples.
So right now, we're able to generate about 25 different types of actors from Houdini data.
The way we do this is we look for presence of particular attributes on points or primitives.
and we also are able to input pretty much any Unreal Asset or Unreal Actor or the whole map back into Houdini for processing as well.
So the standard approach of inputting data into Houdini is you basically just input geometry, but what happens if the data that you're trying to input is not geometry?
This is basically where it gets complicated.
You essentially have to resort to encoding that data as geometry by basically encoding attributes on that and decoding that on Houdini's side.
So our way to solve this problem is to convert all the data.
In Unreal, basically what we do is we input, we go through all the properties in the Asset.
We convert all the properties to binary JSON.
Same happens to the, if we want to input the Actor as well.
We basically input all the properties of the Actor.
We also traverse all the components and harvest all the data as well.
And we do this recursively.
And on the coding side, we have two operators that we created, custom operators.
One decodes the binary JSON into text JSON so that we can use Python to basically parse that and extract information.
And the second one is the actual geometry unpacking, where basically it knows how to interpret certain types of vectors.
So any kind of mesh, BSP, landscapes, and so on.
So our way of aggregating data is to use a system that we call a recipe system.
A recipe is an aggregation of parameters as well as meshes and foliage types and landscape materials and landscape layer objects, basically anything that's necessary in order for us to construct an island.
So this is an example of a mossy biome forest that was created using a mossy recipe.
So we're going to quickly cover our overall pipeline at kind of a high level, just to give some context to some of the later pieces.
Yeah, so what you're seeing on the left, left, sorry, is just sort of a high level graph that sometimes describes, generally describes the overall connectivity, the main spaces and any sort of navigation that's intended.
So, node one is, or area one is connected to two and is implied that there's sort of a clean navigation route from A to B.
so this is sort of a part of the input that would go into a complete level.
We take that and we have an intermediate 2D stage where we essentially just use a Voronoi fracture cut in two dimensions that we use to resolve a lot of the sort of high-level constraints of the map.
So that's things like there is a path from one to two or whatever that's specified.
It's 2D, but we are taking some three-dimensional things into account, you know, like we don't want a slope of more than, that exceeds a certain, say, value.
So the route from one area to the other may sort of chicane or cut back and forth and that kind of thing.
So there's a lot of stuff that kind of goes into this.
There's also a ton of attributes that we're sort of calculating at that point to figure out how far things are from the island edge.
groups for this is part of a navigation network, this can't be obstructed, and some outcrops and some other stuff which I'll describe.
From there, we work on a sort of initial, like a very clean 3D mesh that describes the general 3D volume, we call it land massing, but the overall 3D shape that has been sort of specified.
We're not covering a lot of the design tools.
We just didn't have time to kind of go over a lot of them, but we can describe those if anyone's interested, just come on up after.
Yeah, so the mesh, like I said, is clean and watertight.
We deal with a lot of bow ties and other more complex 3D situations.
We just want to make sure the squeaky clean results at this point are very predictable for a lot of our next stuff.
We also tag all the primitive groups here.
So basically we know where the cliffs are, where the traversable areas are.
Yeah, a lot of groups and attributes and stuff like that.
define the overall 3D shape like the island bottom is sort of determined by sort of manipulating another 3D fracture, Voronoi fracture, but we're just sort of manipulating the cell points, the cutters basically.
of that to sort of produce sort of a rocky shape that sort of fit our style.
So the gif in the top right is just sort of a 2D section cutting through the island that hopefully describes sort of how that's created.
So the points along the top are actually the same Voronoi cells from the previous, but the ones down the side basically are the new guys.
So we, like I said, we've got outcrops, is just sort of our term for it.
This is just some land that is sort of shoved up or down on the surface of it.
These, their orientation and some other stuff is determined from some sort of fake stress vectors that just give some initial alignment that makes sense and follows kind of where we were headed artistically.
But to be honest, most of these get sort of shoved around in particular, shoving around land and the outcrops themselves.
is the majority of sort of the design input, that iteration, that a level creator would kind of go through.
So usually noodling with this kind of thing is just part of just the exploration.
From there, we determine, or we create a whole number of additional sort of assets that sort of branch from that point, and we instantiate everything from reflection probes to player starts, foliage, and fitted landscape that sort of matches the, well, what we just described earlier.
The overall, number of these stages we can't actually describe at any sort of level of detail within half an hour or whatever that would be meaningful for you.
So rather than covering all of them at like a breakneck pace, we decided to sort of dig into a select few interesting ones and also provide you with sort of a building block again, like a starting point so that anyone that's interested or whatever knows how to like maybe incorporate a piece of this back in their pipelines.
So the first example we created was the point instancing example.
That's probably the easiest one.
You have a Houdini network on the right, which basically generates random points.
It assigns random transformations to those points.
We also have an attribute, a string attribute on a point that is a path to Unreal Asset that is going to be instantiated.
And we also have our plugin-specific attribute, which defines whether to create instant static meshes or static meshes.
And then on the left, you have basically Unreal Asset, sorry, Houdini Asset that is in Unreal.
You can see all the parameters exposed in the detail panel, and you can choose the static mesh to your instance as well as how many points you want.
you'll most likely want to provide additional transform information for a lot of things depending on what you're doing. Um, so we commonly would have say scale and orient. These are just the normal, uh, sorry, scale is a float three oriented, a quaternion float four. Um, you, yeah, I would most likely want to supply those as well to give them a complete transform information for an instance.
These are just the standard Houdini ones, so they refer to the engine docs, Houdini engine docs, sorry, for the kind of better description of what each of those does.
It's actually a great time to point out that random scattering is actually rarely the solution for a lot of the things that we wanted.
It has its place, it's like Perlin noise, it's useful, but it's not the be-all and end-all solution for most of the things you'll probably end up wanting to do.
So quite often we wanted a very, very specific arrangement, probably an artist-controlled arrangement of meshes for a specific thing.
So we're talking about cliffs here, but our solution is actually generic for other situations.
I'll try and cover those.
this target needed to evolve, so we kind of wanted a non-destructive setup that would allow us to continue improving what that would end up looking like, and have that just sort of natively work.
So our approach works off of the idea that if we had a few or a number of samples that described ideal.
So if art were to arrange, in this case, cliff rocks in a way that matched what they wanted to see, Jen wanted to see, we could pick the smartest one that made sense for a new target scenario and adapt it.
So yeah, what we came up with.
Jen gives everything a really fun name, so she called this one quilting.
What we came up with was we would define inside of a blueprint the ideal arrangements that covering sort of a range of scenarios and that kind of thing.
And we could determine a mapping of sort of meshes and transforms from a source scenario to a target scenario.
So we supply a number of parameters, inputs, that would help Jen prioritize certain things.
So this would let us, say, minimize overall scale changes from source to target, non-uniform scale, you know, like stretch along any kind of one axis, and overall just distortion differences, because we're dealing with a very organic island.
We're not, we didn't...
format the stuff from land massing to, you know, snap to a very specific number of scenarios.
You know, it's not a tile set in that regard.
So we needed to supply ways of sort of configuring what was important.
And here, yeah, you can see some example output of our Quilter asset.
And this is untouched from the system, so this is just the raw output of the biome, its recipe, and the Quilter asset along primitives that we have grouped or marked as cliffs, whatever, from some of our earlier things.
Another example of point instancing is foliage.
In our case, we were pumping this into Unreal's instanced foliage actor and system.
So, again, Jen could paint flowers and keep adjusting these the way she was before.
but the result would work with standard instancing and stuff like that, so the difference in our plugin and I think where the current state of the Houdini engine plugin is wouldn't be all that different with that one change.
We liked Unreal's instanced, sorry, foliage propagation volume.
We did like the results, but we wanted to take a little bit more direct control over where instances sat.
specifically where and how it kind of grew.
So we just wanted to add a little bit basically to Unreal's and the best way to do that was to sort of, we rolled our own.
So, we have a very, very simple growth simulation.
I'll describe a little bit more of that in a second.
We use, lightweight, like low res 2D vector field, it's just noise.
We call it like a fake prevailing winds.
And the reason we do that is just so that when we scatter plants and scans, sorry, plants reseed and propagate, that we just get slightly more natural looking, you know, cluster shapes and bunching.
So yeah, you can see here, they're faint, but the plants themselves or whatever, and the vector field.
So this is just a quick GIF of it showing we run a fixed number of generations.
Each plant respects a shade radius of its neighboring plants.
These are all configurable through Recipe, which we showed earlier.
We didn't get into all the details of the Recipe.
Just know that there's a handful of controls like its shade, like what it covers, where and how it grows.
So that can include in water, near water.
in shade or like in cliff shade or stuff like that.
So we just, we gave a few sort of inputs.
Gen actually controlled like the number of generations it would sort of repropagate to.
For perf reasons, we let every plant try and repropagate and if it fails after sort of a couple times or a threshold, we cut it out or we don't allow it to try again kind of thing.
So age is sort of, you know, tracked along with some of the other parameters and that would drive scale, which you can kind of see here.
So the oldest plants are the biggest water.
So we created another example, which is our landscape example.
It's very similar to the point example, but HeightFields and Houdini are primitives.
So it operates on primitives.
We create four HeightFields here.
We tag them with attributes, such as name, also with the path to the landscape info object.
And we create four HeightFields here.
One is visibility, one is the actual HeightField, and two are the landscape layers.
We normalize them here as well.
And on the left is the result.
I meant to say this earlier, but the first few steps with a pipeline involving Houdini Engine can be a little intimidating.
There's a lot to take in.
It's a piece of cake for Mikola, but it's not for me.
So everything you need to theoretically build one of these should be visible or should be covered in this slide.
Ask us if we missed anything.
But if there's any interest, we can definitely supply the assets themselves.
It's probably worth pointing out as well here.
I should have covered that earlier.
We're talking about Unreal here, and sorry, this slide specifically would be very kind of Unreal, but a majority of what we're talking about is actually pretty applicable to Unity as well.
So this is the result of our snowy biome recipe where we adjust the water plane level and entrance that affects the landscape layers and in turn affects the foliage instances.
I forgot to mention that too.
There's, yes, a fitted water plane that basically goes along with the biome results, whatever from some of these.
We do keep, it's worth noting, we keep landscape layers normalized ourself.
Unreal does have its own weighting within the layer info object, sorry, but...
we decided to keep that, the normalized state of it, tracked ourselves.
Just give us more control.
Was able to, much easier to predict or explain exactly why a specific weight ended up being exactly what it was.
The layers and their count, the number of layers, they're all recipe driven as well.
So we just showed some sort of normal-ish materials, but we started with some of that.
But a lot of the look that we were kind of going for really wanted to give sort of a thickness or a depth to a lot of the surfaces within the world.
We wanted a natural-looking buildup that made all the world objects look like they kind of belong there.
So you can see a few examples here with snow.
We did this with sand.
It's potentially moss and some other things as well.
Dubbed that one accumulation.
And so this was just another landscape layer, but this had sort of some special properties that we tied it to an Unreal render target.
And that meant we could do, so we did vertex offset, sort of vertex displacement, essentially, or whatever for the vertices and a correction to the normals after the fact.
Anyone that gets into vertex offset has to deal with that.
Yeah, and this simulates just natural build-up against those world surfaces.
I use quotes with simulate a lot because it's not a simulation and we're not doing any Houdini's capable of all sorts of really cool simulation, but this is not a simulation.
Really the goal here was very configurable.
art configurable profiles and we really wanted Gen to be able to kind of drive the result.
And being able to control like its max depth and that kind of thing or whatever or where and how it built up or whatever is just anyone that works with simulation knows it's a little hard to kind of tame those sometimes and make sure that the results for those are predictable.
Not that that route is impossible, it's just that we opted to drive that a little more carefully.
Here you can see a capture from the engine as well.
This just shows, like, in the case of trees, the windward and sort of leeward profiles that let Jen basically kind of build up a shape that she liked for a natural-looking snow buildup.
Yeah.
I'm glossing over a number of the details with this.
We're not trying to protect anything or just not give those details that just didn't all fit, but we're not trying to hide anything.
So if there's anything that's sort of useful or whatever, feel free to ask us.
We definitely want to fill in the details.
So we're coming basically to describing things that we learned by working on this.
The first one is just don't do the uberasset.
It's hard to maintain in debug.
Then what really worked well for us was decomposing.
all our pipeline into multiple stages.
So we have about 15 assets, I believe, or so.
And they got rearranged a few times, so the modular nature did pay off.
So that allowed us to work in parallel on some of them, as well as allows us to switch certain parts of the pipeline out or disable in order to speed up the cook, and also allows us to achieve some form of polymorphism, as long as we respect the same inputs and outputs.
Also, rapid prototyping, use Python for that.
So that's basically what we do.
And then we optimize that into either VEX nodes or HDK.
And speaking of HDK, it was extremely useful for us to actually spin up the Visual Studio, which is basically attached to an HDK node and debug that, because that was just an easier way to debug geometry than do printf debugging.
Yeah, you can put a Python snippet into any of these networks super simply and it's super powerful.
There's a lot you can do with it.
It's not quite as fast as VEX or HDK, but we've found situations where it can become unwieldy, especially when you start, if you're shoving like you know, three pages worth of stuff into a wrangle, you know, you're getting to a part where I found it just kind of hard to deal with. Um, this, the solutions to some of that, but for just the best debug ability, um, Mika and I are both, I think a little bit more comfortable in a kind of a C world. So that made us, uh, kind of opt for HTK in those cases as well.
So another thing was ability to save the HIP file.
So basically if something went wrong with our system, our artist would save the HIP file, and we would open that scene in Houdini and just inspect that and debug that.
Like we mentioned before, getting a very Unreal compatible output ended up really being, I feel like, a good choice.
We did opt for, or did discuss options like not using Landscape and things like that briefly, but being able to just resume editing one of these levels and exactly how, if it was created by hand or whatever, was super valuable.
You might rephrase some of this as like getting us to kind of make sure we were thinking more Unreal-like in some situations versus getting Jen to say, to think more Houdini-like.
Most of the Houdini stuff is essentially invisible, or not invisible, it's not a great word, but we did not ask Jen to sort of get more Houdini-proficient basically in this.
And the last point is being able to input arbitrary types, not just the geometry back into Houdini was extremely valuable to us.
It actually allowed us to build assets that do deterministic reasoning, essentially.
Yeah, so like raw proper classes back and forth is, yeah, it's really nice.
Whereas the attributes themselves, if you try and contain those any other way, it gets real messy, so.
Yeah, thank you.
Thanks.
