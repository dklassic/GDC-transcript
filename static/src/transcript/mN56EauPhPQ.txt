All right, let's get right into it.
Here's the agenda for this talk.
We're gonna introduce ourselves.
We're gonna talk about the audio direction for Division 2, the Snowdrop engine, the Raycast-based runtime audio systems in Snowdrop.
We're gonna cover procedural reverb and ambient systems for interiors.
We're gonna talk about immersive weather sounds and Dolby Atmos support.
And we'll finish off with a short conclusion.
So my name is Simon Kudryavtsev.
I'm an audio director for the Division series.
I've worked on Division 1, 2, and Far Cry, Assassin's Creed, and World in Conflict.
Hi, my name is Robert Banton.
I'm an audio programmer on the Snowdrop team.
And similarly, I worked on Division 2 with Simon.
And previously, I've worked on other games.
I list here Dirt 4 and Guitar Hero Live as examples.
So we work at Massive Entertainment, which is a Ubisoft studio.
It's been open since 1997, located in the south of Sweden in Malm√∂, and became part of the Ubisoft family in 2008.
Last year we grew beyond more than 670 staff, including 50 different nationalities.
Currently we're working on Division 2 and Avatar.
The audio team during peak production were around 25 people on Division 2, collaborating across Ubisoft Studios in the UK, States, and Europe.
But the core team is around 12 people at Massive.
So this talk is obviously based around the game we shipped last year called The Division 2, and I wanted to kind of first show the kind of core pillars of the game, which is that it is a modern clancy shooter RPG. It's grounded in reality, set in a collapsing world where you are agents of change, and it's always online and code-friendly.
In simpler terms, it's a third-person online co-op shooter in a huge open world where you get sent in as a last resort to try and save the city and fight the enemy factions that seek to take control amongst the chaos.
So whilst the first game was set in New York, we decided to have the sequel to be set in Washington DC.
And why is that? Well, we wanted to give our players a fresh play field.
DC is obviously vastly different compared to New York City.
It's an iconic location with plenty of recognizable landmarks that helps ground things in reality.
And timeline where seven months later after the events of the first game.
It's also hot summer instead of winter in New York.
And it's also has vastly different nature and wildlife, which also gave us a lot of opportunities with sound.
So here's a small teaser from the second game.
How could this have happened here?
Of all the capitals, of all the nations on Earth...
None more vigilant.
Safeguarded.
Prepared.
Was it vanity?
Bad luck, or something more sinister?
One thing is beyond question.
This is where we push back.
This is our defining moment.
If we fail, our nation falls.
We are the free world's last line of defense.
So the audio direction for Division 2.
Obviously we're a Tom Clancy brand, so we wanted to keep things as grounded as possible and where we can't really go too much into sci-fi land.
The world in our game plays a huge part, so making it dynamic, responsive, and immersive was key.
We really wanted to have a polished mix that adapts to player actions and location, so the player can really tell what's going on just with help of audio.
The goals for the team were to improve on the established identity that we did in the first game, but with better assets, improved workflow and tools.
And again, the worlds that we're building are huge, so we needed to come up with a way, a smart way to implement and utilize the world that's already built in our advantage.
We also wanted to increase variety and quality of our assets.
And the Snowdrop engine?
So the Snowdrop engine is developed.
in Melmo by Massive.
It's a cross-platform engine.
It has a graphical editor, and most of the behavior is node-based.
It's flexible.
It has a modular architecture, which allows you to make different types of games with it.
And we also have an abstraction that connects us to the Wwise Audio middleware, which allows us to do all our sound design.
And as an example of the different types of games that you can make with Snowdrop, here are some things that are already shipped.
Obviously, the first two divisions are very similar, but then you have Mario Rabbids versus Kingdom Battle, and Fractured But Whole, and Starlink Battle for Atlas.
These are many other soft titles that are in production that are using Snowdrop.
So the question is, why use Snowdrop?
One of the main things that we try to maintain is a quick iteration speed.
We want to empower the end user, which is why we have the node-based editing.
better real-time processing based on the physical properties of the game world, leverage of offline background processes, so we have nightly builds that do a lot of data processing for us, and better reuse of, in our case, custom DSP algorithms, so we're not constantly authoring new plugins for every game.
And so before we go into the meat of this talk, I wanted to go back and mention the two problems that we wanted to solve going into the Division 2.
It is how do we increase the quality and variety of the audio in our game?
And it is how do we decrease manual labor?
We found that Raycasts for audio was a good solution for us.
So before we go into the different systems based around that, let's learn about the basics for Raycasts.
So, raycasts, what are they used for?
This diagram just demonstrates a plan view of a very simple L-shaped room, and the black dot is where the player is supposed to be.
And I'm demonstrating eight raycasts pointing out, and where they hit tells us where the walls are.
So the raycast can be used to determine space around the player.
Acoustics is about spaces of air between materials.
Estimating those spaces helps us make sonic decisions that match the player's view.
So in the case where you have different types of materials, I've just adapted this diagram to show you that we have metal, we have concrete with brick, and we have some chain link fence.
And here we have the same ray cast, but ideally we want to ignore the chain link fence because that won't actually have an effect on the way the sound is propagated.
So the important thing is we have to contextualize the ray cast.
Concrete walls block sound, chain link fences do not block sound.
How do we differentiate materials?
So, we have this concept of single hit raycasts, piercing hit raycasts, and material IDs.
So in the example where we are doing piercing raycasts, what happens is, is that we go from the point of origin, so again where the black dot is, where the player is standing, we're looking at him from above, and then we raycast out towards the brick wall. But first we hit a chain-link fence. We look at the material ID and we go, we don't care. So we then have to re-raycast.
We hit another chain link fence, we don't care, and so on, until we eventually hit something we do care.
So the thing about heavy use of raycasting is it's very CPU expensive.
So it's vital that it's done efficiently.
As you can see from this diagram, we've done a bunch of raycasts, when in actual fact, ideally, we would have wanted to do just one.
We have to be very conservative about how much we use them, because we're working towards CPU budgets.
Piercing raycasts and material filtering adds a huge performance hit.
This is a problem.
So I just want to show you this is a screenshot I just took from inside Washington DC.
The left hand side is the normal rendered view with some gizmos from the editor, and the right hand side is actually showing the triangles that are actually drawn in the renderer marked up according to material ID. So those different colors mark up whether it's concrete, glass, steel, that kind of thing. The triangles are marked up with a very specific bit mask.
So it's material-based, bit masks used by Raycast to ignore certain mid-triangles.
So as the Raycast goes through the world, we can basically check the triangle and go, do I care about this triangle or not? What we did instead was, was that we ended up essentially marking up the world according to what we collide with, being whether it's actually acoustically opaque or not. So here we have the same diagram again, only now, I'm now demonstrating what actually blocks sound and what doesn't.
Snowdrop's audio raycasts only see acoustically opaque materials.
This helps reduce complexity.
Most of the time, single hit raycasting replaces the piercing raycast that you saw before, and there's no need to use material IDs.
There's no material ID filtering anymore.
So by reducing that complexity, it enables us to do more raycasts than we could do before.
It's available as a filter in the editor for debugging, which is how I made the screenshot.
So one of the first systems we designed around raycasts was Bubble Space.
And the idea is to basically determine the average width around the player.
It's a real-time geometry-based raycast system that determines not only the width, but also if there is anything obstructing above the player.
It's used to blend.
It's basically to ensure that the audio experience of the player is varied based on the surroundings of the player.
And it's used to blend between different type of ambience pads, reverbs, random effects and even switching weapon tails.
So we have a video to show how the system works. Keep in mind that this was really early in development.
of death of death of death of death of death of So let's get into the core module.
So we have these high level requirements.
We need to give the sound designers a metric for the free air around the player's horizon, a metric for the free air above the player.
Bearing in mind in Division 2, the player can't leave the ground unless they go through an animation.
The metrics must be consistent.
This is very important because the sound designers need to hook into those metrics how they mix sound and they need to be reliable.
Metrics must not be unstable or appear glitchy as the player moves around the world.
We don't want any weird edge cases.
The only active when the metric is invalidated by the player's new position.
So we measure and then we stop.
Until the player moves again, we don't remeasure.
That way we can save a bit of CPU time.
So here is another plan view, and I just want to explain four phases.
So one, two, three, four. The black dot is the player's position, the green dots are a raycast that's ended and hit nothing, and a red dot is where a raycast has hit something, and the black outline is the wall. So what we're having here is that At each of these phases, the raycasts are going out in three evenly spaced directions, and then they rotate a little bit. So over the course of four phases, they've covered a full 360. So we've actually got not just three points, but we actually have 12. The raycasts horizontally around the player, it occurs once per frame.
Three raycasts outwards, evenly spaced around the player position.
the distance of the two shortest raycasts are averaged.
So if we have hits, that is.
New average is then added to a four element short history buffer.
So the idea is we're recording the last four measurements we took, and then we're constantly working out what that average is.
Now then, the result of that is, is that this diagram is showing how the player can move around the world.
So one, two, three, four, this is the player moving around, and you can see the resulting horizontal space growing and contracting, depending on the space around it.
Target value updates from an average of three smallest values in the short history buffer.
The raycast directions rotate 1 12th a turn as I demonstrated in the diagram beforehand.
Raycasting begins again and then the final metric smoothed out over 16 frames, approximately half a second, until it reaches the new target value.
The moving average updates the real-time parameter control in Wwise if the delta is significant.
If it's a very small value, less than 5%, we don't bother changing it, and we also spare some CPU on that.
Final metric keeps updating while the player is moving, or until 16 frames have passed.
The reason why we do this is that if the player stops moving, we still have a history buffer to work through.
So essentially, once the player stops moving, we keep processing for 16 frames, and then it stops.
For the zenith plane for above us, we do something quite different.
So here we raycast vertically above the player.
As you can see, there are three raycasts.
Because once per frame, three raycasts upwards evenly spaced around the player position.
All three height measurements have to approximately agree with their average.
If you see in this diagram, I've placed a cube above the player, which means that one of the raycasts hit it and the other two did not.
In this instance, the red dot represents the raycast that we don't agree with because it doesn't match the average.
So therefore, we would only be looking at the two green ones to actually use as our height metric.
When they do agree, the target value is updated.
The final metric smoothed out over four frames until it reaches the new target value.
Yes, and here are some examples of how we've been using the value that we get from the system.
At the bottom there you see our switch container which contains three random containers with our weapon tails for a weapon. And we have large street, narrow street and open field. And as you can see that switch container is actually controlled by a game parameter which is called GP bubble width.
So when the value is 0 to 0.25, I think, roughly, we use the narrow street tails.
If the value is bigger than 0.25, then we use the large street tails.
And if the value is larger than 0.8, we switch our weapon tails to the open field tails.
And this gives a nice effect where the weapons sound like they adapt to the environment in which they're being shot.
Another example of us using the value is modifying the random effects in the ambience to be louder and triggered more often when in a more narrow space.
Finally also here's an example of us blending between different exterior reverbs.
So depending again on the width value we fade in and out different convolution reverbs, everything from alleys to streets to crossroads etc.
So the next system we're gonna talk about is Slapback.
And this is a player-centric outdoors system.
And the goal here was to really add more variety to our weapon tails.
It creates emitters on building walls around the player, and it replicates the player's short pattern with a few distance-based effects and full surround.
So this gives the player more spatial awareness, it gives the guns kind of a nice feel, again helping with spatial awareness.
Okay, so what's actually going on? The following diagram is a plan view again. So the black dot is the player position and we're ray casting in six directions horizontally around the player.
The red dots represent when we hit something and those blue lines are the surface normal we get back from that that raycast hit. The green dots when we hit nothing and then blue dot is where we think there is a reflection point. So the rule that we use is similar to optics where we essentially use the surface normal to infer where the reflection point is.
should be. So what we do is we use a 3D emitter pool. We compute once per frame if the player's movement exceeds a certain threshold. That's important so we don't keep processing.
Raycasts around the azimuth and upwards of the player position. We use simple 3D geometry to infer new candidate reflection points from raycast positions and normals.
So what we're actually doing is we're creating candidates.
We first of all ray cast out those six candidate positions and we may get some reflection points, some blue dots.
We then compare the ray cast position again.
So we ray cast out from the black dot to the blue dot and see if we actually get the same result.
If we do, then that candidate's good.
If we don't, that means it's just anomalous.
So we basically remove it from the candidate list.
Any duplicates also, because you might end up with blue dots overlapping each other, they also get removed.
And this is what we call basically a series of sanity checks.
There are existing reflection points that we had from the last time we processed.
If they are updated, they get culled.
And the idea is that they're streaming something, so they fade out with a 200 millisecond fade out.
And then any candlelight reflections left over become a new point, and they start playing again, so they start streaming.
It computes a series of sanity checks, as I said, to remove erroneous reflection points.
duplicates or near duplicates based on threshold. So sometimes they don't line up exactly but they might be close enough to cause a flamming effect so remove them anyway. So what is actually causing the streaming to occur? So we have here the idea of a double buffer. So the broadcaster module is this place where sound is recorded and played back. It supports multiple broadcast channels up to eight.
each broadcast channel consists of up to eight audio channels. So when I talk about a broadcast channel I just mean a broad like a radio channel essentially or some kind of not sound. So what's happening is is that we've registered so every time Wwise finishes its render it calls back to the system and it says please swap these two pointers we have one pointing at the record buffer and one at the playback buffer and they're constantly swapping frame by frame.
That's what I mean by double buffer system. Some of you will be familiar with this some of you will not be.
In a sense, it's like a digital tape loop. So we have a record head and a playback head. So whilst one pointer is recording in, another one is playing out. The loop gets moved on from a callback, we register it in the rendering chain as I already said. This actually adds a little bit of latency.
So if our buffer size is 1024 samples, that means the latency we add by creating this recording loop is 1024 samples. But we're not so bothered by that because we're going to add a bigger delay in a moment anyway.
So we just take that on the chin.
So how do we take sound into the record buffer?
We have a sync plugin inside Wwise.
This is a fake audio device, so it looks like an audio device, but in actual fact it's communicating with the broadcast module.
What it's doing is, is essentially...
taking when Wwise is essentially completing its render passes and all the master mixer graph endpoints, it then sends sound that has gone to that particular bus to the record buffer, based on the record channel that's been set in the plugin, of which there are eight.
Sends audio to the broadcast record head, as I said, and then controls what broadcast channel it uses and how many audio channels it needs.
It's called once at the end of each render pass.
So every time we render out 1024 samples, this gets called at the end.
Now, the receiver plugin is reading out from the broadcaster system.
It gets audio from the broadcaster's playback head.
It's triggered by the core module play event.
So essentially, when you have this actor mixer graph, every time one of those nodes that has this plugin on it is called to basically play out, it starts rendering a bus.
It's faded out by the core module stop event.
What I said, that was that 200 millisecond fade out.
That's what causes it.
So the control is what broadcast channel it reads from.
So the idea is that you can record out to one channel, but you can have multiple things reading from that.
So we could actually have multiple receiver plugins reading out from the same playback head.
It gets its audio channel layout from the broadcaster by way of the sync plugin.
So whatever the channel layout was, if we said it was 7.1, that's what we receive.
It's called once at the beginning of each render pass.
And as you see from the diagram, what I mean is that when the active mixer graph causes nodes to render, that's when this is called.
The actual effect is then applied to the receiving end.
So after we get sound back from the broadcaster, we then apply a delay effect.
So we had to make a custom Wwise effect plugin.
The reason why we did this is because we need to solve the problem of flexible delay time.
So just to explain that, we have a circular buffer, so what happens is as each sample time progresses, we have two pointers chasing each other, and the gap between them is the delay we actually get.
In this case it's about two seconds of delay, because that's what we need to actually have a realistic delay effect in a world.
The input sample buffer is constantly being updated, and then as the output samples are pulled out, we actually do a power complementary interpolation.
Basically, because we are moving so quickly, we have to be able to essentially move out any artifacts.
and this is what deals with that problem.
We also have phase control because reflections usually come back the opposite phase that they went in.
This is actually an oversimplification, but it's a good optimization.
And we also have some filter EQ in the plugin so that we can actually modify the sound that gets reflected back.
And then that becomes the output sample, which goes to the output buffer.
And that's what gets sent on from the source plugin into the Active Mixer node.
As I said, it has a built-in EQ and phase inversion.
There's one instance per slapback emitter.
So as we play through a slapback emitter.
this effect is at the end of its DSP chain before it gets sent back into the mix engine of Wwise.
Wait one second, okay. So just to demonstrate here, the top left one is the, that's the sync plugin.
So when I said before, you set the draw cross channel and the number of audio channels we actually need to describe and that basically gives us.
what gets recorded.
The bottom left is the source plugins.
That's what's coming back from the broadcaster.
And then all we do there is select what channel we want to read in.
And then the actual emitters of course are 3D.
So therefore we position them accordingly.
And then what we do is then we have the delay time is predicated by those Raycasts we were making.
The distance from the player to the Raycast reflection point is what we compute for the delay time.
And as you can see here, the actual conversion scale that we use isn't actually a realistic one.
We wanted to have a kind of like a movie type dramatic effect, so that's why it's curved like that.
And here's an example of us using the slapback bus here on a weapon shot sound.
At first we tried it on the whole weapon sound and quickly realized that we maybe don't want to have the weapon mechanics being reflected off of walls so that's why it's applied to the shot only.
So the next thing we're going to talk about today is procedural ambience.
And before we go further into this, I just wanted to do a quick shout out to Ubisoft Reflections audio team, especially Seb Thomas and Andy Muchu, who were integral for basically designing and coding the system.
But basically the idea here was to kind of add more variety and remove the manual labor from the sound designers for covering all of our interior spaces.
In Division 2 we had over 2.5 thousand...
rooms, what we call interior rooms. And the problem that we wanted to solve is that we didn't really have enough time for applying manual sound design and reverb design for each of those spaces.
So we thought about how can we use the already existing geometry and data and basically designed a system a procedural offline system to generate impulse responses for each room. Those impulses would be based on each room size and the materials of those of those rooms. And also coupled with that we designed a custom Wwise Convolution Reaver plugin to load those impulse responses at runtime.
And we even took it one step further and used the IRs that were generated for each room to create an ambient sound for each of those rooms as well.
We did this using the IRs combined with brown noise and the wet signal of the props of that room.
But I'll go into more detail about that shortly.
So the convolution implementation.
is active when the player is indoors.
We know the player is indoors because of terrain markup, which is also used by the graphics people.
It's instance partition convolution engine.
So we have multiple engines and then we can appropriate them as needed.
The actual processing occurs in Snowdrop.
So when we talk about a custom convolution plugin, the actual plugin part is just feeding sound back from Snowdrop.
The total number of convolution engines is scalable.
as it happens that we used four this time, but that's according to how much CPU bandwidth we had to play with.
The Customize effect calls into Snowdrop via a dedicated interface.
So that's how we've done it.
We have basically a callback interface to pull out the audio that we processed inside Snowdrop.
Okay.
And this is a debug view of the kind of thing when you start ray tracing inside a large space like that, all of those white markers are reflection points generated by the raycast.
What we do is that we generate an impulse response that is static for the room space.
So this would be one single impulse response.
But because we purposely filter out the early reflections first, all you hear is the late reflections and in actual fact, they don't really change that much.
So it doesn't really matter that the player moves around that space without it really changing.
Let's hear it.
He's trying to get behind you!
Cover that target!
Get back!
Fuck off!
Cover won't save you!
Go! I got your back!
Important thing to mention is that we didn't use this system everywhere.
Of course, where we wanted to use ambience to kind of help the narrative of the game or if the space was a very particular space, we of course designed manual ambiences and reverbs for those spaces and simply did an override for whatever was generated there by the procedural system.
But we were really happy with the way the reverbs were sounding and so we wanted to basically find out if there was any way of using those reverbs to generate an ambience pad for those rooms as well.
Since our game basically takes place in a scenario where most spaces are abandoned, we wanted to avoid making create the sound of an empty office tasks for our sound designers.
So we're thinking about what actually defines a room tone and we're experimenting in Wwise with noise and reverbs and here's a little example of how we reached the conclusion that Reverb is actually a very integral part of a room tone.
Here's just using noise in Wwise, a short example of this.
you So, whilst researching this topic of acoustics and rooms and room tones, another interesting concept we found is critical distance.
And this is basically where the level of the direct sound equals the level of the reflected sound.
So in the graph here you can see the black sound pressure line flattening out as it passes the critical distance point.
This knowledge gave us more confidence to use generated IRs from the rooms to create the actual ambient sounds.
So in an offline process, there were two virtual microphones placed in each room, 22 centimeters apart, which is roughly the average head size.
And they recorded the wet sound of the props, blended it with brown noise and produced perfect 15-second loops for each of those spaces.
hear how the results actually sounded in the game.
And obviously they're not meant to be played this loud.
While a juror is in theroom, the jury-in-waiting works a special effect on the statute ofende of the juror.
The jury-in-waiting updates that the jury has given the evidence of this specific juridical aspect of the legal procedure.
The jury is exactly what the juror did.
The jury will remember the juror.
The jury will remind us of the jurors' predictions, but also remind us of the jurors' mistakes.
So now we're going to talk a bit about immersive weather sounds.
We knew that weather was going to play a huge part in our game and so therefore I wanted us to see if we could create a system that gives the weather sounds more depth.
So we created an emitter manager to control all of the emitters on all of the props in our game, and this allowed us to basically play back the sounds of rain or wind playing on the actual props.
It's of course material-based, so depending on what material the prop is, that would be taken into account when playing back the sound.
And it's also a cool thing because it's props that otherwise don't make any sound like tables or road signs But we can actually play sound on them now if it rains or if there's a heavy wind Nice to see you agent Hello there.
So, it was actually when I first heard what we did in the prototype for the first weather system experiment that we did, when I heard sounds of rain above me, that I was thinking that Dolby Atmos would be a really nice fit for our game.
And as we kept developing the game, the systems that we designed, whether that's bubble space, slap back, or...
as I mentioned the weather sounds or even like the in-game things like helicopters and drones it just made sense to kind of try out this technology and the first time I tried it I felt not only that it was enhancing the mix in the vertical plane but also in the horizontal one where I could feel better perception of enemies behind me and things in front of me, it just felt like we gave the mix that much more clarity.
So it was a pretty obvious choice, and Dolby were also very helpful with integrating this into our game.
And of course, not everyone has Dolby Atmos Theater at home, but most of us have headphones, which Dolby supports with Atmos for headphones.
So, yeah, I thought it was a really good fit.
And this brings us to the quick summary of the talk today.
So we talked about us wanting to create a dynamic audio experience for our players, and we utilized already existing geometry and raycasts to analyze the surroundings.
We then used that output of the raycast to modulate and change the soundscape.
The procedural system helped us with greater variety, more quality, and less manual labor, and Atmos was a great match for the systems that we designed, and it opened up for more space in the mix.
Thank you very much for listening.
Thank you.
Oh, by the way, if you want to find out more about the sound of The Division 2, you can search for it on YouTube.
