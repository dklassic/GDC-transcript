Hi, everyone. Welcome to GDC.
Today our topic is finding harmony in anime style and physically based rendering.
So first, a brief introduction about me and my team.
My name is Chen Zhou. I joined NetEase Games back in 2014 and work for Tensha division.
which is one known that is game studios with teams in Hong Kong and also in Guangzhou and Shanghai.
As a technical lead, my work is mainly focused on game engine and graphics development, collaborating with artists and technical artists to build games with nice looking visuals and acceptable performance budgets.
Over the years, I have worked on and shipped several online mobile games.
As you can see, most of them are manga or anime styled.
Currently, we are working on Exordino 1's Mirage, which is a multi-character RPG based on the same IP as our online game, Exordino 1's MOBA.
Here is a real-time rendering from the game.
For this game, we are trying to create a world that combines the art style of anime and elements of photorealism with nearly 100 playable characters and over 100 levels.
including indoor settings with complex lighting, modern day city, and outdoor environments in both day and night conditions, which brings us to today's topic.
The game's targeting all platforms on mobile devices and PCs, and it's built with our in-house game engine, Messiah.
I should point out, since the game's still in development, so any images shown here don't represent the final quality.
This presentation is split into two parts.
Part one is some background knowledge on the anime art style, its typical characteristics, traditional cell shading techniques, its real-world limitations, and some possible solutions for combining cell shading and physically-based rendering.
While in part two, I'll introduce our goals and methods for building a physics-based game world while preserving notable anime features in our characters.
using a single pass deferred rendering pipeline, special lighting and shadow techniques with customized shading models, as well as an in-game look dev pipeline we built for integrating characters with all the environments.
And finally, conclusions.
So first, what is an anime art style?
Or in other words, what makes characters feel like they are anime?
Here is a classic theme from the traditional 2D animation.
we can see a lot of traits of anime in this scene.
Looking at the character, the most obvious clue is a flat face with manualized colors and shadows.
Another clue is the silhouette lines, which are important for creating a sense of form and structure.
For example, a piece of curved line can become the character's nose.
And if we look closer, we see the face is in a non-perspective projection.
which is just saying, the size and position of mouth and nose are off-center.
The mouth is a bit off to the right.
Moving on to the hair, the character's hair has a ring-like and stroking specular shine.
We like to call this a shiny headband.
And then the eyes and eyebrows.
They can be partially blocked by locks of hair, but redrawn on top of it.
We call this the see-through hair.
And to keep the clean look of anime images, especially around the face, in most cases, they use only one main light source, and the shadows have hard edges formed by hair blocking the light.
These traits are the stylistic conventions of anime creators formed over many decades, and they come together to form the anime art style that audiences have come to expect.
To render this art style in real time, a lot of solutions have already been explored, thanks to the emergence and popularity of anime games on the market.
So we started to work on our first Toon style game in 2014.
And to get our feet wet, we first experimented with 2D sprites.
Because it was simple and 100% controllable by concept artists, we used the morph animation for each piece of sprite.
and the shading is simple, just samples the texture and blends with the unified globe lighting from Scene Ambient Map. This had the added benefit of being performance friendly for mobile devices at that time. I might add, in recent years, other games have started using additional techniques for 2D, like adding localized lighting and rigging, which brings more variety to the sprites tag.
Then in 2015, inspired by the GDC talk Guilty Gear XR Artstyle, we started to explore cell shading in our game.
Cell shading, also known as toon shading, is a type of non-photorealistic rendering technique designed to make computer graphics appear like they are hand-drawn.
The basic principle is pretty intuitive. To reduce the color gradient to a base color and a shadow color, a step function is used by passing the dot product of normal vector from geometry and the light direction. And after that, we can add outline slats to the geometry during post-processing, using edge detection or natural backface paths with vertex biasing.
We explore a lot of techniques or tricks in real-time cel shading in our previous games.
As shown on the right, most of them focus on character rendering.
For example, fixed or per-character lighting to get the best look for each individual character.
We can use smooth step functions to create tiny gradients between light and dark for the skin, which mimics the feel of subsurface scattering.
We also use multi-channel rep textures for easier artist control.
modified normal vector in vertices for lighting, and pre-integrated facial shadow masks to suit most light directions, and customized tone mappings to enhance color precision in HDR pipeline, and many others so on.
All of them are designed to keep the final shading result as close as possible to the anime art style, a hand-drawn concept art.
So shading is great for making games look like anime, especially for the characters, because most 2D animations focus on the characters and less on the environment.
But for games, that's not always true.
In our previous game, MMORPG set in modern day city, cell shading had limited our environment design and in turn, diminished the player exploration experience.
and it was hard to create complex scenes with lots of details, since hand-drawing the base color is a critical part of the cell shading process.
This step didn't have clear standards, so results varied from artist to artist.
Last but not least, as mobile devices grew rapidly in terms of performance, many modern rendering techniques become feasible on all platforms.
But we found it very hard to introduce them into a game based on cell shading.
Compared to traditional cell shading, physically-based rendering imposes fewer restrictions.
It describes both the mathematical theory as well as its practical implementations.
PBR decouples material and lighting, so it is easy to create assets which are well-suited to all kinds of lighting environments.
A wide range of materials can be defined quantitatively using close-to-real-world physical parameters.
And most important of all, the standardized PBR production process has been widely adopted by the game industry.
And that level of universal acceptance means game asset creation can be outsourced to many external suppliers, which is very helpful for building a large game world.
So we started to explore the possibility of creating a physics-based world while keeping anime charms.
Here is a side-by-side comparison of the concept art from our current Extraordinary Ones MOBA and our upcoming new game based on the same IP.
We found that skin, especially around the face, eyes and hair are the major factor of what gives us the look and feel of anime.
Imagine a photorealistic head on a cartoony body that would feel really bizarre and out of place, but the reverse actually feels quite harmonious.
Realistic materials on cloth, such as silks, velvet, or translucent materials are acceptable, and complex lighting and shadow will be a plus as long as they contribute a harmonious overall look to the character.
To prove this opinion, we conducted a survey with these two concept arts above, among a group of core players from our Extraordinary Ones MOBA game.
Everyone interviewed agreed that both pictures are the same anime character that they are currently playing in-game, and over 70% of people prefer the new version on the right, and most of them are anime fans.
So here comes the goals for our game's graphics.
Combine the look and feel of anime and PBR, build a wide variety of materials suited for all kinds of lighting environments, and take advantage of modern rendering techniques.
while keeping the graphics flexible to handle everything from low-end mobile devices to high-end PCs, and try to keep a low overhead and be compatible with as many devices as possible.
This is what we get in our game.
As you can see, the entire environment is under photorealism, while most of the traits of the anime mentioned above can be found on the character.
So in general, our approach is to create a graphics pipeline using a photorealistic deferred shading framework, supporting various factors such as sunlight, local lights, global illumination, shadows, and ambient occlusion, but adding special shading models for parts of the character, like rendering skin, hair, and eyes, and preserving the constraints of energy conservation when integrating different BXDF functions.
We implemented custom rendering techniques to imitate noble features into the anime, which leads to a very similar look and feel.
And we built an in-game look dev pipeline for character and the environment artists to perform sanity checks on material captures and ensure the overall shading result looks harmonious from all feasible positions and angles in the game world.
Here is our Minimoji buffer layout for mobile devices.
which is designed to be compatible and performant for as many devices as possible.
At the low end, our devices with max color attachments equals to 4 on the GLES, which keeps the color buffer as thin as 128 bits to meet the performance guidance of Mali GPUs.
Thanks to the TBDR architecture of modern mobile GPUs, fragment shaders can read the render target's data from tile memory.
perform calculations, and write composition results back.
The tree buffer generating, deferred lighting, and composition are implemented in a single render pass.
This is very important, not only for deferred shading with limited memory and bandwidth, but also some custom animated stylization passes we will introduce later.
First, the main light source, mostly the sun and moon when outdoors, or some major local light when indoors.
To support variety shading models for characters such as skin, hair, and eyes, we use a hybrid forward and deferred lighting method.
For the forward part, when generating the G-buffers, we calculate the indirect light, using baked light maps for static models to restore light info, while dynamic models interpolating the SH coefficients from light probes to get the incident lighting function.
Materials emission is also added to G-buffer A.
As for those anime-style shading models, they are shaded with indirect and direct main light while the GBuffer is being generated.
And the results are also stored into GBufferA, just like what we normally do in a forward rendering pipeline.
One beetle stencil is marked as forward-shaded, so those pixels get masked out when the GPU comes to this particular main light.
The reason for pre-calculating the main light is that not all parameters for anime-style shading models can be held in the limited GBuffer channels.
This step also gives a speed boost for the lighting path, since fewer branch instructions are needed for integrating different BXDFs in Fragment Shader.
We also sample the Cascadia shadow map, EMV map for image-based specular lighting, and a real-time reflection map in the base and lighting paths.
which is done by a lot of other games as well.
A significant advantage of deferred shading is that we have an easier time rendering large numbers of dynamic lights.
These local lights helps a lot to make the environment and the character looks well-integrated.
We put all the point, spot, and red lights in a single sub-path.
The number of shadow-producing lights is limited according to the device performance level.
Shadow maps are cached for those shadow lights and updated when needed.
Other non-shadow lights often act as ambient lights and are drawn instance with simple calculations.
For devices with Apple A11 and later, rest-order groups helps to increase the parallelization of the GPU's fragment shader for shading all these lights.
The most notable trait of anime characters are their flat faces.
We implement this using the intuitive method of shifting normals to the face's forward vector.
The curve intensity is provided by a texture, so artists get precise control of lighting in the face.
For example, the soft edges of the cheeks and the shadows around the nose tip.
Flat faces and slanted normals are not enough because they can be modified by scanning the animation.
and aren't very accurate after interpolation.
The lighting equations are kept the same, except we specify a normal vector for calculation, so the face can still interface with multiple local lights naturally.
And to avoid flickering lights on the character during slight movements, such as breathing.
The face samples lower degree of spherical harmonics for the GI part.
The flat face gets along quite well with physically based materials in the front view, but sometimes gets an overexposed color when looked at from the side, under certain lighting angles.
The lighting with a specified normal vector pointing forward from the face is mathematically correct, but the resulting image looks off when viewed from the right.
Here is a set of diagrams to illustrate this problem.
For normal human faces, light reflection is a range of values, while the flat anime face acts as one single plane.
So now our solution is for local lights, no casting shadows, to reweight the light from VXDF using a wrap function affected by V dot L.
And for the main light, we use a precomputed path for statistical coverage calculation using a shadow map, which will be mentioned later.
By doing this, we get a more natural-looking result.
And for rendering skin other than the face, we use a different shading model.
To make skin on the body feel like part of the environment, we use the same lighting equations as physically-based materials, but intentionally manipulate the colors.
Instead of an artist giving us a shadow color or a ramp texture, like in traditional cell shading, the color manipulation is hard-coded and then turned into a formula for the pixel shader.
This is important especially for multi-character games, since hand-drawn colors differ from artist to artist, which will lead to a lot of color variance when placing multiple characters into a single shot.
For this shading model, We preserve the anime characteristic of hard transitions from light to dark.
Then we shift the hue by the luminance of direct light and increase saturation, especially for dark areas and edges, according to the saturation of direct and indirect lights.
By manipulating the color, we get enhanced structural details and better contrast.
Next, we'll talk about shadow rendering.
Most cell shading games don't bother with incrementing casted shadow for their characters, since they usually calculate a generalized shadow color using a well-designed algorithm.
While for PBR, detailed shadows and ambient occlusions are important components.
We need to make the two co-exist harmoniously, since many parts of the character's body use realistic materials. Like a lot of games, we use a cascaded shadow map for the main light.
And since anime-style cutscenes often feature extreme close-up shots, where the character's face might occupy the entire screen, the shadows also need to be high quality, especially self-shadows produced by fringe soloxel hair, which are the only form structures on top of a flat face.
And we have to create hard outline shadows for our anime-style faces.
This is showing the same cutscene, but with different ranges for the lightbox, from 0.2 to 0.8.
As we can see in the image, traditional per-object shadow map like the 0.8 result still leads to soft shadows around the face, which is not what we want for our anime style.
To achieve more accurate close-up shadows, we implemented a view-adaptive shadow system for cutscenes that support distances from close-ups to multi-person wide shots.
This is done by calculating several pivot point capsules in screen space, then adjusting the center position and size of frustum when generating shadow map of the first cascade to frame the characters tightly within the camera.
The adaptive algorithm can be fitted for nearly all the shots in the game.
and it's how we achieve that point two shot above.
But there are still edge cases that require manual adjustment.
We provide a one-click method for artists to adjust the shadow as they see fit and align it for timeline frames.
Lighting and shadow can be customized for individual shots during cut scenes, while it doesn't work with outdoor environments due to the time of day system.
We added a hair shadow proxy pass to create a nice self-shadow on face under any lighting circumstances.
A simplified skeletal mesh with the same skinny data as the hair is drawn between GBuffer and lighting, with a slight vertex position offset according to the face vector and main light direction.
We fetch the basic data from GBuffer and overwrite the result by recalculating only the indirect light from light props.
we weaken the indirect light result so that the hair shadow look is always there, even viewed from the backlight.
From this perspective, the hair shadow proxy logically works as a shadow and ambient occlusion path.
And it's only applied to the face, since we can determine that by the underlying shading model ID.
Besides self-shadows, we also wanted to have environment shadows in our game.
which brings the partial shadow program.
This is where light is masked out irregularly for the face.
For instance, if a character was standing under a tree, there should be double shadows on his face.
These shadows are totally out of control and makes the enemy face like a mess.
So now our solution is to use a pre-compute path for all characters' face in a line-shaped render target.
and we built a polygon for each face in FragmentShader, find the average in shadow value from the shadow maps, and store it into one pixel.
Self-shadow is filtered out by adjusting the depth in light space when comparing to the shadow map on dynamic objects.
Now, when we load the corresponding pixel, we get a flattened shadow on the face.
Eight floats of uniform data for each character are passed to GPU for pre-computing.
Notice that there is one standing for weather in the same character as last frame.
As an added bonus, this gives us smooth shadow transitions temporarily for free, which is very helpful when navigating within the game.
The anisotropic specular hair shine is a typical anime trait, also known as a shiny headband.
We shade the hair with classic Kajiaki model, which shifts tangents along to the direction of the normal.
But we don't use the traditional half-angle vector for strand specular lighting, because when there are multiple local lights nearby, the shine will become spread out across the hair strand in each direction, which while realistic, doesn't convey the sense of anime art.
Instead, we use a fixed vector interpolated from view direction and main light direction.
store the main light speculum mask into G-Buffer and reuse it instead of GGX for other local lights.
By doing this, we get a concentrated anime shine spot as shown in figure 3.
The see-through hair is another important trait in anime.
Since the eyes and eyebrows are important for conveying the character's personality, some comic artists choose to draw them on top of the hair.
The standard way to achieve a translucent look is using alpha blend, which doesn't work well here, because the shadows would also be blended and make the hair look dirty.
We accomplish this effect by using programmable blending in the G-Buffer generating path.
The hair is drawn with lowest priority, so the GPU can fetch the underlying shading model ID and distinguish the maxed out eyes to perform a differential blending.
On devices that do not support separate blend mode, an extra draw path is needed for filling the other G-buffers.
Since we are using a deferred rendering pipeline, traditional translucent materials are blended after the lighting path, which leads to inconsistent lighting results from opaque elements, because only constant lights are being calculated.
To implement these materials correctly, we store the triangle face info inside the shading model ID with different diesel patterns stored in GBuffer.
And we run a dedicated convolution blur path to do pattern recognization and correct multi-layer color blending.
And finally, the colors are merged back into the scene using a stencil mask.
Using silhouette lines and other notable features for anime will not exist in photorealistic rendering.
We try to keep the outline as thin as 1 or 2 pixels by extruding normals in clip space, with consideration for the render target size.
A second set of smoothed vertex normal data are stored in vertex color for use by Silhouette vertex shader.
We draw the lines in GBuffer as ordinary render elements, with a specified shading model ID.
By doing this this way, we preserve the ability to render these lines later to correspond with different environments, and also keep the right linear depth buffer for use in lighting and post-processes, for example depth of field.
But because these lines are so thin, they may simply disappear after anti-aliasing.
to deal with temporal anti-aliasing.
We did some special treatment on those pixels in velocity buffer with less worldview projection to recalculate their precise position.
Besides shading techniques, facial expression animations also contribute to the anime feel.
Using non-perspective projection for the eyes, nose, and mouth is a notable feature in 2D.
Anime characters often have large eyes, but tiny noses and mouths.
This makes their 3D model look natural from the front, but off when viewed from the side.
To prep for this, we built an adaptive facial modifier system based on 4 to 6 poses, manipulated from fixed angles by animation artists.
and adjust the bone matrices at runtime, corresponding to the original animation data and the pitch and yaw of the camera in the facial space.
We simplify the calculation at runtime.
The modified bones are all set up using the same coordinate space.
Since we are building a game with nearly 100 playable characters and over 100 levels.
we need to check if the characters can integrate harmoniously with all the environments.
We used a lot of custom shading models besides the standard TBR ones in our characters.
So as the amount of scenes that needed to be checked grew, we built an in-game pipeline for validation and signed off new characters and environment assets.
For characters, we provided artists with an engine look dev tool with fast environment switching and neatly arranged reference spheres in standard PBR roughness and metallic gradient to help them quickly correct for issues in customer shading models under complex lighting conditions.
And we have a fast character parade mode to check for the consistency of animated shading models in a given environment.
since those non-standard parameters can vary from artist to artist.
And as for environment design, the tool has an option to quickly place characters randomly in a level at a specified density.
We can pile a bunch of characters on top of the generated nail mesh, and auto-rotate them or play random animations at our command.
On larger levels, the characters are placed in a fixed area of interest and get updated when the in-game camera flies over the entire map.
This is how we do look-depth with a large number of characters and environment combinations, using real-time tools to help the character and the environment team get immediate feedback and iterate and polish the game.
Finally, let's look at the summary.
In this presentation, we talked about stylistic conventions of anime art style, different approaches of achieving anime-style visual creation in game, and our choice for Extraordinary Ones Mirage.
I shared some key techniques we used for recreating anime features in 3D, and how to coexist in harmony with physically-based rendering.
The thought process for bringing modern graphics techniques to stylized rendering can also be applied to other art styles.
That's all. Hope this presentation has given you some food for thought. If you have any questions, please feel free to contact me by email. Thank you for listening.
