Hello?
All right.
Hi.
So my name is Debra Henderson and I'm going to be talking today about how design can ensure the impartial scientific fairness in user research.
But first I want to introduce myself and ask you to please turn off all of your phones and various noise devices, those are what they're referring to.
I'm going to talk today about a bunch of different things.
But first, I am a user researcher.
I have a Ph.D. in cognitive psychology.
I've worked on a whole bunch of games.
You'll see a lot of examples from the games that I've worked on in this talk.
But first and foremost, what I am is I am a designer.
I am a designer of research, and I have come to ask a favor of those of you in the audience who design games, which is, I'd really like to stop talking about methods.
And this may seem like a very odd request to make as a researcher.
I in fact love methods.
Boy do I love methods.
But the reason I want to stop talking about them is because what methods are for me is they are tools.
They are tools that I use to design things.
And when people want to know whether or not my designs are good, the question you want to ask is not, did you use an axe?
But are you an axe murderer?
The way that you use a tool is actually vastly more important than the tool itself.
And so instead of talking about methods, these are the three things that I want to discuss.
One, I want to talk about what UR needs from game design.
Because, let me be clear, without the assistance of game designers, I will never be successful.
Two, I want to talk about how I design bespoke research, because every single piece of research I design is in fact bespoke.
And three, I want to talk about how to ensure a just interpretation of data, because I think this is probably the largest friction point between game design and research, is making sure that the interpretation is in fact fair.
So to begin with, I'm going to talk about what you are needs from game designers.
And to talk through this, I'm going to use a case study.
It's a little bit of an unusual case study because it's from the 1740s.
Allow me to introduce you to a designer.
This is Samuel Richardson.
And he is a very good designer.
He designed, he's known as sort of the father of the modern novel.
So the field that he is working in is the novel, right?
That's the hot new technological wonder for entertainment of the day.
And he builds basically the first successful franchise.
And that I mean, he writes two books.
One's named Pamela, one is named Clarissa.
In the tradition of all fine franchises, they are essentially identical books with different titles.
And he is hugely successful with both of them.
Okay, and the real problem that he wants to solve is how do you get people to care about fiction?
And this might seem like a problem that some of you in this room are also working about how you get people to care about your imaginary imaginary worlds and things like that, but he means this really literally because this may not be obvious, but the problem with fiction is it's all lies.
Why would anyone care about a character they know is imaginary?
This is the kind of problem he is trying to solve.
And his main theory for how to solve this is something that's going to be known as the willing suspension of disbelief.
He thinks that if you just get people to treat fiction the same way they would treat nonfiction, you can get the right kind of engagement.
You can build presence, right?
So this is an idea that Coleridge is later going to sort of codify when he says, a semblance of truth.
sufficient to procure for those these shadows of imagination, that willing suspension of disbelief, for the moment which constitutes poetic faith. And so the goal that Richardson has is to sort of minimize the gap between fiction and nonfiction, so people can treat his fiction as though it is nonfiction.
And his solution for this is actually a really interesting one because it is a technical solution. It is a solution that can be applied independent of content. He thinks The future of the novel is the epistolary novel.
So the epistolary style is where you write letters back and forth.
And he thinks this for really beautiful reasons.
He sits down and just tries to calculate how you minimize the gaps between fiction and nonfiction.
He points out, with this letter writing style, you just minimize the number of leaps of faith you ask your consumer to make.
So you have to assume the following things.
One, you have to assume that people are slightly more verbose than usual.
And remember, this is a time when letters are actually delivered three to five times a day, so it's more like email and less like snail mail.
Second, you have to assume that your letter writers have good memories, so they include all the good bits in their letters.
And third, you have to assume that people have an unusual propensity to keep their letters, because in theory, what the novel is is a collection of those letters.
And what I love about this is this is both a beautiful demonstration of design thinking.
This is good, good, good, clean, crisp design thinking.
And it's also just fundamentally wrong.
And it's in that sort of tension between really good design thinking and like anybody who's ever actually read the book and thought, no, the epistolary side, I don't need that.
Like that tension, that's not only where user research lives.
That's actually where science lives.
Because what we have here is falsifiability.
And this is the one thing that every scientist will have in common, no matter what their field is, is a deep and profound belief in falsifiability.
And falsifiability is the idea that a scientific theory must be capable of conflicting with possible or conceivable observations.
This is how science builds its identity.
In fact, when we talk about science, we will draw a line between science and not science that is entirely based on falsifiability.
So for instance, Albert Einstein is not science.
In 1905, Albert Einstein has what is known as the Annus Meroblis, where he publishes four papers, one on the photoelectric effect, one on Brownian motion, one on special relativity, and one on that old chestnut, E equals mc squared.
Of all of these papers, he wins the Nobel Prize for the first one, the photoelectric effect.
And the reason he gets the prize for that is because he's a physicist.
is not because his thinking was better in this paper than it was in anything else, but because a man named Millikan could come along and test it, and he could run an experiment, and he could set Einstein up to fail.
And you know what? Einstein didn't fail.
And because of that, we had evidence, we had scientific evidence that he was on the right track.
And it's important to recognize, too, that falsified is actually a relatively new idea.
So Popper, for instance, died in 1994.
This is all 20th century thinking for science.
But for games, I actually think this idea is core.
Because, as Lou Gehrig once observed, I love to win, but I love to lose almost as much.
I love the thrill of victory, and I also love the challenge of defeat.
Game designers understand that a victory isn't a victory because it's a win.
It's a victory because it could have been a loss, right?
The value of the win is not in making a whole bunch of cheap wins.
It's often in really sort of riding that ragged edge of destruction, right?
Getting really close to failure and pulling up, because that's a real win.
You really earned that one.
So what I need from design is I need you to be Einstein.
I need you to build me that theory.
I'm going to talk through a sort of practical way to actually do this.
So the way that we do this at Microsoft is we have designers.
The first thing you'll do when you start onboarding with UR is we won't run any studies.
Instead what we'll do is we'll sit down and talk about your design intent.
We talk about it in a very sort of systematic way.
We want to build out a theory that we can then test in a falsifiable manner.
And we do this in a couple of ways.
We try to build out game-specific experience goals.
What are the promises you as a game designer are trying to make to your player?
Second, we break this down into subcomponents, because generally speaking, the people that we work with are very ambitious.
We need to make them into sort of more consumable units.
The next thing we do is we make sure that all of these units are measurable, and by that I mean falsifiable.
And then finally, the sort of last and most important thing that we do is we write these in a way that's feature independent.
Because here's the thing.
Games start off, they're scoped in a certain way, and then some features work and some features don't, all right?
And it's okay.
for a feature to disappear as long as the experience ends up being there in the end.
So I'm going to give you just a real example of this.
I've neutered it a little bit to sort of hide privacy and things like that.
But here is a sort of basic way that we break down sort of open world.
Frictionless exploration.
That's the goal.
And what we mean by that is I'm motivated to explore and do so without limits or friction.
I then break this down into sort of more useful components.
All right, need pushes me out.
My base is not self-sufficient, but needs me to go out and explore.
Curiosity pulls me further.
I may head out with an initial goal, but the environment I see and the people I meet tempt me, making me want to explore.
Exploring is frictionless.
Exploring is rewarding.
And as I read through these, these may seem very generic.
But they're not, and they're not in a very specific way.
So think about the word curiosity.
This probably aptly describes a game like Fallout 4, where you start off, and you're walking out, and there's an awesome silhouette, and you wonder what the hell that is, and you move in that direction, and you're just, what's over there?
I should go look over there.
But I'm not sure that it accurately describes a game like Don't Starve, where the reason to explore might just be fear, right?
So this is specific to the games.
It's just not because of the features that they're building in, it's because of the experience they're trying to give a player.
So if you can build out a theory like this, I can then design research for you that's specific to test this particular theory.
You as the designer have defined what success should look like, and I'm going to measure against that.
And I'm going to walk through here a number of the sort of design pillars that establish good UR practices, and these are things that independent of methods should be true.
So the first thing I want to talk through is power.
So power is basically the relative strength of a signal.
It's the loudness of the thing that I'm trying to measure.
So I'm going to give you an example.
This is a video.
There are actually five people here.
The fifth one's in the missing corner until they'll show up shortly.
And what I want you to do is I want you to watch these videos.
I want you to look at the eye tracking.
That's the little red dot.
And I want you to tell me, is there a pattern or isn't there?
Can I do this?
So when I look at this, I think those people are behaving in a very, very similar way.
And this tells me there's a lot of power to the signal in this data.
And this is an important concept because oftentimes the first question that somebody asks after I run a study is, how big is your N?
And what they mean by this is, how many people did you run?
And this is oftentimes viewed as a way to judge the sort of goodness of a study.
And the answer people are looking for is, oh, it was terribly big.
I ran lots of people, right?
And non-researchers hear this, and they're just like, this is awesome, this is a very expensive study, I'm super excited about this, they're like, lots of people.
Here's the problem, I will ask this question, but here's how I'm asking this.
When I ask this question, what I'm really saying is, are you cheating?
Do you have a real thing there?
Or did you just run enough people that you could...
Overcome a very small power thing and discover it.
Because one of the things about things that are not very loud, things that don't have a lot of power, is that you can test them if you just test a lot of people and they show up and they are fancy things like statistically reliable, right?
And that's not actually a good reason to care about something.
You probably care about something if it actually is really truly meaningful.
And what's interesting about this is that the way that you calculate an appropriate end does vary based on the method that you use, but this is actually just a bunch of math.
And any of you are is going to know how to do this math, right?
Sometimes you can use a small n.
Sometimes if you're asking about emotional questions like, is the game fun or something like that, you are going to want to increase it.
But overall, when I say increase the n, I'm talking about maybe running 30, 35 people at once in a given condition.
And if you have somebody who's presenting you data, and they're like, this is statistically reliable, don't worry.
I've run 2,000 people.
The question you need to ask is not, how big is your n?
But what's your effect size?
This is a very basic math thing.
There's a standardized way to interpret this.
You can ask that instead of what's your N.
You can Google what the correct answer is, but it should be big if you want to care about it.
What's great about this is that power is one thing, but what we really balance this against is an assessment of risk.
And there are a number of ways to judge risk.
I think the most common way that people judge risk is in terms of just popularity, right?
How many people does this affect?
As an example of this, if you're ever captured by Hannibal Lecter and he gives you the option, do you want to break your leg, but it'll heal, or I'm going to mess up your ankle, but you're going to limp forever.
The option you want to pick is actually to have him break your leg.
And the reason is because this is a sharp, painful thing, but our stress systems are very capable of dealing with this.
Conversely, if you have a very small thing that nags at you for the rest of your life, this is awful.
It makes people terribly depressed.
It's really bad.
An example of this in gameplay is re-traversal.
If you mess up your checkpoints, you make people rerun that like half a second of like information that they didn't want to see, that cut scene that they didn't want to watch four or five times, this becomes stunningly grating, right?
And it's because of just the pure repetition that generates risk.
The problem is there's another way to generate risk.
So one of the things that we'll do oftentimes is we'll present people with graphs that look like this.
And a common reaction from designers is this is awesome.
Most people love my game.
Those people at the bottom, I plan to ignore them.
They are outliers.
From a math perspective, you might have a point, though there's actually, again, a mathy way to actually calculate that correctly, and they may not actually be outliers.
But from an experience perspective, I will never throw those people out.
And the reason I won't throw those people out is because those are the people I am most worried about.
Those are the people I need to pay the most attention to because something has gone terribly, terribly wrong for them.
And I need to know what that is so I can make sure that never happens again.
Let me give you an example.
So this is an example from a game called Haunt.
It was a Kinect game, so the way that it worked is you would fight various ghosties in a house, and they would give you instructions on how to battle them.
So in this case, you were supposed to cover your eyes, so you'd put your hands over your eyes, and the Kinect would see this, and you would essentially be shielding against an attack.
But here's another prompt that they used.
So there's cover your ears, and then there's this one.
And what is interesting about this is this is in some ways not a failure in any way, shape, or form.
This player, in fact, beats the ghost.
He is a winner.
Because Kinect can't tell the difference between this gesture and this gesture when you do them repetitively.
But I would argue that you need to see very few people make this type of error.
Before thinking, I should really fix that.
I should not use the word thrust here.
I should maybe use push or shove or something that implies that I'm using my hands, right?
This is another kind of risk.
And one of the things that we will do just in all of our reports is we will go through and we will give a severity assessment based on this kind of risk.
So every time we will write up a report, some things are very important, some things are less important, and we'll just communicate them to people.
So another pillar that we want to talk about here is control.
And control is the idea that if you subtract information out of a situation, you can in fact reveal.
So I'm gonna give you an example.
I'm gonna show you two videos from a usability session.
We are playing by standard usability rules here, which means that none of the participants know anything about the game.
And I want you to just spot the differences.
So that's the first one, and here is the second.
What?
What are you doing?
You're supposed to be in the hot lava.
What is that?
So this second reaction turns out to be, I would say, the closest I've ever seen to universal in a sort of like you have an infinite number of things you could do and what do you do?
You get under the lava.
If you are under five and have not played this game before and have never had the experience with the idea of hot lava.
Everybody just climbs underneath it. There's in fact an achievement.
If you do this, which presumably only children receive, because it is so common, right?
And what we've done here is we've controlled out for the most common thing that I have to control out for, which is knowledge.
In this case, it wasn't actually the designer's knowledge. It was the parents' knowledge.
He looked at the game and he said, I know what this is. I know how to play. And they played that way.
And they had a wonderful time. It was delightful and fun. But this also was pretty amazing for people, right?
And controlling for knowledge in the lab is important, but it's also actually important as you sort of expand outwards.
And the other big place that you need to consider control is actually in your org chart.
So this is my org chart.
In it, game designers here represent, go up through Phil Spencer.
There is a layer cake of useless people in between.
And on the other side, there are user researchers.
And I report all the way up through somebody else.
This means that for a game designer to come into conflict with me and in some way limit what I'm able to say in my analysis.
They have to go over Phil's head, and I'll be honest, nobody ever does that.
Conversely, this means that design is always also free to bet against me, right?
Because the thing is, what makes a good designer and what makes a good user researcher are very different things.
And what you want is somebody who knows something about research judging me, and somebody who knows something about design judging a designer.
So the main thing that we control out of things is noise.
And I'm going to give you an example from Toy Soldiers Cold War.
So in this game, this is a screenshot from the released version, but I want you to notice the little bar at the bottom that says barrage.
This is something where players would go through, they'd get in a gun, they'd start killing things and if they killed enough things quickly enough, they would unlock basically this awesome reward which is a little Rambo and you could run around and shoot things, it was amazing.
The problem is that when they first built this, that bar in the corner, it wasn't blue, it was red.
And this meant everybody thought, oh, my gun's overheating.
So instead of going and actively pursuing this barrage, they would get close to it and they'd be like, whoa, I gotta stop.
And everybody started actively avoiding it.
And what was going on here is that players just didn't understand the mechanic.
Now imagine for a moment if I put this game where people actually understand what's going on and the previous game into a play test and I asked people to say which one was fun.
Odds are this one is going to be more fun, the released version, simply because people actually understand how to use it.
Comprehension was a blocker for me.
It added a bunch of noise.
I couldn't ask about fun until I could be certain that people, I knew that people understood the mechanics.
We talk about this a lot at Microsoft.
And we basically think there's sort of a consistent flow in terms of how you need to test things.
First off, you need your core mechanics.
These are like what you're doing in the game, walking around, shooting things, picking stuff up, that kind of thing.
But the next thing that we actually need are learning systems.
And the reason we need learning systems is because we need the game to successfully communicate to the player how they need to interact with the game.
And these don't have to be overt tutorials.
I've seen a number of lovely talks here this week already about sort of subtle ways to do this kind of teaching.
But people do need to understand your game.
And they need to understand your game before I'm willing to actually ask them what the experience is like.
Is it fun?
Because if I don't ensure that they know how to understand it, they're going to judge you very, very harshly in an unflattering way and for really unfair reasons.
So with all of these principles, I can go through and I can build a great.
sort of design for research.
And that I mean a process to run in a lab.
But the next stage is actually what I do once I get the data back.
And there's another question of just how do we ensure a sort of just interpretation of data.
And I think that this is a really important one for us to talk about because I'll be honest, I think most of the time game designers look at you and think you're a big, flamey sword wielder and don't really understand how we maintain the sort of fairness of our interpretation.
And again, there are some basic principles.
The first one is a priori.
So this is Latin for what comes before.
And it's a basic rule that before I run any study, I should know how I'm going to interpret the data.
So an example of this is at Microsoft, we will go through and we will do things like fun scores.
We have a whole bunch of scores that we do.
And oftentimes the first question I get back from a designer is, well, I mean, I got a 4.1.
What the hell does that mean?
And that's a fair question, right?
Is it good?
Is it bad?
I don't know.
And the reason I don't know is because it actually sort of whether or not 4.1 is good depends on two things.
One, it depends on the quality of the thing that you're actually testing.
So it might be, it's going to be a reflection of that.
But the other thing it's going to be a reflection of is the willingness of participants to actually pay you compliments.
And this, it turns out, varies based on the type of thing that you're asking them about.
So in this example, I've got three things graphed up here.
We have collected for years and years and years in the exact same way a whole series of questions, and we test hundreds of games with them.
And this is our distribution for them.
And so this tells us sort of the average score that every game gets.
And what's interesting about these is that the distributions differ.
So if you look at them, HUD doesn't score very well ever.
Okay?
And this is because typically a good HUD is an invisible HUD.
And if you notice the HUD, so you could pay it a compliment, that's a little unusual, right?
Most of the time when a HUD's working well, nobody has anything negative to say about it.
Conversely, graphics.
People will lavish praise on graphics.
Like, people know how to talk about it, they know how to judge it, they feel really good about it, it is beautiful.
You get five out of five, right?
They're willing to do that, and fun falls somewhere in the middle.
So whether or not a 4.1 is good, it depends.
For HUD, it's probably friggin' amazing.
For graphics, no, that's embarrassing.
And what's interesting about this is that I know all of this before I run my first study.
If you sign with us and I go and I say, hey, I'm going to be a user researcher, I can tell you three years in advance what I want to see before you actually ship.
I'll be like, these are the scores I want to see.
This is how you should be doing before I'm going to feel comfortable.
So the next thing the data needs to be is it needs to be actionable.
And what I mean by this is you need to actually be able to use it in some way.
If I go through and I tell you you're a terrible designer but I don't actually tell you how to help that or fix that in any way, that's really, really problematic.
And this actually means that I do testing not necessarily on games but on my own methods.
And I'm going to give you an example of this.
So I'm going to show you.
I'm going to show you.
Two trailers with some eye tracking.
One of them was a prototype, and one of them was the release version.
And the question that I had when watching these was, could I actually test the prototype?
And would I actually get data that was representative of what was going to happen in the release?
In other words, could I use the prototype to predict the trajectory of this trailer?
And so I could test it early enough so somebody could actually fix things and get ahead of it.
What's convenient about this clip is that because they edited things out of the prototype, the prototype and the release are...
out of sync, so you'll be able to look at one and then look at the other.
So...
Oop!
That didn't work.
Where is my mouse?
Oh, hey!
Welcome to Spectacular Sunset City!
Okay, here's the situation.
A contaminated energy drink...
So if you're watching this clip, one of the things you should notice is there's a really close relationship between the prototype and the release.
And what was fascinating about this is that there would be a close relationship even when the monsters weren't yet in the early prototype.
Because of the way that they were moving the camera and the lighting and the positioning and the sort of scene, they could get an entire cluster of people to all focus in the right spot before they put the enemy in.
And what this tells me when I see this kind of pattern is that if I see problems in the prototype they're probably going to show up in the release version.
And this is just a test that I do for me and I have done this on every single method I've ever run.
So if you have questions about whether or not a method is actually predictive, any UR should be able to give you a reason for why they think it is.
The other piece of this is that when you talk about things being actionable, it's not just a mix of the right question to ask, it's actually also when to ask.
So one of the things we did in Halo 2 is we actually had a little survey that popped up during the game.
And we knew what we wanted to ask, this was the question that we asked people, but we didn't know how frequently to ask it.
Because this actually is a problem for design, right?
This depends on the level design.
If we ask it every hour, maybe that's useful for some person.
But in our case, it turns out that the cadence that we need to ask this was every three minutes.
And this was because that was the cadence that the game moved on.
And again, this is an example of if you for some reason are not in contact with your UR, you probably need to be, because this is the kind of information we really need from designers in order to design the right kind of study to make sure you can actually use the data.
The next thing that we need from data is that we need it to be consumable.
I'm going to give you an example out of science.
This was the first visualization that was made using scientific data.
It was a gentleman named Von Humboldt.
He goes to South America.
He does a lot of measurements on mountains.
What's neat about this is he drew them out and he saw that there were all these layers in the mountains.
He went back to Germany and he handed this out and people looked at it and they looked at the mountains outside their window and thought, hey, I think there's a pattern here.
And this is actually what visualization should provide to you.
They should provide that kind of high level pattern.
So here's an example from game design.
This is how we visualize all of our difficulty questions.
So you have four different missions.
We ask after each of these missions, how difficult was the mission?
And you get four different responses.
It should be, I hope, relatively clear if you study the slides for a couple of seconds, that the one that you want is the one with the tallest green bar, right?
Cuz this is the one that has the right balance for the most people.
And conversely, if you compare the bottom two visualizations, if you get that feedback on two different missions, you should respond in very, very different ways.
Because one of them is broken because it's too easy, and one of them is broken because it's too hard, right?
And the way that you handle that is just fundamentally different.
Finally, the last thing that data should be when you're presented with a report is diagnostic.
So I talked a lot about falsifiability to begin with, but there's actually another way that science sort of generates hypothesis, and this is known as inductive reasoning.
And before Popper, this is sort of what a lot of scientists did.
Essentially, they went out in the world and they observed things and they looked for patterns and they generated hypotheses.
And so when Newton went and gave his sort of theory of light, one of his methods that he had to actually demonstrate so that other people could replicate it was how to build a prism so that you could actually go and break apart light correctly so other people could see it, right?
Here's an example from game. This is a terrible graph. I made it.
The reason it's terrible is that it was actually based on telemetry.
So it was a bunch of currency that we were earning in a game.
And what I found from it was that there were these huge spikes.
Like people were starting off with a certain amount of currency and they were getting these four times increases from certain encounters.
And when I presented this to the game designers, their response was, that's impossible.
That your telemetry is bugged.
I don't believe you.
That is not okay.
And it turns out the telemetry wasn't bugged.
This actually was an error.
But we just couldn't tell why this error was happening from this graph.
Instead we needed to actually go through and visualize the behavior in a different way.
So this is Rise, and the way that you earned currency.
was typically through building up your combo and as your combo got higher and higher, you got an exponentially larger reward.
And when we went and we looked at sort of the low breaks in terms of combos and the high breaks, there was something really distinct about the high breaks, which is that they all tended to occur during these instances where you weren't strictly speaking in combat, you were instead in these kind of moments where you would be put in a ballista.
And you could kill a bajillion things all at once, which meant it was very easy for people to get combos of 300 rather than 12, right? And this actually was what was throwing things off, and why the designers needed to sort of fix that.
The other fallback that we have here is actually to just look at behavior.
This is one of the most common ways that we will use telemetry, which is that we'll build a map, we'll mark out things that we think are interesting, and then we'll just link it to videos.
So if you are the guy who's in charge of cars and you just want to watch 500 videos of cars, great.
I will make it easy for you to do that so you can figure out what's working and what's not.
So I think a fair question to ask at this point is...
Wasn't this whole talk about methods?
And to a degree it was, but a very, very sort of high level.
And the reason I'm keeping it at this high level is because if you want to know what a good method is, this is really the test.
You need to make sure that it follows this complete cycle, right?
You should be able to generate a design theory.
You should do some falsification testing.
You should reject whatever doesn't actually work.
You should diagnose what's wrong.
And then you should be able to generate new design theories so you can go off and make new, better versions of it.
So let me give you an example of how to evaluate two methods.
Say you have the following question.
Where are players getting lost?
You don't want anybody to get lost.
You want them to walk forward, shoot the right things, and never get lost in your level.
Fine.
There are two possible methods here.
One, telemetry.
Everybody loves telemetry.
It's the greatest, the newest, the wunderkind at the moment.
I'm gonna compare that against a really old school method called asking people.
Did you get lost? Yes? Where? So this, I did this on the same level, so these are examples from Quantum. That, by the way, is a great example of what getting lost looks like for telemetry. What the hell is that blue dot? These are all heartbeats of players. What the hell is that blue dot out there? It's somebody who has escaped the level. They've jumped a fence, run out of it, and they just keep going until at some point they fall off a cliff and die.
Figuring out where they got lost from this data, however, is very difficult.
And in comparison, I'm going to give you the other side of the slide, which is that actually when we asked people, 95% of people said that they got lost, which I don't think is clear from the telemetry.
And we asked people where they got lost, they gave us these beautiful explanations, right?
And you looked at it and you thought, oh, I know what I did wrong.
I've got to fix this one, right?
It was much, much clearer to figure out where you needed to intervene.
But the other important thing about this circle is that I own the boxes in gray, right?
As a user researcher, I do falsification testing and I help diagnose problems.
But design owns the horizontal, okay?
And if design doesn't keep up their half of it, this whole system fails.
And to show you how that works, I'm going to go back to Richardson.
Remember, I think he is a very, very, very good designer.
I do feel the need to say that at the moment.
But as it turns out, the epistolary style is not the reason his books were very, very popular.
Instead, what he had done is written a couple of books where basically there's a young woman and then there's this darkly romantic hero who does all sorts of really fundamentally inappropriate things.
He kidnaps her, he like sorts her letters, he does all sorts of weird things.
And this is considered very sexy and romantic.
It wasn't that he had figured out the next great technological breakthrough.
It's that he had essentially written the twilight of his day and age.
And it turns out, he figured this out at some point.
He was like, this is not working the way I thought it was going on.
And what was interesting is he had the option.
Was he going to reject his design hypothesis, which in his case was that people think about fiction just exactly the same way they think about non-fiction.
To do that, he would have to say, hey, maybe people hold fictional characters to a different standard.
Or he wouldn't reject his hypothesis, and he would have to believe that people think that the behaviors described in books like this are appropriate and okay to do in the real world.
And that's the interpretation he went with.
And he did try to solve this.
His solution was to issue another copy of these books with footnotes in explaining the moral degeneracy of his sort of romantic hero.
I have no idea if this wasâ€”I don't think this was effective.
I do want to compare and contrast this with another designer, a designer who did in fact reject, in my opinion, Richardson's sort of core hypothesis.
Because when she writes, it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife. This is the sort of statement that can only be made as fiction. This is not true, right?
And what's interesting about this is that you can debate the quality of a designer, and you can debate the quality of their art, which of course is a separate thing.
But I'm willing to bet that there are more people in this room who know the name of this author and can tell me where this quote is from, than can tell me the names of both books that Richardson wrote, and I told you that at the beginning of the hour.
It's Jane Austen, Pride and Prejudice.
And it's interesting, when you think about these two designers, it's very easy at our 300 years distance, right, in our vast knowledge of how the novel actually works, to think, I am definitely Austin, right?
I got this.
But here's the problem.
If you were to time lock the beginning of the novel and the beginning of video games, it turns out that we're a lot closer to Richardson than we are to Austin, okay?
And maybe...
You actually are just vastly smarter than this designer who was the inventor of the modern novel.
But when I look at this, I think, well, crap.
I have got to find a way to get a lot smarter than I am a lot faster.
And for me, that solution is to embrace the following idea, that daring ideas are like chessmen moved forward.
They may be beaten.
But they may start a winning game.
Thank you very much.
I was curious, in research usually you can find results that either confirm or refute a hypothesis from the game design team, but I'm curious what happens when you get maybe a highly controversial hypothesis that in fact you find a null result, no relationship or that something doesn't affect what you thought it did.
So typically, it's actually harder to find null results in game design testing than it is in science.
Because typically, the hypothesis is, I made this better.
Did I make this better?
It doesn't necessarily mean that you haven't, in fact, added an improvement if you don't see that kind of jump.
Because it turns out that you oftentimes, there's an idea in psychology known as the just noticeable difference, where you actually need to make a sort of a large jump before people are willing to pay you compliments on it.
And so.
Unless you're actually adding a feature that's generating problems, typically we're like, yeah, that's probably a good idea.
Or at least it's not bad, let's keep moving, right?
Because we're on a production schedule, people, we gotta keep working on this.
So that tends to be how we handle the kind of null hypotheses.
Hi, I'm Bim from Poki.
In one of your slides, I think it's the slide about the benchmarks, you're showing fun as a metric.
I was wondering, how do you measure fun as a metric?
We ask people how fun was this game.
In that particular case, we have used, Microsoft uses a benchmarking process when we're actually establishing these kinds of things where we have asked these, the same set of questions in the exact same way across a multitude of games to sort of control out any of the variants in terms of just how people interpret this.
Obviously, fun is not the only thing that you can use as your high level metric.
It just happens to be a very common one because it is, generally speaking, a good high level KPI, right?
All right, thank you.
Mm-hmm.
Hi, so when on the PM team you're trying to test quickly and not really engaging with UR, how can you build this into like A-B testing that you would show in the game?
Like, for example, if you're building a new Fatui and then you put it in the game and hash it to a specific percent of people who can see it, if the Fatui doesn't work or doesn't test well, then a PM might look at that and say, like, oh, okay, we shouldn't have a Fatui.
But then another question could be just, like, was that the right Fatui to show players?
Is it that they don't want a Fatui or should you just continue to iterate on it?
So I think in that case, my answer is going to be you probably shouldn't have production running your tests because that doesn't sound like a test that actually tested the question that you were interested in, which was, hey, should we have a Fatui, right?
If you're just sort of throwing things out there and hoping data is going to tell you the right answer, you're kind of not doing it right.
There are a few really catastrophic errors that designers can make.
When consuming data and ceding design to players is one of them.
You are always a designer.
It is on you to find the solutions, right?
Players are going to tell you what they want sometimes, but that's not necessarily what they need.
And it is on you to get that.
And that's part of why we really strongly recommend pairing with a researcher so you can actually design things to actually get the data back that you need that can be actionable for you.
Okay, thanks.
Hi, thank you for the talk.
So do you have any thoughts about psychological framing with your questions?
The notion that if someone asks their lover, do you love me versus don't you love me, that you might not get the same answer.
Do you have any thoughts about the framing of the questions?
I don't just have thoughts about the framing of the questions, I have thoughts about how you see people in the lab.
I care about that topic immensely.
There's lots and lots and lots of research showing that basically you can convince people that something is true or get them to say something that they know is untrue if you put them in the wrong sort of social group.
So the classic example of this is there was a group where they basically imagined if everybody in the front row was a participant except for that guy, and the row was actually full, and we went through and we asked everybody, hey, Here are three lines.
Tell me which one is longest.
And every single person in the front row picked the wrong one.
By the time we got to him, he probably picked the wrong one too.
And this is just a demonstrated effect.
And one of the things that we do is actually when we test people in large groups, we put blinders on them.
Essentially, we put them in little cubicles so they can't see people.
And so there's no indication of whether or not you're progressing, for instance, slowly through things, or whether, because you should be going at your own rate and all of that kind of stuff.
So we definitely worry about that an enormous amount.
It's also why one of the most common things that if you are, for some reason, running your own research and you are a designer, the number one recommendation is stop talking to people.
And while you are running things, right?
And I jabber constantly during usabilities, but that's because I've been trained in the right way to sort of ask questions and things like that.
Thank you.
Mm-hm.
I think I'm out of questions.
Oh, you're going to get another one.
Okay, so when you're looking at data that you're getting from an experiment you're running, sometimes you might, or typically you would expect the data to follow a certain story.
So like you're increasing engagement and retention and then thus your average revenue per daily user goes up.
But oftentimes with experiments we see like maybe average revenue went up, but then there might be like a 0.3% reduction in your retention.
Or revenue went up but engagement is down.
How can you kind of filter through your data or then design more tests to get to what the heart of is really happening through this test and what their real reaction is to it?
So it's fine to use those kind of high level metrics to sort of keep track of sort of your progression on something, like I'm moving up or there's a red flag here.
But in the same way that like the graph that I had around currency tracking, if you're too high level, it's just not actionable, right?
You have to get more context there.
And frequently do it just through qualitative questions.
You made a choice.
Why did you make that choice?
Okay, great.
Explain it to me and then I'll go through and understand it that way.
I guess if we have data on a quantitative level, it can be representative, but then how do you know that the user you would be talking to on a qualitative level would be representative?
of all your users?
Well, because you sample correctly, is I guess the answer.
It's the sort of thing where you have to, sometimes people think that the best way to run studies is to basically run them like the census.
That's not true.
You don't need to get every single person in your database to answer a question to know whether it's representative.
Instead, you correctly sample and find the right people.
You're talking a lot about formative testing.
Do you have a process for looking at analytics post-launch?
And what is that process like?
We pull the data and we look at it.
Typically, what we do is, so you can't just hook everything in telemetry.
You have to actually go through this entire process to begin with.
If you want to have effective telemetry, you need to start off with a design question that you're trying to answer.
You need to figure out how you're going to answer that with data.
You typically need to prototype this a couple of times to make sure that it actually works.
Then you need to get it hooked.
You need to then check your pipelines to make sure that they're hooked correctly.
And then things will come back and be informative.
I'll be honest, I am mostly in the business of trying to prevent problems before they are released.
But we do definitely do ongoing engagement stuff, and we certainly use telemetry.
I'm more likely to use it in the lab than I am just post-release, though.
Thank you.
Mm-hmm.
All right, this time for reals.
Thank you very much everybody.
