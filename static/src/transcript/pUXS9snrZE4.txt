Hello, everyone.
Like Dave said, my name is Eric Martel.
I'm a technical lead AI programmer at the Ubisoft Quebec City Studio.
So we're slightly different than Montreal, but it's a quite nice studio.
So come see me if you want to learn more about it.
So I programmed a little something.
As soon as the slides are done updating, there should be a little camera in the right side corner so that you guys don't take a picture of a half animated slide.
It's a gimmick, but.
I've seen so many people take pictures of slides that only have the content that maybe it'll help you guys.
So first, thank you to being here.
It's really appreciated to have people coming and seeing the kind of work we do.
And also huge thanks to Ubisoft Quebec, because they're really supportive, and I wouldn't be here if it wasn't for them.
So senses, most of you know that most human beings have five senses, vision, hearing, smell, touch, and taste.
In video games, sometimes we have what we call a sixth sense and that's not about seeing dead people.
It's usually feeling stuff that...
that is not in direct perception of the NPC.
So for example, standing next to an NPC in this little bubble could trigger the sixth sense for the NPC to maybe do a little look out over the shoulder and see if the player is hiding there.
I'll be making the differentiation between perception and detection.
To me, perception is really mechanical.
It's on or off, you're either perceiving or not.
It's the ability to see, hear, or become aware of something through the senses.
whereas detection is more about identifying what I'm perceiving.
A lot of gameplay code works in the detection space, but in this talk, we'll mostly talk about perception.
Since it's a quite short talk, I'll be mostly talking about vision.
I'll be explaining how to build geometric sensors, how to do point-to-point perception, and how to provide those sensors behavior.
Basically, those are the building blocks to create a simple to somewhat complex sensory system for DNPCs.
For the people who are interested in seeing other senses, in case of hearing, most systems will simply use a spherical perception, so just a radius check.
If the sound is emitted within a certain radius, I hear it.
If you guys are dealing with interiors, so basically complex buildings, you might need to implement sound propagation.
That's something we did on Assassin's Creed Syndicate by doing 3D pathfinding using volumes and portals, which were already used by the audio designers to manage the sound that was being heard by the player.
So reuse assets if you can, but 3D pathfinding is the way to go for that.
For a sense of touch, it's really rare that the NPCs will need to receive feedback on what they're touching.
on their own.
Like the decision they made to go interact with something, they don't really need feedback to it.
And whenever something is touching them, in most of the cases, you can simply forward the event that you received directly from the physics engine.
It can be quite simple, just a bump event or a hit.
And from that, the NPC can do some deliberative decision making.
For smell, very little games use the sense of smell.
But we played a bit with it in a very old Far Cry title, where the player was emitting some sort of breadcrumbs representing his smell.
So it started with a really intense little breadcrumb.
And over time, it would expand and lose intensity.
So from there, you could follow the breadcrumbs and find the player.
For the sense of taste, unless Cooking Mama requires an AI to test your recipe, I don't see a real use for the taste.
But if you guys have any ideas, just go for it.
I will also not be talking about serious stuff.
Right now, the sensors are quite trendy.
If you're talking about real world sensors, like smart homes or autonomous vehicles, they tend to use computer vision, deep learning, all that kind of stuff.
I wish we could use that in video games, but in most of the cases, we just don't have the CPU to do it.
And the computer vision would actually solve quite a few issues that we'll be discussing later on in the talk.
So hopefully by the end of the talk, you'll have a better understanding on how to create sensors, how to give them proper behavior, how to be designer-friendly, how to be animator-friendly, tips to make it faster and more flexible.
In my experience, sensor systems is really a team effort.
It's not just the programmers working on their own.
You need to have good communication with the game designers and with the animators.
And this way, you can achieve a better game.
Before we dive in, everything that I'll be showing you has been tried, tested, and shipped in some game.
So it's stuff that works.
And hopefully, you can base your own implementations off of this.
I'll be showing you a quick demo.
It's built with Play Canvas.
So it's an HTML5 WebGL rendering framework with assets from Mixamo and CGTrader.
So you'll see it's quite simple.
My goal was to show that.
Within a few hours, you can build something that's simple, that's easy to understand, that shows off what the sensor system can be.
So that's basically the target we're going for.
We'll be using agents.
They could be simple or animated.
In our case, they'll be animated because most games use humanoids characters.
We'll be using point of interest, so it's basically what we're trying to see in the world.
In this case, I'm not really original.
I use the target.
And finally, for the environment, it's whatever's blocking the vision of the NPCs.
So in this case, I used a crate, because most games have crates.
As Xavier was saying yesterday, vision is quite simple.
There's a few steps to follow, and most of the times, it'll be good enough for your game.
So basically, filter which entities you want to try to see.
test to see if they're within the sensor range and or shape.
And then use a ray cast.
That's the expensive part.
That's the part you want to do the least.
You will have multiple target types.
Depending on the complexity of the game, your NPCs might be trying to see other characters or only the player.
It really depends on your game play.
Particular events, so as was described earlier, I think yesterday, sometimes you want to try to see something that happens within one frame, and sometimes it's something that's not even there, so you just leave some piece of information in the level, and if the NPC sees it, he can understand that something happened and react on it.
Some useful data whenever you're dealing with sensors is to convert everything in local space.
It makes the computation really simple and compute the relative angles.
Sometimes you'll want to do it on a top view and sometimes a 3D angle.
So we'll dive in the anatomy of a vision cone, but to me in most games I worked on, it's more like a pie.
So it's a triangular shape with some sort of height.
It can be quite simple.
If you're working on a top view game, maybe you can stick to simply have an angle opening of how wide your sensor is seeing, and then far distance for your sensor describing how far the NPC can see.
Then if you want something more complex, you could add a near distance to control which region of the environment in front of the NPC is being seen by the sensor.
If you're going 3D, you will obviously need to control the height by describing a top and a bottom cutoff for the sensor.
So again, it's something quite simple.
And if you can make it dynamic for your designers to play with and see in real time how the sensor's being affected, I think everybody wins.
Obviously, something simple as putting an offset on the referential for the sensor.
and then attaching the sensor to a specific bone.
You can either attach it by position or position and orientation.
In this case, I'm showing off the same sensor being attached to the root of the NPC, then to its hips, and then to its head.
You can also control the shape in more complex ways by, for example, pitching the near end, pinching the near end of the sensor to be able to drag it up or down so that you can have sensors that go upwards or downwards.
You can also smooth the edges of the sensors using splines.
So either to smooth out the sides of the sensor or to have the close part of the sensor detect less side than the far end.
And if you want to be really fancy, you could use some height maps to draw bitmaps describing what's the height of the sensor at a given x and y coordinate relative to the NPC.
You should try to build like a big library of shapes that your designers can play with.
So a sphere is quite simple and you might need one for your auditory system anyway.
So it's a perception in all directions.
Cylinders, that's quite useful for six senses.
You can have just a small cylinder around the NPC and whenever something's entering that bubble, the NPC gets notified.
And if perception is not.
If you're not building a stealth game or a game where the complexity of the sensors have almost no impact, you can simply use bounding boxes.
So whenever something is within a box, either object-aligned or access-aligned, you perceive it.
Build anything you can think of.
The more tools you're providing your game designers, the better they can build an experience.
For example, a few years ago, Martin Walsh presented the splinter cell sensors.
They had success using coffin-shaped sensors.
Just do it.
As long as you can build a primitive that's easy for you to test whether a position is inside or outside, just go for it.
And experiment.
Spend some time with your game designers and with your animators and try to think of new ways you could provide them with tools and simply do it.
More advanced systems will be using multiple shapes.
So, for example, in this case, you could have some metadata on the closer sensors saying, whenever something enters this sensor, it's highly dangerous.
Whenever something is perceived in the yellow sensor, it's not as dangerous.
So maybe you can change your behavior accordingly.
The attachment of the sensor on an agent, well usually your agent will use a skeletal component.
You can either attach it on the entity root, the hips, or the head like I was discussing before.
But you can have some weird results whenever you're starting to apply animation of the NPCs if you're adjusting the rotation of the sensors based off of it.
So right now, I'm using assets that were not meant to be used as sensor providers.
So in this case, you see as soon as I'm putting a more dynamic animation, the sensor will be going all over the place.
This could be the desired behavior, but in most cases, it just makes it harder for a player to understand when he's getting detected because the sensor keeps moving.
What I prefer is to use some sort of hybrid bone.
In your MaxScene, you can simply parent the hybrid bone to the root so that whenever the root is turning, the bone will be following.
But then to have some MaxScript, for example, allowing the animator to say, for particle or scene, I want the bone to be following the orientation of the head.
So.
The animator is in full control of where the sensor is looking at any time and can allow you to create some handcrafted behavior.
So you're not relying on the visual asset.
You can create gameplay just through your animation.
And it's also really good to delay some abrupt changes.
Sometimes players feel cheated when an NPC does a 180 turn because the detection cone can turn really swiftly.
In this case, the animator could simply put a slight delay in the rotation of the detection bone, making it so that the player gets the impression that he's about to be seen, goes into cover, and then the sensor turns, making it.
giving him the impression that he's better than he actually is.
Adaptable setup.
What I like in some system is to provide my animators with markups.
So the animator themselves can control whenever the sensor should be attached to the head, to the hybrid bone.
Just let it so that data can control your sensors.
Also, in some cases, the sensors won't be able to, well, the animation selection won't allow the NPC to properly track a target.
In some cases, it can lead to behaviors where you have your sensor toggling between, I'm seeing the target, I'm not seeing the target, because the target is moving next to the NPC and he's trying to track it.
In that case, what I prefer is simply putting a small state machine on the sensor and detaching it from its referential.
From that point, the sensor is simply following the target and keeping a line of sight to it.
As soon as there's occlusion, the state machine goes back to following the referential, making it that the NPC is simply following its target and never losing track of it.
Whenever you're targeting something.
Depending on the type of entity, sometimes you can simply use the entity position.
It's as simple as it gets, but unfortunately, it's usually on the floor, making it so that any rock, any crate, anything on the floor can occlude the target, making it less interesting for NPCs.
If you're trying to see some, I don't know, some loot on the floor, maybe the entity position is good enough, but for most characters, it's not.
Then you could use an offset.
I've seen many games using like a meter 60 offset from the root and looking roughly around the neck or head area.
And that's what most games use, and it works.
Unfortunately, it requires that you need to provide with a static offset for every single stance your NPCs might have.
So if they crouch, you need to adjust the offset.
But you also need not to forget ragdolls.
In many games, there's some stealth aspect to it to hide dead bodies so that NPCs don't see them.
If you have an offset sticking up a meter 60, well, it's really hard to conceal the ragdoll.
By using a bone from the biped, again, we're tying our logic to art style.
And I don't like making it so that the animator needs to think of how the animation will look just so that the gameplay works right.
Fortunately, if you're going to do this, at least the changes in stance will work right away.
But I still prefer a dedicated bone, a bit like the sensor attachment on an NPC.
With the dedicated bone, you can easily tweak for any gameplay context.
So for example, if the player goes into cover, maybe you want to move the bone towards the wall, make it so that he's still detectable from behind, but NPCs that are in front will have a harder time because the bone is closer to the wall.
Unfortunately, it makes it harder for QA.
So work on your debug tools.
Make sure that it's easy for them to see where the bone is located.
The more...
The more data you have, the more chances you have that an employee will create a bug.
It's unfortunate, but with that kind of flexibility, sometimes you have to take more precautions.
If your target is the player, oftentimes it's better to use multiple bones.
Multi-sampling kind of works right.
Because the point-to-point perception is really easy to disrupt.
Whenever you have even a twig in between the NPC's sensor position and the target position, the twig could be occluding that position.
And the NPC thinks, well, I don't see my target.
When actually, he sees like 99% of the target, just not that little point he was looking at.
By using multiple bones, it's quite easy to just go to another one if the one you're looking at is occluded.
As an example, I split it up a bit, but you can see an NPC looking at another one, and just the fact that the animation is slightly moving on the NPC makes it so that the head bone and the hips bone sometimes get occluded, sometimes they don't.
So by having multiple bones, the NPC walking can always see the left hand of the NPC and know that he's seeing the target.
Warning.
Whenever you're using bones, you can run into level of detail issues.
If your skeleton stop being updated, you need to be careful not to be using its data anymore.
So in a lot of engines, whenever the NPCs get far away, only the root and maybe the hips get updated.
So if you were trying to get the head position, it's possible that it's not there anymore.
So you need the fallback system to potentially go back to a fixed offset on the root.
Using ranged weapons can also lead to some issues.
There's quite a difference between a line of sight and a line of fire because most NPCs will have their gun at a lower level than their eyes.
Having two different referentials can give different results.
So the NPC is seeing its target, he wants to shoot at it, but there's no line of fire.
Therefore, he's going back to his decision making and be like, well, I would like to shoot at it, but I don't see it.
So.
One of the solutions that I like that I used in the past is to unify the referentials as soon as the weapon comes out.
So basically, the NPC starts seeing from its gun or shooting from its eyes.
Most of the times it's better to just see from the gun.
Occlusion.
What's causing the occlusion?
Well.
In most games, it's static geometry, dynamic geometry, and see-through geometry.
I'll explain in detail what's the difference between all of those.
For static geometry, it's basically anything that's building the static part of your level.
So walls, roofs, ground, anything from rocks to trees, it's all static geometry.
It's detected using a raycast, but the good thing about it is that as soon as the raycast hits one of them, you can stop the raycast.
You know you won't see through it.
So as an example, investing in some debug display, that's some quite simple debug display.
But it's really useful for your QA to be able to identify why the behavior of an NPC is changing based on the occlusion.
So if you can do something real quick, even just lines is good enough, especially when you have multiple NPCs.
Dynamic geometry, kind of.
depend on the context.
You'll be trying to occlude the vision of the NPCs using the characters in the environment, the animals, the moving objects.
For example, in Assassin's Creed Syndicate, we had the carriages block the line of sight.
But the thing is, sometimes the rigid body of the dynamic object is a bit too crude and blocks too much.
So in that case, you need to fall back on the ballistic mesh, which is much more expensive.
It's a much more detailed mesh.
But at least if there's some sort of slight hole between two parts of the objects, the occlusion won't be triggered.
The relevance also depends on the context.
By that, I mean that if I go back to the example of Assassin's Creed Syndicate, the NPCs could see through vehicles as soon as they got into conflict.
Otherwise, it was way too easy for a player to just go run the streets and cut the line of sight and the NPCs would go into a search behavior.
So in our case...
Whenever the NPCs were in a default behavior, they would be occluded by carriages.
But as soon as they would get excited and go into a fight, they would simply ignore all of that.
See-through geometry is quite interesting.
Whenever you're playing with smokes of clouds, tall grass, what makes it so that the occlusion happens is usually by computing.
how far you are within that see-through geometry.
And sometimes you want to sum all of the hits that happen.
So for example, if you have a thick, small cloud, you might be detected if you're just at the edge.
But if you're right in the middle, the depth that the raycast has to go through makes it so that you're not detected.
So by having different thresholds for every type of see-through geometry could be quite useful.
Also, quick tip for that.
If you're using transparent windows, just make it so that NPCs can actually go through some sort of door and reach the other side of the room.
Because there's nothing worse than having an NPC stuck into a room, seeing a target on the other side, and not being able to go.
Environment modifiers.
Sometimes you want to reduce the perception range in certain conditions.
Heavy storms or thick fog could be modifiers that make it so that the NPC doesn't see as far.
But these can be also applicable for auditory systems.
For example, on a previous game I worked on, we had heavy machinery.
So whenever the NPCs was being close to the machinery, its auditory system would be reduced, making it easier for a player to create noise in the environment and not get perceived.
If it's possible, please update the visual language of your NPCs to explain to the player why the NPC is not perceiving as he should.
Sometimes something as simple as an additive animation, make it so that the NPC puts a hand over his eyes, could be good enough.
The modifiers.
Players tend to learn patterns.
So they play a game, they learn the rules of the game, and they recognize the patterns.
If you're breaking the patterns by putting modifiers, please just reduce the perception of the NPCs.
If you're making it more punitive by making them see further, the player won't understand what's going on, and whatever he learned just gets thrown out the window.
So if you reduce the sensors, the player can still be safe within the ranges that he learned.
And he can exploit it a bit more because he realizes that the NPCs are being modified.
Also, make it a stack.
I unfortunately saw it a few times in game engines that modifiers are global on an NPC.
So a classic example, the mission designer makes an NPC blind.
You go next to him.
You throw a flashbang.
The flashbang puts an effect on it, blinds the NPC.
As soon as the effect expires, the flashbang's like, well, you're not blind anymore, and you just broke your setup.
So if you're stacking modifiers, it's much easier.
The flashbang will simply push a modifier, pop it, and whatever the mission designer put in place is still there.
Light and shadows.
That's where it gets complicated.
So if you can avoid it, I would suggest you to do so.
Determining what is lit in an environment, it's usually quite expensive.
You have to go through all the nearby light sources and do a ton of ray casts.
And oftentimes it requires multi-sampling.
As we were discussing before with seeing a target with multiple bones, if you have a big thing that you're trying to see if it's lit or not, you probably should be sampling multiple points on it.
Though that solution simply won't fix the issues of having a dark shape in a lit background.
It's possible that the entity itself is receiving shadows from everywhere, but it's standing right in front of a fireplace, for example, making it quite obvious for anybody looking at it that there's something there.
Maybe I don't see the detail of it, but there's something there.
The Raycast solution won't fix it unless you're...
Adding even more raycasts, at which point your engine programmers will come and hit you in the head.
Seeing light source moves and seeing projected shadows.
Again, it can all be fixed using raycasts, but that's probably not something you want to do.
Reflection is the same problem.
Raycasting could fix your issues, but it's really hard for a player to understand what's going on, especially if the player's not actually looking at the reflective surface.
Some quick tips for optimization.
Vision requests can be deferred.
So if you can make it that your NPC simply put a request in a system.
tweak a bit the priority, and then maybe the distance of the NPC from the camera, its gameplay state, or its interactions with the players.
For example, an NPC that's actively interacting with the player might require the priority to be put up through the roof.
This way you can sort your raycast requests and treat them accordingly.
and cache as much data as possible.
If you have multiple NPCs testing the same position, maybe you can reuse some of that information.
So just to wrap up, go data-driven.
Game designers and animators are your friend.
Just give them tools.
Build a library of components.
That's classic tips that everybody should be using these kind of tips.
But specifically for the sensor systems, it's useful.
And let the designers and animators take ownership.
They're not separate from the AI programmer.
They need to work with us in order to create a great experience.
And experiment and iterate.
You won't nail it the first time.
It really takes a lot of play testing.
iteration to find a good setup.
I'm trying to be fast because Dave wants me to stop.
And fail early and avoid raycasts.
Raycasts are expensive, so try to avoid them at all costs.
Test everything.
Sensors are really binary, and they're at the root of everything.
So if an NPC doesn't perceive you, he cannot act intelligently towards you.
So unit test everything.
And only profile and optimize when needed.
I think that's it.
I'm not sure if we have any time for questions.
Awesome.
Real quick, you said it was perhaps a disadvantage to have your eye point wobbling around the head.
Actually, Red Storm, in fact, found out that that was almost mandatory because the animator had a idle loop for a guard.
There was a player sneaking up on the guard with a knife.
The guard has an AK-47.
And the guard's idle loop includes the occasional glance over their shoulder right at the player who is like, five meters away and the guard is totally not seeing them because the bone at the chest was the perception bone and not the head.
Exactly.
That's why I'm trying to have my teams always use some sort of hybrid approach where the animator can control when the bone is turning.
So the player never feels cheated that the NPC randomly glanced over his shoulder.
But if he does, the player will see the glancing animation, and then the sensor will start turning, making it easier for a player to avoid these kind of situations.
And if you guys have any other questions, I guess I'll be at the wrap-up room, or you can contact me on my work email address.
All right.
Thank you.
