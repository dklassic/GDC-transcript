A lot of people here want to learn about banning players.
That's pretty cool.
I've been asked to remind you, please put all your phones and tablets and anything else on silent, please.
My name's Devin Connors.
I'm the community manager for Psionic Studios.
We make a game called Rocket League, and today I'm here to talk about the language ban system that we put into Rocket League last summer.
And just a few of the things we'll be talking about today is just a brief catch up on what Rocket League is and how we had to catch up to our own growth over the last two and a half years.
The inspiration and the goals for this language ban system that we built, how we put the team together and the process that we used, the execution of that, the results and the reception from our community, and some of the changes and future plans we have in mind.
For those of you who are unfamiliar, Rocket League launched in summer 2015 on PS4 and Steam.
It was a PS Plus game on Steam right near launch.
We followed that up on Xbox One the next year and then Thanksgiving last year on Nintendo Switch and coming later this year we're going to be doing a version for Xbox One X as well.
So far we have over 43 million players worldwide with over 2 billion matches played.
And last but not least, RLCS, our chief eSports league, along with all the other eSports activations that we have.
It's one of the few eSports that's truly family friendly and appropriate for all ages.
So with Rocket League, when the game launched we were expecting about 10,000 concurrent players in the launch window.
That ended up being a little more than that, closer to about 100,000, which is great.
Business is good, but when you grow that quickly, you have to catch up to your own growth in many ways, whether it's content, features, or in my case, with the community team and the customer care team, catching up with the pitfalls of player behavior in online games.
The more players you have, the more matches are being played, and just the more natural opportunity there is for the typical kinds of online verbal text harassment, stuff like that that you see in online games.
And to help catch up with this, we hired for key positions in a few different departments, including the community team, the customer care team.
Is Franco here?
Franco, maybe?
I think he's on his way.
Our customer care manager is popping in at some point.
and also our online services team, which is half of our engineering squad that focuses on our back end, the matchmaking system, getting players into servers, the trade system, stuff like that.
What really inspired us to build this tool was kind of the community.
community feelings around how toxic players are being handled in Rocket League.
We've had a ban tool for a long time, since launch, but it was a manual process for us so we just weren't hitting the quantity of players that we needed to.
And even though we weren't talking about it publicly very much, the players of our game were pretty hip to that within the first few months after the game launched.
And you know, here's a great example on our subreddit talking about how the report system does nothing.
I think this is something that you'll see in a number of online games, big or small, you know, just seeing harassment in matches immediately leads to the conclusion that there's no sort of disciplinary action being taken against these players.
So that really inspired us to flesh out this system and build it in a way that we were hitting as many players as we could, and really driving the narrative that Psyonix and Rocket League are very much community first, and we're doing everything we can to work with the community and make sure that their opinions are heard.
So some of the goals, again, community first, that's something that we take very seriously at the studio.
And with that in mind, we really wanted to serve the community, moderate the community, without completely sterilizing the online play experience.
Rocket League is a very competitive game by nature, similar to fighting games and other sports games that you might see.
And with that competitive element, you're going to see some amount of appropriate Competitive banter, we'll leave it at that for now.
So really, you want to draw a line in the sand of what's appropriate versus what's inappropriate, and that's where this goal came in.
We also wanted to reduce the manual workload on the community and customer care teams.
You know, we were really focused on the worst offenders when it was a manual process, and we wanted to take that same philosophy and apply it to all of our players.
Using this automation, was important to us, but we wanted to make sure that it fit in with our current terms of use and our code of conduct.
And we also wanted to deploy this without dramatically affecting the production schedule.
We try to push out six to eight major updates a year, and being able to build and execute this system without affecting any of those updates was really important to us.
So assembling the team, it was definitely a multi-discipline effort at the studio.
The community team handles player behavior and the code of conduct.
That's myself and Kevin Sweeney.
Our customer care team, led by Franco with Derek Harvey.
Our chief data scientist, Eric Carpenter, who did a lot of the heavy lifting on the algorithm side.
Dan, Thomas, and Matt at Online Services for helping to build out the backend tool that we use to...
you know, execute bans and other game-altering punitive stuff.
And also additional support from Corey, our game director, Bobby, who's our technical producer.
He's kind of the lead producer on Rocket League Prime, as we call it.
And also additional input from Jeremy, our VP of publishing, and Phil, our in-house counsel.
We call him Legal Eagle for some reason.
I'm not sure why.
And the process for this language ban system is pretty simple once all the tools are in place.
During a match, one player uses inappropriate or harassing language. They're reported by one or several other players in the match. The chat log after that report is filed, the entire chat log from that match is saved in our back end. And then our algorithm scans that chat log and looks for words that are on our language ban list.
If it's triggered, we also look at the last seven days of reports against that player to see if the same word is popping up more than once or if other words are popping up as well.
If flagged words do appear, depending on the threshold, and I'll get into that on the next slide, depending on the threshold and if they meet that threshold for these words, they're then escalated into the ban system starting at 24 hours, 72 hours, a week, two weeks, and so on.
Right now we don't have permanent bans as part of the automatic system.
We still want a human eye to look at these reports when it comes to an eventual permanent ban.
And we also give players the opportunity to de-escalate, so after they're banned, if they behave themselves for 30 days afterwards, they then de-escalate so they're not getting dinged as a strike two six months later, if they've behaved themselves for those six months.
And here's where it gets interesting.
We call words grams when we're talking about this system, and the threshold's tied to those grams.
So the language ban list started with about 20 words, including variants for those words, misspellings, using leet-speak, stuff like that, trying to account for any sort of sidestep that players would use to get around this language ban system.
We also don't treat all these words equally.
Racial slurs, gender slurs, homophobic slurs, they all typically have a threshold of one.
So if you use it once, you get reported, you're immediately escalated into this ban system.
Or if you're using a word like shit, we give you a little more leeway because we understand that you might be playing with your friends, you and your friends might talk to each other in a certain way.
So we wanna give you a little bit of leeway that way you're not immediately getting the ban hammer every time you might have a slip of the tongue.
This list is constantly evolving.
We're always looking for new variants of these words, looking at misspellings and stuff like that.
The original list though was completely data driven where we took a chunk of reports, I think it was over six months, looked at these reports, pulled out all of the common words that you would use in gameplay.
simple stuff like and, if, or, or but, or just common Rocket League related phrases and words.
And we're basically left with the worst of the worst.
So for us it was having it data driven like that means that we're not putting too much of a subjective spin on it.
We're looking at words that the community is actively using and policing it from there.
And then we take a look at this list every once in a while to make sure it's still on point.
The execution of this system was also a multi-team effort.
We crowd-sourced the list.
It was community, customer care, and our analytics team.
We used existing UI to inform players for the band reason and the duration.
As we make changes to this system, the UI team and the client teams will definitely be involved.
The algorithm that we use is created by our analytics team and then deployed into the game by online services.
On the internal side, it's BigQuery scanning these reports and how it hooks into the game is C-sharp, just a few simple lines of code.
The online services team, they did a lot of heavy lifting here as well.
They completely built what we call the Rocket League backend tool, which is where we do game bans, coming soon chat bans.
looking at trade history, match history, virtually anything that's tied to player data that we have need access to.
The customer care team is also involved at every step because they're effectively the front line when it comes to talking to players who have been banned, no matter how hostile or mild-tempered they might be after the fact.
So customer care is involved to help inform the decisions we were making on the list and also to help anticipate player reaction.
contact volume.
And then last but not least, the community team did most of the messaging for this.
So blogs, social media posts, being active on the subreddit, on our discord, and making sure that players knew that this new system was coming, how it works, and how it's going to be beneficial for the community in the long run.
And again, that really ties in with this community first, kind of the cornerstone of our internal mission statement.
And the fun part.
The results I think speak for themselves.
Taking a look at kind of a six month range, three months before, three months after we implemented this system, you see a 33% increase in the number of reports filed.
So after our messaging went out about this language ban system being added, a lot of players who assumed that the report system did nothing started actively using the report system to report toxic players.
Within the first 90 days, we had almost 100,000 bans.
80% of them, or 81.6% of them, were 24-hour bans.
And then from there, you see a pretty significant drop from players who were banned once versus twice versus three times.
And last but not least, 98% of players banned in this window came back to play after their ban was up.
So we saw an incredibly low churn rate with this new system being implemented.
And here's the same information in graph form.
Everybody loves grass, I think.
But right in the middle is where you can see a pretty significant jump.
Kind of the waves that you see at the end, those tend to be tied to weekends when most of our players are active.
And the community reception, as I said, was strong and it was also very positive.
Once we started banning players with this automated system, we had players who were banned by the system showing up on the subreddit, not posting in anger, but saying, I'm sorry for what I said.
I was having a bad night.
Also, I'm glad to see this reporting system is working, so you guys can totally believe Psyonix when they say it's working.
And stuff like that really does have a ripple effect among all of your, especially your hardcore players, but any of your players who are on social, on the subreddit that you might control or have moderators on, and whatever Discord or voice service you might be using.
The public change in attitude is very important for us, and it helped laid the foundation for any of the other player behavior tools and methods we want to use in the future.
This is something that we're always evolving, especially with the word in the gram lists.
We definitely want to add languages beyond English.
Our game is predominantly English speaking in the US and Europe, and of course, our studio.
So now we're trying to.
effectively crowdsourced lists of slurs and curse words in other languages, which is always a fun conversation with people on Reddit who are like, oh, I speak Dutch, let me give you the 50 worst words I could possibly think of in my native tongue.
Fun fact, Dutch people really like to tell people to get cancer in online games.
I don't know if that's like a European thing or just Dutch or what, but just Dutch.
Well, you know, health is important, right?
Yeah.
We also adjust thresholds for these words when appropriate.
An example I like to use is the KYS, which typically stands for kill yourself.
It's something you never want to see in a chat log directed at another player.
We have it at a threshold of two, not because we think that KYS isn't serious enough to be a one, but because it's very close to the word keys.
And because we have a crates and keys system in Rocket League, we give it a little bit of buffer in case someone was legitimately making a spelling mistake.
We don't want them to immediately be banned.
With the seven day window that we use, if someone's using that to really say kill yourself, they're going to be saying it more than once in a given week.
So by setting it at two, we give you one mistake and then you get escalated into the ban system as appropriate.
Here are some of our future plans.
The first one coming up, this is going to be part of Rocket League's spring update in a couple of weeks.
Chatbans are not replacing game bans, but they're going to slot in in front of game bans.
So this language ban system that we have will be tied into chatbans first.
So if you're reported, you go through the same process.
instead of being banned for 24 hours from online gameplay, you'll be banned from chat for 24 hours.
And you'll go through that same escalation process.
If you're banned from chat enough times, you'll eventually escalate and convert into game bans to really help drive that message home that you need to course correct how you're talking to other players.
Trade bans should be coming later this year.
Those aren't as chat heavy all the time, but because people do try and game the system when trading with other players, there is a chat window in the trade system that we're going to be looking at as well, potentially.
And it's just another player to player interaction that we want to start policing in some way.
Further down the road, we're also doing research on third-party tools that are out there to see what benefits there would be to converting from our homegrown system into a larger, more robust third-party system.
Also, much further down the road, how machine learning could potentially play a role in player behavior.
This is something that we're treading very lightly with because machine learning is a very powerful tool.
But I think you want to be careful when you're using it in conjunction with how people talk to each other.
And, uh, that's it for the presentation.
So I'm happy to take any questions.
Hi.
Yo.
It's interesting that the bans are always triggered by a player reporting bad conduct.
Have you contemplated triggering a ban automatically based on just always scanning the chat logs?
We definitely talked about it internally.
I think the reason we stick with active reports is we understand that, especially with the party system in Rocket League.
A lot of people are playing with their friends, and if you have a certain way of talking with your friends, and there's someone outside of your friend group who's in the server who might be cool with that, then they're not going to report you.
If they, if it's something that they take offense to, they can file a report.
And that's also where thresholds come in is, as I said earlier in the deck, we're not trying to completely sterilize this competitive gaming experience, but we do want to set up some pretty strict boundaries when it comes to certain things.
So with scanning all of the reports, it's something we've talked about, but the system as is seems to be working pretty effectively.
And really, it really just comes down to playing with your friends and the kind of language that you're comfortable with with your friends.
Thank you.
Great talk.
Thank you.
OK, so kind of a two-part question.
You use the term chat a lot.
Are you talking about chat as in people are typing in text while they're playing the game with the controller, or chat as in people are talking?
Text chat.
So it's purely text chat, is everything that you've reported, is everything that you're doing here?
Yeah, there is voice chat in Rocket League, but it doesn't get used very frequently.
And chat logs are much easier to record and analyze after the fact.
Especially on Steam, you know, we have a really big population on Steam and console players can type pretty quickly.
Usually just asking to trade with other players, but if...
they're having a bad night or whatever the case may be, they'll definitely get their point across for better or worse.
So what about post-hoc?
How do you do bad actors in the community that don't actually get reported but are visible?
For example, people who are on e-sports games and then type bad things while the games are going on and they have 100,000 people watching them.
What's your current strategy on that and how do you deal with that?
That's a great question. I can definitely answer that from, point of view of streams that we control. Basically any, any public commun- any public platform that we have that we have any sort of influence on, that also extends to in game behavior. So with the subreddit that we have and our discord server, our mods treat players the same way using the same code of conduct as we do in game. And one thing that we've, we've done this a little bit in the past but we're going to be ramping it up as we've made changes to our code of conduct is.
behavior outside of the game can affect your ability to play the game online in the future.
It's something that we were wary about originally with how the code of conduct was set up, but now that we're making it a little more clear in that code of conduct, telling players like you're gonna have your streams, you're gonna have your Twitter and Facebook accounts, but if you're constantly harassing other players and you're mentioning Rocket League in these tweets and these posts, there are potentially going to be in-game consequences.
Thank you.
So, thank you for the talk, that was awesome.
I work on RTS games, so the breadth of toxic experiences that players go through beyond verbal abuse, the breadth just expands a lot.
So I was wondering what sort of toxic behaviors your players report or would like Rocket League to be able to react to, such as AFKing, griefing, if anything.
Want to get some insight into whether you see in the future any sort of automated verification for those toxic behaviors and whether they happen frequently enough in order to make it the effort worth it.
Sure, that's a great question, thank you.
So right now we have a few different report reasons in the game.
So during the middle of a match, you need to report a player, you hit start, you go to this report slash mute.
So in the same window that pops up, you can mute any player in the server or you can report that player.
Once you hit the report button, you have a few different categories you can choose from, including verbal text harassment, but you also have unsportsmanlike conduct.
You also have cheating and if crate farming isn't in there, we're gonna be adding it soon.
So that list, we're working on making it static data so we can update it on the fly.
So as trends pop up, we can add new reasons without forcing a client update.
Unsportsmanlike conduct is probably the biggest reason outside of verbal or text harassment because that covers a number of different things including playing for the other team.
So if you're in a ranked match, your team goes down 1-0, we've seen players who just immediately take that as an opportunity like, well, we're going to lose this game so I'm just going to be as toxic as I possibly can and start playing for the other team against my teammate.
So unsportsmanlike conduct is a frequent report.
Those we still have a bit of a manual process for where we do analyze the chat logs to see what kind of context we can get.
We also look at frequency of reports against a player.
If a player is being reported two dozen times in the span of a couple of days for unsportsmanlike conduct and then we can get other clues from the chat logs with players saying, this guy is on-going, can everybody in the server please report him based on how that conversation goes.
Do you have any measure of trust attached to that player based on the reports that are being received or the types of reports?
Not at the moment.
What we really use as a measuring stick for that is how many times the player's been reported and also how many times they've been banned.
We try to be pretty lenient with permanent bans unless it's a very severe case because we always want to give players.
ample opportunity to course correct and then come back to the online community and be just a happy normal member of that community.
As far as feedback goes, that's also something we're going to be doing in the spring update.
So along with chat bands, you're going to be getting two kinds of feedback.
If you're banned, you're going to be getting feedback with chat lines of exactly what you said that triggered the ban.
So there's no ifs, ands, or buts about it.
On the other side, we're also including report feedback, and it'll be vague intentionally, but if you report somebody who's been banned within a couple of days, you'll get a pop-up message saying, like, thanks to your report, we've taken action against another player.
We're not going to show you that player's name or when they were banned because we don't want to gamify it too much, but that kind of just simple feedback will also reinforce that the report system definitely does something and bears fruit down the road.
Awesome.
Thank you so much.
Thanks.
Hey, so when you announced on Reddit the new report system and ban system, were you, like how explicit were you in?
like detailing, like how it works, like you, or did you leave intentionally some fuzziness in there to keep people from kind of gaming, like, oh, well, if I just don't say these, whatever.
We kept it pretty vague.
Yeah.
And I know that's probably a topic that's up for debate among many in this room.
And the reason we kept it vague is for that exact reason of we wanted to avoid players trying to game the system.
We have an appeals process with an appeals email that our customer care team funnels players to.
So if you contact the appeals email, we'll tell you exactly what you said to get banned.
And that's generally what players are looking for, and that's why we're including it with the chat band system.
Because our goal, when we first kicked this off, was to have minimal impact on the production schedule, we wanted to launch a system that didn't require any major UI or client work, which is another factor for that.
But really, it was to keep players from trying to circumvent the system all the time.
And because we're giving them the exact reasoning now, we're kind of balancing it out.
OK, thank you.
Thanks.
Hi, I really enjoyed your talk.
Thank you.
And I found it really useful because I am involved with a project very similar to that in my own company.
My question, I have two questions. First one is I was wondering how you deal with the indirect insults. For example, I want to see your mother tonight, something like that. It is a clear insult, but the word system wouldn't detect such verbal attack.
So that's one of the reasons why we're evaluating third party tools that just have a team with more experience in player behavior and also have a tool set that's already built out that can address the kind of language that we're already addressing, but also take that kind of nuance into account and effectively moderate it, whether it leads to a ban or just muted chat temporarily or just censoring one line like one and done.
So that's a big reason why we're looking at third party tools that just have an expertise in this area.
So those third party tools would be like machine learning based detection systems?
Machine learning based but also more just based on linguistics and having kind of a more fleshed out word list and sentence and phrase list.
So I think machine learning is part of it, but it's still grounded in human eyes, analyzing this data and making sure everything is working effectively.
I see.
When I was talking about machine learning earlier, our goal is to never flip a switch on and then just leave it be.
You know, even with the system that we have here, it requires a lot of human interaction on a day-to-day basis, making sure that players are being banned or policed in the right way.
I see.
My second question is like your game Rocket League is also huge in Asia like for example in South Korea We have a lot of users playing the game, and I was wondering if that system also serves the Korean language, too Not yet, but we're working on it. That's one of our goals is to get as many other languages into the system as possible It's tough when you don't have anybody at the studio who speaks that natively.
So we're doing a lot of crowdsourcing with this where we're talking with community members, we're talking to members of our third party customer care team, 5CA, who they have people all over the world who do customer care for us so they can offer insight into those languages.
The tough part is you can't really open a dictionary and look for this stuff because a lot of it is very slang based.
So you really have to get into the weeds and talk to people who are using this kind of language online inside of a game or out, just with their friends or in state of day life, and kind of build out a list that way.
Thank you very much.
Thanks.
Hey, quick question.
When it comes to the, you say you're gonna, or I don't know if you're doing this yet, it was giving them the exact string that they said or the exact message in email.
Are you concerned that they could reverse engineer and go, that's right, I said that to so and so, and then they're gonna go harass that person?
I think that's something you always have to take into account and we just need to be ready for it.
Because if that happens and we start talking to players and are getting feedback that it's happening frequently, we'll adjust the system as best we can.
And also just go and find the player or players who are being this level of toxic and just remove them from the game and have a zero tolerance policy towards that sort of thing.
It's part of the reason why we don't have the reasons in there now.
It's something that we've had to put a lot of thought into.
But for us now, it's worth the risk because we want players to know exactly what kind of language is inappropriate.
At some point, you have to make that decision and you can't keep it vague forever.
And then one more question, and that is, do you have anything in your system that considers friendships when it comes to a complaint filed against them?
Because I'm in moderation as well, and we'll see it where this one person continually gets banned, banned, banned, and they're like, man, I keep getting banned, and they gripe to their friend about it, and that's the person who is...
filing the complaints and continually doing this.
That's a bad friend.
So do, right?
That's the question.
Do you have any kind of like feedback about that or is there anything in your weight ban system that you consider?
We've gotten a little bit of it, and the appeal system definitely helps with that.
We don't have it built into the automated process.
I mean, we could potentially down the road, depending on, you know, if we made any relevant changes to the party system and that sort of stuff, but the way the game is built right now, there's no...
I think the only way we could do that is looking at if the person who reported the player was in a party with that player.
But you can't rely on that too much because they could be in a party because they were trading with each other or found each other on like Rocket League Garage or Rocket League Central, like a third party fan site where people try and find teammates all the time.
So even if you're in a party together, you might be relative strangers and after a minute or two, you're like, I think this guy is terrible.
I'm just gonna start being as mean as possible to him, whatever the case may be.
But it's definitely food for thought.
Okay, thanks.
Thank you.
Howdy.
Hi.
Yeah, you said when you're looking at reports for unsportsmanlike conduct, where I'm assuming it's difficult to find real data of what's happened, and probably also because of that, difficult for people to appeal.
Are you doing anything to protect minority groups, women, racial minorities, LGBT people, who are often targeted with?
reports of unsportsmanlike conduct when really they just noticed a rainbow flag, like, on their car.
Yeah, that's a great question.
And I think that's why when we're looking at unsportsmanlike conduct reports, we still go to chat and analyze those chat logs.
Cuz it's a pretty rare case that somebody's exhibiting unsportsmanlike conduct behavior, but they're not saying anything in chat, right?
And with those kind of context clues that we get, especially on the other side where if someone sees a rainbow flag and decides to target that player without, no matter how much or how little they know about that player.
they're also likely to say something about it in chat.
So it's definitely not perfect, but with the chat logs, you can draw a pretty good bead on what's leading to this behavior in these kinds of interactions.
But it's definitely on our priority list.
We're an all-inclusive game.
We're rated E for everybody.
We want everybody.
inspiration for this system was to make sure there was zero tolerance, especially when it comes to gender slurs, homophobic slurs, racial slurs, and stuff like that.
We don't want anybody to feel unsafe or targeted while they're playing our game.
Very good, thank you.
Thanks.
So that's probably the last question, right?
Hi.
That's the last.
You mentioned before that the people who are actually getting banned are getting a report of some kind saying why they got banned.
But the people who are actually doing the reporting, they report someone to get banned.
They don't get one, but they will get a vague.
Report that thank you for your report and everything So are you trying to encourage people to report more by doing this and if that's the if that's a yes Have you considered?
applying gamification techniques To encourage people to report more I think we're walking the line.
With the Spring update and the chat ban system that we're going to be putting into Rocket League, the report feedback is going to be part of that.
And we want to give players feedback on when they've successfully reported somebody.
But by gamifying it too much, our concern is it'll lead to too many reports that are irrelevant.
Where you're just, you know, potentially, especially younger players going into a server.
just immediately reporting everybody and it just leads to higher volume which makes it harder to sift through all these reports and address new kinds of language being used that we actually have to police. So with this report feedback that we're keeping relatively vague, we feel like we're walking the appropriate line of giving feedback without incentivizing it to the point of manipulation.
And also, please fill out your surveys after leaving the room.
I've been asked to remind you about that as well.
And yeah, thank you so much for coming, everybody.
