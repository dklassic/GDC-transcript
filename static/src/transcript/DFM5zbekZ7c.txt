Welcome to this first presentation of this rainy day.
I hope everyone had a good night of sleep.
My name is Fran√ßois Paradis.
I am a programmer and a team lead at the Ubisoft Quebec studio.
And I worked on Assassin's Creed Odyssey.
And today I want to present my team's approach to cinematic dialogues.
How we procedurally generated many elements of our scenes and how it impacted our workflows and our production.
Assassin's Creed Odyssey is a single player RPG with a strong narrative focus.
It is set in ancient Greece and you go on an adventure to find your family and become a legendary hero.
And the game lets you choose your own destiny and craft your own story through interactive dialogues.
The choices we make, no matter how small, can put us on a path to greatness or lead us down a road to ruin. The choice to define your own path. To be who you want to be.
Help us! We don't have to die. The choice to be compassionate. Please! Let them go. What? You're no God.
Or to hate.
Peace.
Or war.
To strike from the shadows.
Or attack in the fields of battle.
In this world, there are no wrong paths, no wrong decisions, only who you choose to become.
This is your Odyssey.
So with Assassin's Creed Odyssey, we wanted the player to be able to connect to the story in an even deeper way and make him an active participant of the narrative using branching cinematic dialogues.
Branching dialogues, they do two things for us.
First, obviously, they allow the player to role play and make choices that will affect the story.
But also, maybe even more importantly, they allow his actions in gameplay or in the world to be reflected in the narrative.
So we invested massively into a new cinematic dialogue system to support this.
Another big reason for this is that we wanted to be way more generous with the player in terms of content.
The game features a large amount of quests, and we wanted each and every one of them to be supported by an interesting story and cut scenes.
To give you an idea of the scope of the game, we ended up with about 30 hours of cinematic content in the main game, spread across more than 1,000 scenes and about 25,000 spoken lines.
In comparison, previous installments of Assassin's Creed had between three to six hours of cinematic content.
It's a pretty big step.
So we knew from the start that a traditional cinematic pipeline would not be enough, and we needed to look at new workflows.
And the way we chose to approach this is to build the scenes using modular assets, for example, by reusing animations and cameras.
And this approach also means that we can iterate on the game more easily, and on a game like this, quests are added, removed, changed, all along the production, and the narrative scenes need to be updated.
And those changes are much more costly if you have to mo-cap every scene.
So I won't spend too much time explaining the basics of such a dialogue system, because there's been many presentations about this in the past, such as the excellent Witcher 3 presentation from three years ago, I think.
Instead, I want to focus on a third element that we really wanted to put forward in Assassin's Creed Odyssey, content generation and procedural systems.
As you'll see, content generation allows us to produce more content and to produce it in smarter ways.
So in this presentation, I will first show an overview of the general workflows that the content creators were using to create the dialogues and the various ways in which procedural generation was integrated in those workflows.
I will also examine more closely how we generate every aspect of a scene, from staging and animation to cameras and lighting.
And finally, I'll wrap things up by going over some interesting issues and topics related to our experience.
So this is a finished cinematic dialogue from the game.
And I think it's pretty typical in that there are some generated elements remaining, but also a lot of handcrafted elements on top.
Socrates! You just make friends wherever you go, don't you?
Cassandra, what a pleasant surprise! What brings you to Thylos?
Once I heard the great Socrates was on Delos, I raced over to hear his words of wisdom.
Ah, but what are words? Can they be wise, or are they simply words?
Oh, no, you don't. I'm not getting sucked into one of these debates again.
Yet right now, there's a situation I would love your opinion on.
Fine.
Soldiers captured the rebel. He stole from the Sanctuary to help finance a rebellion.
When guards attempted to apprehend the man, he killed one of them.
Murder on Delos? They're probably going to kill him.
Indeed. Though it's illegal to end his life here, so he awaits his fate to be transported to Mykonos.
Where do I come in?
Would you say this rebel deserves death? That killing him brings justice?
OK, so this kind of dialogue system was really new to us.
It was the first game that we did that.
And the biggest changes were not only technical, but also in a lot of ways in the way the content is produced also.
And the biggest change was probably in the content team structure.
So instead of having a big team of quest designer and a big team of writers who would work each on their side, we created small integrated teams to create the content.
And the narrative part of such a team is composed of a quest designer, a writer, and a cinematic designer.
And together, they are responsible for designing and realizing the content for a region in the game or a specific type of content in the game.
Branching dialogues are a lot more intricate, so these people need to work together a lot more than with linear cinematics.
The design of the dialogue was mostly the responsibility of the writer.
In some companies, this is handled by a narrative designer.
At Ubisoft, this role is not very widespread yet.
It's starting, but we were fortunate enough to have super dedicated writers who were courageous and took on this new role, which required them to be a little bit more technical.
The cinematic designer is also a new role within our teams, and it's the cinematic designer who is responsible of delivering the final result on the screen.
It was important to us that a single cinematic designer can own a scene from start to finish and deliver all its elements.
He kind of becomes the director for his scene.
So, we chose people with very different backgrounds to fulfill that role.
So, we have animators, camera artists, scripters, and even some game designers.
Having the cinematic designer embedded early in that team is super important because it can help the writer and the quest designer make smarter decisions about what is and what's not possible with the system.
And they can also iterate on the cinematic content earlier.
And procedural systems act kind of like a wingman for the cinematic designer.
It helps them create content more efficiently.
And the reasons to use generation are multiple.
The first one is that it makes the content always playable.
So as soon as a Quest designer integrates a scene, it is immediately fully working, functional cameras, animations, without needing any initial involvement by a cinematic designer.
It also means that the scene is updated as the script changes.
And that does happen a lot.
Because of generation, cinematic designers never need to start from scratch.
So even the first time they open a scene, there's already something there and they can start working from there.
And for simple scenes, they can keep the generated result when it works, and the parts that don't work.
That way they can focus on adding value where it really counts.
Generation is also directly available to the cinematic designer in the tool, meaning that they can regenerate some elements of their scene at will.
They can change some presets or parameters to get a different result.
And it becomes another tool in their cinematic toolbox.
And of course, generation is key to produce the amount of content required for a game like Assassin's Creed Odyssey.
Around 20% of our ship content is fully generated.
And a vast majority of scenes in the game still have procedural elements to some degree.
And this is in addition to all the runtime procedural systems that are still active at runtime in the game at all times.
I know people like to see screenshots of the tool, so that's the only time you'll see it.
So what you can see here is that I am playing the same scene over and over, but between every time I play it, I click the magic generate button, and the result is slightly different.
You can also see at the bottom, you're going to see the content of the tracks being changed.
And that's because the output of generation is simply clips in the appropriate tracks in the timeline, just as a user would have done it.
And the goal here is to be able to have and crafted content coexist, live side by side with generated content.
So what the cinematic designer can do is just remove stuff that he doesn't want, or change stuff that was generated.
And then we know what was generated and what was encrafted.
And on the next generations, we will take that into account and not erase any work from the cinematic designer.
So this puts the power in the hands of the cinematic designer.
You can take a step back and just drive the generation, change some parameters.
Or you can dive in and take the complete control over a cinematic, a scene, using all the tools you would expect from a cinematic pipeline.
So clips and tracks and a curve editor and a preview window.
A scenario where we found this pipeline to be really useful is for the creation of post-launch content.
So for Odyssey, in addition to the traditional bigger DLCs, there are content teams delivering free content every two weeks.
So for these teams, the time to player is really short.
They only have a few weeks to design and realize the content.
So this makes for a really good case for using and reusing modular apps in combination with procedural generation.
So given that we have that goal of shipping as much generated content as possible, how do we decide whether a scene can be delivered or shipped only by being generated, or whether it needs more attention from a cinematic designer?
And our initial naive approach was to consider that all scenes would start as fully generated, and then the content team would identify which scenes needed to be upgraded and improved along the production as the need arises.
But this didn't work very well because there are lots of things that the generation just won't handle, like characters needing to move around the scene, or prop handling, or more complex actions.
So writers didn't write with any limitations, so every scene would end up needing some intervention by a cinematic designer.
And that would have led to a big production cost over production.
Instead, what we ended up doing is establishing target levels for scenes with clear expectations for each level.
So we had the level one scenes, which were fully generated and were meant to remain untouched throughout the production.
We had level two scenes.
They had some handcrafted changes, but no custom assets.
Level three scenes had more changes and usually some custom assets made for them.
And level four scenes are completely custom.
And by doing so, we could establish a budget for each type of scene, and it allowed us to control our scope.
It required writers to really understand the capabilities and limitations of each level, though.
So they had to write the scenes with those in mind.
If a writer is tasked with writing a level one scene, because that's what we have the budget for, you couldn't have the characters do everything that they could do in a level two or three scene.
This is where we land in the end, so a lot of level 2 content.
So what do we use as input for generation?
Of course, we use the text and the videos of the scene.
But in addition, we also allow writers to set up additional attributes that are very useful for generation.
They can specify a mood for the speaker with what we call the hashtag emotion.
And they can also set up who the line is directed at.
This is especially useful in scenes with multiple participants.
Finally, every text in VO is analyzed in order to figure out the timing of every word and phonemes of the words and align them to the sound.
That's called phoneme alignment.
And this gives us precious timing information that many procedural elements will use, both offline and at runtime.
So this is a completely untouched, fully generated scene.
You come to steal from me too, mis dios?
Is that how you greet all your customers?
I'm sorry.
The soldiers have taken everything from me.
Even my beloved Spiro.
Soldiers take what they want.
Who is Spiro?
Only the best horse I ever owned.
You could always get another horse.
These things cost drachmae, you know.
Drachmae you don't have, by the sound of it.
I need him back to cart my shipments.
Once Dracon finds out I can't get him his oil, I'm as good as dead.
Dracon? The Viotian champion?
He's my biggest customer.
I'm trying to hunt him down.
Save me from Dracon Thrath.
Find my Spearow, please.
If you need him so badly, I'll find your Spearow.
Be quick, please.
I can only imagine what those thieves are doing to him.
So this is the kind of result that you can expect for any new scene, any new dialogue.
So from the moment the script is first imported, of course at first it's gonna be robot voice probably.
So you can see the value in this, if only as placeholder.
Now that we've seen how the users could use the system, let's see what goes into actually generating a scene.
The first thing we're going to do is place the characters on a stage and get them speaking one after the other so it's very VO-centric.
The VOs are already assembled to form the backbone of the scene.
And the goal of generation is then also to get you camera editing, body and facial animation, some lighting to highlight the characters, and some look-at behaviors to really bring them to life.
And some of these elements are generated as clips, as we saw earlier, and others are computed at run time as the game is running.
So let's start with the stage.
A stage is a real character and camera setup.
So they define the character placement, and they come with a camera bank that cover all possible angles.
There are many stages to choose from.
So we have a 1v1v1, a 2v2, and a 1v4 here.
And even for the same number of actors, we have a few variations, like a two-people stage could be normal, closer, or side-by-side.
And a three-people stage could be more 1v2 or more 1v1v1.
And for cameras, a camera bank for a typical stage will have between 12 and 65 different cameras, depending on the complexity.
And they cover all the different shot types and sizes that we want for the layout.
So typically we'll have standard cameras, side cameras, and over-the-shoulder cameras in our stages with various shot types, shot sizes, sorry, so from close shots to wide shots.
The interesting part is that all these cameras are 100% procedural, meaning that they can adapt to the actors or their actions.
So these are not fixed cameras or keyframe cameras.
They don't have animated positions and orientations.
And you can imagine it as a virtual cameraman that does its best to keep up with the actors and frame them.
And the idea behind this is to try to keep the characters at specific screen space positions and compute at runtime where the camera should be to achieve a specific composition.
This is done by taking into account a few constraints, like the distance from the characters that we want and the angle.
to the scene and then we optimize the rest of the parameters to get those targets as close as possible to where we want them on the screen.
This also allows the characters to not always be, they don't need to remain on their slot on the stage.
They can move around the scene and the camera will still work.
Another advantage of this approach is that it's a lot more robust.
So you can have tall characters or small characters.
You could have characters in slopes higher or lower, and the camera will still remain relatively OK with a good composition.
So that gives a lot of robustness for the scenes.
The scenes will change.
The characters will change along the production.
The animations will change, and the cameras will remain OK.
We also taught the camera how to move with different reflexes for both position and orientation.
And we do that by adding some damping dynamics to the camera.
So depending on the shot size, we will need to be more passive or more aggressive.
And typically on closer shots, we need to be a little more aggressive to remain tighter on the characters.
And so it will take into account the animations.
And most importantly, it will follow the actors when they move.
Again, with different reflexes depending on the shot type.
By default, our camera is modeled after an NL camera.
There's a light NL layer that is also procedural and adjusts itself to the motion of the camera.
But depending on the scene or the needs of a specific shot, the camera can be transformed into a more normal Steadicam or even a more fixed tripod camera.
So now we have a good cameraman, but we also need a good editor that will generate the montage.
And in a dialogue-driven scene, editing is just as important as framing, if not more.
So we're going to start by looking at each VO and figuring out who's speaking and who's listening.
And the first thing we want to do is to generate a basic shot-reverse-shot editing.
So for each VO, we'll use a camera showing the speaker from the perspective of the listener.
Sorry for my mic.
So for each speaker listener scenario, the camera bank has a lot of options to choose from in terms of shot type and shot size.
And the generator will choose one randomly according to some rules that we have established.
Some rules will be about changing the probability of making a certain choice.
Other rules might be about trying to establish a nice progression within the scene.
So we might want to go from wide shots to closer shots during the scene, or the inverse, going from close shots to wider shots.
The amplitude of the gestures will also have an impact on the final decision.
Typically, we need to be wider when the gestures are big.
For a scene with multiple characters, we have additional options to consider.
So for example here, this same wide shot could be chosen when either character is speaking, in addition to their single shots.
And we can even keep this shot for a longer sequence when both characters speak in turn.
And finally, we'll also use a shot like this if one of the characters speak for the first time in a while, so that the viewer understands its place in the scene.
So we have a functional edit, but there's still a few things we need to do to improve it.
There are things that we never want that never work.
So for instance, we never want shots that are just too short.
So we will remove them.
And on the other hand, a shot that is too long can become a bit boring.
So we can add an insert or a reverse shot there.
And the final step is to decide the precise timing of every cut.
And we could choose to be really conservative here.
We could cut at the end of the VO or in the middle of silences.
This is classic, but a bit boring.
So we tried to do a bit better.
So we can cut before the end of the VO.
And that's like a reaction cut.
So it allows us to see the listener a bit before he answers.
And at the opposite, we can cut after the next video starts, and that gives the impression that the cut is triggered or called for by the sound of the new speaker.
So these two give the most organic feeling.
Okay, now let's see how we generate the animation.
We divide animation in two layers.
We have the idle layer and the gesture layer.
The idle layer gives us the base pose or stance for our character.
The animations on this layer usually have some very limited looping movement.
And then we have the gesture layer, which will be made up of a lot of small modular gesture animations that are blended together.
These animations are usually additive, although they can completely override the idle layer if needed, for instance, for entrance and exit animations.
So for every idle state, we build a collection of about 50 to 100 gesture animations that work with that stance.
And the generator will use that library to create an animation track for the character by choosing a few idle states throughout the scene and many gestures to overlay on top.
Choosing the idle state starts from the emotions that we have from the script.
We try to find a dominant emotion for a character in a scene.
Every emotion is associated to a list of idle states that we can choose from.
So if we're looking for a proud character, we could get to choose from these.
And that list is different for male and female characters.
And you can also override that list per character so we can give a more controlled personality to some important recurrent characters.
We have a little more than 100 idle states in total in the game.
So the generator will choose one of those at random, let's say this one, and then we need to generate some gestures.
And in order to do that, we are gonna look at the words in the view.
Remember that we have the timing of every word.
So the goal is to examine those words and find potential animations for them.
We'll use the words me and quickly as an example.
And to help us, we define dictionaries of words related to a specific concept.
For example, we have the me concept that conveniently has the me word in it, but also I, myself, personally.
And we have a now concept that contains the words quickly, but also fast, today.
And when a gesture animation is integrated, the animator will associate it to one or more of these concepts.
So if we're looking for an animation for a word in the me concept, we are going to get to choose from these.
We're almost there.
Now we have a big list of all the words in the VO and some candidate animations for each one of them.
So now we're going to assign a score to each one.
And there's multiple factors involved in the scoring.
For example, we don't want to stretch animations if we can.
So if an animation that fits a hole that we are trying to put an animation in will score higher.
And then another example is the pitch of the word will affect the score.
So a higher pitch will score higher.
And from that big weighted list, we pick a winner at random, and now we can insert the animation in the track.
And we will repeat this process until the track is full or until we reach some density that we are aiming for.
Notice how here we are avoiding having any overlapping animation, which is a safe thing in general.
Because if you're not careful with additive animations, you can get very broken results.
But if you simply disallow any overlap at all, you also don't get a very good flow.
And you can feel every gesture always ending completely before the next one starts.
So our solution to that is to analyze the animations and extract what we call a motion curve.
It's a simple scalar curve that represents the general motion of some of the important bones in the animation.
And it is computed by looking at the relative displacement of the bones relative to their initial position.
So now we have that curve, and if we define some motion threshold above which it is not allowed to overlap, we get an effective range for the animation, which is shorter than its full duration, and it is in that range that we will not allow the overlap, but outside of that range, we can overlap and blend.
Another thing that this motion curve gives us is the extremum of the animation or the point of maximum motion.
And this is useful because this is the point in the animation that we want to align to the word for which it was chosen.
It's not always exactly what the animator wants, so we also allow them to override that moment per animation if they need.
So, the final result looks more like this, a series of slightly overlapping but never breaking animations.
And that's how we generate body animations in our scenes.
Facial animation is a lot more dynamic, and it's built on the fly at runtime.
So here's an example.
All right.
Then do you have the money you owe me?
Do I have the money I owe you?
Of course, of course.
Well, no, not at the moment.
Then get it.
Instantly, my friend, instantly.
But maybe you should do that.
There is a merchant in Sammy.
I'm not very good at these things, as you know.
You want me to collect my own debt.
So there was a lot of work involved in making sure our characters can show some emotions.
We have around 60 different facial states associated with the various emotions coming out of the script.
And each of these states have their own little internal state machine with their own logic.
And we can use the same word concepts like me and now that we use to generate the body gestures and we can use them to trigger some facial responses.
On top of that, we have a few additional layers.
One example is what we call head bobbing, where we tilt the character's head around while he's talking.
Another example is talk anticipation, where we change the face of the character slightly before he speaks to mimic how a person prepares to speak.
The lip sync animation is built using a blend tree and the funnel information that we extract from the view.
The challenge here is that the FUNM information that we get is not 100% reliable.
And so there's a good amount of lines where we needed to fix the lip sync more or less manually.
Also, the FUNM information we had was only available in English.
So for localized languages, we needed to try something a bit different.
And we worked with a group at Ubisoft called LaForge, which is like our internal R&D department, and we used sound matching.
And this is a technology that uses machine learning to generate facial animation strictly from the sound.
And there's a lot of promise in getting reliable results more often.
So here are some results from the game.
My political projects concern you in no way.
Just know that I only seek the recognition of the well-founded of my strategy.
Let yourself not be intimidated by the society here.
Ah, Rikaon. You are a mercenary.
Did you come for treatment?
I told you to stay away from the bad guys.
Ha! Fuck, never!
She asks the faithful about the movement in the ports and then tells me.
That's it.
If you want more information on this, Daniel Holden is having a presentation tomorrow called A New Era of Performance Capture with Machine Learning, in which he talks about sound matching.
Because the body animation is made up of a lot of reusable, small pieces of animation, we have to rely on a look-at system to really bring the characters to life and establish a real connection between them.
And in terms of generation, the look at intentions are drawn only from the text, given two simple rules.
When you're not speaking, you're going to tend to look at who's speaking.
And if you're speaking, you're going to look at the person you're speaking to or the last person who spoke.
And those simple rules will work most of the time.
But there are also some exceptions.
And that's why we also allow the writer or cinematic designer to override who the line is directed at.
To add even more life to characters, we also generate a secondary layer of look at motions that we call glances.
So depending on the mood of the character, we will generate small motions, shifting the gaze of the character for an instant.
And we have a few rules to choose when to trigger those.
A good occasion is using the micro silences in the view, mimicking some hesitation or thought by the speaker.
And another one is using the gesture motion to attract the eyes for a second.
The final element that we'll discuss is the photography, which is essential to get our final look.
And as is the case with most games, the lighting from the in-game world is not enough when the camera gets closer to the characters in a cinematic context.
Without additional lighting, the characters would look very flat.
So what we do is we add a procedural light rig with multiple lights that target all the actors automatically.
And the position and orientation of the lights are re-computed dynamically for every shot, as you can see here.
And this rig will use different rules, intensities, and color temperatures depending on the time of day and whether the scene is played inside or outside.
The exposure values for the camera is also constantly adjusted to make sure that the character faces are always lit properly.
So this system allowed the majority of scenes to be lit appropriately without needing any intervention by a lighting artist.
There were a few more problematic scenes where a lighting artist had to come in and create a custom light rig for that scene.
OK, so we've seen how we change the way we work.
We've seen how each element of a scene can be generated.
And those two were the biggest challenges that we had.
We introduced new roles, asked writers to be more technical.
We iterated constantly on generation, trying to see how much we could generate and how far we could push it.
Now let's look at some additional interesting topics that we worked on.
So Assassin's Creed has a living open world with NPCs that move around, and we always kept in mind to try to allow the scenes to take place anywhere at any time of day.
Now, of course, there's also plenty of scenes where we do know where the characters will be waiting and where the scene will take place exactly.
But for simple scenes, we can afford to give that freedom.
And this is only made possible because of the procedural systems like the screen space camera framing and the lighting rig, for instance.
Another challenge is the crowd around the scene.
You can easily photobomb your scene if you're not careful.
So we have a simple avoidance system that will make them take the long way around.
OK, the next challenge is localization.
So in a traditional cinematic pipeline, the localization costs for recording videos is higher because of the strict timing constraint that must be respected.
across all languages.
So we are using a technique called timeline scaling to be more flexible in that regards.
This is inspired by a technique I first saw in Richard Tree's presentation at GDC 16.
And as a reminder, timeline scaling is about adapting the timeline to slightly different view lengths for every language by stretching and squashing its content.
And it was especially useful for our game because we have two playable characters that you can choose, Alexios or Cassandra.
And as far as recorded audio is concerned, those can be considered as different audio languages.
And so we also benefited greatly from not requiring a precise timing match when recording both actors, even in English.
Stretching content sounds scary, but it works quite well, actually.
Most VOs do have a similar timing, but some VOs could have up to 10, 20, even 30% difference in length across variations.
Here's a scene from both player perspectives where the difference is extremely visible.
The eastern coast, can you be more specific?
I think they've settled in an abandoned house by a small forest south of Sami.
One thing that we always underestimate is the effort needed to track, test, and polish all that content.
And procedural generation is both a blessing and a curse here.
It does give us the ability to create a large amount of content faster than ever before.
But how do we green light this content?
How do we test it?
And also, how do we ensure that all scenes are up to date with regards to generation?
And our initial idea was that the cinematic designer was responsible for delivering a scene.
So he should be the one opening the scene, pressing the magic generate button, and reviewing the result.
And that would give us kind of a QA for our scenes.
But that was a disaster because we never knew if scenes were up to date, when scenes had been last generated.
Was this scene using the latest iteration of look at generation?
We didn't know.
We added an animation to the bank.
Why don't we see it anywhere in the game?
Well, the scenes were not regenerated.
So eventually we took the leap of faith and decided that all scenes would be regenerated every night which in hindsight was the right thing to do.
Yes, it meant that a problem in generation could break the game overnight, but also it meant that an improvement would be made visible in the game very quickly.
So that was a big win overall.
Still having so much content was a challenge and a big strain on QA.
Even if we tried to have dashboards and processes, but there's still a lot of scenes that need to be validated.
I want to come back to this notion of mixing procedural systems with handcrafted content because I think it's the source of a very interesting tension or conflict among the team.
You can imagine that if you tell an artist that a robot will regenerate and possibly change elements of their scene every night, they might not be super happy about that.
Even if you tell them that this robot is super careful.
And there's a similar tension with the runtime systems that work in the game.
Those systems are going to get improved across the project.
Look at dynamics are improved.
We add features for facial animation.
And this is for the better, but it can be a source of frustration for the cinematic designers.
Because when they worked on a scene, they made it work.
It was looking good for them.
And then they come back to it a month later.
And it's not quite as they left it.
Speaking of procedural systems, however smart and magic we try to make them, cinematic designers do need some kind of control over them.
But it's a constant challenge to find the right level of control to offer.
So yes, we need to give control for specific moments in the game where we need to turn off some systems.
But if the procedural system is constantly being taken over or controlled, then we lose once more the ability to improve the content as a whole very quickly.
So in the end, this issue is very important about discussing with the cinematic designers about their needs and making sure they understand all the procedural systems at play.
And sometimes we do need to give them the ability to override the system, but also sometimes we can try to show them that their problem might need to be addressed in the procedural system rather than in their specific scene.
The last thing I want to mention is the artistic direction process is obviously very different when using generation.
Pre-production becomes a very important period to try to get the result that you want out of the machine.
You need to work out the rules at the very broad level.
So it's very different from working on a scene per scene basis with the script, the actors.
And all the content creators, and especially writers, need to build a shared understanding of what the system can and cannot deliver, because there are limitations.
So what should you take away from this presentation?
We saw that this kind of system requires a different approach to content creation.
Then hopefully you have some insight into how we generate the animations, the cameras, and such.
and the benefits and issues of procedural generation.
But I guess the most important thing to remember would be to keep asking ourselves whether we can create our content in smarter ways.
And the most offending thing I still see is people creating data in a very manual or convoluted way because systems were created by a team that basically decided that tools were not worth it, or even worse, that the tools team would eventually do it.
So first of all, if you create a new system, you are responsible of how the users will use it and create content for it.
You know the system, you know the users, so you should be building the tools as well.
But we can do better.
We can build systems on top of systems to generate data or avoid having to create content at all.
And we saw this in the past with level design, procedural level generation.
We saw this with level art and procedural world building.
We generate huge worlds with forests and cities and mountains, and we couldn't do that with a bunch of level artists.
It doesn't mean that we don't need those level artists.
They need to figure out the rules and guide the system.
But I believe this can be applied to cinematic content as well.
We will never replace cinematic artists, but they can be used more efficiently by having them work or concentrate on the important or the hard parts.
Procedural systems will get better.
I think a big next step will be to incorporate machine learning in some of the aspects of generation.
We started using it for lip sync animation, but why not for gesture generation?
I think a cool thing to do would be to be able to choose the best sequence of animations to get a character from point A to point B in a scene.
That would be tremendously helpful for the content creators.
Hopefully some of you are already secretly working on interesting things that have to do with procedural generation.
That would be awesome.
So anyway, even if the scenes that we generate were not all masterpieces, they were still tremendously helpful for the production.
Even in the cases where the cinematic designer ended up redoing everything manually, you still had a working scene to start with.
It was useful to quest designers who could get the quest running earlier.
It was useful to writers who could playtest their scenes in the game and get a feel for it.
It was useful to directors and playtesters who could understand the narrative of a quest more easily and earlier.
So for us, it was immensely useful, and it's definitely something that we're going to continue to push for and improve.
And I hope this will also inspire you to push for automation and generation on your own projects.
And thank you.
I'll take questions.
Hello.
You were saying you were using the word recognition to place the beats of the gestures.
Are you doing that for different languages as well?
No, we do that only for English because it's used to we kind of bait the clips in.
So we generate actual clips.
So we don't have a track for every language.
OK, thank you.
Hello.
Hey.
It looked like there was an amount of randomness in there.
Like you had a lot of rules, and then from the best options you would do a weighted selection.
How did that work with regenerating the scenes every night?
Would they change even if there were no generation changes or no content changes?
So the question is about the randomness in the generation and whether the scenes would change a lot every night.
There is a randomness in the system.
Actually, if you have the same scene and the same inputs, you get quite consistent results.
Like you're going to get small shifts, but not too much, because we put in so much rules to take into account the mood and the words that the end result is not completely deterministic.
But there's not a lot of changes.
But yes, the scenes would be regenerated every night, even if there was no content change.
Thank you.
Hello, thank you for a great talk.
I have a question.
You know, sometimes in games, dialogues, they sound a bit superficial because people, they speak one after another one, while in real life, sometimes you kind of interrupt the person.
Do you have some kind of system to generate this kind of behavior?
No, the generation is pretty safe in that regard.
It will space out the VOs.
according to some rules.
And we wanted to experiment more with that.
We could change the pacing of the scene according to the mood of the characters.
We ended up not doing that for scope reasons.
But no, we never overlap the views in generation, but the users can do that manually if they need to.
OK, thank you.
So it's awesome.
So my question is, I guess there is any chance to add more animation assets?
So once added, so it would be possible to change the.
scene, everything, so because it's procedure generated, so it's very difficult to guess when are the more animation in the asset, so is there any way to check the difference, because it's impossible to check by the tester, everything.
I'm not sure I understood the question.
So, yeah, when...
I guess it's possible to add more animation because it's impossible to prepare all of the animation for all of the dialogue.
So if you add it, everything is generated by procedure.
So it would be possible to change the animation if there is much more good animation for the dialogue.
So is there any way to check the update?
So the question is how do we make sure that we have enough animations basically in the system?
So we started by our animators, we knew the list of word concepts that we were going to have and we made sure when going shooting that we had gestures covering everyone.
And then after the fact, we had reports that would tell us the usage of animations across the game.
So we knew, like, OK, this concept happens more frequently.
So maybe we need more variations of animations for that concept.
But it was on that level that we worked.
Thank you.
Hi, thank you for the great presentation.
I have a question, just a clarification maybe.
I didn't understand how the system work with the body animation.
So the system analyze the voice over here, and after apply the animation over it?
Yes.
And...
But you have exact moment, you know, with the motion and the words.
It looks like it's very natural. How did you do this?
With the voiceover and body animation, it looks like, you know, like motion, full motion capture in terms of body animation and face animation.
I didn't understand this moment.
The question is how we choose animations?
How we apply animation over the voiceover.
Well, we choose many animations and we place them on a track.
We lay them out and we blend them together.
So the system is analyze the voice over, yeah?
And after that, apply the animation.
Yeah, we know the words from the text.
And we know their timings in the view, in the audio.
And that allows us to choose the right animations and generate those clips.
OK, I see.
Thank you.
My question is about the nightly re-export.
Given that you just said that the exports were not deterministic, but close to it, were your users ever able to opt out of the nightly re-export if to mark a scene as final?
Yes, users could at any point lock part of the content, lock by track, or lock the whole scene if they need to.
Thank you.
Hey, I was just curious how long it took to get first iteration of this system working, and how many people you had working on it.
My team was four programmers, two animators, one camera specialist.
It's hard to say because we didn't have any system at start, so we developed the system with the tracks and the cameras and also generation simultaneously.
I would say after a year we had good results and then the last 20% took the next two years.
Thanks.
Hi, so my question touches on the gestures when you're selecting animations for the speaker.
How much of that goes into, or if you have put any emphasis on the listener for the much larger scenes when there's more people as far as words for them to react to?
The question is how do we handle listeners and that's actually a problematic area because we don't have a lot of information about them.
in terms of generation, like if you only have a script, you don't really know what the listeners are doing.
You can infer a mood for them from the previous fields.
So actually, we don't generate any gestures for listeners.
And we do have some procedural facial animation, though.
So when we have a reverse shot or an insert where we see the listener, we tend to focus on the face.
And that's where we can get some results.
But we don't generate any body animation right now for listeners.
So when we do have them, it's because a cinematic designer added them.
Have you guys maybe thought about using the speaker's words themselves so the listener could react to their words?
Yeah.
That's probably how we would do it, yeah.
Thanks.
Thank you.
My question cycles back a bit to the locking question.
How do you make sure that with the regeneration every night, if let's say an artist has modified a single track and that whole system can be locked or unlocked, how do you make sure that the rest is not modified?
And if so, how do you make it modifiable if he wants it to be modified, if you modify the single little element?
I don't know if your question is good.
So nightly generation tended to be super safe, so more on the safe side.
So if we were, like the rules were very strict.
I think that for a sequence, for a block in the graph, if there was any uncrafted change, we would be on the safe side and not regenerate it.
Personally, I would like to be able to go in more finer details, because our generation is still pretty safe.
Like, you could change some animations in a portion of the track, and then you could generate the rest, and it would work.
And we take into account the fact that you changed some stuff at the beginning.
Okay, so basically once changed and locked, it's over for the generation.
Pretty much, right now, yeah.
Thank you very much.
Thanks for the talk.
I was wondering, when you're talking about the, you have different stages of like 1v1, 1v2, and some of them had between 16 and 65 cameras that were possible.
Were those possibilities hand-authored camera positions for each stage setup, or...?
Well, the cameras are procedural, so yes, they are end authored, but it's not as easy as positioning the camera and saying it's okay.
So it's actually a lot of parameters to tweak per camera, set up the angle, the distance, and then the camera frames automatically.
So what you set up is more like an intention for the camera, I want this shot type and shot size, basically.
And then the cinematic designers, they ended up also adding a lot of custom cameras to every scene.
Because the stage cameras are there as a base, but you can add more afterwards.
OK, thank you.
With things like voice cloning, how do you see this sort of system expanding and evolving in the future?
I missed the first part.
With things like voice cloning coming down the pipeline, basically from text to realistic sounding voice from just text, how do you see this sort of system expanding in the future?
I think we're going to need to do more stuff at runtime, possibly.
Like right now, we do a lot of stuff offline.
And the reason we do that is because we want someone to validate the result.
And at some point, we want to be able to lock it and say, OK, this is what we ship.
But if we construct our scenes more dynamically, or we assemble lines dynamically, or we create audio dynamically, then we'll need to embrace that and go with the stuff that we generate at runtime.
So that might be a direction to go.
I would also like to see.
More tools, like I said, to help the cinematic designer achieve specific tasks, like having a character go from point A to point B in a scene.
That's very common, but he has to, like this is, since he doesn't necessarily have mocap, an exact mocap animation for this, he needs to stitch up many animations, like starting to turn, walking a bit.
getting rested in its final position.
So I think we could just let the promise in having a system where you just give your intention and then we find the animations for that.
Thank you.
Hey.
Hey, so just one question about your, so you had animators or designers going and tagging up animations to say, oh, it's like happy or sad or whatnot.
Is that...
determination of what the intention of the animation is, is that something you foresee that could be determined procedurally?
I don't think we...
The question is whether the tags on the animations could be decided procedurally.
I don't really see how...
Maybe with machine learning and classification.
I'm not sure it's worth it.
I'm not sure.
I'm not sure.
I don't think so.
We didn't tag animations with moods, we associate them with idle states.
Although that would be an interesting idea.
Yeah, that's it.
There's no other question.
So thank you, thanks a lot.
