Hello everyone.
So I'm Gautier Boida.
I'm working at Square Enix in the advanced technology division as an AI engineer.
And today we'll talk about verbal interaction with emotional character AI.
So at first, before going further, I would like to say that all the content and assets that has been created for the demo I will show later, has been created only for conferences and studies purposes.
So it's not a new IP at all, not a new game, it's just experimental.
And this asset has been created by the people in the bottom row.
So mostly technical artists and technical animators.
And the top row, we have two AI engineers and two product managers.
So thank you to all of them, and also thank you to all the people that helped me on this project.
Okay, so let's move to the motivations.
And what are we trying actually to improve here?
So when we started to experiment in virtual reality with non-playable characters, we were amazed by how we were able to feel their presence in this world, giving us a real feeling of being part of it.
But we were with our own bodies, so we wanted to interact with them in a natural way.
But we were limited to buttons or other classic mechanisms that did not apply well to VR.
So we wanted to do more.
And on the AI side, it was looking at us as if we were a ghost, just looking through us, not really caring about what we are doing.
So we wanted to also improve this aspect.
So how can we do it?
By bringing more natural interactions, through voice, also through body language.
And on the AI side, by creating a more aware, expressive, and lively agent that can interact with the player appropriately, through actions, emotion, and reactions.
but also to answer their own needs.
So at first, before the whole explanation, I would like to show you the demonstration of what we have done so far.
And we'll be able to see that the agent, Kobun, will express a lot of different emotions and reaction to what he's experiencing in the world.
And he will also react to my voice and my speech, and also what I am pointing in the world.
So let's check it.
Hello.
A bit loud.
Can you plug the tank in the hole?
So here you understood what I was talking about.
But the canister has a lot of smoke emitting from it.
It's quite poisonous so it's fleeing away right now.
We have to fix that.
Can you plug the nozzle in the white thing?
So here I am trying to fix the white thing which is the canister in this case.
Great, yeah, repaired.
Can you plug this in this hole, please?
And here I was using my hands to point at the object in the world, and the engine was able to understand.
Great.
Take the orange clip.
Here again, with another predicate, orange, and it understood that it was a clip.
But yeah, I was a bit bad to one of my AI at the end.
He's not very happy about it.
But he didn't know that the electricity was something bad for him, but he learned about it.
And so he flew away from it.
So what's on the menu today?
So we'll talk about our speech recognition pipeline and then the decision making with the emotional components and some factual statement at the end.
So let's start with speech recognition and pipeline.
So the first block is our speech recognition engine.
It's taking the voice and translating it into a sentence or a list of predicated words with their part of speech.
And we are sending this data to our game experience through the voice pipeline.
And the first block is a grammar parser.
And here the goal is to translate this language-based grammar into our ontology-based grammar.
So you can see some new types, such as predicates and object, which is much more easily understood by our AI.
And then we have a word abstraction step, where the goal is to translate these words into a language independent representation.
And here it's a colored block you can see here.
So why do we want to do that?
Well, we want to support multiple languages without limiting the player set of vocabulary.
So if later I want to add French to another language, I don't want to do all the bindings one by one manually.
It's too much work, and I want a better way to do that, a method that is maybe automated.
The main problem here is that worlds are language-based, so they don't have binding one-to-one between languages.
So we need to abstract them.
And the idea here was to create the DNA of a word, in this case, thinking about what could be the genes.
So if I say, take an apple, or take a break, these two take has different meaning.
So you could actually have your word take and tag it with all the different meaning it has.
And one meaning could be one gene, and then the DNA will be just a list of meanings.
So with that idea, we decided to use WordNet.
A database of set of creative synonyms named Synset.
It's expressing a distinct concept.
And it's about multiple languages, which is what we want.
So let's take an example.
Let's say that we want to create a concept of big in our experience, as in a big apple.
So an adjective here.
At first, we search in our database what are the different meaning of big.
We have maybe 20.
but it's too much, so which big meaning are we actually interested in?
So we said an adjective, so let's remove the adverbs.
And then we're going to take all the meaning one by one and select the one that we're interested in.
In this case, we're interested in above average in size or number or quantity or extent, and also significant.
And these two meanings will make our DNA for the concept of big we wanted to create.
If we have some doubt about one of this meaning, we can check the WordNet database in detail and understand which words they are actually bound to in the different languages it supports.
And we also have sentences examples at the bottom.
So it's very easy to understand if this meaning is the one you want.
Okay.
So now our statement is totally language independent, and we are supporting multiple languages thanks to WordNet.
And also, now that I'm on the previous one, yes, this one.
So now, the goal will be to try to understand this abstracted world into the concept that we're actually supporting in our game world.
For instance, pickup and enormous are not part of our game, but we have take and we have big as well.
And because they are close to pickup and enormous, our grounding step will be able to translate them into take and big.
So here we are going to use a UTT-based scoring method.
So here is a simple one.
We have the word enormous with its DNA.
And we have a list of predicates that we are supporting in our game world.
So big, small, red, with their own DNA.
And we are just going to compare their DNA and check which DNA is the closest to the DNA of Enormous.
In this case, big is going to be the closest, so Enormous is going to become big.
So now, if I say, for instance, take a giant apple, giant will also be translated into big.
So we are actually now extending the player set of vocabulary.
And we are also supporting multiple languages.
So now we don't even know, actually, how many words we are supporting now.
However, here we are still outside of our AI.
We are just in our voice pipeline.
So we cannot ground everything.
Objects particularly relies on the knowledge of each agent.
For instance, on the left, agent A known about a bowl, a banana, and a table.
So if I say, take a fridge, it will try to understand the DNA of fridge against a bowl, or a banana, or a table.
It doesn't know yet what is a fridge.
So we'll do it later.
Okay, so now our statement is ready.
So we just store it in our statement memory and we can move to the next step, which is the decision making.
So our decision making system is very simple.
We have a goal manager and a planner.
And the goal manager is UTT based.
So we have a list of goals and we are going to compute a score for each of them.
And we are going to execute the highest scored one.
In this case, going to be listen to player.
And the plan that we are going to have is listen, which will just copy the statement from the statement memory into the AI memory.
And then the agent can decide if he want to execute or not this order.
In this case, he does want to.
So we are going to have a plan that will be look for this object and then go near this item and take it.
And that's where we're actually going to do our grounding for the object.
So the look for action, we try to understand to which object it is the closest to inside the AI memory.
And to do so, we are going to use again a utility-based system, it's going to be the NPMT-AXIS utility system.
So what I didn't say earlier is that goals can actually have targets.
So you can have multiple goals with different targets and compute a score for each of them.
And that's what we are going to do for the look for action.
We are going to have one goal, find suitable objects, and plenty of targets.
And these targets are going to be the items that my AI knows.
And we're going to compute a score for each of them.
So now we are going to try to understand how close or similar is this target against the object that we have in our statement, which was our enormous apple.
So it will depend on different axis.
The first one is object type.
So we are going to compare how close the type of target and this object DNA is.
And then the size and a lot of different axis, such as color, location, closeness, and confidence.
We are going to have a weight, a score for each of them, and combining the score, and we get the score for target.
So let's move into our object type axis and understand how we can compare the type of this apple, for instance, against our object DNA.
What we can do is just compute the DNA of apple and compare both DNA together.
In this case, we have the same DNA, so we are going to have a score of one.
And for benign tables, the DNA is totally different, so the score will be zero.
I don't know if you see it now, but we're looking for an enormous apple, which is an edible fruit.
Nevertheless, the score of banana and table are the same, really far away.
But banana is a fruit as well, an edible fruit.
So it would be better to have a better score than zero, but maybe not one, because it's a UTT system.
So we have actually a better way to do so.
WordNet organizes all these things or meanings into a hierarchy.
And our object apple is actually going to be located right there, below edible fruit.
Which is also the case of banana, but not table, which is located quite high in the hierarchy.
So we can see how we can get a better score here.
So at first we are going to check where our object DNA is located inside this hierarchy, and set the node to one, and just interpolate into the root.
And all the nodes below, because they are also apples, are going to be set to 1.
And now we can compute a score for all the different objects we have.
So apple is going to be 1, but banana is going to be, this time, 0.86.
And table is going to be 0.14.
So now we have a better score.
And for instance, if in my game there is no apple, and I say pick up an enormous apple, maybe the agent will actually take a banana instead.
But it's still close enough and acceptable.
It's better than a table, right?
Okay, so let's take an example this time with our apple A, and try to score it.
So the type is the same, so we are going to have a score of one.
And then for the size, we are going to check our AI memory.
How big is this apple against the thing I know?
It's average in size, so we are going to have a score of 0.5.
And we do the same for the other axis, and we get a score of 0.91 for apple A.
We do this for all the different items, and the winner in this case is apple B, our biggest apple inside the memory, which is what we wanted.
Okay, so the look for action has found that the closest item inside the AI memory is Applebee.
So now we are just going nearby this item and take it.
And that's what I am going to show you inside this video, but with different objects.
It's going to be an orange clip.
And because from now on, we are going to talk about emotions, you will also see some new things, such as emotional reactions with liking, disliking behaviors.
Take the orange clip.
Sorry.
Please, take it.
You don't want to take it, right?
I will remove electricity then.
And now, can you take the orange clip?
Great.
You are willing to take it this time.
Can you plug it to the red pole, please?
Cool.
So the goal of this experience was to fix the rocket, and by plugging the cable to the red pole, we fixed half of it.
That's why the agent was happy at the end.
But earlier, he has been shocked by the cable, so he was not really liking it at all.
But at the end, he was maybe starting to like it.
So I will explain this in the next part.
But before, let me introduce our emotional components.
So it's composed of three different modules.
The first one is emotion, so a very short-term feeling that evolves quickly over time, such as joy, distress, and fear.
And we have also a mood module, which is a much more long-term feeling that evolves slowly and smoothly over time, like exuberant, depressed, and afraid.
and they are going to influence each other.
And on top of these two module, we have a personality module that will define the agent.
That has no real evolution over time, and it's things such as curiosity, shyness, and laziness.
So let's go in our emotion module at first.
So this is an inspired OCC model.
It's not the original one, but it may still look a bit complex.
So I will explain it.
So it's taking as a top a valence reaction or an event, and we just assess from our AI point of view if it's a positive thing or a negative thing.
And then, depending on the type of event, like an aspect of object, if I saw something in the world, for instance, I may like it or dislike it.
Therefore, I will generate a love or hate emotion.
If this event is a consequence of something that happened in the world, or a consequence of an action, we may express joy or distress.
And if this event is an action of an agent, if it's my own actions, I'm going to express pride or shame, and if it's the action of the player, for instance, I may express admiration or reproach towards the player.
And also, if there is a related consequence to these actions, if it's a related consequence to my own actions, I'm going to express them to be gratification or remorse.
And if it's a related consequence to the action of the player, I may express gratitude or anger toward him.
So that's the model, it may even still be complex.
So let's take an example.
So earlier in this video, the agent has been shocked by the cable, and this cable was activated by the player.
Yes, press the button, right?
So this event is negative for our agent, I think, right?
It was a consequence of the event, of the action of the player, activation of the button.
So we are going to express our distressed emotion.
We are not really happy that we have been shocked by the cable.
But we are also going to express anger towards the player because it's his fault that I have been shocked by this cable.
And that's how we are going to generate our emotions.
So now, how can we like or dislike the thing in the walls?
Well, earlier, when the agent has been shocked by the cable, we have generated our distressed emotions.
But you can do one more thing at this step.
We can also add a negative effect to the cable object, but also to the electrified predicate.
And an effect will just be an intensity with a memorable duration, so that it can forget over time.
And now, thanks to this effect.
The agent will not like anything that is a cable or anything that is electrified.
So if there is another electrified something over there, it will not get close to it.
So great.
The agent does not like the electrified cable anymore.
So what if we tell him to take it again?
Well, the plan is still the same.
We're still going to look for our cable.
We are going to find it, and then we are going to try to go nearby this cable.
But because he's hating it, he will refuse, and the plan will be canceled.
OK, next, the mood.
So it is based on the PAD model, so pleasure, arousal, dominance.
And the P means how pleasant is an emotion.
A is about how intense is an emotion.
And D, a bit more complex, it's about how much control and influence the agent has over situations.
And inside this cube, we'll have all of our emotions.
And this cube can be separated into eight octants.
For instance, here we have the mood afraid.
And at the top right, where joy is, it's going to be the mood exuberant.
So we have eight of them.
And inside this cube, we have our default mood, which is currently at the position 0, 0, 0.
And this mood is going to move smoothly over time, depending on what emotions are going to be generated.
So let's see the update now.
So let's say we have a joy emotion with a certain intensity for five seconds.
Depending on the intensity, the speed at which the dot will move towards the joy emotion will be different.
And when the emotion has been expressed, there is no emotional activity anymore.
So we will slowly go back to our default mood.
In a more complex example, it can be something like that.
So here we are exuberant, and even more exuberant right now.
We can see that he's very smiley now.
And now a bit of eight, so we are just in the hostile mood just at the beginning, and we can see a sort of vicious smile.
Now he's quite not happy at all.
And now due to the same emotion, he's very sad, a bit afraid.
And now a very strong hate emotion, and we can see that he's very not happy.
So thanks to this PID model, now we are able to blend our different animation for the different moods and get some new things, such as the vicious smile.
So it's a very powerful tool.
And lastly, we have our personality module, which is the simplest one.
It's just UTT parameters, such as laziness, curiosity, honesty, obedience.
And I want to introduce at first how we have done the laziness one.
So in this video, at first, yes, laziness is 10%, so not very lazy.
So let's check.
Take the purple clip, please.
So here, yes, he's just going to take the purple clip.
Nothing special.
Take the purple clip, please.
But now he's very lazy.
He does not care anymore about the color or any other thing.
He just want to take the clip, the closest one, because he's very lazy.
So how did we do that?
So personality laziness parameter is going to affect our look for action.
We'll favor closer object over farther object.
If you remember earlier, I introduced our find suitable object action with a certain axis named closeness.
And you can see that the weight is 0.1, so very small.
So maybe why is it small?
Maybe that's the first question.
Well, we just wanted, just in case we have two items with the same score, we wanted to favorite the item that is a bit closer to the agent, but just a little bit.
But in this case, the lazier the agent is, the more important the closeness axis should be.
So instead of having a static weight, we are going to have actually a way that will depend on this laziness axis.
And we can add a range if we want.
And thanks to this, we are now able to express our laziness personality parameter.
In general, the personality parameters can affect a lot of different things in the decision making.
They can affect your goal manager, goal score, or the set of actions you will use for the planner.
They can affect also the tolerance on liking, disliking, or the way the emotions are going to be expressed.
For instance, with shyness.
So it's a very powerful tool, despite being very simple.
And it allows for a great variation of place.
And if we have multiple NPCs, they will all feel a bit different to each other.
Okay, so lastly I would like to talk about a factual statement, which is about informing the agent about the world.
Maybe there is a nozzle in front of the rocket.
Can you take the nozzle?
So here he's looking around in front of the rocket.
And he found the nozzle, very happy.
Maybe there is another in front of the rocket.
Can you take the nozzle?
So obviously here there is no nozzle, but he's still going to look for it in front of the rocket.
But yeah, this time.
There is no nozzle, right?
There is no nozzle, so he's not really happy about it.
We lied to him, so he's not very happy.
So how did we do that?
Well, at first, the player is just going to say a statement, such as, there is an apple on the table.
So we are going to listen to the statement, and we are going to create a factual apple in our memory.
We don't know yet if it's actually existing.
And then the player is asking the agent to take an apple.
So we'll look for action, we find that the best object is this factual object, this factual apple.
But we don't know yet if it's actually existing in the world so we are going to try to ground it.
So another goal we'll take over is going to be the ground fact goal and the goal will be to go on the table and just look around.
In this case there is an apple so we're just going to replace our factual apple with this apple.
So when the takeApple action will take over, we'll find the correct apple and the agent will be happy about it.
So what if there is no apple on the table?
Well, the ground fact action still, goal still happen.
We're still going to go on the table, but we're going to find no apple.
So we just remove the factual object.
And then, when the lookFor action restarts, We find just nothing, so the agent is not pleased about that, so he's quite angry about it.
We lied to him.
So, what did we achieve so far?
I think we brought more natural interactions, so at first the speech recognition pipeline, but also we created a more aware and expressive and lively agent, thanks to different emotional reactions, the moods, but also the emotion.
And this agent has upgrade variations thanks to the personality module.
And this agent are also environment aware.
They can like, dislike, and react appropriately with this knowledge.
They can, for instance, refuse to do an action that is involving an object that they are hating.
And the NPCs can also react to truth and lies regarding the environment.
And what can we do from here?
Well, the cool thing would be to develop a relationship between the player and the agent.
For instance, if the player is polite, maybe it's going to be OK.
But if we're angry towards the agent, maybe he should flee away or be scared, this kind of thing.
You could also have multiple agents, especially in VR.
It's a big world, but we are just alone with this little creature.
It would be even better with a lot of different agents that interact with each other through emotions, but also with the player.
And also, this is from play testing.
We are working with speech recognition, so it's important to have a lot of diverse feedback from our AI agents.
The most easiest one is certainly, I did not understand your speech.
But something such as, OK, I understood, but I don't have the ability to execute your order.
This is some kind of feedback that you need to actually create so that the user understands what's happening.
So I think that's it today for me.
And we have two minutes for questions.
Three, two, three?
Yes.
In one of the videos, the little character, it, when you told it to go, get something, it turned and knew exactly what you were talking about.
Yes.
But then when you lied to it, it also turned and knew exactly where to go.
So what's confusing, it seems that it knows that there are things that are there in the first case, but when you lied to it, why wouldn't it know that there is not an apple there?
And I know that's a double negative.
I think I did not understand the first example.
So when you first spoke to the creature, it knew.
It behaved as if it knew everything that was around it.
But when you lied to it, it behaved as if it was surprised that you didn't know.
That seems contradictory.
So you're speaking about the factual statement where at first I am teaching the AI, like for instance, there is an apple in front of the rocket.
And then the agent actually checks and there is nothing, so he's quite angry.
So this little difference, right?
Well, there is that, but it seems that at the beginning, because it didn't discover with its eyes, what was in its world, right?
It knew what was in its world, but when you lied to it, why didn't it know what was not in its world?
Because it cannot see everything.
We have a perception model here.
So what you can't see will be inside its memory.
But maybe I am able to see something that is in front of the rocket that he was not able to see.
And in this case, that's why he was still checking and then be surprised that I actually lied to him.
So there's a camera view that it sees that is different from yours.
So yeah, the agent has a perception module.
just a cone or something in front of him and just add this knowledge to its memory.
It's not what the player is seeing, it's what he is seeing himself.
Okay, thank you.
If you have other questions we can go to the wrap-up room. I don't know which one it is.
I'll just discuss after. No problem.
Thank you.
