Hello? All right, cool. I'm on.
Hey, everyone.
I hope you're having a good morning, bright and early here.
This is early for me.
I'm here today to talk about real-time cloth solutions on Marvel's Spider-Man.
My name is Sophie Brennan. I'm a character TD.
I work at Insomniac Games.
We are...
celebrating our 25th year anniversary this year.
That's a long time for a game studio to be around.
I just want to acknowledge that apparently we're doing all right.
We're doing good.
And here's to many more.
So a little bit about me.
I am a character TD, as I said, or rigger, or technical animator, or whatever you wanna call me.
I have around seven years experience.
Obviously, I just shipped Spider-Man.
Before that, I worked on Lone Echo, which was a game for the Oculus VR game.
And then before that, the Order 1886 for PS4.
Firstly, I just want to say thank you to our entire team at Insomniac.
Obviously, this project wouldn't be possible without them.
At Insomniac, we try to stay, like, really humble and acknowledge each other as frequently as we can.
I like to carry this through and say, you know, while I'm doing a talk today, like, everyone's involvement in this is super important, and I just want to say thank you, because they're an awesome team.
So without further ado, let's talk a little bit about Spider-Man.
And firstly, I have a little cloth reel prepared to show you kind of how many various solutions and where the cloth was in our game.
So here's a little taste of that.
Thanks for watching my Dnew video & don't forget to Like & Subscribe, it means a lot to me!
Cool, so now you guys got a little idea about what we did.
Let's begin at the start.
So what kind of existing cloth techniques can already exist out there?
So, first of all, we have skinning to existing joints.
The advantages of this, you know, cheapest method, it's easy to maintain, all the stuff's already there, right?
Depending on where the cloth is, though, it can look bad, it's unrealistic, complex articulated areas suffer.
If you have a sleeve that's modeled hanging down when you twist your arm, the pose doesn't adhere to gravity.
I'm sure you've seen that in many games before.
We also have animated cloth joints.
I hope you guys enjoy the picture on the right here.
We actually don't have any key framed cloth in the game.
So I just let you run wild with your imagination and say, imagine animating all these controls.
So the pros of this is that it can look awesome depending on how talented your animation team is.
But there's a ton of disadvantages, which is it's super time consuming to do.
It's really difficult to animate cloth convincingly and hard to make it look realistic.
There's also bait simulated cloth joints.
The advantages of this is that it's accurate and realistic, depending on how your setup is.
Disadvantages are.
that it's time-consuming as well.
It's hard to maintain, especially in a game dev environment where we're always iterating.
If we change the scene, we'll need another pass.
It needs to be re-baked and re-simmed and baked down.
And then the output is the curves that are really nasty for animators if they actually want to edit that stuff.
So then we have real-time cloth, which the advantages are it's super flexible and reacts well to change because it's already been calculated at real time.
But also it responds dynamically to player movement.
So if a player is controlling a character, which is what we had in our game, and they flick the stick around, there should be follow-through and overlapping action that normal baked simulation couldn't give us.
The disadvantages are is that because we're running at real time, the cloth quality can be quite low, and detailed cloth is really out of scope for us.
It's also an extra expense in engine versus just running an animation, and this one can be specific to whatever kind of cloth solution you're using, but most lack self-collision.
So if you have layers of cloth, they won't react with each other.
So why did we decide to choose real-time over all the other options out there?
And so one of the internal pillars for Spider-Man was to create a believable world.
And with that, realistic cloth fit in there, right?
To make a world that felt believable, it had to look believable as well and look real.
On previous projects as well, we'd fallen down a rabbit hole, the simulation, and it was really, really time-consuming, especially for our team.
And in a project this size, it just, it really wasn't...
feasible for us to spend this amount of time simming things.
And also, animator's time is valuable too.
We didn't want to take up animator time on animating cloth.
And if you know any animators, they probably hate it anyways.
And most importantly, as I mentioned earlier, it responded to player control, which was a benefit that none of the other methods had.
So.
A big component of real-time cloth are the tools used to achieve it.
So we had two choices, which were develop your own proprietary tech, or use an existing third-party middleware.
Developing your own tech has pros and cons, right?
Pros are you have complete control over what features you want and need to focus on.
You can design the workflow to match your needs.
But the cons are it's a huge upfront cost.
It will come online probably way later than you need it, which is usually immediately.
And it will require maintenance, and you'll have no external support.
For us, this was just not simply in our budget, but the results that we wanted to achieve.
So we decided to go third party.
And the choices out there are fairly limited, but we already had used Havoc in-house for our soft and rigid body physics, and we'd used the cloth technology in Sunset.
So it was a pretty easy step for us to just continue on with Havoc.
So while I want to cover some more general topics in this talk, it's really important to give you context around the tools that we're using.
So for that reason, I'm going to give you a little intro into Havoc itself.
So Havoc is essentially kind of plug-and-play in that it supports a 3D program like Max or Maya with a plug-in that you install.
And then also it has its API, which will plug into your engine.
And...
So from there, you will be in Maya or Max, and you'll generate your some mesh, and it will either output that into a file, or there's a non-modal way as well.
So modal is when all the data is contained within the program itself, and non-modal is when it spits it out to external files and uses its own executables to manage that.
But either way, it takes the content that you generate in the 3D package, puts it into the filter manager, Havoc's filter manager, which is basically where they handle all of the systems across all their physics components.
It basically runs a series of filters and operations on it to send the cloth, prepare the data, and send the data to the cloth setup tool, otherwise known as the CST.
The CST is basically where you set up all the behaviors for your cloth.
So this is where you set up gravity, dampening, the radius, the sign collision, capsules, all that.
And then it outputs that to a data file, a .hkt, which is read by the cloth API in the engine.
So here's a little visual just of what the actual tool pipeline looks.
We use Maya specifically.
So I'm sorry if you're a Mac user.
I won't be covering the pipeline for that.
We take it from Maya into the filter manager, so you can see the purple box down there is the filter manager, and then from there that launches the cloth setup tool, where we can iterate on our cloth setup.
And let's talk a little bit about vertex versus bone cloth.
And what does that mean?
Havoc supports both.
Vertex cloth is when your sim mesh is your render mesh, so it's basically a direct connection.
This means you have to have the correct materials applied to the sim mesh.
It also means that it isn't possible for it to do double-sided cloth.
And the resolution of the mesh has to be fairly low to run well in Engine.
It also means that any content changes to this cloth might change the behavior of the cloth itself as well.
And irregular shapes aren't great for simming.
Havoc prefers quads, and therefore, you have to make sure you give it a quadded mesh.
There's also bone cloth, which is what we used.
And this is different in that.
the sim mesh will drive a bunch of joints on the surface of it, which you can in turn use to drive your render mesh.
And this allows for the possibility of additional animation as well because you're using joints.
For us, that gave us the ability to turn off the cloth and override it if necessary.
However, as you can see, there's a lot more intermediate steps and it complicates the process.
So there's a lot more layers of content to debug when things go wrong.
So here's the content authoring tools in Maya.
There's a mesh properties toolbox, which allows us to assign vertex selection sets.
And this is where we would set which vertices are simming versus which ones are pinned, and also allows us to create little selection sets for optimization purposes.
For example, if you only want an operation to run on a smaller subset of vertices instead of the whole mesh.
There's also the mesh properties toolbox for painting vertex properties.
So here you'll see I'm painting properties on the mesh.
This is a distance.
It's just like a paint weighting interface.
And this can control many things like distance, mass, minimum normals, particle radius, basically anything that's exposed in the Havoc cloth tool, you can probably paint custom various information for.
And you can see here I'm painting on the mesh.
And you can also set minimax limits on the property.
So you'll see me toggling the overall distance to change the limit of the properties themselves.
There's also collisions.
So you need to get your cough to collide with your mesh.
You need to assign and create colliders.
You can do this with the Havoc bone collider toolbox.
And basically you just select a joint, assign a collider to it, and mess with the properties on it.
The properties are radius, height, and taper.
And you can also, through this interface, add your own custom collision shapes.
And there's a variety of methods that Havoc uses to get that into the engine.
And finally here, I think this is our cloth CST tool.
There's no native API inside the Havoc to let you alter the properties live.
So all the cloth data is authored and loaded in a game.
And the CST is where the magic happens.
So it contains ways to create and assign some meshes, apply properties, constraints, colliders.
They can get pretty in-depth, but for the sake of brevity, know that almost all the settings related to general behavior are set here, where the specifics of those behaviors are added in the content creation part of the process, or in our case, Maya.
Oops.
Oops, sorry, I forgot about this.
Obviously, when you get it into the engine, you want to debug it, right?
So Havoc has an in-built debugger.
It draws the Havoc sim mesh and colliders.
It lets you filter specific elements, and it also provides performance information, as you can see there.
So let's talk a little bit about some of the initial tests that we ran on cloth.
So we first started with our inner daemons.
and our thugs.
They both have very distinctive behavior but in very different clothing.
So for the thugs, they needed baggy loose pants and for the inner demons, smart suit jackets that held form.
So here's some initial tests.
We used them to test our animated normal maps and how they would work on the cloth as well.
And also to see how many enemies we could draw on screen at once with cloth.
And the answer is apparently a lot.
Through this, we wrote up some internal documentation in the process.
However, the workflow was pretty complicated with many points of potential error.
And this would continue to cause us a lot of problems and headaches down the line.
So this is where I jumped in.
This is all previous to when I joined Insomniac.
So for clarity, before joining the company, I'd really, really touched cloth beyond the proprietary cloth set up at my previous company, Ready at Dawn.
And even then, my exposure to it was minimal.
So perhaps this was a bit of a bonus because I came in with fresh eyes.
And my first test case was MJ.
And MJ had a few things that made her a challenge, which were an extreme crouch pose.
She had to be under one meter high for designer metrics, which I actually went into my backyard and tried to shoot reference of me crouching and run walking at the same time under one meter.
And it just isn't really possible to be honest, but.
It was really hard.
So it was a very extreme pose.
She also had two layers of clothing, which was her outer jacket and her inner jacket.
She had a ponytail and a scarf.
So she had a lot of layers on her.
So I ran that first test through her.
Apologies, I don't have any captures from that, probably for a reason.
we just, quite quickly, I was able to identify that there was issues in the pipeline.
So, authoring content took a really long time and exports were failing often and we didn't really know why.
And you can see there in the picture, that's our infamous white cube that indicated everything went wrong and you're terrible and maybe you should cry a little bit.
Also, iteration was really, really slow.
So, and this was partially because there was a debugging disconnect, so we were unable to see our cloth draw on top of our assets directly in engine.
And obviously this means that there's a lot of guesswork involved when it comes to finding out where your problems come from.
Especially when you have layered elements on top of each other and multiple colliders.
We lacked enough debug information to even, like, draw which elements were which.
So without labeling, we were kind of in the dark as to where our issues were coming from.
And yeah, that meant information was hard to parse, and without any filtering, we couldn't isolate bits of the setup, which is usually, you know, the best method of solving problems.
So...
I began to streamline the pipeline.
So my first thing was I wanted to address anything that could be automated.
There were a lot of different windows, as you saw.
There was a lot of repetitive actions.
I wanted to try and minimize that and make the process a lot more straightforward.
So my first goal was to, when you're setting it up, obviously you get the content from Maya, you send it to the Havoc filter manager, where it does all the filtering stuff, and then that goes to the Cloth Setup tool.
Now the filter manager, we were running some kind of specific stuff in there that I found we could really generalize and actually make a template from if we were smart enough.
So this was a process that was stopping us because we were generating content, then jumping into this filter manager, and then setting up the filters, and then exporting it to the Cloth Setup tool, and then once again editing content.
So the first thing I did was find a way to genericize the template enough that Boom, I could remove that filter manager, and we could take our content straight from Maya into the cloth setup tool.
I also developed a UI.
It's not a very fancy UI, but it did its job.
which was I had it house all the tools that Havoc gave us.
So they were easily accessible.
So you can see there we have the mesh channels toolbox and the cloth collision toolbox we went over earlier.
You could also launch the cloth set up tool, the Havoc visual debugger and filter manager all from within this tool.
And you could, that nice big green button is when the one you wanted to press when you wanted to export or update your file.
And on the left here, To get the joints into Havoc the way we wanted to, we had to create selection sets.
And these selection sets were necessary because the other methods that the filter manager had weren't always great at capturing the exact joints we needed.
So we would pass them in with a selection set that was selected on export.
We also had our filter export nodes.
So Havoc has a habit of, when you go to export, basically going through every single node in the scene.
And that's a big problem when you have big scenes with lots of blend shapes and lots of mesh and lots of proxy mesh.
And it would basically make our export times It's huge.
So the first thing I could do here was make sure that it was only exporting what it needed.
So we just created a filter set for that.
Unfortunately, Havoc doesn't have a.
support for doing two selection sets upon export, so I had to find another way to pass information.
And apparently, the best thing I could come to was a visibility toggle, which if any of you use Maya in any kind of frequent basis, you'll know that there's about 5,000 ways to hide and unhide nodes in the scene.
So I had to write this very brute force method that went through every node in the scene, captured every possibility of its visibility state, saved that, then hid it, the ones that weren't in the list, and then exported, and then restored the state of them.
But it was kind of brute force, but the benefit of that was it cut down our export times dramatically, because no longer were we parsing the, you know, 50, 60, 70 blend shapes in the file.
So another one of our big challenges was to get the cloth on our playable characters.
And since this was all dynamic, we needed to test the cloth while moving the player to be able to test the motion on it.
Since the content had to be authored before export, we needed the iteration time to be as small as possible.
So changing one small value caused an asset rebuild.
And if that meant reloading the game or reloading the level every time, it was going to be really hard to create quality assets within a reasonable time frame.
So we needed the updates to be live.
So we worked with our core department, our engine department, to make sure we could do this at runtime without massive reloads.
And this video shows me changing the particle radius.
See, I'm exporting it there, and now I'm waiting for it to come into game.
Wait a second.
It's rebuilding the asset.
Boom.
So that was a wee bit of a wait, but that is nothing compared to reloading the game.
And that was amazing for us.
Our core team worked tirelessly to give us that feature, because that meant that instead of reloading the game every time we wanted to see a change, we'd just press a button, and it would rebuild the asset and load it in.
So now that we've made improvements to our pipeline, let's talk through some specific cloth examples we had.
Oops.
Our first example was our full body lab coats.
So we knew we were going to need these early on.
There was a pivotal scene early in the game where Peter and Otto finally develop a working prototype for their prosthetics research.
And we needed to showcase what we could do with the system, so we knew the quality bar was high.
You also meet Otto at the beginning of the game, and I knew Peter was going to be playable in his lab coat, so we had the challenge of creating real-time cloth for both cinematics and gameplay in this scenario.
So to reduce the complexity, I wanted to be able to share the setup between both gameplay and cinematics.
In hindsight, while this was easier for us in implementation, it made for quite the challenge, because the needs of each setup could be dramatically different.
So the first thing we did was we bought a lab coat online.
We looked at both official lab coats and costumes, and we ended up with this, which was the closest we could get to our concept art.
And then we did a bunch of stuff, like running around in it and doing some arm poses and whatnot.
We recorded some general movement and you can see here our fellow TD, Bell, modelling it for me because one, the coats were looking like I was wearing some sort of bin bag and I was nowhere near as buff as Peter.
But this gave us a good starting point, although we ended up going for a kind of heavier cloth.
And we also spot specific shot references.
So we knew what poses we were going to hit from our mocap, and we knew we had a couple trickier ones that might be difficult to hit the cloth alone.
So we took some specific shots for that.
So here's an example of one versus in the game, and from the other side as well.
So here's some early screenshots of our first tests.
As you can see here, obviously needed some work, but we started our sim from the pectorals down and we simmed the sleeves as well.
But we needed to do a ton of extra work to kind of hit the quality bar that we needed.
So we dived into making some blend shapes for it.
And as mentioned earlier, we wanted some very specific shapes, especially around the shoulders that weren't gonna be possible with the real-time sim.
also because we weren't running on the shoulders themselves.
So we developed two types of corrective shapes.
We developed a generic system for bending the arms and reaching the arms up and forward.
And we also developed very specific shapes that would come on in very specific poses in cinematics like this one.
So here's a small example of what these blend shapes look like in action and where and how we use them.
So you can see on the left we have our blend shapes, on the right we don't have our blend shapes.
And that little debug UI there is probably hard to read for you, but the little orange and yellow entries are the blend shapes triggering on and off and animating on and off for each shot here.
So for a finish and touch, we weren't really, we weren't able to get enough detail with just blend shapes or cloth.
I mean, we'd already at this point doubled the resolution of our lab coats to get the silhouette on some specific shapes, but it still wasn't cutting it.
So we made use of our animated normal wrinkle maps to get the detail we needed.
So you can see in this video here in our tools, I'm animating that normal value up and down.
And then in game on the right, you can see I have this custom attribute called IGAnimLabCodeCustom, where we stored that texture map.
And actually, we assigned in the material at least one channel to cinematic textures, where we would swap them as we needed per shot.
Finally, one final hack we had to do is our cloth didn't support collision with the environment itself.
So many of our rigs would include a cloth collision plane, which was basically just a plane collider or some sort of ground plane, in the actual rig file itself that animators would animate.
And our vanity system worked with cloth collisions, so we were able to toggle this on and off.
So here you can see it.
We generally turned it off for the sleeves because the sleeves slightly penetrating the table was better than them completely freaking out because they were pinned between two colliders.
But we used that to shift the lab coats out of the way when they were bending over and whatnot.
We used it in a bunch of places all across the game.
There you can see it.
Easy, easy, easy.
So let's move on to our second example, which was suits.
So we had a lot of suits in the game, like so many suits, it's stupid.
Mr. Negative was one of our biggest villains, or one of the big villains in the game, and he needed to look the part.
He was smooth-dressed and clean-cut, and that needed to be shown in his suits.
We also had suits being used for all of these thugs, as well as Norman Osborn.
So, the big part of the suits was them to actually hold their form, especially when characters lifted their arms, right?
A good suit that's well tailored will hold its cut when you lift your arms.
So once again, we dressed up poor Bill in a suit, and he modeled it for us.
And we took a bunch of reference footage of bending arms and lifting arms.
And we paid attention to how the arms moved, how the shoulders bunched, and the arm before this reference.
We knew that.
It was going to be the same deal, getting this jacket to work, where we're going to need additional work with both real-time cloth and blend shapes.
So here's an early progress shot of us working on, this is specifically Bill's work on the train sequence with Mr. Negative.
So that was a first pass. You see the jacket isn't lifting at all.
We do have real-time cloth swimming though.
A secondary pass, we wanted to get a little bit more movement out of that, so you'll see it lifting a fair bit.
But we're still not there yet.
Shoulder shapes don't look as good as they could.
And finally, what we shipped with, with the shaders as well, which made a big difference.
You can see that jacket opens all the way up.
Looks awesome.
Took a lot of work.
So once again, as I said, we crafted some specific blend shapes.
And these were first simulated in Marvelous Designer by our character artists.
And then we sculpted them iteratively to hit the shapes that we needed.
And this was a really long process of a lot of back and forth.
So there was no shortcut to this.
We just had to.
you know, test it out, see how it looks and send it back for more tweaks.
But luckily, unlike lab coats, a lot of the suits were mostly black or had effects on them, which meant it was much more forgiving overall when we were doing blend shapes.
So let's talk a little bit about how we made the suits lift when their arms lifted, especially with the real-time cloth, because the big issue was when you lift your arms, if the real-time mesh sim doesn't move with it, then it's going to remain, say, at the hips, and it's going to collide if colliders no longer sync up to where the mesh is.
So we needed to have the two matching.
First we had a rivet mesh.
So this was a direct duplicate of our sim mesh.
And we would also have a series of cloth control joints.
And there wasn't just four.
Whoops, keep going.
There was more than that.
But you know, this gives you the point.
And these were driving the rivet mesh with clusters, skinned in a smooth gradient to the top.
And this stuff doesn't go to game, so it doesn't matter that we're using clusters to control it.
From there, we also drove this rivet mesh with a bunch of other things, because this was all additive.
So they were being driven by our core skeleton joints, our pelvis, spine joints.
It was skinned to that.
But also, on top of that, remember those blend shapes we were last looking at?
We're adding those, too, on top of that.
So we got a wrap deformer, matched the rivet mesh to the blend shapes that we were authoring, and then applied them as blend shapes on top of that.
and they were being driven directly by the rig deformation controls themselves.
So the rivet mesh was called as such because it had a bunch of joints stuck to it with rivet or point-on-surface nodes, or however you would like to drive it, but basically it was driving a bunch of nodes underneath it.
And those nodes were skinned to our cloth sim mesh.
So now our cloth sim mesh would go into game not only with animator adjustments, but with any movement from the blend shapes, so our collision could stay accurate.
And that cloth sim mesh was obviously being simmed by Havoc, was driving a bunch of cloth joints from that, which were skinned to our render mesh.
So looking at that, we had two things feeding into our engine.
We had our cloth control joints that were skinned to the cloth sim mesh, and we had our joints that were driven by the cloth sim mesh being skinned to the render mesh.
And those were both being sent to engine.
So if you look at it as a whole, it's not too bad.
Pretty clever way of getting our blend shapes on our cloth set up, because inherently, Havoc didn't support blend shapes on the sim meshes.
So another thing was is we had this wind effect, if the video would play.
Hang on.
Sorry guys, I don't know why that one's not playing.
We had a wind effect which was a rippling on the suit and this was generated by a series of, oops, I see a play button.
No.
No!
Why would it tease me like that?
These are all on one mega slide thing, so it's probably got messed up somewhere.
Sorry about that.
But yeah, they were offered with noisy blend shapes, so we basically just added the noise multiplier onto a series of blend shapes, and there was about 12 of them, and it would just cycle through them all, and this gave a rippling effect on the jacket itself.
This was different to Spider-Man's suit, That was done by animating normals.
However, because the suits were so dark, normal animation would be lost, so we had to create like a moving surface to get the wind rippling effect.
And after this, we tuned our shapes and behavior in the cloth, ties and sleeves, all that stuff to get the weight and dampening of the cloth feeling correct.
So with all these three elements, which were blend shapes driving the sim, real-time cloth and corrective blend shapes helped sell the look of the suit jackets in Spider-Man.
So our final example is MJ.
And I mentioned MJ earlier in the setup, so let's go over her particulars.
So once again, she had her hair, her scarf, her outer jacket and inner jacket.
Also meant to be playing a video.
Yeah, sorry, these separate elements had their challenges.
But the biggest issue was that, again, we had no cloth-to-cloth collision built in, so we had to be very careful with our limits and use collision capsules smartly to make sure we didn't have cloth clipping issues.
For us, I mean, in cloth, that was our biggest hurdle, right?
If you've played Spider-Man, a big...
Part of our challenge was we didn't want cloth clipping.
And you might notice a lot of other games, the cloth sim clips all over the place and they kind of just don't care what happens.
And we didn't want that for our believable world.
So to avoid this as much as we could, we built a lot of cloth scenarios that would avoid those kind of, or cloth setups would avoid those kind of scenarios.
So here's an example.
This is really not working today.
Another moving video, I'm afraid.
Here's an example of the sim meshes on the left and the behavior and game on the right.
So you can see all the different sim meshes.
We have her hair, we have the ponytail, we have the scarf, the inner and outer jacket.
And much like our other setups, we use a series of blend shapes to enhance the shoulder and elbow shapes in MJ's jacket.
But on top of that she had the layer jacket and not only that, with MG's crouching position, it would often get caught between her torso and her legs.
So some games avoid that trap by modeling the legs to fall to the side, but we didn't want to do that.
We didn't want to compromise.
So what we did was I had to have a concession where I did give in and gave animators controls and they'd use them to pose the mesh, the sim mesh out of the way so that it would naturally fall to the sides instead of being caught between the torso and the legs when first simming.
And we actually did sim the two separate layers of the jacket because I could have put them in one sim, but then and skinned them maybe differently, but I would have risked clipping, especially how close the layers were.
But this wouldn't have got such realistic behavior.
So the main trick for this was to make sure that the inner layer had very, very strict distance limitations.
That meant that it couldn't really escape this very tight cone of forward-back behavior, but had some nice side-to-side motion.
And this is really not good for me right now.
Once again, the scarf overall was a fairly similar setup.
However, the side-to-side motion, we wanted to limit it because the jacket was on either side of the scarf.
So to do this, we added small colliders parented to MJ's chest to keep it within a small range.
However, we didn't want to reduce the overall range of her scarf because she bent over a lot, and if we did that, we would hit a wall.
So, by adding two colliders on the side of the scarf that parented to her chest, we kept it in this very narrow area, which allowed it to have a lot of forward motion, but no side-to-side to avoid all the clipping that came with that.
And once again.
Yeah, the same with the ponytail.
We, MD wore it in a variety of outfits.
However, it would have to work with each outfit, so we didn't want to double this up by, like, you know, duplicating the sim mesh and the geo mesh out.
So we tried to coax it within a range of using colliders.
Not only that, but when we needed to limit the side-to-side motion, we couldn't actually constrain it with colliders on the head because it would look really weird if she turned her head very fast.
So eventually what we did instead was we lengthened the sim mesh way past the point of the sim and widened it a ton and this prevented it from twisting and getting caught between her shoulders and her neck.
So you can see, oh no, this must be an old version or something.
Okay guys, this is real bad, I'm sorry.
There were some scenarios as well where, so yeah, this was the picture of the setup, and then there were some scenarios as well of MG jumping from the top of the penthouse, where this scenario we couldn't do because we had to create a unique setup, and we didn't have time for that, so at this point we just gave it to the animators, and they did it with cloth controls entirely.
So let's get on to some nitty gritty with Havoc's best practices.
This is more specific to Havoc itself, but since this is what we're offering with, I thought it'd be good to share the knowledge here.
There was a lot of learning curves for us on this, so I just want to make sure, you know, spread the knowledge.
So, first of all, massive gravity, despite how wrong it feels.
So, we know gravity to be this fixed number, but actually in Havoc, we used it a lot to...
basically cheat whether things were heavier or lighter.
And even though it felt so, so wrong, it worked so, so right.
We also used the bone-driven method so that we could have thickness on our mesh, as talked about earlier.
We turned on motion transfer, which was an attribute on the cloth for playable characters.
And if you play games and you stop a character in a dime, usually there's a lot of overlap, or sorry, overshoot on the sim, where it goes crazy, because its velocity is coming to a complete halt.
So the motion transfer in Havok's cloth allows, basically puts a limit on the velocity it can achieve so that when you stop a character on a dime, the cloth doesn't overshoot and go super fast, it just settles back down.
And this worked really well for us so that when a player was flicking the stick back and forth, the cloth not only behaved realistically but didn't go through all our colliders and make us all cry for all of our hard work getting destroyed by players once again.
So and then we use mass to weigh things down, but a caveat to this is it needs to be painted.
Having the mass attribute is a, you know, changing that static attribute doesn't do anything.
Mass needs context, so you need to have that painted on the mesh itself.
And then soft gradients rock.
So I just, most of my skinning and most of my, or rather most of my attributes I painted, I usually just painted 50% solid, 50% filled and not filled.
And then I would just smooth between the two without selecting the top loops.
And that literally got me 99% of the behavior.
So I just use soft gradients all over the place.
And here's some more stuff specific to the constraints themselves.
So Havoc has a series of constraints.
It's kind of overwhelming sometimes.
So here's some good ones.
The standard and stretch link constraints were always in our setup.
Standard link basically made the cloth, you know, like, constrain the particles to each other.
And the stretch link made it so that it wouldn't stretch out super, super long.
We used bend link and bend stiffness to add stiffness, usually in slightly different ways.
Volume constraint tried to take the cloth back to how it was in bind, but it was overall expensive and it didn't really get us the results we liked, so we tended to avoid it.
Bone particle angles and bone global plane could be used to constrain a sim within a predefined area, kind of a look a lot like distance.
But it was a nice alternative if you didn't want to mess around with your distance attribute because you'd got that working the way you'd liked.
And overall, these were the areas that are actually marked in red are the areas that we would tend to sim.
So we would avoid complex articulation areas, which were the thigh and the hips, the ankles, the shoulders especially, the neck.
We try to keep things in this area to avoid overcomplicating things.
And for our sim mesh, the instinct is to get your mesh and then create an average mesh, which is between the outer and inner shell of your mesh.
But actually, we found out that if you kept it on the inside of the mesh.
if there was any thickness, there wasn't any guesswork when it came to collidables, because if you use the middle of the mesh, then there would be some arbitrary distance between your cloth and your collision capsules that was challenging to account for.
So remove the guesswork and make the sim mesh on the inside so it's closest to your collision capsules.
Speaking of collision capsules, so there's custom collision capsules and there's standard capsules.
And I found that we got smoother results by layering up multiple collision capsules versus custom collision objects.
So even for an oddly shaped thing like a torso, I would use a series of multiple collision capsules instead of a custom shape.
Some very rough napkin math.
This is my own math, so it's probably totally wrong.
But.
it made it seem like it took up to six collision capsules to make the cost of one single collision, custom collision shape.
So I tended to stick with the capsules themselves.
And also, there was, I found that there was jittering or harsh transitions if I went from, if a sim mesh slid over a collision capsule and went to a custom collision mesh.
And so the rounder the capsules were also, the less likely they were to create big lumps or oddities in the sim itself.
Also, we talked a little bit earlier about kind of nailing down your sim mesh.
This is, there's differences between the high and low resolution meshes.
So, because they had a behavioral link, right?
The higher the resolution, the more salty the cloth.
The lower the resolution, the more stiff the cloth felt.
You have to be really careful with this one because there was some areas where we started with a mesh, did a lot of work on it, realized that we actually had to increase the resolution of the mesh because the collision capsules were...
kind of sliding through the mesh at certain points.
And when we did that, we had to rebalance pretty much everything because the behavior was completely changed.
So try and nail down your SimMesh resolution before you're doing other iterations on it.
Otherwise, you're causing yourself a ton of rework down the line later if you decide to change it.
So now that we've got some tips and tricks covered, what else can you do?
Well, if you're lucky enough like us at Insomniac, you might have an in-house engine team, and you might want to get yourself some extra features.
So out of the box, Havoc was able to get us a lot, but we still needed our extra engine support.
And luckily, this is where our awesome core team came in.
So the first thing they did was they removed cinematic cloth pop.
And you may have seen that in other games where you go into a cut scene and all the cloth was crazy and then settles for a second.
And this usually happens because a character is teleporting in space, and the cloth is accelerating at infinite miles per hour, and it finally stops, and it freaks out.
So we had our core engineers work tirelessly to remove that by basically pausing the cloth sim when we were transporting characters and giving them a couple of frames to settle before we went into a cinematic.
Also, as a side note, it's really great having an in-house engine team that you get to directly interface with and get feedbacks and questions to.
So I just want to say, or ask questions, so I just want to say thank you to our core team as well.
They also added on events for turning on and off cloth when we needed it.
So there was, as I said, there was points in the game where we needed to disable it, and they added events that supported that.
And as well as that, they added some real-time events for us to adjust gravity and dampening on the fly.
Because there was a couple of instances in the game where, no matter how we tried, we couldn't get a cloth setup to behave how we wanted in a single shot.
And rather than completely duplicate the setup and swap it out for a single shot, Core gave us the ability to have custom stuff that we could then all are at runtime.
So what were our findings?
It was real-time cloth was great for overall movement.
And detail could be added with blend shapes in normal maps.
It was good for, oh sorry, it was good to give animator control.
So despite my want for not burdening animators with any kind of animator cloth controls, it still came in handy, and we really still needed it in quite a few circumstances, because when we needed it, we really, really needed it.
It requires a lot of iterations, so be prepared to spend a lot of time on polish.
And some behavior changes have knock-on effects, so when you think you're just going to adjust a small thing, like the radius, or, say, the dampening of a setup, it can completely throw all your other finely-tuned attributes out of whack.
So be cautious of that.
And how could we improve going forward?
So we'd love to be able to have more real-time iteration.
So instead of outputting the cloth stuff to a data file and then loading that in, being able to adjust cloth real-time on the fly would be amazing because, you know, it would give us more instant debugging and feedback, and we could alter properties at will to see what kind of effects that we're having on the behavior.
It'd be nice to have better debug display and feedback.
This was a big problem for us, as covered earlier, and being able to support that with more stuff rendered in-engine on top of our characters would be really useful going forward.
And then also swappable cloth profiles.
So we usually had one cloth profile per character, and if we had to change that for a character, we had to duplicate the setup or the entire asset itself.
And the ability to swap out cloth profiles on the fly would be awesome, because if we really like to set up on a particular scene, we could save that out and then know that it's always going to behave on that particular scene, and then iterate on other scenes without damaging the work we did on the previous scene.
So in conclusion, did we deliver on a promise?
I believe we did.
We made some amazing, realistic, and believable cloth.
But not only that, but it was abundant.
It was everywhere.
Cloth was on our enemies, it was in the characters you talked to, the NPCs, and our vast cast of main characters as well.
Across the glance at our depot, we had around 70 different assets with active real-time cloth, and that doesn't include all the separate setups inside the models themselves, as well as the fact that, for example, our inner demon thug faction was one model entirely, so having that times 70 was pretty crazy.
And we did this with our small and versatile rigging team.
There was no one on the team that was solely dedicated to cloth.
We all did our part and made a little bit for the game.
And so while we were at it, we improved our cloth pipeline and tools a lot.
So our engine team worked with us to give us the best results, whether it was removing cloth pops or better and quicker reloads.
From start to end, the project, the pipeline was transformed.
I also helped streamline the content side, as I removed as much room for error as I could, and made our content pipeline much more straightforward.
And perhaps, finally, this is subjective, but I believe we made some of the best real-time cloth ever developed, especially on such a large-scale open-world game.
Well, we pushed the bar in the tech for real-time cloth, and for that, I'm proud of us and the entire team at Insomniac.
And that is me.
Speaking of, we are hiring if you wish to join us.
I am ready to take questions, and I apologize for the mishaps in the presentation.
You can reach me also at Twitter.
My DM is waffles.
Sorry, I meant to say my handle is waffles and DMs are always open, but you know, whatever.
Do we have time for questions?
I have a question for you.
Yep, sure.
Fantastic presentation.
Great job, Sophie.
I'm on my second play through of your game.
Absolutely love it.
Thank you.
Seems like you put a lot of work into working around weaknesses and deficiencies in Havok.
Yes.
And I'm curious if you could go back and choose, seems like that choice was made before you started.
Yes.
Would you choose to use Havok again or on your next project?
Would you use Havok again or try something else?
I mean, we're always open to looking at improving our pipeline, but we have invested a lot in making Havoc work for us a lot better, so it also makes sense to keep using it.
There also isn't a lot of other alternatives out there, and to develop your own cloth in-house, which I know a lot of other studios do, is a really, really big undertaking.
So that's something we'd have to evaluate at a company-wide level, whether or not it's worth pursuing that.
But for us, I mean, our process has massively changed since the beginning of the project to the end, and I wouldn't want that work to be wasted at the same time.
But thank you, it was a good question.
I have a question.
Great job, that was pretty amazing.
And I was just curious about the, so the pipeline from the sim clock to the joints, do you have like an average of how many joints we're talking about?
It really depended on the setup, but like, I don't know, I'm trying to think about it here.
Our lab coats, because they were simulated from the kind of chest level down, I think we had like...
maybe like eight chains of eight joints.
So, yeah, it was that when we added cloth to a model, it was no like small thing.
Like there had to be a lot of resolution on the cloth and the cloth joints to get the kind of results we needed.
I don't think we, there was a couple of simulations where we kind of went like.
Over that with, I think, Rio, she wears like her entire top is simmed and I think in that one we have a vertex, vertex, sorry, per Joint pervertex on the SIM mesh, yeah.
Because it was a full body thing, we wanted it to be as accurate as possible.
But yeah, for most of it, it was a series of chains of joints and it really depended on the complexity of the setup, how many joints we put in there.
But it was always quite a lot.
It was always more than the body rig itself.
Yeah.
Thank you.
Hey there, amazing talk, thank you very much.
Quick question, could you maybe please elaborate a bit on how you actually managed to eliminate the problem of cloth popping whenever a character is teleported or a cutscene starts or whatever?
Oh, so that would be much more of a question for engineers.
But as far as I was aware, they were, basically pausing the sim, telling the sim to stop evaluating before they knew a character was going to teleport.
They'd teleport the character so that the sim would be unaware that the character had moved, and then they'd always give it a couple of seconds to settle, a couple of frames, sorry, before the cinematic to let the sim settle into place in its new pose.
Because usually not only was it a teleport, but it was also a pose change.
That being said, we made sure that a lot of our cloth, when we were doing dramatic teleports on our characters, animation had to match the pose from between the moves.
Because if they match the pose, then it was much less likely to cause any kind of popping issues as well.
So it was a little bit of content changes we had to do, and also changes on the engine to make sure that it was smooth.
Thank you.
Hey, hello.
Hi, there.
I'm here.
Thank you for your talk.
I was curious if you used the same techniques for environment art, like flags or maybe curtains.
And did you have any kind of interaction with other rigid bodies?
So as far as I'm aware, all of our flags and things in the background were all vertex animation.
So they were all pre-simmed and then cached out into Olympic caches or something similar like that.
And then they also, because of that, didn't have any kind of real-time physics running on them, so they didn't have any kind of need for reacting with other collidables or other interactive objects.
They were usually out of the way where the player couldn't get at them anyways.
Okay, thank you.
Thank you.
Hi, I was wondering, given your current setup and how you're hoping to move forward, how realistic would it be to add removing cloth or putting on a shirt?
Sorry, the question was?
Do you think you'll soon be able to put on one of those, like the suits, or remove it, or have a character interact with it?
Oh, you mean like a disrobing scene or something like that?
Ah!
No, no, I mean, so we had instances like that with, with specifically a couple of points, Peter pulls off his mask in the game, and those were often a mix of Sims and Maya, and also animator, animators animating, to have like cloth removal scenes, like.
Especially real time, like I feel like that would be such a challenge that it wouldn't make sense to do it in real time You'd probably want it because usually it's for a cinematic instance So you'd probably want to you know do that with big simulations because you have a lot more control over the situation At that time and there's a lot more solutions out there for that Thank you. Thank you Quite often when you simulate cloth, especially in real time, you have this jiggly motion from just not enough iterations or lower resolution.
And the usual way to solve it is apply drug.
drag and air resistance.
And I noticed that in your videos, there is a fair amount of drag.
I'm wondering if you hit the point when the artists wanted you to remove the drag, but at the same time, keep cloth feeling dense so that it doesn't fall like this, but rather fall like this.
Yes, yes.
And if so, then what were your solutions?
Well, so first of all, we were the artists in this case.
We were the ones authoring the cloth and usually providing feedback on the cloth.
I would say that we did sacrifice behavior a lot of the time for the fact that we didn't want it to clip.
And because of that, it meant that, like a lot of our cloth setups, could be quite restricted.
And generally, I settled on getting behavior that meant that the cloth was much more.
stable than trying to tune it for a specific type of behavior.
So in the instance of drag, a lot of time I don't know if I'd pay as much attention to that as I was just making sure that it didn't go through the legs or whatever.
Because especially if your character's wearing black pants and you've got a white lab coat and it goes straight through, for me that is instantly much more noticeable and takes you out of the experience more than it not behaving one-to-one as it should.
So in instances where we didn't have to worry about collision, I'm sure we would play with the behavior of the cloth a lot more, but definitely we were accounting for not breaking the realism of clipping stuff over anything else.
Of course.
Thank you.
Yep.
Cheers.
Thanks for a great presentation.
I'm wondering how much did you have to use events in the cut scenes to control the cloth?
Not that much.
So a lot of that, like, my approach to doing the cloth was make sure that the base is as good as I get it.
And we, like, those things can quickly scale out of control.
Like, if you altered the behavior in a scene, then if you made changes to the base cloth, it wasn't additive or wouldn't take that into account.
So it was kind of like a permanent change to the cloth.
So we always tried to solve the solution, solve the clock at the root, rather than use the events that would affect it in runtime.
And this is mainly just because, yeah, like that stuff gets way out of hand way fast, and I'd rather have a solid core solution than be patching things all over the place.
Yeah, and if you were teleporting a character like within a cutscene, were that, did your system solve that as well?
Did you have to specify like now the character is teleporting?
We had flags.
We had flags that we would set on characters to say this character is teleporting and please apply this kind of behavior on them.
So yeah.
Okay, cool, thanks.
Cheers.
Hi.
You said that you were missing in-game debug for the code.
Yep.
In-game visualization.
Was it hard to do for the core team?
I don't know if how has the possibility or the API for doing some things like that.
So the question was, we lacked debug information.
And oh, sorry.
Were you going to say something else?
Oh, so we lacked information, debug information, and the engine.
And was this hard to add for core?
And it was less of this would have been hard to add for core, and that our core team is amazing, but they are always planned way ahead in advance.
Pitching for their time, you have to have a pretty strong case.
And we were able to work around the constraints we were given.
Given that, however, now that this project is done, our core team is taking time into adding debug information into our engine.
And they are trying to give us those features now.
But it was one of those things that came online as we were making the game.
And we didn't.
We, our core team just couldn't give the time at the time.
There was much more pressing issues to solve.
And we had workarounds for it.
Okay, so it wasn't a limitation in Havoc, but it was a time issue.
No, but it was something that our core team would definitely have to build themselves.
Like, Havoc has API hooks to let you do that.
It just, they need to write it.
And we just didn't have the time scoped for that.
Thank you.
Cheers.
Hi Sophie, fantastic presentation, I'm sorry to keep you.
Very quick one, would you worry too much about a character, say for example if you had a character with a cape and it were to fall off a ledge, would you worry too much about that cape clipping into the platform that's just fallen off, for example?
So, the question was, if we had a character with a cape, were we worried about the cape clipping into a ledge?
Well, most of our characters didn't have cloth that long, or at least playable characters.
So, for example, the only one I think that had cloth that long was maybe Sable.
And Sable was always in cinematics, at least in the base game.
So.
we just had to make sure the cinematic was planned around that and that we accounted for that as well.
If, for example, she was going to jump over a ledge and the cloth was going to catch, we would make sure that actually happens by adding in some sort of collider object.
But, yeah, it would be nice to get also collision with the environment as well, but for our purposes, that was needed so rarely that we were able to animate it with colliders and the files themselves.
Thank you very much.
Cheers, thank you.
So how's we good?
Time out.
All right, we're done.
Thank you.
