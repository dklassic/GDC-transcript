Alright, cool. So hello, thanks for coming out to the TA bootcamp, hanging out for my talk, which is Honey, Where's My Wingsuit? The Building the Just Cause 3 Animation and Rigging Pipeline.
My name is Brian Vinesky. I'm the technical animator at Avalanche Studios, located in New York City, where for the past three and a half years I've been working on Just Cause 3.
Now I'm going to do my best within the next hour to cover a general outline of some major points of note.
as well as some specifics that are hopefully useful and interesting for you all to see.
Just a bit about me and Avalanche to start.
I myself am originally from the Boston area and I went to college in Boston at Northeastern University.
The program at the time was pretty broad, but I found myself really enjoying the rigging aspect of animation.
I dabbled by watching digital tutors, studying rigs I found on the internet, and dabbling a bit in mal-scripting.
My first job out of college.
was working on 3D visualizations for a combat simulator.
And that actually ran off an open source engine, so I got used to game-like content pipeline pretty early on.
About a year out of college, I was hired at 38 Studios, located near my home in Massachusetts and later Rhode Island.
My official game dev career began as a tech artist there.
At 38 Studios, I learned all about automation and pipelines.
while getting smarter and more knowledgeable with things like Python scripting and more advanced rigging techniques in the process. I was at 38 Studios until its very end, where I then found my way into Avalanche. It was at Avalanche where I got to take all of the power and knowledge that I gained at 38 Studios and use that to build the pipeline that I'm about to present to you today.
For those of you that may not know, Avalanche Studios has been around since 2003, where the first Just Cause was brought to life in Stockholm, Sweden.
In late 2011, Avalanche expanded into the U.S. and opened up the studio where I'm located at, in the Soho neighborhood of New York City.
New York City is the location where Just Cause 3 was created, and is the first and currently only game in the Avalanche catalog to actually be predominantly created outside of Sweden.
Not only do we have the Just Cause series, but we have the recently released Mad Max game.
the Hunter and Hunter Primal, Renegade Ops, and a recent jump into mobile with Rumble City.
A few quick questions, who here has actually played Just Cause 3? Quite a few. Who here is an animator? How about tech artist or rigger working with animation teams? Awesome. So some of you might know a little bit about what I'm about to talk about, some of you might be completely new with it.
But either way, I hope that I put something together that everyone can learn something from or at least that's interesting So for those of you that may not know what just cause three is it's a massive open-world sandbox There are tons of random characters from civilians enemies allied forces and some animals including cows Which for some reason people love cows?
Now of course this talk Says that it's going to be about animation and rigging pipelines But taking it past that What is this talk really going to be about?
What challenges do we have?
What solutions did we come up to solve the issues?
To break it down a little bit, there was a transition of animation software that I'm going to talk about.
The transition of animation software was that we had MotionBuilder as an initial animation software and we wanted to move that into Maya.
Some pipeline building we'll go through.
includes character setup, creation of rigs and solutions, FBX data handling and motion capture, dialogue animation creation, and a quick post-mortem at the end.
So, to plan the pipeline, we have to figure out what we have and why we have it.
Well, we have Maya.
We use that for character model, skeleton, ragdoll, and skinning weight export.
That would get streamed to MotionBuilder, where a rig was created, animations are created, and all of the animations could be exported out to the game.
So why do we have this pipeline?
Now, the team in Stockholm was actually a completely MotionBuilder animation team.
Any tools, exporter rigs, etc.
had MotionBuilder support and no more.
Both the tech animator and animation team in Sweden had been using MotionBuilder for years, dating back to, I believe, the first Just Cause, definitely the second one, and Mad Max used it.
Just Cause 3 started with the same pipeline as Mad Max, but the team in New York wanted to move to Maya, as that was both the preference and expertise of most of the team.
Now, Just Cause 3 is also a game that's about crazy, over-the-top style.
Our main character Rico is a sports car riding, wingsuit flying, parachute deploying, explosion making son of a gun.
We need a hand key pipeline that Maya provides to create much of this new content.
We were planning to have such as the wingsuit.
Now what do we want and why do we want it?
We wanted to use Maya as our main animation package for its benefits with hand key animation.
But that includes the graph editor and tangents, oiler filters.
and easy to find scripts and tools on the internet such as comet tools which is an oldie but a goodie and a lot of other things you can find on a site like creative crash. It's also a little hard to capture mocap on something like a cow or a goat or a parachute or wingsuit so we really wanted to be able to hand key the animations for those things and make them look really organic and great.
Now with this setup of using Maya for rigs and animating, we would also need to make sure we can get our Maya animation content from Maya to JustEdit, which is our editor.
However, we still wanted to retain the features that MotionBuilder does so well, such as story mode, a take system for comparing and blending multiple motion capture takes of the same animation together, and pretty much anything that will allow us to easily and quickly stitch and blend together all animations without having to create.
or need additional tools.
So the first phase of our pipeline was to set up an initial workflow that allowed animators to both work in Maya and still have and get their content over from MotionBuilder. So a simple question is what do we need in order to make this move?
Well we need a Maya rig.
All of our animations are on MotionBuilder native rig, which at the time was FBIK, since we were using Maya 2012 and MotionBuilder 2012 at the time.
FBIK stands for full-body IK, which is based off the human IK solver.
We wanted to make an equivalent rig in Maya that would work hand-in-hand.
We also needed to transfer animation from MotionBuilder to Maya.
MotionBuilder uses a take system.
which is pretty much a container that stores animation data for objects in a MotionBuilder scene.
Now I mentioned before that it has built-in properties that allow you to blend animations together and easily compare multiple versions of the same animation.
Say you have a run animation and you have five different versions of that run.
You could call the file run, but each take would be run version one, run version two.
Our MotionBuilder pipeline at the time was set up to be run with...
within the file, run base, run aim 90 left, run aim 90 right, if you're using additives.
The problem is, MotionBuilder takes aren't supported in Maya.
Now at the time I showed up at Avalanche, we had about 1,000 animations in MotionBuilder, all stored in takes.
Now as you can see here in this picture, we had about roughly 40 to 50 animations in this idle animation alone.
we needed to figure out a way to easily extract these one by one as separate files and get them into Maya.
We also have an in-house data format for animations, so we needed to get a custom exporting process working in Maya, working the same way that we have in MotionBuilder.
So to the Maya rig.
When making our first iteration of the Maya rig, there was an importance on time.
We really wanted to get into Maya as soon as possible and kind of leave MotionBuilder behind, at least for the moment.
We were gearing up for production, and we couldn't afford to waste much time trying to figure out new things in MotionBuilder, while at the same time trying to build new content in Maya.
So we used the existing skeleton that we already had, and since time was of the essence, we didn't really want to take the time to re-target and fix any content, which would be a huge...
headache at the time since we really wanted to get into Maya and any sort of skeleton updates would be done later. It also made sure that we could get one-to- one transfer of animation and not have to worry about any sort of retargeting.
We also built the rig with straightforward features and the basic needs for animating.
There weren't any bells and whistles, and it was really important that the animators also had something functional and usable to do what they needed at the time.
The animators also needed to get used to Maya again.
At the time in New York, MotionBuilder was being used for about eight to 10 months since the studio had opened, and other animators on the team had been using MotionBuilder for longer at previous jobs.
Using the same skeleton also allowed us to debug issues.
There are some issues on our rig that cause a little bit of undesired behavior in the game.
This Maya rig actually allowed me to take a look at some of the outstanding issues and able to debug them in Maya versus in MotionBuilder.
Such as, I don't know if it's easy to see on this picture, but the middle spine here is rotated 180 and Z.
One issue that we had was that the spine was in negative 180 joint orientation, which would make the top spine have to get countered in 180.
In MotionBuilder, you would have to retarget or go through every single new animation with the FBIK rig to actually fix any sort of skeleton that got screwed up like this.
Now in Maya, I was able to fix the skeleton between that and on the new rig, since we kept the same joint position, I could easily transfer the animation over and it would sort of fix itself on the web.
Now we need to go back to MotionBuilder takes in order to discuss our animation transfer process. Since Maya doesn't use them, we chose to organize our animations one per file and use folders and naming conventions for things like additive animations to organize them.
Now how do we get the takes exported as single files? While I was busy working on the Maya rig, Another tech artist at Avalanche at the time helped me by teaching himself some Python API within MotionBuilder also known as PyFBSDK He also helped me along the way to learn this But this allowed us to find all of our animations and takes and physically cycle through each one while batch exporting them all as single files These animation files contained only a skeleton and baked down keyframe animation on that skeleton The single files were also completely stripped and cleaned of any artifacts and rig information.
These data files could actually be exported straight from the game, or perhaps transferred to a proper Maya rig.
And here's the main function that we use to actually transfer this animation over in Maya from that FBX export from MotionBuilder.
What the script does exactly is allow the animator in Maya to choose an FBX.
It adds the rig to a fresh Maya file with the FBX animation.
It constrains them together and it bakes that animation data down onto a Maya rig.
For good measure, it copied all of the key frame animation, opened a fresh new file, re-referenced the Maya rig and pasted all of this data back onto a Maya rig.
This was done just to ensure that we didn't have any unneeded artifacts or data that we didn't want inside the file.
Now just to note, we did later have a batch process to do all this, but of course, since time was of the essence, we didn't have that right now, and the animators would just need to import the MotionBuilder files on an as-needed basis.
Now I also mentioned the custom format at Avalanche for exporting and reading animation data.
Originally, that export process was written in C++ and run using a plugin that loaded it up in MotionBuilder.
that C++ was converted to Python and modularized so that there was a common core used to write our custom data structures and wasn't software dependent, allowing us to write our Maya exporter and wrap up our animation data into the proper format in Maya the same way the motion builder side of things were done.
So now that we have a Maya rig, the ability to move our current animation content onto this rig and a way to export our animation to our editor, it's time to build a proper pipeline.
So let's take a quick look at character setup, which will provide the base for our rigging.
Here we have a basic mesh and our NPC character skeleton.
Every character model we have has a file that looks pretty similar to this, which is just a bound mesh and its skin to the skeleton.
This is the file that we use to export our skinning data, model information, and skeleton to the game.
From this, the game knows our default bind post.
It's also the same setup that we build our rigs on.
Now each skeleton has an accompanying ragdoll, which we use Havok technology for.
The ragdoll itself has its own file with the skeleton referenced in.
Now not only do we use Havok as our ragdoll solution, we actually use it for our skeletal data and animation as well.
We had a few hundred unique characters that needed to be bound, skinned, and exported though.
So we had a need for a tool that allowed the character artists to take their created characters and quickly bind them with any resulting weights.
that were much better than the Maya default setting and at a level good enough to see and review those models in game.
But binding a mesh for export to just edit has a few strict rules.
One example in particular is that even if a joint has zero influence on a whole mesh, such as a reference joint, it still needs to be a known object to that model in game.
Now, most of you might be aware of Maya having a setting in Smoothbind called remove unused influences.
we needed to make sure that remove unused influences was unchecked.
But a setting like that could easily be passed over.
So this tool sort of did that exactly for you.
And that basic mesh that we just looked at was actually an envelope mesh and is used to reference into a character file.
have its weights copied over to the actual mesh, and then it would be unreferenced.
The result isn't quite 100% shippable skin weighting, but it got us to maybe 80% of the weight we needed to be.
It was good enough so that arms, legs, torso, and for the most part the rest of the body moved around as expected without any crazy deformations, and skinning polish wasn't nearly as bad as it could be.
Now the tool itself also had a vert weight export and import feature that was run using some Python API because of its speed benefits over commands or even mount.
This tool also has the ability to see if a mesh has over R4 joint influence maximum on any verts and allow you to auto prune that to 4 influences or even just check to see if anything has more than 4.
So you might say that it's just character skinning.
Why do we actually need some fancy tools to bind?
Well, this is why.
If proper settings weren't applied when binding, we ran the risk of running problems like skeleton and model mismatching.
It could lead to something that we dubbed in the studio as the mesh monster of doom.
All right, time to talk rigging.
We used an auto build system built in Python using Maya commands.
Now, auto build systems are pretty par for the course in many studios today.
But hopefully the look into the way that we did it could be a little different than some of you are used to.
So we didn't have a ton of unique rigs that a game such as an MMO or a MOBA with lots of characters, shapes, heights, what have you, would actually need.
And AutoBuilder was important to us due to the amount of iteration we planned to do and actually did.
And it also helped to iterate quickly due to a small rigging team size.
Now the bind post skin character models you just saw were that base that we built the rigs on, as I had said.
Each unique rig would get a build file, which is the blueprint that would tell the system how to build the rig.
This is where our character would be set into a rig pose or T-pose, add reference controls, set up secondary animation controls, call our build modules, save the rig out.
If it's not in this blueprint file, it's not going to be in the rig.
Now all rigs use the same initialize and finalize process.
which is where groups and nodes are created to organize everything into a single top group in the outliner and any housekeeping and clean up was done to neatly package a rig together.
Also modules were being called by the build files were the same for each character. Cows could use the same leg as a human for example. So I'm going to do my best to take a quick look at our build files for you. With all this code and what not going on.
But let's break it down to a few pieces to actually try to understand it better.
To start the process, we would initialize all our modules in classes that we'll be calling.
As you can see, we called the build class first up at the top, which would hold all of our base rig modules such as armor leg.
We then start the build by initializing and adding a meta group and master node.
The rest of the modules are then called, which are later used to create things like reference controls such as root position arrow, cameras used for visual reference in Maya which would include a follow cam, made to mimic the player camera that you would see in game, and deformers such as like arm twist helpers.
Now that deformer module is used to set up most of our secondary and helper controls used for corrective animation.
Setup secondary was the simplest form.
and we'd just create a simple control on something like Rico's harness straps in order to be hand keyed.
You could also add a driven node to the control if you wanted to or an expression.
Setup driven, which would create expressions and utility nodes such as multiply, divide and clamp could also be created here.
This allowed us to create something where we could have the arm rotate up and say multiply that by a certain value to get a translational value of a helper joint in the chest.
I had a setDrivenKeys command written up that was an easy way to create setDrivenKeys by stating the value of a particular attribute of a driver and giving you the values and type of animation curve you wanted.
These were used when we wanted more organic motion in our corrective joints and were able to use tangents on the in and out rather than using expressions straightforward.
Now to finish the process we would t-pose our rig after all the helpers and correctives were created and build the core base of our setup.
This included the torso, head, arm, and leg.
And to note, the arm and the leg were actually built on their own IK limb nodule, and just inherited that class from that.
Next, we would add hands, which for Rico included five fingers, and for NPCs included three fingers, which was actually the thumb index in these three fingers as one.
Feet would come next.
And here we give values for toe tip, heel, and sides of feet for foot bank.
And before finalizing the rig, we could save it out, add a face setup if we wanted to, which for cinematic rigs was just pure FK, and for in-game was about a 10 joint face with some drivens on it.
One cool process that we utilized was a controls.py file, which was used to actually build custom controls per rig.
The file was full of functions like this clavicle control, and allowed us to create unique curves, shapes, and colors based on a per rig basis.
Shapes could be created by hand, and a script would be used to convert them into a point-by-point form, which then we could copy and paste into this file.
Now you can see some of those unique curves between rigs here.
Doesn't matter if you're a human, animal, or wingsuit, or even a cinematic-specific rig, every rig gets a similar build file.
animals such as the cow, more similar to a human byped set up which get four legs instead of two arms and two legs. The wing suit doesn't use an arm or a head of course, but we still set it up, create utility nodes, set driven keys, so forth. And we finalize it in the same way. Same goes for the parachute. Now for anyone who's actually a Max user, I recommend checking out a talk from last year, I believe it was Richard Katz, who spoke a little bit about Max, a Max rigging pipeline that he had set up.
And here's a few essential tools that accompany our rigs.
I mean, these are pretty standard as well for the course.
Our IK-FK match was able to do a single frame timeline or specified range of arms, legs, and spine.
Whether we're going from an IK or whether going from FK to IK or vice versa, our parent space switching could do the same thing.
and would allow, in eBulletools would actually allow us to multi select controls and run the same process on up to as many controls as you wanted to at once.
We also had a weapon constraining tool that would allow for quick placement of weapons and props if you're a civilian into the left or right hands or holster positions.
You could also choose to have a weapon follow the hands in either FK or IK as well as have a weapon drive the IK arm itself.
Any setup could easily be deleted and recreated should you choose to switch it up.
Now going back to the rigs themselves, one really cool technique that we utilized in conjunction with our set-driven keys and expression nodes was joint riveting to follicles.
If you're unaware of what a follicle in Maya is, you can get it from creating a hair system and deleting the actual hair node.
You're left with a small little red crosshair-like object.
that has UV properties that allow you to slide it along the UVs of the mesh.
I'm not sure if you can actually see the little red crosshairs on that picture.
But you can use this follicle to rivet or constrain any object to that mesh.
Now on the right side, there's a bluish mesh that is the duplicate of Rico's jacket.
And Rico being our main character in the game of Just Cause 3.
This driver mesh, or this mesh rather, is skinned.
just to the base skeletal joints of the character, which is spine, shoulders, and arm.
These follicles were set up to, and then follicles were set up to coincide with the joints set up for the harness of Rico, which you can see on the left side.
Now this driver mesh on the right, thanks to being skinned, moves with Rico along with it the follicles, and in turn drive controls and joints that we have set up with the harness.
This was set up in a way that the harness controls would not only move with Rico's jacket, but also have the ability for extra driven keys or expressions on top of that, which helped create some nice subtle offset movement and fix problems like mesh clipping.
On top of it all, you could still tweak animation by hand on the control itself.
So as you can see, lifting the arms up, the harness will move with the body.
Same goes if you twist the spine left-right or back-and-forth.
Also taking this riveting one step further, we used it to simulate a cloth-like look on the wingsuit.
Now the wingsuit mesh was duplicated as well, follicles applied per joint.
And here we have the duplicated mesh in wireframe form with the follicles attached to it.
Now for the wingsuit, we took a sine wave deformer and applied this to that driver mesh, which could be keyframed with settings like amplitude and offset.
to create some really nice secondary motion to simulate some wind movement on the wingsuit.
Add in some driven keys created from extreme poses made by an animator and with just a few animation controls we get some great movement on the wingsuit.
And all of these cloth-like movements was just me playing around with keyframes for like maybe four or five minutes.
Just remember this is all simple joint based animation on a skin mesh.
There's actually no sim going on.
And if you think about the tools like sine deformer or even go further like bend, wave deformers or using end cloth to drive a joint, a mesh, you can get some pretty awesome organic looking results with this technique.
Well now that we can build our rigs, we have some, we need to, and have some, sorry.
Now that we can build our rigs, we need some essential tools to go with them.
It's time to tie MotionBuilder back into this pipeline and get to that full circle that we talked about at the beginning of the talk.
This was mainly done with the power of this tool.
On top, there's the import FBX to Maya rig.
On the bottom, there's export animation for MotionBuilder.
You simply choose your target.
and the Maya animation or FBX you want to import or export and you run it.
To set up our rigs for the motion builder side, we could build them in Maya thanks to its integration of the human IK plugin.
Any human or biped skeleton was characterized before their respective rigs were built, done thanks to the function here on the right.
We ended up building not only our Maya rig after characterization, but we built an HIK rig as well.
When transferring animation, the HIK rig here on the left would be imported into a fresh Maya file.
Then the Maya animation on the Maya rig you see on the right would be referenced into the same file.
When importing to Maya, the result is an FBX animation transferred and baked down to our Maya rig.
In MotionBuilder, you have your Maya animation baked down to a human IK rig.
Now earlier I mentioned that the initial rig the animation team used...
was an FBIK which was based off the human IK solver. So using human IK or HIK as I'll refer to it as wasn't any different for the team. And for reference, anyone who may not be familiar with the human IK solver or rig solution, HIK is a system that includes manipulation controls with the ability to pin them as if you're using an IK control in Maya set to the world. It is both a full body and body part keying solution.
So here we have a short run cycle on Rico.
And we run that script, it would bring that HIK rig in, and we would get that animation into MotionBuilder.
With...
And that's just the same exact animation, mesh hidden, so it's easier to see the HIK rig for you.
So now that the animation is baked down onto our HIK rig...
The Maya rig would be unreferenced and we can open that up in MotionBuilder as you just saw.
Once the animation is in MotionBuilder, the animators can then edit this animation up, compare takes, bring it into story mode, stitch things together, do whatever they needed to that MotionBuilder provides.
All that we need to do is save this file out, and then use that tool in Maya to import it back.
Now we have a way to wrangle our FBX files and motion capture.
using both Maya and MotionBuilder.
Here's a quick look at our motion capture solutions.
We did use your typical studio capture for both cinematics and in-game animations.
We also had a really great in-house workflow using Xsense technology.
And here's just a quick picture of me putting the suit on just a few weeks ago.
For anyone not familiar with Xsens tech, they're gyroscopic motion capture suits that you can use at home, in studio, or pretty much anywhere people won't find it weird that a person in a skin tight spandex suit is moving around like a crazy person.
Now real estate in New York is not the easiest to come by, so having something that we could use right in the studio was pretty necessary for us.
Xsens proved beneficial in the fact that it allowed us to work really quickly, and we can capture multiple takes and get pickup animations all in a, Pretty quick moments notice.
Now all of our motion capture whether it was studio created or Xsens created was easily brought into MotionBuilder for stitching and editing and then brought straight into Maya for some hand-keyed love.
Now HumanIK not only allows us to bring our animations from Maya to MotionBuilder for editing and back to Maya but we reap the benefits of its retargeting properties as well.
Our Ricoh skeleton had a different proportion and size from our NPC skeleton.
So if an animator wanted to take a Ricoh animation and transfer it to an enemy or rebel, which is what you see here, or a civilian, the FBX Import-Export tool could be used to do so.
As you can see, this is an idle on Ricoh, and the same idle on the rebel.
Rebel's a little shorter, but retargeting over wasn't too much of a problem.
Now all you need to do is actually choose the target character that you want the source animation to be applied to.
And then the reach and pull settings on the HIK rig would be set for the arms, legs and torso and kind of lock them into place.
Now here are those HIK reach and pull settings if you're unfamiliar with HIK.
There's a HIK properties node that every HIK rig has, and we'll help the HIK work the magic of retargeting.
Now with this in mind, let's go back to our initial Ricoh skeleton that we had during the transfer of animation from MotionBuilder.
Now I mentioned we would add the bells and whistles later, and here's the bells and whistles.
While the animators were busy working happily in Maya, let's call it our transition to Maya phase, or our transition to Maya rig, rather.
our new Ricoh rig was built on a completely remade skeleton.
This skeleton was a little taller, included slightly different proportions than the existing one, which was necessary and made for better deformation than we had.
And overall look of animation was better as well.
We were able to utilize our nice new retargeter, thanks to HIK, which allowed this final switch from the old skeleton to the new skeleton.
All we had to do was run a batch of this retarget, of the FBX import export tool, onto a rig in a new file and rename it out.
Now having this dual Maya and MotionBuilder workflow was a huge win for us and gave us that original goal of being able to utilize both motion capture and traditional hand-keyed animation in a pretty painless way.
Now, we also had an in-game dialogue system.
that we wanted to set up with an automatic lip-sync process to drive animation.
Now we actually achieved this using MotionBuilder's built-in voice device.
Now if you're unfamiliar with it, it is similar to face effect software and allows you to create automatically generated animation based off of sounds given off from audio files.
Now here we have a face GUI that is driving our NPC skeleton.
And this is showing frame poses, one frame poses that we had created and used as additive animations.
This included things like mouth open closed, mouth narrow wide.
Once the poses were figured out and set to sliders, we got a typical face rig suitable for animating.
The face setup was pretty simple for NPCs. About ten joints, and even Rico had the same in-game face setup.
It made pretty quick to iterate and work out the way sliders drove the face.
as well as stay within memory budget when you start populating the screen with dozens and dozens of these characters.
A character face is used to set up phoneme shapes.
If you need a quick refresher, here are some basic shapes.
Phonemes are basically the sounds we make when we speak the distinguished words from each other.
And of course our models move and make different shapes depending on the sounds that we're making.
Now, CharacterFace is used to set up phoneme shapes.
CharacterFace is actually a node within VoiceDevice in MotionBuilder that you use to drive the face.
Using CharacterFace, we would set up cluster groups in order to tell it what controls and nodes that we wanted to create the phoneme shapes with.
So for this, each slider control was its own cluster group.
Now, after some iteration, I decided on a particular set of phoneme shapes that seemed to work really well and tweaked them in our slider controls.
Now on the right we have the ah and fv sounds which are made using a combination of a few sliders so there's different percentages of our 0 to 100 percent.
A wave file is imported to MotionBuilder and linked to the voice device. This voice device picks up the phonemes from the wave, does its best at least, and plays them as we specified in the character face. The result is such.
Rico Rodriguez, it's really you! I have your old race car tattooed on my back! Welcome home!
Now, lucky for me, this technique was sort of piggybacked off of Mad Max. The tech artist there sort of helped me learn his ways of doing this, and it meant that we already had a batch export solution waiting for us to use to get all this animation out, and it had the tech that would allow us to drive this in-game. I won't go into detail about specifics.
but I was able to sort of hack and slash my way through the Python code that they used to make it work for our project as well. Essentially the batch process for this was a C++ plugin that these Python scripts would call. It proved pretty effective and we could get thousands of lines of dialogue batched overnight. In the end we had around 13,000 lines of just English dialogue.
Now the magic of these animation files themselves, there was In-game there was actually no animation data involved at all.
Remember those face poses that we looked at?
They were used in conjunction with float data.
The rig sliders that drove the face were hooked up to specific float tracks.
These float tracks were from 0 to 1, and they corresponded to the one-frame face poses.
The game would read this compiled animation, which at that point was just a bunch of frame-by-frame data from 0 to 1.
and it would stream a corresponding animation file whenever the wave is triggered, both named the same thing, and blends together at runtime using these poses.
The great thing about all this that we're using, the great thing is that because we're using all of these poses to drive through data, in order to create Rico, all we had to do is create new poses for Rico, and the same exact animation files could be played on him as well.
We didn't have to make another new template just for him.
So, quick post-mortem.
What didn't go so well for us?
I do actually feel like we had a lot more wins during this process than we did losses, and great things to come out of the process as well.
So, I'm only going to bring up a few things here before we get into some of the things that really went well.
Time.
Okay, yes.
Lack of time is always an issue in game development, but there are so many things that I would have liked to have done.
UI beautification was the first thing. The tools I made were built using pie side as a UI. And it's all default pie side buttons, check boxes, et cetera. I would have loved to have given the chance to give them their own specific avalanche flavor.
I wish I could have used a lot more JSON and metadata to accomplish some tasks as well. At the time I started building the pipeline, I wasn't super well versed on things like JSON and I sort of learned them as I went on.
and looking back I wish I knew more about them before the process started. If you don't really know much about JSON or using metadata to drive animations or within rigs, definitely look up those two topics. And obviously more rig features. By the end, we definitely had a solid rig that animators could use to create everything they needed to, but there are also a handful of things that we wish we could have gotten done and added to the rig within that time.
But on a positive note, these will all be good learnings for the future. Another thing that didn't go so well was the aftermath of retargeting RICO. HumanIK did give us an awesome way to create our Maya motion builder dual workflow and allow us to retarget. But when we went through the actual process of retargeting RICO, it proved to be a nightmare at some points.
We did manage to completely remake RICO skeleton. And it was necessary.
but it did add a lot more time to actually having to get these animations into game without bugs.
The animators did spend a lot more time fixing poses for animations, and I also had to go through and, after a full batch of these animations, chase down animations that weren't checked in to Perforce, and were kind of living locally on other animators' computers.
So in the end, everything kind of worked out, but chasing these things down, fixing these things up, took a lot of time.
So if you can avoid having to actually retarget animations onto a new skeleton, I would completely tell you not to ever try it or think that it's gonna take less time than you want it to.
So what went well, though?
Well.
We accomplished our main goals.
We didn't really shoot for the moon.
We made our major scripts and tools that were created, and we made them as needed.
So there were a lot more tools that I didn't get to show you just due to time, but we had a full range tool set that really helped us get our work done quickly.
And all of these tools were just created as we needed them.
We didn't kind of make a tool hoping that it would be used by an animator or somebody else.
We made it when somebody said they had a need for it.
working closely with the animation team.
It was really important for me to sit down with the animators and actually see their issues or talk out a new rig feature.
I wanted to be hands-on as much as possible.
I didn't just want to take requests, sit in my corner, and figure out the best solutions on my own.
Our animation team was always looking to learn how and why tools and rigs work the way they did.
And it was really fun working with a group that was so ambitious and willing to do more technical-related things.
The knowledge share with the Stockholm team, most importantly the tech animator on their project was awesome. Not only was he on Mad Max, but prior to that he had worked on Just Cause 2 as their tech animator. There was some really useful tech, such as the dialogue animation system that you saw, and other things that he always was happy to teach me or answer my questions on. And even though we had that 6 to 7 hour time difference at times, he was always there to answer my emails within the...
the day or even get back to me right away if it was at night time for him. Now Maya went really well. We were able to use found tools and we actually had an animator who owned our third party tool set deployment that was separate from all our other in house tools. This way animators could gather, find new tools and I wouldn't have to worry about taking the time to add them to our tool set.
We utilized a lot of outside tools, but one tool of note was actually Studio Library.
It's made by the creator of Advanced Skeleton, and it actually allowed us to have a pose and animation library that was super robust and very production-ready for us, at least, and didn't make me have to take the time to actually build a full-featured pose and animation library.
Now, also managed by the animator were Animation Team Scripts.
The animators actually took it upon themselves to create their own MEL scripts for processes that weren't too involved since I was busy working on sort of bigger picture things. It was a huge time savings and I was pretty impressed by the scripts that they actually came up with. It was super helpful and super awesome that they were really that ambitious. And just a note before I conclude, come say hi, we're recruiting this week and hiring actually. We have a number of positions open in both New York City and Stockholm.
We need lots of programmers. We have a few tech artist positions open even. And I should be there midday between Wednesday and Friday if you specifically just want to come say hi to me.
Oh, and rather I should go back to that slide because we are at booth CC2124. Now quick thank yous. Thank you to Fred Hooper for letting me use his slide templates. I completely stole them off of him. Check out his talk on Thursday.
He is speaking at 1130 in West Hall 2020. It's all about the VFX in JC3. So all those sweet explosions and a lot more will be covered by him. Also Emil Persson, a programmer in our Stockholm office. Oops. Emil Persson, a programmer in the Stockholm office is speaking tomorrow at 3.30. If anybody here is interested in DX12 stuff, that talk is also here in West Hall at room 3007.
I just want to thank the animation team and fellow TAs at Avalanche.
Again, they were super awesome and I can't really say that enough.
Thanks to you guys for coming out to the boot camp.
And just to remind anybody who plays Just Cause 3, Sky Fortress DLC came out today.
So you should probably go get that when you're home at GDC, play some Sky Fortress.
It includes a Bavarian jetpack on the wingsuit, so you can jetpack yourself around plus wingsuit.
I guess the coffee break is coming up now. So feel free to approach me if you want to ask any questions that you don't want to necessarily ask here. There's so much more I wish I could have like talked to you guys about that I was constrained for time. So if you have anything you really want to know in terms of my pipeline or what we had at Avalanche, please approach me.
I'm happy to talk. So yeah. Time for Q&A if anybody has questions.
I had a lot of helper joints on that skeleton. Did you bake all that animation out and they all have their own animations in the game engine or did your game engine had equivalent helper joints running in real time?
The question was baked down helper joints. Do we have that in-game or was it all baked down and exported out?
Yeah, it was actually baked down and exported out. We didn't have any runtime features in our engine Okay, and then are you you're blending all those animations together did all the helper joints blends?
Okay, or did you have any issues with that? Yes for the most part everything did blend. Okay There was some issue with Going into save like ragdoll. Yeah It actually it wasn't awful you know, it still came out okay, but, yeah, it wasn't able to sort of twist the arm joints, right, if you went into ragdoll. So yeah, that was one issue that we did have.
Thanks a lot, Matt.
Yep.
You said there were more features you would have liked to implement into your rigs.
Could you give a couple examples?
One in particular that we had talked about to help sort of creating additive animations that I would love to implement in the future if we get the chance to.
was actually our animator, one of our animators kind of built a set up for me and I had looked at it with him, would be to have a master control that you could say create an animation where you would point uh say two revolvers ahead and be able to just rotate one control and sort of bring the entire body up with like a nice uh sort of fall off um very similar to I guess the way like a motion builder type rig would work um.
but just more features like that that would allow animators to instead of sort of have to rotate the spine back and bring the arms up just one big master control that would allow them to sort of twist the body and bring it up to create sort of ninety degrees up or ninety degree left or right sort of additives.
Thank you.
You mentioned Studio Library as one of the Maya tools that you guys found really helpful.
Are there a few off the top of your head that you think were super awesome to have around?
as far as Maya tools and plugins?
Um, the question is, I mentioned Studio Library, are there any other tools that I would recommend?
Check out Red9. Red9 is really impressive. It was created by a guy named Mark Jackson.
There's a bunch of animation tools in there. I'm actually looking into using it for metadata right now.
Because it's super robust and there's actually even an animation binder in there.
If you look up Mark Jackson master class, he actually goes through retargeting using that.
So that's another retargeting option as well.
But Red9 is certainly one of the tools I'm looking into right now.
Thank you.
Hi.
Hi.
Just curious what your experience was using Havoc.
I don't know how much knowledge the team had, especially tech artists.
Basically, we're using it now and it's a lot of emailing support and figuring it out by looking at demo projects.
Yeah, the question was what our experience was using Havok.
I myself didn't use Havok a ton.
I mean, I used it to create ragdolls, export them, export skeletons out.
But we actually had a really close relationship with Havok and we used it for all of our destruction as well.
So, we kept a pretty close-knit relationship with them and I'm pretty sure that any of the guys working on the destruction stuff had a really good, like, back and forth with them.
Oh yeah.
Yeah.
Cool, thanks.
Mm-hmm.
Alright, thanks guys.
