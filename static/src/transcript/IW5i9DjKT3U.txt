Hello.
I'm Kevin Dill. I'm a senior solutions engineer at Kythera AI.
And before we get going, just a quick shameless plug. So I've been at Kythera for about a year and a half now, and it's really been a dream job for me because a lot of my career has been about trying to find reusable architectures, build reusable tools, and then carry those with me from project to project to project. And obviously as a middleware company, that's what Kythera is all about.
So we have a really solid sort of end-to-end AI solution.
If you're looking for some tools so you don't have to build them yourself, if you're looking for some help building the AI for your game, either way, I definitely encourage you to reach out and have a talk with us.
That's the kind of thing we can help with.
My email is here.
There'll be more contact information at the end of the talk.
But enough about that, because this is not a talk about Kythera.
This is a talk about testing.
So this is me.
Here I am in my office away from the office.
I've been an AI developer for over 20 years at this point in games and simulations.
I would say for probably the last 15 years, I've been what I would call really unit test curious, right?
by the idea of unit testing, excited by the promises that it seems to bring, but time and time again I would try and get tests going in a project and I would find I was spending far more time fighting with the tests, rewriting them, maintaining them, things like that, than on any benefit that I was possibly getting back from them.
Then about probably six or seven years ago, oh, sorry, but unit testing just didn't seem to work well for games, right?
And that's something that I've heard a lot of people say in one way or another.
And then about six or seven years ago, it finally clicked, right?
It finally just worked.
So I had built my own test framework for a project I was working on.
And since then, I've been consistently building tests for most of the work that I do.
initially on my own because that was a project where I was working by myself.
And now at Kythera, we already had a test framework before I got here and a bunch of tests for it. And I've been working with the engineers to get us writing more tests and using it together more consistently. And especially doing it as a team, we're really starting to see major benefits that pay off far beyond the cost of writing the tests.
So this talk is going to be about first why you should test.
What I'm doing differently from sort of conventional unit testing.
I've had a lot of software engineering gurus look and go, well, those aren't unit tests.
They aren't integration tests.
They aren't feature tests.
I don't know quite what those are.
So I'm going to drill into that a little bit and talk about it and hopefully figure out how to get it working in games.
Some tips and tricks.
once you've gotten that far that have worked for me to try and be more effective with my testing.
And finally, a few takeaways that will hopefully help you get over that initial hurdle and get started with testing.
So some quick definitions before I get going.
If I say Gaia, that's the game AI architecture.
A lot of you have probably heard me talk about it before.
It's a modular AI architecture I developed at Lockheed Martin.
I've used it on a bunch of different things.
I gave an AI Summit talk about it in 2016.
I wrote a Game AI Pro 3 article about it.
If you want to know more, it's there.
I won't mention it much, but when it comes up, I just wanted you to know what that is.
CSN is City Scale Navigation.
This is the main feature that I've been working out at Kythera.
So this is what most of my examples are going to be from because this is what I'm writing tests for right now.
It's a new feature under development where we're going to fill large open worlds with ambient vehicles and pedestrians, and it uses a graph-based spatial representation rather than our underlying NavMesh.
So you can see here is an example of the graph that I use in a lot of the tests, and because vehicles drive in lanes on roads, and pedestrians walk on crosswalks, those are all pretty linear, and a graph is a really natural representation for that.
So this is my one sort of by the book software engineering slide.
What is a unit test according to standard lore?
And I'm just want to do this really quickly because hopefully everybody already knows. But a unit test is an automated piece of code that invokes a unit of work in the system, right? The smallest thing that you can that you can check and then checks a single assumption about the behavior of that unit of work.
And a good unit test should be all of these things.
I'm not going to go into them now because we'll come back to them.
And the idea is you're going to have lots of these or hundreds of them, right?
Each testing a single assumption about one thing.
They're going to be in their own library that depends on the game library.
It's going to be written alongside the code in C++ or whatever language you normally develop in.
And it's going to cover as much of the code as possible.
So you're validating that as much of the code as possible works the way that you think it should.
It's run every time you build.
So you hit Control-Shift-B, watch it compile, and then the tests run and immediately tell you whether they pass or fail.
It's run on continuous integration.
So if for some reason you didn't run it or you missed something, it gets caught in continuous integration, and if the tests fail, it warns the team so they don't pull the bad code.
It's run in the nightly build so that it can run on all the targets that we don't have time to do in continuous integration.
So why do this, right? I mean, this means writing a bunch of extra code, maintaining a bunch of extra code.
That's going to take time. We don't have a lot of extra time on most game projects. So why?
The first really major benefit is that you get instant feedback, right?
As soon as you make a mistake, you know right away that there's a bug.
The best time to fix a bug is just after you wrote it, because all of the code is fresh in your head, right?
Everything that you're doing, the way it connects up to everything else, all of that is right there, as opposed to when you come back later and you have to try and rebuild that, but it's imperfect.
It's also the safest time to fix a bug.
You are least likely to make another bug, because all of that is fresh in your head.
It's like the easy button for bug finding, right?
When a test fails, you know exactly where the bug is.
It's in the code you just wrote or in the code that the test is testing, as opposed to getting a bug report from QA and then you blame it on animation and they blame it on somebody else and then it gets thrown back to you and you mark it no repro and then the testers say, no, it definitely happens and throw it back, right?
And then you finally start to dig in and try and find it and you realize it is you and you're really embarrassed.
all of that time is saved because you know exactly where it is. And you have an immediate repro case because the tests are repeatable because they do the same thing when you run them again.
If you have some difficult to reproduce, hard to catch bug, like you've got one vehicle is speeding up only a little bit, another vehicle is slowing down but not enough to stop, and there's a traffic light that's about to turn red.
And on the far side of the traffic light, there's a crosswalk with a pedestrian that's gonna go across it.
And in this combination of things, something goes wrong.
And in CSN, we have definitely found cases where it's weird combinations like that, right?
When we get a test that catches that, or we set up the situation in game so that the test catches it, then we could just rerun the test and hit the problem again and again and again, and iterate really, really quickly trying to find it.
This may seem a little strange, but stick with me on this one.
The tests document the code in a really powerful way.
They let you step in and see what's happening with fully instantiated objects.
It's much better to go in the debugger and see the object with all the data filled out and everything hooked up than it is to try and look at the header files and figure out how everything hooks up.
especially when you're getting back up to speed on some feature that you worked on a while ago, or when you're trying to look at somebody else's code and you want to get up to speed with what what they did, going and looking at the tests really helps. And I'll give you a concrete example of this. The first thing that I worked on when I came to Kaithera was the CSN path planner.
Right, so as you recall CSN works on a graph rather than an avmesh, so it needed its own path planner.
And most of the time we don't use it. Most of the time we have a scheduler that just picks it random when it comes to an intersection. But we expect that the designers will want to build missions where a vehicle goes a certain place and they're going to want to take advantage of all the code in CSN to make it...
follow the rules of the road and look good and all of that. So they need a path planner that works with CSN. As I said, this was the first thing that I built when I came to Kythera. So we did it pretty early and they just weren't ready for it yet. So it wasn't hooked up in-game for almost a year.
And by the time they got ready to hook it up, I was off on another project. So Michael's a co-worker of mine. He came to me and he said, tell me how this works. And I started talking about how gives you all of the lanes that the vehicle could use, so it's not always cutting to the inside, and going on and on.
And I had vehicles on the brain, because that's usually what we think about first.
And he said, well, yeah, but hang on, because I'm working on pedestrians.
Does it handle crosswalks?
Well, shit, I don't know.
Crosswalks are kind of tricky, and I remember we had some trouble with them.
Because if you notice here, for instance, we have this crosswalk that's going across these vehicle lanes.
And it breaks off in the middle of this sidewalk here.
Whereas everywhere else, our lanes connect up end to end.
And that caused us a lot of trouble.
I remember that they weren't working when I first worked on path planning, and I don't know if I ever got them working with the path planner.
So let's find out. We go to the test file, start down at the bottom because it would be one of the last things that I did.
And sure enough, here's pedestrian crosswalk 3, right? It's the third of three tests.
So it creates a start position. It creates a destination position.
It tells the tests that those both have to be valid, calling into the is valid code that's built into CSN.
So this is one of the things I like to do, and we'll come back to this, is â€¦ have a lot of checking that I'm doing so that I can catch a lot of different bugs.
It then calls the path planner, plans a path, it prints that out so that if the test fails, we can see what it actually found easily instead of having to try and inspect the objects in the debugger.
Here's the path that it expects to follow.
And that looks like this, by the way.
So it's coming down across this crosswalk.
There's a crosswalk here.
And there's a third crosswalk here.
And yes, this test does paths.
So it is definitely going across crosswalks.
And finally, it checks that.
So this is where it checks that those two paths are the same.
And it also does a bunch of other checking that this path object from CSN is well-formed and that everything is kosher.
So not only does this let us confirm that yes, we did get as far as implementing this feature, but also it made it a lot easier for Michael to get in and understand things like, how does it handle this special case where it breaks off in the middle of a lane?
Because I still don't know how it does that. I don't remember what I did.
But instead of me having to go back and help him try and figure it out, or him spending a lot of time sort of fighting through and digging in, he just sets a breakpoint and sees what it does. There it is. So it's just a lot quicker to get up to speed.
The last thing that I'll give for benefits is that the tests give you a safety net anytime the code changes.
And this is game development.
Our code changes all the time.
We have to tune, we have to balance, we have to find the fun, we have features that change, we have to fix bugs.
Hopefully we have some time to do some refactoring now and then.
Game code probably changes more than code in most other industries.
And I think that's one of the reasons why testing at the code level, like unit tests do, can be really challenging.
But it's also really powerful because when I go and make a change, I'm going to test the thing that I changed that it fixed what I thought it was going to fix, but it's really difficult to test everything else that might have broken in that process.
And I can throw it to QA for regression testing, but they're coming down from the top, they're not coming up from the bottom. So they're testing in different ways, and they're going to find different bugs. There are a lot of bugs that unit testing will find much more naturally than QA will.
So again, I'll give you an example just as quickly as I can.
So one of the features I've been working on is at very high level of detail, pedestrians are going to avoid each other using ORCA, which we already have, so we're just really hooking it up. At lower level of detail, they just walk right through each other.
So this requires that we add a component onto the entity when the LOD is raised, and then remove that component when the LOD drops back down.
The problem is when we spawn a player in, we go from the lowest LOD, because there was nobody there, to the highest LOD, because the player is standing right there.
So the first thing we do is spawn some actors, because we don't have any pedestrians at the lowest LOD that's far enough away that we just don't model them.
And when we spawn them in, we create that avoidance component for them, because we're spawning them in at a high LOD.
And then we go through all of the actors that are in the area where the LOD is changing, and we change them from low to high. And in the process of doing that, among other things, we create an avoidance component for them so that they will avoid each other.
And then we crash because we just created two avoidance components and they conflict with each other.
So it seems really natural to just reverse the order of operations, right?
First change the LOD on any existing actors and then spawn in any new actors so that they only get one avoidance component.
But does that create another bug? Is it safe?
Probably.
But it's a pretty complex system and there could easily be some dependency either that we know about or that we didn't realize that it would break.
So now, unfortunately, we have no tests for this.
So I have to go a lot slower. I have to be a lot more careful in my testing.
I have to go interrupt Sandy, who's the guy who wrote the spawning system in the first place, and check with him, like, does this seem like it's safe?
And in the end, I just feel a lot more nervous about it. So it slows me down.
And it misses the opportunity to catch bugs early.
So hopefully by now I've got you at least unit test curious, right?
Hopefully by now you're going, wow, that sounds pretty good.
I wish I had some of that.
But I'm not sure how am I going to get it to work for games?
Cause that problem is still out there.
So, so the first thing I think we need to do, of course, we all recognize this slide from earlier is change this, this definition because it's too restrictive.
So instead, let's say that a unit test, in quotes, is an automated piece of code that invokes the system and tests it.
And yes, I still call them unit tests, even though software engineer gurus will tell me those aren't unit tests, they're something else, I don't know what they are.
And so here's our list of things that a good unit test should be.
And these ones are critical. It needs to be fully automated.
We need to run it every time we build. The whole point is to catch bugs early.
So it needs to be readable so that when we get a bug, we can easily understand what actually happened, what we expected to happen, and why that thing is wrong.
it needs to be maintainable. The code is going to change and as it does, the tests will often need to change as well. So we can't just hack them in. We have to treat them as first class code.
It needs to be consistent, meaning that it always returns the same result.
It always does the same thing. It always exercises the same code paths.
For the reasons that I already said, because that's how we get those really powerful repro cases, where when we catch some hard-to-reproduce bug, now we have an easy way to reproduce it, and where we can use the test to intentionally create things like that.
And it needs to be order agnostic, meaning that I can run them in any order, they don't depend on one another. And I'll get to the reason for that in just a moment.
Yes, they need to be fast, but let's not get crazy here. They don't need to run in 30 seconds, right? A single CPP compile for me takes about 30 seconds. So, sorry, they don't need to run in three seconds, which is the standard sort of, if you Google that's how fast they say they should be.
Archithera tests take a couple of minutes, two, three minutes to run the whole way.
And that's slower than I would like.
I have some ideas about how to fix it.
Someday I'll do it.
But is it the most important thing?
Hell no.
I've got a milestone next week.
That's the most important thing.
When I get some free time, I'll speed them up.
I can work with them even though they run slowly.
The biggest thing that I do is I take the tests that I'm working on and I put them up at the top.
And that's as easy as going into the CMake file and putting that test file at the top of the list.
So those ones run first, right?
If I've got a fast compile that I hit control shift B, it compiles, I watched until my tests, my new tests have passed, and then I can go back to work.
And I should circle back later and check if something else broke, in case I broke something in some other system and didn't realize it.
If I forget, then when I check the code in, it'll run on the continuous integration server.
And if I broke something, that'll get sent out to the whole team and I'll be embarrassed, and next time I'll remember to do it.
I don't care if they run in memory. The real point of this was to get them to run fast, but the Gaia tests, for instance, take about 10 to 20 seconds to run, and every one of them is an XML file that gets loaded in when the tests run. Disk is fast these days. Keep it in memory if you can, but if there's a reason to hit the disk or the network or the database or whatever else, if that's going to let you test something more effectively, then by all means do it.
And being atomic...
I feel like this is actually what was screwing me up, right?
So trying to test the smallest possible piece and totally isolate it by building mocks for all of the other systems and then those have to be maintained.
This was a nightmare, right?
Instead, when I step up the level of granularity and I test, for instance, at the behavior tree level or in Gaia at the reasoner level, then I'm testing, say, I'm testing my selector node or my sequence node, right? And I'm building some custom test nodes for some of the pieces underneath that, but I'm exercising a lot of decision-making logic throughout the architecture. And I've got these roots that are growing down into the architecture and sort of touching different things, having more opportunities to find bugs and broadening out my coverage.
almost for free. So you want them to be focused enough that when a test fails, you can get in and find where the problem is as easily as possible. But it's also nice if they actually branch out and test more than just the thing you're intentionally testing, because you'll catch bugs in other places. I found a bug, for instance, where my distance squared function was taking the square root. And that had been in there for probably five years. And top-down testing never caught it.
But when I wrote a test and just threw in there, oh, and by the way, the distance should be this, and it wasn't, then we found it. And boy, was that embarrassing. So that's really the core of it, to me, is get that granularity right. Test the bigger pieces. And don't be afraid to have your tests that call into things that have dependencies that are testing more code.
So on to sort of nuts and bolts, tips and tricks to, you know, now you're building tests, you're testing at a good level of granularity, how do you get it to work? One of the big things I think that messes people up, especially early on, is that you need to come in through the class interface, right? And oftentimes your test library is dynamically linking, so you need to come in through the API, which may not even be all of your public interfaces.
If you're exposing things just for the tests, it's either moving them into the public interface or maybe making the test a friend class of the thing it's testing or something like that, that's a really bad idea.
That's a really bad code smell.
Not only because you've got stuff in the public interface that shouldn't be there, but more importantly, that's a sign that you're trying to test too small.
You're testing implementation details.
And those are the things that are gonna change the fastest.
You wanna test.
through the interface because that's going to change less often. Refactors, for instance, aren't going to break it. But you can't test everything through the API, right? You can call a function and see that what you got back is what you expect, but you can't test a lot of the details that went on in that. And so, you can't really get in and test everything at the code level. So, how do we solve that? The solution that has worked for me is to inject my tests into the code. What do I mean by that? Use asserts.
Use errors. Use warnings. Make your errors and warnings cause test failures. Among other things, then you'll clean up all your damn warnings and they'll become useful again. If they're unexpected. If errors and warnings are expected, you should have a way to tell the test, hey, I'm expecting to get three warnings or three errors, right? And then do things like load a bad data file and confirm that you actually get them so you can test that your data load catches errors properly.
This means that the code checks that the edge cases work correctly.
And it's much more natural than having a separate set of tests to check that.
As the code changes, the asserts and errors and warnings in the code will naturally change as part of rewriting the code.
You put those in your code anyway.
And then you have tests that just call the code to give those asserts a chance to happen.
And the tests can deliberately try and set up edge cases to confirm that, yes, it does the right thing when the input is 0 or whatever the case may be.
Sorry, so this actually was meant to be much earlier.
So we have.
hundreds and hundreds of lines, for example, that validate that the CSN network is correctly formed. Probably over a thousand lines at this point. That nothing is duplicated. That everything hooks up correctly. That if I've got a connection from here to here, that I also have a reverse connection from there to there. All of that kind of stuff. Every time we ran into a bug, we added something to these validate functions.
They are if-deafed out if you're not in a debug build.
They're hit every time you load a level if you are in a debug build.
But we also have a bunch of tests that load different levels, including that map that I showed you.
And we found a bunch of bugs because the validate gets found, and we find, oh, there's a bug in the way the graph is getting formed in this case.
Once you've got tests injected into your code, then you can do things like write stress tests.
And this is a great place to start, right?
Just load a large map, create lots of entities and let it run for a while.
When you're trying to exercise something like our scheduler, which is what controls what all the vehicles and pedestrians do normally.
So we load in, in this case, this map, which is pretty big.
We create a ton of vehicles and pedestrians.
Then we start with a very small time step, and we go through a thousand iterations, incrementing the time step each time, and then calling update.
So this, among other things, tests that as your frame rate gets really bad, does that cause problems?
And the answer was, yeah, it does.
We find bugs that way.
And then we also have, again, just testing everything that we can think of.
So here I'm testing serialization that we use for networking every 200 frames.
So the schedules have time to change a bunch, and then I try serializing them.
This test takes about 30 seconds to run.
So that's too slow to do it every time I compile.
So we have this slow test filter.
We're using the Google test framework.
So this itself isn't built into Google test, but there are really easy ways to disable tests that is what this is leveraging.
And I have a compiler directive that if it's enabled, then I'll run this test.
If it's disabled, I don't.
It's enabled in the nightly build and on continuous integration.
I don't usually have it enabled when I'm working.
But I build my code, I write my own tests, my faster tests, I make sure that they all pass.
Then if I'm being good, I go into CMake, I click the box, I recompile, which only takes a minute because it's only a couple of files that change. I run the tests again and I make sure that the slow tests pass. If I'm being lazy or I forget to do that and I broke something, you know, or these catch something, which they often do because they can catch those weird edge cases.
Then it'll break in continuous integration and send out the email to the whole team and I'll be embarrassed and next time I'll run the slow tests before I commit.
So that's, I think, probably all I have time for as far as takeaway, or sorry, as far as tips and tricks.
There's so much else that I wanted to talk about and had to cut out.
So hopefully a future talk, and I would love to see talks from others.
There've been really few talks, especially about unit testing, like really getting into the code and testing.
And I would love ideas from all the rest of you as well.
But I wanted to give just a few, what are the reasons that we fail in the games industry?
And the takeaways that I hope will let you get over the hump and get started with this.
So the first reason that we often fail is that it's just overwhelming, right?
You've got all of this code, none of it's under test.
Where do you even begin?
don't angst about having perfect coverage. If you have anything under test, that's better than not having anything under test. Michael Feathers, who's one of the real unit testing gurus, said that bad tests run often are better than good tests that aren't run at all.
So, you know, as long as the tests aren't so bad, I would add, that they're slowing you down more than they're helping you. But you'll know if that's happening. So get anything tested, right?
Start with your next line of code. Or start with the next bug you fix. Put in a test to make sure that bug doesn't come back. And now you're doing regression testing at the code level that's complementing the regression testing the QA does.
The next problem, and this was the one that killed me so many times when I tried to get started, is that especially early on, while I was figuring out what to do, I just didn't have enough time.
The test maintenance was way bigger than the test payoff.
If you find that this is happening, think about your level of granularity.
Try writing coarser tests that test bigger systems and then implicitly test the smaller systems because they call down into it.
build the validation into your code, make it so that your errors and your warnings are test failures so that you don't have to assert everything, get asserts in there.
If you have more expensive things that you want to put in errors and warnings, if def, debug them or something like that.
And try and get that coverage broad in the easy, cheap ways.
And as soon as you start finding bugs and seeing how easy it is to dig in and fix those bugs when you've got a test pointing right at them, you'll start to really appreciate having the tests there.
The last thing that kills us is a lack of buy-in or a lack of discipline.
Right?
And in the games industry, we often tend to be cowboys.
We're pretty arrogant.
We know that we work in a really hard industry and we're all pretty smart to be here.
But that doesn't change the fact that tests will let us be more effective, work faster.
So there are times when it's not the right time to do it.
As I said right now, we've got a big milestone. And I just came back onto CSN recently.
And as I did, I started, especially because I've been working on this talk, so I've been thinking about it a lot.
like, you know, let's try this out. Let's, you know, when we move a task into the review column, just write on there where the tests are, and then the reviewer can go and run the tests and look at it that way, maybe instead of having an online code review.
And I immediately got back from the project manager, how long is that gonna take?
Because we've got this really tight milestone.
Do we have time for it?
And my answer was, well, it's not gonna cost me any time because I'm gonna build the test anyway.
I can't not test my code, that's ridiculous.
But, well, we do have some guys on the team who've been doing testing with me and, who are definitely up to speed and bought into it, we have a couple of guys who haven't really drunk the Kool-Aid yet.
And I will concede that maybe this week isn't the week for them to try and figure out how to start testing.
You know, fair enough, but we have to circle back.
And the project manager is totally on board with, yes, we want this, maybe just not right now.
So you have to circle back.
You have to have that discipline to, even when you set it aside for a little while, to come back to it.
And so at the end of the day, my answer to that, like how long is this gonna take, is we don't have time to not write tests.
Because there's some overhead and startup costs to be sure.
But once you get going, they will improve your velocity.
You will notice it.
And especially they will help you when you get to that crazy end stage where any mistake could sink the game, right?
Just before you ship.
having that safety net in place, having the ability to better test the code yourself before you throw it to QA, all of that is really going to help in that end stage get it out the door part.
But if you've given up on it before then, then you won't have that to help you.
So that's all I've got. Thank you for listening. I hope this has helped. I've been wanting to give this talk for a while now, and it's, you know, really been trying to figure out what is it that's made it different for me, that caused it to just flip on and start working. So hopefully this is helpful. As promised, here's the contact information. If you want to talk about AI, talk about testing, definitely reach out. I love talking about that kind of stuff.
If you're looking for some help with some tools or looking for some help building your AI, reach out about that too. I love building AI and that is something that we can help you out with.
So that's it. Again, thank you for listening, and I will see you on the other side.
