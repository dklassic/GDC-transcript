Hey everyone and welcome to our talk, Spatial Audio in Budget Cuts.
Ironically, we will not be playing back any sounds, but you have to use your imagination.
And speaking of that, you will all receive a feedback form after the talk, so use your imagination and pretend it was super good, and give us a good review.
So my name is Jonas Kjellberg.
I'm CTO of a company called Gesturement, and we make super secret unannounced music middleware.
But more importantly, I was the composer and audio designer of the game Budget Cuts.
And I'm Lakulish Antani.
I am a Valve person, and in my Valve time, I do Valve things like spatial audio.
So this talk is really going to be about our experiences in integrating spatial audio into budget cuts via Valve's spatial audio tool called Steam Audio.
So, before we get into all of the details about all of our experiences, just a quick overview of this talk.
I am going to be talking about the technical side of things.
Jonas is going to be talking about the artistic side.
So kind of like good cop, bad cop here.
So first off...
Spatial audio has been getting a lot of attention in recent years, but already there's so much confusing terminology out there that it's kind of hard to keep track of what someone means when they say that they've implemented such and such spatial audio feature.
So I'm going to give a quick overview of some of the concepts, and the first one is HRTF, head-related transfer function.
So in the real world...
The shape of your head affects how you hear things.
The sound bends around your head before it gets into your ears.
So the effect of your head, your torso, these flappy bits of your ears, they all affect how you hear things and where you perceive a sound to be coming from.
And an HRTF is a set of filters that models that.
HRTFs are typically measured from dummy heads or real people, and they're measured in a bunch of different directions, and all of this information is used to re-synthesize the positionality of a sound.
The other concept is ambisonics.
Ambisonics is kind of a surround format, but unlike your typical 5.1 or 7.1, it's a spherical representation of a sound field, so it includes verticality as well.
One way to maybe think about this is it's like 360 video, but for sound.
Again, ambisonics can be measured, I mean recorded, using a bunch of different kinds of microphones.
You can also encode a monoclip into ambisonics, given a direction.
And you can decode ambisonics using hardware, software decoders, and you can spatialize ambisonics using HRTFs.
I'm going to hand it over to Jonas, who's going to discuss a few more spatial audio concepts.
So what you have to take into account when working with all these things is stuff like distance cues.
And that means a sound will get softer and more muffled.
The highs will get rolled off the further away the sound is playing from the listener.
Also, if the sound is played back in a reverberant space, you will get much more of the reverb and the late reflections.
Then we have stuff regarding the rooms.
So, a room size cue would mean that the way a sound will bounce around the room, the reflections will give an indicator of the size of the space.
Stuff like how long the reverb tail is, but also which materials are being used in the room that will affect the coloration of the sound.
And this thing is super important for presence and selling the illusion of being in a different space.
We also have stuff like sound propagation, and that in effect means the way sound flows through a space.
So sound will be bouncing, being blocked, being scattered, diffracted, going through different materials and being colored by that, going through openings, doors, windows, just how the sound is bouncing around basically.
So this is pretty cool, but it's nothing new.
And Lakulish, why are we seeing this talk about special audio right now?
That's a good question, actually.
All of this tech, or much of this tech, is really several decades old.
So why is this all relevant now?
There's a few reasons.
One is that computers are way faster now.
You have multi-core CPUs, GPUs.
You have a lot of compute you can throw at problems like spatial audio.
So that's now computationally feasible.
The other thing is VR.
With the rise of VR in the last few years, the importance of immersion has gone up, and spatial audio is a very important way through which you can add immersion to your audio.
And a side benefit of this is that with VR, most of your users are going to be using headphones, which is the ideal delivery mechanism for spatial audio.
So all of these factors together are kind of why spatial audio is all the rage these days.
So that's an overview of spatial audio.
So now I'm going to hand it over to Jonas.
He's going to tell us more about Budget Cuts, which is what all the kids are talking about these days.
So Budget Cuts, it's all the latest rage since like three years ago.
It's a rhythm game where you have two lightsabers and...
Oh, wrong game.
So it's a VR spy game, basically.
So here's a snippet of our trailer.
And it features stealth, deadly robots.
You have to hide from the robots and try not to get seen.
Or you could choose to kill the robots the only way possible, with knives and scissors.
So the game is designed around two core concepts.
It's all about room-scale gameplay, so you'll be crawling on the floor, hiding behind cubicles, crouching, ducking and weaving, stuff like that.
And most interestingly, it features a very novel teleportation portal mechanic that works like this.
So you fire a ball with your hand cannon.
It opens up a portal that you can look into and preview the new location.
And once you're ready, you teleport to that location and the teleport portal will envelop around you.
It feels very immersive and it also aids in gameplay because you'll be able to pre-survey where you're going.
So I'll...
be the first to argue that spatial audio should be implemented into every game coming forward from this point.
It's super nice, but most importantly for a game like Body Cuts it's even imperative for a stealth game.
Spatial audio will give us clues about where the enemies are situated, where they're patrolling.
We will get cues about the space that we're in, if we're hiding behind safe cover or not.
We've all been talking about immersion forever in VR, but it's really about that.
So audio normally is used to sell the illusion and even more so in VR, so it makes sense that we actually use this tech to get that extra mile.
So we've been using reflections, for instance, as one of our core gameplay cues.
which allows the player to properly form a strategy based on what they're hearing.
So while you normally in a game would be able to hear that, ah, there's a sound emitting from the next room, with spatial audio technologies, we can hear the size of the room that's on the other side of the wall.
We can hear if...
the robot is walking away from us or coming towards us, just based on the way the sound is being colored.
So that helps the player to correctly assess a threat and time their strategies accordingly.
Also a core gameplay cue, we use occlusion, which means...
The way a sound is being blocked, essentially it's a ray trace from the audio source back to the listener, and if there is geometry intersecting that, the sound will get muffled according to what kind of material the geometry is intersecting.
has applied to itself.
So this can tell us if we're behind safe cover or if we're in a direct line of sight to an enemy.
Also there are these telephones similar to the Matrix.
where you'd pick up the phone to get new information about your mission.
And then you can hear quite clearly if it's in a different room, even though it's far away.
You can pinpoint which room it's coming from.
So, all was not well working like this, and Lakulish will go over some of that.
All right, so for the next few minutes, I'm going to talk about some of the things we had to do to get spatial audio working well in budget cuts.
What I will not be talking about is HRTFs or occlusion, because this is 2019 and that's not really as much of a problem as it used to be.
So what I'm going to be talking about here is sound propagation.
So this is a quick overview of what we did.
We split sound propagation into per-source early reflections and a single physics-based reverb.
The early sound paths for every source were guided using a custom portaling system.
And in addition to all of this, we faked some of the spatialization for ambience and music using the Steam Audio Ambisonics reverb system.
So let me start with the real-time reflections and early paths.
So ideally, and this was our starting point, we wanted to model sound propagation from every single source individually, trace rays all over the place, apply convolution reverbs for every individual source, and everything would be great.
But obviously that's a lot of compute.
There are ways to mitigate this.
You can use multi-threading.
You can use GPUs.
There are options, but those are not always suitable for every title.
And in particular, they were not suitable for budget cuts.
So we decided to do the following.
We said, let's split sound propagation into two parts.
For every source, we're only going to compute early reflection.
So that's like one or two bounces.
And then the reverb will be applied separately.
And I'll come back to the reverb in a bit.
So we implemented this.
It worked more or less okay.
Performance was more or less okay, but it wasn't perfect.
And in particular, we had an issue where sometimes we encountered sudden frame rate hits, which were kind of hard to track down what was going on.
So we did a whole bunch of investigation.
And what we found was that there was something quirky about how Unity's audio engine was working.
And what it was doing was every time an audio clip was played from an audio source, it was reinitializing a whole bunch of spatializer state.
Now, this is interesting because spatializer state is not really tied to an element in your mix graph.
It's more tied to the physical game object that's moving around in your scene.
So we obviously reported this issue to Unity and they began work on a fix, but that fix was not going to come out in time for budget cuts to ship.
So we decided to approximate things further.
And our solution to that was to first take a step back and really think about why we cared so much about these early sound paths.
And the important cue here was we really only care about knowing which doors have robots on the other side, which windows, which vents have robots on the other side, so that you don't go there.
And so what we did was let's manually mark those up and then use that information to guide the Steam Audio Ray Tracer so that it doesn't have to calculate as much.
It can run way, way more efficiently.
So that's what we implemented.
Just want to point out that this solution was developed specifically for budget cuts and is not part of Steam Audio at this time.
So that's the early paths.
The other issue was reverb.
So we obviously couldn't apply reverb to every source.
So what we do is we apply a physics-based reverb for a submix.
So Razor traced out from the listener's position, gathered back at the listener's position after a whole bunch of bouncing around.
And that is sufficient to convey a sense of room size, room shape, materials, that sort of thing.
But there are issues.
Well, before I go into the issues, one of the benefits of doing this is that your compute cost is independent of how many sources you have.
You're just applying the reverb to a submix.
So now to the issue, which is that because of the way physics-based reverb works, because of the way rays are gathered, When you move close to a wall, what happens is the size of the room begins to appear a little bit smaller.
And the solution to this problem is to use a directional reverb, so you model how energy decays in different directions.
And so if you do this correctly, what will happen is when you move close to a wall, the tail of the reverb will shift away from the wall, and you'll get a smaller, shorter echo from the wall itself.
And so that's what is implemented in Steam Audio and in Budget Cuts.
But there's a gotcha there as well.
And that is latency.
So physics-based reverb is gonna update at two to 10 hertz.
And that kind of latency is fine if you're just moving from one space to another, but that's not fine if you just turn your head around and have to wait like half a second for the simulation to update based on where you're looking.
And the solution to this is ambisonics.
So one of the nice things about ambisonics is that you can take any ambisonics buffer.
and rotate it to any arbitrary orientation.
And so what we do is we run all of our physics-based reverb simulations in world space, so independent of where the listener is looking.
We do our convolutions with these world space reverbs.
And then once all the convolution, all the audio processing is done, we just take that one frame of audio and we rotate it to match the listener's orientation.
So we get decoupled orientation updates that are calculated at audio frame rates.
So those are the two issues that we solved with reflections and with reverb, and now Jonas is going to talk about ambience and music.
I hope all that was crystal clear.
So, spatial audio is essentially a whole new paradigm when it comes to how you approach your mix.
Things that are still stuck in an old paradigm is how you treat music in games or stuff like ambiences.
When mixing spatial audio together with a flat stereo sound file, it feels very disconnected and doesn't feel part of the experience.
Music and games are supposed to propel the player further into the experience, not pull them out, right?
So there are different approaches you could do with music in VR.
And one would be to always, from the design stage, use diegetic sound sources so that the music is actually always emanating from somewhere in the space.
We did some of that in BodyCuts.
There are a bunch of radios scattered across the world, and if you move closer to them, the music will fall down into a specialized radio.
And that gives a pretty nice effect.
Another approach I used was to, essentially, when mixing the music, use binaural panning plugins and slowly automate movement around the player where the instruments will be slowly facing around in the soundstage.
which doesn't at all line up with what is going on on screen, but the facing of the sound sort of allows it to blend more properly together with the spatialized sounds.
The preferred solution would instead be to play back an ambisonics music mix, so that you would mix your music in ambisonics, and then when playing it back in the game, you can choose to have it rotate together with the...
players had or independent of it.
So that, for instance, drums would always be coming from that direction or whatever, depending on how you're moving.
Another similar issue comes with ambiences.
So you'd need some sort of like, to convey a space, you need to have some kind of ambience.
And the preferred way to go about it using spatialized sounds would be to just use reflections and have...
a ton of bounces and that in turn would create a dynamic room tone, right?
But since we had performance issues with reflections, we couldn't do anything like that.
At the same time, the game world is quite empty because it's an abandoned office building with petroleum robots, so you need some kind of sound to get back something from the room.
So the way we did that was to fake things up with piping, like a mono sound file that played back filtered white noise.
I attached that to the player, so it's always moving with me.
And as you're teleporting yourself into, say, a small vent, that noise will then fill up the space, and you suddenly get a sense of the size of the room.
The same goes when going out to a big space, then it would be sounding bigger.
So, spatial audio is super great, super cool, but it comes with a lot of caveats.
For instance, all of these plugins you can get for free, more or less.
There are many that are not, I suppose, but what's not free is performance.
You need to budget for performance before starting with this.
Like, especially the more of the features you add in, the better it's going to sound.
And you will notice this drastically, but it will also tank your performance.
So you need to rein your expectations in a bit, and try to not use all of the good stuff just yet.
And of course, being a beta kind of software, there is no community if you stumble upon an issue.
You need to directly talk to the developer.
And I'm not sure that everyone is so lucky as we've been to have them on speed dial and say, hey, could you add this thing?
But yeah, you need to keep that in mind, that it's not like any of the big things like FModelwise that has a lot of community around them.
And the documentation is always changing, of course.
And it is not without bugs, and sometimes you can question your own sanity, and it turns out it was a bug, like with the Unity thing all along.
And a bug like the Unity bug did cause me to have to redo the whole mix several times, because we had to cut features.
So that's like, just a couple of weeks just doing that.
The last time, two weeks before release, even.
It's not a plug-and-play solution, is what we're trying to say.
So you need to really think about this from the get-go.
You can't just activate the plugin and it's going to sound nice and spatialized.
So, giving back the word to Lakulish, he's going to talk about a summary of what we did.
For those of us who just zoned out through most of that, this is what we talked about.
We split spatial audio.
We split sound propagation into two parts, early reflections for every source, and reverb for a submix based on the listener position.
We guided the early reflections using manually placed portals.
And we used ambisonics reverb to spatialize ambiences and music.
So, you know, things didn't go exactly as we planned.
So what would we have liked to have done better?
You know, what are some ways in which things can improve?
So one is, obviously, we would have liked to have had more success with the early reflections.
The bug that we reported to Unity that was leading to a lot of the performance issues that I believe has been fixed and a fix is shipping in 2019.1 or thereabouts.
Having said that, the portal system that we came up with is great.
However, it may not be suitable for all kinds of titles.
It depends on what kind of scenes you have and it worked well for budget cuts.
So we would ideally want to try to...
Maybe come up with something more semi-automatic, something more flexible that can solve this problem more efficiently than just retracing all the things.
The other thing we would like to have is what is called near field HRTF.
And what near field HRTF means is when you hold some sound source really close to your head, that's called a near field source.
So in budget cuts, for example, you can hold a phone right up to your ear and you want that to be rendered using a near field HRTF.
The HRTFs you normally have, including the one in Steam Audio, they're far-field, so they're recorded from a few meters away from the listener.
So you want to get near-field data because that adds a more visceral kind of feel to that specialization.
So those are some of the technical things we would like to improve.
So Jonas, what are some of the less technical things we would like to improve?
Well, before getting to that, I want to take the opportunity to say that we are going to update BuddyCats to the new Unity version so that we can put in all the nice things back in.
I'm not sure that we're going to do that, but I'm saying it here so that the team can then be forced to do it, essentially.
So I promise.
So what we want to do more as well is, so the reverb works great, but like I said before, it's all about selling the illusion, and sometimes realism is not as good as what you can.
and that's not enough. So we want to be able to tweak the reverb further and like cheat a bit and make reverb tails longer for instance or use the reverb calculations to control a third party reverb or something similar. Also since we're using this hand cannon thing as soon as we fire the cannon we already know where the portal will land so then we can start pre-computing the reverb before even going there.
Um, yeah, so instead of having a half a second delay of going to a space until you start hearing the change of the space, now we can do it instantly if we do it like that.
Using ambisonics for ambiences.
So recently Steam Audio added ambisonics playback.
It's already available in the other specializing suites as well.
And that means that you can have an ambisonics recording of, say, you know, a city or a room, and play that back and have that rotate together with your head.
That works super well for far-field sounds, but for close sounds you'd want to not do that, because you can see that they're not moving, or that the sound is not moving the distance with those files.
Anyway...
So, conclusions. It's super expensive, performance-wise, so you need to account for that.
When working with any bleeding-edge tech, be prepared that everything is going to break all the time.
But, as a VR developer, you're always working with like...
the vanguard of that stuff, so you're probably used to it.
Spatial audio, it's extremely important for VR immersion or rather like room presence to get you tricked into being in a space.
Imperative for any kind of stealth game.
And it's truly a next-gen audio experience.
Me as an audio designer, I'm super happy about the advancements of audio technology in the past few years within games.
And it's in due time that all games start to sound much better.
Yeah, that's me.
And what about the bad cop?
And as the bad cop, it is my duty to inform you that if you want to use spatial audio for your next project, start early.
It is not just something you can tack on at the end, flip a switch, and be like, yes, this is awesome.
You're going to have to change a lot of your workflows.
It's going to affect a lot of the different aspects of your production.
It's going to touch level design, artwork, everything.
So just keep that in mind.
And that's all from us.
Thank you, and we're happy to answer any questions.
Hi. Thank you. That was great, and great game as well.
Thank you.
Are you biased?
How can I ask this?
Is there other packages we should try before we try the Steam one or in addition to the Steam one?
I think you should try all of them.
Like, they all have different timbre, if you will.
The filters sound a bit different.
The HRTF panning, it's sort of the same with most of them.
But they all have different ways of dealing with certain issues.
So, yeah, try all of them.
Some are better for some things, like Google Resonance is better for mobile.
But it's not as powerful.
Thank you.
Thank you.
More.
Here we go.
Hey, do you want to talk a little bit about the ambisonic reverb that you used?
And what product you found there or products out there that we can use for our solutions?
So.
In this case, the ambisonics reverb that I was referring to was the one that's built into Steam Audio.
So all of the reverb calculations happen in ambisonics.
So you can then rotate the reverb freely.
And did you find it impacting performance or was it just smooth?
So for sure, it depends on how many channels you have, right?
So the performance impact scales linearly in how many channels you have.
So first order ambisonics is four convolutions, second order is nine, and that sort of thing.
But overall, as long as, so we did a listener-centric thing, so it was just being applied to a single submix.
So since it wasn't scaling with how many sources you have, the impact was not that significant.
Thank you.
Yeah.
Hi.
Thanks for the talk.
I just wanted to thank you as someone who also lost a week and a lot of hair for the Unity spatialization re-nationalization bug for opening up a channel to them.
Let's give a round of applause.
Yeah.
That was not great.
Veterans of the Unity re-spatialization war.
But, yeah.
So, I will be...
I took a picture of that screenshot.
I will be sending it to my team.
Thank you.
Awesome.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
I just had a question about the wall materials So being in an office space, I used as many materials as you would expect in an office.
Carpeted floors.
I'm not used to cubicles.
Sorry, what?
I'm not used to cubicles.
Right, right, right.
And some of the materials I had to...
Suddenly used all the nice reflection parts, I had to redo all the materials to allow them to transmit more of the sound through them.
To fake that.
So yeah, it's mostly the standard materials but tweaked.
Thank you.
I'm definitely more curious about your approach to that kind of ambient layer that you said you kind of had like a for like room tone you put white noise in and then you sent it through the reverb.
Did you actually stick white noise in or did you kind of design like a room tone for each room and like what was your kind of thought process and how did that end up serving as a bed for everything else?
Right, so about the faked room tone.
So, essentially, it's really just a filtered white noise.
So not all of the high frequencies are still there, but I figured since the reverb is using impulse convolutions, I just wanted to trigger those frequencies.
So that's why it's a noise.
And it's not really a tune at all, that's why it's so neat.
It's just something that's easily added and you can hear instantly that you're in different spaces.
And then my follow-up question is, when that reverb was kind of like following the listener for that kind of situation where you're like near the wall.
Like, how closely did you parameterize that to distance from the wall?
Was it like a gradual rotation that happens, or was it like a focus?
Like, what was the kind of parameterization you thought of there?
So originally the reverb was not directional, and I had that issue that if you were close to a wall, the reverb would think that you were in a smaller space.
But Lakulish kindly fixed that.
So now it doesn't really matter if you're close to a wall.
You can still hear that it's the sound of a space.
So you weren't actually rotating it in that situation.
It was just happening naturally because it was already a spatialized reverb.
OK, cool.
are we done for time? So last question.
So I'm not a sound designer, but this talk is fascinating because I'm interested in spatial audio.
When I've been messing with it, I've found that open-backed headphones are the best in terms of sound reproduction for spatialized audio, but when listening back on speakers it sounds very thin.
But there are certain times that you do want speakers over headphones for certain games.
Is there a way that we can...
kind of make a middle ground that does have specialized audio for those who do want to go with headphones and a special mix or something for headphones or is there like a way to make sure that those play very well together, or no?
You want to talk about decoders?
Sure, so there's a bunch of different options that you have.
One is obviously to just assume that everyone has decent-ish headphones and say that this requires headphones.
But if that's not an option, you can then rely on external decoders.
So there are some systems that have ambisonics decode in hardware.
and they can handle spatialization.
They will typically do things like cross-talk cancellation to improve the spatialization over speakers.
Or alternatively, you could just do what we did for Counter-Strike, which is like you turn, you have a special setting for stereo with headphones versus just stereo, and the user explicitly makes the choice.
Am I using headphones or not?
And if they're not using headphones, the HRTF goes away.
Thank you.
And to add to that, for more human, intellectual speech, no, well, so the nice thing about ambisonics is that obviously we're using a HRTF decoder, but you could essentially make a decoder that says I have 200 speakers in a sphere around me, play it back to those.
Or I have stereo, play it back through those, that kind of thing.
So the format itself is very versatile like that.
Thank you.
Thank you.
We'll be in another room, I suppose, answering more questions.
Thanks.
