Please welcome to the stage, the studio head and directors of Kojima Production, Hideo Kojima.
Hello, guys.
Thank you for joining us today.
As promised, I have our new trailer.
Take a look.
Don't you die on me, dammit!
He's dropping!
Interbate, now!
Cardiac arrest, he's in B-fib!
Clear!
No response, hit him again!
How's he doing?
Well, he's stabilized, but it took too long.
He's in a coma.
What about him?
Thanks for listening!
We won't preach as you whisper You've been in a coma for quite some time.
Yes, yes, I know you would like to know how long.
I'm afraid it's been...
nine years.
Diamond Docks.
Our new home.
Did you like it?
Yeah!
Ah...
It's hot.
From here on, I'll be presenting as Hideo Kojima.
As you've seen in the trailer and probably understand now, the Phantom Pain and Metal Gear Solid Ground Zeroes put together is Metal Gear Solid V.
The trailer itself is all run real-time.
Since today our session is about the Fox engine, I'd like to show you a little something that's actually moving in the engine.
Metal Gear Solid V is an open world game, but the beginning of Phantom Pain is a tutorial, so it's more of a rail game.
The Phantom Pain is an open world game, but in the beginning as an introduction and a tutorial, it's more of a real game to understand the controls and the system overall.
The tutorial itself is a sequence where a snake wakes up from a coma from nine years, and is all of a sudden attacked by an unknown enemy, and he has to escape the hospital.
Since he's been in a coma for nine years, Snake has lost a lot of his movement capabilities.
So you start off from crawling and then standing up, falling onto things to gather your composure, and that's the rail game that we'll be showing today.
And everything is running on PC right now.
Where am I?
I've been...
I've been watching over you for nine years.
You can call me Ishmael.
What the hell is going on?
Well, the good news is, you're in the land of the living.
Bad news? The world wants you dead.
On your feet, soldier.
It's our only place to come down.
Need a little pick me up?
The drug's not working.
We're getting out of here. Move it!
song riiiiiiiiiiiiiiiimp pppppppppppppppppppppppp Pew pew pew pew pew pew...
ResidentSleep has become infuriatingly obese 有人angry The emergency stairs, come on!
This way, hurry!
Thank you.
From here on out, Snake will be able to walk freely, open world, and whoever he wants to.
And from here, we'll talk about the engine. Let me introduce you to the members.
First, the CG art director, Hideki Sasaki.
And our lighting artist, Masayuki Suzuki.
And our technical director, Junji Tago.
And our technical director, Junji Tago.
Alright, let's get it started.
Welcome to the session, Photorealism Through the Eyes of a Fox, the core of Metal Gear Solid Ground Zeroes.
Now that Metal Gear Solid V has officially been announced, our presentation title will be reflected to show that.
The fox in the title refers to our development environment.
Let me give a brief overview of what the fox engine is.
The fox engine is the engine created in-house here at Kojima Productions.
It contains the level editor, cutscene editor, effects, UI, sound, motion, and others.
This is what we use to create our game.
Because the Fox Engine covers such a wide range, I unfortunately do not have time to touch on each aspect.
I would like, however, to focus on the visuals.
I will show you an overview of how we go about creating MGS5 using the Fox Engine rather than each individual detail.
Here is the agenda for today.
I will start off showing you how assets are created for MGS5.
Next, the lighting artist, Mr. Suzuki, will talk about the lighting, shaders, and camera.
And finally, the technical director, Mr. Tago, will go over the graphics engine.
Well, let's jump right in. I will be covering how we go about creating assets.
Does this picture look familiar?
Some of you have probably seen it before.
This is an actual photograph of our conference room here at Kojima Productions.
On our Kojima Productions website, we've asked visitors which image was an actual photograph and which image was rendered in the Fox engine.
Here, the image on the left is the photograph.
The one on the right is rendered in Fox.
Here again, the left image is a photograph, the right rendered in Fox.
You can tell which is which quite easily by looking at the differences in the assets, but if you focus on the lighting, they become very hard to distinguish.
Promotion was not the goal of making the conference room, but rather to have a good reference for artists to use when dealing with linear space lighting.
https://TheBusinessProfessor.com Linear workflow will be talked about more in the next talk by Mr. Suzuki, but it basically means rendering, taking into account your monitor's gamma.
It is the foundation of physically-based rendering.
When working in linear space, the most important and difficult problem we faced was the way to create diffuse maps.
Using conventional color maps, the artist's job was only to make a texture look good.
In physically-based rendering, on the other hand, the artist has to be aware of diffuse reflection.
To make our texture references, we use a real-world camera and use the captured raw image without correction.
Artists are used to creating assets that look good without worrying about detailed parameters, but this would create problems in linear space.
So in order to test if the results look natural in linear space lighting, we needed to create a reference model.
This model is the conference room.
Now I would like to show you the actual Fox engine in action.
Please wait just a couple seconds.
Gotta get these computers running.
There it is.
So this is the actual Fox engine running in real time.
Here we're changing some of the view styles.
That's the normal map.
Here is the specular mask.
And here's the roughness map.
For the roughness, blacker is glossier, while whiter is more of a matte finish.
And here is the diffuse albedo map.
This image is being created from values taken from a real photograph.
Unlike conventional workflows, textures do not have as much shading or detail applied to them.
This image is not something that an artist has created, but simulated based on the reflection values of each surface.
Well, let's move the camera around.
It sure does look real.
While I was playing with the simulation back at the office, one of the programmers was looking over my shoulder watching me do this, and he thought we had security cameras set up in the conference room.
I guess that's a testament to how real this really does look.
We frequently use this conference room to check shaders, assets, and the like.
for example.
This is a soldier that has appeared in the Phantom Pain trailer.
Some kid hanging out in the back.
I'll explain about the kid in just a little bit.
This is the soldier that appeared in the Ground Zeroes trailer.
You can see that he looks a bit wet from the rain.
In order to check for the weapons, the artists have created a gun rack here, and here's what it looks like.
The tank and helicopter were also shown in the trailer as well.
Here are the details of what the weapons do look like.
We think it looks pretty real.
Because we have a real-looking environment, it is easy to spot things that do not look natural in our created assets.
If we need to test what something looks like, we just take pictures of it and can quickly and easily contrast what that thing looks like in real life and what it looks like rendered in Fox.
These images of cloth and leaves are from Look Development.
The left image is rendered in Fox, the right is a real photograph.
We sure do think it looks quite similar, the rendered image looks quite similar to the photograph.
And the asset creators can check very easily for any inconsistencies when creating their assets.
We are able to keep our textures looking like real life, even when using linear space sliding.
Texture creation will be touched on by Mr. Suzuki in the next talk.
I have talked about how we go about making our textures look real life, but what about our models?
I now want to talk a bit about photorealistic modeling.
The conference room you have seen rendered in Fox was created using photographs.
This character on the other hand was created by 3D scanning.
3D scanning is not a new technology by any means, but it is the first time the Metal Gear Solid series has used it.
The model shown on the previous slide was created from refining a laser scan, but we are also using a different approach, that of photo-based scanning.
To generate a model from a photograph, we used PhotoScan by Agisoft.
PhotoScan automatically generates a 3D model using multiple photographs as a reference point.
The photo scan is not just limited to models.
It can also generate textures and camera data as well.
There have been tools used in the past to generate a model from a photograph, but the quality and accuracy with which this is accomplished has increased dramatically in recent times.
In order to make one important character for the game, we made a photo real sculpture and scanned it.
The character in-game is over 100 years old, so the wrinkles and sags and skin needed to be accurate.
Here is what we did.
We constructed a clay mold from the actor, and then added special effects makeup to the mold to create the final character.
Finally, we scanned in the finished mold and stuck the character back into the game.
Here is this process in action.
Here is the life cast of the model.
The model on the right is the original life cast, and the one on the left is the clay press-out.
As I said previously, we added special effects makeup to the character and scanned it in, but we also did a scan before adding the makeup as well.
The person we requested to do the sculpting was Mr. Kazuhiro Tsuji.
He is famous in the field of photo-real sculpting and has worked on many projects, including The Curious Case of Benjamin Button and Looper.
Unfortunately, we are unable to show you what the finished character does look like.
We can, however, show you the skin of the clay model before the addition of the special effects makeup.
And here are some photographs of the model that we used to scan.
I want to show you a short clip of this as well.
Sometimes movies don't play, there it goes.
We use PhotoScan to create the 3D model and camera data, and then use SoftImage to display it.
The blue objects shown around the model are the cameras.
The cameras took pictures of the model at these coordinates.
The 3D model and textures are then generated from these photographs.
Many cameras were used in this example, but it is completely possible to scan with less.
Here is what the model looks like with textures.
Since these textures were generated from the photographs taken, they match the model perfectly.
And here is the wireframe.
You can even see that detailed features like wrinkles in the face are kept faithful to the original model.
On the left is a photograph of the sculpture, on the right is the image of the 3D scan model.
You can see how accurate the scan model is.
There is virtually no difference when compared to the photograph.
All right, I'd like to show you one more quick clip.
This is an example of how an environmental asset is scanned in PhotoScan.
For this specific asset, we placed the rock on the turntable, rotated it, and photographed it.
We then flipped it over and did the same to get a scan of the rock from all angles.
One advantage of doing photo-based scanning is that it creates camera data.
You can put the camera data, as an FBX file, into a 3D tool and can easily modify the textures from the given camera projection.
For example, when using the camera data in Mudbox and setting the stencil, we can paint the texture onto the model, matching it exactly.
Photoscan can automatically generate textures, and in recent versions, it actually looks pretty good.
So if you want to modify a small part of a certain texture, it is possible and quite easy to do.
And here is the finished product.
For another example, here is a scan of a tree trunk.
Of course, we can use the scan as is, but we can also extract the height map and use it as a brush in ZBrush or Mudbox.
This example is especially cool.
This is a scan of a seashell.
Even with such a complicated surface, the scanned data is an almost perfect replication.
What surprised me the most was that even the very thin parts of the shell were reproduced perfectly.
Next, cloth modeling.
For character clothing and environmental cloth assets, we used Marvellous Designer.
Constructing real-looking wrinkles in cloth is very difficult, and when doing it by hand, can have a fabricated look.
During Metal Gear Solid 4, we tested some simulation-based modeling tools, but it was difficult to use these tools for any high-res models that required baking normal maps.
However, when using Marvelous Designer on this project, it was very quick and easy to get high-quality cloth models.
In the final scene in the trailer, snake stitching on his shoulders was done completely in Marvelous Designer.
No one had to touch it up by hand, and it sure did turn out pretty well, we think.
And here's another demonstration of Marvelous Designer.
It is incredibly simple in Marvelous Designer to set up a specific pattern to get a high-quality result.
And that's what Marvelous Designer is all about.
Just by setting a pattern and generating the model, the time saved for a modeler is enormous.
Here are some of the sleeves being adjusted.
And here are some wrinkles on his jacket being adjusted.
One big advantage is being able to adjust by hand the simulated result.
Finally, the polygon density is raised and recalculated.
The stitching near his shoulders turned out quite well, I think you can see from this example.
Snake's goggles and gloves were both generated in the same way.
And here is the final result.
Next on the agenda is Mr. Suzuki to talk about Fox Engine's shaders, lighting, and camera.
Thank you. I would now like to talk about linear workflow, shaders, lighting, and the camera.
Metal Gear Solid V uses a linear workflow. Without a linear workflow, physically based rendering would simply not be possible.
Many of you may already know about linear workflow, but I'll give a brief summary here.
And what we see here is the basic outline of how linear workflow works.
In conventional nonlinear workflows, values are rendered as is and then output to the monitor as is.
In a linear workflow, however, all values are rendered in linear space.
This is called gamma decoding, or D-gamma.
After the values are rendered, they are then changed back into monitor space, and this is called gamma correction.
And this is the basic tenet of linear workflow.
And the image on the left here was created in nonlinear workflow, while the right was done in linear workflow.
You can see the light in the right image is brighter when compared to the left image.
In reality, the far back wall would be receiving more light, as seen in the right-hand image.
In real life, the inverse square law determines the attenuation.
By not using linear workflow, you get a darker image when compared to real life.
And this example shows both non-linear workflow and linear workflow, showing an addition of brightness by a factor of 0.1.
Which is correct?
The example on the right, which uses linear workflow.
The value used is 0.1 in linear workflow, and linear workflow actually uses this value in the calculations to be successful.
However...
If you ever had a problem where adding multiple effects would wash things out, or, you know, would make things too white, I think many of us have had this problem.
Or maybe by making the light a little bit stronger, everything suddenly gets really bright.
The reason why this happens is because your monitor is not showing the correct value.
As seen here, TV monitors tend to exaggerate darkness.
For example, using the gamma value of 2.2, all values will be raised to the power of 2.2 and shown this way on the monitor.
In Windows, the sRGB rule is not exactly gamma 2.2, but it is very close.
For example, on a monitor using a gamma of 2.2, the value 0.1 will become a brightness of 0.006 when displayed.
So in order to get a brightness value of 0.1 on the monitor, you actually need to input a value of about 0.35.
Using Linear Workflow allows rendering using the correct values, and this is necessary for creating lights and textures using physically based values.
Okay then, next I'd like to give a simple introduction to the way diffuse albedo textures are created at Kojima Productions.
We use a photography room as pictured here in order to photograph our textures.
Modern cameras have image sensors that can capture images linearly.
In addition, we use a camera that supports a raw image format and use care to preserve linearity when developing the image by eliminating any unnecessary tone curves.
That way we can preserve linearity when upon delting the image.
So first we photograph a gray card that has a reflectance of 18% using a proper exposure.
Next, in the same environment, at the same camera settings, we photograph our desired texture.
We then develop it using the same linear processing settings as the gray card.
After developing, we further tweak the values that the RGB value of the gray card represents a brightness of 18%.
In this case, with a gamma of 2.2, the RGB values fall within the 117 or 255 range.
The same adjustments are applied to the desired texture.
Using this technique, you're able to get a photograph with the correct tone values.
As you can see here, there are various other details that you should be aware of and be cautious of, but for today, I won't go into too much detail.
After getting the corrected photograph, we then manually eliminate highlights and shadows to complete the texture.
And this is the basic flow we use for creating our textures.
And the materials we use are also physically based to a certain extent.
So, as you can see here, we can create various types of materials.
By creating presets, we can simplify the setting of numerous material parameters.
And this is the screen used to adjust those settings.
We observe the texture of various surfaces to evaluate our shaders and determine what parameters to use.
As seen here, we use a light attached to an arm to observe light hitting objects from various angles.
We use this method to create our various material presets.
Now, a new feature we've added is view-dependent roughness, which smooths out subtle roughness when looking at surfaces from the side.
This illusion occurs on certain rough, flat surfaces at particular angles.
As you can see in this photograph, the smaller the angle you view the surface from, the smoother the surface looks.
And this is an example of the effect simulated in Fox.
In this image, the black arrows indicate the viewing angle.
The white arrows indicate the direction of the visible reflected light.
If you view a surface head-on as seen in the top image, the reflected light becomes disrupted.
However, as the viewing angle and the direction of reflected light align, the occluded vectors are not seen by the viewer.
Therefore, as the viewing angle becomes closer to horizontal, disruptions in the reflected light become less apparent.
Also, due to the Fresnel effect, horizontal refractions are emphasized, enhancing the smoothing effect.
And next I would like to talk a bit about lighting.
The parameters we use for lighting are based on real-world parameters.
By doing this, it's easy to compare and contrast in-game lighting with real-world lighting, so that we can create lights easily.
The main units of measurement that our artists use to create light are luminous flux, illuminance, and luminance.
Also, the color of light is adjusted by using color temperature.
Illuminance is used for the sky and sun, luminous flux is used for artificial lights and flame, and luminance is used for monitors and other self-imaging models.
Light attenuation is simulated by using physically based techniques.
Therefore, when using a point light source, physically based attenuation is calculated using inverse square attenuation.
This is made possible only because we're using LWF.
Also, spotlights, etc., are placed like their real-world counterparts.
The more narrow the light, the brighter it gets.
And this sort of light can be simulated easily by adjusting the light distribution.
Ambient lighting is used to replicate bounced light.
The value of bounced light varies depending on the direction, and we set many of them within space.
Here's an example of ambient light placement.
Skylight is based primarily on a sky simulation.
This contributes to the ambient light.
Here's an example of what the passage of time looks like.
This is what it would look like during morning.
This is evening.
And this would be night.
The scattering effect of the atmosphere is also very important, and this has been replicated in the engine.
And now we'd like to give a brief demonstration of the 24-hour day-night cycle.
Camera parameters and color correction for each phase of the day is set to specific key values.
For example, afternoon would use a high exposure, while night would use a lower exposure.
Furthermore, when it becomes night, the color balance is made cooler and bluer.
And the auto-exposure used in Fox has been adjusted to only focus on lighting.
This kind of exposure is not real by any means, but it does have its uses when creating a game.
Normally, whatever image is seen by the camera determines the exposure.
However, this exposure is influenced too much by the material's brightness, and the resulting image looks like an amateur photograph.
So, let's take a look at a real-life demonstration.
This movie was taken by a camera with auto-exposure.
When zoomed in close, the black.
Cameras are able to distinguish between very black and very white things, but a camera cannot.
A camera is not able to distinguish whether an object is bright because it's white, or just because it's being lit up.
Objects that are very black are also the same.
By having the exposure focus only on the lighting element, hardly any influence comes from the material's color, and the result is a correct exposure.
By doing this, we are able to create a stable exposure.
The gain camera is set with parameters that a real-world camera uses, such as exposure, focal length, shutter speed, aperture, and so on.
Shutter speed influences motion blur, and aperture influences depth of field.
Exposure is set by the exposure value.
In order to simplify exposure settings, shutter speed and aperture do not influence the exposure.
Other basic features exist as well.
For example, motion blur, bloom, depth of field, effective focal length, and others.
We started using effective focal length for the first time on this project.
These images show, from the top left, depth of field, motion blur, flare, and color correction.
When using color correction, we use a 3D lookup table.
And now as you can see here, depending on where the camera's focus is, the focal length changes.
So you can see it changing focus here.
In other words, the field of view changes.
And here we have an example of a movie with the focal length changing in real life.
So this is an actual movie of real-life footage.
And here's the same example replicated in the Fox Engine.
All right, and that concludes this section of the presentation.
Now I'd like to hand the mic over to Junji Tago, who will talk about our graphics engine.
The artists have talked about lighting, shaders, asset creation, and the like.
How exactly does the Fox engine actually do all these things?
The rendering method Fox uses is deferred rendering.
One reason for using deferred rendering is that we wanted to be able to support a large number of lights.
Another is to keep a consistent look for both the characters and the environment, making the characters seem immersed in the environment.
Here are the steps taken in deferred rendering by the Fox engine.
First of all, the G-Buffer is generated with geometry to render.
The G-Buffer stores diffuse albedo, normal, velocity, depth.
specular albedo, roughness, translucency, and material ID.
The lighting is based on the normal, specular albedo, roughness, translucency, and material ID.
The kinds of lights that are supported in FOX are sunlight, ambient lights, point lights, and spot lights.
The ambient light is used as light and stored to the light accumulation buffer.
Diffuse and specular are stored separately in the light accumulation buffer.
The lighting result is generated by combining the light accumulation buffer and diffuse albedo.
Fog is then combined with the lighting result.
Finally, in post-processing, motion blur, depth of field, tone mapping, and so on are calculated and used in the final output.
During tone mapping, auto-exposure is applied by referring to the diffuse element stored in the light accumulation buffer.
By only referring to the diffuse element, we are able to achieve a stable auto-exposure.
The lighting model is based on the familiar Blinn-Fong model.
The material parameters are determined from references, such as photographs.
The parameters that are able to be extracted from references are the parameters we chose.
Therefore, we use a model that has diffuse and specular separated.
For diffuse, we use diffuse albedo, and for specular, we use specular albedo and roughness as material parameters.
Diffuse albedo is created from photographs captured in linear space.
Specular is created from the refraction index of materials or from photographs captured from various light angles, like the picture on the right.
Specular is generated from the roughness and then normalized.
This keeps the highlights looking natural.
We also support translucent material in the deferred rendering model.
All lighting shaders have support for translucency.
The reason why we support translucency in our lighting model is because characters play an important role in our games.
Also, subsurface scattering is aggressively approximated, evading a potential bottleneck.
This approximation is inspired by half-Lambert shading.
Some examples that translucency is used for are skin, hair, cloth, and vegetation.
And here is an example of skin using translucency.
This is with translucency off, and with it on.
You can see the softness of the shadows near his nose.
So off, and on.
Here's an example using hair.
This image is with translucency off.
and with it on.
You can see how the light is shown through the hair.
So off, and on.
In deferred rendering, the number of material parameters stored in the G-Buffer is limited.
In order to overcome this limitation, we write the material ID to the G-Buffer, and during lighting, the parameter texture is then referenced from the material ID.
For each unique material, the parameters hold different values.
The stored parameters are ones related to Fresnel and translucency and specular color.
In the figure shown, all models are the same, but each has a different material ID.
Only by changing the material ID, we are able to change the type of metal the object is.
Since specular color is also stored to the material texture, the colored specular particular to the metal is shown.
We are able to achieve a natural specular look just by adjusting the roughness.
The images on the top and bottom have the same material ID, but different roughness.
The guns on the top are glossy, and the guns on the bottom have a matte surface.
Roughness is stored to the G-Buffer, so by changing the roughness after the G-Buffer renders, it is possible and easy to produce some effects.
By lowering the value of the roughness, the entire screen takes on a wet look.
Using deferred rendering does pose one big problem.
It only supports a single lighting model.
In order to produce various shaders, we have to tweak the value of the G-buffer.
Some shaders that have tweaked values are eyes, anisotropic specular surfaces, and view-dependent roughness.
These calculations are not accurate, but cause no practical problems.
This image shows eyes drawn with the eye shader off.
and with it on.
If you look at the image of the eye on the bottom left, you can see the refraction in the eye.
So off and on.
The left image here has anisotropic specular disabled, while the center and right images have anisotropic specular enabled.
When the G-buffer is generated, specular albedo is tweaked to take the shape of anisotropic specular, causing it to look real.
This method is a practical way of achieving this look.
This image shows the results of view-dependent lab work.
Depending on the angle between the view and normal vectors, the specular value is sharper the more parallel the view vector is to the surface.
Here is an image of the roughness value stored in the G-Buffer.
Looking at the wall on the right, you can see how the roughness gets darker the further back you look.
We use light probes for ambient lighting, and they are placed by the artist.
The effective range is set by a bounding box.
Cube maps are captured in the level editor, and irradiance is generated from the cube maps.
Irradiance is stored as second-order spherical harmonics.
Light probes are rendered as light when the light accumulation buffer is generated.
Since there are many light probes, they are rendered in quarter resolution.
The ambient light can also be blended between probes, so the boundary between them is quite smooth.
As the sky plays a very important role for the ambient lighting, we simulate atmospheric scattering.
The results of the scattering simulation become the actual sky in fog.
The sky is included when the cube maps get captured for light probes, and these results are then stored in the light probe.
Ambient light supports the day-night cycle.
Light probes exist for major transition points in a 24-hour date.
The values between these points are interpolated.
Ambient light also supports the weather cycle.
Light probes exist for each major weather condition.
The weather is represented as a one-dimensional parameter, from sunny to cloudy to rainy to stormy.
The values between weather conditions are also interpolated.
Here is a short demonstration of how cube maps are captured.
You can see how bright the environment became when the cube map was generated.
The trailer we released last year of Ground Zeroes took place at night, in the rain.
Here is a demonstration of the same trailer, but taking place during the day and sunny.
All the materials used have not been changed, only the light.
Since the material parameters are correct, the visuals do not break down even when the light is changed.
Kept you waiting, huh?
Sorry, my snake voice was not very good.
All right.
Up to this point, our presentation has focused on the digital or technical aspects of game creation.
But I'd like to shift gears a bit and turn our focus back to the analog, or real world.
To get physically accurate assets, artists need to observe and have knowledge of our actual physical world.
We need to know things about lighting, cameras, surface textures, and how they are represented.
Even in simply capturing textures or models, artists need a thorough understanding of the input devices used.
But more importantly, it is good to remember that we have many real-world examples within reach.
For example, on the left is a shirt that one of our staff cut up in order to understand the construction of clothing.
This was very beneficial in understanding clothing patterns.
Even though we have a marvelous designer to do most of the dirty work, it is still a great way to understand modeling on a deeper level.
And the dirty yellow shirt in the center was us trying to simulate Chico's muddy clothes.
An artist actually went into the park in the middle of the rain and played in the mud in order to do this.
And you could only imagine the look of utter horror on the face of this poor old woman who happened to see him doing this.
And on the right, we have the top of an artist's desk.
There are various items scattered about, branches, rocks, and the like, and these are used to study textures.
Ironically, the more that technology evolves, the more we have to study and understand our physical surroundings.
In that way, you could say that technology is a very important tool for helping us understand our real world.
Even with regard to lighting, we've had to re-familiarize ourselves in the way that real light works in the real world.
The majority of our artists have an idea of how lighting basics like key, fill, and rim lighting work in a CG environment, but not necessarily in the real world.
But luckily, we had lighting director Yoshiharu Nishiyama, famous for his work on commercials and movies, who blessed us with his expertise.
He demonstrated various real-world lighting techniques to our team and taught us how to use these techniques in our game.
He has been to our studio to see our lighting in action within the Fox engine and has given us useful guidance.
And here we see some pictures from a lighting study.
And you can see that simply adjusting the light dramatically changes the mood and feel of the scene.
As mentioned previously, it's interesting how advancements in game lighting have necessitated a deeper understanding of how real-life lighting materials and reflectance works.
But it's also important to note that even though we can mimic reality, creating captivating visuals may sometimes necessitate breaking from reality in order to exaggerate certain things.
Just because something is physically correct and realistic doesn't mean that it'll look good.
Therefore, we shouldn't always strive for complete realism, but by using real-world simulations as a starting point, our artists can use their skills to meet their visual goals in a convincing way.
So, what I'm trying to say is that in order to get realistic, physically accurate output, you need to study the real world.
Observe with your eyes, capture with cameras and 3D scanners, measure the details of the world around you.
To reproduce the real world, you need to study the real world.
At the same time, however, creating captivating visuals is an artist's primary job.
Perfectly replicating the real world may not necessarily be what a game needs.
So to make a quality product, an artist's eye is essential.
Simply reproducing reality would only be a traced image.
So basically, yes, you really need an artist's eye in order to do something great.
And these two aspects, realism and artistic quality, within the context of a game, are what we mean by photorealism through the eyes of a fox.
Thank you for listening.
But wait, there's more.
Everyone, please welcome the studio head of Kojima Productions, LA studio, Tom Sekine.
Applause Hello everyone. This is kind of different topics.
Kojima Productions has been well known for always make high quality products and always surprise the people, to the excited people, multi-generation, multi-generation, multi-generation of all those parents and kids, my generation.
Anyway, There was talk about Kojima Arts production has been built in Los Angeles area.
And as you've seen, all those recruitment advertisements showing in our website and also Gamasutras.
However, this is, as of today, it is official.
And our studio is really building Los Angeles area.
And this is our new logo.
Red Fox was born from the fox.
Kojima production philosophy is surprise and also excited people with a high value entertainment product surfaces the generation boundaries and always making a really high quality product bring to the users.
And also create a product with a critical acclaim and also succeeding commercially.
This is the Kojima production philosophy.
So we inherited this core philosophy and tried building a unique studio and a new culture in Los Angeles.
So our vision is create the highest quality interactive entertainment, which brings a deep emotional feeling.
This is a must.
This is a combination of all those new technologies and art, and also a unique game design element.
To bring in all those three components is equally...
putting together to make a really great, high-value entertainment product.
Without touching people's emotions, we cannot survive.
We cannot keep surprising people to the next generation.
So our mission is to become the top studio in the world.
This is very ambitious, but once we're building up very competitive market in the Los Angeles area, we have to become the number one, one of the best development studios, as like Parents Studio and Kusuma Production Japan.
Our value is quality.
Quality is so important, not compromise to the quality.
Game experience comes first.
This is no compromise.
And then the collaboration is so important.
Always cutting edge technology and also innovative art style is keep changing all art styles and touch the people's feeling, emotional touch.
And also evolving interactive design elements by promoting a creative environment.
So technology, the programming.
and art and design and the production management.
Those components are quite important for us and the collaboration work is very important to us.
So we are hiring the best people possible and also give the best creative environment.
So we are building our Los Angeles studio in the South Hurricanes campus in Playa Vista, Los Angeles.
Studio design concept is open communication and collaboration.
The space is about 30,000 square footage.
We'll have a testing room and also a screening room.
So this is the screening room and the next one.
We're hiring.
So we are hiring.
Please join us.
You guys can buy our booth in Recruitment Events space in the South Hall, I believe.
The booth number is CP2308.
We really need good people.
And we really want to push the next envelope to make a great product.
So please come to join us.
Thank you very much.
That's all.
Unfortunately, we're not recruiting at Moby Dick Studios.
This concludes our session for the Fox Engine.
Thank you for waiting and checking out the session with us.
In 2009, I gave a keynote speech titled, Solid Game Design, Making the Impossible Possible.
Maybe some of our guests today attended or listened to the presentation.
And maybe you have checked it out that time too.
And at the keynote, I presented the fact how to make sure that technology is moving forward and how to make the idea move alongside the point where the advancement of technology and hardware is used as a ladder to make the impossible possible.
I stated that I used an idea on top of a platform to make sure that we rose above with the game design and move from there.
At the time, I said that instead of that, we should use Western-style technology and put a platform for technology and put a platform for game design on top of that platform to make a more accordant game.
And finally, I mentioned, we would use the advancement in technology from the West to further elevate ourselves as well.
It's been four years since that day, and I believe we have built that ladder we call the Fox Engine.
It's been four years since that day, and I believe we have built that ladder we call the Fox Engine.
It's not about making the game design impossible.
We want to make technology impossible, but we want to put game design on top of it, and aim to make games that are more interesting and more high-level.
From here on out, we will make the impossible possible using game design only.
We'll make the impossible possible with technology, and then add on good ideas using good sense to formulate solid game design.
And on top of that, not just technology, but we would like to incorporate storytelling ideas and anything that would really work with games from the filming industry and novels as well, and take that and move forward and elevate ourselves that way too.
And we take in these things because we're not epic Metal Gear Solid.
It's because we're Metal Gear Solid.
Oh, and I also forgot to mention that we're hiring as well.
Please come by if you want to help us make a new MGS on Fox Engine.
Thank you for coming to this conference. Thank you. Thank you so much.
APPLAUSE Okay, so it looks like we have a few minutes left, so we're going to have a quick Q&A.
If we could strictly keep it technical, we'd love to answer your questions.
So Josh, go ahead and please take over here.
I think we have some microphones set up on the side over here.
So if anybody wants to ask any questions about technical thing, we got somebody.
OK, go ahead.
Hello.
Very impressive presentation.
Thank you.
First question, my.
Can you speak a little bit closer to the microphone?
OK.
My first question is, since you use such a big ranger, but you focus on the lighting to do the adaptation.
Then how can you avoid the image in some place, it's pure black and affect the gameplay?
I didn't catch the last part of the question, I'm sorry.
Okay, it's like that. The range is so big.
Okay.
And you've used the light as a reference for the eye application.
Then, it may cause the problem is the image is super dark and the player cannot play it.
It's the main problem when we develop the game.
The artist feel, ok, it's ok, the image looks good, but for the game player, they can't play it.
Because in some players, for example, the NPC is too dark.
Okay, let me kind of rephrase what maybe I'm thinking you're asking.
If we keep light real, sometimes we might run into places that are too dark, and that doesn't suit the game.
And how do we go about overcoming that, is what you're asking?
Yeah, on the technique side.
Okay, let me kind of ask real quick.
If you place a light that has a certain reality, if it doesn't match the game, for example, there are some parts that are too dark, and it doesn't match the gameplay, so how do you solve that problem?
Is there a way?
Change the level.
Change the level.
And match it to the level design?
Match it to the level that needs the game.
For these kinds of places where the light is too dark in some areas, we usually talk to the level designers or the game designers who are creating those levels to make the stage fit with that lighting style.
And it's kind of a back and forth between the lighting designer and the level artist, or the level designer to get a good level to show.
Does that answer your question?
Okay, I understand. Thank you.
The second question, yes.
I just want to...
No.
Okay.
Okay.
I'm sorry.
Sorry about that.
No.
Over here?
Yeah.
Yeah.
First of all, congratulations on the Fox Engine.
Thank you very much.
It looks really good.
And the second question is, Photoscan looks really cool, but how much detail you have to sacrifice when you make that model, which is hundreds of thousands of polygons, into an actually in-game asset?
OK, I understand.
Let me go ahead and repeat the question, if anybody.
He's asking, when generating a model in PhotoScan, it comes with tons of polygons.
How much detail do we have to sacrifice when actually getting it in the game?
Yeah, exactly.
I'm asking, when generating a model in PhotoScan, it comes with tons of polygons.
How much detail do we have to sacrifice when actually getting it in the game?
Yeah, exactly.
We use a normal map in order and bake the normal map in order to overcome this limitation.
Okay.
Yeah.
I was thinking more about polygon count, exactly.
Oh, okay.
How do you calculate the number of polygons?
The ones that are made by photo scanning?
Yes, the ones that are made by photo scanning are full of polygons.
You mean photo scan model or actual game model?
If you guys want to, so you can ask other questions, we can talk about it later.
Alright, that's okay. We'll probably catch you later then.
All right, cool.
Thank you.
Thank you.
Thank you.
I think we have time for maybe one more, if that's OK.
I think we've got some people lined up.
But go ahead.
Just have a really quick question about the actual deferred rendering.
Can you speak a little bit closer into the microphone?
I noticed you have a lot of different buffers for your gbuffer.
How are you actually packing those before you send those to the graphics card?
SPEAKER 3 IN JAPANESE SPEAKER 4 IN JAPANESE You want to answer?
No.
If you cannot talk about it, that's all right.
No, no, no, it's OK.
He's trying to think about how to go about best answering your question.
All right.
Yo, material ID.
It's got a little bit of a custom head.
Well, first of all, we have the material ID.
And we put a lot of parameters into the material ID.
So that takes away a lot of those other rendering, makes the rendering pipeline a little bit simpler.
OK, so we're going to eat.
is his answer.
I'm a little more thinking about as like the Crytek engine packs everything into three buffers.
OK.
I noticed they have six different buffers for the Fox engine.
Are you packing that into like two or three different G buffers as well?
So you're using multiple G buffers?
So Crytek uses all of them?
Four.
Four.
Four.
There's four of them.
So we're using four multi-render targets.
We are using four multi-rendered targets.
Is, what are you saying?
That answers my question, actually.
Say again?
That actually perfectly answers my question.
OK, well there we go.
Awesome.
How much time do we have left?
We have like five more minutes left, but if you'd like to.
OK, we can get one more.
And I think this guy's over here.
You go ahead.
I have a question about photo scanning, too.
How you get specular and rawness information to the photo scanning?
the specular and what else?
Roughness.
Roughness?
We do not use PhotoScan for getting the specular and roughness.
as much as we're going to say.
So it's difficult to get the specular and roughness from Photoscan, so we don't use Photoscan for that purpose.
OK.
I think that's going to be it for the Q&A session.
So thank you guys for coming out, and have a good rest of the day.
