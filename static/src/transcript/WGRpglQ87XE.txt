All right, good morning everyone.
Thanks for coming, it's very kind.
Again, a little bit of housekeeping before we begin.
Please set your phones and noisy things to silent airplane mode.
We'd ask that you please complete your post-session surveys after this session ends.
We've left some time for questions and answers at the end of this particular session.
Please keep those questions concise so that we can answer as many as we can.
And lastly, if there's lots of questions or you want to chat to our lovely speakers at the end, the post-session area where you can chat to all of us is actually just outside this room out there in the corridor.
So we'll be there to have a quick chat if you'd like.
Okay, good morning. Hello.
My name is Sebastian Long. I'm Director of Player Research, a games user research agency, a player data...
consultancy.
And it's an honor to gather and introduce these four fantastic talks on improving retention, specifically how player data can make better games and game design choices, leading to players sticking around in our games for longer.
We all want to make games that players love for a long time.
To do so, to ensure we design a game that holds players' attention for months, we cannot leave game design to chance, nor to intuition, nor to uncertainty.
And the bigger our player bases get, and the longer they play, the more distant players' real experiences get from our own assumptions or expectations.
Player data has always been used to combat these assumptions.
The more we know of our players' journey through our games, the more deliberate we can be to achieve good design and keep them on that desired journey through user tests, through player interviews, through surveys and polls, in-game telemetry, and a whole toolkit of data-gathering research techniques.
Applying player data to game development has never been more widespread than today in this era of live games and evolving games and connected devices.
where retention is measured more like in months than in minutes.
But getting rich insight from players, our existing player base, is really tough.
And researching potential retention before public launch is tougher still.
So in this session of MicroTalks, we're going to take four case studies in using player data to explore retention toward building a game that players love to come back to again and again.
In each talk, you'll be hearing from games user experience researchers.
So let's talk about that first.
Games user experience researcher is a professional game development role dedicated to gathering and analyzing player data.
To inform game design choices, games UX researchers design and conduct research studies to generate player data, data that is actionable, that is rigorous, that is on time, and that is persuasive.
Design teams, perhaps yourselves, will roll that data into your decision-making toward better design.
It's sometimes player data that inspires new game design ideas to be inspired by our players because we know them better, their desires, their motivations, their spending.
It's sometimes data that continuously improves game design towards continuous improvement through iteration by better understanding our players' interactions with our work-in-progress designs that we have.
Games user experience researchers, like us, typically bring background in psychology, cognitive science, UX, anthropology, and similar.
These backgrounds in research ensure that the data we generate is rigorous and can be trusted by the game development teams to affect all parts of game development.
As a user experience researcher myself, I often find that player data means many things to many people.
So before we hear from these case studies, let's talk about data.
Player data is more than just analytics.
It's more than just surveys.
It's more than just app store comments.
In fact, there are more than 30 formalized ways of getting data from players.
Each of these information gathering techniques has strengths and weaknesses.
For example, analytics is fantastic at telling us what players did in our game, but it's often very poor at telling us why they did that.
Eye tracking is good for telling us where players were looking at the screen, but in isolation can't tell us whether they understood what they saw.
The primary role of a games user experience researcher like us is to choose from these many research techniques and match them to the game development challenges you face.
And it's a universal truth that using just one of these, only one, leads to blind spots.
So it's not just ideal to use more than one, but necessary to get good data to inform design.
In research terms, each of these is called a method.
And when we combine and triangulate using multiple sources of data, that's called mixed methods.
Here, we're talking about retention in this panel.
So we'll be focusing on these research approaches, ones that are designed to gather data over long periods of time, that is rich, actionable, and useful to you.
Although we'll touch on other methods as well.
In each talk, you'll be hearing advice from these guys about why player data was needed, which methods were used, and the impact each of their studies had on their games.
To help bring these learnings back to your own teams, I propose considering them through these four lenses.
Firstly, consider how these studies were made rigorous.
We captured sufficient data.
We reduced biases in that data.
Secondly, how they were made actionable, ensuring they informed specific design choices.
Thirdly, how they were made to be on time, these studies we're gonna hear about.
How are they made to be done not just quickly, but cheaply, using resources we already had, and critically, at the right time during development?
And lastly, how they were made to be persuasive, to ensure that the data they generated was efficiently actioned by the development team.
These four topics are the hallmarks of good player data.
OK, please allow me to introduce our first speaker.
It's Alex Gehorst from Glue.
Thank you all for this opportunity.
It's a pleasure to be here and to share some insights that I, along with the teams at Glue and CrowdStar, learned in one of our projects last year, focused on understanding the first-time user experience and improving retention, specifically in our game, Design Home.
Before diving into the details of this case study, I wanted to share some background information for anyone who may not be familiar with this game.
Design Home is a free-to-play mobile game that gives its users opportunities to release through creative spirits, through interior decorating and designing.
Through partnerships with many top brands, users are provided the opportunity to design spaces, including but not limited to living rooms, dining rooms, and bedrooms, using furniture and decor that is representative of real life.
To provide more context for this talk, it's necessary to share some information about the core loop within Design Home.
Design homes core loop is all about room design challenges.
Players are presented with five or so challenges every day, and each challenge has its own description to prompt players with ideas about how to style a given room.
These challenges can be separated into two main categories, those with design requirements and those without.
For example, in the House Hunters Challenge, that is shown on this slide, the perfect painted lady, players need to design a living room with eight required items and up to eight additional optional items.
And players were required to design this room with four eclectic styled items along with a blue sofa.
Last year, our research team worked hand in hand with the design home team on a project aimed specifically to improve early day retention, targeting days two through seven.
Typically in free-to-play mobile games, you'll see retention curves similar to the one on this slide, where there's high churn by day two, but with a flattening tail with each day after that.
Usually in free-to-play games, retention has a cascading effect, where the more users that are retained earlier on, the more users that are retained later.
So using this chart as an example, if you were to improve day two retention from 35 to 40 percent.
Retention on days three through seven should see improvements as well, which is exactly what we in the Design Home team were trying to do.
So now that we knew that we wanted to improve our early day retention, how would we do that?
We hypothesized that improvements to the first time user experience would address this, and so the team designed and created a new Fatui, which would focus on teaching players core interactions earlier in their experience.
For this project, we relied on two primary methods.
The first was play testing, where we watched how players interacted with the first-time user experience design, noting any usability concerns, along with noting what players did upon reaching the core loop after the onboarding process.
In addition to this, the second method we relied on was A-B testing to compare different versions of the first-time user experience.
For playtesting, we partnered with our vendor, Playtest Cloud, which allowed us to recruit our target audience, as well as to distribute builds and watch how players interacted with different Fatui designs.
We were able to recruit our target audience, which was casual gamers that had no previous design home experience.
We asked that players would play Design Home for approximately 20 minutes, and we wanted to see how players would go through their first session in a completely natural way to assess how they interacted with the Vittui, as well as to see if they were able to recall what they learned once these players reached the core loop.
Because we wanted a very natural gameplay, we provided no additional context and no additional tasks outside of the onboarding process and what was already in the tutorial flow.
For this project, it was broken out into four phases, three particularly that were focused on playtesting different iterations of the Vitui, as well as an A-B testing phase.
When we tested the current design of the Vitui, we noted that we had two main issues.
The first issue was that players didn't fully understand the context for designing rooms.
And that players would place too much emphasis on certain keywords that were in the challenge descriptions.
In the example shown on this slide, the description told players to design an eco-friendly living room.
During testing, we witnessed several players become hyper-focused on this eco-friendly keyword.
We noticed that these players would scroll and scroll through furniture, endlessly looking for any sort of furniture item, whether it was a couch, a table, or a side table, that was eco-friendly, even though within Design Home Game, none of these furniture items were tagged as being eco-friendly.
The second issue that we witnessed was that players who moved on to other challenges did not notice that some of them had specific requirements.
Players who did not notice the requirements would design some very beautiful rooms like the one in the slide, but unfortunately would be met with a notice that told them requirements had not been met when they went to submit their designs.
Not only was this a very frustrating experience for these players, but what made matters worse was that these players had the potential to spend most, if not all, of their currency.
The first prompt states that this is a game development transcript.
We hypothesized that these issues stem from the short and straightforward design of the original Fatui.
Players were introduced to the game and taught how to choose and place furniture and decor within a room, but they are not taught specific aspects of the core loop, including requirements and designing within constraints.
Based on this hypothesis, the team redesigned the Fatui to teach players additional fundamentals and interactions that they would encounter when reaching the core loop.
In the new version of the first time user experience, the major change consisted of adding two new additional tutorial challenges.
These additional challenges were added to teach players key features and interactions, including inventory management in the second challenge, and understanding challenge requirements and how to use filters to optimally find furniture in the third challenge.
When the design home team was ready with the redesigned Fatui, we set certain criteria or success factors that players needed to demonstrate during playtesting before we would integrate this new Fatui design into the live game.
The first success factor was that all players should notice requirements, and they should notice this before they try to submit a room design.
The second success factor was that we wanted to see if players used filters to optimally refine furniture for the designs within any given challenge.
When we tested the newly designed Fatui 2.0, we encountered the same requirements issue that we saw when we originally tested the first design.
Players were still not recognizing that challenges in the core loop had requirements, and they still weren't using filters to optimally find furniture and decor.
So why were players not recalling that there were requirements within challenges?
We hypothesized that this issue was caused by the interaction design of the requirements and filters tutorial.
Essentially, this tutorial was guiding players to specifically just press a button.
They weren't really retaining information about filters or requirements, which is supposed to be the meaning behind that particular interaction.
So the team iterated on the design, specifically the third tutorial challenge, and revised it to include a more granular approach to filtering, where players will walk through the individual steps of seeing that the room has requirements.
Opening the filters menu, noticing the variety of filters available such as brand, color, style and even the classier requirements filter options to streamline their experience and then finally removing the filter once they had already met the requirements.
After making these changes and retesting the design, all players within the playtest demonstrated understanding of filters and requirements.
At this time, the team had also addressed some other issues that are outside of the scope of this talk, but it was at this point that we felt confident in the design and we wanted to release this to our player base.
So after making these changes and retesting the design...
So at this point, the team set up an A-B test with two different versions of the Fatui.
And to mitigate risks, traffic in the live game was split with 95% of our new users going to the old Fatui design and 5% receiving the new Fatui variation.
And if positive performance was recorded during the A-B test, we planned to increase traffic to the new Fatui.
The Data PM team on Design Home then tracked retention rates, specifically looking at different cohorts, and also implemented event tags to track how many users were reaching each step of the Fatui.
Diving into the data, what we saw when we measured day two retention was that we had significant lifts across all day two cohorts, suggesting that the redesign to the Fatui created a better user experience that retained players.
When we evaluated retention at day seven, we saw a slight decrease from the lift scene in day two, which makes sense as other factors, such as economy design and weather players are progressing, could be having more influence at this point.
But we still saw significant lifts in retention at day seven as well.
I mentioned previously that we implemented event tagging to do granular tracking and funnel tracking during Fatui.
Anytime you're testing a first-time user experience, tracking each step of the funnel will uncover insights on which areas can be addressed that will also have the largest impacts to potentially retaining players.
In our case study, during A-B testing, we uncovered two major churn points, which happened to correlate with two usability issues that we saw during player testing.
Unfortunately, due to the time constraints of this talk, I can't elaborate on these.
However, the point I would like you all to take away is that by playtesting the first-time user experience design before implementing this in the live game, we were able to have insight into what was causing these turnpoints in the funnel.
The team had specific direction that they could take with data to further improve the first-time user experience, which would then incrementally improve retention.
So in summary, by taking a test, iterate, and retest approach with the first time user experience, we were able to prove early day retention both at day two and day seven by targeting specific areas within the first time user experience.
So thank you all.
And at this time, I would like to introduce our next speaker, Molly.
Hello everyone. My name is Molly Sarota. I'm the lead researcher at the Sony San Diego studio and I'm going to continue our talk on retention.
Okay, so specifically what I'm going to talk about is leveraging a method known as a diary study to study retention and I just want to get a feel for the room. How many people raise your hands if you're not a researcher.
Okay, it'll help me understand how to articulate what we do at the San Diego studio.
So this is going to be a high-level view of how to run a diary study, but then I'll get into the details.
So in the beginning, you're going to recruit a handful of users.
It'll depend on your game and what your question is.
For us, we typically recruit around 50 people, a mix of spenders and non-spenders, a mix of first-time users and experienced users.
And we send them a virtual diary, which is pretty much a survey link, but it's not a survey, it's an open diary.
And users play your game, then enter feedback in this virtual diary, and then you read the feedback that's coming in.
And then that loop continues.
They play your game, they give feedback through this virtual diary.
You monitor data that's coming in.
And the benefit of a diary study is that it is leveraged to see how your game plays out or unfolds for the user over an extended period of time.
So depending on your question, some people run diary studies for a few days or a week.
We have a tendency to run diary studies for three to four months.
And that's because I work on a baseball game, MLB The Show.
And what we're trying to do is capture our audience for as long of the baseball season as we can.
But there are other benefits to running a longer diary study.
For one, you only spend money the first time you recruit people to this study.
And now you have a user base to constantly tap into any time questions come up in your studio about your game.
So let me get into kind of the details behind a diary study.
For one your diary study is it gets at really the end benefit is that extended period of time.
To outline it is you're encouraging your users to enter data on a daily-ish basis.
Really it's just whenever they're playing their game, your game, and it's any time that they're playing your game in their own environment.
You're not bringing users into your studio or into your office to test.
They are playing in their own environment and giving you feedback when they're actually experiencing it.
The other thing with diary studies is that as a researcher, you're giving them a suggestion of a topic to think about for the diary, like gameplay or customization.
But other than that, it's up to the user to then decide what aspect of customization or gameplay they're going to discuss.
And the benefit of this is instead of using a survey where you're specifying the topic that users have to give feedback on.
You're now getting feedback on what's most important to the user in their experience.
And the strength of this is that sometimes when you're designing a survey, you are selfishly putting in questions on topics that you want feedback on, but you might not realize that there are other topics affecting the user experience that you didn't even know to ask a question on.
And that's another beauty of a diary study.
The other thing is that you are getting real-time feedback.
on the user's actual experience.
When you bring users on site to your studio, you are trying to simulate a gamer experience.
With a diary study, they're allowed to play on their own time in their own environment.
So you're getting feedback on their actual experience in their real world environment.
Now as was previously mentioned, with any research method there's strengths and weaknesses.
So this is going to be an outline of how you can strengthen the areas of a diary study that have some blind spots.
So one thing about a diary study is that it might not always address the topic that you were hoping people would discuss.
For example, again, I work on a baseball video game.
Maybe there's some discussions that day in my office where people want to know is something going on with hitting, are people struggling with hitting.
And then I look at the diary feedback and everybody's talking about pitching.
You have already recruited this panel of users.
You don't have to pay to re-recruit.
You don't have to wait to find the people.
They're already available to you.
So you can quickly send out a survey with closed-ended questions that get at any area in the diary study that was missed from hearing from people.
The other thing about diary studies is that it depends on the participant in terms of how articulate they are in the feedback that they give you.
So there are times where their information can be vague or just misunderstood.
And let's say you're reading diary feedback coming in and you can see that people are struggling with a certain aspect of gameplay, but you can't exactly tell why.
Again, these people are already recruited, they're already available to you.
Hop on the phone.
You can call these people and ask more detail.
You can hop on Skype and have them video stream gameplay if you need to see what they're doing.
If they're local, you can bring them into the office at this point and get follow-up feedback on where the issue is actually occurring for them.
The other aspect of a diary study and really with any type of qualitative research.
is that you can't tell the frequency at which it is happening, and that's because the sample sizes are smaller.
So again, I said that we typically recruit 40 to 50 people for our diary studies, but we have millions of players.
So it's not going to tell us at what frequency is this issue occurring.
For how many of our users is this issue occurring?
So if in your diaries you're seeing that people are struggling getting into a certain area if you're in your game.
Then reflect back on telemetry data so that you can see to what degree is this impacting everybody who's playing our game.
So now I'm going to get into specifically what we've done with MLB The Show to study retention.
So MLB The Show is a game that releases once a year.
But we do have live content that we release every few weeks in the form of baseball themed events and the whole purpose of these baseball themed events is to offer new and different ways for users to play our games.
So that they continue to play our game.
What's shown up here is an example of one of the events that we put out to users.
And it encouraged users to build a baseball team made up of just baseball players that are of the age of 25 or under.
Then they go online and play against users who have created similar teams.
So again, the point of these events for MLB The Show is to enhance retention.
So as a researcher, I come in with a diary study to see, is it working?
Are we actually retaining people with these events?
And if not, why not?
So let me talk you through what happened in the beginning of our study.
So the 25 and under event was the very first event that we launched with the launch of our game.
And what happened in the diary feedback that we started getting is people were talking about the theme of creating a baseball team with baseball players who were of the age of 25 and under, and they liked it.
Great.
Motivated just to check out the event.
We want that.
They liked the rewards.
They wanted the rewards.
Great.
We're still working towards retention because we're capturing people's interest.
But from there on out, we kind of went downhill.
Users started to try and create a team of baseball players that were of the age of 25 or under and could not for the life of them figure out how old people were.
This information was, you had to dig, really dig on a player by player basis to find out how old they were.
And so it got to the point where users gave up.
We thought we were gonna be getting data in the diary study on gameplay.
How did this theme or this particular experience play out for users?
Did they keep playing to get more rewards?
Were we keeping their attention?
And then we found out people couldn't even get in to play the event.
So what do we do with that type of information?
We create the next event off of everything that we learned from the previous event.
So what we did was One thing that happened with the first event is that it launched with the beginning launch of our game, which means that our user space inventory was very small at that point.
Even if they could figure out how old baseball players were, they didn't have a lot of their own inventory to pull from, which means we're suddenly pushing a lot of people to the marketplace, which was not going over well with people.
So we changed the second event to work better with the inventory that users already had available to them.
The second event had users build up a baseball team that had to do with right-handed hitters and left-handed pitchers.
This is something that the designers know is very easy for users to do at an early stage in our release of our game.
The other thing that they did with the data that came in from the diary study is they put out a patch to quickly address this aspect of having no idea which players to build your baseball team off of because they don't know who qualifies and who doesn't.
Well now there's an icon that goes on any player within the baseball game that identifies them for the event going on.
The second that event closes, that icon goes away and then comes back up on any player that then matches the qualifications for the next event that opens up.
The other thing that designers did was they leveraged information that the user was going to need to build their baseball team with aspects that were front and center on a baseball card.
Whether a baseball player is left-handed or right-handed, it's on the front of a baseball card.
This was information that was going to be easy for the users to identify.
And what ended up happening is at this point, we're seeing positive feedback coming in through the diary study when comparing When comparing how their experience went with the first event.
But we also look at telemetry data and we see that we've more than doubled the amount of people who are able to actually get into the event in the first place.
So in sum, the thing with diary studies is that it allows you to study your users and your gameplay experience over an extended period of time.
So that even if your game is testing well when you bring them on site for a play test, for an eight-hour play test, a two-day play test.
A diary study allows you to see how your game experience unfolds over time and to not only identify where people are dropping off, but why people are dropping off of your game.
And for us, it allowed us to quickly iterate so that we could minimize the amount of people that we were losing and therefore impact.
that our ability to retain users' attention, to retain users' interest in our game.
And with that, I will now introduce Jen.
Hi everyone.
My name is Jennifer Torson and I am a user researcher over at Scopely.
I'm a user researcher at Scopely.
I'm a user researcher at Scopely.
I'm a user researcher at Scopely.
I'm a user researcher at Scopely.
I'm a user researcher at Scopely.
Scopely is the publisher of free-to-play mobile games and we have a really diverse portfolio from casual word games to RPGs to 4X strategy.
Research is a really important part of the development process at Scopely and we have a team of very serious and professional researchers working on all of our titles.
At Scopely, our research team works to deliver actionable insights at every stage of development, from early concept to live operations.
Because of this, we believe in researching games holistically, trying to understand how every facet of the game contributes to its overall long-term experience.
One of the ways we do this is by running longitudinal, or long-term, studies, where we conduct research that looks at play experiences over a span of days or weeks rather than a single session.
While that may sound like it would take a huge amount of work, the reality is long-term studies are easy to set up and run.
They only need an extra day or two of research or time than a traditional usability test would, and the benefits are huge, giving us really deep insights into the game.
Today I'm going to talk about two cases where we use these tests for great results.
First we're going to look at Star Trek Fleet Command, our mobile 4X strategy game.
During that game's development we used longitudinal testing to understand player motivations and progression during the first week.
Next we'll look at The Walking Dead Road to Survival where we tested post-launch to redesign the way we taught players about characters during the tutorial to help increase retention down the line.
So longitudinal studies at Scopely typically take place in three phases and use a variety of qualitative research methods.
The first phase is an on-site usability test.
These are pretty standard sessions that are widely used in the industry.
We recruit about six people to come into our office and play the game one-on-one while a moderator sits beside them to observe and talk about their experience.
And this is where we can capture initial impressions about the game as well as any usability issues that come up during the first hour of play.
Our second phase of testing is a diary study.
So after they finish their usability session, we send players home with the game installed on their personal phones and ask them to play every day for the course of a week or two weeks, usually about 30 to 60 minutes a day.
We might let them do this as self-directed free play if we want them to just kind of test the overall experience, or we might ask them to do specific tasks every day, just kind of depending on what we're trying to look at.
Players are then sent a survey via email every evening and asked to record their experiences from the day.
What they did, how they felt about the game, if they were ever confused, things like that.
These are generally very broad, open-ended questions just to get the player to give us as much detail as possible without us guiding their responses too much.
The final phase of testing is a follow-up interview where players come back to the studio to show us the progress they made in the game.
We prepare for the interview by reading their diary entries every day, and that really helps us to plan more in-depth and specific questions that really dig into the nuances of the player's experience.
Taken as a whole, these three phases allow us to see the effects of various issues over time and to better suggest how to fix them for long-term results.
So the first case study is gonna be from Star Trek Fleet Command.
Again, this is our mobile 4X strategy game.
When we did this testing, Fleet Command was still in development.
We had no idea at that point how players were going to be engaging with the game or what their first week was going to look like.
One of the features in the game is a mission system.
So a series of ongoing quests that we designed initially to help players learn about the game fundamentals and then later to give them reasons to keep coming back and getting into more difficult content.
Our design team wanted to know if the system was working as intended.
Players understand the mission system.
Were they motivated by them?
So we decided to use a longitudinal test to see how players were actually going to experience the system.
The first days of the study, the on-site usability testing, was the first hour of play, and we had really good results that first day.
We found some usability issues, such as with iconography, a few places where we were using jargon, but overall players were able to understand and complete their missions.
So that first 45 minutes of play were really solid.
So then we shifted our attention to the next seven days' experience.
A few days into the second phase, we started to see some issues popping up that we hadn't anticipated.
In their nightly diary entries, players started to talk about how they were frustrated with how difficult the missions were becoming and how they were left with nothing meaningful to do while they were trying to grind the materials.
or levels they needed to complete those quests.
And by the end of the week, they were also reporting that the missions felt repetitive, since they follow a pretty standard formula, kill 10 of this, gather 20 of that, and they really wanted something to mix it up.
And we knew that these could turn into really big problems over time.
Players who feel like they can't progress or who are just bored are not gonna stay around in your game.
So when players returned for their follow-up interviews, we really dug into the issues and what we might be able to do to help them.
And from these conversations, we decided a good solution would be to branch out from the mission system and provide additional ways that players could engage with the game.
First, in addition to daily missions, we added daily goals as a way to give players easily achievable quests that could be repeated daily for good rewards, giving players stuck at a difficulty wall in missions a meaningful way to get the materials they needed to progress.
Next, we updated events, which we had planned as a late game feature for elder players to be available right from the beginning of the game.
These limited time events are always changing, which helped relieve the repetition that players were starting to feel.
This solution of adding more ways to play not only helped to solve the issues that we'd seen with missions, but continued to benefit players long-term through rewarding multiple kinds of engagement.
It also gave our developers multiple ways to introduce new features into the existing loop after the game went live.
Now the second case study is from the Walking Dead Road to Survival, and this is our RPG.
After the game had been live for a while, the game team decided that the early retention was not where they wanted it to be.
We looked at the analytics from players' first weeks and realized that our players were not progressing through the story campaign as quickly as they should.
They were losing battles that they should have been easily winning.
Our hypothesis was that these losses were causing player frustration and churn, which was explaining our low retention.
We guessed that the most likely cause for losing battles was using characters incorrectly.
So we decided to use a longitudinal study to see how players were actually using and understanding their characters during that first week.
Phase one seemed to go really well. During the on-site usability test, players went through the tutorial and a bit of free play and during this time they were able to articulate the key points that we were trying to teach about the characters.
That you can get new characters and that you can level them up.
We sent players home with the game and as they started to fill out their daily diary entries, we realized that our tutorial had not been as successful as it seemed.
Our players wrote that they were frustrated by losing battles and progressing slowly, just like we'd seen with the analytics data.
And when they returned for their follow-up interviews, we looked at their characters and our suspicions were totally confirmed.
They were using far weaker teams than they could have been.
Through all of the phases of our testing, players were able to understand and recall the major points of our character tutorial, winning new powerful characters and improving them through leveling up and evolving.
But from their interviews and their diaries, what we realized is they didn't remember how to tell which were the powerful characters that they should be focusing on.
So the first problem we found was when players won their new, more powerful character during the tutorial.
During the test, players win Dr. Stevens.
He is a very obscure character from the comics that none of them really remembered.
So it wasn't a particularly exciting moment to win him from Gotcha.
It didn't make them hyped to keep winning new characters.
So we ditched him.
Now players win Carl.
He's a major character and a fan favorite.
Even better, while Dr. Stevens is a healer, Carl is a badass attacker whose special skill is super strong in early battles.
The lesson we are teaching is now much more memorable and clear.
If you pull for new characters, you can get someone familiar who's going to be awesome.
Another problem came from the character that players learned to level up, a guy named Jed.
Now Jed isn't even a character in the series.
We made him up.
So right off the bat, not exciting for players to be using him.
Worse, he's also pretty weak in combat, even after leveling him up.
When players returned, they were able to remember how to level up their characters, but they were also now using Jed in all of their teams.
when they had much better characters available.
So they were taking away the very wrong conclusion that he was an example of a good character to use.
We solved this by having players learn to level up Rick.
Rick is the main character of the series, a big fan favorite, and one of the most powerful starting characters in the game.
He helped us to drive home the lesson that we wanted to teach.
Good characters will be made better if you invest in them.
These changes not only helped our players to start winning those early battles in the game, increasing retention, but we also ended up doing a better job of teaching the value of characters and creating motivation to engage with Gacha, and both of those are incredibly important for the long-term health and revenue of the game.
So now we've seen two examples of longitudinal research and action.
What both of these studies really show is that optimizing the early experience of the game has long-term consequences.
Teaching players what to value early on lets them know what to invest in later, and creating paths for players to follow encourages them to engage in the behaviors that you want to see and gives them a reason to keep coming back.
These studies also show that many important issues only reveal themselves over time.
Looking at the game as a holistic, ongoing experience was the only way to discover how players were really going to use and understand what we had built for them.
Longitudinal studies are not only powerful as we've seen, but they're also a really easy method to add to your regular testing.
Most of the play time is unmoderated.
Players are just off doing their own thing at home.
So adding length to the study does not mean adding effort.
The surveys used for the diary portion should be short to encourage completion and don't require a lot of time to set up or analyze.
We tend to ask the same broad questions every day and for every game.
So surveys can even be reused between studies.
The usability sessions and the follow-up interviews are the most time-consuming aspects since they require in-person moderator time.
But when you consider that you are condensing a week or more worth of data into a couple of hours of moderator time per player, the time is really a bargain.
At Scopely, we now use longitudinal studies on all of our games, from Fatui's and beyond.
I hope that sharing our studies today will help to convince you to do the same.
Thank you.
And I'm now honored to pass on the microphone to our last speaker, Sarah from Glue.
All right, everybody.
Who's ready to talk about Days 100 and beyond?
My name's Sarah.
I work at Glue Mobile.
We make mobile games.
Great news.
A new culinary award show has just launched where accomplished chefs can earn extraordinary rewards.
Even better news, we're going to talk all about how we did UX research for this update.
First I'm going to give you a little bit of a background on what Cooking Dash is and what went into the 2.0 update.
And then I'll provide some solutions for testing unfinished features designed for experienced players that unlock late in the game.
I'm going to show you how you can empower developers to make decisions based on facts.
I'll provide ideas on how to keep the research fresh and timely when studying live events.
And finally, we'll talk about how all of these things work together so that you will see adoption from your teams and you'll impact on your wonderful products.
Okay, so what's Cooking Dash?
It's a mobile free-to-play game where you must serve food to angry customers within a limited amount of time while using griddles with a propensity to burn things.
Fun, right?
Cooking Dash launched in June of 2015.
and the update went out in November.
And about five months before that, Glue User Experience Research had the honor to work with the talented game designers, producers, and product managers on this update.
Up until the 2.0 update, Cooking Dash relied heavily on new restaurants as a retention driver.
The team wanted to rely less on this content-heavy approach.
And instead, shift focus to a live event called Trial of Style.
All levels in this event are procedurally generated.
Additionally, many complex layers affect success in this live event.
And all of these things work together to motivate players to play in new ways.
By complex, I mean the most positive sense of the term, much like a delicious layer cake.
On the bottom layer here, you see the trial of style, but all sorts of things work to benefit you to get on the leaderboards and to do well on the levels, such as prize wheels, outfits, a gauntlet set of episodes, and even pets.
And then doing all of these things earns you a set of golden saucers, which in turn impact your ability to be successful in the trial of style.
So when the team came to UX Research to talk about what they wanted to learn, they said, we really need to know if players understand this concept of style, and if they understand how to get more of it.
We said, that sounds great.
Those are great research questions.
We'd love to help you figure that out.
Because style equals success.
And in the great leaderboard of life, those without style perish.
Certainly users are going to understand this metaphor.
So how did we work to understand if players were getting this relationship, if they were themselves understanding style?
We started first with something that we're going to call today a moderated tutorial playtest session.
Then the team iterated and we moved on to some diary studies.
The team said to us, hey, what do we do?
There isn't a tutorial yet.
And these features unlock really late in the game.
We said, no problem.
First, you need to recruit some really experienced players.
And then you create this moderated tutorial.
A moderated tutorial involves something like a script that you work hand-in-hand with the team to create.
And we do this like a play test.
We do this in our lab, but it contains even more, such as this example.
Here we see the trial of style.
And the moderation would be you, the researcher, and you're basically becoming the tutorial.
So for example, we let the play tester know that the trial of style has unlocked.
It's on their home screen.
And then we follow up with some questions.
These questions unpack things like player understanding of the new features, relationship to the core loop, and we observe the ability of the player to be successful, and we listen to the player describe their success.
So what did we learn? We asked the player to say, hey, did you tell me what style is?
And here's what we found out. They didn't quite get it. So they said, oh, it's an additional test of your skill, but I don't really know how I got 91 super coins.
We said, super coins?
Oh, no.
PIMS, Cori, what are we going to do?
And they said, don't worry.
We feel very validated after this study.
And we actually have so many ideas.
We feel so inspired.
And I've got ways to address this already brewing in my mind.
So after completing the moderated tutorial play test sessions, the team went back and iterated based on the things we found.
And three months later, we conducted a diary study.
A diary study for this particular, for this update, lasted 10 days.
We recruited 19 participants from CookingDash and we got a mixture of spenders and non-spenders.
Over the 10 days, participants took a survey and the daily surveys are basically the same thing every day.
On the last day, they take a daily survey and they also take a wrap-up survey.
Now daily surveys keep it vague.
because we don't want to lead the player on.
We want to know about what they're uncovering organically in the game.
So we ask things like, what was your intent to play today?
Did you encounter any frustrations?
Did you observe anything new?
What were your goals?
Did you achieve them?
And how motivated were you to play today on a scale of one to seven?
You can plot this motivation.
over the duration of the study and you can understand the dips, the highs and lows and the dips in their experience by looking at their free response questions.
Here you can see that someone is trying to strategize in new ways.
So they are indeed contemplating success.
Wrap-up surveys get to specifics.
We ask things like, can you actually define the features and you can name the features and ask them to explain it.
What did you discover and when did you discover it along your journey?
How challenging were each of these features for you?
How satisfied were you about each of these features?
You can show them icons or images from your game and have them describe them or describe the function or describe what they think they look like.
And you can have them talk about relationships.
A great way to ask someone to explain to you if they understand something is to actually have them Pretend like they're explaining it to their friend or to someone that isn't familiar with your game.
All right, so we've talked about the 2.0, and we've provided some solutions.
But how do you help your team get to decisions based on facts?
Creative teams benefit from observing the player engaging with their designs.
For example, Cory and Pims actually watched the sessions.
And here they started to realize that people were really confused about the bonuses.
And the participants were thinking that the bonuses were actually rewards.
Those players were really confused about the bonuses that led to success in the trial of style.
They thought they were rewards.
How can we help players learn that all the cool stuff you design, like outfits and pets, also impacts style?
Corey says, I already know how to fix this.
Don't worry, guys.
What's important is to talk to your team about consequences.
The fact is, and hey, we're trying to get to the facts, is that players think style is just coins, but we need them to know it's more than that.
How will this misunderstanding impact the entire player experience?
Well, if this is an entire meta update, the entire system breaks down.
Another way to help your team get to the facts is to layer data.
So we'll use that cake metaphor again.
You need to layer things people say with the things they do.
And in this case, I'm talking about telemetry, stuff you're pulling from inside of the game.
Because you're gonna be able to come to really strong conclusions.
And maybe you'll also make your dev team a little bit hungry.
Let's take, for example, the prize wheels.
What we learned is that 26% of the items earned throughout the duration of the study were related to outfits.
We also learned that all of those things in red, participants didn't quite understand the purpose of.
Let's take for example outfits.
This participant says, I don't understand how to get outfits.
Only by spinning?
What are the benefits?
We also saw that no one even earned an outfit saucer over the duration of the study.
So you'd think that people might be really confused about prize wheels if half the stuff they're earning they don't really get.
But it turns out they really love them.
It was the most motivating feature in the game.
Another thing they didn't quite understand was pets.
And when we asked how motivating was it to get a pet, they said, well, I don't know.
I didn't see this feature.
I don't understand these.
I didn't get any of this.
So I don't know what they will do.
So together, we could come again to the team and discuss what we found.
And Pim said, dang, people like prize wheels, even though half the time they don't know what the purpose of what they are getting even is.
So with some tweaks, we can make this feature amazing.
And Corey said, I know what to do.
So we've gotten to the facts.
But how can you keep your research fresh?
One thing you need to do is launch your diaries before the live event goes live.
That way, you can gauge satisfaction before and after the event.
For our study, we used a net promoter score type question, and we saw a 16 point increase overall.
So if you're keeping things certified fresh, are you going to see adoption?
How did it go for us?
One of the things we saw, and I talked about it earlier, was that people were really confusing style.
They thought it only related to the coins.
The team iterated and made this big purple S.
In order to decrease this association that it's only coins, the team reverted back to just the typical coins that it looks like in any episode.
And then any boost that relates to style also became purple.
They also fixed the issue of confusing boosts for rewards by removing the progress bar.
Here's what it looks like now if you were to play the 2.0 update.
And what about those pets?
Originally, the pets only had an aesthetic effect.
But after seeing what happened and seeing people's misunderstanding, they decided to add an actual gameplay benefit.
So the pet now auto-surfs food as you're playing the level.
And what about impact?
Cooking Dash has maintained steady retention for over three years.
And hey, we even saw an increase in revenue after the 2.0 launch.
So how do you test unfinished features designed for experienced players?
We recommend moderated tutorials and player diaries.
You can empower your teams to make decisions based on facts by helping them observe and get close to your players engaging with the experience of their game, and you can layer data.
Keep your insights fresh by launching before the event goes live.
And if you do all of these things, I promise you will see adoption and you will be impactful.
Thanks so much for listening to our talks today and thanks to all of our dev teams for letting us work on their wonderful products.
So, Seb's going to lead us through questions.
Okay.
So, there's a microphone in the middle of the room, I think, if anyone has any questions.
We've got time for a couple, maybe.
Hi.
Thanks for the talk.
It sounds like a lot of your diary studies had very small sample sizes.
How do you go about recruiting to make sure it kind of represents your audience, your users?
So, yeah, representative sample sizes.
Sure.
So it depends on your project on what type of player is going to best represent what you need.
But you always want to get some portion. So even if it's a smaller sample, let's say I was recruiting 12 people for my diary study, then I want and I need to see a mix of experienced users and new users. I'll break that in half six and six.
Still a small sample size, right?
Well, data that's coming in from the diary study, if there's a trend, trend meaning at least two or three people, which sounds really small, but these are unrelated people.
They don't know each other.
If they're talking about the same topic in the diary study, let's say they can't get into a certain area of the game.
You want to balance any research method by acknowledging its weaknesses with another research method that is strong in that area.
So a diary method doesn't have a large sample, so then you look at telemetry data to see how many people are actually getting into that area in the game, and as a designer of this game.
Is that a problem?
And how big of a problem do we think that is?
So you don't need a large sample size to address what's going on in the game.
You need it to understand why.
You only need a small sample size to understand why something is happening.
Regrettably, I think that might be all we have time for.
But we're all gonna be outside in the corridor out here.
So do come out and ask us some questions.
Thanks very much.
