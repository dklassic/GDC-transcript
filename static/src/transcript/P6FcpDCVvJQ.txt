Thank you.
In 1986, Fred Brooks proclaimed that there would be no single development that would deliver one order of magnitude improvement in a decade in software engineering.
Fast forward 30 years, this slide was presented by Chelsea Finn at the Deep RL Bootcamp at UC Berkeley last summer.
And what this slide is showing is that every generation of machine learning algorithm is a 10x improvement on the last.
That means that a problem that was taking 100 million steps just a few years ago, is now been able to be learned in real time on a robot in about 20 minutes.
And we're seeing these improvements not every 10 years, but every six to 12 months.
So my hypothesis is, If reinforcement learning along with reward functions, if we can apply that to software development, that will result in a multi-magnitude improvement in productivity, reliability, and in simplicity.
And to use video game AI as proof.
So today I want to take you through a couple of stages.
I want to talk about, kind of a brief, give you a brief taste of reinforcement learning.
And then I want to share the results of the research that I've been working on into this space.
First, a little bit about my background.
I started in the 1980s on the Commodore 64.
I'm dyslexic, dysgraphic, I also have ADHD, and I really struggled at school in traditional learning, but I had this aptitude to be able to understand a computer, and I had this passion for video games, and so while I was failing at school, I was winning contracts to make video games, and I left school at 15 to join the industry.
Over the next 25 years, I worked my way up and grew with it as the industry grew, working in multi-disciplines, so in engineering, production, and creative direction.
And at the end, I was an executive producer with Xbox up in Microsoft.
Over the last couple of years, I've been working in a couple of different areas.
So on the one hand, I'm working with neuroscientists and people at the bleeding edge of academic research on projects that take three to five years to bring to market from neuroscience.
And I also work with a guy called Nils Leyer who is...
He is one of the fathers of video on the internet.
And he's also a VC, so I work with him building his companies.
And both of those kind of pull into this research that I do into reinforcement learning, which I started about three years ago.
So there's a problem for me with learning reinforcement learning.
If you look at this equation at the bottom here, I literally cannot read that.
And to make matters worse, my big hack for reading complex materials is to use voice-to-text software, but math notation breaks that software.
So I have a couple of tricks that I've used.
So the first thing and most important thing for me is building a mental model.
The second thing I would say is get hands on and get hands on as quickly as you can.
I would say don't waste your time with all these online courses that you see everywhere because you're going to be writing gradient descent algorithms in Python and that really doesn't offer you a lot of advantage.
Other things, if you are dyslexic, I recommend doing podcasts, videos.
I also have learned to skim read papers.
The one online course place that I did find is machinelearningmastery.com.
That is built for engineers who want to learn machine learning.
So it's high on the engineering and low on the math background.
So if we think about this model here, this is the classic reinforcement model that was created by Sutton and Bartow in the 70s and 80s.
And the picture on the left there is from the Atari DeepMind paper.
So to give you a little kind of instinct, or try and build your mental model of how this is working, so the environment in this case are raw pixels coming in, and they go into a convolutional neural network.
And if you think of that as compressing the pictures, so the problem space becomes much less than having to work on the raw pixels.
Then there's an action space, so in the case of breakout, you can go left, you can go right, or you can just do nothing.
The third step is delayed discounting.
So when it gets rewarded at the end of the game or a reward of a point, it steps back in time and gives a discount of that to those previous steps.
And then each of those, each individual frame and that reward and the step that it took, the action that it took, they go into something called an experience replay buffer.
And this is a million big, right?
it randomly samples that.
And that is really the core of the DeepQN algorithm.
It does no planning, it's not thinking about anything in the future, but it is able to find this advanced strategy of working its way around the side of the breakout.
One thing just to talk about is the exploration versus exploitation trade-off.
So exploration is just randomly searching inside the environment.
Exploitation is taking the signal that has the highest future reward.
And there's a trade-off there and algorithms have to work in different ways to explore the space in an efficient way.
For the DQN paper, they had a very naive approach.
They just, for the first million steps, were totally random.
And then they reduced that so that over time, 2% of the choices would be random.
But I wanted to step back to dyslexia and neuroscience and automaticity because it turns out that us as humans have this trade-off as well and different sets of people will explore problems in a different way.
So there's a researcher in the UK called Rod Nicholson, and he researched dyslexics, and he found that for them to learn things through rote memorization and automaticity, it took n times the square root of n.
So if it takes 100 repetitions for a normal student to learn the math times table, it would take 1,000 repetitions.
But the trade-off there is that a dyslexic will find, will be looking at more different solutions in life.
So I recommend you do keep them on your team.
In terms of the landscape, so if you wanna get your hands dirty and start playing with reinforcement learning today.
These are three of the many players, but I think these are the top ones to look at.
So first, DeepMind.
They write some really great papers and do a lot of research that you're probably familiar with.
The frameworks that they have, StarCraft 2, if you're a StarCraft player, that's maybe a good place to start.
And DeepMind Lab.
But they're not so good at publishing algorithms.
They did publish one version of the DQN.
OpenAI, that's where they excel.
So their baselines has nine world-class algorithms that you can use.
It's all open source.
They have OpenAI GYM framework that has Classic Control, Atari, Majoko.
And they also built Universe on top of that, that allows you to play games like GTA or Flash games, or to learn against those games.
And the new kid on the block is Unity, and Danny's gonna talk about this.
So they launched ML Agents last year.
So they already have a couple of algorithms there, and they have at least 10 different sample environments that you can use.
The first thing that I did with machine learning, this is going back two or three years, was I was playing around with a soccer, some soccer AI, and I took a high cost function, and I sampled this over 24 hours, this function, I put it into a linear regression, so I learned this and I put the model back into the game, and I was seeing that this was choosing the same outcome as the function 90 to 95% of the time.
And so that gave me enough kind of confidence to carry on looking at this space.
Today I've kind of built up my own system.
I call it Analog.
I started before Unity had created their system.
What my system does is it bridges between OpenAI's baselines and Unity, and it gives me this nice UI so that it kind of like being ADHD, I remember what I was working on when I go back and it's finished a training run.
So next, I want to talk about the research that I've been doing and share some of the results.
The first area I'm going to share here is using reinforcement learning for locomotion.
So if you think why we might want to do this, well, locomotion is very hard to get right and is very expensive, is a big part of our budgets.
And it would be good to have an extra tool set to be able to do that.
So the video here is from DeepMind.
This is a character that's learned completely from scratch with a reinforcement algorithm.
and the one in the bottom left is from the same paper.
And this one is from OpenAI where they took that algorithm and they improved on it using something called parameter noise.
The photo in the bottom left there is, this is from a talk from a couple of GDCs ago from Ben Fody, he was talking about physics and making gains with physics, and he came up with this idea to use a MIDI controller to tune all the parameters that.
you need to do just to do a 2D game that he was doing.
And so it would be good if we could put that to rest and give people the ability to learn those things.
So in terms of the methodology, I'm building the architecture on top of my analog.
I'm using a couple of algorithms, Actor and DDPG.
I went through some stages.
So initially I wanted to use my naivety of not really knowing Unity, not really knowing the physics system inside Unity.
And I'm taking the baselines of the Majoko, which is the world-class gold standard physics engine.
With Majoko, the simulations are so strong that if you train a model on Majoko, you can put it onto a real-life robot and it will reproduce that.
And then I went through stages of progressively looking more into the research to improve my outcomes.
So initially, things worked very well.
I was able to get this little worm moving.
And I was able to recreate the ant from Majoko.
But to give you a sense of the time that took in terms of the experimentation, so it was 44 different experiments just for the ant.
And that's over 86 million training steps.
So to get things, you know, when I plugged it in some other.
characters, the Hopper or the Humanoid.
I started to see that I'm getting some results, but they're kind of strange, right?
They don't look like you're seeing in the papers.
And so to address that, I dug into the research some more.
One of the most important things I found was that a lot of the finesse comes from tuning the reward functions and also the termination function.
So if you see at the top, we've got the Hopper.
and it will terminate the hopper, so say it's failed if the angle goes over you know more than 20 degrees either way. It also has things like a cost function so it's kind of...
penalizing the AI for overexertion.
And the one on the bottom is the open, sorry, is the deep mind reward function for the walker.
And that has things that rewards it for being upright and the height.
So there I was able to reproduce the benchmarks.
The ones we're seeing on the left or on the top, that's about a million steps.
And the one on the right is about 3 million.
And then the final stage I went through is comparing the results with the Mojoco.
And I was able to see that the training time was within about 5% and the steps were within about 10%.
It does train differently, which is kind of nice because the algorithm gets around problems that you may have in the physics implementation.
So in conclusion, I've been able to reproduce the hopper and walker inside a commercial game engine.
I expect that those deep mind results, so having more elaborate obstacles, will be reproducible, and this is definitely worthy of more study.
In terms of future work, there's a lot of new algorithms that I'd like to...
look at as well as mixing motion capture data with reward functions.
In terms of the total scope, I added this up last night, so everything I did in that space was 380 experiments and that's about 750 million steps.
So the second area that I'm going to talk about is learning and advanced control in 9-9 training steps.
So the reason to do this is that often the problems that we want to solve are pretty simple, right?
They may not have the same problem space as learning locomotion.
And we may want to retrain this very quickly.
So this was an exercise to see how much I could optimize it and what were the steps I needed to do to be able to optimize it.
The problem space I created was teaching the AI to hold fire.
So it's got two actions, it can have fire or not press fire, but it's got to hold fire over an appropriate number of frames to hit the target an appropriate distance.
So we know from usability that people like my wife have struggled learning this control, so I thought it'd be fun to train an AI.
The way to get this down is I use something called a grid search.
So this is taking the number of layers in a neural network and the layer size, and just running iteration over iteration until I'm seeing a trend or a convergence on one combination.
And then taking in another variable, so in this case, the number of stacks to the number of observations, previous observations that it takes in a row.
and then doing a search to break that down into 99 steps.
So I was very surprised I was able to get it to train this quickly.
One caveat is that the solution to the problem is probably correlated to the design of the network and there isn't a lot of research over that.
So we know that there's a correlation but we don't necessarily know, okay for this problem what network should we do?
The third area I want to present today is the case for real-time server, real reinforcement learning.
So let's say you want to publish a game today, you have your Python back end that's doing your reinforcement learning.
Could you publish that using?
some of the technology that we use in multiplayer games here.
So my inspiration was for the visual reaction delay.
So on average, we have a 250 millisecond delay between the visual stimulus and being able to react.
My method was to train 20 frames a second with a 10 step delay.
So it's 200 milliseconds delay.
And what I found is that this slowed down learning by about 30%.
So it's not a silver bullet, but it is worth looking at if you want to launch today and bring a product to market.
Okay, so that concludes my talk.
And I'm going to hand over to Wolf.
And please come find me.
I love talking about this stuff.
Thank you very much.
All right, that's been my talk.
That's been my talk.
It's been great talking to you all.
We'll move on.
Hi, I'm Wolf, and I'm from Google.
And the reason why you're listening to me is because I am the technical lead for developer relations for TensorFlow.
TensorFlow is a giant framework that does machine learning and a bunch of other stuff.
It's built by the brain team at Google.
And I'm not going to talk to you about TensorFlow much at all, because honestly, there's no way I'm going to teach it to you all in 20 minutes.
But we are at this sort of, but oh, and I should go back and say that I'm also a game developer.
I've been a game developer.
I actually worked on applying machine learning to AAA games like a decade ago.
It's tricky.
It's interesting.
And what I wanted to share with you today here, oh, well, sorry.
And as we alluded to before, you know, we're at this sort of magical time in machine learning where things that we, that a decade ago that we thought we were going to be able to do, we just can't, or sorry, we finally can do things like self-driving cars, and obviously, you know, our friends in London having success playing games.
We have, we're beginning to apply machine learning and deep learning to medical data, which is pretty amazing.
and actually getting quite interesting results.
That's looking at retina scans.
And of course, making little stompy robots, which, you know, everybody likes stompy robots.
He's in our lab.
But in this talk, as a game developer, I want to take you on a tour of a bunch of things that I find interesting in machine learning, ways to make the process of making games more interesting, more exciting, more efficient.
But I'm not really going to talk about NPCs, because NPCs are cool, but it's a different problem.
So let's start with part one.
Let's talk about statistics.
So your players are all out there creating data.
And in that data, you are finding out about the kinds of things they like and they don't like to do.
And you should be taking advantage of that.
you should be, ah, there we go, you should be taking advantage of that.
So, you know, is your player gonna quit today?
Is your player gonna spend today?
These are all sort of time series models you can use.
You can learn to do anomaly detection with cheating.
You can learn about, check if they're grieving.
You can find out if they bought X, are they gonna buy Y?
These are all sorts of things you should be doing.
And, you know, talking about sentiment analysis, you know, this is a well-studied problem.
Learning, you know, learning about your community and learning the.
kinds of things that they're saying, and trying to figure out if they're saying the right things or things that maybe you don't want so much, this can make your game a more interesting experience and more pleasant experience for your users.
You should be totally doing this.
There's open source models where you can try this stuff out.
It's a lot of fun and you can set up a service to do things like sentiment analysis or time series prediction and make your game more efficient.
But I want to talk about things that would affect kind of the look and the feel of the game directly that the player interacts with, rather than the sort of subtle things.
How many of you know about style transfer?
Eh, some of you.
So style transfer is the idea that you take the semantic structure of an image, like this picture of a food dog I took in Japan, and, uh, a model trained on like a famous piece of art, like Hokusai's wave, and you put them together and you get sort of Hokusai's food dog.
This is kind of a neat trick, and it comes out of the research we did into finding out how convolutional networks actually do their work.
So you find out that the beginning level layers on a network can find small features like lines and circles and things, and then as you go down, you see textures, and eventually it's detecting.
are activating around groups of features like dog faces and things like that.
And with that, you can do things like style transfer.
You can do Deep Dream.
And I'm not gonna show you pictures of Deep Dream because they're really trippy, but you can look it up on the internet.
That's where you imagine dogs on everything.
There's actually a really great paper we published recently on distil.pub about how to understand what's going on inside convolutional networks.
It's totally fun.
I recommend checking it out.
Style transfer is the kind of thing that you can actually do with a good GPU.
Frame rate.
In this example, we're actually blending between multiple kinds of styles at the same time.
And you can imagine, as a game developer, using style transfer to bring up that emotional punch that you need in your game by beginning to add one of these effects based on art that maybe your artist drew to get this.
So we're blending between like.
pastelly and wooden.
I mean, think about the ways you can do this.
It's pretty neat.
And of course, there's other image transformations like CycleGAN, where, and this is from the Berkeley AI Research.
You can go try it out.
And here you can do transformations between things like horses to zebras and apply other kinds of style.
The neat thing about this is it doesn't need paired images to do it.
So, we had a student at the Jeju ML camp in Korea that we did last year.
She was doing image simplification.
So what she was really doing is converting pencils to inks without actually having an entire corpus of inks to train on.
She just had some inked things and some pencils and then she was able, using a variant of CycleGAN, to train on.
Do an okay job of kind of inking in this picture.
It's not a thing that your artist would probably be completely happy with, but it's something you can do in a hurry if you want to try something out.
So let's talk about generators, speaking of generated images.
Text generators are fun.
They're just an example of the kinds of things you can generate.
RNN generated text, it's been around for a while, it's kind of fun.
You predict sequences of text.
Andrej Karpathy has a just wonderful blog post about the unreasonable effectiveness of recurrent neural networks.
There's a guy, some folks got together, it's called RoboRosewater, and they began, they took all of the cards in Magic the Gathering and poured them into an RNN and then began predicting.
more cards, and on a per-character basis, not on a per-word basis.
So sometimes it's sort of nonsense, but as you train the network, it becomes more and more plausible, and with the thousands upon thousands of Magic the Gathering cards, you eventually get into kind of fun stuff.
They actually made an entire draft deck out of RNN generated things.
Only about 40% of the cards are playable, according to this blog post, but nonetheless, everybody's really excited to try out these things.
And I actually, just for fun, I did this...
How many of you played Dominion?
I did this with Dominion.
Dominion doesn't have nearly as many cards, so you run the risk of overfitting pretty badly.
But I ended up generating this totally ludicrous knight card.
or Dame card where you reveal the top three cards of your deck, you look through your discard pile, then you trash a card that's not a treasure, and then you look through your discard pile again because the network is just in this little like loop, and then eventually you trash a card in the trash.
costing three to six, and then you gain one of them.
So this is a very silly card.
But the thing to think about is, if you have a lot of content, pouring it into a giant hopper and sort of shaking it around and having it sort of generate things back to you can give you perspective on ideas that you haven't had before.
The fact that you can make an entire draft deck out of completely novel Magic the Gathering cards is pretty playable.
With hard work and editing means that this is a tool.
This is a little thing generating away.
You as an artist are still doing.
all kinds of work, but you have this helper that's making you efficient and creative.
So let's talk about image transformation.
So this is a task called super resolution.
And it's an area of study.
And basically, you start with the high resolution photos on the left, you down sample them, and then you learn the mapping between the down sampled, pixelated images to the high resolution images.
And then at any point, you can hand a low resolution photo, and it will generate a high resolution photo for you.
Which is crazy, but it totally works.
Not all of them are perfectly convincing, but you can imagine handing this off to your pixel artists and them having fun.
You can imagine handing it to your players and having them sort of poke around with some pixels and generating art that's in style into your game, things like this.
Neural editing is a really interesting task and there's a lot of different ways of doing it.
And once you have one of these models, you're starting to do this adversarial training stuff, you can start generating things.
So this is, there's a Japanese engineer, started, took a giant database full of Japanese pop star faces, and then just began generating more pop star faces.
Which, again, can be another way of sort of shuffling your data around and trying to find something new and interesting and give you ideas.
And now we're getting into tasks that are a little bit more like what your artist might do.
This is infill painting.
So in infill painting, you basically give part of an image to a trained network, and it tries to fill in what's missing without any context.
In this thing we have in the upper right-hand corner, that's a human trying to fill in the painting.
Lower left-hand corner, we're using kind of an average-y loss on a network, and you get kind of an average-y result.
And, you know, if there were a car in there, sometimes cars come out kind of brown because that's the average color of a car.
Very few cars are brown, but if you average them all together.
But if you use adversarial loss, where you have another piece of your network that's saying, this picture is not true.
It makes that, you know, this is not what cars look like.
You eventually end up with these kind of much more crisp things.
And again, this is, this is, you know, an artist can totally do this work, but this might be a way for an artist to be faster or more efficient or get ideas.
OK.
Now let's talk about latent spaces.
And here I'm going to be talking about Project Magenta, most of their work.
Project Magenta is a team at Google that works on the intersection between art and machine learning, and I just really love their stuff.
Variational autoencoders.
What are these?
Well, you take an image and you encode it into some latent vector, z.
And then you train another decoder that takes this sort of compressed format, this just a series of floating point numbers, and you expand it out to make the image back again.
And what you're trying to do is reduce the loss between the picture on the left and the picture on the right.
And you also have another kind of loss in the middle, in the latent vector, where you're trying to make the latent vector of these variables contain the most information they can.
It's complicated to explain, but that's basically what you're up to.
And when you're done, you end up with a vector space that can describe a lot of things, and a whole bunch of things that the network has never seen before.
And with the special loss, you end up with a smooth space of describable things that doesn't have a lot of holes in it, like a lot of weird jumps or nonsense in it.
So you want to train on a big data set.
And we wanted to work on sketch data.
So we asked our friends at the Google Creative Lab to make a game, and in that game, we asked people randomly on the internet to draw objects that they.
know about, like draw a bear, and then the AI would try to guess whether or not they'd drawn a bear.
And it was a fun game, but it also generated a huge data set of sketches of common objects.
So going back to this picture, imagine these aren't pixels.
Imagine these are now vector drawings.
So now we have a vector drawing of a cat, and then we pour it into this fancy encoder, which is now no longer convolutional, but instead this sort of LSTM thing.
Goes into the vector and back out again.
And so we can actually reconstruct the drawing of the cat.
And you can see it's kind of generalizing, because if you draw a sideways cat, and you draw human input, and then the output is another sideways cat, and if I draw a cat face with five whiskers, I get a cat face with six whiskers, because most cats have balanced whiskers in the data set.
If I draw a three-eyed cat, I get a two-eyed cat, for obvious reasons.
And if I pass in a toothbrush, well, the model hasn't been trained on toothbrushes, so I get...
toothbrush cat.
And we can do it with pigs as well.
If the model trained on pigs, pig face, pig face, sideways pig, sideways pig, eight-legged pig, four-legged pig, and truck, you get truck pig.
And because this is a vector description of this, and this is a giant vector space, you can begin to do vector math inside this space.
So if we pick any two points in this space, we can interpolate linearly between these things, and then we can ask the decoder to generate a picture in each one of those things, even though it hasn't actually seen, necessarily, this particular point in space before.
And so in this case, we're interpolating between a cat face and a pig.
And you can see as we get closer and closer to pig, it begins taking on both the sidewaysness of the pig and also the little curly tail eventually pops up in the snout, because that's the thing that distinguishes the concept of a pig and a cat, really, when you think about it, on drawings of this scale.
And we can do it as well, interpolating between cats and trucks.
I just, the little, the cat that's just to the right of the middle one is like my favorite cat.
I love that guy.
And with a model like this as well, you can begin to do image completion.
You can start drawing something and then it will try, like, oh, I've seen this before, let me see if I can help.
And with a little bit of variance, you can draw all kinds of different kinds of, say, mosquitoes, if you have a model trained on mosquitoes.
And again, thinking about the kind of work that you do from day to day, you may not actually want, you would obviously not necessarily take any of this art, but you might try it, just to see, try it on for size, see what it looks like.
And you can do it with fonts, too.
This is, here we're interpolating between the sans-serif fonts and serif fonts, in the upper left and the upper right.
And we can sort of, and the results are in the lower right there, kind of going back and forth between serifs and boldy and less bold and more serify.
And again, you can imagine trying this out.
In fact, this is a little toy you can play with if you follow that link.
It's totally neat.
And lastly, with these latent spaces, we can also describe music.
So here, this was just published last Thursday, music VAE basically has a giant space of melodies and then you can interpolate between any two places in this melody space.
So let's take a listen.
Whoop, that didn't work.
Let's try actually pressing the button and take a listen.
So this is the first melody.
And this is where we want to go.
Okay, let's start in triple A.
Thanks for watching!
So it's really cool, and it's open source.
So we have blog posts and more ways to play with this, and other ways to think about it.
So I'm running out of time, so let me wrap up.
I know this from being a game developer myself, making games is hard.
And when I think about all the things I spent time, I spent making games, a huge amount of it was trying things and throwing them away over and over and over again.
Trying to find in the latent space of games what the fun thing actually is and what the beautiful thing actually is.
And I think tools like this and thinking about these problems like this might be a really interesting way to become more creative and more efficient.
Although, I will warn you, Machine learning can be a lot of work up front for good results later.
So by no means am I promising you you will get things done faster, but you might try a lot more things.
So going back to the very beginning, right now there's tools right away where machine learning can make your game more efficient or more easy to manage.
But later on, I think, you know, and even now, with some of these tools that we're bringing out and people are building in the community, deep learning can help you generate, you know, sort of unique, interesting, compelling content.
But we always need artists and programmers and designers.
Because machines know what they've been taught.
And you're the people out there being creative.
And, you know, these things are here to help you.
If you want to know more about any of this stuff, TensorFlow.org is a great place to go.
Obviously, a lot of the research I talked about here, although by no means all of it, because we talked about other stuff, not by Google, research.googleblog.com, and Magenta has its own link there.
And that's it. Thank you very much.
Thanks.
Okay.
This is going to be a tough act to follow because I don't even have audio in my presentation.
So anyway, I'm Danny Lang.
I'm running the AI and machine learning team at Unity.
And for those of you who don't know me, because this is my first time at...
at the AI Summit here.
I have been leading the machine learning efforts at Microsoft and Amazon and Uber.
Both Amazon and Uber, I rolled out company-wide machine learning platforms that basically have been used in every corner of both companies.
Coming to Unity, I decided to deal with one of the harder questions in this world, which is what is AI?
And I was a bit involved in some of the Alexa work.
People ask me all the time, is Alexa and Siri, is that AI?
I'm like, nah, I don't think so.
It's sort of really hardwired.
Though there's a lot of machine learning in there.
on the speech recognition part and the text to speech part.
So there's a lot of small elements here and there, but they're not really AI, yeah?
And then of course, there's all the recommendations work.
I did some of that at Amazon, you know, Netflix, all these recommendations.
Is that AI?
No, it's not really AI, it's machine learning.
But again, there's some very, very good elements in there.
There's actually some really incredible stuff going on, say in the work on recommendations.
the latent vectors that the previous speaker mentioned.
That's really advanced stuff that goes on in there.
Fraud detection for your credit card, that's another fantastic area of detecting strange patterns in there.
and catch the bad guys when they tinker with your credit card.
Yeah. Equity trading.
A lot of machine learning there where you get in, you know, tens of thousands of new sources and you try to trade stock.
Facebook feed, you know, get all the best fake news for you.
And and then, of course, one thing that AI is really that is in every other job title on LinkedIn.
I think in my title, I had sort of doubts that, you know, people are going to talk about me behind my back, but now everybody has it.
So I feel like like an everyday Joe now.
So what about real intelligence?
Yeah.
And if you I hate people doing this, showing definitions from the dictionary, but it's something about, you know, your ability to acquire and apply knowledge and skills. So.
That's sort of too abstract for me too.
So I thought a lot about it.
And when you really think about intelligence, it's in action around us.
It's basically the senses and processes that nature gave all living things so that they can run around or grow.
by consuming energy, yeah?
So they have to eat to survive, yeah?
They have to grow, they have to consume energy.
They also have to avoid getting eaten, or at least delay the process of becoming energy themselves, yeah?
They have to multiply, we're not gonna go.
deeply into that today. And they have to be very careful with physics, in particular, you know, with inertia and gravity, you know, when you climb after that apple and you fall down and break something and kill yourself, etc. So this kind of intelligence in action is really everywhere around us, yeah? It's from bacteria, which are not particularly intelligent, but they have some stuff built into them that make them a little smart, yeah?
Bees are a little smarter than that, and humans, we are...
sort of, sometimes very smart, sometimes not so smart.
So it's really all that intelligence is made from infrastructure.
So it's really the evolution created complex apparatus for that.
Chemical mechanisms, cellular structure, all this stuff was created so that nature the version of nature we live in survives and grows.
Yeah.
There may be another version of nature that failed because it forgot some of these things here.
Yeah.
And, and we see that organisms, organisms have messaging systems.
That's what neuroscience is about.
And we had developed sensors, you know, like vision came around 450 million years ago that suddenly led to.
the creatures at that time suddenly having a very different life and evolution took them in a very different direction.
So if you think about the game engine, it's pretty close to what I talked about.
It's a 3D environment where there's a physics engine, so things have inertia, there's gravity, and it's a controlled self-sufficient ecosystem that sort of mimics the real world.
Looking at that, that's actually an awesome environment for really working with your private AI biodome.
So at Unity, we launched ML Agents because it adds one last thing to that equation of the three environment, this extension, and that's the...
mechanisms, the neuroscience mechanisms, yeah?
That all those things in that game engine can now start exhibiting some kind of intelligence surviving in that 3D world with a physics engine.
So what is Unity ML Agents?
It's essentially, it's open source.
It's an AI toolkit.
It uses environments which are worlds which are unity based.
So you can create your own biodome.
Yeah?
You remember the biodomes?
You can create that in Unity and then you can add NPCs in there.
You can add agents.
And they're essentially game objects that are attached to an agent.
The agent can observe.
And that's actually the sensors, yeah?
You can have a camera in the agent so it can see visually, or you can have some ray tracing going on, you can have collision, you can basically mimic everything that nature created.
Let's see, how many sensors is it that we have?
Some people say five.
Let's see, that's, you know, we can taste and smell and see, feel, and hear.
There's something about, we also feel heat, cold stuff.
We can also feel the balance and we can feel movement.
We actually have many, many sensors.
All those sensors you can actually build into your game objects.
Think about it.
You can actually invent new sensors.
We actually had a guy, he did a LIDAR.
You know, the laser thing that robots and self-driving cars have.
Boom, add that as an asset into Unity and now you have another sensor.
So.
So observations of these game objects or the agent is really about senses.
They can take actions, they can move around, and when they accomplish something, they can sort of send a signal back and say success, or they maybe fall over and die, and they failed to have success.
That's the reward signals.
And then we use reinforcement learning.
We talked a bit about, first speaker spoke a bit about reinforcement learning today.
So it's essentially the idea of running a simulation, learning from that simulation which is basically coming up, let the system come up, the policy that is essential and always think about a policy as an optimal mapping between what you see and what you do.
So there's always the random policy which is, you know, you throw a coin to make decisions, yeah?
But that's not an optimal policy.
So we try to let the system learn the optimal policy.
And then when you have that policy, you can now move into the inference phase which is you can let the system play out.
In the diagram.
Very briefly, this is what it looks like.
You have the learning environment, that's Unity, the big gray thing.
There's a Python side to it.
The Python part of it provides you with the mechanisms and the ability to hook algorithms up outside Unity.
So we're doing all of our work on TensorFlow, but you can hook it up with your favorite machine learning system.
But if you go to the ML Agent GitHub, there will be this of how to download and install and get TensorFlow hooked up with this.
So you have that training taking site outside Unity.
Then inside Unity, there's the Academy that embeds a communicator that you don't have to care much about, but that's the thing that sort of takes things from inside Unity in your Unity environment out to the, for training outside.
So in the Academy.
You have basically agents, that's game objects, you have game objects attached to agents and they can do stuff, as I said, observe and have actions and send rewards.
And there are one or more brains.
The brains is sort of what you try to build here and that's what I call, that's the new science part of our biodome.
training scenarios.
So we already heard about some of them.
But this is actually why I'm so excited and why we are doing ML Agents because by making, basically democratizing AI so that everybody, not just the big corporations can play with this.
Yeah, it's really bringing it out.
And what I hope you take home from this is that there's so much to be done.
We're just scratching the surface.
And what we're trying to do with the ML agents at Unity is to open the door for you to take this and run with it.
So I'm gonna give you some examples.
And this is, you can just imagine how.
how many opportunities there are here when I go through this.
It's like as an example, you have a single agent with a single brain doing something, yeah?
It's a very, very simple combination that can be a game object trying to get from A to B or learn to get from A to B with some obstacles in between, yeah?
Single agent, single brain, fairly straightforward.
But then you can also have examples of multiple agents.
with a single brain.
And what that does is that it sort of gives you an opportunity to have a lot of action going on, a lot of reward signals coming back in, but you accumulate all that knowledge in a single brain.
There's also adversarial self-play.
You have two agents.
have a single brain, single set of skills, but for each agent, the reward signals are inverse.
I try to beat this guy, and this guy tries to beat me, and we feed all into a single brain.
So we get two guys who are actually really, really good at playing against each other, and they make each other better and better because all the experience goes into the same brain.
So you can see a lot of this computer science stuff, it really sounds more like what we do every day together by learning and exchanging information and playing games.
And I'm not even computer games, but maybe even soccer games or things like that where we do things together.
So brings me on to cooperative and cooperative multi-agent scenarios where we have multiple agents, multiple brains.
They may have shared or inverse rewards.
So hopefully you can understand that the opportunities here of working and combining these brains and agents and the rewards are basically endless.
I'm gonna tell you.
So the training environments, that's basically the setting in which your game objects are doing things.
And what we've been doing is that we have provided you with a bunch of pre-created training environments that you can just grab and start playing with, which means you can actually go home and do all these things I show you right away.
But you can also take these training environments and change them.
or you can clone them and then change them like crazy.
There's no limits here.
So a lot of the work going on in reinforcement learning and other machine learning and AI areas ends up being a lot of papers that you read and then you're kind of, oh, I wish I could try this.
This is what we're giving you here, is this ability to actually stop maybe even writing all those papers and actually do some of these things and share them.
I'm gonna show you a few examples.
Here's the 3D balancing ball.
It's one brain, but it's 12 agents.
And I'm gonna show you the video here.
So it's about balancing a ball on a pad.
And it's 12 agents.
And the observation for these 12 agents is actually the location and velocity of the ball.
So where's the ball rolling?
Based on that input, When we run this with 12 agents, they learn from scratch, what we call tabula rasa, from a clean slate.
They learn to balance that ball in 30 seconds.
In this case, by adding 12 agents in there, it's a way of just generating more data.
And I just said something very important there, because what we use the Unity engine to here is to generate the data.
We need tons of data to train these systems.
I'll show you another one here.
It's basically two brains.
It's two person talk, or four people, two on each team playing soccer.
So it's really two brains.
and four agents, so before I run it, I just have to explain that there's a goalie brain and a striker brain, yeah?
The two goalies, they both share brain because their job is the same, and the two strikers share brain because their job is the same, yeah?
But they're on two different teams, so the rewards are inversed because they're trying to beat the other guys, yeah?
So you see, In this case, the observation is actor Ray Karst.
So you will see the little square agents.
I apologize for the graphics here, but it's sort of to keep things simple.
Has a little eye, yeah?
They are one-eyed.
And they look at the strikers, by the way.
Have you noticed the strikers are looking sideways all the time?
Yeah, but the goalies are looking forward.
Because the goalies need to look at the ball coming against them.
But actually when you're a striker with a 180 degrees view, point of view, then if you're on the side looking at the field from the side, you see the most.
If we would narrow in that view of field, they would probably start turning a lot more around to scan the space.
So that's one thing that's important to remember here is that you have opportunity to to basically put a camera on the agent or put a camera over the entire world.
So in this case in soccer, here the agents actually don't see everything.
They only see their point of view.
They don't see the entire world.
You could in other cases have cameras that show the entire world.
Let's do some, oh that was a joke that was not supposed to get in here.
But it's wall jumping where we basically give an agent two brains because it needs to do two different things, yeah?
It needs to learn to push another block next to the wall so it can step up on the block and get over the wall, yeah?
So look there, it's cheating.
It could jump on the low wall, yeah.
But in this case, it has to basically learn two skills.
So one is pushing the block, and I wanna tell you how it learns that, because this is, again, so fascinating, yeah?
Because it learns that through curriculum learning.
So what we do, and we support this directly in ML Agents, we basically give you a chance to teach the agent the simplest case.
where the wall is actually just tiny, tiny, tiny.
And then we make the wall taller and taller as the agent reach some objective, some threshold with the low wall, then make the wall higher, yeah?
And what's so fascinating is that you get a better model out of curriculum learning rather than just let the agent try out to scale the tall.
wall right away.
Actually, well, you can see the orange one is its ability to scale the tall wall, just never seen the low wall, yeah?
So it's a bit like when you go to school and go to first grade and second grade, third grade, you sort of learn things stepwise, yeah?
Instead of just jumping right into it.
Isn't it weird that these systems, with all their primitive technology from TensorFlow or wherever it comes from, exhibit that same behavior, yeah?
It's the same behavior that building upon a previous learned skill.
We have seen other examples of that where if you want to build a robot hand that really grabs stuff, you teach it a lot of primitive things it can do and then it gets much better at learning the hard task.
So That's an important message because it really tells everybody that you need to sort of take this out of pure research and really put it in the hands of a lot of people because they would actually be able to explore many more aspects of this.
I wanna mention another one which is essentially a sweet little one using LSTM, which is basically called long short-term memory.
It's the ability to remember something and use it later on.
So what this little blue cube does is that it goes and look at the big cube, and then if the big cube is red, it goes to that side, and if it's orange, goes to the other side.
But it has to do two things again, yeah?
It has to use raycast, identify the color of the first cube, and then decide where to go and find its way over there and end up in the right corner.
So again, two different things, yeah?
So it walks, goes to the orange side, this means goes to the red side, go to the orange side, looks at it, and then go to the orange side, yeah?
So it's an example of the use of specific, of memory, yeah?
I think that's fascinating too.
With the Unity engine, we have high fidelity graphics, we have physics, we have rich assets.
I mentioned the radar, light, stuff like that, our sensors, and you can create scalable simulations where in theory you can run 100,000 instances of Unity and gather data.
And, oh, sorry, I went too fast here.
I just wanna show this if it runs.
Is it running?
Yeah, here we go.
We are working with Microsoft and the city of Bellevue.
And the idea here is to actually train a machine learning system to detect dangerous.
traffic situations through cameras.
But the traditional way of doing this would be to record hours and hours of video and have a thousand people hand labeling the video, etc.
To train the machine learning system.
And what we're doing is we're showing that you can actually create these scenarios that in unity that will basically kind of fool the machine learning system and think this is the real world and train it.
And on a very sad day like today.
when there was a pedestrian killed by an Uber self-driving vehicle.
This is kind of a way to train vehicles, train machine learning systems.
in an artificial environment before you actually bring them out in the real world.
So both for cars, for robots and also for systems that actually deal with what we call real world situations.
It's a fantastic environment for doing that.
And this is sort of when you look at ML agents and then you kind of put them on steroids.
OK, that was all I had for today and I appreciate your time.
