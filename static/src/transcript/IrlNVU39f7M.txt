Hey everybody, how's it going? My name is Colin Foran, and today we are here to talk about the Westworld VR experience.
I've kind of crammed a lot in here, so I think the time's going to get a little tight.
Just wanted to let everybody know that we'll be hanging out outside if anyone has any questions, and otherwise I'm just going to try to punch through it.
My name is Colin Foran and I'm the creative lead for an internal HBO team that prototypes interactive experiences.
My background is in art and design, first in concept art, art direction, and now creative direction.
The HBO Interactive team is around 15 team members, all from a wide range of backgrounds.
We're a combination of artists, designers, and engineers.
The team has game experience from the indie through AAA space, movies, enterprise software, and so on.
The Westworld VR Project is a really interesting cross-pollination between the game and film industries. This talk will broadly explore the issues that we encountered and how we used a combination of design and tech tools to get past them. But first, some quick background.
When the HBO Interactive team was formed around three years ago, the task was figure out what's going to be interesting and then go make it. It was a very experimental, fail fast, learn fast team with an explicit focus on narrative. It was very important to us.
Our jobs were to identify cool tech coming down the pipe, bang on it for a little while, and build up a little production expertise to have ready when a showrunner approached us with an idea.
That led to a lot of greyboxing, test shoots, and demos across a handful of platforms.
Engines and tech, not strictly VR, but always with a focus on storytelling potential.
We were plugged into the development scene in Seattle where we were able to get our hands on VR prototypes pretty early.
During that period, we were doing a lot with navigation, scale, tethered, untethered, stylization versus realism, showing player body, all the usual suspects that I'm sure will be familiar to everyone in here.
At the same time, we were building relationships with the showrunner staff and getting up to speed on how the rest of HBO's content is produced.
Different productions are siloed from one another, so we spent a lot of our early days banging on doors and introducing ourselves to people.
That turned into months of roadshows, showcasing our early demos and VR work.
We call it VRpalooza, and that started around December of 2014.
We found a show-don't-tell approach worked best, so our demos started looking pretty sophisticated.
Not quite vertical slices, but with a certain degree of polish for sure.
We had a broad range of people and partners and were able to drum up a lot of internal interest around HBO's potential spin on VR, specifically trying to advance the state of character-based storytelling.
But we knew a lot of thought had to go into the stories that would feel appropriate to the platform.
After a final round of internal demos, we were encouraged to meet with the Westworld showrunners.
We met Jonah Nolan and Lisa Joy in June of 2015.
They explained that the series would be about having your perceptions manipulated and the effects that could have on a person in a controlled environment.
The idea of doing that from a first-person perspective was fascinating to everyone and the pitch came together fairly quickly.
That kicked off about a year's worth of focused prototyping and development.
That started a surprisingly open working relationship.
Throughout development, we worked closely with the show's writers, production staff, and the show runners themselves to ensure we were making something interesting and true to the show.
Now we needed to figure out what we wanted to make.
So what is it?
Westworld VR is a multi-format, room-scale, destination experience in three-ish parts, with a strong focus on narrative and character.
We debuted the experience at TechCrunch shortly before the show premiered.
It combines room-scale real-time content, 360 video, physical props, and an in-universe physical build-out staffed by actors.
The story itself was written with the Westworld showrunners and writing staff and created interesting challenges for the team to tease out.
Meaningful character interaction in VR, onset 360 capture that progressed the story and felt true to the show, as well as traditionally difficult film set challenges like, for example, micing naked actors doing stunt shoots.
Any one of those on their own would be challenging and combining them in a way that felt coherent and really contributed to the story made it even more so.
So let me walk you through the user experience.
The user is greeted by actors at an in-universe Delos pop-up installation.
They create a reservation and spend some time chatting with the actors.
When the reservation is ready, they're taken to a back room staffed by Delos tech personnel to gear up and headset.
Once they're acclimated to the equipment using an in-headset lobby area, they're transported to the Delos dressing room and greeted by a park host that they had chosen earlier during the reservation.
The host is preserved throughout the duration of the experience.
After the user gets some brief narrative background, the rules of the park and selects a weapon, they transition into the interior of the park itself.
A little more background is sprinkled into the conversation with the host and the user gets to do some light exploration of the space as well as some shooting.
A quest giver NPC appears.
In the fiction of the show, it's the sheriff and he attempts to enlist the user's help to track down Hector Escaton.
Mid-delivery, a fly, which will be familiar to viewers of this show, lands on the sheriff's face, triggering an error in the host programming.
They turn homicidal, executing your host, and eventually turning on you.
The player wakes up to a semi-conscious state surrounded by lab techs and equipment, where they're urged to sit down in a physical chair.
This abstract chapter serves as our transition from real-time to 360 content, and then the corresponding transition from fully interactive to zero interactivity.
Finally they awaken in the Delos lab, revealing that they are, in fact, hosts themselves.
After being wheeled through the lab, their companion host wakes up, attacking their captors and imploring the user to remember what has happened before all kinds of horrible things happen.
And it's lights out.
It was also a test to see how effectively HBO Interactive and the Westworld production teams could mesh pipelines.
This wasn't simply two production teams sharing assets, this was a fundamental meshing of working styles.
Writers, actors, asset creators, and producers all learning to speak the same language.
Interactive tools and their advantages are foreign to many of the production teams, because of course they are, they're different industries.
Most of our group comes from AAA gaming, and so we had a responsibility to educate and show by example what these technologies could do.
High-end game and show productions are very different, but there's enough of a shared vocabulary with slightly different meanings that it's easy for misunderstandings to pop up.
So what were we trying to accomplish?
This isn't a comprehensive list, but an idea of what we were aiming for.
What are questions that, if left unanswered, could compromise our ability to work with future showrunners?
For me, this was a vehicle to show that the technology was ready for mature, complicated stories.
People needed to look at it as an example and have no doubt that, yes, they could tell their stories using this medium.
But that's actually a pretty ambitious statement.
There are a ton of things wrapped up in that concept.
I tried to imagine questions coming from the showrunners.
Can I put one of my actors in this thing?
Can they give a meaningful performance?
How do I move around?
I know that I can't force people to look at what I want them to, so prove to me that that doesn't matter.
Anything we could do to prepare ourselves for future meetings and speak with confidence.
At a high level, it was simply building production confidence.
Based on our experiments, we could project outward and say that we could make some of this stuff.
But there was nothing that we could point to definitively and say, it'll be like that.
We had to build up a ton of trust.
We also needed to test our game design theories in a production environment.
Not to belabor the point, but remember, these were the early days.
We were past the point of knowing that, yes, we could technically do something.
But from a narrative perspective, should we?
The team was excited to roll these disparate demos and experiments into something that felt significant.
We got a ton of mileage out of our gray box work, but this was a chance to really stress our ideas.
There's a lot to try, from navigation, affordance, chapter length, transition intensity, and so on.
We certainly weren't the first to be thinking about these ideas, but we had never seen an experience that put them all together in the way that we wanted.
We also wanted a gesture towards what a high production value experience could look like by using design tricks and misdirection to punch above our weight a little bit.
We were a small team attempting to show what a larger team could potentially do.
From an environment art standpoint, that meant working hard to imply that there was more of a world to explore just around the corner, or to use dramatic lighting and suggest more mystery and depth while obscuring our production limitations.
We had similar ideas on the character side, that if users tested an interaction with a character, they would get a response implying that there was a ton of robustness behind that character.
If I tried to shoot my host, for example, they would acknowledge it and respond.
But there's only so far you can walk down that road before hitting production limitations.
Which is a good time for an aside, the first of many.
Know what you're making and for whom.
You'll hear me say this a few more times, but I think it's important to take away, or it's an important takeaway that we only learned with hindsight.
In the example I just gave, we spent a huge amount of time designing response trees to every possible interaction, which is a cool idea.
But we were making a destination-based experience that people could only realistically go through once.
If we had accepted that sooner, we could have saved ourselves a ton of production time that might have been invested elsewhere.
You can still do service to that idea in a more shallow way, but without totally over committing your team.
That is the end of the first aside.
Narrative goals.
We knew we wanted the characters to acknowledge and respond to you, but in a way that didn't torpedo the story or cause users to go into the antagonistic, how do I break this mode.
And pacing.
We wanted a slow reveal that would build over time and really land the emotional impact in our third act.
Could we actually control the spooling out of the narrative in a way that made sense?
It's kind of sort of related to the how do we get them to look where we want problem.
And also, could we build a narrative that stood on its own without knowledge of the show?
Remember, we were launching before the first season, so we couldn't take any shortcuts.
Add to that that the story was attempting to feel disorienting intentionally.
So tech, can we play with the format and medium to exaggerate shifts in perception?
Can we utilize real-time video and pre-rendered CGI in a way that feels purposefully disorienting?
Putting a handful of ingredients into a bowl does not make soup.
It needed to be coherent.
This one felt very high risk, high reward.
We didn't want to proclaim that real-time or video was the definitive future of VR, but rather that they were both tools in a toolbox.
Similarly, can we play with real world objects?
Is adding one more thing just going to confuse the story?
After running through a few prototypes that experimented with the idea, we knew we wanted physical interactions.
But it had to be in a way that adhered to the spirit of the show, same as the last point.
It had to feel coherent and no gimmicks.
Can we rely on interactions and room scale?
Remember, these are the early days.
There was a lot of design conversation around how to build an experience that wouldn't completely stall out if the tech failed.
In the end, the polished hardware got us where we wanted to go, but there was some doubt in the beginning and we had to design against it.
And performance. It's an oldie but a goodie.
This was a constant issue. Hitting frame rate was a nightmare for a long time.
The platform was being developed while we were trying to lock things down and it added a lot of uncertainty.
The ability to solve for performance issues by dropping more hardware into the box was absolutely a motivating factor when we decided to become a destination-based experience.
With those goals in mind, I wanted to run you through the project pillars that we established at the beginning of the project to codify what we were talking about.
It's a little dry, but I think they're important.
Humanistic, convincing real-time character interaction.
Emphasis on performance with a seamless blending of reaction to player input.
Expressive, nuanced facial performance that encourages an emotional connection between characters and the user.
Mature storytelling that acknowledges and embraces the potential of the medium.
We strive to advance the state of the art, combining and enhancing real-time and recorded content.
Our viewers leave having felt unsettled and transported.
Interactive reinforces the connection with characters and narrative.
Within that rule, a user's agency in the real-time environment corresponds to real-world expectations.
We are not staging a play in front of the user.
We are embracing presence to transport them to a convincing location that responds to them.
We are always faithful to the Westworld story as defined by the writing team.
That comes up later.
Headset-centric content creation on the production side, thoughtfully target fidelity to an asset or video's use.
Asset detail increases or falls off with proximity to the player.
And sets our stage to maintain viewer interest.
As much as possible, we don't author or capture detail that will be lost in the headset.
Within the limitations of our hardware, we are targeting realistic real-time scenes and high-definition video capture, all of which sounds very obvious, but at the time we struggled with it.
So how did it go?
What landed and what didn't?
Before we touch on anything else, we need to start with communication.
One principle we continually return to was delivering in the spirit of the request.
There were times that, because of our lack of shared vocabulary, the showrunners would ask for something that would completely freak out our team.
It was important during those moments to step back and say, all right, we know we can't literally do this thing, but how can we deliver in the spirit of the thing?
The best example of this was a request from the writers to have users manhandled by lab techs during the transition from interactive to video, actually having an actor tech force you into a seat.
That was a non-starter for a handful of reasons, but it was important to dig into the emotional response that they were searching for with that request.
The idea of uncertainty or helplessness was absolutely appropriate for the story at that point.
We were able to target that in a way that respected user comfort in the overall safety contract the user has with the people running the installation.
Similarly, there were a handful of times where our production wires simply got crossed.
A request would come in, for example, to change an outfit.
As an art lead, my mind went immediately to a game-centric pipeline ramification mode.
How could I explain that if we took the time to change an outfit, none of our characters would have hair because the same artist was responsible for both, when in reality, the comment may simply be, the characters are looking kind of samey, which is totally valid and much simpler to fix.
Tweak some color blocking, and you're good to go.
And frankly, the show staff are very, very smart people.
They have great ideas and knew the source content way better than we ever could.
If you get too caught up in production minutia, you risk missing out on some really interesting ideas.
Aside number two.
A challenge we had early on was the show defining real.
Real on this platform is a pretty loaded word.
I'm sure most of the people in this room equate real with a feeling of presence that we haven't seen in games before.
Real for the show meant video.
It was the truest possible representation of an actor's performance, which was the most important thing that they wanted to preserve.
There was also a feeling that no matter how good our assets looked, they would never look as good as the real footage.
We actually did a fair amount of R&D before committing to real-time characters.
We did a handful of experiments around stereoscopic billboards and volumetric video.
But there were issues around hitting the quality level we were looking for in the various performance considerations.
It was really track controllers that won the day for real-time.
We had to spend a lot of time making the case that internal consistency in an environment was more important than surface level fidelity.
If we dropped a prerecorded stereo billboard into a real-time scene, it wouldn't matter that the performance you captured was more real, it wouldn't cohese with the rest of the scene and only serve to pull you out of the experience.
But that brings us back to working towards the spirit of the request.
What they were after was nuanced, humanistic performance.
Once we proved we could do that within the limitations of the platform, we had them on board.
That's not to say we didn't find a ton of value in experimenting, though.
We did a lot of experimentation around different ways to get video into the experience, but that's when you need the production discipline to say, no, let's revisit the pillars.
The billboard environments we did were cool, and there was probably a satisfying Westworld experience in there somewhere, but in the context of our pillars, they just weren't right for the experience.
That, I know it might not work, but let's try it anyway to see if anything interesting falls out, is a very cool way to work, if not a little spooky from a production perspective.
So with that out of the way, we can talk nuts and bolts.
Our interactive pipeline would be familiar to anyone here.
Storyboarding, concept art, modeling, PBR texturing, integration into the engine, so on and so on.
The only real hiccup here was recalibrating to our fairly aggressive hardware limitations.
We tried to get around that by building out beautiful corners and vertical slices to aggressively zero in on appropriate fidelity.
It takes a little bit longer in the beginning, but it's worth doing.
If things got too heavy, they got triaged out.
Digital doubles and their blends were hand sculpted.
I did not write a what I would do differently section, but if I did, I would say that we would almost certainly use scans going forward.
It was a significant production hit.
In general, quality increased over time as Epic made continual optimizations to the engine.
Props and environments went through a familiar process, concept, model, texture, etc.
There are a lot of interesting design conversations around what to keep faithful to reality and what to exaggerate for the sake of drama or wow factor in VR.
In the case of this revolver, it's like this big, but it feels cooler to feel in the headset.
Some more environment art.
Some more VFX dev look.
Animation was our long pole and would continue to cause headaches throughout the production.
We used mocap for face and bodies and continually struggled with how to incorporate edits from the show writers as well as achieve a humanistic feeling of interactivity with the hosts.
One always seemed to damage the other.
And there are two elements to that, I think.
One was our trying to build in more animation robustness than we needed.
That goes back to the know what it is you're making point from earlier.
The other was not having a solid solution for incorporating notes and feedback from the performance from the show.
Our desire to show, not tell for the writers was directly at odds with our mocap pipeline.
We were able to fake it a little bit with hand animation, asset store assets, and in-house grungy mocap, but that only got us so far.
We wouldn't be able to really evaluate our performances until fairly late in the project.
And I don't know a way around that.
We did loads of storyboarding and block out and headset, but sometimes you just can't make a call until you see it.
A similar thing can be said for ADR.
There were certain bits of dialogue we just couldn't evaluate until we heard it all in context.
We should have planned for the need to do a full ADR session from the beginning, but we let it surprise us.
And I think that's my takeaway from the whole thing.
In the end, we over-engineered some things while allowing ourselves to be surprised by others.
Anecdote time again.
Bandits versus Sheriff, aggressively cutting things that don't work.
Originally, you had the user attacked by two bandits that rode onto the bluff, menaced the roadhouse, and murdered the players.
We wanted to capitalize on scenes from the show.
A user would point their gun at the bandits, fire, and nothing would happen.
That was originally the point at which the user would understand that they were, in fact, a host.
Bandits felt like a satisfying mission setup, but it was confusing to people.
Why couldn't I shoot the bandits?
Did I do something wrong?
I tried reloading the gun, and it would let me, et cetera, et cetera.
The bandits weren't working and no matter how many small tweaks or adjustments we made we couldn't get them to read correctly.
So we did a script rewrite and cut them.
We replaced them with our kindly sheriff.
Whether people recognize the character from the show or not, we could rely on the symbol of a sheriff and the benevolence implied there to more clearly convey the state change to the viewer when the fly lands on his face.
It was really challenging to cut that much content, especially the mocap, from the experience, but it had to be done for clarity's sake.
Affordance.
We spent a lot of time throughout the project developing a strategy around the dividing line between traditional game UX and game design object interaction.
There was a lot of design and conversation around gamey user communication, like glowing outlines on interactive objects versus trying to keep things discoverable.
In the end, we tried for physical where possible, but if an interaction was worse without explicit call-outs, then we would put them in.
I wish it was cleaner than that, but there you go.
Another benefit of the experience as a destination is that we had total control over the user experience.
It didn't end up happening, but if we ever got into a situation where someone simply wasn't getting an interaction or becoming stuck, we knew we had the actors available to provide feedback or guidance through a microphone.
That kept us from having to rely too heavily on voiceover, text overlay, et cetera.
In the headset or it doesn't count.
It was kind of incredible how often we would land on a design idea that we were totally confident in, only to have it fall down once it was in the headset.
That also taught us not to get too precious with our ideas.
If the wine is bad, spit it out.
That applied to the general environment layout as well.
We spent a lot of time with a variety of gray box testing whether or not an environment should match the show perfectly or be altered drastically to meet the physical borders of the play volume, or be some combination of the two.
And in general, we struggled with affordance.
We had the general design challenge of wanting items to behave as you'd expect them to in the real world.
We were able to solve for a lot of that by keeping things just out of reach and implying that they could be interacted with.
I worried about that approach early on, but people bought into it without any problems.
The chute. Our chute took place on location in the Delos lab set.
I don't think I've ever seen a location more hostile to the idea of a 360 chute.
Rectilinear patterns everywhere, nested reflected surfaces, tight corridors, and dramatic lighting all promise to make stitching and processing a nightmare.
Thankfully, we had the support of Spherica for capture and processing, and the Westworld production team for set support on the data chute.
Of course, lighting, sound, and blocking all had to be approached differently, and watching the department leads solve these problems, often for the first time, was very impressive.
Sometimes we just had to improvise.
For this shoot, we didn't hire a director of photography.
Rather, I was on set explaining what we needed different elements of the shot to do, and then we blocked the set accordingly.
They came together, but some issues were more difficult to address than others.
For example, we had to move Video Village off set, but due to the range, the monitors we were using continually ran in a wireless signal drop-off.
Eventually, we were able to duct tape a solution together, but it's those sorts of issues you won't see coming until you're in the middle of it.
The group had done a lot of experimentation with mono and stereo 360 video before, but this was on a totally different scale.
We spent a ton of time both before and on the day of the rehearsals experimenting with single shots, crossfades, and so on.
We were terrified of introducing nausea, but in the end we were being too conservative.
Eventually the director made a hard push for the one long continuous shot.
with all of the turns involved with that.
We were spooked, but found that if you were gradual enough in your movements, we didn't introduce any discomfort.
We also had the wheelchair follow the outside of the room, meaning that the user always had a dark wall to one side of them for most of the trip.
That had the double benefit of allowing us to block action and guide gaze, as well as give users some visual rest if they needed it.
And then there were the typical issues that you would run into on any production, but were new to our team.
Casting was especially difficult.
I had worked with actors in games to capture a likeness, but the contract negotiations around live-action casting were new to me.
For this experience, we needed actors that were comfortable with a digital double in all of its associated usage and distribution, nudity, and stunt work.
You can imagine that pool of people getting progressively smaller and smaller as you go digging.
And then the questions we never would have thought to ask.
How much is a squib?
How do you hide it during a 360 shoot on a naked person?
All of it needed to be figured out pretty quickly.
And I'm really impressed with the results that we were able to achieve.
And a quick note on audio.
Our friend Chris Hegstrom will be giving a talk around some of the interesting work we did with sound capture and emitters.
I encourage you to attend that if you can.
And that is a horse.
We had a horse on set.
It was a nightmare.
The build-out. The build-out was pretty extravagant, with six actors on site at any given time to take reservations, make people feel comfortable, and get them through the experience.
We had front-room actors dressed as hosts who would interact with people, take their reservations, and get them excited about the universe.
We also had backroom techs that would run the booths, manage the builds, and offer any assistance needed during the demos. All of it was done in fiction.
The build-out itself supported two play spaces behind a large, outward-facing lobby facade area.
This was incredibly effective.
People loved it, and while I was a little spooked at the idea of actors improvising in world dialogue or people trying to trip them up, they did it wonderfully.
People really responded to the feeling of one-on-one interaction with the actors.
It made it feel much more personal.
Reception.
Our feedback was extraordinarily positive.
We agonized over the balance between making things feel mysterious versus clear for a year and a half, and I was shocked at how well people seemed to follow the story.
We have a few interesting pieces of data here.
We premiered at TechCrunch before the show was out.
But our next showing was at New York City Comic-Con after the show had premiered.
We got two different but equally positive reactions.
For the people that hadn't seen the show, they loved being swept along on an experience that felt large and interconnected.
The broad emotional strokes that we were using were enough to make people feel satisfied.
For people that were familiar with the show, we had a much more specific reaction.
People were thrilled to see familiar reactions and hear familiar lines.
Specifically, the fly landing on the sheriff's face was a crowd pleaser.
In VR, of course, that fly is the size of a golf ball, and we were still afraid people wouldn't be able to notice it.
But post-show, people snapped to it immediately and totally lost their minds.
Everyone understood where they were and what was happening.
It was very satisfying.
The people familiar with the show also wanted to push the bounds of the park.
There was a lot of, I wish I could have gotten on a horse and then robbed a bank and then climbed a mountain, which at that point, you're just describing Westworld.
We couldn't make that.
But that's good.
It's better to leave people wanting more.
Though we were wary about it, we didn't have any negative responses to the violence.
We spent a lot of time balancing the faithfulness to the show with an awareness that we had to be careful with how intense we went.
We used to call that knowing how hard to swing the bat.
Action that felt right at home in a game or a film could feel way too intense in someone's personal space.
And we did have a few people quit early due to sensitivities around gun violence, but no extreme reactions. It was all very positive.
People can acclimate to a lot if you give them time.
We were really concerned with removing people's agency midway through the experience.
If someone told me that they wanted to pitch that now, I would tell them it was a bad idea.
But if you give them a narrative justification for it, they'll buy it.
The experience was originally much more interactive, but we weren't happy with how it detracted from the story.
We could poke and nudge shirts, pix everything up, open drawers, and so on.
This concept applied to the interaction with your host as well.
How close can you get physically with the host without feeling like you're daring the player to interact?
Some version of this discussion existed throughout production.
It's easy to say that I'm satisfied with where we landed now.
But after a while, indecision and half-answers to these questions were coming to a head and bogging down production.
I had been agonizing over this for a long time and finally came into the office with a new proclamation.
If it didn't support the narrative, it was out.
The conversations around prop interaction, gun loading, branching, secondary narratives, all of it had to satisfy that condition.
This was highly contentious on the team.
Are we turning our backs on the promise of the platform by doing that?
My parting thought, I think, is that the onus is on the content creator to present the user with something that feels intentional and concise.
If you don't know what it is you're making, the user certainly won't.
A great exercise was zeroing in on our most inexperienced user possible, someone who had never used high-end VR, had probably never used a mobile headset, and to push it even further, had never held a console controller in their hand.
Think of all the things that you lose that you took for granted once that gets stripped away.
If we were making an arena shooter, our criteria absolutely would have been different.
But since we were event-based and couldn't control who went through, we had to prioritize simplicity.
To emphasize that even further, our experience was around 10 minutes long.
We couldn't support long, thoughtful education loops on things like gun reloading with our players.
So for the target audience I just described, the person that had never held a game pad before, Things just needed to work or we risk frustrating and pulling people out of the experience.
Often this meant going the simpler route.
Rather than a complicated system of prompts and tests to load individual bullets into the gun, for example, we auto-loaded.
Did we lose out on an engaging interaction?
I think we did.
But we also ensured people's minds would be on the story and not how they couldn't get the reload action to work.
You may not like the cuts you're forced to make, but the experience will be better for them.
That is my time.
I think we have something like five minutes left if we wanted to do any questions.
And so we'll do our best to make it through.
And like I said, anything that we don't get to, I'm going to grab a coffee and hang out in the lobby.
So I'm happy to talk to anyone.
Hey, you talked about having this interaction gradient. That's a really interesting point. I look at other VR experiences like Job Simulator and the creators of that really, they talk about trying to focus on having a level amount of interaction within their systems. Can you talk a bit about how you, with that gradient, how you set up the expectation for the user to be accustomed to losing interactivity or gaining it back?
I think it's easy to look back on that now and say, just do whatever feels appropriate.
But there was a huge amount of fluctuation throughout the entire experience.
Job simulator is an easy one, because of course, as soon as everyone goes through it, you say, oh my god, I want it to be like job simulator.
But that was not appropriate for the thing that we were making.
We got away with what we did because it was a very gradual removing of agency for people.
There was always a narrative explanation for it.
I think if we had hopped around or gone more intense, then pulled back, then gone back intense again, it would have felt disjointed.
And it wouldn't really have cohesed in the way that we wanted it to.
Hello there. Thank you for that. Wonderful. And I love the show as well.
So just a quick question about sort of, I mean, there's a lot of VR experiences featuring violence and shooting and that kind of stuff.
And also just sort of looking around and wow, you know, I'm in VR.
But did you kind of experiment with any kind of, any sort of positive or sort of more nuanced emotional interactions with the hosts?
Certainly, that was the entire first chapter.
So a lot of that is you are in the dressing room.
Your host comes towards you, gives you a selection to make, walks around, gives you another selection, and is generally very warm and flirtatious.
It was harder to do, but we were intentionally aiming towards something much more subtle.
And it all played through the face.
So it was never a quest giver in a T-pose behind a table.
It was a lot of leaning forward, reaching for your hand, trying to make eye contact.
Looking back on it now, that certainly doesn't telegraph as well as the violence did.
But as a statement of intent, I'm happy with where we landed with it.
Thank you.
Did you experiment with voice input or anything like that?
Way, way early days.
In the beginning, it was still very fill fast, learn fast.
And so we threw everything at it.
Everyone wants it.
I think it's very challenging to play through an experience like this where someone is talking with you in an intimate way and not want to respond to them.
We just couldn't get the technology to stand up.
So I mean, the way, way next one, totally.
Hi, thanks for the talk.
Did you, can you quantify the number of people that came through the experience?
And then I had just a separate question.
In terms of the feedback for people that experienced, can you qualify the reactions to people based on...
There's a very different sort of pipeline for character creation that you described, video and CG characters.
Is there any sort of feedback from the users about the quality of those characters and whether they found something more rich and detailed?
And that'd be great to hear.
But as far as people that had gone through, I think we topped out at several hundred.
It was across a couple of conventions.
So a couple of booths running all day, getting as many people through as we could at about 10 minutes a pop.
So it's around several hundred people.
As far as the feedback.
Honestly, it's kind of a tricky one because I think we got a little bit more of a boost than we expected by, oh my god, it's my first time through VR and this is amazing, you could have showed me anything.
We did specifically pick at that intersection between the digital doubles and the actors transitioning.
I think we were able to get away with more because we allowed each of those time to breathe and didn't really insist on it.
So in the first two chapters where you're interacting with your digital double.
It doesn't feel hurried.
And then we have that interstitial chapter where you don't see them again.
And then when you have them walk by.
in the 360 video portion and finally have them go through their climax.
It's one, I think, either of those stand on their own.
So if you don't make the connection, you're not penalized for it.
And second, it's a very passive connection.
We're not saying, oh my god, identify that this was your host companion or something negative will happen to you.
At that time, it's very explicitly passive, and you can just sort of take in what's happening in the scene.
So.
I take from that that we did a very good job.
But I don't think that that's honest critique, really.
I think we really did get a boost from people just kind of being dumbfounded by what was happening around them.
Is that time?
That puts us at 30?
No, we've got time for one more.
No one's yelling at me yet.
I was just curious when you mentioned that you were able to keep production time of it down by having a smaller number of affordances in given scenes.
In scenes where it looked like there were a lot of objects like under glass cases and everything which I assume to eliminate that kind of interaction, did you find any users kind of like wishing, oh man, like I wish I could have opened that cabinet or something or like being frustrated with seeing things kind of just out of reach or was just the presence enough to really enhance the experience for them?
Everyone tries it once, but after trying and not getting anything out of it, it completely goes away.
So in the beginning, all of that stuff could be interacted with.
And what we ended up doing, in a way that was really effective, was just putting everything outside of the fence that pops up.
So it's like you try to walk over to the coats to play with the coats, and then you never think about it again.
When we invited people to do it with some things and not others, that's when the whole experience fell down.
So at that point, we could not get their attentions back because they were trying to wind a watch or throw a horseshoe or something.
So we found it was a little upsetting because we had spent so much time laboring over each of those interactions and trying to make them feel good.
But in the end, it was, rip it out.
It's just not working.
It is a possible positive solution for an experience, just not this one.
I think that's it.
Thank you, everybody.
