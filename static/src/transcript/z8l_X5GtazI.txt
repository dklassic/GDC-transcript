Thank you for joining us today. My name is Romain and I'm a senior gameplay engineer at ILMxLAB, which is Lucasfilm's immersive entertainment studio.
Our core mission at xLAB is to pioneer a new era of interactive storytelling via connected stories that cross platforms.
The goal is to move from storytelling to story living where we invite you to step inside living worlds to meet beloved characters and become the hero of your own personal adventure.
We're excited to talk to you today about how we built an expansive Star Wars world in mobile VR with Star Wars Tales from the Galaxy's Edge.
Tales began in the fall of 2019. It was a continuation of our partnership with Oculus Studios, which previously we released a very immortal, the VR series, for.
That experience delivered on the Oculus Rift, which is desktop powered, and the Oculus Quest, which was Oculus' first untethered mobile VR headset.
Similar to Vader, Tales is also a flagship title for the recently released Oculus Quest 2, which is a more powerful entry into the mobile VR market.
But even though our experience was to be featured for the Quest 2, we still had to deliver a performance experience for Quest 1.
And that's what we're gonna tell you about how we did it.
So now in mobile VR, there are challenges that we've run into, especially for a platform that is not as powerful as Quest 2, which is Quest 1.
Some of those challenges include explorable world with long sightlines, there are more characters, more interactive cinematics, lots of weapons, inventory, collectibles, and of course lots of combat.
All of this challenges the device.
First, what I want to tell you about is the Game Thread challenges we ran into.
So in this project, I was the lead Game Thread performance engineer.
And we ran into a few issues.
Some of those I'm going to mention here.
and how we went about addressing them.
Some of those issues include moving complex component hierarchies, spawning and despawning hitches, ticks during combats that are not needed, and stalls on the CPU.
So first, component hierarchy.
So it's, as a rule of thumb, the bigger a component hierarchy is, the longer it will take to move.
So there are a few tips to know whenever we run into those kinds of issues.
First is to try as much as you can to use actor components instead of scene components, especially for things that don't need to have a transform all the time that is up to date.
Things that are not visual, for instance.
Second is to use Unreal's built-in scoped movement, which allows you to do all the transform calculations ahead of time and only do the move once at the end.
Which goes into the next point of making sure you only move heavy hierarchies once a frame at most.
if possible, of course. Lastly, we managed to build a system to detach components that were not needed and to reattach them when needed.
And I'm talking about components like VFX or audio. Things are not present all the time, but that might show up every once in a while.
Now there's a special case I want to address, skeletal meshes.
So on top of the issues that I have detailed before, skeletal meshes have to update their bones twice, once when the character moves and the second time when the animation evaluation ends.
This is especially a problem if you have lots of bones, if you have lots of things attached to your bones, things that themselves have lots of subcomponents as well.
What we did for those is what I called the detachment optimization.
So for this, we detached the skeletal mesh from its parent component.
And instead of moving the components, we used the anim graph to move all the bones where they're supposed to be.
So basically, we saved the transform that the component was supposed to have and put it on the root bone.
This was great.
It helped us a lot, especially during combat, where we have lots of characters on screen.
We used it on the player pawn, all enemies, and some of the cinematic characters, too.
There obviously are some drawbacks. You can't use the transform of the component anymore, you have to use the root bone transform.
And some anim nodes don't know how to work with this, so you have to go in and fix them.
Thankfully we managed to keep that, although it didn't take us too long to do that.
So another issue that comes about when moving component hierarchies is overlaps.
There are a lot of times where overlaps are turned on, whether it's by default or it's people thinking they're doing the right thing and turning them on.
There's a lot of cases where actually you don't need overlaps.
So, for instance, when you're only going to be doing traces on a specific collision.
So it's great to educate people as to how collision and physics work, what the settings are for and when to turn them on.
and definitely make sure you tell them to leave things off by default unless needed.
And finally, one thing that has helped us a couple times is to switch to a list of targets as opposed to collisions. For instance, if you have a sphere collision that only detects enemies, you can maybe instead keep a list of the enemies and just do a distance check against them. That will take a lot less time.
All right, so moving on from components, now we have hitches.
There are hitches in Unreal Engine, and especially when spawning and despawning actors and components.
So the best weapon we have to fight against those is to use a pooling system.
There isn't one built into Unreal, so we had to roll out our own.
And it helped us a lot.
Basically, the concept is you spawn everything ahead of time, you hide them and use them when needed.
And when they're done, you re-hide them again.
Make sure to turn off all the ticks, collision, everything.
It's great. It helped us get rid of most of the hitches.
It also allowed us to use caps for all our enemies and for also weapons and lots of items that could potentially balloon otherwise.
Another thing it helps is with spawning and despawning of objects when they're far away. So that's my next slide.
During combat, and even when you're just exploring the world, it's very common to see things ticking, rendering.
or even updating their collision when they're really far away and you don't actually care about all this.
So what we did for that is to use a player distance system that we put on almost all of our objects, interactive objects and enemies, well not enemies.
The idea is to reduce the impact when you're not close to the items.
And we leverage our pulling system to spawn the items when you're close enough and despawn them when you're far away.
So that was a great help.
All right, finally, and I want to say last but not least, game thread stalls.
On Quest, especially Quest 1, we only have three cores that we can use.
The rest are reserved.
And in heavy moments such as combat, we are using all of those very intensely.
By rendering, by audio, gameplay, everything is acting up all at once.
So it's really hard to address stalls.
Sometimes they're just there because there's just too much work to do.
But there are some cases where you can solve some of them.
In order to do that, we've used a tool called Unreal Insights that you may be familiar with.
This tool is aiming to replace the stats capture that we've been doing in the past.
I think it's great personally.
I think there's still a little bit of work to do on it, but we've been using it with great success.
It's great for people who are more visual in understanding things.
It helps you see what things are happening in which order.
It also has a smaller impact on performance than the stats capture, because it doesn't have to save everything to a file.
It just sends it over the network.
Now it's a little bit hard to set up on Quest.
There are some resources online that are not too hard to find.
And as of our version of Unreal, we didn't have any of the object names that were causing the stats.
So it was a little bit tricky to interpret.
Thankfully, we're able to go in the engine and add some of that back.
So hopefully we can see that in the future version of the engine.
So with this tool, we were able to see one thing in particular.
And if you can see on this image, there is a stall that I've circled.
And right after you can see a parallel animation evaluation that's completing.
Thanks to this graph, we were able to determine that One of our skeletal meshes had a prerequisite on it that was forcing it to complete in an earlier tick than what we were okay with. By fixing this, we were able to let the animation complete in parallel and finish way later in the frame, which saves us some of the stall.
So to use Unreal Insights the most effectively, the way we did, it's critical in my mind to understand the task graph system, so dive a little bit if you have to.
And definitely be careful of tick prerequisites because those get added automatically when you parent components to each other.
So keep an eye out for those things.
I want to close the game thread parts with some miscellaneous tips. Some of those are obvious, but I just thought I'd mention it.
No blueprint ticks. Please keep everything in native because you will have to do it at some point. I guarantee it. You will have to move things to native.
Except of course in some very trivial cases.
In the same vein, no blueprint implementable or blueprint native events on ticks, because those incur the same kind of overhead as the regular blueprint tick.
And even with blueprint native events that are not implemented in blueprints, you will have to maybe remove that.
Favorite non-dynamic delegates. Dynamic delegates actually use similar tech under the hood as blueprint does, so you will also have an overhead with that. So try to keep those two minimum.
Finally, beware of blueprint timers.
So the blueprint timers actually show up in a different area of the stats capture than the regular ticks.
So in our case, it took us almost until the end of development to realize that we had three millisecond hitches happening very frequently in the timer and that were causing some of our issues that we just couldn't see happening in the regular tick section.
So definitely remember that and keep an eye out for those.
Now.
Game thread is not the only thing that we've had challenges with.
Of course, we've been trying to push our rendering, and I'm going to hand it over to Jeff to talk about our rendering challenges on this project.
Thanks, Romain. Hi, I'm Jeff Gridby, Technical Art Director on Tales from the Galaxy's Edge.
As TAD, it was my responsibility to oversee the render side, performance and optimization approach, as well as investigate techniques for visual enhancements.
This was an exciting opportunity for us to partner both with Disney theme parks, Lucasfilm Storytelling, to give our fans an opportunity to step well beyond Black Spire Outposts and explore the lands of Batuu.
We knew early on from our experiences with Vader Immortal, thorough optimization was going to be essential to bringing about our rich environments and interactive storytelling content to life.
Unreal Engine has numerous built-in methods of optimization.
I'm going to share with you some of the methods we chose to use, enhance, as well as some of the features we implemented.
We want to achieve vast, diverse regions for you to explore.
And to do so, object inclusion is going to be vital to keeping our performance metrics down.
This was by far our best means of performance gains.
We want systems that were thorough, automated, and avoided human error, and were also computationally efficient.
Above all, we want our RSTeams to spend more time building our worlds and less time focused on optimizing them.
For mesh occlusion, we opted to use Unreal's precomputed visibility system.
It's a powerful tool in that its simplicity is set up, as well as its automation tied with light beak generation.
It works by scattering cells over every single static mesh within a volume, emitting rays from those cells to detect mesh boundaries.
Unfortunately, though, most of the default configurations lead to noticeable bumps and inaccurate occlusion queries.
To remedy this.
the sample settings need to be set extremely high for accurate results, especially with long sight lines filled with various obstacles.
We also needed many cells to increase the occlusion effectiveness.
Too large of a cell size and we ended up just drawing too much.
That said, higher sample settings can lead to exponential computation times.
So it's critical for the effectiveness of this tool to limit the amount of cells.
And the only option to do that is to hand place individual volumes over each location where you want these visibility cells placed.
In a large map, this can be extremely difficult to maintain.
So instead, to reduce the computation time and retain accuracy, we modified the system to only place cells on specified meshes.
We also exposed a method to export the navigation mesh's geometry, which we could then use for selective cell placement.
This approach dropped us from roughly 40,000 cells using individually hand-placed volumes to only 5,000 cells.
And another nice perk was that we only needed to place a single volume to encompass the entire map, which made maintenance a breeze.
We also had a player jetpack to account for, which tripled the potential player height.
But with our cell count nice and low, we added some logic to support cell stacking.
Here, we were able to add three layers of additional cells to cover each potential player height from the jetpack.
This way, the cells were made nice and small to maximize occlusion.
We also exposed controls within the world settings to as well as mesh quantity sampling thresholds to circumvent fluctuations in the amount of meshes that we needed to process.
Early on in our development, those mesh quantities would shift drastically.
This helped to provide us predictable competition times for pre-computed visibility.
One last bit that was troublesome was how each run of the pre-computed visibility system would generate fresh occlusion IDs.
with every static mesh and store those IDs within the levels containing those meshes.
That typically meant dirtying every level within a map.
Unless those levels were available to be saved, you ran the risk of a potential update and essentially an incorrect ID map.
Even when all maps were saved, the ID structure deteriorates rapidly with mesh additions and modifications.
And you're stuck with a flickering incorrect occlusion queries.
With an up-to-date editor session, though, with a not up-to-date editor session, it can be very difficult to determine what to optimize.
So we modified the system to store the mapping IDs in the persistent level, tying the static mesh IDs to their footpaths.
This provided us a system that would not deteriorate and remain valid.
With our iCard precomputed visibilities now in Editor, we set up scene cameras to run through our maps, not only to optimize and visualize our precomputed visibility, but also to detect other objects not included.
Even with optimized special version.
Given our sightline detail, we still had significant draw calls over draw and memory consumption.
We were averaging roughly 600 draw calls in GPU well within the 20s.
And we knew we had to perform some form of texture at the scene and mesh combining.
So once again, we looked into Unreal's built-in performance solutions for the web.
Two solutions that stood out to us, the hierarchical LODing system, which is an automated system of mesh combining, and the Merge Actors Texture Atlas and Generation system.
optimization tasks. But we ran into some complications. The systems degraded visual quality either by overriding some of the customizations we built into our source materials or by packing some of the tileable textures into too small of a unique texture. The systems were also kind of cumbersome to implement by our artist teams and in some ways were destructive to their workflows.
Plus, we experienced numerous lighting and geometry LOD transition pops, killing our immersion.
So we wanted to see if there was a way we could leverage some of these systems with some enhancements.
Textures and audio usually accounted for the lion's share of our memory allocation.
We found that we achieved higher detail of 4K tellable textures as opposed to numerous unique textures at lower res.
To increase variation, we blended multiple tiling textures through mass regions and offset the tiling by object size.
This way, we could kit bash our environments with very small base set of meshes and still achieve a diverse look of terrain.
Through the use of custom parameters, we could drive unique mesh overrides through a common master material.
But for this to be effective at reducing drop calls, we would need to utilize hierarchical lighting technique that would support our customizations.
Unfortunately, that's where we ran into some snags with Unreal's built-in HLOD system.
So we set about to see if we could make some modifications to simplify its implementation, make it non-destructive for artist workflows, support custom material systems, and see if we could also utilize it to reduce overdraw.
First off, we modified how HLODs are clustered, opting for a very simplistic approach, parenting source meshes under an overridden exposed LOD actor.
LOD actors are the invisible HLOD actors in their level.
This provided us not only a readable, deterministic system for our artist teams to define how their HLODs are clustered, but it was also non-disruptive to their workflows.
And by exposing the LOD actor, we could control key specifics, including the ability to create our own customized HLOD meshes.
Utilizing Houdini, we built a tool that could ingest and export a set of HLOD clusters.
not only combining those meshes, preserving our material systems, but also utilizing an exported navigation mesh to scatter visibility rays to remove triangles not visible to the player region.
Taking that a step further, each H-slot would be re-sampled and LODed at varying view distances to further decimate non-visible triangles, as well as to incorporate the correct set of source mesh LODs from that distance.
And the whole process was automated and batched over our server front.
That meant we couldn't rerun the entire process in roughly 15 minutes for a full map, which typically contains somewhere around seven sub-levels at 50 to 100 page slots each.
Automation meant we were achieving far better results than what we could manually achieve.
And this drastically reduced overdraft.
And because the system was now exposed within Houdini, we could further continue to modify its performance and functionality.
In situations where the sightline was still too expensive, we leveraged tiered H-slides to cull additional triangles visible from specific vantages.
Here in the mouth of the cavern, the player could see the entire region below, which was far too expensive to draw.
Using a tier 2 H-slide, we trimmed the triangles from that particular vantage, reducing the GPU.
Once the player drops down, the sightline is contained, and we restore the view.
transition is not perceivable.
Given the H-slide's effectiveness at reducing overdraw, we opted to set our H-slides to always be visible and replace the source meshes entirely.
This had numerous benefits from removing H-slide transition pops, lighting LOD pops, saving memory by eliminating source mesh lightmaps, And it improved the PCB computation time because we had far less static meshes to compute for pre-computed visibility.
And Unreal's internal HLOD mechanics allowed for us to leverage the source mesh collisions, which were instanced, keeping our memory footprint nice and low for our collisions.
With our memory gains, we could actually go back through and increase the lightmap resolution of all of our HLODs.
Despite the effectiveness of our HLADs, we still encountered occasions of GPU performance drops, notably during intense combat.
Nothing breaks immersion like a game stall.
One thing that we pursued early on was to implement dynamic resolution on the Oculus Quest, which currently functions on Oculus Rift by dynamically modifying the render target size.
Modifying the render target size, though, on Quest was out of the question, as it requires a reallocation of memory that is far too expensive.
I'm stabbing.
One of our rendering engineers looked into mimicking this behavior by simply adjusting the viewport size.
And this had no incurred expenses.
Once functioning, we were able to gradually apply and remove this offset during moments of GPU overage, which was really helpful in just a pinch.
Those render side optimizations opened the door to applying a few render side adjustments.
We integrated the use of stereo layers in our game.
game menus and subtitles.
Since stair layers are drawn to an independent render target, they benefit from crisp text and displays with aliasing disabled.
One of our challenges though, by our engineers to overcome though, was to bring the hands over the menu.
We couldn't use a depth test.
then draw behind other objects within the scene.
But instead we found a solution contained within the Oculus SDK for Android that allows for the creation of a poke a hole material which essentially acts as a depth layer to composite against the stereo layer.
Another feature we added for increased reflection detail on surfaces.
was mobile parallax reflections.
This allowed for a lighting team to place specific lighting objects and sources to capture accurately reflected reflections within our surfaces.
Another solution our engineering team provided us with was the inclusion of decals in forward rendering.
Unreal only supports decals in deferred.
But we were able to mirror a technique from a previous GDC talk that allowed for up to two decals per primitive.
Vertex animation proved to be an efficient means for storing and delivering large amounts of animation data.
Whereas joint-driven animation roughly capped out at about 200 to 250 joints per animatable object, we found with a mobile GPU we could handily support upwards of 4,000 objects utilizing vertex animation.
This allowed for us to use traditional film tools for destruction elements.
As opposed to storing animation data within individual vertices, vertex animation rigidbodies can store their animation per component.
allow for greater quantities.
We couple this with positioning by turning an interpolation to reduce the data to exercises as well as procedural overrides to further maximize gain performance.
Unreal's hierarchical instancing system provides an efficient means at clustered draw call reduction, LOD support, and viewport frustum calling for instances.
We tapped into overriding a per instance random float.
passing individual instance arguments to drive unique characteristics and control our instances.
One example of this was our customized foliage system.
We combined several offset variations of each foliage type into a single mesh, and then use a texture driven vertex deformation along with unique baked texture atlases to create variation with minimal draw on GPU effect.
Scene lighting was then passed into the emissive materials by pre-sampling the nearest light probes to each instance.
and baking those values into a color lookup table, which we could then reference in the material using baked IDs that were set through the for instance parameter.
This approach scaled our viewable foliage from the 20s up to the 200s.
Another use case for instancing a couple of vertex animation were crowds.
Here we would take our traditional film crowd simulations, extract per-component animations, Store that within the data texture, mapping IDs to the source mesh UVs.
For Unreal, on the Unreal side, we set up a Blueprint system to assign those individual instance IDs from data files right now from Houdini.
Further variation can be achieved through packing multiple assets together, as well as texture atlasing variants.
And here's a wonderful example of that process delivered by one of our FX team members that really helped bring life to Black Spire Outposts.
Combining these processes together was how we were able to achieve the final growth system in the Jedi Legend for the engaging conclusion of Part One of our experience.
Here we needed a system that would spawn and generate corruption growth over various surfaces within our final map.
supporting hundreds of growing instance finds.
We really wanted to engage all the senses and provide you a means to interact with the environment with a light saber.
This would not have been possible without utilizing some of the techniques we presented here today.
In summary, we learned a great deal about core performance optimization methods in designing a VR project on mobile.
Using those practices as a core and breadth of our work on higher-end processors.
We'd like to give special thanks today for the support teams at Oculus, Unreal Engine, GDC for hosting this talk, and the entire XLAB team from production, engineering, QA, and the artist teams. We're really privileged to be here to represent them today.
And thank you for joining our talk today, Star Wars Tales from the Galaxy's Edge Part 2.
next time. Thank you.
