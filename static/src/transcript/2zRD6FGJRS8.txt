Hello everyone, my name is Blair, my pronouns are she, her, and this is Ambisonics and the Great Outdoors.
So, as I mentioned, my name is Blair.
I am an associate technical sound designer at Team Audio, and my credits include Modern Warfare 2019 and Warzone.
But really, I'm just a nature lover and spatial audio fanatic.
So what is spatial audio?
Well, we can break that up into three main categories.
Spatial audio is the sound's position, the space the sound is in, and the ear that hears the sound.
And we're going to go through each of these categories one by one, starting with the sound's position.
But in order to talk about a sound's position in a virtual environment, we have to understand how we perceive the position of a sound in a real environment.
So how does the human body interact with audio?
Well, we can break that up into two main categories.
The human body interacts with audio with the head-related transfer function, or HRTF, and the folds of the ears.
And these two concepts combine to create something called binaural audio.
But the head-related transform function, to clarify what that is, is it's really just the difference between when you play a sound on one side of your head and that ear hears it versus when you play it on the opposite side of the head and the same ear hears it.
It's how the shape of the head interacts with audio.
The folds of the ears, as I mentioned, that's really just positioning sound a little bit more clearly.
But that combines with the HRTF to create this concept called binaural audio.
So when we talk about binaural audio, we're talking about simulating the shape of the head and the folds of the ears.
Traditionally, however, we don't really use binaural audio in video games.
Most games use a channel-based audio approach.
So that means that for each speaker, there is a channel of audio.
There aren't any differences between the channels of audio that are being fed to the speaker system and the speaker number themselves.
Dolby Atmos, for example, is generally 714, I believe.
At least that's what this image is here.
on the right. What that means is it's got seven speakers on the horizontal plane, four speakers above, and one subwoofer.
However, when there's in this traditional game audio signal flow, the spatial resolution of the player's audio is going to be limited to the amount of speakers that they have.
And this creates an equipment inequality.
with our players.
Because ultimately we can't all have setups like this.
We can't have a million different speakers for hyper positionally accurate gameplay.
Instead, what I would suggest that we use is something called ambisonics.
Ambisonics is a technology used for the simulation and or capture of an environment's audio.
including height, by the way.
What does that mean?
Well, when we break up this sphere, this sphere of potential listening positions that sound can come from, if we break that up into, let's say, four points, that is first order ambisonics.
Nine points would be second order.
16 points is third order, up to fifth order ambisonics, which is 36 points, sample points, for each point on the sphere, rather, is broken up into... sorry, let me rephrase.
The sphere, broken up into 36 sample points, is fifth order ambisonics.
It's effectively how many speakers, virtual or actual, we break up a sphere into.
And that's ambisonics.
Ambisonics came about around the 70s, but it wasn't really widely used until virtual reality and digital signal processing.
There may be some research people who might beg to differ with me on this.
Really, digital signal processing allowed for widespread use of ambisonics.
And virtual reality, in general, allowed for consumers to use ambisonics in their gameplay experiences.
So ambisonics is a technology that provides for varying amounts of spatial resolution based on the order of ambisonics used.
So.
If we go back to that sphere of all possible positions that a sound could come from, we would sample that at four points for first-order ambisonics, up to fifth-order ambisonics, which would sample that at 36 points.
And that would result in a much higher spatial resolution or spatial accuracy for listening.
So when we talk about that spatial resolution, generally that's referred to as something with spherical harmonics.
And I'm not going to go into that because, quite frankly, it's a little bit dense.
But this is generally the diagram that people will point to.
And what I want you to take away from this is just, as the order of ambisonics increases, the more specific each sample point gets, the less amount of audio information that sample point has to communicate, but the more sample points there are.
And ambisonics allows for us to immerse our players more fully and equally.
Because ultimately, when we use ambisonics in conjunction with binaural audio, we're able to...
really mitigate this equipment inequality that comes from one player playing on a 5.1 setup, but another player playing on headphones.
Because we, as game developers, effectively need to design for multiple different speaker setups.
But ultimately, ambisonics allows for us to do so a little bit more easily.
So I believe that ambisonics provides a good solution for the sound's position, providing good, high-quality positional information.
And I believe it can also provide a solution for the space that the sound is in.
Because ambisonics, if we're using ambisonics to simulate the position of sounds, that opens up the possibility for highly spatialized, precise, and immersive convolutional reverb.
Convolution reverb is a type of reverberation, reverb, echo, whatever you want to call it, that takes a impulse response or a frequency sweep or really even a gunshot in space, a sound recorded in an environment and it differentiates that between the sounds that was played.
So by differentiating, say, a frequency sweep file and the frequency sweep played in an environment, we're able to come up with that sound information, that difference, that reverb.
But because of that reverb, sorry, let me restart that sentence.
There are no high-order ambisonic impulse responses of exteriors on the market at this time.
Because convolution reverb, while that opens up the possibility for highly spatialized and exciting information to be carried within the reverb that we have, right now, there aren't any exterior impulse responses that are in high-order ambisonic.
Think about what we could be hearing in our games.
We could be hearing sound bounce around us like a cityscape from downtown San Diego or the trees carrying and bouncing and muffling sound in a really interesting way.
We could be hearing all of these environments in our games, but because there aren't any high order ambisonic impulse responses of exteriors out there, we're not.
So I'm trying to fix that by recording third-order ambisonic impulse responses in exteriors across California.
So ambisonics is a technology that allows for the simulation and or capture of the 3D sound of an environment, including height, basically taking that sphere of possible positions and dividing it up into an amount of points.
But when we take all of that information, we need to turn that into something that the player can listen to, because the player isn't gonna have 36 speakers.
They're gonna have two, or they're gonna have a five one setup, or maybe even Dolby Atmos.
So converting that format to the ear that hears the sound is done through something called a binarization plugin.
Generally.
However, the PS5 accepts ambisonic input or object-based input from the game.
So we don't need to use a binarization plugin for that particular console.
But for the Xbox Series X and PC, while they do have native Dolby Atmos support, it's still a channel-based approach.
So it still depends on the amount of speakers that you have.
And ultimately, for developers shipping on the PC and on Xbox Series X by neuralization or the converting of, let's say, fifth order ambisonic audio.
The converting of any form of ambisonic audio to a listenable format.
That conversion or binauralization must happen on the game side via a plugin.
Luckily, there's a lot of plugins out there for this.
especially THX spatial audio, which is something that I'm very excited about personally.
Oculus Audio SDK is a tried and true method that's, I believe Phasmophobia uses this, but ultimately...
It struggles with handling more than two speakers.
Atmokey Ears allows for customizable HRTF based on age.
That is to say, it is really useful for adjusting for if you're a child or a teen, I believe, are the settings that it has.
But THX spatial audio is something that.
I'm very excited about because it allows for multiple speaker setups to be converted to.
That is to say, it allows for the conversion of ambisonic audio to a 5.1 setup, or a headphone setup, or a 7.1.4 setup, but all within the same plugin.
With binauralization, we can terrify.
Because binauralization allows for all of that positional information to be simulated.
Speakers no longer have to be physical.
They can be virtualized.
It could be right behind you, and you wouldn't have to have a speaker there.
Because ultimately, with binauralization, we're able to take the positional information from ambisonics and convert it into a smaller channel amount, whether that be stereo or Dolby Atmos.
So there are a few limitations to most binarization plugins.
For instance, the Oculus One is only stereo.
And everyone does have a different HRTF, or head-related transfer function.
So what might sound really positionally accurate for one person, depending on the HRTF that is used in the plugin, it might not for another.
So these are just things to be aware of.
However, I believe that ultimately, modern games can use ambisonics and binauralization to achieve high-quality spatial audio. That is to say, ambisonics allow for us to simulate the sound's position in a positionally accurate way. We can take an environment.
And instead of breaking it up to, let's say, a 5.1 setup, we can sample it at up to 36 different channels.
And from there, we can simulate the reverberation of that space on those channels and convert that all into something that the player can listen to regardless of their speaker setup.
Thank you.
Yeah. Questions?
You mentioned that you capture impulse, impulse responses in the city using a third order ambisonic microphone.
So I wanted to ask what I mean, where do you place the chirps, the sweeps that you play?
How many trips would you play and where would you place them?
Um, I am in the process of recording and pulse responses. I have yet to, um, I have yet to practically record in the city at this point. Um, I'm in the process of making that happen. Uh, but if I, um Off the top of my head, I would be careful with placing it directly in the center of the street because that would result in potential phasing issues.
That's really something that I want to be careful of when I do record in the city is.
I want to be careful of phasing issues, so I would probably.
Place that speaker in.
Roughly off center.
like just a little bit off-center enough so that Like if this was a If this was a two-way street, I would place it in one lane of traffic Obviously not while cars are going by but That's how I would do it because I would worry about facing otherwise.
And that way, we can say, OK, this is how one area kind of sounds.
And from there, you can kind of build out through different positions.
You can build out different types of impulse responses.
You could try it on the sidewalk.
You could try it in a tighter city street.
Yeah, there's really a million different ways to do it, but that's just how I would do it.
Got it.
And one last question.
Maybe you can give your thoughts of how to transition from 3Daf capturing to 6Daf rendering.
I mean, how would the ambisonic, a static place that the ambisonic was captured from being utilized in a 6Daf gameplay?
All right.
In terms of converting from various singular captured impulse responses to 6 DOF gameplay, I would say that...
I honestly don't have a great answer to that question, unfortunately.
Yeah, I just would record as much information as I could with individual sample points, and kind of like ambisonics, try and mitigate the...
I would try and take those sample points and build from that a particular image, if you will.
I don't...
have um I don't have a great answer for you unfortunately in regards to uh the six degrees of freedom um but that being said um I would say that if you were doing Six Degrees of Freedom, try and record with multiple microphones at the same time, potentially.
That I know Azalea has a Six Degrees of Freedom's like time-coded link setup that they use.
That's something to look into, potentially.
Thank you so much.
Hi, thanks for the talk.
Are, is ambisonics or binauralization impacted or reliant upon standardizations of like other audio protocols or standards in volume and format and platform?
Do a number of things have to be aligned to make, to maximize the effect of ambisonics in binauralization?
Is that making sense?
Yes.
Are you basically asking if ambisonics is a fit for every game?
Yes, every game and every platform.
every sound format?
Like, are there external influences that really throw a wrench in the gear of this having the desired effect?
Yes, very much so.
So I find that building a game for PC and current gen consoles, especially if that is a first or close third person, a game, like a RPG type layout, that is where ambisonics really shines because then there's uh because there is a little bit of a performance cost as I understand it.
with ambisonics because it's handling that much more information, but that's not something that, as I understand it, is much of a problem unless you're going into, say, the PS4 and potentially going into mobile and things like that. So if I was building a RTS, I would maybe not use ambisonics.
really, I'm presenting this as a tool, if that makes sense, because I'm finding that in the types of games that I have worked on and am working on in the future, I have found that Ambisonics has provided me with more information and more more gameplay information, really, for the player.
So this is all to say, no, Embisonics is not a one-size-fits-all solution.
Perfect.
Thank you.
Hi Blair.
I'm wondering as an indie developer how ambisonics might be able to be integrated into my work.
And if there are any cool ideas for gameplay that would be really awesome with ambisonics that you have and would like to share.
Yeah.
So ambisonics is ultimately That would be where you would set up your listener to use an ambisonic format.
If you're, say, making a first-person horror game, let's use Phasmophobia, for example, because I love that game, and I die a lot in it.
But if you're making, say, Phasmophobia, you would set your listener to...
third order, fifth order, ambisonics, whatever you deem gives you the right amount of balance between performance and audio information.
And from there, you can take that and when you play sounds, whether they be mono or...
Well, I think they would have to be mono in this particular case.
But when you play sounds and it's picked up by that listener, that would then get converted down to the listener's format through the binauralization plugin.
I'm so sorry.
I lost my train of thought.
Can you please repeat your question?
Yeah, I asked as an indie developer how I might be able to integrate ambisonics and if you have any awesome Gameplay ideas that would be absolutely perfect with ambisonics Okay, I think gameplay wise Ambisonics is really good at emphasizing positional information so if you want to be able to say hey, there's a sound exactly there and Ambisonics is great at that.
So really, in terms of gameplay, it's really limited by what you want to do with your sound design.
And in terms of integrating it as an indie developer, kind of like the previous question was saying, it is a bit limited by the platform and the...
Yeah, really it's limited by the platform and the genre of game.
So ultimately, I don't see any reason why an indie developer might not be able to use Ambisonics if they're shipping on the same platform as a larger developer might be.
Yeah, I hope that answered your question.
Yeah, thank you.
Hi Blair, thanks for the talk.
It's cool to see interest in spatial audio.
There's a lot of folks here.
When I was looking into spatial audio maybe five or six years ago, it seemed that the variation in HRTFs from person to person was a big hurdle at the time.
I guess hoping to see that with a lot of the VR, you know, steps forward that have been happening that there might be some advancements in that, but it seems like that's still kind of an obstacle.
Are you aware of any recent research or steps forward to combat that?
A lot of spatial audio developers, like taking the THX Spatial Audio, for example, they're focusing on, they provide an out-of-the-box, in their words, high-quality HRTF, but there's also, people are trying to implement the personalized, rather.
HRTF of each consumer.
So I think with the THX spatial audio, the goal is to implement like you would take pictures of your head from different angles and then send that off to THX and they would generate an HRTF for you.
That is really the only big advancement with that that I've noticed.
I think a lot of people are trying to make a one size fits all.
And if I recall correctly, there are different companies that are trying to work on customizable solutions like with preset type.
HRTFs, but ultimately that is still a hurdle.
Customizable HRTF, if someone makes that plugin, like a plugin with a swappable HRTFs based on your age, based on your head shape, and you can pick the one that's best suited for you, that.
that is something I would be interested in seeing.
But ultimately, I think the closest we're getting is the promise of personalized HRTFs from THX at this point.
Cool.
