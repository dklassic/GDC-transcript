Hi, everyone.
Thanks for attending our session.
It's pretty late in the day, but thanks for making it out to hear our talk.
This is Tune Rendering in Hi-Fi Rush.
And before we start, GDC told us to remind you to make sure your cell phones don't make noise and fill out the survey after our talk.
Okay, we'd like to start off the talk with brief speaker introductions.
Hi, I'm Kosuke.
I've been working as a graphics programmer at Tango Game Arts since 2011.
For Hi-Fi Rush, I worked as lead graphics programmer working on our core tune rendering.
I went from bloody VFX for zombie headshots and moody survival horror lighting to tune rendering.
Hi, I'm Takashi Komada.
I am a graphics and physics programmer.
I've been working as a programmer at Tang Gameworks since 2016.
On Hi-Fi Rush, I was a graphics, physics, and animation programmer.
Here's the agenda for today's talk.
I'll cover the core Toon rendering topics in the blue frame, and Komada-san will present our ToonFace shadow implementation in the green frame.
Okay, I'm going to start off with a brief introduction of our game.
So before I start, how many people here have played our game?
Can you raise your hand?
Oh wow, that's a lot, thank you.
Okay.
Yeah!
Okay, great.
Okay, but some people didn't raise their hands, so not all of you have played our game, so I'd like to introduce our presentation's pop-up characters first.
The character on the left, Kale, is one of the enemy characters in the game.
Since he's an evil guy, he'll mostly be making difficult demands for the graphics team.
The character...
The character in the middle, Chai, is the main protagonist of the game.
Since he's our friendly hero, he'll mostly be making enthusiastic comments.
And finally, the robot on the right is John.
Despite not having a catchy food name, John made a strong creative push for our tomb visuals, had a conveniently available texture, and is Hi-Fi Rush's creative director, so I've included his comments.
Okay, it's the main topic of our talk today, but Hi-Fi Rush is a game utilizing toon rendering.
A lot of games use toon rendering for characters, but not the environment.
In our game, everything is rendered in a toon style.
A big challenge for the graphics team was how to utilize Unreal Engine 4's modern graphics pipeline and make it work for a 2D toon look.
Another defining characteristic of our game is that we're a rhythm action game that requires a rock-solid 60 FPS in order to minimize input latency and provide the best gameplay feel.
Programmers and artists were told from the very start of development, FPS is top priority.
But at the same time, we were going to support the new Gen 9 hardware, and the team wanted to pursue great graphics that would do justice to the hardware.
With goals of great graphics, the keywords that our art director defined for the team were sharp, clean, and colorful.
I'll go over the technical makeup of our attempt at great graphics in more detail, but notice that two of our keywords were sharp and clean.
Image quality was very important to us, and in addition to high frame rate, one of the key technical features of our game is its high resolution.
We aim for native resolutions on our console targets for sharp, clean, artifact-free image quality.
We're aware of super-resolution tech, and we support super-resolution plugins for PC, and they're great.
Because our resolution is high to begin with, they allowed us to push the limits of our PC low-end specs even further.
Because we prioritized performance and image quality, a nagging fear during development was that people would be disappointed by our technology.
In the end, it was a great win for us that the game's visual arts was well-received and we got great technical reviews.
Balancing performance, resolution, and rendering features is hard.
As graphics programmers, we're excited by new rendering tech and want great graphics.
But our game emphasized performance and image quality, and the rendering features we chose for the game were carefully considered so we could hit our goals.
If a graphics programmer were to introduce our game, that would be it.
I'll now discuss our toon rendering, starting off with an explanation of the core tech, our deferred toon renderer.
HiFiRush uses Unreal Engine 4, which out of the box is a photorealistic deferred renderer and not a toon renderer.
The lighting is forced, but this is what HiFiRush looks like when toon rendering features are disabled from the game.
The main point of the image is that if we were to use the default UE4 lighting, there would be gradation everywhere, and the image ends up looking too much like it's rendered using a 3D engine.
This is the same scene rendered with Toon rendering features re-enabled.
This is what we were going for as a Toon look.
Shadow gradation is replaced with sharper shadows and our comic shader has been applied to the various 3D render passes, adding a 2D touch to the image.
This is what sharp, clean, and colorful looks like for us.
Compared to past games with strictly cel-shaded environments, we incorporate a lot of 3D lighting rendering features into the game.
We added rendering passes like static shadow maps and decal-tuned lights But we also stylized and extended the excellent base UE4 graphics features, such as SSAO and SSR, and made them work for cell shading.
It's possible to write a toon renderer without engine modification in UE4.
In the simplest case of using UE4 post-process materials, the toon post-process is applied after the scene lighting is finished, and most of the rendering is complete.
It's the simplest approach and we wanted to do better.
Since we planned on supporting many lighting features, we wanted to be able to apply tune stylization to each lighting layer with their own stylization parameters.
Another thing we wanted to improve on is that UE4 post-process materials are applied per camera.
We felt that this was too restrictive and wanted to apply different tune colors and stylizations to different areas within the same camera.
To achieve our goal, we customized Unreal Engine 4 and combined deferred lighting and tune post-process into a single deferred tune rendering pass.
Here's a quick review of regular deferred rendering.
Material information are rendered into G-buffer render targets and the lighting pass is decoupled from the geometry pass.
Point lights are rendered with sphere geometry and spot lights are rendered with cone geometry in a pass independent from the geometry they are lighting.
In a similar fashion, we want to decouple toon rendering into a pass separate from the mesh's actual geometry rendering.
Our toon post-process volumes work like deferred lights except the shader is a post-process.
Like deferred lights, our artists can freely move around the tune post-process volume to locally change the tune rendering.
In the slide video, when I move the billboard mesh inside the smaller tune post-process volume, notice that its tune rendering changes.
At its core, our tune post-process volumes are using standard deferred rendering calculations.
We use the scene depth to recreate the world position and use G-buffer information to apply the volumes to post-processing on not the volume box geometry, but the underlying scene geometry.
Our environment artists place many to post-process box volumes throughout our levels.
By placing these volumes, our artists adjust to shading locally per volume within each area of the level.
The above is a PIX4Windows capture of the timeline for our deferred tune rendering pipeline.
I'm showing you the render passes that are executed in an actual scene in preparation for the tune post-process pass, the render pass of the vox volumes in the previous slides.
quite a few render targets end up getting generated for the toon post process pass.
My earlier test map was very simple looking, but there are quite a bit more lighting layers in an actual scene.
Toon shaded environments tend to look simplified, but there are a lot of layers to our environmental lighting.
Because our deferred toon rendering is a combination of toon lighting and toon post-process, notice that the scene color is completely dark except for emissives before the toon post-process rendering is applied.
A regular deferred renderer has scene depth and G-buffers.
For deferred toon rendering, we have scene depth, G-buffers, plus the various individual lighting pass results, all waiting to be applied.
the TunePose process has available to it all these render targets.
What do we do with them?
Because we have all lighting results as input textures, as was one of our initial goals, we can apply comic stylization to each lighting layer with individual parameters.
Because we apply all shadow types to the scene inside the TunePose process pass, we can easily control the visual look when different shadow types overlap.
For example, when the player shadow is drawn inside our cascaded shadow maps, we can detect this overlap and adjust the player shadow darkness so it doesn't appear too dark or light.
Stylized shadows and side shadows is a very cool tuned thing and we wanted to make it look good.
UE4 has a great looking deferred rendering implementation and it was a natural progression for us to extend it to a deferred toon renderer.
We tried to utilize UE4's strengths, but at the same time, didn't want to lose our own visual personality by using the engine too directly.
Locally toon shading different areas by placing toon post process volumes like deferred lights seemed like good usability for our artists.
Deferring all lighting and toon post processes into a single pass made sense because we wanted our artists to be able to control how different lighting layers combine and interact for Toon.
Okay, that was an overview of our deferred Toon rendering.
Next, I'd like to highlight unique aspects of our Toon lighting during deferred Toon rendering.
I'll cover shadow color 3D textures and ambient cube maps.
The major requirement for our Toon lighting was artists need to be able to add color and adjust brightness, but it can't look too 3D.
To make the scene not look artificial for Toon, artists need to be able to paint the colors themselves.
Environment artists need a way to give color to each location.
Our art director's keywords were sharp, clean, and colorful, So it was important that artists be able to control the environment's shadows with the colors they wanted In the slide image, the shadow color is a uniform black This is what the scene looks like with shadow color volumes applied The black shadows are now properly colored with artist-authored shadow colors I'll explain in more detail in the following slides, but shadow color volumes work by mapping a 3D texture to a 3D world position.
Here in the slide video, I'm adding a new local shadow color volume to the scene with a whiter, grayer shadow color 3D texture.
The rendering of the shadow color volumes is like cartoon post-process volume rendering.
The shadows are not applied to the volume box geometry, but are projected onto the geometry enclosed by the box.
If we look carefully, the shadow color volume colors gradually become darker with increasing height.
This positional color change happens because the enclosed surface world position is converted into the shadow color volumes normalized local 3D coordinates, and this coordinate is used to sample the 3D texture.
The gradually changing colors inside the 3D textures are hand-adjusted by artists to give the desired shadow color nuance our art director wants for the scene Here's an interior scene and its corresponding shadow color 3D texture For interior scenes, our environment is initially unlit and in shadow and we light by adding lighting layers to this state Because the scene is initially in shadows, artists can use shadow color volumes to adjust the overall base color makeup of the scene.
In addition to 3D textures for shadow colors, we also use cubemaps for post-process ambient brightness adjustments.
Like with our shadow color volumes, we apply ambient cubemaps to the underlying geometry using a box volume.
As mentioned earlier, our interior scenes are initially unlit, and to this state, we add lighting layers, one of which is ambient cubemaps.
Cubemaps need a sampling direction.
Because we are a deferred tune renderer, we can use a world normal stored in the G-buffer for this purpose.
In the slide image, the ambient cubemap volume is being toggled on and off Notice that in the scene, the underside of the surface is lit brighter than the top and the sides, representing a difference in directional ambient brightness.
This directional brightness comes from the cubemap texture, which our environment artists create by hand.
Okay, in the next section, I'd like to talk about our comic shaders.
It's a recognizable trait of our toon look, but we implemented a comic shader for rendering halftone dots and hatching lines.
Halftone dots are the round dots that we apply to the lit portions of our lighting.
Hatching lines are the rotated lines we apply to the dark shadowed areas of lighting.
The goal of our toon renderer is to render a 3D world as 2D toon.
Our comic shader is a stylization, but at the same time, it's an important element in our deferred tune rendering for making the world look less 3D and more 2D tune.
Here's what a scene from our game looks like with the comic shader turned off.
SSAO GI Bloom make the world look richer, but since we're just applying 3D engine functionality, the final image looks 3D.
Also, the performance-oriented low sample count nature of some of our rendering passes become especially noticeable on our clean-tuned textures.
Here's what the scene looks like with the comic shader enabled.
The scene is stylized, but at the same time, gradation is removed, making everything look less 3D.
The dirty look of GI and SSAO have been made sharper with lines and dots and the image looks much cleaner.
Here's the comic shader on-off as an animation.
2D sine distance functions are often used in procedural shaders.
For our comic shaders, programmers use SDFs to create the comic shader halftones and hatching lines procedurally inside our shaders.
We just do basic stuff in terms of SDF, but for clarity, I'll go over an example halftone shader.
My example is a post-process, which uses screen space UVs.
The first step is to calculate the distance from the center of the screen.
If this distance is below the halftone radius, we draw a white color.
If the distance is above the halftone radius, we draw a black color.
We take into consideration the aspect ratio to go from an ellipse to a circle.
We don't want large circular dots, but want multiple smaller dots in a grid across the screen.
Using the FRAC instruction, we scale and wrap the screen space UVs.
Art wanted halftones rotated 45 degrees.
For this, we just rotate the grid UV.
Because we are using a simple zero-one step function for the distance function threshold, aliasing can occur.
By using smooth step instead of step, and alpha blending the boundary regions of the halftone dots, we can implement anti-aliasing.
The area inside the yellow frame is the width of the smooth step function where AA is applied.
Since we want to apply anti-aliasing only on the threshold border pixel, we use DDX, DDY instructions to calculate the per pixel rate of change of the distance function and use this value as the basis for our width.
UE4's temporal AA blurs the halftones, so the AA is only obviously noticeable on halftones applied after TAA, such as our bloom halftones.
The difference also becomes not very noticeable with camera movement, so the benefits of AA is somewhat limited for us.
But we kept the calculations for the moments that the user might notice the difference.
So that was an explanation of half tones are rendered using screen space UVs.
Hatching lines are the same shader code, just with UVY fixed to zero.
For effects such as bloom and volumetric fog, we use screen space UV coordinates in actual scenes.
We don't do anything complicated.
An example shader is close to how we render actual in-game screen space halftones.
Screen space UVs work well for stuff without depth, such as bloom and volumetric fog, but on actual polygonal surfaces, we want our halftones and lines to look as though they are printed on the surface and not attached to the front of the camera.
For these cases, we use world space UVs.
To calculate the world space UV, we use the G-buffer world space normal to calculate the dominant axis of the surface and just project onto this axis's plane.
For example, if the Z value is the largest direction of the world normal, we want to project onto the XY plane, and so we simply use a scaled world position XY as our UV coordinates.
For intersections of planes, we don't use triplanar mapping.
We experiment with it, but blending artifacts were noticeable, and we simply project onto a single plane.
I didn't get complaints about the halftones and lines cutting off at the plane intersections, and the final implementation was kept simple.
One final important feature of our comic shader Because we procedurally generate our halftones and lines, we can easily adjust their look based on the scene lighting conditions.
We use scene luminance to adjust halftone and hatching line sizes.
For the image in the slide, the GI halftones become larger the closer to the emissive light source, and smaller the further away.
It's a simple technique that allows us to express the lighting's distance attenuation while avoiding gradation.
which makes our scene look more computer generated.
Okay, the next section I'd like to move on to are toon lights.
Hi-Fi Rush's toon shading uses a simple two-tone lit shade toon shading model.
The toon shading's lit shade calculation is not performed in a deferred pass, but is calculated by a global key light that is forward rendered in UE4's base pass.
We chose to forward render our key lights because we could do so without UE4 engine modifications.
In later stages of development, we began utilizing UE4 render commands for data passing and render thread delegates for render pass callbacks that would allow us to implement original render passes game side.
We're now more experienced about how to approach UE4 engine modifications, but that realization came after our core tech was implemented.
We eventually needed to support two key lights, and it may have been more performant to associate the key light with a tuned post-process volume and render it in the later deferred pass.
Our lighting's mix of deferred and forward is probably something we can improve on.
Besides key lights, artists can use familiar light actors such as point lights and spotlights to perform local environmental lighting.
For placeable lights, again, we supported forward lights for the same reason as key lights.
A lot of smaller teams probably struggle with this, but how much UE4 engine modification is safe is something we struggled with, especially in the earlier phases of development.
We initially experimented with forward lights, but decal lights, which run in an original deferred pass, were easier to optimize and eventually more useful for our artists.
My toon light explanation will focus on decal lights since they're more interesting.
Usually with placeable light actors in a 3D engine, light distance attenuation is expressed with color gradation.
For Hi-Fi Rush, this was a big no because using gradation led to the scene looking 3D.
In addition to halftones and hatching lines, We use cutouts to 2D stylize our tuned light distance attenuation.
The colors become darker the further away from the light source in cutout steps.
The scene in the slide looks unlit.
Add a decal light with cutout light attenuation and the scene is lit by a 3D light without making the scene look 3D.
Decal lights support arbitrary cutout textures Our art director prepared seven cut-out texture patterns to give variation to the decal lights used throughout our maps.
Decal lights are Hi-Fi Rush's deferred lights, and so when a decal light is placed in the world, they are deferred rendered with a sphere or cone geometry, just like regular deferred lights.
Decal lights' namesake, and what makes them special, is that they can be assigned to a decal light decal volume to render as a decal.
Environment artists can place lights as light sources in a traditional way, but they can limit the projection volume of the light separately using decal volumes.
One obvious benefit of projecting to a decal volume is a smaller area to render and better performance.
In addition to performance, decal lights provide finer artistic control over which parts of the environment the light affects For example, we can add a separate decal light with its own projection texture specifically to light the wall The scene displays a star cutout projection for the ground, but a tear cutout projection for the wall In The Evil Within 2, our PBR lights could use clip volumes to prevent spotlights from light leaking to the next room.
Decal light decal volumes also function as clip volumes and can prevent light leaking.
Some of you might be wondering, doesn't decal volumes mean characters are properly lit?
Let's try lighting our game's hero, Chai, with a decal light.
In HiFiRush, character lighting is a separate system from environmental lighting.
Forward lights and decal lights don't affect characters.
Next, I'd like to explain what we do for shadows in HiFiRush.
Having proper shadows for the environment and characters is extremely important for quality, even in tune rendering.
Shadows can be very performance intensive if applied without a strategy.
In UE4, shadows look good, but can be costly.
To balance quality and performance, we were very careful in how we chose to render our shadows.
We limit shadow casting lights for performance.
Toon lights are unable to cast shadows.
For outdoor environmental shadows, we use UE4's standard cascaded shadow maps.
We have our own pre-baked static shadow map system and use it for parts of directional light shadows for performance, but for the most part, we needed the extra quality that dynamic cascaded shadow maps provide us with.
For dynamic shadows other than cascaded shadow maps, artists are required to use shadow-only lights.
Shadow-only lights are customized UE4 PBR lights specializing in only casting dynamic shadows.
Because we were going for a 2D look, having different lights for lighting and shadowing was not a problem.
An exclusive shadow-only light was assigned to the player character.
In outdoor areas, the player's shadow is not drawn by the cascaded shadow mod, but by this separate, player-exclusive shadow-only light.
Giant bosses are allowed their own shadow-only light because their shadows can look very dramatic.
The character shadow-only light in the slide doesn't cast shadows from the player or the environment and only casts shadows from the giant boss.
Player shadows and some of our static shadow maps are drawn with stylization for the shadow-to-light transition.
The hatching lines become thinner the further away the shadow becomes from the shadow casting object, representing a 2D toon-style shadow-to-light transition.
Non-player characters use the more cost-effective capsule shadows for their shadowing.
Capsule shadows are standard UE4 functionality.
Artists prepare capsule shapes to approximate the character mesh and these shapes are used to calculate per pixel visibility towards a global light direction to approximate soft shadows.
It wouldn't work for a photorealistic game, but we can get away with the blobby look of capsule shadows because we are tuned.
Because capsule shadows don't require shadow casting local lights, we get cost-effective character shadows outdoors, indoors, regardless of lighting conditions.
Capsule shadows become too large and disperse for our small characters that are floating in the air far away from the ground.
For these characters, we implemented a simple AO volume decal system.
CPU raycasts are used to find the proper decal placement world position.
Finally, we also allow a single shadow-only spotlight for the environment team, and that's all they get.
They worked hard.
For the scene in the slide, a shadow-only light casts exclusively from the moving giant robot arms.
The only thing shadow-only lights can do is cast shadows, but just like for the character boss shadows, A well-placed shadow spotlight can be very dramatic in a tuned look.
I've mentioned we have a static shadow map system.
I'd like to explain this system in more depth in this next section.
Static shadow maps are offline baked shadow maps.
They are important for us both in terms of performance and artist's ease of use.
UE4 has a static shadow map system which is integrated into light mass baking but we created our own for better integration with our deferred tune rendering pipeline.
Shadow mapping is composed of two steps.
The first step is light space depth map generation from the perspective of the shadow casting light.
The second step is comparing the pixel's light space depth against the depth map to generate the actual shadow map.
What we pre-bake in static shadow maps is the first depth map capture step.
In the slide image, I'm toggling on and off the static shadow map display.
The environment team had strict limitations on dynamic shadow map usage, and interior environmental shadows will typically disappear completely without static shadow maps.
By placing static shadow map actors in the environment, environment artists can use static shadow maps.
The static shadow map actor is composed of two components, a scene capture camera for capturing the depth map and a deferred box volume for rendering the shadow map.
When the static shadow map actor is selected, the captured depth map is displayed in the lower right corner of the screen in the editor.
Artists use the depth map preview in the lower right portion of the screen to manually maximize depth map coverage of shadow casters to improve shadow map quality.
Standard comparisons between the pixel's light space depth position and the depth map's light space depth position are performed to render a screen space shadow map inside the static shadow map decal volume.
Without AA, our screen space shadow maps look very jaggy.
We use 4x4 PCF for anti-aliasing our shadow maps.
Here's a GIF animation of 4x4 PCF being toggled on and off.
As you can see, the algorithm does a pretty good job of alleviating jaggies for us.
Here's a screenshot of static shadow map camera placements in an actual scene.
Many static shadow map cameras with small coverage are placed throughout our levels.
Each static shadow map camera renders its own depth map and its associated render volume owns that static shadow map's depth map texture data.
Even the small handrail in the slide image has its own static shadow map.
Our artists use many static shadow map cameras, not just for artistic reasons, but for texture streaming efficiency as well.
One of the first concerns we had when we considered using static shadow maps was memory usage and streaming hedges.
We manage static shadow map texture streaming workloads by monitoring texture size and focusing on being able to efficiently distribute streaming across multiple frames.
For this reason, we can't have individual static shadow map textures being too large.
Our static shadow maps are placed in their own streaming level and within the streaming level We distribute texture streaming across multiple frames by placing a per frame streaming memory limit.
The final topic I'd like to talk about is how we handle global illumination.
Global illumination is being toggled on and off in the slide image.
We felt supporting standard 3D lighting features in our tune rendering was important for quality, and this included GI.
We decided early on that we wanted to try to use UE4 Lightmass since we were satisfied with its quality and baked lighting is great in terms of performance.
At the same time, we also thought a workflow using Lightmass lightmaps was too costly for our artists due to needing to deal with lightmap UVs and possibly long GI bake times.
Toon assets require artists' hand crafting for quality and all artists, including our environment team, were super busy throughout the project.
We had noticed that lightmass volumetric lightmaps could be used for the environment.
Lightmass volumetric lightmap light probes bake faster and don't require lightmap UVs.
We decided we would customize the volumetric lightmap functionality with a focus on further improving bake iteration speed and usability.
A big factor that allowed this customization is the fact that our game has a tuned art direction, and we can be artistically selective in which parts of the map we apply global illumination.
In this slide image, only a small area around the window has global illumination, and that can be okay for our look.
Here, the same level with the GI Light Probe debug display is enabled.
The environment artist has decided to bake probes around strong emissives, but in areas without a strong light source, no probes are baked.
UE4 has a functionality called Light Mass Importance Volumes that limit light probe baking to the area within the volume.
We customized this functionality to implement our own global illumination lighting volume actor In Vanilla UE4, light probe data is stored per level.
We customized UE4 so that in our game, light probe data can be stored per actor.
I should mention that we did optimize the UE4 volumetric light map data to save memory.
Because we only need directionless ambient global illumination, we cut down the light probe spherical harmonics 3D textures from eight to one and it's a huge memory saving.
Since I emphasize bake iteration speed and usability as key points, I'll give a quick overview of what our GI bake workflow looks like.
We start by enclosing the area we want to add global illumination to with a GI lighting volume.
After the volume is placed, artists select from the menu, build selected volumes.
A customized light mass GI baking code is executed, and the GI bake is limited to the area enclosed by the lighting volume.
Bake times are dependent on light mass settings and PC specs, but as a general idea, for a standard dev PC at our company, a bake should finish in a minute or two.
Since the bake is isolated to the selected actor, artists can iterate baking on specific areas quickly.
The GI lighting volume contains parameter for both baking and rendering.
An often used parameter for baking is the detail cell size parameter, which controls the distribution granularity of the generated light probes.
Our environment artists use this parameter to balance bake quality and memory usage.
Finally, the GI lighting volume not only defines the GI bake extent, but also serves as a rendering volume.
Here, I'm toggling on-off the GI box rendering for a specific volume.
Note how all the other volumes in the scene are unaffected by the toggling.
Deferred rendering of box volume is a local lighting technique that is used throughout our game and makes a final presentation appearance in this slide.
Here are my main references.
Now I'm going to hand it off to Komada-san for a section our tech, our ToonFace shadow tech.
I'm going to explain the face shadow of Hi-Fi Rush in this section.
The self-shadows of the character in Hi-Fi Rush are in cel-shaded style.
The shadowed and non-shadowed areas are clearly separated, and the shadowed areas are given a specified shadow color.
With shadows like this, The quality is all about the choice of the shadow color and the shape of the shadow area.
HiFiRush's art concept was sharp, clean, and colorful, so the shadows also needed to always have clean shapes.
Whether pixels in the inner shadow area is determined by whether the dot product of the pixels, normals, and the right vector is less than the threshold.
This is a typical method for cell shading.
The vertex normal is used for the normal.
However, this method is not used on the face.
The face requires higher quality than other parts.
However, there are problems in achieving the desired quality with vertex normal shadows.
The first problem is that using vertex normals makes it difficult to create a smooth curve.
As shown in the image on the right, depending on the direction of the light, the shadow shapes can become quite messy.
It is because there is a limit to how much geometry can be divided, and the shadow shape is affected by how it is divided.
The second problem is that the shape of the shadow created by vertex normals is easily broken by facial motion.
When the bone orientation changes due to facial motion, the orientation of the vertex normals also changes, which unintentionally breaks the shadow shape.
In this scene, where the facial expression is extreme, the shadow shape becomes considerably broken depending on the direction of the light.
The third problem is that it is difficult for artists to make adjustments.
Artists can adjust geometry and vertex normals, but it is very difficult for them to avoid problems one and two completely through manual adjustments.
It is difficult to intuitively understand the resulting shape of the vertex normal and geometry adjustments in all the various lighting directions.
Even if you use a normal map instead of vertex normals, problem one may be solved, but problems two and three will not be solved.
To solve these vertex normal problems in HiFiRush, we used a height map-like texture.
Internally, we call this texture a threshold map.
I will explain how to authorize in later slides.
But we'll first explain the calculations involved.
First, calculate the angle between the horizontal component of the right direction and the forward direction of the head.
The horizontal component here refers to the horizontal component in the cardinality system of the head bones.
The threshold value is the angle divided by pi and normalized to a range of zero to one.
then it is determined whether the area is shadow area or not based on whether the sample value of the threshold map is greater than the threshold value.
Since only the horizontal component is considered, the shadow can only be moved in the horizontal direction.
However, this is enough for HiFiRush.
In this video, the light is shining on the right side of the face, but when it is on the left side, the texture is reversed horizontally.
How to use this threshold map is a reproduction of the method introduced in the session by Mihoyo at Unity Seoul 2018.
I will now explain how to author a threshold map.
First, use a desiccator to bake the shadows of the face into textures by moving a directional light at fixed angles 180 degrees around the character, as shown in the image.
We moved the light every five degrees, so we baked 36 textures.
At this point, Shadows are shaded using the vertex normals so the shape may not be smooth or shadows may appear in unnecessary areas.
These textures are then detached to the artist's satisfaction.
This process creates a clean shape so artists don't have to create perfect vertex normals.
As you can see, retouching textures is much more intuitive to adjust than vertex normals.
Artists can preview the final look of the shadows in our DCC tool.
Finally, using a dedicated in-house tool, all textures are interpreted like a distance field interpolation and merged into a single texture.
The threshold map is now complete.
I now review how threshold map solved our original vertex normal problems.
Regarding problem one, the shape does not become a smooth curve with a vertex normal, but with a threshold map, it is always smooth as shown in the video.
This is thanks to the artist adjusting the shadow shape through retouching.
Also, since the textures are connected smoothly using distance field like interpolation, the shape changes smoothly when the direction of the light changes.
As an aside, threshold maps have similar characteristics to distance-filled textures and are less prone to artifacts even when the resolution is lowered.
However, in order to maintain a sharp shape even when the camera is zoomed in, we used 2K textures.
Problem two, where the shape is broken due to facial motion does not occur with a threshold map.
Because in case of threshold map, the shape is mapped with UV, so it's not affected by bones.
As an example, let's change the direction of the light and compare.
When shading with a threshold map, shadow shapes are clean in all right directions.
Problem three is that shading using vertex normals is difficult for artists to adjust.
Again, in the case of a threshold map, artists can intuitively modify the shape by retouching the baked textures.
When shading using vertex normals, the vertex normals were directly used for shading.
When shading using a threshold map, in between light baking and texture merging, there is a step where artists can make intuitively quality adjustments.
This concludes the explanation of face shadows in our game.
Thank you.
Okay, that's it for the content of our presentation, but I'd like to end our talk with some final concluding words.
So our game wanted to do a stylized cel-shaded look for both characters and the environment, and this presented unique rendering challenges.
Unique problems call for unique solutions, and we developed an original deferred post-process volume approach for our attempt at tuned rendering.
We're using UE4, and we adapted many 3D graphics features for toon rendering.
We use well-established graphics techniques and build on the great works of others.
We talked about tech today, but it's toon rendering.
And as you can imagine, there was a lot of crafting and optimizations by our artists not covered today that ultimately determined the quality of the toon graphics.
We had goals of rock-solid 60 FPS, high resolution, and great looking tune.
And the tech we talked about today helped the HiFiRush team achieve these goals.
And we had more tech we wanted to share info about, but probably be too much of us talking, so we didn't do that.
But we do have bonus slides at the end of the presentation, so when the slides become available for download, everyone can read them.
And our final page is our special thanks page.
We get to present this stuff, but our tune rendering was made possible by the hard work of everyone on the HiFiRush team.
Our final slide is a special thanks page to give a shout out to all the people who made today's talk possible.
OK, that's it.
So we have time for a A little bit of time for Q&A, so if people have questions, please go to the mics.
Hi, I have a question.
First of all, fantastic talk.
Second, when you were talking about all the more physical 3D features, I couldn't help but notice that you didn't include other PBR inputs like roughness and metalness.
Was that a conscious decision to exclude those because they didn't add anything?
Or what was the thought process?
And am I right that your final only texture inputs to the deferred PBR workflow or deferred lighting workflow was just color?
Yes, we just use color.
In fact, we do use roughness because we have SSR.
So we use the standard UE4 SSR at its lowest setting.
That's about one millisecond.
But still, UE4 SSR is great at the lowest setting, and it looks good.
And we apply a tune stylization to that SSR and use it.
So we do use roughness, but metallic we don't need.
Yeah, we don't do standard PBR.
I think it's mostly base color and roughness for us.
Got it.
Thank you.
And our bonus slides include a section on our G-buffer stencils.
We actually use the UE4 G-buffer slots that we don't use for stencil bits that we use in our tune rendering.
Got it.
Thank you.
Hi, how well did your artists receive this more unique lighting pipeline?
I'm sorry, I couldn't catch.
How well did your artists receive this kind of unique lighting pipeline?
Well, we made the pipeline with artists, so I worked with the environment, lead environment artists to create the pipeline.
And like our ambient, our shadow color volumes and our ambient cubemaps, I worked Part of that was suggested by them, and this was necessary for the look we were going for, so they were all in.
Does that answer your question?
Hi, I had a question about the threshold maps.
So if I'm understanding correctly, are you creating a sprite for merging all of them together?
And then are you doing any special processing for the base color textures to look sharp?
Can you come back with that question later?
OK.
Sorry.
Sorry.
Hey, I noticed that you guys use a lot of shader code in your presentation here.
I wanted to ask if there was an advantage over using shader code over node-based shader.
We use shader code because our materials were all implemented by programmers.
So it's actually more confusing for us to use shader code.
And one of the benefits of using shader code, we save all our shader codes out into a text file and manage it through Perforce.
And we can search what other programmers shader code easily.
So it's easier for us to write, and it's easier for us to check up on the code that we wrote.
Cool.
Thank you.
There's no more questions.
Thanks for attending our talk.
