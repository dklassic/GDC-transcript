My name is Chris Preet.
I'm the founder and CEO of Two Hat Security.
Today we're gonna talk about trolls and the cost of doing nothing.
And I wanna just preface this kind of conversation before we get too far, because oftentimes when I talk about that, we think what we're talking about is profanity and silly little funny quirks that people say.
And we've seen a lot of that, because we process about four billion messages a day for the games industry.
And we've seen a tremendous amount of hilarious stuff.
But what we are is we're an AI company that finds the most toxic, disgusting things on the internet, like rape and genocide and hate speech and sexual abuse of children and child pornography and the worst things that there is.
And we also work with the games industry in order to deal with the trolling problem to build reputations and find our own users.
And what we'd like to share today is a bunch of research we've been doing out of the research arm of our company where we dive deep into the problem of trolling.
and try to understand how can we make it better and how can we be involved in it.
So it's a bit of a serious conversation today, but hopefully we'll have a little bit of fun in light of that.
So, this is me, long time ago.
How many people had a computer like that, long time ago?
Yes, I figured it. We've been in this industry for a long time, you guys.
20 years for me going at this, having like a great time.
I'm going to show you someone else. This guy here is my brother.
My older brother. We used to make games together back when we were quite a bit younger.
And my brother ended up making a really cool game.
So he started off in 1999. He's like, okay, I'm going to build a game and it's going to be for kids.
Now, nobody builds games for kids. Not in 1999.
So in 1999, the chat rooms were very, very dark and not appropriate for children.
And so there's two things about building a game for kids.
One, you're gonna get sued.
Number two, the kids don't have any money.
So that was the mentality at the time.
So he went out and created this game.
And he built it and he was able to achieve 300 million users.
And if we look back on this game, it ran from, so the early experiments were in 99, and this month they're finally shutting the game down, and they're moving it to a mobile experience, which is really cool.
But how did he go about and do that?
What are some of the lessons learned?
Well, one of them was that community was incredibly important, because he found out that if you put energy into making it incredibly safe, so if we're gonna make this for kids, it has to be exceptionally safe.
And when we did that, it became a place where kids hanged out, built friends, built community.
And their parents were like, hey, my kids are happy.
They're having fun.
I'll pay money.
And it turns out that parents have credit cards.
And if it's safe, the lawyers don't come after you as much.
So that was really, really cool.
And what I found out is that when later on Disney came by and they offered my brother $350 million for his game, I was in charge of doing a bunch of the due diligence.
And they asked me this question.
They said, what is your churn rate?
I said, what the heck is churn?
So I wasn't measuring churn because we didn't really have a concept of churn because people love the game and they would just stay and stay and stay and stay and I had to learn what it means is how fast are you users quitting.
Because when you have a strongly engaged community you can end up with a product that lasts for well over a decade and you can still have people very sad after they've been playing for 13 years and more.
But it's not just kids-based products that we're talking about.
It's becoming very, very popular today that this issue needs to be dealt with of trolls, toxicity, and all that kind of garbage.
We can see and watch what's happening with Twitter in the media, and they've been slaughtered.
And they're trying really hard.
They're doing a bunch of things.
But they've just been slaughtered in the media about how much of a problem it is.
And so you have famous people that are being onslaught with constant bombardment of toxic and trolling-based comments.
And that's highly problematic because if you want to create a platform where there's free speech, if so much hate speech is allowed, that what ends up happening is nobody wants to share.
You end up with this situation where everyone just shares pictures of puppy dogs and silly little emojis because if anyone engaged in a real conversation, if anyone engaged in their actual liberty of being able to share their true mind and their opinion, they'll be destroyed.
Because 95 or more, maybe even 99% of users are good, but the 1% and the 5% are so toxic that they're making these platforms unaccessible to have true expression and truly reach their potential of what they could be.
And so you have situations that gets even worse where you take your best users, your whales, the ones that believe in your platform, the ones that are building it up, the ones that are inviting other users, and they are the ones that are leaving en masse.
And so you can see on places like Twitter where the most famous people are abandoning Twitter and they're doing this campaign of like, I'm quitting Twitter.
And it's happening so frequent that it's becoming common.
In this case here, it's Lily Allen was being attacked on Twitter because she lost her baby many years ago.
And she's developed mental illness because of it.
And because of that, people are telling her, well, it's your fault.
You're the one who killed your baby.
And they'd make little memes about a frog being stillborn.
And it was like so devastating to her that she's like, I've had enough.
And so the cost of doing nothing, the cost of allowing such toxicity to be pervasive has a direct impact on the financial bottom line of a company.
And so this is Twitter's stocks.
And you can watch kind of how they go down.
Now there's many factors involved in that, but one of them that's being drawn out in the news and the media is that they have not been able to take this matter, not actually been able to deal with it.
Because they've had a platform where this free speech is everything, and this is your own private room, and do whatever you want, except for we have to understand that the private room has a glass wall standing on Main Street, because we gave them a microphone, because they're supposed to tweet to the world.
And there's a disconnect between those two concepts.
And now they're actively working and trying to do some amazing steps to transform the situation and launch some new technology, but we hope it's not too little, too late.
In comparison, you can compare them up to Facebook, which is in green.
Many different factors involved in there.
There isn't just one, but one of the factors that's different about them is how they've gone about taking care of moderation.
If Facebook hires thousands of people to go and review content and user reports and take action against it and try to remove hate speech within 48 hours, they've been actively engaged in it and it makes a financial difference to the whole thing.
And today we want to measure, we want to take a look at that cost of doing nothing.
I want to follow up from a talk.
There's a great company, a great presentation a few years ago.
Riot Games did a look at what they were doing.
And they said, oh my goodness, if we analyze our chat logs, honestly, we know that users are about 21% of the conversation is toxic.
And they said they had to do something because what we've observed is that when conversations hit around the 20% mark, it's close to death.
And so Riot Games got really smart and they started thinking about it and saying, well, what can we do about it?
And Jeffrey Lin came on the stage of GDC and gave an amazing breakthrough research of what they needed to uncover and discover.
And one of the things that they did in their study is they said a user who experiences toxicity is 320% more likely to quit.
That motivated them to make a change.
And they did things like they set up a tribunal system so that the community could take care of some of those issues.
They started telling people, this is the reason why you were banned, or why you had action taken against you.
And they noticed that there was a higher reform rate.
At one point, they brought the toxicity all the way down to 2%.
If anyone knows Riot Games, I'd love to connect and figure out some of the techniques and stuff and how it's going now for that, and then be able to present that more as we go on.
But sitting in the audience at the time was my buddy, Alex, and he was from World Series Poker.
And he was listening to it and he was going, he's thinking back about the different kind of feedback he's had from his users, the comments he's had, and all those things, he started thinking, okay, when you're playing poker and you put your money in the kitty, if another user can be toxic to you and troll you and make your experience terrible, and you're like, I've had enough, and you just throw the cards down and leave and fold, there's a disadvantage.
And that person, by trolling you, can make a financial benefit.
And that becomes problematic because in all of our games, we work on the game mechanic.
We spend very careful time meticulously thinking about every single detail of what items are gonna be added to the games because that balance is critically important for fun and also for retention and growth and monetization.
And if we allow one item to be there, like a giant sword that you can slash everyone, and every time you use the sword, you always win, that game sucks.
Well, we need to put the same effort into community mechanics as we do into game mechanics because community is absolutely essential.
to the growth of our game and we need to grow up as this whole thing and have more and more of this research which we're seeing coming out and saying how can we design our communities just as effectively as we design our games.
And so we go into the question about what is toxicity.
Toxicity is a poison, so if you imagine that you can put a poison into an organism like ourselves, our human body, and if it begins to spread, the organism will begin to die.
And we see that when we launch new games, they start off with about 1% toxicity, roughly speaking, and then they go up and they start getting around 5%, and then it starts tipping the scale.
And then they'll quickly grow up to around 20%, and it's a toxin, it's a poison that's spreading through the organism, and an organism that's toxic begins to die.
which in the game industry means loss, death of users, users leave, it's no more fun, the game eventually shuts down.
Now the troll, we have to define troll.
There's a great discussion on YouTube on the GDC channel about this talk.
And a lot of people were challenging and said, we need to make sure you define troll because a lot of people are just throwing this term out there everywhere.
It's basically a lot of the accusation was that we use troll to say people that don't agree with me.
This person is trolling me on there.
And there was a very engaged, lively discussion.
where one guy was, you know, tell me what you think, you worthless piece of whatever, you need to explain your position and he went on and on and on about this whole thing.
The conclusion was that a troll has this intent of evil.
They have an intent to destroy and it's an ongoing repetitive thing.
And in the case of this one user, people accused him of being a troll and by his own confession he just said, no, I'm just an asshole.
So we have to be able to tell the difference between some of the two because that is a slight difference, but a troll is the goal is just to be destroy the community.
Where other people are just downright obnoxious now probably need to get rid of both.
But we should make a little bit of distinction so that we treat them slightly different.
So as we move through we want to understand the cost of doing nothing and trolling toxicity those kinds of things.
So we said, okay, trolling, toxicity, where can we start?
Oh, let's go start with Reddit.
So we went and looked at the first year of Reddit before they developed a reputation, and we did a big study of how does each individual subreddit, how does health, how is it affected by toxicity?
So we measured health as things like growth, you know, organism that's growing, so more users, more conversation, more whatever, just more of stuff.
And toxicity we defined as basically let's take anything bad and throw it in there.
So people raping each other, racism, vulgarity, just whatever, it's bad stuff.
And we measured the two and tried to find a correlation.
Well, the expected result of course is that you'd see that as the bottom line of toxicity goes up, the red line, which is health, goes down.
Well, that kind of makes sense.
That's what we're anticipating.
What we like to do in a study is find stuff that we didn't anticipate.
And what we didn't anticipate was the second column, which is the game industry.
that the toxicity was starting around 4% and it grew all the way up to around 6 or 7% during the course of the year.
And guess what happened to the health?
The health went up.
So when toxicity went up in the game sector on the subreddit, then so did the health.
And so there's not an absolute direct correlation between toxicity and health.
You can have a toxic community and you can still grow.
Which is a drastically surprising finding.
However, there's some caveats to that.
Now our understanding for that is, notice that it's on the lower level of toxicity.
So a community has enough resilience to put up with a little bit.
A community can handle a little bit of toxicity.
In addition to that, a community can begin to have an expectation that surrounds it.
It begins to draw people of like-mindedness towards the same thing.
and other people self-elect to go away.
So the health could have potentially been faster, we're not sure.
But one thing we did observe is that when there's a drastic change in toxicity, there's a drastic change in health.
And it works on the positive as well, because World of Warcraft, on their individual Reddit, you could see that when they made a difference to the toxicity level, so when that toxicity level starts dropping on the bottom, you can see that eventually the health started going up.
So when the community began to trust, the forum again, they began to come back and they began to engage again and began to be a place where they could share.
And on a completely different nature of topics, of amateur, it's, don't look it up, it goes up, it goes down, it goes up, so there's a direct correlation between the two and whenever there's a drastic change, there's a drastic change to health.
and toxicity is common other people are doing research and finding that things like half of people playing playing these kinds of games discover that they at one point are being attacked by this toxicity in addition to that.
A third of us basically at some point in our life get pissed off and become that toxic trollish type person under certain conditions.
So that becomes quite interesting.
Now I wanted to really dive into and say, okay, what does this actually look like?
And in the game industry, we love the idea of an A-B test.
So let's separate an A-B test.
So I'm gonna create two different kinds of games and illustrate for you their journey of how they're gonna go ahead and reach $21 million.
and what difference it makes based upon the decisions they make using the data that we have now.
So these are fake games.
They're based on real stories and real data behind it, but they are fake for the purpose of this demonstration.
I want you to consider them basically the same.
Now the first one is a futuristic space game called AI Warzone.
The second one is a medieval fantasy game called Trials of Serathion with an emphasis on the word wrath in Serathion.
Now, they're both MMORPGs, they're both over 13, and they're predominantly male, with a good mix of female players.
For the sake of this study, I want you to kind of think about them to be essentially the same.
So, at the beginning of our study, they both have a million users.
It cost them $2.78 to spend advertising money to buy a user to come and play the game.
And that user, when they join the game, spends about $13.51 in miscellaneous little tiny things in order to give them power ups and whatever else inside the game.
So in-app purchases, which is great.
We love to have that kind of markup on our things.
So oftentimes we're fighting for pennies in here.
They started off with a 20% churn rate month over month.
And I'm going to show you in a moment how they dropped that to 5% churn rate.
And then there's executives walking into the room as they always do like me and they say OK this year you guys are going to reach this objective no matter what you're going to get $21 million of revenue.
And the team goes looking at so OK to hit that number we have to add to our base user base 10% every single month.
So they tend to see they went to this talk and they revealed another study that we did this year so we did a study of a first person shooter.
I can't share the name at this time, but it's a first-person shooter.
It's been around for a long time, mostly out of Turkey.
And this game here, when we did the analysis of it, like most people do that understand churn, we looked at how many users were on day one that continue to day two, day two to three and five and blah, blah, blah, blah, blah.
and you measure that that's a really important to know if you games mean successful or not of course this is pretty basic but then we segmented the users into different groups to try to find different behavior models that indicated that makes a drastic difference in behavior in the one that we identify was that users who engaged in chat.
are three times more likely to come back on day two.
So any user that types in one line of chat or more is more likely to come back on the second date.
We didn't figure out the exact why.
It might be because they brought friends with them, they may have already known people, they may have found a place of belonging.
Whatever the reason was, more study to come, they're three times more likely to come back in day two in this story.
And then it extends that after 2 weeks, they're also 3 times more likely to come back.
And so what both of our games decided, OK, let's make chat a primary focus.
Let's make sure that people somehow engage in chat and they find a place of belonging and they're able to extend their thing.
So here's a mathematical form.
It basically means the users next month, so the total number of users you're going to have next month is the total number of users you have this month minus all the users who quit plus all the users you added.
Now that you can imagine it like you're climbing a mountain, but the mountains made of sand.
And every time you take a step, some of the sand falls down.
And so it's really hard to reach the top of the mountain because the mountains keeps eroding.
So if you're going to reach your 21 million dollar goal, you got to go grab a bag of sand and you have to pay money to dump to buy those users and put them back and replace all the ones that lost.
So you have to constantly be replacing all your users.
So they each took a different approach to how they're gonna climb this mountain and get to the top of $21 million of revenue.
The AI Warzone said, we're gonna have a moderation strategy.
We're gonna take care of our users and we're gonna spend money on that.
And Trials of Serathian just simply said, you know what, whatever.
I've heard so many wonderful things.
I've heard, those are our best users, but it's fun.
That's what we want the game.
I've heard so many different kind of stories, but that's the path that they took.
It'll take care of itself.
So when you run down the numbers and you watch and so the key numbers that we can look at is They began to diverge and so AI Warzone had to spend more money to pay for their moderation and the outsource, like those people that they were using to do all that.
However, they were able to achieve more money in the end.
So at the end of the first quarter, Serathian had to pay more money because they had to go and buy that extra users, the sand, to keep dumping on so they can replace all the users that they were losing.
So for those numbers then you can roll it out and you can go down to the end of the year and the CEOs are sitting down with their meeting and they're saying, okay, so how did we do?
How did the game perform?
And at the end of the year, because they had to spend so much money replacing their users, they ended up spending $7 million extra.
And going through one part I missed on the math.
There's some little numbers at the top.
So the math that I was using there is Seraphian, using that Reddit study, said that, look, our users can tolerate some toxicity.
So they had about a 6% toxicity level.
My eyes can focus close enough, 6.6, yeah.
And they said, okay, our users can put up with that, and anything above that we're gonna take care of.
And then Seraphian said, I don't care.
And so what they ended up, they had the Riot Games study that a user experiences toxicity is three times more likely to quit.
So you can see 10% versus 30% roughly, 320% or three times anyhow.
So going forward.
So what did we learn?
So Serathian ended up doing a study and saying, okay, we gotta change this.
We're just losing money left, right, and center.
We gotta make a change.
So the first thing they did is said, okay, Riot Games did something cool.
Let's rip off that idea, and we're gonna do it, but not as well.
And so they decided that every single toxic chat message needs to be, or every human report, needs to be reviewed by their best users.
So imagine this scenario, you got your best users who love your game, who tell everyone that they should play this game, who are deeply committed, playing a long time, and giving you all your money, and you send every single message that is toxic and the worst of the worst to those users.
Now if 300, if user experiences toxicity is 320% more likely to quit, guess what's gonna happen if you take your best users and give them 100% of the toxic crap?
You're gonna wear out and burn out and discourage the best users that are paying you all the money and bringing in all the next user base.
That was like a really bad idea.
That was just not cool.
And the second thing they did is they said, okay, these users, they keep getting offended, they keep annoying me with all these complaints.
I'm just gonna add a button that you can mute another user.
So whenever that user says anything, it's still said, but you just don't hear it.
So let's look through the logic of that.
So if imagine that 95% of users are good, because only some of us sometimes make talks of things, but not all the time.
If most of the users are good, and a few of the users are causing the problem, why is it that the majority of our users have to go and push a button every single time that they're offended?
It's not like it's their fault.
It's not like these toxic users are like making us tons of money.
They're actually destroying our revenue.
So why did we, it's kind of backwards.
We should have put the algorithm on the other side.
In addition to that, they just keep creating more accounts and they have to keep muting them.
And they're still talking about them.
They just can't hear it.
So everyone in the room is laughing and they don't know why.
So how do you know if you're a Serathian?
So go into your forums.
going to read it going to your app store if you're on the if you're on there and look at the comments and say if people are saying I love the game but I hate the community you know you're pushing towards that level.
So what it what it a I was on do.
So I was on.
they realize that not every community is the same.
Communities are created differently, so you have to figure out what kind of community you're creating.
So in real life, we go and create communities like a playground for kids.
And if we're making a playground for kids, it has certain cultural expectations.
So if I'm hanging out there with my four kids, and some 13-year-old punk comes along and starts swearing to their friend, it's perfectly acceptable to turn to that 13-year-old and say, hey, this is a mixed audience, do you mind?
And most logical people, oops, sorry, I forgot, and then they move on.
Now, we can do that with kids-based products.
We can say, look, you can't swear here.
That's not appropriate.
Now, if we're gonna go and create a different kind of community, let's say we're gonna create a nightclub, it's for 18 plus, there's certain kind of social expectations that can happen in a nightclub that are different from the neighborhood park.
So each location, each community, each party that we create has a different goal, and we create a community for a reason, and we have to figure out what that reason is and design that community around that.
And it's perfectly acceptable to stick a bouncer at a nightclub.
Because we want most of the people to be having fun and a good time and if one drunk idiot comes in and starts disturbing everything, bounce them out.
That's perfectly acceptable.
And if we're going to build Burning Man and we're going to have to take everyone out into the desert and we're going to have a really good time, even there, there are certain things that are culturally unacceptable.
Like you can't go and murder people and rape people and do those other kinds of things.
Like that's just not, that's just not cool.
So there's different cultural expectations that can be enforced for each area.
So they realize that each zone has a different kind of threshold.
So if you can think of a vertical axis, you can draw a line somewhere in the sand and say, look, we will put up with this and no more.
And that needs to be age appropriate and game appropriate.
So if you're targeting middle-aged women in housecoats, there's certain different expectations than if you're targeting hardcore gamers.
So that's only one vertical axis.
There's also a horizontal axis, because we all know that even if you allow swearing, there's a certain point where someone just swears and swears and swears and swears.
It's like, get, like, what?
You're so annoying.
Like, contribute to the conversation, or like, at least contribute to the game, for crying out loud.
So there's a point where we have enough is enough.
And so they're actually vertical lines of two dimensions.
One is the severity of the comment, and the other one is how frequently it's said.
And for each age group or each demographic, you can draw a line and say, look, if it continues, I'm gonna draw a line in the sand and say enough is enough.
And by doing that, you can break your users down into different kinds of groups.
And so you know if you launch a game, the first thing, and you put chat there, the first thing a bunch of users are gonna do is they're gonna type in fuck.
Because they want to know, does this system have a filter?
What's this game for?
What is the nature of this community?
And most people do that because they're going to test the boundary.
That's perfectly normal.
And if they find out that they can't do that, they go, oh, this game is expected behavior of blah.
And then most of them back off, and some of them are like, ooh, this is a mini game.
Can I get around it?
And so they keep pressing on, and they persevere.
So you have different kinds of users and different kind of ways, but we can then take those users and we can deal with them differently.
Thank you.
So the way we deal with them differently is you take that line in the sand and anything that's above that line is blatantly obvious.
Those are the things that you just don't say in that kind of context and automation can do that.
So there's no need to crowdsource that comment and send your best users to look at the complete filth that you already know the answer for.
So anything above your line, you just filter, block, whatever you want to do.
Hell band, shadow band, whatever.
The next thing that you want to do is the stuff that's just a little bit below the line, the stuff that's difficult for computers to find out, you want to have your humans help you with that.
So you add a report mechanism, people report it.
Well, if a human hates it and a computer's not sure about it, chances are it's not good.
So you can auto-action that one as well.
Now the only thing you're left with is the stuff that is subjective, the stuff that requires context, the stuff that's innuendo, the stuff that's difficult, and that's where you crowdsource your humans.
And that's where a tribunal-like system becomes powerful because your humans feel valuable because they're dealing with a real problem and it's not the obvious and they're making a real contribution to the community.
And then your trolls, the ones in the upper right hand corner, the ones that are defined as they keep ongoing, ongoing, ongoing, ongoing, won't stop, desire to destroy your community, get rid of them.
Just expense it away because they're just going to cost you money.
So I want to talk about how this is put out in real world situations.
So recently, the great folks at Twitch, they implemented a system like this and they allowed for each channel.
to set their own resilience level and say look for this the purpose of this community is going to draw the line here.
And it's been incredibly well received it's amazing to watch the reception of people like yes finally this is so wonderful I wish everyone had this functionality.
So kudos to twitch an amazing job on that product.
So the 3 things that we can learn is that if.
We take care of toxicity.
We can make money.
It was a $7 million difference in that one scenario that I was outlining.
In addition, it'll help your brand.
So if you wanna go and raise more money, if you wanna go and get more publicity, if you wanna have people share about your site and invite their friends into it so you don't have to buy those users, really, really good.
We talked about, it began with the story of Club Penguin and how you can keep your users for a long, long time if they feel that place of belonging.
But what's also really cool is when you launch your next game, you've got a massive user base that loves your brand, and they're going to come with you to the next one.
So thank you for your time.
I really, really appreciate it so from here we're going to take questions we're going to go out the hall down the side there's a little like place with a bunch things we can stand with take a whole bunch of questions there got some free shirts we can hand out.
I got my team that can also help answer questions if you can't get a hold of me.
In addition to that please fill out the survey and they told me a bunch of other stuff I'm supposed to do.
Yeah, blah blah blah.
Upload it to the Internet watch this share with your friends on the GDC vault.
Okay, thank you for everyone for your help.
