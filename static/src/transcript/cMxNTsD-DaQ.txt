Hello, and welcome to my presentation, A QA Perspective on Live Game Production.
My name is Michael Borg-Larsen, and I am the lead tester at Retomoto in Copenhagen. And we are making and maintaining the game, Heroes and Generals.
And before we begin, I've been asked to just remind you to turn off any silent or silence your noise-making devices.
And also, I would appreciate it if you rated this talk in the survey that you will get after this.
I will try and leave some time in the end for questions.
And if we run out of time, I've been told that there's an area we can move to.
And I will be happy to take questions.
So let's begin.
First, my background.
My background is I've had, in seven years, I've been working in development, so I've developed systems myself, content management systems, and I've also created some logistics handling systems.
So I've been through the whole loop from concept to design and development and on to test and maintaining it.
And that has come in handy now.
in my function as tester and as lead tester.
And before I started at Resumoto, I was the editor of Tom's Hardware in Denmark, where we started up.
So I was helping with creating the brand and the format.
And also, I had the task of creating or writing news articles from the hardware and games perspective.
And also I translated a lot of the technical articles that came from the mother site.
And then I've played a lot of games over the years, spanning down to the classic arcade games and home computers.
Every console there's been, and of course on PC.
So that gives a good base also to have an idea as a tester what to expect of bugs.
I work for Retomoto. It's a studio in Copenhagen.
It was created by the initial founders of IO Interactive, and the founders or creators of the Hitman series.
So when they sold their company to Eidos, they moved on and created Retomoto.
And they created the game Heroes and Generals, which we operate to this day.
And to give you an idea of the scope of our production, I can tell you a little bit about the game.
Heroes and Generals World War II is a first-person shooter, and free-to-play.
And as the name suggests, it's set in World War II.
Battles all over Europe are going on between the factions, Americans and Germans and Soviets.
And they all fight for dominance. There are no predetermined alliances.
one might expect, but it's open to the players and the clans to make alliances in our game.
So we have combined arms combat where players will be able to play as infantry or tankers or pilots, recons or paratroopers.
And all this going around in the same battle, and yes, all three factions can be in the same battle.
And the action client, as you see, it's not based on a licensed engine.
So it's a proprietary engine that we created ourselves, which makes it we have a lot of different tasks that we otherwise might not have when we change anything in the render engine.
Or if we change anything to the network code, we have to test that from scratch.
And on top of that, we have a strategic layer.
And as you see here, this is just a replay of one of the wars we've had.
It went on for over a month, and each blip you see on the map is one battle that starts up.
So we have our generals move their forces around on the map to create these battles, and that will fire up a battle for up to 18 or up to 36 players, 18 on each faction, or 12 on each faction for a three-faction battle.
And if not enough players are online, then it will simulate the results from the resources available.
And it came out of closed beta in 2013.
So we went into open beta at that stage.
And in 2016, we released it fully.
And since then, we've had regular updates.
And that has added new weapons, new vehicles, new maps, and new exciting features.
And it's available for free on PC at heroesandjournals.com or on partner sites like Steam.
And now on to the interesting part.
So, we've gone through different stages and in the six years I've been there, it started out in closed beta.
And that was a comfortable stage to be in, because our players kind of expected there to be some bugs, and we had some leeway.
It was still not cool to let a bug slip out, so we still had to be good, and we still had to use our resources efficiently.
And we figured out that we could...
We had some dedicated players, where we took the most...
mature of those and invited them into a closed beta group.
And we used them for feedback and gave them the opportunity to see upcoming features.
And uh...
we found that as a very valuable asset at that time. Of course you have to have those dedicated players and uh...
if they're not mature then uh... you might have to have some battles with them but uh... we've been lucky in in having some good players to assist us.
And in Open Beta, we kind of removed or we didn't, we no longer had that control over how many players would come into our game and put load on our servers.
So we had to adapt and find out how to best test for load issues and for scaling.
And for Steam Early Access, we were one of the first games that were greenlit for Steam Early Access, which was a program that Valve made to give the players, or the users of Steam, the opportunity to decide themselves which games they would see on the platform.
And that put pressure on us, in the sense that we could see that there was a lot of interest, and we had to...
Of course, always look at our processes to see how could we prevent the issues that could lay down the game to come out on our live stream.
But until release, we had a three year period until there.
Or we went from open beta to release.
And that time we spent to hone in on the best practices, and to make a really tight process, and make sure that we caught the most of the broken bugs that would otherwise go out.
And beyond that, since 2016, we've had a lot of updates.
where of course we have frequent updates that has to be tested overlappingly, so we have always new upcoming stuff that has to be tested in a way where it doesn't have to break or break the, or prevent the development process. And that's something I will go into more detail with.
Of course, the main goals might be obvious, but it's good to remind ourselves we are in QA, and quality and stability is what it's all about.
And with a live game, that is not always as easy to maintain.
So over the years, we've added many weapons and many vehicles and skin variations, new features, new maps.
And with each new data that we add, that adds a lot of new failure processes.
And we have to scale up and see how can we best prevent all of these issues that could, I mean the regression testing of it just grew and grew.
So we looked into how could we better perform in our QA and make a better or hone in on a better process.
And one really obvious answer to us was to be more agile.
And of course, there's a lot of literature on that.
But we tried some ways to implement from what we read and what we heard from others to implement.
But we also tried out what we thought would fit best into our way of working.
And a good way to illustrate that is by this comic.
This is one type, one way we tried to implement it, and one that didn't work very well.
And, for example, in this case where we have a new house master in the game, we would know that there were some issues with it, because it was not finished yet.
It was testing it while he was making it, our graphics artist.
So we tried to figure out what could we test, what could we benefit from testing early on it.
And we would look at it.
We would know exactly what this should do.
This should be placed on a map somewhere.
And it should just be working as somewhere on the map that added to the level.
So we could see from the get-go that there was a hole in the roof.
And we would expect there to be a bug or two somewhere.
And what would happen when we approach it?
We turned the knob.
And everything just broke down and went to flames.
And we had a developer that was really not happy about it.
We had a tester that was really not happy about that.
He spent some time to test this, and it wasn't ready.
So we looked at that and figured out how can we better implement that process.
And communication is a good thing.
Communication is something you can always improve on.
And in this case, we should never have.
turn the knob because the knob was a part that wasn't done yet.
So we should have done it more synthetically, so we should have tested the parts of it that was actually done, like the collie, like the stitching of the textures, and of course we should have made a more specific test case from it.
And that's what we learned from it.
And now on to how we work with streams.
If you're familiar with Perforce or something similar, you probably know the term stream or branch.
And the way we use it is that whenever we start working on a new feature, we take the code base that closely resembles live, copy it down to that stream, and make sure that it's as up-to-date as possible before we start to work on that feature.
And that means that if we start working on a new game mode, we can make sure that no other feature in progress will have any effect on whether it works or not.
So if we find a bug on that stream, we would know that the one responsible is probably one guy or those guys that are working on that.
So the blame game is kind of eliminated from whenever we find a bug, who's actually responsible in fixing it.
This also means that we can have parallel tracks of testing, where we can prioritize into what we want into next update, and focus on what are the areas that we want to make sure are working in the process where we are in the early process, in the sense where some of the features might be very prone to errors, and we would know that we would have to have focus on those.
in early on, so we could check all the test breaking stuff and make sure that those get fixed before we move on.
So in this case, we have six different features that are ready for test.
We might only have time for three or four to start with.
And by doing that, we will have one tester be assigned to one of these features and apply a first QA pass.
And a first QA pass for us is that he takes an approach that fits best.
and works just by experiencing it either from the black box, white box, or gray box approach.
And from there, he will go through finding all the bugs.
Bug fixes come back.
He checks them.
If they're all fixed, he greenlits the stream.
And when it's greenlit on the stream, it's ready to be merged up to the main development stream.
And from there.
If it's not ready, we will, of course, not merge it up.
And we will still have features that are quite not ready and some that we have not have time to test fully.
And that will be a risk management process where we would see, do we really need this in the next update?
Do we have to move our test focus on that before we move up?
And this is also a process where we would use some test automation to make sure that we don't spend too much time on the stuff that are very time consuming, and it will be able to assist us in finding the bugs that can occur.
So once we are ready to move it up, we have all the green-lit features from our first pass.
And these don't have to be perfect.
These don't have to be bug-free.
These just have to be non-test-breaking, in a sense where we know what to predict from it.
So in the feature phase, where we are now, that would be two to three weeks before release.
We would know that we're pretty sure that we can make this release.
So predictability is what we are gaining with this process.
But of course it's not always that easy.
in the perfect situation. Sometimes we have a main content driver that we need to get out, like a new map for example in this case, where based on player feedback from playtests, we can see that there are some areas that still need work. And in that case, our level designer will still be working on it when we are going to the phase where we need to move it up to our test stream.
So that just means that we would have to focus on other areas, get our test resources focused on the stuff that is ready, and assist the level designer, if we can, in the process, and move in to test the stuff that he does. Get ready early. And of course, something like the sound update, that might be something that is not a critical asset or a critical addition.
So that means that...
There's low risk in merging that up because we would be able to test this fully before release.
Well then also, some other things can come in to the side.
And this can be based from metrics and based on decisions where we want to move a little faster to see the results of.
It could be an iterative process where we want to iterate on a process where we just need to find the issues with it, the bugs with it, get it out there, and then measure the metrics from it on the live environment.
So it's often not optimal, it's often not perfect when working on a live game.
But that's where we have to be good at scaling it and scoping what we test and report to our project manager what the risks are if we move on with this.
And then optimally, we have all the features greenlit.
Or we will get to that stage before release.
And in that case, we have the stream greenlit.
And we will move it up to a staging stream, which will be a stream we reserve for the critical fixes, the hot fixes where we need to have a clean stream that is closely resembling live, and to make sure that nothing else impacts anything if we are fixing something isolated.
But right before release, we move it up there and we do a short or longer smoke test, depending on where, what the risks are with the build, just getting around all the corners.
And once that's greenlit, we release it to live.
And in this process...
I've seen during this GDC there's been a lot of focus on automated testing.
That's also something that we have had a focus on that we could see we could benefit from.
I think everyone should in QA because it's something that both can spare you a lot of time and it can actually add new cases that you will never be able to test.
So we have two different tools as we've explored different ways that we could implement this.
The first thing being a war data visualizer.
Because we have a game with a lot of items and a lot of attributes, it can be daunting to look into all of these data and figure out if one value somewhere is way off.
So what we have, we have a tool that, across streams, can compare all the different data, all the changes.
And then we have a manual process where we have to look at these results and see.
or the red flags where the problematic issues could arise.
And this means that we can focus our testing directly towards the areas that actually have reason to be tested, where it's not just, we think this might be affected, but this is actually affected.
Might be a bug, it might not be, but it helps.
And the other track we have is an automated compliance and performance tester, which runs a nightly build every night.
And it's scalable, so we have two worker PCs or more that we can set up with different hardware to make sure that it can actually run the nightly build.
We can set it to a higher degree, so we can set it to each commit we have.
But it takes a while for it to run through all our maps from a different set of camera files.
So we usually have it set to a nightly build.
And that will catch, so when we meet in the next day, we will catch if a build has failed.
It will catch if something has been committed that is very taxing on performance.
And we will be able to see where is the performance historically moving on our streams.
That's also something that is very time consuming if we had to do this manually.
And looking at the methods, of course, there are a lot of different methods.
And one of the points I want to make today is that what we found to be very important is diversifying with the resources we have.
So in the perfect environment, we would have all the resources in the world.
We will have all the testers to make all the different test cases that we want to complete.
But of course, that's not rarely the case.
So we have to diversify and look into what methods are good to use.
Sometimes it's obvious, depending on the test case.
But other times, there might be some that are outliers in what will we benefit from using this method.
And for the first test pass, it might make sense to do the obvious.
And for the second test pass, it might be obvious to try and diversify into another method and another approach.
In that case, we would get around other corners and get around to other areas of whatever we are testing.
And approaches are also something that we are diversifying with.
So we'll see.
Thank you.
Box testing, I look at black box testing as a luxury, something where we can, if we have the time, I would love to have a first pass just being a black box, where we would have a different outcome of it, instead of having the focus of having all the details from the get-go.
So our tester, if he knows nothing of a new vehicle we've made, he would experience it much more like the player would.
He would look much more into, how does this fit into what else is in the game.
And in the other end, if we don't have that much time, white box testing is the way to go.
White box will be able to dig deep into the data, dig into the code, and see what has actually changed.
And from that, make a decision on what should we test from this.
But often times, it's a grey box situation where we are looking into the data behind, or the code behind.
And then we are doing a task where we give that information, or some of that information, to our tester to test from.
But it's much dependent on what time you actually have for the task.
And another approach, or two other approaches, are what I call authentic and synthetic.
The authentic being the way the player experiences something.
So this will be if we have that new armored vehicle, that would be log into the game.
find it in the store, find out how you unlock it, then unlock it, purchase it, and equip it to your character.
And from that flow, go around all the corners figuring out what can I do with this new item, and finally matchmaking into a battle, driving around in it and experience it, like the player would.
The synthetic way of testing that, that would be just firing up a local build, firing off a command just to spawn that vehicle, fly around, fly through it, add a lot of debug layers and outputs, and testing it that way.
Both are important and necessary, but it's something where if we diversify by focusing the first pass on one or the other, we see that we can actually benefit from that.
Then we have the methodic or the chaotic way, as I call it.
The methodic being, might be more of a temper-based approach.
Where when we are looking at hiring people, we are trying to fill out different profiles.
So a profile could be a very analytical person, very, what's the word?
Someone who takes it slow and quiet and kind of looks at it, steps back.
when he experiences something, and then test it from that approach.
Another profile we have is the chaotic guy, the guy that just clicks on everything, and if it just responds within a nanosecond, he will click it ten more times.
And in that case, he will probably find a lot of crashes that way.
But that's also something where we can benefit from having one type of tester, testing something in the first pass, and have another tester with another approach in the second pass.
And then the last one is objective versus subjective.
This is more dependent on, I guess it also carries over to the black box testing, where if we take objective testing, well, that is the definite box.
That is, if you are testing a new vehicle and the collision is all wrong on it, you can walk right through it.
That's just a definite bug.
That's a defect.
That goes into the bug reporting system, and that's getting fixed.
The subjective part, that's more of.
does this look right? So this vehicle, when testing this from a black box perspective for example, does this fit in to that category of vehicles? So if it's an armoured vehicle like a medium armour tank and we look at this and see well it has an incredible front armour and it's pretty much impossible to penetrate, you cannot deal damage to this tank.
Maybe that's based on historical data, maybe that's intended by the developer, but maybe that's not intended.
And that's where the subjective part comes in.
From the tester's perspective, he will look at where it fits in, see, well, this doesn't seem right, and make a report from that.
And sometimes not just situations that you can test, sometimes that, well, you can test it, but it will not be in an authentic manner as our players would experience it.
And that's a good way for us, or we opened our eyes to this when we had more and more players.
join our game, where we started to have much more open tests, where we are inviting them in on our prototype server, testing a new feature, and then seeing what actually comes from that, getting the feedback from them, and opening it up for anyone who had the time or the interest.
And this is from a player on live that experienced something that, if we put this out, we would never, in a million years, be able to reproduce this, I think.
And that's something where we can see that the physics system and the network code that's actually working properly and we could reaffirm that it's working like we would like.
And another example here from a very good pilot.
what he sees is, uh, he doesn't see his friend, but, uh, his friend is dropping a bomb, and we can see the hitbox actually, uh, and the netcode being up to par. So the bomb passes right through the, the model, as it should. And now for the conclusion. I know I'm a bit quick, I can see. So I will expand more on these points.
And the first point is, get QA involved early on.
And that, of course, carries over to the agile part of it.
And it can be a battle.
It can be a constant battle to get QA involved early on.
And the way we've done it is, we can see we benefit from having a tester in the design meeting itself, where we can see, or we can raise a flag and say, well, this decision is something that would result in this and it would give a problem in this area where the designer might not have thought about that. And of course that's a very diplomatic process and we have to make sure that we are not killing off the creative process because then we wouldn't be invited back. But we've seen results of this working where we have prevented.
some issues to arise where we haven't thought of the bigger picture.
And so before we started production, before we started programming it, we've actually prevented something to come to the test phase.
And another point I want to make is test features on dedicated streams.
And if you are able to do that, it's something that we, of course, have benefited from in the sense where we can see that we had had issues where we tried to compile different features on the same stream.
And that just made a mess of things.
So what we did with separating it on each stream, we could see that it was not only much more.
Simpler to test but the bugs that came from those tests were much more In a merit that they would take these as granted as okay This is my bug and it's not something that someone else has caused and it also allow us to much more diversifying the the approaches we make And it will add that extra step into before we go to release We actually make sure that we have proper time for some of the features that might not be as important.
So for the lesser features that we still want out, those shouldn't hold back a release because we haven't had enough time to test it.
And of course, also by automating some of our processes on the lower streams, we'll be able to catch something much more easily than if everything was jumbled into the same stream.
So especially with our data visualizer, that's possible to isolate a few different changes instead of a huge set of changes that clutters around if it's a lot of different features.
And of course, diversifying is one of the things that we've benefited most from.
And that's much dependent on the resources you have.
And for a live game, sometimes you have to move fast.
And by moving fast, you have to take a different approach.
Make sure you cover all bases.
Make sure you cover all different areas where the risk is highest.
And of course, you can never prevent bugs from getting out, especially if you have to scope it down into where you have to allow for certain processes to finish early.
So if you only have a day to test something that takes two or three days.
Our approach would be to first look at it in a very deep manner, just look into the data and look into the processes and the code and then figure out what do we need to test from there.
And also with the hardware we have available, that's also something that we diversify with.
That's something where we try to, with the hardware we're using on a daily basis, we use some different graphics cards and CPU setups, also operating systems.
And for our automated performance and compliance testing, that's something where we have different setups, of course, that we'll test it on a daily basis.
And then involve your dedicated amateur players.
This is also something that might take some effort from our own part.
It's something that sometimes takes some maintenance, but it's something that we benefit a lot from and something that we can see the value of.
So that is also something that I would want to point out as, if you have that opportunity to involve them.
being on Slack or any other sort of communication, Discord or just from a forum, then that's something where you can benefit a lot.
And automated testing, that's been a big, big word on this GDC and that's something I would also say that maybe you can...
Maybe you don't know exactly how to approach it if you don't have anything yet.
But it's something where I think everyone should explore what is possible and what would make sense.
And of course there are different approaches that will never make sense.
We've made the decision that we would never automate our UI testing because that's just something that changes a lot in our game.
So we would have to use a lot of time just to maintain that test.
and the benefit from that would be minimal. So we're identifying the areas that actually make sense to test and then focus on maintaining that, because there will be some maintaining, but it's something where if you don't have any automated testing, you probably have that.
the task for your testers where you see that this is just a tedious task, you see them being maybe annoyed or it's tiresome for them. But if you can automate that process, that's something that everyone can benefit from. And that leads us to questions. If anyone has any questions, then please use the mic.
Alright? Oh, yeah?
I had a question. A very interesting concept with the separate streams.
Where do you do the integration testing?
You had an example with a new map and a new vehicle.
So when those two come together, where do you do the integration testing for that?
we do that when we move it to our main testing stream. So that means that when we've tested it isolated, we know that that part works. So when we move it up to a higher stream and they're collected there, we will do the integrated test. So usually if it's something that is, if it's layer related, we will probably develop it on the same stream and test it on the same stream before we move it up.
But we've seen larger updates, larger features where we have collected too much on the same stream.
And that has just ended up in a mess where you can't really, we couldn't find head or tails in it.
And it took us a long time before we actually got it to a state where it's usable and it's ready for release.
Thank you.
OK, thank you.
