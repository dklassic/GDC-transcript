So I'll start with just a short introduction.
My name is Kjartan Emilsson, CEO and co-founder at Solfar.
Like I said, it's a small studio based in Iceland.
We are three co-founders that all of us came, were working previously at CCP Games, so we've been like, our background is really in gaming.
But we decided to jump ship to do a pure VR studio startup in late 2014.
And what fascinated us with VR is the concept basically of putting people in impossible situations.
And that's kind of the theme of the type of products that we're looking at.
And, but if, as this talk is specifically about Everest VR, there might be a question why we chose Everest specifically.
And like I said, one of our goals as a company was to put people in difficult situations, and Everest is certainly one that most people will not have the opportunity to experience.
And we also thought, as gamers, that we didn't feel that VR at that point had...
enough knowledge about how people react and how people play VR games, such that we felt confident in doing a bet on a single game with maybe two, three year budget or something like that. So we wanted to try out several different things and experiential products is one of those.
After having been working now on Everest for the last half a year or so, it taught us an enormous amount of things and I think all of these things will trickle back into the games because you need to master a little bit how to create emotions in VR and how to control them and experiential products are good for that.
But why Everest? There's another reason for that, which is the name of the other company that is involved in Everest, called RVX. It's actually a special effects company based in Iceland as well, a good friend of ours. And they have been working on a lot of Hollywood movies, notably Gravity, and they have been working on Everest as well.
And through that, they had actually acquired sort of a know-how to create landscapes using advanced techniques.
So in this talk, I'm just going to go through basically all the steps that we went through to create this product.
And it's not launched yet, but it's not ready, but it will be soon.
But first I want to show you very quickly what this product looked like one year ago.
It was basically the pitch that we were making.
So our goal with Everest was to try to recreate the emotions that you have at various milestones that you have in climbing Everest.
For us that was the most important thing.
We didn't want to do a climbing simulator.
at this stage, but really just mastering how to create these emotions.
And there are all kinds of emotions involved in climbing a mountain.
It's not only vertical, it's also a sense of awe, a sense of beauty, a sense of foreboding, etc.
What we do is basically we create first-person experiences that are pretty well framed within a given milestone of climbing Everest, and give you simple things to do, but it's not very complex.
For us, this had to be a very accessible product.
And we go through some of the major milestones, like I said.
Base camp, Kumbu Icefall, the Lhasa phase, camp four, which is just above the death zone, the Hillary step, that's the one that we've been climbing at this show.
And then finally the summit.
And so yeah, this was our storyboard that we make.
We started designing this with the interesting design for Vive because we thought it was cool to try to experiment a little bit what you could do with a room scale tracking.
But then we also have an interaction design that works for Rift and also just a sitting experience.
Though obviously it might be less immersive.
So here's a few of the milestones that we have.
Base camp is one.
It's basically like you're witnessing the puja ceremony where the Sherpas do offerings to the mountain gods at the start of the season.
We wanted this to be a very simple scene, but serene, early morning.
And you sort of like take some food and give it to the gods, so through this altar that they raise.
And then there's a crow that comes, and depending on which way she flies, or it flies, it's a good omen for the expeditions.
And then we have more dramatic kind of a situation, the Kumbu Icefall is like a tumbling glacier that is at the start of the mountain.
And here, of course, there are all kinds of opportunity to recreate a vertigo and also experiment with the type of movement that you can do.
And so you're crossing crevices on ladders that actually Sherpa's put up there.
And then Hillary's Step, it's a...
It's one of the more dangerous hurdles on the Everest, because it's a slightly technical part of the climb.
But it's in a death zone, and this is where people are really tired, and they are running out of oxygen.
And there's only one file that can go through.
So there are often lots of incidents that happen at this stage.
This is just before the summit.
And you might have had the opportunity to try it here already.
I was really pleased yesterday, Dean Hall, the game designer at DayZ, he's actually gone to Everest.
He's a friend of ours, he's been to Iceland and all, but we put him into a hilarious step yesterday and he got goosebumps.
So if somebody has climbed Everest, get goosebumps, we're happy.
Yeah, no, there's a summit, obviously.
But I'll skip through this and go to the part of building a mountain.
Everest is a big mountain, and building it takes a little bit of effort.
And especially knowing the constraints of VR, as you probably all know, where you have to put yourself under the constraints of having 90 frames a second, overdraw 140%.
and the min spec of 970.
It's quite a challenge to get at least graphical quality to a level that is acceptable.
But we really wanted that to be the case in this case.
So we worked a lot on the optimization part of it.
And throughout, I'll tell you a little bit the tools that we're using.
We don't have many programmers.
We actually have only one programmer in our team.
So a lot of things we do are optimizations and shaders.
And then we use tools from third parties that we integrate into the Unreal Engine.
But let's start first from the source.
So RVX, as I told you, is the special effects company that has been working with the techniques of stereophotogrammetry.
So for those that are not familiar with that technique, it basically consists of if you want to acquire a 3D landscape, for example.
You basically take pictures of it from numerous directions, and as many directions as you can, and with knowing also the position of the cameras.
And then you take all this data of photos, basically two-dimensional photos, that have 3D information on how they were taken.
And you run that through a computer to crunch for a long time, and then recreate the 3D mesh that is underlying, and also the color maps.
So in this case, Davi, this is the creative director of DarbyX, he actually was at Deverest and took some photos.
A lot of photos actually.
And this is for example the Khumbu Icefall.
You can see like little dots there, it's a tent actually.
To give you a little bit of a sense of scale there.
And then there were like...
helicopter rides. So one thing with this for Everest, you can fly a helicopter up to a certain altitude, but after that it loses lift, so you cannot actually capture all the summit. So in this case, we actually had to also do sculpting, hand sculpting of some parts of the mountain. But otherwise, it was basically the whole mountain that was there.
Then we use the service of another Icelandic company called Designing Reality.
So they have a custom solution to do stereophotogrammetry that they build themselves and the software as well.
It's kind of like a mini supercomputer in a box.
28 cores, 128 gigs of RAM, six GeForce Titan X and SLI.
bunch of straight SSD and all this database of photos, high definition photos that you put in.
In the end, you get I think something like 105 gigapixel of point cloud data.
And the resulting mesh is 1.5 trillion polygons, which is quite a lot.
And I think it took about two or three months to calculate.
So this was like all part of pre-production for this.
And after that, you need to somehow get hold of that data because the resulting meshes, of course, you need to adjust it a little bit.
We split things up in 126 patches, and I explained to you why.
Internally, it's easier to work with smaller data sets, in Maya, for example.
And so we reduced down to 24 million polygons in the end.
But note also that in the experience, we need to recreate near field often.
And the near field actually takes more polygons than the whole range mountain.
Because you need to have a lot of three dimensional detail in VR as people can get very close to the surfaces.
You don't want to break the immersion by failing on that.
So coming to Unreal, we do all our work in Unreal, and here we show you a bit, this is like the whole range that we have.
We have this far field as well, which is in a much less resolution.
And then you see the little patch in the middle there, that's the actual average range.
I don't remember the size of it, I think it's about three or four kilometers on the side.
So there's like four big patches there that take care of the far field.
But then this range itself, we split up in this 126 patches.
And so they were both the reasons to do that was to be able to work with it in a healthy manner.
And also to be able to do visual culling of these meshes.
So like often when you position yourself somewhere within a scene you don't see much of the range so we can get away with losing all of it.
But we still wanted this to be able to run the whole scene in one scene.
So here you see a little bit more of the patches there.
I think each patch like this is typically about maybe 100,000 to 200,000 polygons.
And at this point, also, there's a lot of hand tweaking that goes into choosing the right edges and stuff like that, because edges are pretty important on mountains, as we see them a lot in profile.
So all these patches are not regular, actually.
It's adapted a little bit to the landscape.
And then, like I said, there is also this near field that we need to recreate.
This is like both snow and rocks.
And as I said, people go pretty close to objects, so there are probably a lot more polygons in this piece of snow than there is in the mountain behind.
So for further reducing the mesh and make it runnable in Unreal, we need to use some loading strategy.
And because this is like the mesh, as it's, this is not the one point trillion polygon mesh, but it's the one we reduced, I guess, to 25 million polygons.
And you can see it's quite dense, like this little detail there.
And it's not a very intelligent tessellation.
So for creating the LODs in Unreal, we use the tool SimpliGum that we integrated in Unreal.
And this gives us a sequence of LODs.
We have five LOD levels, I guess.
This is the highest LOD level.
And you can see successively finer and finer detail.
And so typically, when you're looking at the mountain, you will have five different levels of LODs playing, coming in, depending on your view.
And this has worked very well for us.
This is just for those that want numbers, this is kind of the LOD hierarchy for a typical patch that we have.
Then the next problem were the textures and the color maps, normal maps, occlusion.
And if we wanted to have like 8K resolution on the textures, and as I said before, we wanted the whole scene to be able to, or the whole range to be possible in view.
So that gave us a texture budget of 24 gigs, which was not the main spec.
So we had some problems resolving that until we found this solution, Granite for Unreal, which is quite remarkable.
So basically what it does, it does both memory compression, both on disk and in memory.
And then it does streaming as well.
And it integrates really nicely into Unreal.
So once you have created this, you basically create sort of like a a database of textures or a texture atlas, where you put all the textures that you have, and Granite then sort of optimizes the data structures.
They do their own compression algorithm that they claim are more suitable for a lot of typical scenes in 3D.
And I can attest to that, that we've seen some pretty impressive compressions in there without seeing any difference in quality.
I can't really show you much picture of it here.
This is a tool that we use to create the tile set.
So in the end, you sort of think of just you're using an infinitely big texture.
It doesn't matter how big it is.
And then within Unreal, you get texture proxy objects that you can use just as it was a normal texture in Unreal.
And everything is done behind the scenes.
So here I'm showing the texture atlas at different levels.
and I don't remember how many levels they have, like I think eight or something.
And yeah, the texture compression that we got for this, I don't have the exact measurements because it has been changing a lot since we're still working on the product.
But I remember distantly one case where we went from an 8 gig to 1 gig in our build in texture memory, and still using 8K texture everywhere and normal maps, et cetera.
So we're quite happy with that solution.
I also mention here TrueSky from Simul, even though we haven't had the time to integrate it yet, or not in the demos at least, but we think it's a great product, and we've done a lot of experiments with it.
And at some point we need to start looking out whether we can tweak it to still fit within our frame budget.
but the nice thing that it gives you is our volumetric clouds, especially seen from above, and a very efficient kind of time of day effect.
So by the way, that's something I forgot to tell you in the stereophotogrammetry process.
When you get the color maps, they do include the shadows from the time the pictures were taken.
So part of the work that has to be done is also to paint away the shadows so we can relit this thing.
So there are some very patient artists painting shadows of the whole mountain.
And then we do a lot of shader work for shading the scenes.
And the problem is that snow is not an easy thing to light.
It's essentially white.
And it has a lot of subsurface scattering.
We use a lot of Fresnel effects for those that are familiar with that.
I can show you here, this is like the final frame for a given scene.
This is a ledge, just the hiller step.
This is a column map.
This is a specular map.
And this is a subsurface scattering.
Combine it all to this.
And then you need to run this at 90 frames per second, and then you have to turn everything off.
So these are all the things we turn off in the Unreal Engine.
And we just have this early z-pass on.
And so, yeah, a lot of the things are just done through the shader.
It looks pretty well.
And then for more interesting effects, we also played around with GameWorks Turbulence.
And they had excellent people at NVIDIA working with us and integrating that.
It's part of the integration into the Unreal Engine.
And it's a very simple metaphor.
Basically, they have a grid that you can place in your level that has multiple levels.
multiple cubes and in that grid they basically solve the fluid equations, Navier-Stokes fluid equations, so they can have like a dynamic simulation of fluids within that grid.
And it can also include a certain amount of convex collision volumes in that simulation.
So like if you want to have like your flow break on a certain path, you can put certain volumes there.
And once you have that, you can basically spawn any GPU particles within that grid, and they will follow the velocity fields, the turbulent field there.
And then the nice thing, of course, is that the GPU particles, they also have screen space collision detection on any objects, which is independent of the collision in the.
in the fluid simulation.
So, and we found it works extremely well to recreate spin drifts and such weather effects that are very like, iconical for Everest, where you see them at the edge of the mountains, like little vortices, swim around.
And the nice thing about this is that these are all real time and interactive, so like, when you're in the scene, you can actually like, put your hands in it and you'll stir the pots, so to speak, and the things drift around you, it's quite cool.
Unfortunately, I couldn't get a video of this because maybe you can sort of imagine how it looks.
And another technology that we use from NVIDIA is multi-res.
This is like a VR-specific optimization.
The idea there is that because of the pin cushion transformation that is necessary for the big lenses that are used in VR, you actually have less resolution on the pixels on the edge.
And in addition to that, your visual acuity is also reduced on the edge anyway.
And people tend to be looking always straight ahead in VR.
Even though you can move your eyes, people usually don't do that.
So the idea there is that you could get away with rendering with less density on the edges.
And if you just fade it, then the boundary, well, with some ATLiasing, it isn't really noticeable.
So basically they create, they split the scenes or the screen into like nine different grids and with less resolution on the edges.
Then they blend it all together.
And this gives, when we were measuring this, this gives us like one or two milliseconds per frame, which is huge for us, it's like 30% or something.
So this.
gives us the ability to squeeze more things into the actual scene.
One thing to note though is that this affects post-processing effects and they have adapted to a lot of the post-processing effects, but maybe not all of them.
But we are not using any post-processing effects, so that was okay for us.
And I can't notice this.
Like, I have a button that turns it on and off.
And I never know whether it's on or off, except I see the frame rate increase a little bit.
So that's great.
So this was the parts of actually creating this mountain and sort of the technical part of it.
But then it comes to actually how it feels in VR.
And I think they The thing that was maybe a little bit surprising is that none of all of this great graphical fidelity helped in any way in...
or of course it helped, but it wasn't enough to create an immersion.
when you actually stood there in this great environment.
You had no sense of scale.
You didn't know whether this was like a small hill or a big mountain.
You didn't really feel where to go, et cetera.
So there are all kinds of things that we need to look at just to make this like an immersive experience.
And this is probably what is the most, the biggest learning experience for us.
So of course there are lots of level design constraints and how you can make your scenes.
And we've been looking both at solutions to work with the room scale tracking and also with standing experience.
And the different input mechanisms.
This is of course all familiar to most of you that have been working in the VR field.
And the things that we came down to are pretty standard I guess now.
It wasn't maybe standard like a...
half a year ago or one year ago. But basically try to optimize the space and the level so that you can reposition the user at certain moments. So for example, you're crossing a ledge, then you're sort of like crossed maybe two and a half meters, so you don't have enough tracking space left. So the trick is then to do a blink and reposition the player in the scene.
And then suddenly you have more tracking space in front of you because you've turned.
But the problem, you can't turn them arbitrarily because you could lose orientation of where you were.
And so this is a little tricky.
But by being smart about it, you can sort of create the sensation this space is much bigger than the room that you're actually able to walk in.
And then the other thing that was, of course, really good news for us, we didn't really know it beforehand, but is to find out that vertical movement does not induce motion sickness.
And because usually if you do horizontal movement in VR, where you move in horizontal path and you're not moving physically yourself, that's like a very immediate motion sickness.
But if you go up, there's none.
And that's good for a mountain product.
So we have a lot of mechanisms where you're actually climbing ropes and ladders and stuff like that.
And there is an infinite amount of vertical space in tracking in this case.
Another thing that we thought a lot about was how to do establishing shots.
Because we are partly telling stories in this product, like for each sort of scene, we want to introduce it a little bit and explain what is happening there.
So you want to be able to show some shots that sort of establish the scene where you are.
Standard stuff in movie making, but it's quite difficult to do in VR if you are doing it in full immersion.
And, but at the same time we wanted to get that feeling that you have when like you're looking down to the landscape from high above and it's very majestic feeling.
And usually that is not a good idea because again it leads to motion sickness when you move like that in the scene.
But what we found out by accident, in a way, was that if you put a frame to start with, and you accustom the player to just look like at a cutscene really in the frame.
It's not an actual cutscene, we actually put the player in a black box with a window and then we just move the box.
But it feels like it's a movie playing on the screen and you're just standing there.
and there you can have all kinds of shots with rotation and everything and that's okay but at a certain moment what we do is that we convert to linear velocity shots and with just cuts so there's no rotation and we dissolve the frame away And we found out that if you start with a frame like that and then you dissolve it, people still feel anchored somehow in the room.
And they do get like a sensation of like, you know, the floor just went out under you.
But they don't get sick, or at least very, very few people have gotten sick.
And we were quite happy to have that because once you're flying like that, it gives you like a great sense of majesty for the whole thing.
Because otherwise we wouldn't have to put you in a cockpit, for example, helicopter, which we tried and really didn't like.
And other people sometimes put a platform under your feet, and it's kind of immersion breaking.
So we're happy with that solution.
That brings me to some things that I like to call the reality cues, which are like this small little psychological...
cues or pokes that you can poke your brain with, and that sort of help in establishing what some people have called the reality contract, which is when your body sort of agrees that what it is experiencing is there and it is real.
And you need to hit a few check boxes like that, and then suddenly that happens.
And one of them, for example, is touching things.
It's extremely powerful to touch things and see the world react to it.
It sort of gives you a sense that it is there.
And we try to integrate as much, and it doesn't need to be much, but as much as possible, when you touch the scene, you can feel some kind of reaction.
So you have gloves in the, or mittens, in the experience.
And we let the gloves block collide on surfaces, and gives you a small tactile feedback on it.
And even though your physical hand goes through, The glove stays there and it feels okay because you don't want to go farther You just got like a response from the world and say okay. You're solid. I won't I won't push you and it's nice also to put things in your field where you need to avoid them because you naturally tend to avoid things and when you're like Physically doing things like that it feels real like you see like a big rock here And you try to avoid it even though you could perfectly put your head through it and get the clipping and everything, then most people just never do that.
Then there's a sense of scale, and that was really the most difficult for Everest, because Everest you need to have that sense of scale, but even in real life it's sometimes difficult to get a sense of scale of mountains.
So you have to use familiar objects, so in this case, in our case, it's mostly people, climbers.
You have to put them somewhere at different ranges so you can see how the landscape is changing.
The establishing shots also help because then you have memories of certain pieces that you're now looking at closer and you realize the scale of things.
And you can also make it with sound, obviously, and light.
and things that fall, for example.
If something falls, you can see it, and then you see the scale reducing, and you realize how steep a slope is, or how big it is.
And the vertigo was also interesting.
There are all kinds of ranges of vertigo, apparently.
And, for example, the vertigo that was the worst for us was when we were just doing early blockouts, just using cubes, not real models.
When you have sharp edges that you're walking on, like on a cube, it's absolutely terrifying.
It's like your brain tells you that you have absolutely no chance, when it's just like a sharp edge.
As soon as we put like the real model of snow and ice, it's like your brain is telling you like, yeah, I might grab that stone on the way when I'm falling.
And you know, so it's like it says, you know, it's not that bad.
So we actually had to change a little bit reality in this case to pull your brain and tell you yes it is.
And other things that are interesting is also the length of things.
So for example, we have a scene where you cross a crevice on ladder.
And if the ladder was very long, then the sense of vertigo was very much increased.
It was shorter, it was reduced.
And I think it was just the brain telling you, like, if I have to cross this, I will feel bad for a long time.
But if it's short, I will only feel bad for like a few seconds and it will be okay.
So that's one way of scaling the vertigo effect, actually.
And then there's this thing that I find the most intriguing.
It's the way the brain actually at some points, when the brain decides that you have reality, it has accepted the reality contract.
it starts patching things up for you. So you as a content developer, you don't need to do anything anymore. And I had a friend that went through one of our demos, and he was describing the experience to me afterwards. And he was telling me he was looking down on the ledge, and he was looking at his boots, his yellow boots, and I told him, we don't have boots in the game.
His yellow boot doesn't exist. And the fact is, is that...
We actually, we cannot put like feet on our model because there's no tracking.
And it will be very strange if you have like, for example, footprints coming out that are not where your feet are.
So we avoid that.
But what we did instead is that we took the acceleration of your head and used that to trigger a...
snow crunch effects or sound effects.
So, coming from here.
So when you move a little bit, you hear like, and it sort of follows what you would expect when you're walking.
And that seemed to be enough to like, okay, I'm standing there.
I'm on snow, I can hear the snow.
And that's how probably he ended up seeing those boots because his brain has just decided that this was all real.
Another thing is shadow.
Because you don't have a body in this kind of VR experience.
Usually you just have these hands.
Then it's kind of weird you don't have a shadow.
But we found that shadows actually help a lot in providing a sense of immersion.
it sort of like projects you somewhere.
And it also gives a good sense of scale because your shadow can extend for a long time.
But the problem is that you don't have an avatar, so we can't really cast a shadow.
But we've experimented both, we cast shadow from the gloves, and that is helpful.
And we would also put like very simple...
volumes like just a cylinder and even that is enough because your brain is just...
see it's like it's a shadow that moves with me it's sort of like it is me and I don't really care if it's not actually me or looks like a body it's just a shadow and it doesn't need to have very sharp edges and such but it makes the things much more physical when you see it and it makes you also feel more physical And then there's also things which are probably things that are already well familiar from movies and such, but controlling physiological factors subtly is very efficient.
So we do have like heartbeat tracks in the audio.
that we can sort of like, we can like pump them up at some certain places and people tend to empathize with heartbeats so you can sort of like increase people's stress level just with a heartbeat and we also have a breathing track and and that's the same thing people empathize a lot with breathing like if you have like a shallow breathing sound you start to feel like you're out of air yourself So that helps us in recreating the right emotions and feelings, depending on the scenes that we have.
So for example, in the Everest, you do go above a level where you will feel the effect of oxygen deficiency, even though you have an oxygen tank.
And it's good to be able to recreate that aspect using these techniques.
And another thing that we are also experimenting with is controlling your movement.
Because also when you're up in a death zone or up a hill or step or even to the summit, again because of the altitude, you tend to move very slowly in real life.
So you don't rush through things.
And there's nothing in VR that prohibits the user to just run through the whole experience.
So we experiment a little bit with, like, if you go too fast to start blacking out.
And after a while, you realize, OK, if I want to go through this experience, I need to go slowly.
And in that way, you're sort of conditioning the user to a point where he becomes more immersed in the situation.
And I think that's interesting things to do as well.
All these little brain hacks.
Another thing that we found is that the wind makes you cold.
A lot of people complain about being cold after the experience, which kind of makes sense when you have snow all around you and wind and particle effects swirling around you.
And we also want to have the breathing when you...
when you like breathe out of your air.
And that's also a very efficient reality cue to tell you that you're there.
What I'm doing now is breathing out, and it is in this world.
So that's kind of a cool thing to add as well.
And that's all the slides I have.
So I was just wondering if we want to go to Q&A.
That's like a, I think.
It's cool.
I think there's an open mic there if somebody wants to...
Hi. Just a quick question. You said that you disabled depth of field in your passes. Now, when you're playing, I haven't had a chance to try the Everest demo, but without depth of field, do you find that anybody, like, you don't feel it as real as it should be? Because obviously you have natural depth of field with your real vision without it being present in the demo. Is there like a disconnect there?
Well, it depends. From From my experience, you as a real person, you don't really experience depth of field.
because usually your eyes are always adapting.
Yeah, yeah.
So it's more like a movie trick where people use that to sort of change the plane of interest.
But in our case, first, of course, the panorama is often very far away, so there is really not so much depth of field.
And then we do have some atmospheric fog that sort of simulates the effect of distance, and I guess maybe that sort of works in a similar way as depth of field.
Okay, cool, thank you.
How big is the room going to be, please?
How large a room do you need for the experience?
So we've just been following the recommended min specs that have been issued by the device manufacturers.
I think it's like for the Vive it's like 2 by 2 meters nearly.
And for Rift it's slightly smaller, like 2 times 1.5 for a standard experience.
We also tried experimenting with actually scaling the content, depending on the room, to try to make the most of it.
But it's kind of difficult to do that.
So if you had a crevice, we could actually move, shift it a little bit.
But as we use pre-computed lighting, it's not easy to do, to have such a dynamic scene.
I'm wondering, you talked about having breathing effects when they breathe as well as like a breathing track.
How do you deal with that, like not being in sync with their actual breathing?
So there is a microphone that you can use.
So it would be in sync with their actual breathing then?
Sorry?
So it can be in sync?
Yeah, it can be in sync.
The only problem is that you want to maybe filter it quite heavily because of course you can't really control what goes into that microphone.
You know, people may be banging their fingers on it.
But you can sort of cue it.
I think also head motions can help.
There are subtle motions that you do with your head when you breathe.
But it doesn't need to be very enforced.
It's just like a very wispy kind of particle effect is kind of enough to feel that there's air coming out of you.
Cool, thank you.
Just a quick question.
I'm wondering whether you could share the total ballpark of the cost that went into production?
The cost?
Yeah.
I can share with you the info that we've, it's about 10 persons for six months.
What about other materials?
If you own a helicopter, it's a little cheaper.
Sorry?
If you own a helicopter, it's a little cheaper.
That's of course in the acquisition of the pictures and all that, which is a fixed cost for our internal production budgets.
