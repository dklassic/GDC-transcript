Ditch nature, we'll come out of nowhere Freak shit, frown like sulfur in the air So many people, many pieces, only pain and Six times knocking on the door No response but hey, you're an imbecile!
Just wanna get a perfect at my game It's plain easy now Wanna have a game?
Guilty Gear Exile I'd like to talk about our art style of our latest title, Guilty Gear Xrd.
Why did we choose this art style?
What made it so hard to fake 2D in 3D?
How did we accomplish it?
What made the difference?
We only have a limited time frame, and I won't be able to cover each and every aspect of it, so it mostly consists on the 3D character art and the animation.
If you have any specific questions, I'll try to leave as much Q&A time as possible at the end of the session.
So please ask me at the end of the session.
Before we go on, I want to know how many people here have actually played the game.
Much more than I expected. Thank you.
How many of you have no idea what the game is about?
Okay. Okay.
How many of you are artists?
Okay.
Quite a lot.
And coders?
Oh, quite a lot too.
Great.
How many of you like anime?
Oh, super.
Okay.
How many of you are interested in non-photorealistic rendering?
Oh, that makes me so happy.
I know our art style is not the most conventional in the West, so I was just curious to know what kind of people were interested in this topic.
Now let's get started.
First, let me do a super fast introduction.
I am Junior Christopher Motomura, a technical artist and character modeler working at Arc System Works.
I've been working at the company for 13 years, mostly as a third 3D artist.
but in many other positions, such as game designer, game director, story script, localization, voice acting, and even some singing, which I personally don't want to touch.
My role in Guilty Gear Xrd were lead character modeler, character look development.
That included writing shaders, rigging, and other bits and pieces.
So basically, I'm the guy behind how the three characters look on the screen.
Guilty Gear Ex-Aid is the latest installment of the Guilty Gear franchise, a long-running series of 2D fighting games.
Previously, most games in the series used 2D sprites.
This title is a reboot from a long-time halt on the series and has some drastic change in art style.
We chose to do it in 3D, instead of 2D sprites, while maintaining the charm of the sprites it had.
This was a great leap of faith, and we're thankful it paid out.
To get a better idea of what we are talking about, let's see another video mainly showing how the gameplay looks.
In Guilty Gear Xrd, our goal was to keep the look and feel of the old titles and represent them in a new form, 3D.
The visuals needed to resemble those of the sprites we had used for so many years.
and look even better with higher resolution in mind.
Although it utilizes 3D a lot, the game itself is still played in a 2D plane, being true to its nature.
As you may see, the art style is strictly based on Japanese anime, which is also a tradition brought over from the 2D sprites back in the days.
This was done by combining custom cell shaders and 3D models made especially for these shaders.
Turning 3D meant that we were no longer limited by the resolution.
The screen is now 1080p for PS4, 720p for PS3.
So that was a great boost in resolution from the old 480p sprites.
The shift to 3D graphics gave us much more benefit than just higher resolution.
It's the freedom we achieved on the camera.
The cel-shaded 3D graphics work well in the battle screen, but the real shine is when the camera swings.
Being able to move the camera at will, dynamic camera angles are now viable in the game.
For special effects and final and finished scenes, the camera moves around the 3D space and shows the action from a more dramatic perspective.
We're happy with the result and the feedback we got from our audience.
Thanks to that, I'm standing here today at GDC.
Thank you.
Thanks.
So we got a new art style, but where did the idea come from?
Why did we choose this path?
Why didn't we stick to 2D sprites?
Well, there's a simple answer.
We already had BlazBlue.
BlazBlue is our other 2D fighting game franchise that utilizes high-res sprites.
And it does it so well, we didn't see much room for improvement.
We didn't want to compete with ourselves.
So for Guilty Gear Xrd, we needed something different.
And that's when we decided to use cel-shaded 3D.
Well, we considered many other options, such as vector art, ultra-high-res sprites, and others.
But what emerged most promising was cel-shaded 3D, which I, at the time, happened to be experimenting with.
This here is a piece from a canceled project that I was working on.
The project didn't work out, but the art style showed a lot of potential.
So when Guilty Gear Ex-Art finally started to take form, we decided to take this art style even further.
If done right, we could maintain our anime style while gaining the various benefits of 3D.
Dynamic camera angles and animated cutscenes were things we were craving for back in the days.
We also found potential in it because the technique was not well explored.
and had lots of space for improvement.
By advancing the state of the art, there was a great chance to stand out in the crowded market.
So we decided which way to go.
Now we needed to actually start moving.
Let's have a look at how we faced the challenge.
Getting 3D to look 2D was going to be a tough challenge, and we knew it.
It was seldom accomplished, so we had to find our own way and pave our own path.
A lot of study went into both 2D art and 3D technology, because knowing both sides was essential to tackle this task.
Elements that were not translated well enough from 2D to 3D were picked out and solved one by one.
We broke a lot of conventions on the way, because they just didn't suit this style.
As a result, we came up with a new unique workflow, mostly built from scratch.
At the same time, we introduced ourselves to new tools in order to face this huge challenge.
First, Unreal Engine 3.
We considered multiple engines, and Unreal Engine 3 was chosen because of multiple reasons.
We needed a new engine that could handle multiple platforms.
Our deadline was strict.
and building an engine from scratch was not an option.
We had a team mostly consisting of artists, and the artist-friendly design of Unreal 3 matched our needs.
The fact that Unreal 4 was on its way and Unreal 3 was around the end of its lifecycle also benefited us.
The engine was well-developed and stable, and we had a lot of freedom customizing it with the full source code.
The other tool that helped us a lot was Autodesk Softimage.
The flexibility in modeling and animation let us focus on quality.
A built-in shader editor made it possible to experiment with cell shading a lot.
Because of its non-destructive workflow, we were able to improve our assets continuously throughout the whole production right till the last minute.
Without this tool, I believe it just wouldn't have happened.
Now, let's dive into the details of how we did it, starting with the look of the characters.
The most important asset of our game were the characters.
Without getting them right, there was just no way we were going to get the game right.
First, the characters needed to represent the 2D design well enough to appeal to the audience.
The models had to be as good as the designs, or even better.
And they had to look 2D as much as possible.
To do this, our principle was simple.
Kill everything 3D.
If you find something that looks 3D, you just have to find a way to avoid it.
This often boiled down to hand crafting by the artists.
Because while the math within the shaders are always correct, correct is just not good enough.
To get a convincing 2D look, everything on screen has to be an intentional choice, not just the result of a calculation.
To achieve all this, instant feedback was key.
We were fortunate to have a real-time shader preview on our modeling software that gave us trustworthy representations of the actual game.
I would like to show you a demonstration of this.
It's not coming on.
Just a second.
Things happen.
No, that's not.
That's the presentation.
I need the other one.
This should be one.
Oh, there we go. Okay, great.
Sorry for that.
Okay, um...
Here is a model of one of our characters.
As you can see, real-time shading is taking place.
and it completely resembles the look in the actual game.
The models and animation were created in this environment, so the artist would know exactly how the end result will look, even without exporting to the game.
For example, the modeler will know exactly how it will look in-game if some vertices were modified.
Well, you wouldn't really do that, but you'll see how it looks in the real game if you do that.
So let me just undo that.
Okay.
Okay, so let's go on.
So let me turn...
Okay.
There we go.
The models have a fairly high polycount, around 40,000 triangles on average.
We knew we were going to do extra close-ups in cutscenes, so the models had to hold up even at extremely close distance.
Details were directly modelled in with geometry.
What made this special about the character models in Guilty Gear Xrd are that they are very texture independent.
For example...
Unlike our average 3D games, we do not use textures such as normal maps for our characters.
Instead, we use vertex properties such as vertex normals, vertex colors, and UV coordinates a lot to store data in.
The reason for this is, again, the extra close-ups.
The problem with texture data is that they're very resolution dependent.
Pixel data easily can get jaggy at super close-ups.
And that cannot be helped unless you have ultra-high resolution.
On the other hand, vertex properties are linearly interpolated between vertices, which is completely resolution independent.
Now, let's go into the actual shading.
In cell shading, the surface is either lit or not, and no value in between.
This is one of the reasons cell shading is so hard to get right.
In a drawing, the artist will choose the most convincing distribution of light and darkness.
But in a shader, it's all math, where unforgiving thresholds mercilessly split between light and dark.
In cell shading, every little noise on the surface will become extremely distracting.
The slightest difference in the surface normal may end up as a huge blotch.
A convincing 2D look can only be achieved with precise control over the distribution of shades.
The code for our cell shader is actually pretty simple.
In the main part of the calculation, a generic step function is used to decide if the pixel is lit or not.
To put it simple for artists, this code works like this.
If the surface normal is facing the light source, it's lit.
If it's facing more than 90 degrees away from the light source, it's not.
90 degrees is the threshold here, but it could be any other value.
What matters here are the following.
the threshold, the light vector, and the normal vector.
And that's all.
Only these three components matter in this shading code.
So what we have to do is simple.
Take full control over those three.
First, let's have a look at the threshold.
We use the channel from vertex color as an offset to the threshold.
This made it possible for artists to make certain areas on the model get darker more easily.
This could be used to represent the occlusion of the vertex.
Areas more occluded from the light have less chance to be lit.
The artist will go around the mesh setting values, defining the areas most likely to be shaded.
The artist may even set the value to zero, making the area always shaded no matter what. We use texture data in the same way in places, but vertex colors served us better in many cases.
Again, texture is dependent on resolution, but vertex properties are not.
The linear interpolation between vertices gave us a very clean result without any pixelation.
Adding to that, it was much easier to adjust the vertex colors because you can just get instant feedback from the preview, even while fiddling with the values.
Next, to lighting.
Unlike normal 3D games, Guilty Gear Xrd does not have a global lighting system affecting the characters.
Each character has his or her own dedicated light vector that lights their idle pose in the best way.
The lighting doesn't change frame by frame in the battle scene, but in cutscenes it animates along with the character to get the best result each and every frame.
And finally, the normals.
Surface normals are one of the main reasons cell shading is so hard to get right.
The smallest inconsistency in normals results in a super evident artifact under cell shading.
The problem with the normals are that they are automatically calculated.
And the result of this calculation is often not what the artist intended.
On the other hand, shading in 2D art is very intentional.
To close this gap, there was only one thing we had to do.
Control the normals with intention.
Fortunately, Softimage had powerful tools built in for this purpose.
Although editing normals is nothing new to game graphics, we took it even further.
To get rid of unintentional shading, we modified the normals on every major feature of the character.
The faces of the characters especially needed to be handcrafted to get clean anime look.
Here, I'd like to show you the comparison.
This character model has his face normals edited.
It looks clean from any light angle.
And here is what, here it was how it's gonna look if I delete the normals.
Yeah.
Pretty ugly.
practically unusable. So let me undo that.
And there we go, clean again. Okay, something in common with these three components I just talked about is intention. Being able to reflect the artist's intention on the final result is the key when it comes to stylized art.
Now, we sorted the lighting out.
We have to think what color to paint the lit and shaded colors.
Color selection in anime is vital.
Every color is carefully chosen to express not just the material, but the character itself.
It is not as simple as just multiplying the shaded area with an ambient color.
The coloring style in anime can vary from title to title, but one thing is in common.
This too is very intentional.
The combination of lit and shaded colors can express a vast amount of characteristics, not just the material, but the atmosphere the character wields.
It's an art in its own sense, and it's a task usually given to professional color designers.
To get the anime look right in 3D, we took this by heart, and worked on a way to replicate this process.
By studying anime art style extensively, we found that shaded colors often express how solid the material is.
A less solid material will pass more light through it, getting a lighter color for the shades.
The light passing through the material will get tinted depending on the material's composition.
For example, shades on human skin get a red tint because of the flesh under it.
In order to implement this in the shader, we used two textures.
The base texture defines the color of the surface when it's lit.
while the tint texture defines how dark it gets when shaded.
Multiplying the two textures, we get the shaded color.
This way, the choice of color is completely up to the artist.
The artist can choose whatever color he or she may see fit for the lit area and the shaded area of the material.
On a side note, as you can see, both these textures are just square areas consisting of single colors.
This is because we simply use them as color lookups and do not draw in any details to the textures as images.
As stated before, most meaningful information is stored in the mesh itself rather than in the textures.
Along with colors and shading, lines play a huge role in cell shading.
In fact, if you think in black and white manga style, lines would be the most important aspect of the visual.
In Guilty Gear Ixard, we use a traditional inverted hull method as outlines for the characters.
A second set of darker polygons are generated in the shader and are expanded in the normal direction to create the hull, which shapes the outline.
We added some new tricks to this long-established method to achieve even more control over the results.
It's common these days to use post effect for the outlines, but we found this method suited our needs better.
First of all, we could preview the result right in the modeling viewport, letting us know exactly how it's going to look in the game.
We had more control over the lines through the vertex shader, giving precise control over the whole model.
With this, we were able to give the outline variable width in places we saw fit, even erasing it completely at times.
Again, vertex colors was used to control these line properties.
We got our outlines solved, but there's another set of lines that need to be discussed.
The inner lines, or the lines on the surface.
We needed a different approach for the inner lines, because the inverted hull method used for the outlines did not work for these kinds of lines.
These lines play a huge role in 2D art.
conducting where one surface ends and another starts.
In order to get a 2D look, we just needed to find a way to get this right.
The easiest way would have been to simply draw the lines on the texture, but we knew that there was a major limitation to this.
The challenge here was, again, resolution.
We needed the characters to be able to hold up at extra close-ups.
without any jaggies or pixelations to be found.
To avoid pixelation, you would need an infinitely high resolution for the textures, which is simply impractical.
So we had to find a better way.
We came up with a technique using special UV assignments to get jaggy-free, super clean lines, even at close-ups.
Here is how this technique goes.
All the lines are drawn as axis-aligned beams on the texture, and the UV is lined up alongside them.
How much the UV overlaps these beams defines how thick the line becomes.
As long as the pixels are aligned to an axis, they don't get any jaggy.
So we figured we could use this to our advantage.
The downside is that you get a very distorted UV for the surface.
But that was not a problem for us, because we do not put any details on the texture.
It may be difficult to imagine, so let me demonstrate it to you.
Okay, this would be how a freehand line on a texture would look like.
Pretty bad.
It may look okay from distance, but it clearly does not hold up at close-ups.
Let me get rid of that and show you the UV.
As you can see, all lines align to the X and Y axis.
This is the area of the shoulder.
Here's the shoulder.
By nudging the UV onto the black area, you can control how wide the line is.
I'll show you.
You can precisely define the width of the line at each UV point and get super sharp edges at places you see fit.
With the same texture resolution, you get a super clean line without any noticeable pixelation going on.
I zoom up, and still no pixelation.
I zoom out, still clean.
These lines look great close by, but because texture filtering is in play, it also looks very good from a distance.
With that, we have a pretty good model with many features representing a 2D feel.
Now let's move on to animation.
Animation is where the merits of the models and the shaders truly come to play.
As with the models and the shaders, our goal was to make it look as 2D as possible.
To do this, we took a style called limited animation.
This is a term often used in Japanese animation industry in comparison with full animation.
Unlike Western animations such as Disney, it focuses more on tricking the eye with less frames. This was the way the old Guilty Gear titles were animated, and to replicate that, we decided to use the same principle, but this time in 3D. We did try the full animation path in early development, but it simply didn't convey the sense that it was 2D.
Having the poses interpolate between keyframes makes it look smoother, but at the same time makes it look more 3D.
So we just stopped using interpolations between keyframes.
Every frame now is a keyframe, and the animator poses the character in the best way possible.
You could imagine stop motion animation where dolls oppose each frame.
Basically, that is exactly what we're doing.
This looked more true to our old sprites.
and also made the cutscenes look way more 2D-ish.
In order to use limited animation as our style, we needed to build a rig to match this method.
Because we were going to have less frames per animation, we knew we had to add more information to each frame.
This was done by having a lot of bones so the animator can move every feature on the model at a frame-by-frame basis.
The bone count is about 500 per character on average.
No simulation was used, because again, it just doesn't look 2D.
There's just no way to capture the anime-style movement with simulation.
Scale animation was used a lot, because it lets us do many sorts of tricks.
Exaggerations of actions, let things hide or appear, traditional squash and stretch, and more.
We knew we needed it, but the engine didn't support it.
So we had to implement the scale system by ourselves.
It was a difficult task, but thanks to our talented programmers, it was done.
And it was totally worth it.
Turning off interpolations between key frames is essential, but that alone is not enough.
You need another trick to mimic 2D animation.
The secret is to deform the mesh every keyframe to add imperfection.
The human eyes and brains are very sensitive when it comes to perspectives.
If a part of a 3D model moves through the perspective perfectly maintaining its shape, the human brain instantly recognizes it as a rigid 3D object.
To avoid this, adding imperfection was the only way.
Nature is imperfect.
The artist is imperfect.
Therefore, perfection looks too artificial.
As an example, let me show you how the animation actually looks like in Softimage.
Every keyframe, every bone on screen is adjusted by the animator to break the perfect transition.
This makes each keyframe more distinctive, adding even more information to a single frame.
You need to think in 2D, is a word used a lot while animating in our team.
3D accuracy is not priority in this workflow. We often sacrifice it in order to get more dynamic composition. Limbs, hands and feet get a lot of scale animation to exaggerate the perspective, and facial parts do not maintain natural positions.
This is exactly the same as 2D animation.
Expressiveness over accuracy.
And what we did was simply bring the same principle to 3D.
And here is how the final cut scene looks like.
For those curious, here's a comparison between full animation and limited animation.
Let's have a look.
This is how a full animation with interpolation would look like.
And here is how limited animation would look.
Simply having more frames, it's obvious that the full animation looks a lot smoother.
However, it's more apparent that it's actually compared to the limited animation version.
The limited animation version may look less smooth, but has more distinctive poses, easier to recognize.
It's up to you which to prefer, but we chose limited animation for the clarity it holds.
So that was some short explanations of the major aspects on how we built the assets for the art style.
Of course, there's much more.
but there's no way I could have covered it all within time.
If you have anything specific, you want to know how it was done, you may question me in a few minutes.
In the meanwhile, let me talk a bit about the reason why we were able to get this done.
What made the difference?
In my opinion, it was figuring out this one thing.
The X factor is the artist's intention.
All the techniques were used.
All the techniques we used are actually nothing new.
No new hardware or software innovation was involved.
Technically, the art style could have been done years ago.
What made the difference was the notion that the artist's intention was the X-factor.
A workflow which lets the artist's intention carry through right till the final result was the core of our accomplishment.
But why was it us and not anyone else?
One simple answer would be that we were lucky.
Thinking about it now, we were in a very fortunate situation to get this done.
We were a 2D-oriented studio that had a lot of 2D experience, many artists who knew 2D inside out.
We were not new to 3D, but not heavily invested in photo realism either.
We were in a need for a new art style to differentiate from the best of our own, BlazBlue.
The project was a reboot.
of a very well-established game so we knew exactly what we were working on.
And the team.
The team was a dream team.
We had exactly the right people with the exact skill set needed for this challenge.
Of course, fortune wasn't all.
The dedication and the hard work of the people involved in the project and the strong support from our fan base was what made all this possible.
Now to wrap up.
Here's my final proposition.
I want to see more studios explore non-photorealistic rendering.
I feel that not enough research has been put into this area, and there's much more room to explore and improve.
Photorealism is great, but it's not the only route.
It's a crowded one, too.
If you look in different directions, there's a whole new horizon.
You could be the pioneer that gets there first.
The frontier is there, waiting to be discovered.
Thank you.
It seems we have a couple more minutes.
How many more minutes?
I'm not really on track.
So if you have any questions, please go up to the mic.
I'll try to answer them as much as possible.
Okay.
How does this go?
Please, go ahead.
Hi. My first question is how many people does it take to create each character?
And how long does it take?
The character model and texture, the rig, is basically done by the same person.
We had about three or four modelers all together.
So, and it takes about two months altogether for one character.
That's only for modeling, and then another two months, I guess, for animation.
So it's quite intensive.
Okay, the second question is, you have a very unique UV layout, so for each character, each character design is very unique as well, so do you just look at the 2D concept art and figure out how many specific patch of UV you need to create and then figure out the final layout for each individual character texture?
So your question is about the UV layout?
Yes.
Yeah, basically we just...
stage of UV layouting, I think, is not that different from any other project.
We just layout the UV depending on how big the area is.
But after that, we kind of line them up into straight lines.
And after that...
Once we get straight line UVs, we draw lines on the textures, and then nudge the UV back onto the textures.
I see. Thank you very much.
Thank you very much.
Okay, um, please.
Um, I totally understand not wanting to do the interpolation between keyframes.
Did you, because it does end up looking more 3D and more artificial, did you guys ever consider...
Doubling the keyframes themselves by hand obviously that doubles the amount of animation work, but was that ever a consideration or?
Having more keyframes in the animation is one thing we wanted to do but Doing it too much also kind of cost cost a lot so Keeping it down and also keeping it down makes it more 2d so kind of we looking for the best balance there Very cool. Thank you. Thank you Okay, please.
So you mentioned that each character takes about two months or so to model and animate, correct?
Well, not anymore maybe. The first stage, while we were trying to figure out how to do it, I think it took more.
It's getting shorter.
Right.
The main thrust of my question is, is it seems like the models for Exert might be substantially more labor-intensive than a sprite-based game like BlazBlue, but then I'm also not sure about that because I believe the BlazBlue sprites start out as 3D models, don't they?
And then they just get, you know, sort of sprite-captured...
Yes.
But the frame.
Yes.
Um, BlazeBlue also uses 3D models to make 2D sprites.
They basically are used as lotoscopes.
The models from BlazeBlue are a bit simpler than the GuildGear ones.
Um, because the hair, the facial expressions...
drawn in afterwards, so they don't have facial expressions in models.
Yeah, and basically they're not designed for close-ups, the BlazBlue models.
So is it a lot more labor intensive to do the XRD models?
Does that make it hard to add in more characters?
Because, you know, obviously Arc System Works likes to, you know, put out some updates to its games and whatnot.
Yes, we would totally love to add more characters more.
Quickly, yeah, but it to stand out.
We just needed to pursue quality, so it takes a lot of time.
Alright. Yeah.
Dizzy when is basically what I'm asking here.
Yeah, I was sort of sort of expecting that.
I wouldn't be able to answer that, so alright, thank you sometime sometime.
Thank you.
So for your inverse hole outline system, did you expand that along the generated normals or the normals that you handcrafted or separate set of normals?
OK, that's a good question.
I didn't really go too deep into it, but the normals for the extraction of the hull polygons are actually separate from the shading normals.
So we have another set of normals that are automatically calculated.
for that purpose, yes.
All right, thank you.
Thank you.
I wanted to ask, so you went with limited animations for the character animations and their finishes and stuff.
I was wondering, what was the reasoning behind not also doing limited animation for the camera rotation as it goes around the character?
OK, that's a good question.
Camera rotation is, it looks ugly.
with less frames.
And in anime, although in limited animation, although the frame per second is like low, eight or 12, but the actual, how the background moves, 24 frames per second so it it differs from characters background and when the camera rotates the background moves with it so it kind of if you turn down the camera too much it gets really snappy.
if that answers the question.
Yeah, I have a friend of mine who felt as though it was very, kind of, for him at least, it was very disjointed, you know, the limited animation of characters versus the very smooth animations of the backgrounds.
Yeah, we tried to, we tried lower frame camera movement, but it just didn't feel good.
Okay, cool, thank you very much.
Thank you.
Did you have to swap out models and stuff for things like Milia's hair?
Yes, I didn't touch that today because it was going to take a lot of time, but yes.
Lots of characters in Guilty Gear have really crazy animations.
Shadows of Zato, hairs of Milia.
They do a lot of morphing animation, and that is practically impossible to just do by a single mesh.
So we just have to make a bunch of meshes to kind of switch in between shapes.
Yes.
Thank you.
I was just wondering with like these highly detailed models, these hundreds of bones per character, a lot of animations, like how difficult was it to get a locked in, solid performance out of the game?
And how scalable is this whole workflow?
The shader itself is not that performance heavy.
So, and only having two characters on the screen.
It's much more forgiving, so maintaining 60 FPS was possible on PS4 easily.
We had to tone down the shader and the polygons for PS3 a bit, but it still runs mainly in 60 FPS, yes.
And it seems like because of the way you're doing your textures, you can actually do them quite low resolution without losing that kind of quality?
Yes.
Textures.
Some textures, we have three textures per character.
Basically, one is 2K and two are 1K.
All right, thank you very much.
Thank you very much.
Please.
First of all, it's an honor to have you here, so thanks for giving this talk.
Thank you.
My question relates to specular highlights.
I see.
I could be mistaken, but for instance, on Sol's deltoids, there was a bright lit area.
Yes.
Do you have a third texture for specular highlight?
Tint?
No, we do speculars.
I didn't touch on speculars either.
Yes, speculars basically.
No, we don't have an extra texture for that.
It's basically based off the base color and the tint color.
So we kind of mix that to get the right specular color.
OK, and it uses the same normals that the shading calculations are used for?
Yes.
That's true.
OK.
OK.
Thank you.
Thank you very much.
OK, I lost track.
Please.
Thank you also, again, for speaking today.
I have two questions, if that's OK.
I was curious if the effects are hand animated the same way, or if they are particles.
Okay, effects. Some, some, okay that's a good question. I couldn't really go into effects today.
Sorry about that. Most of the effects are particles, but not just particles flying around, but animated particles.
So, the hit effects and everything like that are actually 2D, but have a set of pictures switching every frame to get the animation.
Some particles or effects, such as the smoke from the character's feet when they jump, they are actually done in a pretty, kind of a brute force way.
They are modeled each frame and swapped each frame, so we have a bunch of meshes swapping every frame as a smoke.
Well, it actually isn't that hard to do.
And we were kind of joking about it, how we're going to do smokes in this game.
And one of the jokes was we're going to model them each frame.
And it turned out to be the best way, actually.
And then my last question was just, were there some key features of XSI that you could only do in XSI for this game?
I think it'll be pretty hard.
I wouldn't want to do it in any other software.
That is simply because you can just go back to any state and continue fiddling with the model without any redoing the skins or redoing the UVs.
It just keeps there, and you can just keep working on it until the last minute, as I said.
And yeah, some.
The normal editing is pretty good in Softimage, so I'm pretty sure how Maya and 3D Studio Max do it, but as much as I know, 3D Studio Max has some problems with normal mapping, I mean the normal editing.
Yeah, so that's about it. Thank you.
Thank you very much.
Please.
So the effects question was actually what I was going to ask, but to expand on that, the parts like if they're swinging swords and there's some drag on the shape, is that just the mesh of the sword being, like the verts being dragged?
Yes, that is a combination with switching meshes, a question. Some characters' swords are switched when they are kind of expanded.
And other times we just add an extra mesh of that shape, because it only lasts for like a frame on the screen, so it's no point putting the mesh into the actual mesh.
So we just add it as an effect.
Cool, thank you.
Thank you.
Hi, I actually had a question about the normals editing.
It seemed like you had some really specific control over the normals on the models, and I was wondering what tools you were using to do that.
Was that just the XSI modeling tools, or did you build your own?
Yes, basically what we know use a lot is Gator.
It's a tool in Softimage that lets you copy a set of normals from a simpler mesh to a more complicated mesh.
This gives you a more smoother mesh, smoother normal, and everything else basically we just hand tweak by the normal editing tool.
Thanks, it looks really awesome.
Thank you, thanks a lot.
I actually did have a second, much more serious question about your lighting solution.
So you have each character lit individually, separate from a global lighting solution.
You've got the lighting for the stage, and then lighting for each character.
But first of all, does the lighting per character change a little bit depending on the stage they're in?
To make them sort of match the feel of the stage better?
Okay, first of all, the background isn't lit.
It's all the shadows in the background are pre-baked.
So there's no global lighting.
One thing that has global lighting is the character's shadows on the ground.
So the shadow maps have one global lighting, and that's independent from the character's own light set.
The light set doesn't move, and that is an intentional choice.
We have the same light set in every stage.
because we didn't want to change the impression of an action.
If the shape of the shadow changes from stage to stage, it will give a different impression in each action.
And that is not a good idea for fighting games.
Right. Which sort of gets to my bigger question.
It sounds like because of the way you have the lighting sort of customized per character and like no global lighting, it would be extraordinarily difficult to use your specific sort of solution for a game that would require somewhat more open movement through a stage, like an RPG for example.
Yes.
Like, you know, where you... there's just sort of this requirement for global lighting or local lighting, and the character has no choice but to interact with that somehow, or else it'll look really weird.
That's true, yes.
Yeah.
Basically, we knew it was going to be a fighting game with a fixed camera.
Yeah.
So it was a choice with that in mind.
Okay.
If we were going to do an RPG or action game, we would take a different route, obviously.
Okay, so yeah, so that just...
Your lighting solution just can't really apply to sort of wider games.
Well...
Probably not.
Probably, yeah.
Oh, okay.
Thank you.
Okay.
Thank you.
You're welcome.
Thank you.
Sorry.
Hi, thank you. First of all, I'd like to say it's great seeing Japanese companies kind of embracing NPR rendering or NPR rather We don't have a lot of companies here in the West that are doing that, and stuff that your company and CyberConnect2 with the Ultimate Ninja Story games is just really inspiring.
My actual question is, I noticed that you're using a lot of the vertex color information for the outlines, but you're also using some to define the lighting on the character?
Are you managed to fitting that all that information in the RGB and A channels or do you have to store it some other way?
Yes, we are using all the four channels RGBA for the lighting or the threshold of the lighting and the other three are basically dedicated for the outline.
Okay, great, thanks. Thank you.
I had another question about the editing of the normals.
Do you allow non-unit normals when you are editing them?
So in the case of the front of the face, that you would actually scale them down.
So when you do the normal dotted with the light, it would more easily bias into shadow or highlight.
Yes, we considered that.
But at the moment, we keep it at unit normals because it kind of.
It kind of gets unpredictable in places, so we just keep it to that state.
Yes.
And one more quick question.
With the textures that you fetch, do you just point sample those only?
And I assume you don't use mipmaps because you would worry about at far views they would sort of blur and give you non-controllable issues.
Although I said we don't really treat them as images, we do sample them as textures.
Without that, we wouldn't get the inner lines like that.
So it's basically the same texture sampling, but with a special UV set.
OK.
But the sampling mode, like for the line control and the extrusion, is that in like a point sample mode where it's actually, or is it linear?
So when you zoom out into the distance, it's smoothly adjusting.
Okay, I assume you're talking about texture filtering?
Yes.
Okay.
Texture filtering in the shader when you're looking up those textures.
Yes, texture filtering.
Having them, yeah, drive the extrusion.
Yeah.
Texture filtering is in use, therefore texture blurs at far away.
Yes.
And that blurring area kind of acts as a fill-in to the lines.
So you allow it to filter it then.
Okay.
Thank you.
Thank you very much.
Please.
For characters that cast shadows on themselves, do you kind of author the self-shadowed areas manually?
Or do you do something special with lighting to deal with those?
OK, so this question is about self-shadowing?
Yeah.
OK, self-shadowing is one thing we wanted to do, but we didn't do.
So basically, we.
We use the threshold mechanic instead.
The problem with self-shadowing is that it's very uncontrollable.
In anime, you get like characters with big hairs.
And big hairs drop big shadows on the faces.
And there's just no way to control that.
You either have to turn it off and kind of deal with it.
We just decided not to deal with the self-shadowing and do kind of a manually set the sort of self shadows by hand.
So for all the characters with like coats, all of this like shadowed regions are just the threshold, vertex paint, manually edited and...
Wow. Thanks.
Okay, thank you.
Yeah, I think that the game looks really amazing.
Excuse me, sorry.
So...
We don't have much time, so this will be the end of the questioners, so please go on.
Yeah, I'm just wondering if you have any further thoughts on what to do to improve even more on the 2D look in the future.
OK, yeah, we have various ideas.
I can't really talk about them yet.
it will kind of ruin the surprise.
So I hope you just stick around and see how it goes.
Yeah, that sounds interesting enough.
Thanks.
Thank you.
OK, thank you.
Thank you very much.
