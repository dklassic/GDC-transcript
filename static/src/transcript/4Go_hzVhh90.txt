Hi everyone, let's get started.
Before we actually start, we just need to send you a reminder to switch off any cell phones and please remember to fill out the evaluation.
Thanks.
Right, let's get started.
Welcome to our presentation.
Welcome to our presentation, Finding Balance, Realizing Responsive High-Fidelity Character Movement in Just Cause 3.
My name is Alex Crowhurst, Lead Animator at Avalanche Studios, and I'm here with Jeet Shroff, also Lead Character Programmer at Avalanche Studios.
We're super excited to be here today, as we have a rare opportunity to share with you some of our feature work currently in progress and before the game is released.
So it goes without saying, what you see here today is very much work in progress and is going to continue to be worked on by the team until the day of release.
So before we dig deep into the presentation, I think it's important to briefly frame the overall experience of the Just Cause franchise.
So let's start with the three main pillars that are the backbone of the series.
At the heart of the franchise lies an open world sandbox experience where the player can have complete freedom to play around with the world and its systems and just causing havoc and fun everywhere.
The second being fast-paced combat, where player responsiveness is key to be able to move fluidity in through high-pressure combat situations.
And finally, vertical gameplay.
Our character has the freedom to move from the land, sea, and skies seamlessly in any way that they choose.
So coming off from Just Cause 2, we wanted to push these pillars even further and take it to our next-gen experience.
And for Just Cause 3, the overall direction was to ensure we keep all the insane and crazy stuff you could do, but ground it in a more realistic style.
And with that in mind, one of our key components we wanted to build on was the realization of characters, most notably our main character, Rico, who you can see here in this image, but the back of him at least.
From a design perspective, there were two main goals we needed to achieve.
Rico must be highly responsive to player input.
The player needs to feel fully in control and should never have to wait for an animation to finish playing to make their next maneuver.
Secondly, we needed to maintain the fluidity of combat.
This means that Rico must maintain the ability to perform combat actions like aiming and shooting, even when transitioning between complex mechanics and states, which results in seamless combat.
Now, neither of these were easy tasks by any stretch of the imagination.
On the realization end of things, our main character can do an awful lot in the world.
He has a full range of on-foot locomotion set, supporting many different kinds of weapons.
He also uses his grapple hook in many different ways and in any situation.
He can now stunt and traverse on top of many different vehicles and interact with them as well.
He can free fall, he can parachute, he can now wingsuit, and that's just the tip of the iceberg.
With what seems an endless amount of mechanics, we needed a way to help save us time and cleverly share as much content as we could across all these features.
Secondly, having such a responsive character is always going to fight against the realization needed to give a character believable weight and motion.
So for animation, it's a bit like trying to fit a square block into a triangle hole.
It just won't fit.
So now that I've given you some background behind the challenges of Just Cause 3, let's take a look at some of the ways we attacked these issues.
Alright, so in this first section, let's look at how we update the motion of our characters.
And I'd like to start by talking about the different character movement models.
So, first we have game-driven movement.
In this case, the game logic evaluates the motion of the character, which is then fed into the physics system, which moves the physical representation of the character.
In most cases, this information is also fed into the animation system, which selects and plays an animation to match the movement of the character.
If the game controller wishes to move the character forward at a certain speed, depending on the speed, a forward moving walk or run animation may be selected and played by the animation system.
The key point is that animation does not control or dictate the motion in any way.
The animations could be thought of as well authored on spot.
Using this has some advantages. It provides us with the maximum level of flexibility as it drives the character independent of the animation.
Motion as well remains highly responsive as we're completely in control.
Alternatively, some games use animation-driven movement, and you guessed it.
In this case, the motion of the character is driven completely by the animation.
Motion itself is also authored as part of the animation.
So we do this through the use of a reference node.
In our system, we refer to this as the root node.
And it sits at the highest point in the skeleton hierarchy.
The root node is an independent bone and represents the displacement and rotation of the character's motion during the animation.
The animation itself will contain the transform data for this bone for every single frame, just like it would for any other bone.
We generally place this under the hips of the character throughout the animation, and it can be hand-authored by the animator.
So, in order to move the character in the game, we need to extract this root motion data out, and then apply it to the character.
So in this simple example of a walk, every frame, we simply update the animation and read the delta displacement or rotations for the root, which then basically moves the character in the game.
And we refer to this sort of extraction as the extracted motion of the animation.
Animation driven movement certainly has some advantages.
Our characters now look the best, since the movement is updated exactly as it was authored.
The visual representation of the character now matches one to one with the motion.
And it maintains the intentions of the authored content.
A lot of games have now moved over to use this approach in the last few years.
And while it's improved the visual aspects of gameplay, there's some costs.
Our characters now start to feel a little bit unresponsive, both in terms of performing and controlling an action.
We're now tied sort of to the animation.
And don't get me wrong, some games can totally get away with this if they're slow-paced enough, and precision and responsiveness isn't that big of an issue.
So, in our case, we're after the fidelity of animation-driven movement, but to maintain character responsiveness, our character can't always be fully animation-driven.
In addition, heavily using animation-driven movement would mean that we need a lot of animation coverage to account for all the varying use cases that we encounter in a game like Just Cause 3.
So here we're looking for the balance between these two worlds, and what we want to do is we want to use the best of both of them together.
So let's quickly dive into representing motion.
Here's a frame of a backpedal animation.
The red arrow here shows the facing direction of the character in world space.
The yellow arrow here shows the moving direction of the character.
Since this is a backpedal, they're opposite.
And finally, we come to speed.
The speed at which the character moves or rotates will result in the change in displacement or orientation of the character, as you can see in this video, as he's backpedaling backwards.
In our system, we can independently update each of these components using different combinations of these movement models, often midway through the animation.
We also often found that any given use case required its own combination of updating these components in sort of slightly different ways.
So to do this, one of the techniques we heavily rely on is what we call motion correction.
When updating any of the components of motion, each of our states and even animations within them can decide which of these components will be either fully driven by the animation's extracted motion, which of them will be adjusted or corrected, or which of them will be completely overridden by the game itself. This is what we mean by motion correction.
A fundamental point we rely on is the importance of decoupling the animation's extracted motion from the animation's pose update.
In other words, instead of directly updating physics with the animation's motion when it's updated, we need to separate the two and then do this independently.
Now this enables us to modify the extracted motion independent of the animation and then correct it in order for us, in order for it to suit our game-specific need.
Alright.
So you may think, well if we're already adjusting, we're already overriding the motion, then why use the extracted motion to correct it?
And why can't we just control the character?
And the key reason here is we want to limit these adjustments and corrections to be based on the extracted motion as much as possible.
And this will help us preserve the integrity of that original content.
We use these motion correction techniques not only to maintain animation fidelity, but also to increase animation coverage and keep our motion responsive.
So here's an example of one of our simpler character updates.
And I say typical because it's not always the case, but generally speaking, the first thing we'll do is gather input.
We do a little bit of motion evaluation to determine what's exactly required.
And then we decouple this. We sample the animation separately, and we extract the motion separately.
After that, based on our evaluation, an additional step is performed to either adjust and correct the extracted motion based on that evaluation, if we need to do so.
We can then push that over to physics, and if no adjustment or correction is made, you can think that the character is being fully animation-driven.
If we ignore all the extracted motion and simply move the character ourselves, then the character is fully game-driven.
Now, as I mentioned, this is a rather typical character update.
We're not limited to doing these corrections before physics by any means.
We have cases where we apply motion correction in various stages of the character frame, including post-physics, for things like interacting with a moving object.
But just to get the point across, this is an example of one of our simpler character updates.
Alright, so in this next section, I'd like to go over how we use these ideas to realize our players on-ground movement, looking at some of the typical use cases like starting to move, stopping and interrupting, and maintaining movement.
It's also worth mentioning that for the purpose of this presentation, we're going to focus on ground movement.
However, these techniques are not limited to just that.
We've used these techniques throughout realizing a lot of our other character feature sets as well.
Let's start off by talking about starting to move.
In the case of starting to move, we use transition animations as a way to move the player from an idle stance.
Now the reasons for this are obvious, if you see in the video.
Animations like this play a big role in visually conveying energy, anticipation, and acceleration that are needed to get a character to move from an idle stance.
However, we still need to support moving in 360 degrees in terms of direction and towards varying target speeds.
So from a gameplay perspective, we also want this input that the player would provide to feel very immediate and precise.
One way to tackle the visual concerns would be to simply drive each component, speed, displacement, and orientation of the character completely animation-driven.
We tried this approach, and things quickly got sluggish and unresponsive as we became bound by the physicality of our content.
Especially accelerating to something like your top target speed is going to take time, and that was too slow for the game.
Also, a given start animation can only rotate the character by a fixed amount.
This made it feel like the character wasn't going in the direction we wanted, especially if the pad input didn't match up with that authored animation.
In addition, while the animation is still finishing, we feel this, you know, feeling of being stuck in that direction.
In this case, you can try to add more start animations to cover more angles, and then you need to account for all the different weapons, different speeds, and then your content starts to grow.
Another approach could be try to blend all these animations together to create more coverage.
Here we've got a decent starting to run in a zero degree direction, start to run forward.
And here's another animation that starts the character to run at 90 degrees to the left.
Now on their own, both these animations look fairly decent.
When we blend them together to get additional coverage, CF45, things start to look a little funky.
Now this is clearly an exaggeration, but too often we use this idea of persistently blending animations together for ground-based movement, and that comes with a set of limitations.
If you've ever worked with animation, you know that each animation includes a wide range of subtlety and body movement, and stride, which is the distance between each step when you move.
In order to ensure that these animations blend well, Usually, you know, you have to make your animator really unhappy and have him or her heavily dampen or eliminate all these subtleties.
In some cases, the animations need to be authored with the same number of footsteps, which limits us both from a stylistic and a motion capture perspective.
So this is something we wanted to limit for our on-ground movement, especially because footsteps, weight, and movement are so tied together.
So here's the technique we use.
It revolves around combining these movement models and using motion correction all within the same animation.
So the first thing we do is identify these key segments of our animation.
So here's an example of the start going from an idle to a run in 180 degrees.
If we step frame by frame, we can see that the first part of the animation includes the character actually turning to face the direction that he wants to go in.
And we can, looking at this, we can basically say, OK, well this is our turning to face segment.
If we zip back a bit, we can also see that around midway through the segment, the character's pushing off his right foot, and he's starting to move.
And he continues to do this all the way until the end of the animation, as this is a start to move.
So we can refer to this as our moving section.
So there's a couple of things that are going on in this one start animation.
Finally, as this is a start again, he's also accelerating to get up to that target speed.
And the idea we use is during each of these segments, we can update the components of motion differently to help alleviate some of the issues we mentioned before.
So the first thing to look at is creating more coverage and giving the player a sense of control without having to sort of persistently blend those animations together.
To do this, during the turning to face segment, we can correct the facing angle of the character.
First, we extract the rotation out of the animation, and then we correct that rotation to give us the target direction we need.
And this is done by performing a fixed adjustment to the rotation after the animation's rotation is extracted every frame.
So let's see an example of how this works.
At the beginning of our turning to face segment, we've got our character facing forward.
At the end, he's rotated an entire 180 degrees, as authored.
Well, what if the player doesn't want to go directly behind, and he or she chooses to move the character 150 degrees instead?
All right.
Well, using the same animation, we just need to account for the delta between the rotation amount from the extracted motion and the intended target direction.
To account for this delta, you can either add or subtract it out from the extracted rotation over the course of the turning to face segment, while keeping that movement target fixed.
So in this case, since the extracted rotation is going to overshoot, we need to subtract this delta out.
It's important to split this delta into even rotational corrections over the course of this segment, to limit any slide or adjustment that the player might notice.
So, on each frame, we extract the animation rotation, and we correct it by that fixed adjustment that's been calculated and is now evenly being distributed on each frame to account for the overall delta.
We keep doing this, and by the end of the rotation segment, since we've evenly corrected out that delta, the character will end up facing the direction of his or her intention, even though the extracted rotation was a full 180 degrees.
And this takes care of the facing angle, or the facing direction of the character, during the turning-to-face segment.
Well, what about the moving direction during the turning-to-face segment?
We can see that there is a little bit of a moving portion that overlaps between the turning to face segment, and then it continues all the way after, like we noticed when we saw the animation be played frame by frame.
Well during this overlap, we simply override the moving direction to be the same as the target pad at the time of playing the start.
Now this means we don't have to worry about the compensation we're doing for, or we don't have to worry about compensating for the correction we're doing in the facing direction.
By updating the facing and moving directions independently, we can start to move the character sooner in the direction we want, even if he hasn't fully rotated to face that direction yet.
We'll see an instance of this a little bit later.
Well, after the turn to face segment is done, the start isn't finished yet.
The character still needs to continue to ramp up to get up to that moving target speed.
During this time, we still need to update the facing and moving directions of the character.
Since there's no more rotation that's coming out of the extracted motion, the character is displacing in the direction he's facing for the remainder of the animation.
To control this, we simply override the facing and moving directions to now be fully game-driven, to match that of the target direction.
So as the player decides to move the pad in another direction, we procedurally override the facing angle and implicitly the moving direction to match that of the intended direction of the pad.
Now we don't want to override this instantly, as this would be visually jarring and unrealistic.
So we smoothly drive the current phasing direction to match that of the target, and we do this by defining a range of authorable angular acceleration rates.
These rates define how fast this current orientation will become that of the requested pad direction.
These, these rates that I talk about are meant simply to match the responsiveness needs, as well as keep things consistent across all ground movement.
But you can use different rates to match different situations in the game.
For example, if you're just moving about, you can use a rather responsive sort of curve or rate.
And if you're doing something like landing from your parachute and then going into a run, we can reduce the responsiveness to better match up with that of the animation.
You can also choose to use whatever smoothening algorithm you want in these situations.
Overall, this gives us a better responsiveness and control as we can continue to steer the character in the direction we want without having to interrupt the start early.
And this allows the start to visually settle into movement, which is what, you know, sometimes you reach the end and you're like, let's just cut everything to make it responsive.
We want to prevent that.
Now this brings us to our last point.
The responsiveness in displacement is going to be linked to how fast the character can be displaced.
As you can see in this slide, in the original authored animation, the character isn't really moving for a good chunk of time in the beginning.
Well, what we really want to do is we want to translate the character sooner.
We want to extend out this translation or displacement segment.
This will alleviate the feelings of being sluggish, but still cont, and, but, but still allow us to control when this can actually happen.
Since we're already managing the direction of moving during and after that sort of turn into face segment, we know exactly where we want to translate the character.
Now it's also important to note that you don't want to extend this moving segment too far out, beyond, or, you know, sooner than the turning to face segment.
And the reason for this is you want to kind of piggyback off the animation's visual movement to hide the fact that we're starting to move the character sooner than what the animation actually intends.
So, to do this, we need to control the rate of movement, that is the speed of the character in this case.
For our starts, we do this by overriding the speed of the character via the game, and evaluate the speed using authorable acceleration curves, as opposed to that of the animation.
So here we have an acceleration curve from the start animation, it's just an example.
And we can see that the animation begins to move the character with a little bit of a delay.
It might be turning to face, it might be doing whatever it needs to do.
And also, it only reaches a target speed of roughly 3.4 meters per second.
Alright, well let's say our in-game run speed is 4 point meters per second.
We now author an acceleration curve that moves the character 1 faster to get responsiveness, and also better match up to the target speed, which is going to be variable and change over the course of your project.
You can keep on reshooting and reworking your content.
These curves are live tunable and are often based on the animation's authored acceleration curve to begin with, to limit inconsistency that the player again might notice, since we're overriding the speed from the animation during the start.
Surprisingly, we were able to hide this adjustment, since the animation that's playing is still conveying the visual intention of the character.
To further help with that fact...
You know, the fact that we're moving the character faster or slower than the animation, depending on whatever this game curve decides, we apply a technique to visually match the animation's pose to hide this difference.
This is done by correcting the animation's playback rate to match the difference in speed.
Now, since the pose and motion updates are completely independent, we can do this without it affecting the displacement.
When we apply this visual speed correction, we need to use a reference speed that determines how much to scale the playback rate of the animation. In the case of starts, the target speed of each authored animation becomes the reference speed. So this goes back to that idea of using your animation's extracted motion and the way it's authored to influence how you're going to correct it. This reference speed now becomes the base for how we're actually going to correct the starts.
In this case, since the target speed is higher than that of the author start animation, we simply play the animation that much faster.
Here, the playback rate is going to be adjusted to be 1.17 times.
And this is done, again, to be within an acceptable range to ensure that the animation doesn't look too sped up or too slowed down.
So here we have it all put together in game.
You'll notice the player pad input being displayed on the top right corner, along with the pad target direction in yellow, and the current orientation of the character shown in red.
You can kind of see that while he's playing these transition animations and starts, he's still pretty responsive, and you can still see, you know, good visual intention, good animation of him moving in the direction he wants.
Here's a closer look.
And to quickly recap, we're updating the orientation direction of our character using animation and motion correcting the rotation, while the displacement direction remains game-driven, after which both directions switch to being game-driven, and speed is completely game-driven using visual speed correction.
We're able to get 360 degrees of coverage using just five animations, a 0, 290s, and 2180s, and on the NPC side, we're able to get away with just using three.
It's important to note that we also drive all our plant and turn animations, as well as all our rotate on spots using these techniques with slightly different combinations and variations.
And you're not limited to just using this fixed adjustment either.
We use dynamic adjustment to deal with aligning to a moving target, for example.
Or you're not limited to using this adjustment to just rotation.
We use adjustments like this that are split across the entire animation to fix up translation or displacement as well, to end up on a very specific spot, for example.
All right, talking about stopping or landing on a spot, in this next section, we'll turn our attention to interruptions and stops.
The motivation for stop realization in the game is very similar to that of the starts.
You know, stops will give the player immediate feedback of releasing the pad, and will communicate the deceleration and the character's intention, especially when moving at a high speed.
Now in Just Cause 3, the need to stop responsively and with extra precision was uber important.
The design of our world is extremely vertical.
And the gameplay sort of is based around that.
So we need Rico to stop on a dime when the player decides to release his pad, even from a run.
Primarily, we want to do this to ensure that the player doesn't unintentionally get in a situation where he falls off things, especially in combat, which can be uber frustrating.
So as Alex will attest to, stopping on a dime is an animator's nightmare.
It's both physically unrealistic and visually unnatural.
If you're using motion capture data, you know, and you want to capture this deceleration, the data will always have significant motion in it.
In addition to all these issues, we need to deal with stopping from various speeds, differences in pose, and turning while stopping as well.
So in the case of stopping instantly, our first approach was to ignore all the extracted motion from the animation.
While this ensured that the character would insta-stop, it just felt pretty visually jarring.
Kind of pops.
So what we ended up doing was updating the moving direction and speed to be driven via the animation, which includes a very harsh deceleration curve to convey the change in momentum.
And we could have done this in the game as well, but it would have been very, you know, pretty much the same thing because we're trying to, you know, get the character to stop very quickly.
Since we didn't want to add extra coverage for all the slight differences in speed, we play the same stop animation from a general speed range and account for any differences by using the visual speed correction, similar to the starts.
In this case, though, we correct the animation's playback rate to match up with the urgency of the current speed against the initial speed of the stop animation, which now becomes our reference speed.
So in the case of the starts, we're using the authored start's target speed as a reference speed.
But in the case of stops, we're using the stop authored initial speed as a reference speed.
Finally, the facing direction of the character was corrected similar to the starts, using the fixed adjustment.
And this was necessary when we need Rico to stop and turn at the same time, and he needs to maintain aiming at a target, for example, while stopping.
On updating the character this way, we began to encounter some blending issues that are worth noting.
Now, these blending issues aren't limited to just the stop case.
They affect all situations where we're relying on the animation's extracted motion to be used as a basis for correction or analysis.
So the issue is this, that when we transition from one state to another, and we're using a transition blend, the root's transform data begins to blend as well, since we just treated like a regular bone.
So in this case, we really want to exclude this blended displacement of the root of the previous cycle as it continues to blend out.
Otherwise, the character still displaces forward as we're using animation-driven movement here.
And that's not what we want. We want him to stop on spot.
So this could be fixed by simply setting the blend time to 0, from the movement cycle to the stop.
However, a transition blend of 0, again something animators hate, would pop the pose and look jarring, which is again what we're trying to avoid in the first place.
The decoupling of the pose and motion extraction comes to our rescue again.
Here we extend the idea to blend the animation poses and the extracted motion separately using different blend rates.
So we blend the pose of the movement cycle with the stop animation as we normally would, but we do not blend any of the extracted motion of the movement cycle on evaluation or update. And we call this pose-only blending. Now this helps create the illusion of deceleration.
and it alleviates the visual harshness of the sudden transition.
You can also extend this idea to poor bone blending, where you're using different blend rates for different bone masks, creating the illusion of motion lag to, say, blend the upper body and lower body at different rates.
Another key problem we had, and I'm sure most of you have encountered, is dealing with the mismatch of poses that occurs when interrupting and transitioning from animation states.
So in the case of our stops, we had to deal with the scissoring of feet that occurs when we have to play a stop animation off phase.
And since we can't wait to play that stop animation, we need to interrupt the cycle at any point to play the stop.
You can see the sort of scissoring that will occur in the feet when that happens.
So you could solve this problem by adding more stop coverage, more animations to best match up with all those differences in pose.
But we handle these problems through the use of a technique called pose matching.
So instead of blending animations based on elapsed time, we blend the two animations based on pose.
In most moving animations, the pose can be defined by what we call movement phase.
As the foot goes from being planted to going into an upswing, and then all the way back down to being planted again, we can think of the foot as being in a swing cycle from plant to plant.
We do this for both feet.
The movement phase of a foot is then defined as the time where the foot is within its swing cycle between 0 and 1.
And now we can use this information to know where in the next animation we should blend into, so we can match up the poses for both these transitions.
Now this allows us to have an uneven number of steps within all our movement animations.
With that said, for any given set of animations that you want to use this technique, you need to define your phase to be based on the feature that you're actually trying to match.
So for example, if you're building some kind of punch animation, then the phase would not be where you are in your kind of arm swing.
This phase information is generated offline and is stored as metadata, which keeps our runtime calculations to a minimum.
States can also define whether to apply pose matching when they blend in and also when they blend out.
And looping animations, for instance, will do this.
They'll pose match on both sort of situations because they have all the phase match data they need for both left and right feet.
Certain transition animations, however, may only want to pose match when transitioning in or when transitioning out.
And this is necessary because you don't want your pose match to skip the most important parts of your animation.
So now when we apply pose matching, we're able to interrupt and transition seamlessly into the stop animation.
In addition, we can save on additional coverage simply by adding an extra step in that original animation and then just let pose matching blend us in.
All right, since we've talked about transition animations to death, let's turn our attention to maintaining movement.
Unsurprisingly, our core movement animations are authored as looping animations, where the start and end frames are identical to allow us to play these animations again and again, continuously.
Now, our character needs to be able to accelerate and decelerate, and best match the movement to the magnitude of the thumbstick.
Since the speed of displacement in these animations is fixed as well, we need to find a way to vary our movement speed.
One way to tackle this is to drive the speed of our character using animation-driven movement, map input to animations, and then blend these animations together to create the in-between coverage we need.
And we all know how we feel about persistent blending.
So also in addition, mapping input directly to these animations sometimes can result in that slow-mo walk or an extremely sped up run if the player is moving the stick in this analog fashion and then you're mapping that directly to animation. So in Just Cause 3, our player character can either walk or run when moving on ground.
And the first thing we choose to do is use discrete target speeds of movement.
What this means is that for given ranges within our pad, between 0 and 1, the character moves at a set target speed.
For example, if the pad input is between 0 and 0.3, say something like 0.2, then this maps to a fixed target speed of 1.5 meters per second, which is our walk.
Between 0.3 and 1, for example.
We set a fixed target speed of 5.5 meters per second, which is our run.
Now the key reason for doing this is that in these kind of maintaining movement cases, we don't need our character to move with the control of the pad in an analog way.
In most games, you rarely need this.
The target speeds we pick are also the speeds we want to support gameplay.
We can then go off and build animations specifically for this, or shoot motion capture for these specific speeds.
By ensuring that our character is always targeting a core movement speed, we're trying to ensure that we always get the best visual result.
These target speeds now also become our reference speeds, which I'll talk about a little bit later.
Even though you're always targeting a core movement speed, our character still needs to accelerate and decelerate between these speeds.
To do this, we update the speed of translation via the game and not animation.
This speed is evaluated using authorable acceleration and deceleration curves, similar to how we described in the starts.
And here's an example of the curve in our in-game editor.
We then match up animation using speed ranges.
and we apply correction to them.
So here you can see our target speed set up at 1.5 meters per second and 5.5 meters per second.
Now no matter where you are on the pad, our character will be targeting either one of these core speeds.
So we go off, we get our animations, in this case we have a walk and a run.
And now we define speed ranges.
Each speed range is associated with a core movement animation.
In this example, we set up a range between 0 and 3.5 to be associated with our walk.
A range between 2.5 and 5.5 is now associated with our run.
If our player was previously walking and now jams his pad forward, wanting to run, based on our acceleration curve, our character would begin to accelerate and pass from one speed range into another.
If the character needs to move at a speed that's inside, say, the walk range, then we simply play the animation that's associated with that range, but then apply visual speed correction again to correct the playback rate to match the speed of the character that's being driven by the game.
The reference speeds that I talked about are these core target speeds now in this case.
So here, we would slightly speed up the playback rate of the walk to match the fact that the character is going a little bit faster.
In the case where he's in the run range, we would slightly slow down the run to match the fact that we're moving a little bit slower in game.
Now, since this is, again, strictly visual, we can adjust the reference speed as we see fit in game to make sure that it doesn't scale the animations to be too slow or too fast.
To smoothly transition between the two ranges and prevent either animation from doing that, from being played too fast or too slow, we allow the ranges to overlap to handle the transition between the walk and the run.
And only during this overlap, while the animations are being visually corrected, we allow the animations to blend together.
Now, since we're always targeting a core movement speed, the character is never allowed to stay inside this blend, or even be in a situation where the animation has to be visually speed-corrected for too long.
We can take this idea a little bit further.
We're in situations where we're not aiming or shooting, and the animations themselves are actually pretty different, like a walk or a run, or a relaxed walk and then a run, where we have different steps and style.
Once we reach that section of overlap, we just allow the character to transition from that walk to a run, and skip the blend altogether.
And this transition animation can be, you know, corrected the exact same way like we've been doing with the starts and the stops, to keep things super responsive and maintain that sort of high fidelity.
So let's not forget about our facing and moving directions while maintaining movement.
They're game-driven, the same way we correct the facing and moving directions for the starts after that turn-to-face segment, based on those angular acceleration rates.
We just need to make sure that the animation's root needs to be animated in a way where we minimize the curvature within its displacement.
All that means is you kind of want to keep the character moving sort of like this in a linear fashion.
That doesn't mean that the character model can't actually come off the root a little bit, but you want to minimize sort of that wavy stuff, because we're actually going to be procedurally correcting that in-game.
Finally, to top things off, we add a procedural lean to show off the momentum change that's necessary for turning the character in-game.
This leaning is linked to how fast the character is turning and is also clamped and stabilized to prevent it from going out of control.
We've noticed if this sometimes can go unchecked in games, your character starts to feel kind of drunk as he's kind of oscillating back and forth as he's leaning about.
Alright, so solving responsiveness in movement was one thing, but we also needed to make sure that combat was responsive and seamless as well.
Right, so combining actions.
Being able to combine actions together is a vital way to ensure combat can flow between features.
I think this image pretty much sums it up to a T.
Rico's in mid-air.
reeling into most likely what is an enemy helicopter, whilst at the same time firing at a crazy rocket launcher, and he can do this all throughout the whole of this sequence.
So, if this doesn't give you an essence of how insane Rico is, I don't know what will.
What this means, though, is that we still need the animation coverage to support all of these actions.
The way we choose to solve this problem is to use animation masking.
This allows us to split our base content into building block chunks, which are created at compile time through the use of bone masks.
We then reassemble these animation masks with our base state machine to build up the poses we need, adding no additional cost to runtime.
In addition, animation masking allows us to be memory efficient.
Data that can be shared is exported out as a mask only once.
This mask can then be reused as needed.
We can think of our final animation pose as being built at runtime from a combination of these bone masks.
So let's dig a little deeper into the example.
Let's imagine this image is an unarmed idle animation.
It has no other dependency, so it can be a full body animation.
So all bones are included in this set.
However, as we build our content to hold weapons such as rifles, we realize that we can probably share some data.
We can reuse the lower body part of the animation, but we want to adjust the upper body because if he's standing in idle pose, we'll need to adjust the spine and the arms to really match up to hold a rifle.
So as long as the animation is in sync with the lower body, we should be able to maintain all the nice motion coming through from the original idle.
We can then base all of our weapon sets off this animation.
So how do we do this?
We firstly mask the original animation into two, a lower body mask and an upper body mask.
We then share the lower body mask with a new upper body rifle idle animation to combine and complete our rifle wielding idle.
So, what if a designer comes up to us and says, we don't want to add rocket launchers into the game.
We realize that we can probably share the head and torso of the rifle upper body animation.
as the overall stance is pretty similar to that.
However, wielding an RPG using the rifle arms is gonna mean that the RPG is gonna start cutting through Rico's body.
So we further separate the mask to reuse that rifle head and torso by creating an armless upper body mask.
We can then create a unique RPG arms only animation in red to complete our RPG wielding idle.
This accounts for the two different weapon types we need coverage for.
So what we end up with is a lower body animation that can be shared across all these cases, an unarmed upper body animation that needs to be its own content, a head torso animation that can be shared across all weapon types, and specific arm-only animations meant for individual weapons.
Splitting up and combining these masks allows us to minimize animation memory and eliminates the need for creating a lot of unique full body animations.
And it's also completely up to the animator to decide the level of granularity we need.
So here's the result of applying this technique on Rico in game.
You can see that the masks that I just talked about all building up to create the final animation of the character.
So our masking system is pretty robust and flexible.
But we also need a way to overlap all our animations.
So we use animation layering to tackle this problem.
Our animation system allows our characters to include multiple layers.
The first layer includes our base animation content.
By masking this, we're ensuring that the data here is only brought into the memory once.
In other instances, we need to be able to perform additional actions that can overlap multiple states.
To do this, we add another layer on top of the base layer to play overlapping animations that also have no major dependencies on that base content.
So it's ideal for such things like reload animations.
They can be masked on top of the arms, whilst Ricoh can move through any transition animation in the base layer.
We then include another layer to handle additive animations that need to be synced with our base actions.
And this is used for something like aiming up and down vertically.
And finally, we can add a layer to handle additives that can also overlap multiple states.
This helps us to give texture to our character's performance.
In this example, let's say Rico is moving across the top of a jet, stunting or whatever, and the jet is obviously moving at a fast speed.
So we can enhance this feeling by bringing in an animation additive that makes Riko's clothes and buckles move and react to the wind.
These layers are then sequentially updated at runtime with each pose being passed through into the following layer.
Finally creating the final character's pose in-game.
So to wrap things up...
Techniques combining movement models and correcting motion give us the responsiveness and coverage we need, without killing the fidelity of our animation.
Using masking and layering to combine actions gives us the ability to share and reuse animation content and further minimize the amount of authoring we need to do.
Well, we hope this was an insightful look into our techniques and systems.
And we hope to share our aiming, movement, and other feature set solutions with you at a later time, hopefully post-launch.
But for now, we cannot wait for you to play this game and push and experiment with our mechanics.
So thank you very much, and we can invite the room for questions.
Do you have my email if you have any questions?
Hi.
Going back to your phase matching portion, you're talking about you identify.
phases in your animation where the foot's planted or in full swing.
I was just curious how you guys do that.
Is it automated by a script or do the animators actually manually identify those moments?
So what we do is, so the question was how do we actually figure out how to generate the phase match data.
And currently, basically, we have the animators add a key on all plants.
And then we have a script that basically runs and looks at the height between plants to kind of decide what the swing is actually going to be.
So for things like walks and runs, we use different height values to kind of know what the max swing will be.
And then for now, because all our walk and run content is roughly of the same height, that script kind of gives us good enough phase match data.
And it doesn't have to be perfect, because we're actually blending as well, but it's good enough.
And you could make that as complicated as you need it to be, but this was good enough for us.
Thank you.
Great talk.
I was curious how you guys handled interruptions of transition animations.
If you need to change your mind mid-transition, how'd you deal with that?
Sure.
The question was, how do we deal with transition animations to interrupt?
sets of movement, I guess.
And I mean, we just play the animation, right?
So basically, again, using kind of these segments, we can kind of define interruption segments as well.
Hey, when is it OK to start interrupting this animation?
And you don't want to kind of push that interruption segment to be too far out, because then you're fighting with responsiveness again.
But say something like a start, I kind of hinted to it.
And in the video, it had some plant and turns that were also playing.
So, say for example you're playing a start, and suddenly you decide to go in the other direction.
We play a plant and turn animation to get you out of the start.
And then while you're doing that, you're doing the exact same movement correction on that plant and turn to now correct it in whatever direction you wanted to go and start steering the plant and turn itself.
So, you have that coverage to allow you to interrupt animations, and then you use the exact same techniques on all that content to give you the responsiveness.
Were you continually adjusting the?
the rotation, angular rotation, throughout the transition animations?
Or was it a snapshot?
That's a good point.
We don't want to do that all the time.
In some cases we do.
But when it comes to starts and plants, you kind of want a little bit of time where the character actually isn't being kind of spun in a different direction.
Because then again, you're going to break the animation believability.
Like if I'm turning this way and the game suddenly makes me go that way, it's going to break.
So you kind of want to wait at least until it's a good time to do that.
and then either plant off this foot, and then allow correction using these kind of data-driven segments that I talked about.
Cool.
Thank you.
Hi.
Did you guys have, for your start animations, did you have stand to walk animations?
Yes.
And if you did, how did you handle if the player moves to a run speed in the middle of the start animation?
Right, so I mean, based on that pad input that I talked about, if the target speed actually happens to just be a walk, and you happen to be in an idle, we just play a start to walk start instead.
And are those not long enough that the player can't...
try to go into a run in the middle of the start?
Yeah, exactly.
So while I'm playing that walk start, if I happen to jam my stick again, I can either play a transition if I want to, or I can just allow that to blend the kind of same way we talked about.
But actually, in the case of if you're already playing an animation, like a transition animation, you want to just interrupt it with another kind of start to run transition.
That would be better than actually blending it, because it will look weird if you blend the animation to a cycle.
Okay, thank you.
I love Just Cause 2, thanks.
One of the cool things is very organic environments, you can do a lot.
Is there like a maximum angle that the character can run up and at what point do you just kick him to a slide as opposed to letting him free-nab up every surface?
Sorry, the question was what is the max angle that the character can run up and down?
Yeah.
Yeah, I mean we want to definitely have I'm not exactly sure what the design I mean, we try to get the character to support as much movement as possible, especially going up and down.
And we, you know, especially in this game, we're using techniques like IK and stuff like that to help extend that range further.
But the locomotion system would handle that to allow you to basically move up a slope as well as go down.
Right.
Hi.
Hi.
My question is about network.
So you guys develop a lot of solutions.
How much of these solutions worked on a network with multiple players?
On a network?
We haven't tried that yet.
Hi.
I have a question about the mocap system.
And how do you guys measure that it's 5.5 miles an hour or 1.3 is in a treadmill?
Or do you guys eyeball it?
You want to answer that?
Yeah, I mean, we went through an iterative process of trying to find out what speed would work really well for our character.
First of all, we had him sprinting around the world and it was just way too fast.
We actually needed to slow him down quite a bit to be able to be quite responsive.
So, you know, we took that to the motion capture data.
We kind of found the data that was good to play.
And we set out, like, yeah, we need it to be this speed.
But luckily the tech allows us to be able to, like, manipulate it within those realms.
So, you know, it's good in that way.
Okay.
So you fudged it a little bit, the mo-captures, to get to the right speed?
Yes.
Okay.
Cool.
Awesome.
Thanks.
Cool.
All right.
Thanks a lot, everyone.
Thank you.
