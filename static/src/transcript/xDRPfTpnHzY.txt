My name is Omar. I'm from NVIDIA.
I'm here to talk about something that I've been working on for the past three and a half years.
The title of my talk is, This Machine Has No Brain, Can It Borrow Yours?
Does anyone have any idea what this is in reference to?
Has anyone seen these signs before, IRL?
This is actually from Fallout, but these signs are whenever you have a machine that can kill you, this sign usually is in some form next to it.
And I'm here to give a warning to everyone, because we're facing a crisis we don't yet see.
In about 10 years from now, we're going to have robots everywhere.
So we already have some form of robots in our personal space, right?
We've had, for example, cleaning robots hang out for the past 10 years.
But now we're beginning to see some changes in...
the effect and the amount of robots that we have in our personal lives or doing logistics for us.
And that's mostly because we have amazing reach nowadays, right?
We have innovations in motors and in microcontrollers that can do stabilization.
We also have innovation in dexterity.
So grippers that used to be something that was very specialized.
are now, we can now manufacture them pretty much in mass manufacturing, even for like the most specialized tasks.
So you would have rubber and silicone in injection molding, which is a mass manufacturing technique, but now it's very common in robotics. So you can do soft robots.
But you can also have...
Robots with tendons that have a very high gear ratio.
Over here, you are looking at a robot from the Cornell Organic Robotics Laboratory that is holding a can, right?
It is very accurate for a machine to be holding a can without crushing it.
This is done pretty much with custom motors.
This is, you know, like we are definitely going in the direction of precise machinery.
And we also have AI, as you well know, and the infrastructure is also going pretty much to personalized robots.
We have data centers at the edge, which means you might have a data center in your next-to-your-traffic light, or just by the base station of your mobile phone.
The economy is pretty much not going to allow it to be anything else.
We are headed towards a completely autonomous future in several fields.
And this is a problem.
This is a really big problem, because what's missing in this chart is that we don't have good design for any of these robots.
So no one's designing their behavior.
And if they're going to be interacting with the elderly, in health care, with food preparation, with delivery, and logistics, they all need to be designed to work around humans.
And so far, it's a hack.
We don't have a stack to actually use robots interacting with humans.
And we need to think about how we're doing it.
And my proposal over here is that simulation is the right way of doing this.
So let's start with a basic question.
I know you all work in VR, but how many people here know these images?
These were the world's first ray traced images.
They were created in 1979 by Turner Whitted.
That was a paper that he released while he was at Bell Labs.
He called these very imaginatively figured six, seven, and eight.
They were part of a paper where he's describing also how ray tracing works, which he described in figures one and two.
One is a description of the actual algorithm.
The other is a description of Snell's law, which is a way to describe how light moves through different media.
We have not gone to simulate actual optics and therefore have photorealistic rendering until very recently.
If you look quickly at the evolution of computer graphics, we had our first popular 3D engine that can run in real-time in 1993.
On the other hand, we had our first ray-traced film in 2006.
If you ask the people at Pixar...
They would say that a lot of the ray tracing there that I would consider fully ray traced was a hack.
I consider it fully ray traced. They actually managed to simulate a whole model of light.
But something happened along the way. We have this now, right?
With GPUs, we can suddenly do things like create game engines which approach photorealistic simulation.
In this case, we are looking at Boy and His Kite, demoed by Epic Games, which did a lot of photogrammetry around the environment. But nowadays, when we have reached real-time ray tracing in GPUs, we can move between what is basically this all the time to this, right?
I am going to do that again, because I love that.
Apologies to everyone.
What this actually means is that suddenly we have achieved several levels of abstraction throughout the years.
We started out by being able to only make games via basically assembly language, and then slowly evolved into what we have now in game engines, like Unity and Unreal.
But the same thing happened in graphics in general.
If you worked in a film in the 1980s and 1990s, you'd be doing everything with command line interfaces.
Now we're using Photoshop and Maya and Nuke.
And this also happened in robotics, because when you were working on robotics in the 1970s, you were building all of your own circuitry.
Around the 1990s, you started having microcontrollers that were very expensive and hard to program.
And nowadays...
The stack is almost completely solved for you.
You have operating systems like ROS that do all of the robotic control on one hand.
On the other hand, you have very good hardware.
You have GPUs on the robots themselves.
Some companies make them. I'm not going to name names.
The underlying idea behind all of this is that from engineering, from a culture that is supposed to build machines for other people to have ideas on top of, we now have design frameworks.
And from the idea of modeling a...
Well, making a mathematical model to solve a problem, we're now just teaching computers to do this, right?
So in a way, you can talk about machine learning being the abstraction to how you design human robot interaction.
So I want to talk about machine learning a little bit so we're all on the same page.
I think this is the first interesting component of deep learning that was ever introduced.
I think this is the first paper from Stanford in the 50s about neural networks.
At the time, they could deal with one.
Basically, one solver which had a bunch of inputs and would output a linear result.
Is this above a plane? Is this below a plane?
As a result, we slowly evolved into these, right?
So you can have multiple layers of many input nodes.
And because of GPUs, we evolved to this.
This is GoogleNet from 2006.
No, from 2016, I'm sorry.
2006, what happened to me?
All of these contain subnetworks, which contains the perceptrons from the previous slide.
This is a very, very big network.
The problem with these is that there is an underlying thing that fuels all of them.
It's data, right?
We have evolved to needing data everywhere.
So if you are building any kind of system which takes resources from the real world and makes a conclusion about something new, it is using some form of data.
And there's a market for data as a result, because everything that used to be modeled is now learnable and learnable with data that doesn't necessarily belong to the company that made it.
And if you're using social media.
You are also part of that market.
You are being used as an example in learning.
This is a photo from my house just before I left here.
I just got a new sofa last week, so I just removed the old sofa.
You don't have a sofa in this photo.
This is from a camera that is connected to the internet, and it helps me know if someone's at home.
But along with powering this camera, I signed a user, like a EULA, end user license agreement, that forces me to give up all of the data from that camera and help the company that makes the camera.
It can train its models on whatever it wants, so it can sell my data to other companies.
It won't release the raw data, but it can use it for training.
I'm not extremely happy with this.
But the reality is, if you buy a camera-attached headset, or if you buy a camera-attached car, which you are doing, they'll have the same kind of end-user license agreement.
Your data won't belong to you, and your data will be used to train newer models for the future of robotics.
There are multiple problems with it, but I think the biggest problem that everyone is ignoring is that no amount of data is enough.
When we look at data that's coming from users, we're looking at the normal behavior most of the time.
It's very hard to catch anomalies.
Anomalies don't appear in the data set.
Or when they do appear, they appear very sparsely.
So if you're trying to train a self-driving car, you need to train it for stuff like this.
That's not an image that you would typically see driving down the street, unless you live in New York like I do.
This is a very hard image for a neural network running inside a self-driving car to parse.
It has multiple lights, it doesn't know where the sources are.
This is a rare occurrence.
Like I would call this an anomaly.
But anomalies exist everywhere in the data, right?
You're not training for this, you're certainly not training for this, right?
So some data sets don't exist right now, but some data sets also can't exist.
It's going to be very hard or unethical to get the data sets in advance for machines to actually use when you're building a product that requires a mathematical model running behind the scenes.
So I have a solution.
This image isn't real.
It's part of our project.
It's part of our simulator for self-driving cars that's called DriveSim.
We've been using this for quite a bit at NVIDIA.
The premise of synthetic data seems reasonable.
There's no data in the real world, you just have to make it.
And there's two things to attack here.
One is generating enough imagery for ground truth to provide data for the networks to learn if you were in a scenario where you have a lot of red lights or if you're in a...
in a scenario which has complexity in the road.
But the other one is generating interesting scenarios.
Creating images like this one, pretty much at random, solves for one.
It solves for the ground truth.
It doesn't really solve for a scenario.
So we have to think about how to create scenarios for robots to learn how to behave.
It comes to things that are hard to capture in the real world.
This is one example of something we've been doing.
This is a bunch of robots together in a product called Isaac.
These are all simulated.
And they're learning how to play hockey.
Right?
So all of them just have a stick.
And all they know how to do is just move the stick.
And they also have a score function which tracks the puck and tells them how close they are to the goal.
After enough iterations, those robots learn that they get a better score if they move the stick such that it hits the puck and they move the stick such that it hits the puck And when they do, they get a super high score so that then they know how to repeat that again.
And they get all the observation from the real world and just use that to make that one specific action.
Now if you think that's strange, you need to get used to it because that's of how all of robotics works right now.
And that's the most intelligent robot will be for a while, unfortunately.
What are we actually doing here?
That hockey thing that I showed you, that's the environment.
Your environment as a robot playing hockey is a field.
There's a goal in there.
There's a puck somewhere.
You see all of that.
And you, as an agent, are looking at the environment.
You're making an observation.
After you've made an observation, time to make some action, right?
So you'd be moving the puck.
It doesn't happen just once, it happens at very, very high frequencies.
So you've been moving the puck a little bit, then you're observing the world again.
It's my... sorry, not the puck, the stick.
Is my stick too fast?
Is my stick too slow?
Until you've reached a good speed and you've reached a good direction, it's time to hit the puck.
And you've hit the puck and stopped.
If you got something good out of it, well, what happens actually?
There's something missing here, right?
Someone has to tell you that you've done something good.
So there's also an interpreter in the process, and if you've done something good, they'll give you a reward.
So this is the basic idea behind reinforcement learning.
And if you look at any kind of reinforcement learning paper, you'll see that there's always some system on top of this.
But this is the basic idea.
There's an agent in the world.
They make an action.
And they get a reward as a result.
Now, I want to ask you.
How many parts of these systems can we debug right now?
So if you would be a reinforcement learning expert.
What you'd be debugging is the interpreter.
You'd be looking at what the robot actually gets rewards back from, and you'd be tweaking that reward function, right?
Get closer to the goal. Cool.
Maybe getting closer to the goal isn't enough.
Maybe you need to hit it at high speeds.
So you tweak the function a little bit so that the stick that's actually moving You get a higher reward if you hit the goal and it's a high speed.
Right?
You could be, for example, saying, as long as you got close to the goal, that's already high score, so do that.
So there's a lot to tweak there to debug something.
And that goes into the agent.
But there's something really big that you're not debugging that way, and that's the environment.
So a lot of things happening with a robot when they're interacting with the actual world, you can't solve because you can't change the environment too much.
You don't have a debugger for that.
So we made a debugger for that.
This is the first example of a VR experience that I created.
This is for SIGGRAPH 2017.
What's happening here is you're playing dominoes with a fellow called Isaac.
This is Isaac.
He has dominoes.
I have dominoes.
I'm going to pick up a domino from my side, and I am going to place that piece of domino on the table in front of him.
So as I do this, I pick this up.
This is done with a.
with the UE4. It's already two and a half years old, so it's been a while.
So I'll place this piece of domino. It's got a four and a five.
And now I'm gonna hit the button.
In Isaac's neural networks, what's happening is that he needs to evaluate, they need to evaluate the type of domino, they need to evaluate what the legal move is, and then they need to act on it.
And in this case, Isaac connected a 5 and a 5, so it's a legal move. Great. And it was almost geometrically legal as well, so good on him.
Now, I put a 6 and a 6 together, so he has a few more options on his board.
So he's going to look at it again and perform the same action.
This time he's taking a 4 and connecting a 4 to a 4.
So what's going on here is that this is a very controlled environment in which we can check or we can intervene with the environment for the robot to play in.
And I want to talk a little bit about how we did that.
So the first thing to do.
It's a train-the-vision network for the robot.
We generated a lot of images of dominoes.
So this is also done in Unreal Engine.
We pretty much simply threw a bunch of dominoes in the space.
And we attached to each image, as we outputted them, the ground truth.
What is this domino?
And the neural network can then learn.
We also added a bunch of distractors so the neural network couldn't pick up on everything being a domino.
Now you'll notice that these images are not photorealistic.
And this is another very important point.
We don't know, we actually don't know, what neural networks consider important, right?
This is a research topic.
So we started from the very basic things, and we slowly improve upon them.
The next step after we trained the Vision Network was to just take a very, very simple agent and to play a lot of games with the robot.
We played about something like 50,000 games until the Robot's Reinforcement Learning Network learned how to make the right moves.
And then we debugged it in VR.
So at every step that we thought that the robot was good enough, we would play a few games in VR.
And now we wanted to do something else.
Actually, at any point in this process, did we assume that the robot was virtual?
We didn't.
Right?
So, if this can work on a virtual robot, it may as well work on a real robot that has the grippers and the right physical configuration.
Right?
Do you think it's going to work?
Yeah, it works.
So this is a robot that we actually deployed in SIGGRAPH 2017.
It literally is running the same networks.
When we first put it up, nothing worked.
And I was really upset.
And I then realized I put the camera 90 degrees off, so I rotated the camera, and suddenly everything worked.
Because the robot just had the same vision network, and it had the same gameplay network, so it could just play the game.
The only thing that was different about it was that, in this case, we were not using the same inverse kinematics as in the control system that powers the grippers.
But everything else was just the same.
So there is no difference when you're actually getting it right between a physical robot and a virtual robot.
And this is a very important takeaway here, right?
So here are the goals that we set up when we started this.
We have to use VR interactively with machine learning.
Most of the team that we had weren't roboticists.
At the time, I was not a roboticist.
We can't use end-to-end training.
We have to modularize the entire system.
And we have to do it really fast.
In fact, we had two weeks to do that.
So building the system was a challenge just from a time crunch perspective.
The way to solve it is we use the best component for each layer, right?
We started with just the image recognition system for dominoes.
Completely separately from that, after we assumed this works, We had separately trained a reinforcement learning network on the gameplay.
And completely separately from that, we were testing a physical control layer, or a virtual control layer for the robot.
So at the very end of the process, we were testing both VR and a real robot at the same time.
The reasoning for this is that we can debug the virtual robot really quick, and the physical robot is a little slower, otherwise it will kill you, pretty much.
So here are the tech stats on this.
So you want to know that the production itself was two and a half weeks.
We had separate teams doing each part of the process.
There was the vision, the policy.
Policy is how the gameplay is played.
The physical robot, the HRI, and the art.
I led the HRI and the art and the VR.
And we trained a vision network with 30,000 domino images with our own simulator called Isaac Data Studio.
The policy network used, I'm sorry, I thought it was 50,000.
It's 80,000 games to get the robot to the right position.
I tested it after about 50,000 games.
It wasn't good enough.
We also had to redo some of the components.
At the time, this is about two and a half years ago, at the time the training was quite, quite slow.
As in, like, it took 24 hours to play 80,000 games.
Which, when you're in a time crunch, actually matters.
So testing it early, virtually, really mattered to us.
And the components were never integrated until deployment.
The other thing that's really important to say here is that all the teams were separate.
I was in my home office in New York.
Some of the AI people were in Toronto, and some people were in Austin, and the physical robot was in LA, I think?
Not sure.
So we were all completely separate, and we could get it done.
Right?
I'm going to skip this.
So, the other takeaways here is that...
We built this in such a way that people know what to expect when they're testing the robot.
This is not a smart machine so far. One day we'll be at a point where this is a smart machine, but so far it's kind of like a five-year-old child learning how to play dominoes. So you have to be patient. So by putting this inside the story, people wouldn't make smart moves at first. We can actually debug using the simple cases.
If you're in VR, you get bored, right?
But if you're in VR thinking you're playing against a five-year-old child, you get bored less quickly because you're thinking, what's going on in their head, right? Is this a good move? Is that a bad move?
So that's really important to keep in mind when designing things like this.
The other thing that we realize is really important is that Players don't remember if you tell them, oh, just don't put one of those tiles sideways, because that's an advanced move.
But they will remember game mechanics really well.
So we've been playing games all of our lives, so we get to know what a game of dominoes looks like.
And we kind of listen to that, or we tune into that.
We don't really tune into instructions like, oh, no, no, just don't touch that.
But if you make something in the game that makes you not touch that, it really helps in the simulation.
Obviously, I talked about it.
When you modularized all components...
We could really speed up because we weren't dependent on each other at any point.
I could generate fake data in the first week and have everyone test with the fake data and then I could bring in real data and replace the fake data and it just worked.
And the other thing we realized as we were doing this, what we were actually building is a debugger.
So we need the ability to cache previous games, and we need the ability to switch between a later version of the framework to an earlier version and test which one is better.
Do A-B testing on that.
So, using this knowledge, you can actually go very far.
You can train very advanced networks to do things like self-driving cars.
These aren't real images. Again, what this is, is output from a neural network looking at synthetic data that we have inside DriveSim, our self-driving car simulator.
I want to talk about it a little bit, just so you know what we're actually building when we're building gameplay environments for robots.
In this case we have this car model that has a bunch of cameras. All these cameras need to inform the car on how to make decisions in an autonomous driving situation.
So cars would be driving around and they get some anomalies.
So we need to emulate all the sensors in the car and we need to recreate dangerous and rare scenarios to do that.
And so we can manually inject inside the environment.
We can inject fog, snow, rain.
We can make some arbitrary traffic congestions.
We can add really backlit scenes with fog.
So the sensors barely do anything, but they can still react properly because we trained them on many extreme cases.
And we get this and we also apply some distributed computing to this because we don't have to train one scenario.
We can train a bunch of scenarios at the same time and kill the ones that we don't like.
The ones that we don't like didn't perform well in a rainy street.
Okay, redo that. We'll use another.
And this approach should work, right?
Do you think it works?
Yeah, it works.
So this is a test that we have from about a year ago.
Yeah, it's about a year ago.
We did a test with one of our autonomous vehicles.
This one is in.
In the South Bay, we're running a track of like 80 miles.
And this is real data that you see here.
So what's actually happening is the car's driving around, and it's picking up on all the road marks, and so on.
In order to validate this information, what we're doing is we're recreating the same track virtually.
So you see on the left here, you see DriveSim.
DriveSim is our self-driving car simulator.
And it's using all the information.
to recreate that ride and validate it to see that we can inject more dangerous scenarios.
In that case, maybe the car wouldn't make it. So we want to validate all these, we add this layer to it, right? We're just changing the environment and we're not telling a real environment from a virtual environment as far as the evaluation network is concerned, because they're not different.
So the layers that we have here are simulation on one hand, emulation of sensors on the other hand, And at the bottom we also have imitation. We're adding more data.
For the machines to be able to use without exactly understanding what a human is doing or what other players in the driving simulator are doing, but still be able to do something with it.
Right?
Like if you don't know why a machine has done a certain thing, but you're still able to navigate around that, well, mission accomplished pretty much the same way.
This is one example of what I mean.
So this is a...
This is a demo by a colleague of mine named Madeline Gannon.
She built this playful robot.
Now, this is an assembly robot from ABB Robotics.
This thing can definitely kill you.
It's very big, and it's very powerful.
And if you get too close to it, just one wrong move can break a few bones.
But in this scenario, she made it look cute and inquisitive, mimicking humans in a way.
This field has grown quite a bit in recent years.
This is a work by Daniel Holden from about a year and a half ago, or two years ago.
He made a neural network that combines a bunch of different motion-captured humans to recreate motion.
Over here, what you see is that this character is getting a bunch of terrains that it hasn't seen before and still reacting properly.
Because the neural network is mixing all of the different terrains that were available in training time, it's just doing it intelligently.
So the thing looks like it's acting, reacting pretty normally.
The same university also released about a year ago, pretty much the same type of network with some modifications, but done on a wolf, right?
There is some improvement there. Also you see that the motion is very, very natural.
Please don't ask me how they motion captured a wolf.
We have been doing the same sort of things only with reinforcement learning.
So over here you have Isaac Gym. It's our own training system just for reinforcement learning where we're training virtual characters to just get up and tap each other.
There's definitely a video game in here somewhere.
There's no mocap here, just physics.
We can do this with imitation.
One character here, the physics simulation that you see on the left, it has...
All the characteristics of a human body.
So it has the biomechanical model of a human body, and it's observing another motion-captured clip, and it's trying to imitate it.
So what we're doing is we're providing real-world data, real human data, into the process, and then another machine is just imitating that, and it's getting a good score for doing it right, right?
It's not actually using the motion-captured data.
It's learning to activate its own actuators and motors.
To move like the motion-captured human.
And it's getting a score on the end result, not on the joints.
Right? Is that clear?
Cool.
And we can make masked games with it.
So in this case we motion-captured a bunch of different positions.
And we also inserted some synthetic data into it.
In this case they're trying to do backflips.
Over here they're trying to do some masked games.
You see how this evolves into the training of a character in a video game.
Next time you kill a monster in Doom, I want you to feel a little guilty.
It's done so much for you.
OK, so now we can create imitation-based data sets.
And we can actually do useful things with them.
So this is one other VR experiences I created in which you're using the same robot that you had before, but now you're making pretzels.
So in this case, the user gets the chef's hat, and they get to transform themselves into the robot.
and control its actuators, control its grippers.
And they'd be folding a pretzel.
So they're grabbing a pretzel from both ends.
This is sticky dough.
It's also soft, so you have to shake it off your hands.
You'll notice here something strange.
The robot doesn't have the same joint structure as we do.
So we need to think...
about what it looks like when we make a motion.
And sometimes we're wrong, and that's when you see this thing turning red, like this virtual gripper turning red.
There's the real world, which is that virtual gripper, and then there's the virtual world, which is where you see the robot.
That was a legal move.
So this simulator is only made to make legal moves inside the VR experience.
And whenever my hand is green, I can do this.
Now, I ring the bell when I'm done, and now you have...
The robot imitating my exact motion.
So after I've recorded it, the robot is going to try again, and they're going to fold the dough, which is fully simulated here, the same way that I had just done.
So this is training a virtual robot to do a completely manual human task.
And now that it's done, I have my little helper robot do a salt bae move here.
This is a year and a half ago. I was very proud of it at the time.
Now it's going to bake the pretzel. Everyone's happy.
I think this is a very significant process in, it's going to be part of the future of labor in many ways.
We're going to train robots to do this small action that we are so good at doing, but hate doing.
Right?
It's going to happen over and over again.
So I want you to notice a few things that we found along the process that might be interesting.
So one is...
It is totally non-trivial the way you choose to control the robot.
We controlled the robot from first person because we thought it was interesting enough to make it such that you would learn how the robot can move from the perspective of a human.
Some moves are legal.
But the other options that we tried are grabbing things by the object.
So instead of grabbing the actual gripper, I could grab some virtual point.
So, we had to make sure that we had the robot stand away from the gripper, where the actual object, the end point, should be, and have the robot attempt to grip it.
That had limitations. We also tried standing at a distance from the robot so we can see what's going on and complete the task from our perspective.
Because there's nothing actually limiting the robot from doing the entire task upside down, mirrored.
We just happened to be piloting the robot from first person.
We could be piloting the robot from third person, and it would be completing the same task from our perspective.
So there's no difference, really.
We tried that, too.
At this point, you see this kind of preview where there's a bit of a rope between you and the robot doing that.
We also tried following the actual gripper with a controller.
That turned out to be weird.
It definitely has some advantages, but you still run into the same problems of legal moves as you had before from the first person perspective, only it's harder to track.
And we also tried doing that at a distance.
Right?
I think the main conclusion out of that is just don't make controls that are special for the game because you'll end up making them anyway.
And as a result of thinking that way, we ended up making the actual controls that we needed.
So we had visible motion bounds for anywhere that the robot couldn't reach.
We also had the legal state indicator to see when the inverse kinematic system that solves for where the robot should be actually doesn't hit a singularity or doesn't hit a motion bound.
We also placed all of our scenarios inside the environment so people can get used to doing the action inside where they should be.
But that wasn't enough, because actually folding a pretzel itself, that task, requires some training.
And so when people started doing it, we thought this would be enough to train them, by just showing them a how-to somewhere in the game.
But it wasn't enough, so when we actually set this up, we had a rope on the table, so people could train on the real material.
And this really matters because what you're actually doing when you're inside a simulation is you're trying to use all your proprioception, your natural senses of where the world might be, and you end up relying on some defaults that suck.
So by having good intuition to what you're doing, this is helpful.
The other thing that's super important when building a robotic simulation is to make sure you understand the difference between you and the robot.
In this case, we had a robot that couldn't cross its arms, right?
So this motion was illegal. Whenever I would fold a pretzel like this, the simulation would fail, right?
And the arm would actually stop somewhere here because there was a motion bound.
However, the robot is more robust than us in other cases.
For example, there's no limitation that the robot could fold the pretzel when it's facing outside.
So make sure you understand what the limitations of the robot are, and make sure that you understand how fast the system can evaluate alternative cases.
In this case, the reinforcement learning system can search in a very limited space very quickly, but as soon as you move outside that space, it's actually not intuitive.
So doing things like going above a hand.
This is not something that a robot can intuitively do. You have to make it very, very specifically perform the one task.
Yeah, and you have to validate your assumptions.
Other takeaways that we had from that...
Users will default to illegal states.
So you have to tell them that a state is illegal.
We used the red grippers to do that.
We also used motion bound to do that.
I saw some people do something like, ah, I don't want to do this.
And then a hand would be stuck over here because the robot couldn't solve moving the hand from here down back to where it should be.
The scale and the torque of the grippers and also the mass affect the user behavior.
So you really have to have visualization and feedback in place.
We never got it right the first time whenever we tried it, so you have to make tutorial levels like we did.
And the last thing, super important, that we didn't implement and I really want to implement, is making the last action that you did easy to undo.
Because you are performing an action on a robot that's affecting the environment.
As far as you care, it's real.
Okay, so last slide, I swear.
When you're building human in the loop systems, here's what I want you to think about.
Make the system modular.
The more you can replace stuff in it, the quicker you can move.
You have to state clear goals, because any system that you're making that's human in the loop has people from different disciplines.
If they know what you're trying to solve, they will work around what you can't solve.
So always communicate well between people in the team.
Avoid false empathy.
I just covered that, but it's really important to remember.
And remember that you're building a debugger, right?
You have to expose parameters, you have to make agents interchangeable, and you have to make states rewindable.
And remember that you're building a debugger and you're also building a game.
So you have to exploit all the game mechanics that you can, you have to move as fast as possible.
Thank you.
APPLAUSE Questions?
All right.
Oh, question.
Yes.
I'm sorry.
Hi.
How do you come up with all of those test scenarios?
So basically it's a test scenario that you're pushing through the whole process, right?
So I don't...
I don't necessarily think that I can call every single of these situations a full test scenario.
We were trying to validate if a robot can complete a certain task.
So we're like, what is good for a human to perform that a robot can copy?
What is a simple task in the case of Dominoes? What is a simple task that we can play with?
Is simple for a human to understand?
And uses the limitations of a gripper that the robot has.
We pretty much brainstormed until we had something that works.
There's no guidelines except for user creativity, I guess.
I'm sorry if there was supposed to be a higher insight there.
No, just user creativity.
Hi, I just have a question. When you were showing the tapping Isaacs and they were imitating the motion capture files, you said it's important to note that their score was not based on joints but their overall performance. I'm just wondering how you scored that?
So there are many variations on the reward function, but the end result is overall pose.
So you can think about overall pose in many ways.
One result would be to project from the perspective of the camera and get a least squares approximation.
Another one would be to test the end effector positions, but not the actual joint hierarchy.
You might have a small offset in the root joint, but still down along the hierarchy there are some corrections.
If you were to judge joint by joint, you'd create an overfitting in the system that is going to be hard to overcome in later steps.
So there's a preference in some composite score that's undefined, I'm aware, but some composite score that would be more helpful for letting the machine be more creative later on, I guess?
That makes good sense. I've done the same thing in dance.
Um, okay.
So I do a lot of simulation work as well, and one thing I find is with the old guard, if you want to call them that, the physical engineers, there's a little bit of skepticism and pushing back.
I'm curious if you've had to deal with that and how you've dealt with that.
I think the proof is that we're getting results a lot quicker.
I have no pretense, especially when it's like 20 or 30 year experienced roboticists, to claim that I am in any way improving their techniques, improving upon their techniques.
I'm producing a new perspective that they couldn't do up until now because tech wasn't ready.
VR just lets you do that quicker, and you know, like game engines.
And simulation frameworks allow you to move very fast.
And they never could.
So if you present it that way, it's a nice carrot.
And suddenly, your skills become complementary.
And they begin inquiring very quickly.
Oh, and how do you do this?
And can we also add some stickiness to it?
And so on.
So I haven't found anyone who hates me for doing this so far.
I just had a quick point of clarification. In the Dominoes example, you said there was 80,000 games.
There were 80,000 games played in succession to train the reinforcement learning.
But not all in VR.
No, no, no, no, no.
So we had a scripted agent make legal moves, and we had a reward function reward legal moves from Isaac's side.
But the scripted network was dumb. All it could do was play legal moves.
When it couldn't, the assimilation would just stop.
So very early on, you'd get punished for making...
I'm sorry, you got cut off.
So it's a little bit, I guess, unclear to me in that example, what the what was the VR adding that you couldn't have done with your sure standard scripted methods?
Yeah, so the scripted method only examines the scripted method only examines What is a legal move to make in front of the robot?
But what you're actually trying to teach the robot to do is to play a game of dominoes like humans would play them, right?
So I could be placing the domino not exactly in the right place, and I still want a legal move out of it.
And I could be placing the domino sideways, and the robot would then know how to do something like...
That was one interaction that we had.
Say that you don't have a legal move.
The other thing that we're trying to do is to make the robot not make extreme motions outside their motion range.
So all these things have to be trained in a scenario that breaks the rules.
And so the last thing is then, so how many VR games were part of that training set for the domino example?
So you put the number of scripted games, and I guess that's what I'm trying to tease out here.
Altogether, probably low hundreds.
Okay.
Maybe like 100, 150.
Okay.
Yeah.
Okay.
Thank you.
