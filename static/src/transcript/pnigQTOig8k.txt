Welcome to real-time rendering for feature film.
We'll be presenting a case study of some work we at the Lucasfilm Advanced Development Group did last year in support of Rogue One, a Star Wars story.
Today's session is going to be shared between the three of us, three speakers.
John first is going to give an overview of the film itself and how this real-time work came to be involved.
Then I will get into a breakdown of how ADG contributed to Rogue One.
I'll speak to the details of the digital assets involved and the workflow for lighting the shots.
Then I will hand over to Natty to discuss the technical details of rendering this content.
We will also touch on some ongoing work in this area happening at ADG and at ILMxLAB.
Then we'll close with some time for, oh, I haven't been doing my bullets.
We'll close with some time for Q&A, but first I'd like to give some introductions.
Since 1986, John has been at Industrial Light and Magic, where he is now chief creative officer and also a senior visual effects supervisor.
As John is an avid computer graphics enthusiast and hobbyist himself, his shows have always been known to push the state of the art in production CG.
John won an Academy Award.
in 2007 for the visual effects work that he supervised in Pirates of the Caribbean, Dead Man's Chest.
John is not only the visual effects supervisor for Rogue One, but he is also the creative mind behind the original idea for the film.
Oh, and by the way, in 1987, John and his brother Thomas created something that you might be familiar with, Photoshop.
John Mueller, everybody.
Natty Hoffman is principal engineer and architect for rendering at the Lucasfilm Advanced Development Group.
Natty has a long and storied career on the cutting edge of real-time computer graphics, from Westwood, Naughty Dog, Sony, Activision, 2K, and now Lucasfilm.
Since around 2004, Natty has been spearheading the transition to practical.
implementations of physically-based shading models across the games industry, which I'm sure many here are familiar with.
Natty joined the Advanced Development Group at Lucasfilm one year ago in March of 2016.
Uh... Natty, come on up.
Uh...
Me, I'm Roger Kordes, Digital Production Supervisor for the Advanced Development Group.
I've been with Lucasfilm since 2010.
I'm one of the founding members of the ADG.
I was the lighting and look dev lead for Star Wars 1313.
I am a real-time lighting and look dev specialist with a passion for generating high-fidelity images.
One of my dreams is to match ILM quality in real-time.
So this work that we're going to be talking about today was an absolute slam dunk for me.
We should also introduce a couple of other concepts here.
The concept of ILMxLAB.
and of the Advanced Development Group.
So in 2016, Kathy Kennedy firmly planted a Lucasfilm banner in the world of immersive entertainment with the launch of ILMxLAB.
XLAB brings together the creative forces of the Lucasfilm Story Group, of Skywalker Sound, Industrial Light & Magic, and of the Lucasfilm Advanced Development Group with a mission to create new, premium immersive entertainment experiences that go beyond traditional film.
and other linear media.
As for the Advanced Development Group, we were founded in 2013 to be an innovation center within Lucasfilm focused on real-time rendering.
Up next we'll show a few samples of the kinds of imagery that ADG Technology has helped to create.
At its core, Lucasfilm is about story, so with that in mind, this first piece is a short cinematic that the Advanced Development Group put together in mid-2014 to show what we thought real-time rendering technology can bring to the creative world of storytelling.
Report. According to our informants, a pair of rebel droids have attempted to make contact with the local underworld. They describe the droids as a golden protocol unit and a blue and white astromech. They may be attempting a rendezvous here at this outpost. Our ground forces have just engaged a transport that blasted its way out of Mos Eisley. Proceed but secure the perimeter first. Those droids must not escape.
Wait for me!
Hello, Captain Gator, Captain, um, Arto and I are ready for extraction and heading to the rendezvous point.
I repeat, we are ready for extraction.
Immediately!
I've got Imperials all over me, but I'll do what I can. I've called in some backup.
That doesn't sound very reassuring.
They're everywhere, Arthur! Arthur! Oh, Arthur, leave me! I mean, Arthur, leave me!
I'll save your soul!
Isn't this how it all ends?
Thanks, Earth-maker! We're saved!
Stop right there!
Oh no.
Then, oh, thank you.
So then last year, the Advanced Development Group developed Trials on Tatooine, a VR experience with which Lucasfilm and Industrial Light & Magic launched the ILMxLAB brand.
And here we have the launch trailer.
Then, this one, this is a high-fidelity rendering test that ADG did, which kind of leads us into what we'll be talking about today.
So with those introductions out of the way, I'd like to turn the presentation over to John Knoll to talk to us about this specific real-time project and what it achieved for Rogue One, a Star Wars story.
So my challenge to the advanced development group was to take this development and use it to create some finished work on a feature film The specific challenge was to take an ILM production asset, you know without completely rebuilding it Specifically for a game engine and rendering it using the real-time render at a quality level that I can put in the film Success story here is that the ADG shots are mixed right in with our render man renders and we have a short reel of k2so shots and it's a mixture of RenderMan renders and ADG renders.
So, can you spot the difference?
Our optimal route to the data vault places only 89 stormtroopers in our path.
We will make it no more than 33% of the way before we are killed.
We could transmit the plans to the rebel fleet.
We'd have to get a signal out to tell them it's coming.
It's the size of the data files, that's the problem.
They'll never get through.
Alright, so we'll play the reel again, but with ADG shots clearly labeled.
Okay.
Our optimal route to the data vault places only 89 Stormtroopers in our path.
We will make it no more than 33% of the way before we are killed.
We could transmit the plans to the rebel fleet.
We'd have to get a signal up to tell them it's coming.
It's the size of the data files.
That's the problem.
They'll never get through.
All right.
So Roger and Natty will present a technical breakdown on how this is all done.
But...
An important question is why.
Why is real-time rendering important for Rogue One?
And the answer is that in visual effects, the ability to iterate is crucial.
Instant feedback and fast rendering mean more iterations, more creatively useful iterations per day.
And a shorter iteration cycle gives us higher quality in fewer hours, and that's why we're chasing this.
So about two years ago, and this is back during the production of Force Awakens, I'd seen some early real-time render tests out of the advanced development group that seemed extremely promising.
In early 2015, ADG generated a version of the X-Wing shot that was in the original Thanksgiving trailer for Force Awakens.
And ADG had rendered the X-Wing elements in seconds per frame, whereas the ILM software renders for the actual teaser were multiple hours per frame.
X-Wings were then put through the exact same compositing step and the results were pretty good.
The true versions don't look exactly the same.
They're not identical.
And there are a number of cheats in the ADG shot.
For example, the aerial lights are crude approximations and the occlusion calculation is a little bit different.
But generally, when I saw this, I felt like, all right, well, this is something we can work with.
Then last year ADG hired Nettie Hoffman and trusted the future of Lucasfilm's real-time rendering to him and that gave me even further confidence that this is a direction to pursue.
So we've established ILMxLAB and the ADG as the place within the company where ILM CG artists sit right alongside game industry talent to build these exciting new things.
That's exactly what I needed for this project.
So the challenge to ADG was to take this idea of rendering production quality images out of experiment and testing.
I thought what I'd seen was promising enough that, all right, well can we, this looks like we've hit a bar that this could go into a feature film.
Let's take it out of theory and into practice.
Let's do a shot.
So the, I was pretty upfront that we had to hit a level of quality that was as good as the RenderMan renders.
If it didn't hit that bar, then we couldn't use it.
But I was pretty happy with how that turned out.
I think it succeeded pretty well.
So talking to ADG about what asset we could try this with.
You know, they were looking, their ideal case was a hard surface asset, opaque, pretty much like the X-Wings, and a perfect candidate for that was our K2SO character.
He's going to be a hard surface asset, he's going to be CG all the way through, and some of the more difficult shading challenges aren't actually present in the character, so it would be a good test case to do.
And with the unified asset standard having been designed and come into fruition between ILM and ADG during the production of Force Awakens, it seemed like everything was in place to give them a crack at this.
And so the first step in this process was to mirror a shot that was being done through the traditional pipeline.
So we had a shot that was being lit by an ILM lighting TD and render man.
That kind of set the visual bar, the look.
If we could hit this, then we could do another shot essentially without a safety net, without a backup plan.
So we proceeded in parallel with the shot.
And once we got to a place where I felt like we could final that element, then they proceeded with doing these shots with no backup plan.
I'll also say, besides for final shot production, there's an important use of real-time rendering just working with actors.
So in pre-production, we had this very sort of crude early version of K2SO that ran in real time on our motion capture stage.
We had Alan come in for a day, and since the character's very tall, he was standing on stilts.
And so he had had a little time to get used to the stilts, and then this was a chance for him to.
get on the stage and sort of puppeteer the character.
And he can watch himself in real time while he's seeing what feels right on the character.
You know, is it, do you do more or less arm swinging?
You know, how loose is your body?
What feels right with the character?
And while this is, you know, a very crude GL, because this was done, you know, way in pre-production before we had the final K2SO asset built, The same asset that we used for the film, we could have flipped the equation.
You know, the constraint for the film was we had to have very high visual fidelity, can't have any noise, you have to have really smooth motion blur, can't be any visible artifacts, but I don't need it to run at 60 frames a second, so I can afford longer render times.
But for this purpose, we could just flip that and impose it's gotta run at speed, and we could live with a little bit of noise in the renders.
So we could.
And in future, using some of this new ADG engine, I think it'll make this experience even better.
And with that, I think I'll turn it back over to Roger.
So what does it actually mean to work on a shot for an ILM production?
Well, it means that we will consume assets from the ILM pipeline.
It means that we will artistically light the shot.
And then we will output renders that are consumable by the compositors.
So let's break that down a little further.
What do we mean by asset?
That's kind of a loaded term too.
There are many, many, many types of assets.
And each type of asset is often represented by many, many, many individual files.
So, for this Rogue One work specifically, the assets and file types that we're dealing with are animated geometry caches, paint and look dev data, cameras, background plates, virtual set geometry, and captured HDRI lighting spheres from set.
As an ILM CG asset, K2SO himself comes to us in two primary components, the geometry stored in an Alembic container and the paint and lookdev data represented in a MaterialX description.
Alembic is an extremely powerful and extensible open standard for transporting geometry between software packages, born out of a partnership between Industrial Light and Magic and Sony Imageworks.
If you're curious about Alembic, you can learn more at alembic.io.
MaterialX is a new open standard that has come out of the Advanced Development Group and Industrial Light & Magic, which allows material and texture information to be transported between different software packages and renderers.
MaterialX is a key component of our Unified Assets standards.
It allows us to transfer unified shading descriptions.
between all of our packages, between paint packages like Mari, lookdev packages like Katana, other DCCs such as Maya or our own internal tool Xeno, and even other platforms entirely like game engines.
If you're interested in MaterialX, please check out materialx.org.
Adding support for MaterialX and Alembic.
into the ADG version of the Unreal Engine was one of the major technical undertakings of this Rogue One project, which Natty will go into more detail there, but the key takeaway here is that because these standards are both so easily extendable, we were able to take assumptions that are baked into ILM's production implementations and reformat the asset data to be suitable for a real-time case like Unreal.
So by the time ADG got our first K2SO turnover, the asset was basically complete at ILM.
We had gone from these paintings to Landis had made this digital asset.
Here we are looking at the control cage for a creased subdivision surface.
There are 600,000 some odd vertexes in the control cage alone, with edge creases that go up to level four.
So that means that for an extreme close-up on one of those creased edges, if you were to uniformly subdivide the entire cage, you'll end up with 600,000 times 4 to the 4th, or somewhere in the neighborhood of 154 million vertexes required in order to properly hold those creased edges.
Fortunately, none of the shots that we were dealing with required this extreme case. We also have 1,700 separate geometries in the animation hierarchy.
and 63 UDIMs across 10 texture effects, with most of those textures stored at 4K resolution, all driving LookDev in the unified shading model shared between Lucasfilm and ILM, which we call Unified Surf.
So each and every component of K2SO, every little piston and ring fitting, each of them can and does animate.
This is the beauty for us of the Olympic cache.
We do not have to be able to evaluate that animation control rig at runtime.
When we are rendering in the game engine, we just consume the transforms from a baked geometry cache.
Here is another view where you can see in his shoulders all the little pistons, how they move from his arms moving, and his knees and wrists and elbows, every little bit inside.
has moving parts.
As I mentioned a moment ago K2's MaterialX LookDev data has textures for driving tan effects in the unified shader.
That includes texture data for a second specular lobe.
And actually a third.
But at the time of this Rogue One work ADG's real-time implementation of Unified Surf only supported a single specular lobe.
So for the shots that ADG contributed to the film, the lighting and look dev TD, Justin Schubert, had to dial in a custom specular look 4K2 that mixes the spec one and spec two effects.
John wasn't exactly thrilled about this, but the results looked good enough, and so we powered through.
Here we see a comparison between the two lobe RenderMan renders on the left.
and the results of Justin's efforts on the ADG render on the right.
I mean, obviously, it's not the same shot and not the same lighting setup, so we get away with it, right?
So now that we have the K2 asset, the CG asset for K2 himself, we've brought in the Alembic of the camera.
We have the EXR textures of the background plates and of the lighting spheres imported into Unreal as HDRI lighting environments.
Now we can get to work actually lighting the shot.
So here we have some screen capture footage of a contrived version of an interactive lighting session, because we didn't actually capture any of this while we were doing the work.
So you can see what the interactive lighting session would look like.
I should mention here that for games industry veterans such as yourselves, real-time lighting is something that we kind of take for granted, right?
But real-time light and shadow feedback on a full resolution, high fidelity Industrial Light and Magic asset with all 63 UDEMs of 4K textures and however many millions of vertexes slammed into the GPU, seeing this kind of real-time light and shadow feedback was revelatory for ILM lighting artists.
I should also mention that at Industrial Light and Magic, you know, the concept of CG lighting, that means area lighting.
And areolites means textured areolites as in a high dynamic range photograph of a Taken on location of a actual physical light source that was illuminating actors and props and sets At ADG we have some approximated areoliting shaders for surface shading We have basic rectangles and disks and also the standard unreal spheres and pills But we don't have textured areolites One important addition also is that area lights at ILM means area shadows, and that one is an even tougher pill to swallow.
Once again, Justin was able to hack his way to the visual result that satisfied production for Rogue One by dialing in a tunable filter radius on standard PCF shadow maps.
At this point, I'll pass it off to Natty to dive into some technical details of rendering.
Thanks, Roger.
So Unreal Engine now has some support for loading Alembic files, but we needed an implementation that could handle ILM Alembic files.
As Roger touched on earlier, Alembic is a very flexible and extensible format, and ILM took full advantage of this extensibility and has extended it quite far.
One of the ADG engineers, Ron Radetzky, did significant work to enable our pipeline to take in this highly extended Alembic format and turn it into vertex buffers that could be directly uploaded and consumed by the GPU.
After animation happens in the ILM pipeline, a bake take is cached out in an Olympic file.
This is a vertex cache of that final animated result.
The bake take is a pure geometry cache.
It's baked every frame.
It has no information on the rig or deformers or anything like that.
It's all baked out for that one frame.
K2SO is mostly rigid.
And so most of that cache is rigid transforms.
There's a small amount of bendy bits, basically, as antenna.
They are very small compared to the rest of them.
And in these particular shots, I'm not even sure they animated at all.
So you could basically think of it as a very complex rigid setup with a lot of rigid pieces.
ILM geometry caches contain subdivision surfaces as well as traditional polygon meshes.
Saron's implementation had to deal with both.
For the subdivision surfaces, at the time of the Rogue One work, we determined that we only needed to render at subdivision level 2 in order to hit the visual quality target.
We did some tests at level 3, but for these specific shots and cameras, level 2 was quite sufficient.
This allowed us to pre-subdivide the entire mesh in main memory.
And this was the most straightforward.
It wasn't the most elegant or the most performant way, but it was definitely the most straightforward way to shim this data into Unreal, given the film schedule that we had to work with.
Now, all of this subdivision surface rendering is built on a foundation of the open standard from Pixar, OpenSubdiv.
For Rogue One, we did subdivision on the CPU.
OpenSubdiv does have a GPU implementation and we took a look at it, but we found that it has some problem cases with UV texture coordinate data.
and it doesn't work well in ILM assets like K2SO.
In the Epic keynote this morning, Epic announced open subdiv support in Unreal Engine.
This implementation, of course, didn't exist when this work was ongoing, but we look forward to evaluating it for possible use in future projects.
Rendering K2SO uniformly subdivided level two was quite stressful for even our most powerful machines.
As Roger pointed out earlier, every subdivision level is multiplying the polygon count by four.
So Ron, who was working on the Alembic and subdivision part of the engine, thought there must be a smarter way to go about this.
So in parallel to the Rogue One work, Ron developed a streaming Alembic solution that did not require subdividing everything up in memory ahead of time.
And he also developed a curvature-based recursive reduction algorithm for removing redundant spans on subdivided surfaces.
As you can see in the slide, the end result is quite a bit simpler.
Using the redundant span-loop reduction method, Ron is able to render a version of K2 that is visually indistinguishable from the full level 3 tessellation, all while maintaining a constant 60 Hz frame rate.
While we did not get to take advantage of this on the Rogue One work, we continue to enjoy this advancement for our current projects.
In addition to subdivision surfaces, another facet of our Alembic geometry cache consumption is handling the motion blur time samples, which is a specific aspect of ILM's flavor of Alembic.
ILM's geometry caches include motion blur samples at three times for each frame, when the camera shutter opens, when the camera shutter closes, and the time in the middle.
This includes full data for all the geometry, any cameras that might be moving, for each of these three points in time for each frame.
Ron's implementation enables us to do temporal supersampling by interpolating the camera and geometry positions in between those three subframe samples.
For the Rogue One shots, we rendered out images at 64 different times spread over each frame.
This is, of course, computationally quite heavy, but we needed to do this if we wanted to get motion blur that was up to ILM standards.
We get some additional use out of these renders by also jittering the camera at the same time using a Halton sequence.
This also gets a spatial super sampling at the same cost.
We have some ideas for applying these samples in another dimension, for example, aerial light sources as well in the future.
Before we talk about rendering, we should talk about compositing, which is the process where the final image is actually put together by a specialized CG artist, a digital compositor.
It bears some resemblances to the post-effects passes that we get in game renders.
Various blurs, blings, blooms, flares, all these effects happen in comp, or in the composition phase, composite phase.
And this would be no different for our shots, just like every other shot in the film.
So we needed to be able to get our ADG renders output and hand it over in a way that is seamless and transparent to the compositor.
At ILM, TD renders are handed off to the compositor in open EXR format.
While Unreal does come off the shelf with EXR output capability, in that feature, the image is written before certain post-process effects happen, and those include, unfortunately, some lighting features.
And we needed to output the full lid and shaded beauty render in an EXR container.
That was a fairly trivial change to make in our version of Unreal Engine.
So we render motion blur, but the depth-based defocus blur occurs in the comp phase.
And this is true also for the ILM renders.
We knew that we needed to have an extremely robust motion blur result in order to pass muster at John.
We also needed to be able to output depth so that the compositor could have control over the depth of field in the final output.
The concept of arbitrary output variables is a staple of offline rendering and compositing at ILM and at other feature film houses.
These AOVs are output by the renderer, and they are used during the compositing phase.
On the surface, these images are somewhat similar to G-buffers that you might have in a deferred shading engine.
One key difference between G-buffers and the AOVs, the A stands for and really does mean arbitrary.
These values can be stored from any point in the shading computation.
It can be even some intermediate value that is only present during a particular sub-phase of the shading calculation.
And to generate that kind of arbitrary output from any point of the shading math in a game engine render would require a lot of plumbing.
So we identified a minimum set of AOVs that the compositor would need for our specific shots.
Depth is one example of an arbitrary output variable that we were easily able to accommodate.
Ideally, this exercise would result in less compositing work than traditional ILM shot.
since something closer to the final result would be visible to the lighting artist interactively.
So we were able to get away with three AOVs for most of our shots.
The beauty RGBA, which is basically the final rendered color.
We have the depth, which I mentioned earlier.
And an object ID buffer.
Some shots or AOV.
A few shots required a fourth AOV, which was an emissive matte.
Rogue One was an ACEScg show at ILM.
At the time that this work was done with an ADG, our version of Unreal was still limited to the sRGB or REC709 color gamut.
This was another area where ADG and ILM had some creative back and forth in order to ensure that the REC709 renders out of ADG fit correctly into the Rogue One ACEScg color pipeline.
So Now that we are able to ingest the assets and do creative work interactively, and we have some baseline capability in place for all of these various technical features, we're ready to render.
But what does rendering for feature film visual effects actually mean?
To quote ILM visual effects supervisor Ben Snow, in computer graphics, a lot of the time we're trying to reproduce the reality that the viewer sees with their own eyes in the world around them, but with visual effects or film, we're really trying to reproduce filmed reality, which is a little bit different.
So what does it mean to make things look like a photograph or like a frame from a film?
Well, arguably one of the most significant components is to have absolutely no visible aliasing.
The ILM render resolution for Rogue One was 4K, but for us to be able to kill all aliasing in our ADG renders, we had to use a combination of the jittered supersampling that I described earlier and uniform supersampling, effectively rendering the frame at a higher resolution.
So for our final output for compositing into the movie, we ended up rendering at 9K in addition to the 64 jittered subpixel samples I mentioned.
Here we see a portion of our beauty pass for this frame at a one to one pixel scale.
Even with all of those samples, our shots were rendering at just about exactly one minute per frame.
that render time is due to several factors.
First of all, it was 9K resolution, and it was rendering effectively 64 times or 64 subsamples per pixel.
And also, a significant portion of the frame time was due to massive file IOM and memory transfers, since we're talking about extremely large output files, and we're talking also about extremely large input files in the case of Alembic.
This shot is one of the first tests that we generated in order to show John what the results of a real-time render could look like.
These renders were pretty far from real time, of course, at about a minute per frame.
But compared to multiple hours per frame, which the RenderMan renders were taking, that's still quite a win.
This is an example of a shot that was also done at ILM.
So our work in this case was to match the ILM result.
The ILM lighting setup used only three light sources.
There was a rectangular area light for the ceiling panel, another rectangle for the back wall panel, and an overall HDRI sphere.
Roger, who lit this shot on the ADG side...
managed to stay true to that using our approximated rectangular area lights and fake soft shadows.
Screen space ambideclusion was our only shadowing factor for the HDRI sphere.
K2's eyes are a complex set of refractive lenses.
We didn't support the unified surf refraction model for transparent rendering in our real time version of the unified shader, so the eyes proved a bit problematic for us.
We ended up offering a custom material and we did a custom pass and and ended up rendering it separately and we got results that were close enough.
Lighting TD Justin Schubert took over lighting duties on the ADG side starting with this shot, an outdoor shot with K2 in the mid-background.
We expected that with a single CG sun and an HDRI sphere and with K2's eyes not dominating the frame, this would be a solid candidate for the ADG test.
There you can see clearly both the rendered motion blur as well as the defocus blur that was done in comp.
The focus pull using an exact emulation of Rogue One's complex anamorphic lens model is done entirely in COMP using the depth mat, AOV output, along with ADG renders.
These results were very promising and got a good response from John.
We used CG geometry for the physical set to calculate occlusion and shadowing on K2 as he walked through the door.
Additional darkening of K2 as he walked further inside was done in COMP as a final bit of sweetening.
This was the last test shot that we did before moving on to actual production work.
This shot was working so well, we actually stopped work on it, on the ADG version of it, before we had the chance to incorporate the final clean background plate paintwork.
This shot presented two primary lighting challenges.
First of all, on the ILM side, the canvas canopies above and around K2 were presented as complex textured area lights.
ILM's lighting process extracts textured cards from the on-set captured HDRI sphere using an internal tool within Xeno called Lightcraft.
Also, the choreographed animated effects lighting from the explosion needed to be addressed.
Our approach to the textured area lights was to take the HDR images from the ILM Lightcraft solve, those textures can be seen here at the top of the screen, and place those textures on cards into the real scene for reference.
Then, Justin built a lighting rig out of non-textured approximated area lights to match the layout, shape, color.
and intensity of the hotspots that we should be getting from the original lighting textures.
For the FX lighting animation, we didn't have matinee support for IDGLMBIC animation system and Sequencer did not yet exist.
So we had to choreograph all of this together via Unreal Blueprint scripts and timelines which would be evaluated during the render process.
Once again, for this shot as well, the results were very promising.
We got this render of the K2SO element to the point that John said that he would have finaled it, leaving unsaid, if I didn't already have this perfectly good finaled shot from ILM.
We did get a great quote from John on the shot, this is the future.
It was at this point that ADG director Hilmar Koch asked John if he would be comfortable now switching over to a no safety net approach for at least one shot in Rogue One.
John agreed.
And Rogue One CG supervisor Vic Shutz identified not one but three shots from a sequence that was slated to be finished in San Francisco.
These are the three shots that ADG contributed to the film.
This was the first of our production shots.
Everything we've talked about so far really had to come together in order for this to work under real production deadlines with no ILM safety net.
We tried go-bulls or slide maps for the Imperial pill lights.
At one point we had a very clear pill pattern projected and reflected in K2's arms.
but creative direction ended up going for a softer look.
Also, the lighting change when emerging from the hallway into the larger chamber was challenging.
Due to self-shadow artifacts with depth-based shadow maps, the top lighting scenario in the larger chamber took some tweaking on Justin's part in order to resolve all banding issues.
This one was a fairly straightforward shot.
This was the first shot where we realized the significance of the object ID map, which is one of those three AOVs I mentioned earlier.
This allowed the compositor Dan Elstrom to ensure that bright background elements came through the defocus blur correctly.
This shot presented a challenge in that K2's self-bounce lighting is obviously missing from our direct dynamic lighting setup.
K2's shoulders should be reflecting a lot of light up onto the back of his head and with the global illumination path tracer you can get that.
In our case we ended up cheating with bounce lights, parented under certain bits of the animation hierarchy.
and keyframe during the shot.
We used a similar timing setup for this that was used for the explosion effect sliding in the last test shot.
This sweeping camera really illustrates the weaknesses of our real-time implementation of the shading model.
This was the one shot where our single specular lobe really started to limit us from reaching the approved or blessed K2 look.
Justin had to do some shot-specific material tweaks.
to make some of the more reflective bits and bobs sing properly.
We are thrilled that we were able to contribute to a project like Rogue One.
The work of innovation and real-time rendering doesn't stop here, though.
Current ADG and XLAB projects in flight right now, such as the upcoming Vader VR project, demand continued advancement in both fidelity as well as performance, and we have a number of areas of active development.
Aerial lights and shadows.
Support for Pixar's universal scene description format, or USD format.
We are excited by Epic's announcement of USD support this morning.
Pushing our real-time subdivision surface rendering techniques further.
Incorporating open color IO.
Supporting true arbitrary output variables during rendering.
And continuing to improve our real-time implementation of unified surf.
In closing, I'd like to take this opportunity to give a huge shout out and thanks to the rest of the team who made this work possible over the past year.
David Mennie, Nick Haynes, Indy Ray, Vic Schatz, Dan Enstrom, Justin Schubert, Ron Radetzky, and Hannah Gillis.
And we have some time for Q&A.
And by the way, ILM, ILMxLAB, and Lucasfilm Advanced Development Group are all hiring.
And Sarah, our XLAB recruiter, is here in the room.
Please stand up and wave, Sarah.
There she is.
Thank you.
Yes?
I'm kind of curious a little bit about the soft lighting and area light approximation that you guys come up with.
You do stuff in general, you probably do that as well, basically using arrays of different lights in a pretty cool way.
So, our area lighting shaders are, so we ended up using both techniques for some shots.
Because render time did not have to be, you know, we weren't making a game to run at 60 hertz.
We had multiple seconds to render these images.
I don't know if you want to talk about the lighting rigs that you built that were using many, many, many lights.
Yeah, sure. We definitely used clusters of lights, basically old-school point light technique of casting a bunch of shadows, getting soft lighting from multiple shadow samples.
Worked, but it was extremely expensive, way too expensive.
So basically, I kind of went over to Roger.
I was like, what do I do?
I had basically a day to figure it out.
What I ended up doing in the long run was taking the PCF shadows that exist in Unreal and putting a multiplier against its depth sampling.
And then that in tandem, like that was then, I implemented a slider into the lights.
And another guy that I work with, Brasher, Christian Maturi, helped me on that effort.
Very good guy.
Anyway, so in tandem with that, cranking the bias, I was able to actually shorten my depth and get softer shadow spread, which, ended up actually looking fairly nice.
So.
Is there something else you want to say about it?
No, I think that pretty much covered it.
The lights are pretty bright.
I can't see.
Oh, yes.
You mentioned that you speed spaced any machine.
Did you just simply darken the frame buffer?
Or did you do something more to it?
No, it was, uh.
You would know the details, but I believe it was applied in this...
It's being applied to specific lights, yeah.
So the question was about screen space ambient inclusion and whether we are applying it to the final image, and the answer is no, we're using it as a shadowing factor, as an occlusion factor for specific light types which had no other shadow calculated.
Mainly, like, really the only light that is shadowed by that is the environment sphere, because we have no other shadow term to pull from that.
Yes.
I noticed that you had certain visual effects elements in some of the shots as well.
For example, sand kicking up from K2SO's feet, and in the X-Wing shots there was all the water vapor and everything.
Did you also render those in UE4, and if so, how did you approach those?
No, no, no.
That's all in the composite.
So the X-Wings are that specific element.
That's what was rendered in the real-time game engine.
And again, rendering those X-Wings fast, even just rendering the X-Wings takes multiple hours in RenderMan, so that was a win for us there.
And on Rogue One, it was specifically the K2SO element that goes into composite.
So the explosion, I mean, actually in that shot, that was a practical, that was photographed.
And then the smoke and dust, those are 2D elements that are just done in compositing.
And the same with the sand kicking up?
Yeah, the sand kicking up, actually.
was in the plate because Alan Tudyk was performing K2 in a motion capture suit so that he was actually kicking up sand and that was retained.
Thank you.
What kind of global illumination approaches are you thinking about using for the future?
So there are various...
mostly pre-computed or baked global illumination solutions that you can use within Unreal.
You can bake light maps, you can bake diffuse indirect probes, you can bake IBLs from within Lightmass, which is Unreal's baked lighting tool.
In terms of more dynamic GI, we intend to sort of address that as we run into the problem, because there is a lot of different ways you can skin that particular issue.
I think the form in which we first encounter a need to solve it will kind of drive the approach that we take.
Yeah, quick question. So in comparison to the approach that you've taken, what's your opinions on like GPU path tracers, like for example like popular ones like Octane, Render, or FStorm?
these kind of up and coming GPU pass tracers that are basically using the GPU to render frames really, really fast.
Like, have you experimented with that and what are the kind of comparisons with that approach?
So, I certainly don't want to speak for John, but for me, the fact that the image is resolved fully and there is no convergence, progressive convergence, during interactive editing, that even though things are wrong, that's a win for rasterization in my case.
For the lighting artist to be able to see an image that is very, very, very close to what the final image is going to be, completely interactively and in fluid real-time during working with no shift, no convergence required, that seems huge.
I thought that movie companies were rendering their graphics with Maya and programs like that.
Now you're doing it with Unreal Engine.
Why is that?
Well, most of the shots in the movie were rendered with other tools than Unreal Engine.
And the full list is probably quite long.
I guess there's the mainstream pipeline.
Yeah, I mean, the majority of the renders on the show are RenderMan RISC renders.
A lot of the environment work is rendered in V-Ray.
Then a lot of effects elements, smoke and fire and that sort of thing.
I think a lot of those are mantra renders, along with a fair number that were done.
In Plume, an in-house package we've written.
So it's a pretty big mixture of different techniques.
But your question about why real-time, why GPU renders, was really about iteration time, looking at the future.
Ideally we're letting artists really see what they're doing and be able to reduce that iteration cycle down to seconds.
I think we get down to being able to hit feature film level quality with renders that are interactive so you can move lights around, you can make material adjustments pretty much in real time.
That does shave quite a lot off of the amount of time that goes into developing an asset.
Time is money.
Do you see a tradeoff between the man hours that is required to prepare a shot for the real time versus just sending it off to the render farm?
That was one of the requirements of this exercise was that there...
we weren't allowing ourselves any custom preparation of the content that goes into these shots.
We had to be able to consume the content directly from the fire hose of the ILM pipeline, and then that's what we executed.
Yeah, I mean, certainly we could have made the job in some ways easier for ourselves by authoring something that's performant in the engine.
But for me, a big part of the point of this exercise was, well, don't wanna.
Do any special, we're going to rebuild it for Unreal.
No, we're going to take one of our standard production assets kind of as they are normally built for RenderMan, and we're going to hand it off to ADG, and they're going to render it and make it look beautiful.
Hi, thank you for your presentation.
As these processes become more and more real time, I can't help wondering if you've spent any time anticipating what will become of the artist experience.
I remember from my own time at ILM that a given asset might take 15, 20, 30 artists working on it.
Do you see any of these processes, these improvements in render time and interactivity altering that experience for those individual artists?
And if so, how?
Well, I suppose some of the things I see happening are one artist being able to do more work.
You know, I'm looking forward to the day when you can have a handful of artists kind of do whole sequences.
It seems like you're getting closer to the city, you know, in real time, performing with multiple artists.
Yeah, it's...
You know, a lot of it comes down to just iteration, you know.
It usually takes...
I have 5-6 takes on lighting renders to really get things dialed in.
But if you don't have to wait for an overnight render and look to see it in the theater, if you can make a tweak and just see what you're going to get, I think you can get there a lot quicker.
Economics are huge in this business.
Anything that allows you to do very high quality work and put fewer man hours into it.
I had a question specifically about just the motion blur.
I'm curious why you decided to do that in engine rather than kicking that to comp as well, where it seems like you might be able to get a little bit smoother result rather than just kicking it out 16 times in Unreal.
That was specifically because that's the way it is rendered at ILM.
That the compositing setup for these shots, like the compositor who's working on these shots is also compositing a ton of other shots that did not come from ADG.
And so he's used to renders coming to him with motion blur in them. And so we were.
We stuck with that.
So rather than rendering a vector pass or something like that, you just figured that would be the way to do it?
Well, also, that would require, like in order to have a motion blur technique that we could render out, we would have to invent one that is as high quality as what you can do with the temporal and spatial subsamples in the Olympic.
I just figured for those three specific shots or something like that where the motion is rather linear and you're not dealing with anything, any crazy motions or anything like that.
Technically, we probably could have rendered without blur and done a 2D vector thing.
But I wasn't trying to give them an easy way out on this.
And we have sometimes resorted to doing motion vector renders to save on render time or to reduce noise.
And it can look good in plenty of cases, but it's sort of technically wrong.
And there are always pathological cases where it just doesn't work at all, like spinning propellers and things like that.
So I think in general that the preference, the culture at ILM is to try and get it more technically right and just let it grind a little bit longer on the farm.
Hi. So obviously you had to pick and choose your battles here.
And there are some compromises you had to be willing to make, um, to get, you know, good enough, but still run close enough to real time that it would meet your, your requirements.
But, um, what do you, what do you see as like, uh, the prospects for being able to start handling more, more broad cases, like not just as restricted as you're kind of currently.
having to be.
So the work that Natty and his group are doing right now will lift a lot of the restrictions on the types of assets that we would even consider tackling at all.
You know, having a full version of the unified shader implemented in real time, there will no longer be a reason to pick a certain asset over another.
Most of the compromise that we made from the what was real time and then what was a minute of frame was all about aliasing.
So for the lighting artist working in real time at his desk, it's an aliased image, but it's a real time image.
And so if we can consume the geometry caches from ILM and we can shade them with the shading model that ILM uses, and it's a slightly aliased image, then I think we'll be in business.
So you think you're on a roadmap to be able to handle non-hard surfaces even?
Well, roadmap is a pretty abstract description of that.
Yeah, more arbitrary hard surfaces is a good first one.
Once you're talking about things with subsurface scattering, it's definitely something that we've been looking into, but that is obviously a trickier one to do at sort of ILM visual standards.
Right.
Yeah, I think just getting support for the second specular lobe and for the refraction and reflection model, I think, you know, opens things up pretty considerably.
Yeah, the second speculo we already have in our latest work and refraction, we've definitely been investigating that as well.
Really good looking soft area lights too.
Yep, and we have some good ideas for that.
Who benefits from this real time rendering?
Is it just somebody who sees K2 and he can look at it?
Compositor have everything composed it and edit it in real life and see how things change Well in terms of the pipeline Yeah somebody who's who's done a lot of sort of offline software rendering You know I've suffered through the the battle days where you would tweak a parameter And you dispatch a test and you you know wait 15 minutes to see a little postage stamp version See, it's, you know, when your iteration times are fairly long like that, you know, there's only so much finesse you can afford to put into a shot before you basically run out of time.
And so the faster you can get the iteration loop, then the better you can make something look.
You can kind of dial something to be just so.
You can do much more high quality finished work on take one.
So the goal here is to reduce the number of takes that are required and how long an artist has to work to get a satisfactory result.
And letting an artist get more work done means there's in total fewer man hours that go into a given project.
So that makes the work less expensive to do.
There's less of a chance that it all goes to third world countries.
To piggyback off of what John's saying too, the process was very WYSIWYG.
Like, what you see is what you get.
It's right there, it's fluid, you can light, what you see is what you get at the end.
Like, there is no iteration in between.
The only effect that you're not getting is like proper anti-aliasing, the motion blur, but the rest of the shot, if you have a back plate behind the thing, you can light to that back plate and get your final result in Viewport.
So that's also the benefit, that's the primary benefit of this tool, right?
I could have like Vic Schutz for instance at my desk saying I need another point light over here, I need some extra spec on his shoulder and I can just drop that in there and look at it and he can approve it and I can say okay now I send it off and render out the sample.
So that benefit for the artist flows upward all the way.
I mean, John benefits from this, getting more useful takes in dailies every day.
I've shot a lot of live action too, both live action things and miniatures.
And I'm also very used to a kind of different workflow.
When you're on set and you're lighting something, you see how everything responds in real time and you'll have somebody kind of move in a light and you'll watch how the shadows fall on a character and you sort of slide things around until.
oh yeah, that's feeling about right.
And much slower software renders, it's a very different workflow.
You kind of take a guess at it and you, and having a tool where you can drag things around in real time is sort of replicating a bit of that onset experience.
I think that changes a little bit how you think about lighting.
Thanks.
So for GPUs, I'm curious, did you guys use the Consumer 980 or Titan, or did you go the workstation Quadro type route, or something else altogether?
So we were using, yeah, we used Quadros.
So you have added UDIM support to Unreal?
We added MaterialX support to Unreal, which kind of brings UDIMs with it, yeah.
Cool.
Thanks everybody.
All right, thank you.
Thank you.
