That's our cue, the usual reminder.
If you could adjust, also turn off your cell phone while the presentation, it'd be really appreciated.
So, thank you for choosing us as your kickoff conference for this morning.
Here's a small agenda so you guys understand a bit where we're going.
I'll go with the introduction of who we are and ADAS and then Nicola will kick in with the work he's done for Hitman's game and a small conclusion afterwards.
So, um, yeah, I know it's a bit early, so I'll try to keep you awake until the interesting stuff kicks in. Uh, it funnels me, I guess, to my intro. 37 year old, proud father of two, including an 18 month old little baby boy, so this might only resonate to the parents of young kid in the audience, but actually, that's a break I'm having right now.
So excuse my Canadian call that I brought, which is changing a bit my voice, so I'll try to be a bit louder. So, born and raised in Montreal by an Italian father and a French Canadian mother, explaining the weird name that you're seeing up there. And I spent almost 10 years in the gaming industry, coming up from the aerospace, and I spent most of those years in two studios in Montreal.
I apologize in advance as I extracted that information from a corporate deck, but let me just draw your attention to some of those facts.
ADOS Montreal moved into its early teenage years, celebrating its 10th anniversary not too long ago, this month, last month.
And over those 10 years in which I was an active participant for seven of those, the studio built itself a strong reputation, both in Montreal, but around the world.
A strong installment of beloved franchise such as Dersac's Thief and Tomb Raider helped accomplish this positioning for the studio.
So points here in the next slide that really drives most of my attention and I'd like to get your attention onto those as well.
Of course innovation and creativity, it's something that my department is highly involved into but also honesty and respect.
The studio is creating great titles from great IP, yeah, but it's always done through an enjoyable environment, which is something very important for us.
Respectful approach towards our most valuable assets, which are the employees themselves.
Great overtime crunch policy, for instance, and strong balance between work and family or other activities.
uh... Amy Inning wrote an interesting paper in the game industry biz not so long ago really recommend it if you if you guys are looking uh...
to learn a bit more about it let's say this straight the active community is growing older I know I'm growing older and we all have a changing expectation towards our our employers which is fine in the end and we've seen this trend in other studios so kindergarten and access to doctors etc so here at ADAS we're kind of a big family and I think our returning rates uh...
is a great indication of this.
So, moving on to OOI's labs.
Well, it all started when our head of office regrouped a bunch of released resource to push technology barrier and act as a single unit.
We were folks from different fields of expertise, but we strongly collaborated together in the past, which actually made a big key element to the success of this team. And we knew each other's strengths.
We've had our fighting moments, stress moments together.
Tears, everything we're experiencing in our own respective game teams, right?
So then the rest are there.
Three years anniversary, we've contributed to three games that I shipped already.
Collaboration with four different studio over three different time zones with all the challenges that this goes with.
14 expert contribution, went into lab structure.
Some went back to production and others, and others elsewhere.
We did over 18 delivered features and games, over 24-ish different investigation, and year-to-year public appearance, which is important for the studio itself.
What are the expectations for labs?
Pretty straightforward and easy to understand, but not so easy to put in context and application, if you were to ask me.
Solve complex industry problems, or at least grow our understanding of it, its limitation, and work around.
There are problems that we're all struggling with, independently from which studios we are.
We're attempting to address the issue that is an actual problem we're suffering with, but we're also trying to keep aware of problems to come.
My team tried to invest the right amount of time in avoiding that these issues ever become a production showstoppers.
Then sharing, very important aspect of my team as well, sharing the knowledge gathered or proven solution to the sister studios to help position our different products.
So how do we operate?
Industry problems, some problems are well documented.
Multiple solution were proposed by expert and big brains already.
But other or totally new issue that we need to investigate, build knowledge about it, and a better understanding.
Always researching for different solution.
I'm aiming at resolving a problem with the first solution that eMERGE might actually show it weaknesses on the long run and might not be as solid as we'd expect.
Search all possible solution best fitting a context.
Constructive and collective and associative mentality is part of lab success.
The collaboration in the team is super important.
Regular exchanges, chair rolling philosophy as I call it.
Just poke your friends and put as more brain as possible into a problem so the solution is just up to everyone's standards.
We regroup conception, prototyping and implementation under one roof, so the idea is to prove its value quickly and fail fast enough to find a big strong winner.
What makes sense and who we're innovating for is important to keep the focus on the client's need given we're working with different studio to not try to solve any personal fantasy that we might have in regards of the problems.
So let me guide through some of the team's achievement that we've done over the last three years of existence and we'd go first with Rise of the Tomb Raider.
So the contribution in Rise of the Tomb Raider includes the creation of six main graphic features that brought a lot of focus on the game.
Those feature are among some of the best technical achievement in Rise of the Tomb Raider according to some press release and my biased self.
We have led, of course, multiple direct and indirect promotion of the game's technical achievement through major industry events, such as the one you're seeing on screen.
Also, two years of continuous collaboration with all of Lab's member, with the dev team.
We worked on over 10 different graphic features, including some that never made it in the game for many different reasons.
Some are tied to the fact that production change, it's scope during uh during the dev cycles and we're kind of a client centric approach so they have the last call. When we also tackle at the same time the integration of uh more mainstream feature that our innovation were requiring.
Then moving on to Durzak's Mankind Divided of course um We had a close proximity with the Deus Ex team and it was a strong benefit.
The different studio experts were accessible resources with collaborating with us along our collaboration.
This led the group in tackling some of the biggest challenges, such as the one you're seeing there, the iconic feature which is called Titan Shield and its vignette.
Also again, mutual confidence allowed the game team to benefit from many of our previous research.
A role that is expected from my team, the sharing of technology whenever applicable, and in worst case, sharing the knowledge gained as broad as possible.
With that in mind.
Labs worked hard in providing DIRS-X with unique pure hair integration, motion blur, volumetrics, particle lighting, R&D, and so on and so on.
Lots of knowledge shared with this group and our collaboration with the team is still active on a day-to-day basis.
So yes, I touched on that aspect.
It's an important pillar for EGDAS Montreal.
Labs was always heavily involved in sharing back with the community some of our innovations.
And additionally, Labs also co-promoted technology with industry-leading partners such as AMD, Hewlett Packard, and Autodesk, only to name a few.
And now, we're moving on to It Men's Game, which is a bit the reason why we're all here today.
We were really looking forward to collaboration with the very talented IEO's team.
The different work have gotten us to collaborate for a little over a year altogether.
And we've had the chance to work on three different unique graphic features for them, including the ocean technology that we're gonna discuss a bit later on.
Yet again.
we naturally promoted the ocean technology in different ways, which explain our presence here with you today.
So while Nicolas come up on stage, we'll just go and have a look at a prototype that we've done.
Without sound.
the the So hello, good morning.
My name is Nicolas Lanchon.
I am a technical artist in the Labs R&D group at Eidos Montreal.
I've been in 3D for a little over 16 years already, with about nine years of that spent at Eidos.
My background is mainly in graphics and VFX, and on the technical side of things, I'm mostly self-taught, which I think is not very unusual for a technical artist.
I'm also a proud dad, and I can bake a pretty decent loaf of bread.
Okay.
So to start, I'd like to note that this talk is not about a revolutionary ocean implementation, but a practical one.
Water surfaces are a difficult topic in real-time 3D, and my hope today is that I can add to the community bag of tricks for creating real-time ocean surfaces.
During Hitman production, roughly around mid to end 2014, IO Interactive approached us to help create a solution for their in-game water surfaces, specifically ocean rendering.
They wanted an efficient, lightweight solution that was up to par with current methods and could easily be implemented in a variety of contexts and have support for physics interactions.
From the supplied maps and reference, I could also see other non-trivial features like transparent water, an arbitrary shoreline, and the fact that you could both walk along the beach and view it from a high vantage point with an unobstructed view to the horizon in several directions, hence the name for this talk.
This implied a seamless transition of surface detail from up close to kilometers away.
Physics interactions included things like floating, things like buoys, small boats, and ballistic impacts, and this being a Hitman game of course, ragdolls, AKA dead bodies.
Supporting physics became the initial focus, as this would drive how the rest of the effect would be designed and function, meaning the solution had to be CPU physics friendly.
So we started looking at, excuse my voice.
So we started looking at what was done at the time, and there were a lot of implementations to choose from.
Assassin's Creed Black Flag had a nice solution, which sort of became my visual benchmark for the effect, even though high-sea sailing wasn't really necessary for us.
Uncharted, Far Cry, and Killzone are some that stood out also.
All very nice solutions using interesting techniques and all with the different context and requirements.
Of course, there was the obligatory read of Jerry Tesendorf's paper on ocean rendering, which is the gold standard for ocean simulation in CinemaVFX, but also more recently applied in some AAA titles.
There are a lot of open water solutions, but unfortunately there's very few for beach interactions, except for the offline and expensive SPH or flip particle simulations.
And it's very difficult to find any solutions that offer a seamless integration of both open water and quality beach interactions.
Solutions that tackle open water well usually have limited beach interactions, if any.
And the effect sort of lives inside of a tank.
A lot of the times the water displacement simply clips the beach or is attenuated by distance field or water depth texture.
Just a spoiler alert, I don't think I've completely solved this issue myself, but hopefully I've taken some steps in the right direction.
So to start, what do we need to make an ocean surface?
Well, we need a surface, of course.
That seems obvious.
But what kind of surface?
Is it a procedural or a static?
How is it parameterized?
How is the geometric screen density handled?
Open water is simpler, but what about our beach?
And how to transition from beach to open water?
Common choices for surface generation are simple static geometry, which is a user-authored mesh, screen-projected grid, or a system of multi-resolution patches.
The screen-projected grid, sorry, static geometry can be okay if you have a limited vantage point and are able to statically distribute from a known source, from a known point.
This isn't our case at all, and we wanted to avoid handling a LOD system.
The screen projection grid is a popular choice for good reason, since it's by nature restricted to the frustum, no frustum culling necessary, and screen geometric density is constant and distribution is more or less ideal.
Parameterization can be, for shorelines, can be an issue, however, as no UVs or other vertex data can be localized on the mesh.
Localized data has to be then stored in an implicitly mapped textures or by other means.
Patch systems incur some system overhead and require seam stitching at the borders where patches of different resolutions meet.
They also have the same perimetrization issues as the screen projected method.
Hardware tessellation was something I had wanted to apply to water for a while.
At labs we had implemented it for our Titan Shield effect and afterwards more extensively in the snow deformation effect in Rise of the Tomb Raider.
During the development of that feature, we had tested hardware tessellation against a comparably dense static mesh and found hardware tessellation to have much better performance.
Using tessellation would allow for perimeterization, I have problems with that word, and arbitrary topology along the base mesh where necessary, meaning I could UV map my shoreline if I required to do so.
Also, transition in geometric resolution would be automatically handled, so no stitching algorithms or LOD systems are necessary.
So, for the ocean, we're using an authored mesh as a base for tessellation, but as a very sparse hull mesh.
For tessellation, we employ a simple distance-to-camera adaptive tessellation technique with a non-linear falloff.
Tessellation density can be kept constant to a given distance.
After that, possibly due to angle of incidence, I found that I could have geometric density fall off much quicker than linearly with little to no noticeable effect in quality.
Tessellation is also kept within the camera frustum.
In the shader, the camera distance function is computed in the vertex shader and passed on to the other stages for modulating the tessellation factor, displacement amplitude and other shader effect weights.
As for the final tessellation factor, there are a lot of guidelines for tessellation.
Sorry.
Most simply state to keep it below a certain number, but this doesn't really take into account the initial geometric density.
My base mesh has huge triangles, so this was not really helpful.
The one guideline I found most useful and stuck to was keeping screen density in check.
That is to avoid having triangles smaller than 16 pixels screen area in order to maintain rasterizer efficiency.
I found this info in an AMD developer presentation, the AMD GNC architecture, a crash course by Leila Ma.
When setting edge and face tessellation factors, only edge factors are handled.
Face factors are simply an average of the former.
With the exception of the beach variant base mesh, which I'll explain later, the base geometry is very sparse.
It's a radial ocean mesh roughly 800 meters across, divided into hectare patches.
So 100 meters by 100 meters.
The total mesh diameter was found by placing circles representing the desired view distance at several extreme positions in the map and then encapsulating those within a larger circle representing the final extents of the ocean geometry.
Base geometric density before tessellation is one vertex per every 10 meter.
Keeping a uniform distribution of triangles will allow to better control tessellation density.
Permanently occluded geometry can be removed, but caps are kept on the 10 meter grid in order to keep the uniform triangle size.
The beach mesh is a special case.
Here we want UVs and vertex colors for shore waves.
We also want close topology match with the shoreline.
So the 10 meter grid goes out the window here.
For modeling, I found it best to model both the water and the floor of the shore using the same waterline curve.
This allowed for matching topology and UVs on the shore floor, which could subsequently be synced to the shore wave water effect and used for a wet sand animated effect.
We still want to have the borders of our beach patch mesh match the 10-meter grid as to avoid tearing during deformation.
However, now we have a problem for tessellation.
Tessellation will divide all triangles by the given amount, no matter what the triangle size.
So, keeping to a 10-meter grid kept a homogenous triangle size and perfectly uniform subdivision.
But now we have smaller triangles with the same tessellation factors, giving us some very dense and unnecessary geometry.
And ultra dense geometry is bad for the rasterizer efficiency as we saw.
So, we want to keep tessellation density relatively uniform.
And to do that we need to encode a tessellation bias value into a vertex color data for the beach mesh.
Here in the image I've applied a color ramp so that it looks cool.
But really it's a scalar value.
For water patches marked as beach type, this vertex data is used in the hull shader to bias the tessellation factors.
To find the bias value, we process the geometry in our DCC and find each triangle's area.
And going back to our 10 meter grid, we know that a normal triangle has an area of 50 square meters, 10 by 10.
triangle area and therefore we can normalize to that value.
This will give us a zero to one size ratio that can then be encoded into a vertex color channel.
Ratios bigger than one are clamped.
So when applied, the smaller triangles are proportionally tessellated to a lesser degree.
So we have a surface and now we need a method to move the points around.
Displacement is pretty common in games now, but for water it should be noted that a height function is really not sufficient.
Water churns and moves and lateral movement is very necessary to properly sell the effect.
So we're not really looking for a height displacement, but rather a vector displacement.
Just a note on compute shaders.
We did quickly look into a compute approach, but ultimately decided against it, fearing latency issues I would incur when getting the modified mesh back from GPU in order to perform CPU physics.
Maybe we could have used GPU compute to generate vector displacement and normal maps at runtime, but this would have brought with it a fixed tiling pattern that would have to be managed.
We also wanted to avoid doing CPU texture fetches for physics as much as possible.
Black Flag had a good implementation using, I believe, offline generated vector displacement maps, but we ultimately did not explore this route since we wanted a stateless function.
Other than vector displacement maps, what other methods have been applied?
Uncharted made use of low frequency Gerstner waves along with higher frequency, what they called particle waves and an additional mega wave deformer.
Our mandate did not call for such rough seas, but I really like the idea of artist's placeable waves.
We did look into FFTs, or Fast Fournier Transform Ocean Deformation, as outlined in Mr. Jerry Tesendorf's 2001 paper, Simulating Ocean Water.
FFT waves are maybe not a good name.
The fast Fournier transform here is a component of a larger set of equations based on statistical models found in oceanographic literature.
There's a reason why FFT waves are pretty much the gold standard for ocean simulation in cinema, and this is the method produces some very convincing results.
And although advancements in available CPU, GPU power in games hardware have recently made FFT waves more viable as seen as the more recent Just Cause 3's Nvidia Waveworks implementation, they do remain computationally intensive.
And for a poor technical artist like myself, a bit mathematically intimidating.
Math complexity put aside though, FFT waves have some issues to consider.
By nature of existing within a domain, they tile both spatially and periodically.
The spatial tiling can't be fixed.
This can only be managed.
They can be difficult to get right, and the generation of normals or CPU-side surfaces for physics requires even more FFTs.
For very water-centric games, the cost may be worth it.
But for us, I went back and revisited Gerstner waves to drive our effect, starting from the well-known GPU Gems article, Effective Water Simulation from Physical Models by Mark Finch.
Despite maybe being considered obsolete, for cinema at least, Gerstner waves have some nice properties that shouldn't be overlooked.
Gerstner waves are computationally efficient.
and the function is easy to derive in order to generate a surface normal and tangent basis.
They might not be as realistic as FFT waves, but they can also offer convincing results given the proper setup and perimeters.
Although there is also a periodic element to Gerstner waves, each wave is independent and not tied to a domain, so it's easier to manage repetition.
One easy way to break repetition, which we implemented, is to employ a low frequency phase noise texture to the waves.
The deformation effect was separated into three categories, or water behaviors, if you will.
There's the generic open water waves, areas where there's no directionality or influence from land masses.
Then you'll have the closer to the shore, the coastal waves.
Would still be open water, but here you would start to feel some directionality and conformity to the coastline.
And beach waves, of course, where water actually meets the, makes contact with the landmass, and the contact line needs to be handled somehow.
Water towards the horizon has no displacement, and we'll talk about that later.
Open water is the generic non-directional motion of the water.
This is the base layer for the effect.
It's a mix of eight Gerstner waves arranged in a way to cover all directions without canceling each other out and create a wave motion not biased to any particular direction.
So while we're talking about mixing Gerstner waves, there's a few points I can share on the topic.
If you're like me, your first instinct would probably have been to mix many wave fronts and just randomly pack them on as much as you can, thinking more waves means more detail, right?
Not necessarily.
What happens is like, if you think of it like adding noise, noise on top of noise, values will eventually converge to the median value.
And in our case, since we're doing bidirectional displacement, the median value is zero.
So the more wave fronts I would add to the system, the more my water surface got flat.
Basically all of the delta positions were canceling each other out.
So more is not better, not at similar frequencies at least.
Which brings the second point.
Separate your wave fronts into frequency groups.
For example, we have a low frequency group of three wave fronts, and a high frequency group of five wave fronts.
Generally, the lower the frequency, the less wave fronts you want to use.
And doing this, separating into groups, I found is a good way to manage complexity of the effect.
Separating into groups allows to unify controls for the waves in that group, and simply apply variance to the individual waves from the original controls.
Meaning the overall effect is easier to control for artists.
Additionally, you may want to skip a high frequency group when sampling positions on CPU for physics, or any other instance where you would need a low fidelity height sample.
And finally, the obvious, you want to break symmetry as much as possible, so odd numbers are your friend, and add variance wherever you can, and avoid having anything on axis.
Our three wave fronts are radially distributed and our five high frequency wave fronts start from the reverse direction.
So no directions in the same group overlap and no directions intergroup will overlap either.
Conceptually, ocean surfaces are composed of an infinite amount of random wave fronts.
But realistically, waves interact.
They collide, they exchange energy, they die, they separate, they merge.
Of course, our water waves don't do that.
They simply add.
This is also the case for an FFT ocean surface, I believe.
I'm inclined to say that these are not technically simulations like you would have with SPH or flip particle solutions where a finite data set is updated through a solver.
We have no data set to update, but we can control how the waves are added and base positions from which they're computed.
So I find there's basically three methods to some Gerstner waves in reference to when they get applied to the base.
sequentially, some delta positions, or some deltas by group.
The first method is to simply sequentially add the waves so that each wave is computed from base, sorry.
so that each wave is computed from positions deformed by the previous wave.
This isn't a great method and is difficult to control.
The reason I dislike it is that the first wave is deformed by all subsequent waves, but the last wave isn't deformed at all.
So the weight of each wave front is not equal in this case.
The second method as seen in GPU-GEMS Gerstner wave surface equation.
is computing all wave fronts from the base position and then adding the sum delta positions in one go.
This way all waves have an equal effect.
However, since they're all computed from the base position, the waves don't really deform each other, which is a nice effect to have.
The third method takes advantage of the fact that we've separated our wave fronts into frequency groups.
In this method, we sum the deltas, but we apply them to the base position between the groups.
So the base position here is deformed by previously applied groups.
And we go in descending order of frequency, which is important.
This is so that small waves get deformed by larger waves, as would make sense in reality.
The bigger the wave, the more inertia.
I also found that this method, I also found this method to be much less sensitive to pinching at the crests, which is a common problem when using Gerstner waves, than simply summing the deltas.
And still allowed the waves to be deformed by each other, like with the sequential method, but in a much more controllable manner.
For coastal waves, you want to have a more directional wave front, fitted to the curvature of the shoreline.
To do this, we first looked into parameterizing the mesh, but that would require a lot of unique assets, and the results were not really that great.
It was also difficult and tedious to have localized control for things like intensity, speed, things like that.
We also wanted a friendlier process than baking into textures and iterating inside that process.
Inspired by forward lighting, I had the idea to create a simple system of artist-placeable wave objects.
These objects work a bit like forward lights in the shader.
Each wave object adds three wave samples per vertex and the shader allows for up to four wave objects per mesh or per hectare patch if you remember.
The worst case scenario is therefore 20 wave samples per tessellated vertex.
Each wave object holds a set of perimeters and passes these to the shaders via constants.
This allows the user to control area of effect, direction, wave amplitude, and curvature of the wave front.
In order to support curved coastlines, it's possible to warp the wave front along the wave direction.
The wave curve perimeter allows bidirectional warping in order to obtain a convex or concave wave front.
This also helps to reduce the number of wave objects necessary to follow the shoreline.
So we solved the tessellation issue for arbitrary geometry and we don't require crashing waves which makes things much easier.
But how do we get the water to slide onto the beach?
The player can walk along the beach right up to the water.
So just letting the water clip is not an option.
Attenuating the deformation to a static position along the shore is common, like we saw either by depth texture or just freezing it to a static position.
But I really wanted to go for that sliding effect.
And to do this, the beach interaction relies on a baked depth slice of the scenery, which is sampled by the ocean shader in the beach variant and allows for the vertices to know the height of the beach at their location during displacement.
This allowed to have the ocean mesh slide onto the beach when moving.
This is as simple as selecting the maximum height between the displaced vertex position and the sampled beach height.
Full scene height map sounds a bit intense, but we know the water height.
So from there we can cut around that and maximize depth resolution in the small strip of height that we really need.
The height map only stores a four meter wide layer of height, two meters spread from the water height, which allows for an acceptable height resolution of around one and a half centimeters when packed into an eight bit texture alpha.
For area, roughly four texels per square meter seems sufficient in our case.
Since we now have this texture covering the map, we also encode a distance field from the shoreline which we'll use for masking foam.
Okay, on to rendering.
Water is a volume, and because of that, a lot of the shading effects are not on, but rather under the surface.
So if not volume rendering, which in games is most likely the case, you have to at least process what's behind the water surface.
As per the requirements of the IO mandate, we went for a lightweight method.
and thankfully we did not need to manage going into or under the water surface.
Shading perimeters were mostly simplified to absorption and scattering, represented using a lip texture mapped to linear view depth.
The shader also made use of a quarter resolution render target for underwater geometries, allowing blurring, refraction, caustics, and optionally dispersion.
For the underwater plate, we render the scene into a quarter resolution offscreen buffer before the translucent passes.
From here, we reconstruct world position from depth in order to generate world coordinates over all of the scene geometry.
The caustic effect itself is very simple.
It's a static caustic render, just a single image distorted by a panning low frequency normal map.
It does the job pretty well, and besides, the effect will further be perturbed by surface refraction.
We then use the generated world pause as UVs in order to project our caustics over everything underwater.
We don't really care about masking our distance here.
Elements above the water obviously won't be visible in the underwater image.
And elements far off in the distance will be obstructed by absorption.
After the caustics are applied, we make a copy of the texture and blur it.
So we have a blurred and unblurred version of our underwater which can later be blended using depth.
Further effects will need to be applied in the surface pixel shader since we need the surface pixel location.
This is because we can't just use depth.
We need depth from surface.
When rendering the surface in the pixel shader, we get two UVs based on world position, pre and post deformation.
Pre deformation coordinates will unintuitively be deformed by wave motion.
So this is what we want to use to map surface textures like foam and normals.
I do use an offline FFT ocean surface in order to bake normal maps.
This is really the best way I find to get water normals.
The periodic nature of FFTs works to our advantage when baking normal maps since they tile perfectly.
We use these wave normal maps to represent the highest frequencies of waves on top of the deformation effect.
Once we have normals, we can quote unquote refract our UVs for sampling our scene depth and underwater plates.
And in the pixel shader we also have the pixel depth so we can compute eye depth from surface.
Obviously, if you want to accumulate an effect like absorption, you don't want to start from the eye, but rather the point at which the eye ray intersects the water surface.
With linear distance from surface, we then map both blurred and unblurred underwater plates and blend them along depth to a given input distance.
So why blur the water?
Is that really PBR?
Probably not.
But blurring I find helps a lot.
Real ocean water has a lot of effects going on.
Underwater haze, micro refractions from very small ripples at the surface and under the surface from microscopic bubbles.
Dispersion and other difficult to create effects.
I find that blurring works visually to convey the impression of a different, heavier medium below the surface.
And I find without this effect, water rendering often just looks like it's just air behind a deforming glass pane.
So, after this we apply our absorption and scatter lid.
Absorption works along eye depth from surface and is multiplicative.
The far point is black, as that would be where the light extinction occurs underwater.
For ocean water, this depends on the wavelength, but generally 200 meters is the point at which most to all light is completely absorbed.
This can be adjusted artistically, of course.
Scattering is a more complex effect, and doesn't really work along eye depth.
But for simplicity and efficiency, that's exactly what we did.
The downside to lit textures is the iteration rate when tweaking values.
In my case, I made a lit function in substance so that I could rapidly regenerate the texture when iterating.
We then apply our cube map reflections and specular highlights, and an SSR pass is done in post.
For foam, it's masked using both a shoreline distance field and a vertex height map computed from our waves.
While accumulating height in the domain shader, total added height is tracked in order to normalize the final height value.
For texturing, a trick is taken from Black Flag.
Multiple densities of foam are packed into a single texture and mixed together in the shader, depending on the gray value of the surface foam mask.
The effect supports dynamic surface ripples from interacting physics objects as well as ballistic impacts.
These don't deform the surface but rather contribute to the pixel normal.
What's cool though is that they contribute to underwater caustics as well.
The SSR method mentioned is a fast, simplified, no ray trace SSR which uses the plane equation of the water surface rather than the real surface and normal.
For the beach, for the beach waves, we already have our sliding water effect from the deformation.
And the beach mesh also has UVs along that strip of polygons.
So to complete the effect, we use a two-phase cross-fading UV animation.
To desync the waves along the length of the shoreline, the cosine of UVX offsets the phase of the animation.
And because we've matched the topology of the water edge and beach, we have matching UVs on the sand.
So we can implement the same animation on the sand in order to achieve a wet sand effect.
For the horizon, although the base geometry extended out several hundred meters, the edge of the mesh could be seen, especially from certain elevated vantage points.
One traditional solution is to simply create a giant grid and scale it until you just can't see the edges anymore.
This however requires a large far clip value which does affect depth distribution.
So instead I recycle the trick I use for mapping procedural skies.
For lack of a better name, I dubbed the effect horizon mapping, simply because it generates planar coordinates to the horizon line.
I think this term might already be used in CG somewhere, so if anyone has a better name for it, let me know.
Here we use the world camera vector and world camera position.
And assuming a Y-up coordsys for this whole presentation, I've been using Y-up, by the way.
We divide camera vector XZ by Y, which gives us infinite planar coords on the XZ plane.
They're camera centric though, so we adjust using world camera position to compensate.
The result is planar coordinates that match world coordinates perfectly.
So if you're using world coordinates to map textures, such as is the case for our ocean, The textures mapped to the horizon mapping should line up perfectly.
And there will be no visible seams, assuming the pixels are shaded the same way, of course.
Normal maps here don't require transforming other than swapping Y for Z, or other coordinate adjustments, since we're already aligned to the world axis.
The world camera vector on the support geometry should be exactly the same as on our virtual plane.
So with the map normal, we can get a reflection vector on the virtual surface as well.
This gives us enough to shade the surface and match our real ocean geometry.
Because the effects support geometry can be relatively close, the camera's far plane doesn't have to be at a very high value, which is much better for your scene's depth distribution.
Pixels above the horizon line are clipped simply using camera vector Y.
And the virtual plane is shaded using a simplified version of the ocean shader.
Reception of the effect was pretty good.
The placeable wave modifiers were especially liked.
Here we can see them used as waves from the waterfall.
The effect proved versatile enough to be used in many contexts across the game.
One nice example here is the bubbling water in the natural springs.
I believe they cleverly used an inverted Gerstner function here.
And it works pretty well.
Many size contexts as well.
Anything from small pools to large lakes or oceans.
Of course, eventually you have to ship the feature.
Still a lot of interesting ideas to explore.
And lots of ideas come after a step back.
And I'd like to share some of those.
One thing I wish I had better was better shore waves.
Our effect did not call for surf waves or crashing waves, but I still wish we had a bit more deformation in there.
Maybe more Gerstners would probably be difficult to control for shore waves.
Maybe animated displacement maps would probably be a better idea given we already have the UVs and we already have the UV animation.
Some other things to explore would be Alembic.
It wasn't really an option for us at the time.
There was no engine support.
But Alembic offers some interesting avenues to explore.
Mesh caches could allow to pre-compute and play back complex simulations of surface deformations, deferring the computation cost to bandwidth.
Pre-computed crashing wave simulations would probably be very nice.
Lots of things to solve and explore on this end though.
Bandwidth cost would be a question.
How to apply the mesh to a mesh with a dynamic resolution would be another.
How to blend alembic deformations together with other deformations would be a good question to solve also.
Better subsurface lighting or scattering.
Like I said earlier in the talk, absorption works well along linear eye depth from the surface, but not scattering.
That's a complete hack.
Volumic ray march from the surface would have been nice here.
Maybe in the quarter resolution buffer, rather than using the wave height, just assume the water level as a flat plane and go from there.
And finally, better foam.
If you look at photos of ocean surf, you'll see that the underwater area right where the foam is is much brighter usually.
And that's because foam is really not strictly a surface effect.
There's a lot of air bubbles mixed in just under the surface, which causes a whole bunch of effects right under the surface in that area.
So I don't think you can really get realistic foam until you also capture this subsurface effect of foam.
Possibly ray march through a foam height map from the surface, or maybe a simple parallax effect with a blurred version of the foam would suffice.
So that's all for me.
I'll now pass the floor back to Jian.
Yeah, well, I think while Nick and I are very happy to be here, we're representing a whole team's work, so I'd like just to add a few names over here, but also include ADAS Montreal and IO Interactive's complete team who assisted us along the way.
It would have been...
a bit of a cheat not to mention them.
And of course, we are hiring and looking for very talented and creative people, especially in my team, but also at other department, team, project and studios.
So make sure to come around and see my friend, Olivier Merville, who's in front over here and he'll handle.
He'll handle any request or explain to you any possibilities that you may have.
So I know we do have a bit of time for a question, but there's two or three reminders that I'd like to give you guys, the evaluation.
It'd be really interesting if you can just fill those evaluation, because it helps us shape the content for our year-to-year appearances and make sure that it was relevant or what would be required to be changed in upcoming presentation.
Along with, if ever you guys have tons of questions, there's a wrap up room that we can head to, which is in a West Hall Level 2. And we're gonna put those slides on GDC Vault, I swear. Any question?
Hi, I have a small question.
So first, thank you very much for the talk.
To be honest, I'm really not a specialist into that, so it's not a technical question.
It's more about how do you integrate with existing game engine?
And does all the studios you've been working with have their own game engine, or they use something else?
And how do you mix with this?
I'll let Jian reply, sorry, the question was, how do you implement with different game engines?
And further question was, does our studio support many engines and everything?
I'll let Jian answer the multiple engine question.
For implementing, one of the goals that drove a lot of the decisions that we took for this effect was really to have a drop-in effect.
So it was very much as least invasive as possible.
I don't know if you understand.
We didn't want to branch into all these subsystems of the engine.
We just wanted to use generic technology, something that we could just drop in and it works.
And I think we were successful in that.
I'll let Jian answer for the multiple engines.
Yeah, we do have multiple engines in our different studios.
It is amended that is given to my team to share the knowledge.
We're sharing engine, we're sharing technology, we're sharing features.
But there are different engine in different studios.
So it asks the team to be very versatile in regards of.
And I think it reached also the point that I was mentioning to make sure that always focus your work to your client's needs.
So they have a different code base, they have specific needs.
As Nicola mentioned, we weren't in for open ocean and so on because it wasn't really demanded.
So, yes, multiple different technology. All the best.
All right. My question is about how do you tune the...
level of investment into realism as you're working on something like this.
Because, for instance, I could see, you could go deeper or you could go less detail, less realism.
And if you go deeper, there may come a point where other things are less realistic anyway, so beyond that point, you're probably wasting your time or something, so do you have any tips or guidelines that you use throughout this process?
that led you in decisions?
It's basically a cost-benefit analysis.
We had a...
We were implementing an effect into a game that was pretty much well along into production, so we couldn't just, you know, eat up the whole budget.
And you're correct about making this super realistic effect and then it drops in, but the environment and everything is not as realistic.
And as you may know also, the more you go up in quality, the more the cost is usually exponential.
So we did have a budget in mind.
We wanted to stay light and efficient.
We made a lot of decisions, like for instance, the scattering was just mapped along view depth instead of doing a proper subsurface effect.
You really have to take a step back and see that you're making a nice effect, but it's not the game.
So you want to make something that supports the game.
We're also exposing a lot of those parameters to the team itself.
As they've used it in many clever ways, we're also allowing them to just tweak it according to their games.
We have the luxury in our team to just push past the good enough, which is often where we're aiming for in developing features for games.
So that allows my team to go a little bit deeper, but in the end, just keep focus on the client and its needs and gives him the right tools so he's able to achieve his vision.
We only have three, four minutes.
Yeah, I was curious if you had any kind of information about particles and automating that kind of aspect of water that's pretty common, or if that was not really part of your solution.
He's asking about generating surface spray, I guess.
Yeah, yeah, essentially, yeah.
I think we kept to just textured foam.
Like I said, this wasn't like a high-seas battle, like a sea black flag, where that effect would probably be more called for.
We had kind of soft seas, nothing strong enough to generate that type of effect.
But if you were to look into that, I guess I would start with height and the tangent basis.
So you guys are obviously running in frame.
Can you give me an idea of what the performance cost and the memory cost was that you ended up shipping?
Obviously it's gonna vary on the scene and everything, but just some rough metrics.
Rough metrics.
I'm sorry, I don't have any, I didn't think of bringing numbers, but we didn't break the build, and we didn't bust performance, so I guess we're good.
And they've used it everywhere, so I guess it's performant enough.
I think the SSR was quite optimal.
Yeah, it was extremely optimal.
And they actually used the water surface effect much more extensively than I originally planned.
So that probably says something for performance.
So given that we're allowed to put our slides up to March 7th, we'll also add the reference as a last slide and it'd be interesting just to put a few numbers in the PDF.
So thank you very much for coming to our talk, guys.
