Okay.
Let's do this.
["The Star-Spangled Banner"] You Well, we had a lot of water on Uncharted 3.
This talk is about creating the flood effects in Uncharted 3.
I've been Cook.
I work at Naughty Dog.
A little bit about me.
I've been in the industry for about 11 years.
Most of that has been at Electronic Arts, working on Medal of Honor, and then Naughty Dog, working on the Uncharted series.
And now I'm working on The Last of Us, our new game, Naughty Dog, which looks really exciting.
Um, let's see, uh, before all that, I studied graphic design, um, with a minor in computer science at the Uni- University of North Texas, and, uh, that computer science minor is becoming more and more valuable, uh, as a, affects artists in games, um, and I think it's gonna continue, uh, in that direction for sure. So, in Uncharted, we did lots of kinds of water, we did puddles, we did waterfalls.
oceans and floods, which is what we're gonna talk about today.
Uh, first of all I just wanted to give a shout out to the Naughty Dog engineers because they really do an amazing job at, um, eking all the performance out of, uh, the PS3 that's possible and, uh, we really couldn't get the, the level of visual quality that we get without their amazing work. So, first shout out.
And this is the effect we're going to be looking at today.
So as an artist, I look back on this, and I still just feel like there's something missing.
It's kind of hard to put my finger on it, but maybe it's Subway sandwich.
$5 for one.
So to make the effect we started with a fluid sim.
We did a pretty coarse fluid sim inside of Houdini.
And we were able to iterate pretty quickly and use it as pre-visualization to work out timing and the flow and the amount of water and all those details pretty rapidly without having to go back and forth into the game.
But it wasn't just for pre-visualization.
We ended up using it throughout the whole production of the effect.
We created the in-game mesh from the simulation.
We also looked at the simulation for inspiration on where the particles should be spraying, and the direction, and speed, and all of that.
And finally, we used the actual SIM data to push the rigid bodies, all the debris in the hallway, all the boxes and things, those are pushed along by the actual SIM data.
And since the mesh came from the SIM data, the particles were inspired by the SIM data, and then the rigid bodies were pushed by it.
It all kind of comes together and it matches.
So to create the mesh, we had a we had to figure out how we were going to do this because the surface obviously has to morph constantly. Um, we considered possibly doing a point cache, but, um, quickly rejected that idea, um, because it would require a lot of new tech that we just didn't want to invest the time in building. Um, we did a first attempt, uh, which, um, was more programmatic and parametric.
in nature where it was allowing us to basically glide bumps across the surface.
We abandoned that because it was very hard to work with as an artist and we just couldn't get the level of fidelity that we wanted.
And so we decided to go with the brute force method, which was essentially to do skeletal meshes, because our engine supports skeletal meshes really well, and to do one joint per vertex. The engineers actually were able to increase our joint count to about 580 joints per mesh, which wasn't quite enough to get the entire flood, but...
I'll cover that in a bit.
In order to optimize, fortunately we had sort of a fixed camera angle.
We knew that the camera was going to be tracking away from the flood itself, looking back at it.
And so instead of using a mesh of squares, I elongated those.
So if you look at it top down, they're actually rectangles, which allowed me to cut out about a third, or two thirds actually, of the.
the points and therefore the joints that were gonna be required.
And when you're looking at it head on because of the way perspective kind of squishes it all it looks like a uniformly covered mesh.
So we used a ray casting method to kind of extract a surface from the fluid sim.
This was also done in Houdini.
But sort of the default.
way that you would do a ray cast is to just cast along the normals of the surface.
And in this case, that would be just going completely vertically up.
And that was going to cause some problems, some artifacts that we didn't like.
And so I experimented with casting the rays more in the direction of the camera.
And that actually solved a lot of our problems.
So you can see here with this diagram that...
casting rays along the normal in that striped area, that's creating an error where there's meant to be an overhang, but there's actually just a wall. And the camera is looking, you know, right at that and it looks horrible. So changing the direction that we're casting from to face the camera a bit more, it reduces the errors altogether because of the way the water is kind of shaped. But also the errors are more hidden from the camera.
So that really helped the look.
So, like I said before, there were more than 580 points required for this half-led, so had to actually like make three separate meshes that were all driven by their own skeletons.
And because...
that gets a bit tedious to work with.
I created a little tool that would allow us to treat the entire surface as one thing, even though it's in the game.
According to the game data, it's actually three separate things.
And it allows us to build it all as one thing, and it really cuts down on the iteration time.
It's not as important on this one because it was only three meshes, but we use this technique in other parts of the game including like in some sand simulations with flowing sand and It became more valuable there because some of those surfaces were like 12 to 14 separate patches which Would just be insane to to iterate on if you're doing it all by hand So here's what it looks like with just the surface and no shader on it.
It's a little bit dark on this display, but you can see a little bit how it's morphing.
So the surface shader, we use sort of our standard water shader that we used throughout the game and other types of water.
It's a really big shader and it has lots of features, but these are some of the relevant ones for this effect.
does refraction, which basically takes a copy of the screen buffer and it distorts the UV lookup based on the normal. So the normal will change where it's going to look at the screen texture, and then also the depth. So the deeper the water is, the further away from the screen pixel that it would show without refraction.
the further from that it gets.
So it gets more distorted the deeper it is.
And then there's also opacity based on depth that helps it to look like there's particulate in the water.
So where it's deeper, it gets more opaque.
And then for reflection, the surface changes so much and it's anything but a flat plane.
So doing real-time reflections wasn't really an option.
But the cube map was totally adequate.
So we did that with Fresnel to make it a stronger reflection on glancing angles, sort of standard reflection stuff.
And then lastly we did foam on the surface, which just kind of helps it look a bit more churned up.
Here it is with the surface shader.
So it really lacks a lot of the visceral quality that it needed to have if you wanted the player to feel like he's really being chased away by this giant deluge of water.
So that actually came from the particles.
Particles ended up being just the real, the really important part of the effect.
The surface was sort of secondary.
We ended up doing 31 separate emitters that were all hand keyframe animated to match the animation of the water.
And eight different particle definitions, so eight different types of particles that those emitters were emitting.
So at Naughty Dog, we have a really great tool set and run time to go along with it.
Our UI is all done through Maya.
and we share a lot of the same features that, um, Maya particles have. Um, one thing we can do that's really useful is create custom attributes so we can store off any kind of, uh, date custom data that we want.
And we can give those data values using expressions.
And expressions are huge.
If you're unfamiliar with expressions, you should get familiar with them and beg your programmers to add it to your tool set because it's super flexible.
It allows you to manipulate data pretty much any way you want.
Our expressions are really similar to Maya's expressions.
can even piggyback off of Maya's like, um, syntax error detection and all that. Um, so if you know Maya expressions, you pretty much know Naughty Dog expressions. And then we have the ability to use ramps, um, create gradients basically, and um, we can use our custom attributes as indices into the ramp, so we can pick where on the ramp we want to pull a color or value.
um using the custom attributes and we can define those through expressions so it all comes together and gives you a lot of power. And then we can also send these custom attributes to the shader uh which allows you to further um control the way the effect looks by controlling shader attributes and stuff like that. Um so it's all very powerful and then one of the other cool features I mean there's a lot of them but this is just.
One last one I wanted to mention is custom orientation.
So our sprites don't necessarily have to face the camera, and they don't necessarily have to be velocity aligned.
We can use our custom attributes and define vectors that will orient the particles any way we want.
Super flexible.
It's really nice.
And all of this is due to Marshall Robin.
He's pretty much the.
lone programmer at Naughty Dog who does all the effects stuff.
He's really incredible. He has a talk today at 2.30 in room 309.
So go to that if you want to learn more about uncharted effects.
From a more programmer-y technical perspective.
So, um, as every game effects artist is aware, fill rate is the enemy number one. So...
We do a couple of things to get around the fill rate problem and allow us to draw a lot more particles, which in this effect gives it the really full, frothy look at the front of the wave.
We render our particles to a quarter res buffer.
So already we're drawing one quarter of the pixels.
So that's a big gain.
And then with most of the effects that we do, they're roughly round shaped.
and the corners of a quad are usually completely transparent.
So we just chop them off and make octagons instead of quads.
And it's a good trade-off between number of points and number of pixels drawn because pixel, we're usually fill rate bound.
So that's a really nice win as well.
So the main particle that you notice in the effect is the froth particle.
Uh, this, these are the color and alpha, uh, maps used for the froth particle.
Uh, they're based on source photography done by a colleague of mine at Naughty Dog named Ikki Ikram.
He's a great photographer.
I love photos by ikki.com.
Um, and then to get more variation, uh, we had four, um, different, uh, looks and just the color and alpha, and then to get.
more variety, we just smush around the UVs using this color texture map. And then the other main feature of the shader is sparkles. Before I added the sparkles, it really did look like flat, dry cards. It still had a real, um, kind of a lively motion to it and stuff like that, but it didn't really feel like water.
But just adding little glints of light really helped a lot, and that was achieved with this on the left of the it's like a black field with sparse white dots, and then that's multiplied by a uniform noise that's scrolled across it, and it ends up just making it twinkle basically.
So Um, the quarter res buffer, um, buys us a lot, um, back in terms of, uh, fill rate, but, um, it also is half, or a quarter of the resolution, so it's, it makes things look a lot more fuzzy. So to combat that, um, I did custom mip maps. I took the, um, alpha and the color texture into Photoshop and down sampled using nearest neighbor, uh, filtering, which.
gives it a much better histogram. The lower image, the lower standalone image is the nearest neighbor sampled one and the top one is, is bilinear filtering and you can see that there's just higher contrast and a sharper image. So I recommend doing that if you're, if your engine supports it. This is what it looks like inside our previewer, our material previewer. One thing to note is that the sparkles on this, the scale of them is not correct.
That's because the scale is controlled by a user variable that's passed from the particle.
So when I was showing you the particle UI, and you had the ability to pass data to the shader, that's, this is one of the things that's affected by that.
So when you run the effect in the game, the sparkles are smaller, not so big and blobby.
So Keith Garrett.
He was the lead effects artist on Uncharted 3, and he's doing a talk that'll cover more of our techniques and tricks that we use for shaders if you're interested in that.
Go hit that one up.
So here it is with just particles.
The particle lighting turned out to be really huge.
Um, it really sits it into the environment and gives it depth and, uh, the color variation that, uh, makes it look much more realistic. Um, you can see it looks really flat without the particle lighting. First thing to know about the lighting is that it was a hack. Um, we didn't really have good lighting techniques for particles in Uncharted 3. We basically used a lot of hacks. Um, this hack was, um, using two ramps. One ramp ran the length of the hall.
So, the darker area at the top is sort of the back recessed area, and then the light spots are where there's light in the hall shining on it.
And then one across the width of it, and the two are multiplied together.
And it basically makes sort of a low-res looking texture.
And that's projected down, straight down onto the particles.
And the wonderful thing about this was that it was all done with existing tech.
We didn't have to go.
to programmers and ask them to implement this kind of lighting, which is sort of a pseudo light map kind of technique.
Just with the flexible tools that we already had, we were able to come up with this idea and implement it without any programmer support.
So here's the final image with lit particles.
that's pretty much it. So, Naughty Dog is hiring. Candice Walker, I think she's going to be outside the room. You can check with her. See if there's a spot for you. There's another shout out there. And of course I wanted to thank all of you. Appreciate you coming. And I wanted to mention that there is a mobile evaluation and check your spam filter in your email to make sure that you get that and fill it out.
And at this point I can take a couple questions.
There's a couple of mics if you have questions please step up to the mic.
starting to see a couple of people.
awesome, awesome talk. Um, question I have is, um, Houdini is used a lot in film effects, a lot of game guys, um, use proprietary engines. What sort of took you guys down that route? Did you hire someone who was a Houdini expert or did you guys, um, train in it?
Basically, between Uncharted 2 and Uncharted 3, I started learning Houdini because I knew how flexible it was and how sort of a more technically minded artist can actually make really useful tools and stuff like that.
So I started learning that and then put it to use in Uncharted 3.
It was basically just my call.
It was a little hard to convince them to shell out the money for it because it's really expensive, but we got over that hurdle.
Hi. Um what was the uh purpose for uh taking the uh oh here sorry. Uh taking the fluid from Houdini into Maya was that just uh because of the pipeline that you guys had for uh the joint placements and all that? Yeah actually um uh it was a little unclear. I we did the fluid sim in Houdini and we actually created um the mesh with the mesh animation inside of Houdini brought that mesh over, we left the fluid sim in Houdini, brought the mesh over with its animation, and then that tool that I was showing that I created also adds all the joints at all the points and sets it all up to be used in the game.
Okay.
I will be outside to field more questions.
Just wanted to mention that this will be in the GDC vault.
