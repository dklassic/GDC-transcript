Hey, welcome everybody.
Thanks for coming out.
I'm Adam Glazier.
I'm a UX designer on Google Earth VR and on other VR and AR apps at Google.
And so me and my colleagues worked on Earth VR for the past few years.
And we're going to share a little bit about some of the UX struggles and solutions that we went through to get there.
But before I do, I'm going to share just some brief background on Google Earth.
So as you know, about 14 years ago, Google started exploring the idea of mapping the entire Earth.
And we started out with satellite data, but then we started using planes and cars to collect imagery, more imagery data, and using that we used photogrammetry to slowly build a more complex 3D mesh of the entire Earth.
We release this on desktop and mobile and You know, this was fine, but really the medium needed to be unlocked like you're there.
And VR is the first time that we were actually able to experience this data in the way it was meant to be.
Which was really fantastic.
But in order to do that, getting this to run at 90 FPS, at rendering the entire Earth, 10 milliseconds every frame, was extraordinarily difficult using the existing code base.
So, to do this, just consider the fact that your average city is millions of triangles.
And as you zoom out, you exponentially start to grow that, until eventually the entire planet you're looking at millions, or trillions and trillions of triangles.
On top of that, we do atmospheric shading, atmospheric scattering, realistic time of day simulations.
And so we need to render all of these things in such a short amount of time with no dropped frames.
So to do this, we needed to improve on the architecture of the existing experience by, we started with frustum calling, draw call batching, and a bunch of other ways to eliminate the bottlenecks caused by the existing pipeline.
Now, last year before we released Google Earth VR, we talked about this at SIGGRAPH.
So if you're interested in the rendering optimizations and ways that you can render entire planet, go back and watch that talk.
We give some insights there.
So the first thing we did when we got a prototype Vive, we were one of the first companies to get the Vive with the Tag Room demo.
We put people in this, like our founders, Larry and Sergey Brin and JJ Abrams.
And immediately they wanted to go further than the limit of the 3 meter by 3 meter room.
They wanted to go down the street or over the mountain and really explore around.
So, to give you some context at this time, this was about two years ago.
We had a prototype Vive.
There were a few demos.
uh... but no great examples of even just how to get uh... short distances so we essentially had no idea what you're doing uh... so we did the first thing we thought of which was allow the user to point somewhere and wherever their ray intersects will teleport them there instantly This was really convenient because you could get to the top of a mountain instantly.
But if you've ever tried this with large distances, it works well in small spaces when you're going small distances.
But if you go to the top of a mountain, users would get there and look around.
and then look down and realize, oh, they're on top of the mountain.
So it's a good idea in theory, but in practice, it didn't actually work.
And this was fine, but users also wanted a higher view or a lower view.
So we gave them the ability to scale themselves by clicking up and down on the touchpad.
They could grow and shrink themselves.
And they got those views they desired but there was a problem that came up is they would teleport towards a building or a mountain and Then they would say oh, I want to see what this is like at human scale So they'd scale themselves down and as they did that something unexpected happened Which is that building or mountain starts to drift further and further away So that thing that was just two meters in front of them also all of a sudden goes way outside the room.
So you'd end up with this clunky teleport, scale, scale, scale, teleport, scale, scale, scale, just to get at human scale in front of a door or at the base of a mountain.
So to make up for the loss of context with teleport, we tried step teleporting by just incrementally getting the user closer.
This worked okay, but it felt a little clunky and it broke immersion because every time you jump to a new place, it just feels artificial.
And then, as you can see, on the way to the top of the mountain, the person ends up with their feet not on the ground.
So, if they stop or look down on the way, a lot of people end up feeling discomfort or have the fear of heights.
So to make up for the fear of heights, we just kept their head in the same place and scaled themselves so that their feet were always on the ground.
And at the time, this was just a crazy experiment.
We thought people would get really sick if we just scaled them up and down as they're going, but it actually worked really well.
So then we tried another thing that we knew we weren't supposed to do, which is like...
flying the user.
So instead of this step motion, it's continuous motion.
And we keep their feet on the ground during this.
And this works the best for preserving immersion.
So people didn't lose context on the way to where they were going.
Whenever they stopped, their feet were on the ground.
But there was one big problem, which is it made 95% of the people sick.
Because what they see and what their body feels doesn't match.
So we had this problem.
Flying was awesome in all these ways, but it made basically everyone feel sick.
And teleport was convenient, but you lose context and you break immersion.
So we wanted to be somewhere in the middle.
We wanted total immersion, easy to use.
There's no context loss and no nausea.
So we came up with a really cool trick to solve this, which we call tunnel vision.
So as you can see in this video, about halfway through, in the outer edges of the user's view, we project a grid on the floor and a horizon line.
So what this does is it's similar to the effect of being in a living room or a theater where the stuff on the TV is moving, but your living room is not.
And it's that outer area that tells your body, OK, this is OK.
I'm not moving.
It's just the stuff on this screen.
But it turns out you don't need a rug and a couch, and you don't need it to be an entire room.
You actually only need about 15% of the user's field of view.
Instead of a rug, you can just project a high contrast grid on the floor.
And it turns out the horizon line is also a really important factor to help provide the users with balance.
So with tunnel vision and scaled flying, we got somewhere close to the middle of our goal.
We lose a little bit of immersion because some people notice the tunnel vision.
But we feel like this is a good combination, and it's worth it.
Now when people fly really, really high into the atmosphere, another problem emerged, which is.
people will look down and they would get tired of looking down at the earth and trying to find what country they're on.
And if the earth wasn't rotated perfectly North, they would just be completely lost. Um, so we allow users to put the earth in front of them and always keep North up.
And then we project a transparent floor under them so they don't feel like they're floating in space.
Once we did this, users immediately wanted to grab the earth and zoom in and out.
So we used the same flying mechanism to enlarge the earth or shrink it.
And we always keep the surface of the earth at the same distance.
So as it grows, it doesn't engulf them.
And to solve dragging, what we did was we put a proxy sphere on the earth where the sphere is the size of the earth's crust.
And wherever their ray is intersecting, we make sure that as they move their controller, that intersection point of the proxy sphere tracks with their ray.
And then we update the Earth's rotation to match that proxy sphere.
But when the user's standing on the earth, now that proxy sphere is the size of the earth, it's huge.
And when the user's intersection point is on the ground and they're dragging the earth, this works well.
However, if the user points above themselves, now the proxy sphere is above them, but they end up dragging themselves into mountains and buildings.
And for some reason, people don't like that.
So we came up with this cool trick, which is a proxy cone.
And we call this cone drag.
So the way it works is on the proxy cone, the tip of it's at the base of the user's foot.
And the base of the cone is where the user's ray intersected the earth geometry.
And by doing this, it means that that intersection point now just runs along a cone.
And this works really well.
Users almost never end up going into geometry like a mountain or a building.
So, you can do really cool things like this with it. You can hop from the tops of buildings.
It just feels really effortless to get around. So, I just shared a bunch of things, but if you're going to take four things away, we felt like scaled flying worked the best. Scaling, meaning scaling the user, keeping their feet on the ground, and smooth movement. But that only worked really well if you coupled that with tunnel vision.
5% of our users can turn tunnel vision off and not feel sick, but the rest need it.
And if you're going to do dragging at different elevations, cone drag is the way to go.
And putting the globe in front of users is really important for them to get around.
So, once we got all these things implemented, you know, it technically worked, but it didn't feel as great as we wanted.
And so, to get the feel right, I'm going to introduce Per to talk about that.
Thank you, Adam.
Hello, everyone.
My name is Per Kalsom.
I'm an engineer on the team.
I'm going to talk a little bit about the work we did once we had performance rendering in the navigation methods kind of figure out. Because honestly, once we were there, we thought we were very close to being done. And we were a little bit surprised when we showed it to test users and they didn't have more of an emotional response. We would get feedback such as it feels artificial, it's dead, it's silent. And I'm going to cover...
what we did to improve two of our most popular navigation methods, drag and flying, and then briefly mention a little bit what we did with background sound.
First off, so we started off by implementing drag very similar to regular Google Maps, regular Google Earth.
We shoot a ray from the controller, and we get an intersection point.
And this is kind of some sort of direct manipulation.
It moves one to one with how you move it.
And this felt pretty good when.
when the planet is very small in front of you.
But in cases when the planet was below you and you were at a pretty low scale, very small movement would move the planet.
It was almost like the planet doesn't have any weight at all.
And we want to create this immersive feeling of like, we want to simulate that the planet should be very heavy, it should be something that you can relate to.
We also identified that just by starting this dragging gesture by pressing the trigger button, it was very hard to do so without moving the hand just a little bit.
And depending on where you're pointing, this could cause this nasty jittering effect, which is very unpleasant.
And once again, it just kind of ruined immersion a little bit.
There's no sort of weight.
So what could we do to improve this?
The first thing we tried.
was distance-based smoothing.
We recognized that the further away you're pointing, the more sensitive it gets for very small changes in the controller.
And we came up with a scheme that would do much more aggressive smoothing the further away this intersection point you would get was.
And this was great for reducing that jitter effect that we saw.
But it didn't really help making the planet feel heavier.
Still, when you were dragging right below you, it was reacting a little bit too much.
So we went back to the drawing board.
And we thought, in real life, if you have a really heavy object in front of you and you push it.
it's not going to move until you have given it enough force.
And that's because you have static friction fighting back.
And we thought, maybe we can simulate the interaction point using some physics-based model.
And we'd end up choosing a spring, because then we'd be very trivial to try static friction.
And then we can switch into kinetic friction once we pass a certain threshold.
And we can choose damping ratio, so we'd get rid of overshoots from the spring.
And the cool thing about this was, like, now when you started a dragging gesture and you just moved the controller just a little bit, nothing would move.
It was fighting back.
The problem with this method...
was that it was, how do you pick your spring parameters?
Like, as Adam was talking about earlier, we have this scaled flying method.
Like, we can constantly leave the user in a different scale at any time.
How do we come up with such parameters that it works in any scenario?
Like, high scale, low scale, you're pointing far, you're pointing low.
And it got really complicated.
And it didn't really feel like, it was a little bit.
We were a little bit nervous that there would be cases where this would not work out at all.
So we realized we're probably going to end up needing something that's more robust, even though this showed great promise.
And the thing that worked the best for us was using filtering.
So in the next slide that's coming up, you remember we were shooting a rate from the controller.
So imagine that is the rate.
the red ray coming out here.
And then we take this through a filter, and we get a blue ray.
So there's going to be some difference between the red and the blue.
And so it's basically like we are applying the filtering.
You can think of it as in you're applying filtering on the controller, which means position and rotation.
And this also means that these two rays, they're not guaranteed to start from the same position.
They can have completely different orientation.
So the filtering we chose was low-pass filtering, and we have two different low-pass, two different filters.
We have a very strong low-pass filter and a much weaker one.
And when you start the dragging gesture, right in the beginning, then we apply this super strong low-pass filter.
So you basically get almost no movement at all to simulate this static friction effect we had earlier.
And the further away you get from the initial position or initial rotation, then we start to apply more of the loose low-pass filter.
which introduced this nice lagging effect that the spring gave us earlier.
And now what's good about this, since it's working on the controller, we're guaranteed it's going to work in any scale in any scenario, because the controller in relative motion to you is going to be pretty much the same.
This introduced a different problem.
Now we do intersection with this.
array that is not even guaranteed to be attached to a controller anymore, what are we displaying to the user to communicate this? Should we show the post filter array that's like disconnected?
Probably not, that would feel weird. We ended up settling for an approach where we take the intersection of the post filter array, we project it to the pre-filter array. This ended up creating us a really nice hull.
And we can use this hole to place control points in a cubic bezier.
And then we always guarantee to have a very smooth rendering curve at any time.
And just to make this look a little bit more nice, we fade out the opacity towards the end of the curve, because otherwise you end up with this awkward, thin, spaghetti-looking thing that acts as a link between you and the planet.
That worked pretty well.
We also wanted to simulate some sort of resistance, like, OK, it's not moving now.
We introduced static friction and lag.
The planet's fighting back.
We want to simulate resistance.
In real life, let's say you attach a rope to a heavy object and you pull this rope, you're going to feel this resistance.
And we thought that we could use the haptics of the controller to achieve something similar to this.
So what we do, for two sequential frames, we take this intersection point of the post-filter rays, and then we compute the delta angle between these two points and the head.
And this gives us a very nice, one scalar value that we can feed into the strength of the haptics.
And we can use the same nice value to be the input for our sound system.
So for the drag effect in Earth VR, we ended up making a slow dragging sound and a fast dragging sound.
And then based on this value, it became very easy to just blend between them, depending on this delta angle value.
So just to demonstrate.
the red lace you see here, this is like a debug render we had.
The more aggressive you drag, you see that the delta between these two becomes bigger.
And then when you stop moving, it will always catch up in the end.
And if you remember the previous slide, when the drag was reacting very one-to-one, it was very artificial, I think this demonstrates.
Now it feels more natural.
It still kind of does what you want it to do, but it feels a little bit more like we added weight.
Secondly, I want to cover flight speed.
So we realize scale flying is probably the method that people like using the most.
And the question is, what kind of flight speed are we going to choose?
Because we are constantly changing the scale.
And many of you in here probably heard, we're in the VR space.
We heard many times that doing acceleration in VR can lead to motion sickness.
And we should avoid it as much as we can.
So the first thing we tried was to pick a constant flight speed in the room space, which means that, like, how big you perceive things.
How should I say this?
you are in, you're like, if you manage your world space fly speed, like it changes, but how close you are to things is always gonna, you're gonna find the same speed.
And this worked pretty well for high altitudes, but as you went down to like lower altitudes, it felt like you ended this like area of mud.
It was super frustrating for users to fly around, like you would like get stuck, and it was like very irritating.
So we needed to do something about this.
So we realized, hmm, maybe we should go against the device and maybe introduce some acceleration.
Who knows?
So we came up with this power curve, which basically means that in room space, we fly faster at lower scales.
And then as the scale gets bigger, we slowly reduce the speed in room space.
And a cool trick about this was that, if you see here, the orange curve, we thought that the value it converges to was a little bit too low.
And we actually just switched to a different constant speed after a certain scale.
And you might think, oh, that is probably really crazy.
That's a really discontinuous acceleration.
That must feel really bad.
But in reality, you try it.
You fly around.
There's so much things that are happening.
You change the scale.
It's not that noticeable.
So I thought that was a cool trick that kind of just worked out.
to demonstrate.
So if you remember, there was this gloob effect earlier when you're flying low.
And now we have kind of like, it feels very natural.
Even though you saw the curve earlier, it's very nonlinear.
As of last week, we launched a new version of Earth VR.
And we have this new mode called Fixed Human Scale.
So instead of scaling the user, always having the feet on the ground, we now have an option to always lock the user at human scale, which means scale as well, which is this blue line you have here to the left.
And then the question is, OK, what do we do about flight speed now?
Should we keep them at this high flight speed?
This was the highest room space flight speed we've had before.
But I think what ends up happening is for very high altitudes.
you're basically like not moving at all.
And it gets even worse if you fly up to the clouds.
Like you can probably spend hours not getting anywhere.
Like you are moving in, let's say, nine meters per second, but that doesn't really matter, like, if you compare that to a rocket ship or something really fast.
So we needed something different.
We still wanted to, we think it had great value that you don't fly too fast in this fixed human scale mode at low altitudes, because now it really gave this big, it really gave this sense of scale.
Like you really felt like, wow, the planet is huge.
It actually takes me a while to get to places.
So we wanted to preserve that somehow, but still make it possible up in high altitudes to travel faster.
We ended up creating three regions.
One, the first region you see here, where the blue curve starts, this is this non-linear region.
So we slowly increase the flight speed in room space, depending on the altitude.
And then we have the third region, which is to the very far, even outside of this graph, where we have a linear flight speed model, which is pretty much the same as if you have this scale always changing and constant speed in room space.
And then in the middle region, we blend between these two modes.
The challenge here that we were struggling with, we had this case like, we had a pretty good idea how fast it should be near the ground and how fast it should be up in the clouds.
How do we make this feel very fluid?
So when you dive down and fly downwards, how do you make it feel natural so you don't end up in this new gluey region where it's like, oh man, it's so much slower here.
And I think the findings were that the longer, the bigger you make this transition region, the less of this effect you get.
So once you have all these things in place, you're flying around, it feels great, you can explore anything in the world, but it still doesn't feel like as immersive as we want it to.
We realized we need a rich soundscape that really sells this idea of flying.
So.
Ideally, in an ideal world, we have this location-based background sounds like wherever you go in the world, you can just listen to the sound and you know exactly where you are.
Unfortunately, this is kind of a hard problem because it turns out that the planet is very big and we didn't really it was not Really feasible for our project. So instead we developed these four location neutral sound that can work anywhere Divided them into four different regions So we have a ground region was like things are very calm birds are chirping in the background Then a second region like more higher up almost close to the clouds, like it's very windy, it's like where the airplanes would go by.
And then we have the atmosphere and space layer where things are much more base, it's rumbling.
And then we take the user's altitude and blend between these background sounds based on the altitude.
And a finding we had here was it worked much better to take this altitude, convert it into a logarithmic space, and then blend based on that value.
We have this feature in EarthVR.
The user can point at the sun and drag it to change the time of the day.
And this caused that for every of these level of sounds, we needed two versions, one day version, one night version.
And then we would just blend between them, depending on the time of the day.
So for the initial launch, when all of these things were put together, it looked and felt and sounded like a little bit like this.
I guess there's no sound, but...
Should we make sound effects ourselves?
Okay, and next up, now Dav is going to talk a little bit more about two of our most recent major features we added to Earth VR.
Thank you, Per.
Hi, everyone.
I'm Nadav, and I'm a virtual reality engineer on Google Earth VR.
And so Per and Adam talked to you guys a little bit about getting the basics right for Google Earth VR, so navigation and the right feel.
And we had this app.
We released this app last year.
And when we gave it to the hands of our users, we noticed the first thing everyone wanted to try to do is to go to their home.
Or in the case of the Bay Area, they wanted to go to the home that they wish they could own.
Anyways, so a lot of our users maybe cannot find their home directly just by flying to it.
So it would be nice if we could just give them a way to type in a query or their home address and to go to that place.
So, we're Google, we're a search company.
Search should be pretty easy for us, right?
Well, we identified three components that we wanted to get right for search.
The first one was input and UI, so letting users enter a query and easily selecting their destination.
Then, once we select the destinations, we wanted to make sure the users know exactly where they are in space and that they selected the right place.
And finally, we wanted to have a search pin that always tells them where their last search was so that if they lose their bearings while moving around, they can quickly jump back to where they were.
So we started with keyboard exploration.
And this was actually quite challenging because at the time, there was no good keyboard solution in virtual reality.
There was the SteamVR keyboard, which we actually started to use for prototyping.
And it was great for us initially, because we wanted to quickly set up a prototype and test search end-to-end.
However, we found that we could not use it in the final version, because it was not customizable for our needs.
And it was also not available on Oculus SDK, which we're planning on supporting at the time.
So this is where Lullaby came in.
Lullaby is an open source library for building VR and AR apps.
It's developed internally at Google, and we use it for laying out our UI, and also for providing a keyboard to the user that they can use to type in a search query.
And then we also show suggestions alongside it, so that you only have to type in a few letters and find the destination that you're looking for.
And the great thing about this keyboard was that it was both available on Oculus and SteamVR, and it worked pretty well for our needs.
So now that the user is able to type in a query and select their destination, we want to make sure that when we take them to that destination, they know exactly where they are and they know that it's the right place.
And the challenges here are, what is the best scale, orientation, and altitude that we need to take the user to preserve their context?
And when teleporting to a large area, like Paris, France, or Yellowstone National Park, we can use the same solution that Google Maps on desktop uses, where we teleport the user, we put the earth right in front of you, and we make the user scale very big so they can see the large area of Paris, or another area of interest that they're looking for.
However, in Earth VR, for smaller search results, we wanted to place the user's feet on the ground.
And that can be a big challenge, because in Earth VR, we can run into areas that have uneven and mountainous terrains, and we also have dense cities.
So we had to come up with a clever solution for this to ensure that the user can always find where they're looking, what they're looking for.
So we start with a basic scene.
We have a house and we have some terrain around it.
And the first thing that we found out was that it's important to put the user about two meters away from the place we're looking at because two meters gave us a comfortable viewing distance for the point of interest.
And we also ensured that the user's feet are right on the ground so that they don't feel like they're intersecting with the geometry or floating up in space.
Then to highlight the point of interest, we add a search pin and we ensure that the user's gaze towards the search pin is at 15 degrees down because that gives the user a comfortable resting position for their eyes.
So now that we have the spin, what do we do when the user moves around it?
So again, the problem we're trying to solve with the search pin is we wanted people to see exactly where their search was, and as they're moving around Earth, they'll be able to come back to the last search location if they've lost their bearings.
So we wanna keep this pin in view, regardless of scale, distance, or whether it gets hidden by buildings and terrain.
And we wanna make sure that it's always legible and does not break immersion.
To solve the problem of occlusion, we came up with a solution where we render the pin at two passes.
We render the pin in a first pass with full opacity and with depth testing.
And so that's what you see when the pin is not occluded by any buildings.
And we also render a second pass with 50% opacity and without depth testing.
So that when the pin gets occluded by buildings, you can still see it.
but faded out a little bit, which ensured that we don't break too much immersion with a behavior that you won't see in real life.
So what do we do about scaling the pin when we move further away from it?
Or when we go up in scale?
We begin with a naive approach of just keeping a constant scale for the pin in world space.
And what happens when you scale up is the pin starts to shrink a little bit.
It's still visible in higher scales.
But when we go all the way to planet scale, the pin can become very small and not legible.
So we wanted to make sure that the pin is still visible and legible even at these far distances and high scales.
So what we found worked pretty well is using a formula that uses the distance in world space between the search pin and the user's head.
And we multiply that by some base scale in world space with dampening so that as you go further and further away from the pin, it still scales up and it still stays in view.
But it does so more slowly, so it still appears like it's getting further and further away from you.
And this ensures that we don't break immersion, but we still keep the tin in view.
And the solution also works when we scale up the viewer.
So to recap all the things that we needed for search.
So these are the three components that we really needed to get right in virtual reality.
We needed a rock solid text entry, which was not available before, for users to enter a query and select their destination as fast as possible.
Second, we needed to take user to the desired point of interest at a comfortable distance and a viewing angle.
And we needed to make sure that the pin stays always visible so that as the user is moving around they know where their last search was and they can get their bearings that way.
So we had a launch for EarthVR exactly last week.
And we, for that launch, focused on enabling Street View, which was another one of our most requested features.
And we've actually been thinking about Street View for a long time.
So in the summer of 2015, the Earth VR team went to see Inside Out by Pixar.
And we saw the memory spheres in Inside Out.
And we were really inspired to try to do something with memory spheres in virtual reality.
So we came up with a prototype that uses photospheres placed around Manhattan.
And the users can move around Manhattan, point and click at a photosphere, and put it on top of their head, and view the entire full 360 photo.
We found it was a very compelling use case for photospheres, but we also thought that, or we also at the time were focusing on getting the core features of navigation in Earth VR.
So we had to table these ideas for later.
But a little bit about why Street View is important for us is because a lot of cities on Earth VR do not actually have 3D data, like Tel Aviv here.
But the city does have Street View, and it would be awesome if the user can explore the city using Street View rather than just looking at a flat satellite image.
Even in cities that do have a street view, like Paris for example, we can still get a lot of value from jumping in a street view image because we can see the city at night and we can see the people walking around in that city.
And we believe that it also provides a much more immersive and intimate experience.
So the two components we identified for Street View to be really good in virtual reality were showing Street View availability, so telling the player when they will actually be able to enter a Street View panel.
And secondly, we wanted to give users an intuitive and quick way to enter and exit Street View.
So we anchored our initial explorations and prototyping in the solution that's in Google Maps.
So Google Maps gives you coverage layers that show where Street View is available in blue, and they give you this peg man that you can just drop into place and enter the panel that's in that location.
So we experimented with this in Google Earth VR, and we also tried to enable entering Street View using these coverage layers.
But we found two non-trivial, two problems with these coverage layers.
And one that it was non-trivial to overlay them on top of the earth because the terrain is uneven and it can look pretty odd when you get up close to the road and you see this blue line around it.
And in places that have a lot of street view coverage, it can get pretty messy to render so many lines around you.
We were also inspired to try out the pegman dropping gesture from maps.
That's all I'll say about that.
So we had to...
we had to go back to the drawing board and find a better solution for street view and virtual reality.
So what we found was the best experience was let's say we have a user standing around the city and they have their controller with the globe or map on their left hand.
And right now street view is not available because the user is at a high scale.
So we have a threshold, visualized in blue, that once the user goes below, we show you a ripple effect, notifying that Street View is available, and then we also put a preview of your Street View that's showing up right below your feet.
So you can see a quick video of how this works here, where the user is flying down, and as they go below the threshold, we pop up a Street View preview image.
And as the user flies out of the Street View, we show an icon to display that it's unavailable.
For entering Street View, we first tried letting the user hold the bubble.
point at it with their right hand, and then pull the trigger to enter.
We found that that was a pretty easy way to enter and exit Street View, but our users didn't intuitively understand this gesture.
What they tried to do is take the bubble and put it directly on top of their head, just like the Photospheres demo that I showed you earlier.
So we listened to our user feedback, and we implemented this gesture that allows the user just to quickly put Street View over their head, and then to pull it out when they want to exit.
To summarize, these were the two components that were really important for Street View and Earth VR.
The first one, notifying the user that Street View is available on their left controller and showing a preview of Street View on their left hand so that they can choose whether they want to enter or just skip that current Street View panel.
And secondly, we needed to provide the intuitive peek gesture of putting the bubble on top of their head and removing it so that they can do so very quickly.
And thank you very much for coming to this talk and listening to us.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
And we'll also do a Q&A now.
If you want to come up to the mic and speak to the microphone, we'd love to answer your questions.
Hi, guys.
Great talk.
I had a question.
When you were doing user testing, how did you quantitatively measure user comfort and discomfort with the various UX components?
You have to, I think, press the button.
Oh, there's a button.
OK.
Really? Does that work? Okay.
Yeah, so the question was how do we test for discomfort?
Well, before even testing, we needed to recruit people, and we needed to understand sort of their typical level of discomfort.
So we asked questions like, when you're on a bus, and you're working, do you get sick, or do you get sick when you're flying?
voting. And so we did as much pre-questions as we could to sort of screen, you know, is this a person that typically gets sick? And we actually isolated studies where we had.
people, large groups of people that weren't susceptible to sickness and large groups that we knew were extreme candidates.
And then we would do things like when we were experimenting with tunnel vision, we'd have like tests where we make them fly down a street with buildings flying by and basically like put them in the worst case scenarios.
I'm not exactly sure like what the researchers did to mark down What happened beyond just like is the user feeling sick or not?
But they did they did follow up with them you know during the study after the study and then like a few hours after the study and even the next day and Because sometimes the effects of VR don't manifest themselves until they leave the room and then they go back to work and they're like, oh man, I have a headache.
So it took a lot of work.
I think that was probably the hardest part for that.
This may not be the right question for the devs, but what is the licensing?
We create school curriculum.
What's the licensing for use of this in that type of a product?
Can you clarify? You're asking about the licensing for Google Earth Creator?
We do. One of the things are field trips, virtual field trips to Kitty Hawk or the Redwood Forest.
Instead of just having a field trip there with a 360 camera, it would be cool to intro that by coming in from above, landing there, and then doing the virtual field trip on location.
So currently we don't have like an open license or way for developers to use this But it's something we're really interested in Hey guys. I know Google has filed some patents on some of the UX that they're developing in VR.
I'm wondering what you guys feel about that.
If it should be something where if you found something that really works, that it should only work for your company or...
Oh, not at all. Google files patents that are defensive.
So we're not interested in blocking other companies from using these things.
which is why we're sharing this stuff, and we hope that you guys can learn from it and build on it.
Having used Google VR for a while, I found it to be a really wonderful experience.
I'm just wondering what you guys as the devs think of the future applications for this particular product.
What do you want Google Earth VR to be in two years from now?
It's a very loaded question, but a great one too.
We have a lot of ideas.
We're still trying to figure out our direction as well as we go.
We're always trying new things.
We're always experimenting with new ways that Earth can be in VR.
Hi, for your testing process, you said you recruited people.
So were they various levels of expertise as far as computer use?
Yeah.
Yeah, during the recruiting, we would also screen for experience with gaming and VR.
So we had checklists like, have you tried Cardboard or Gear VR or DK1 or that sort of thing.
So yeah, so that was part of our one way that we sort of we're more sure about our findings, in terms of how experienced people were.
Great. And to just springboard off that, what size of a pool for a prototype would you find, okay, this size pool gave me useful data on a prototype, whether it was working or not working?
Yeah, so, I mean, if you've ever done a user study, like, you could bring in like four or five people, and if they're all, you know, throwing off the headset or getting frustrated, you know you've done something wrong.
So, you know, sometimes we'd try things and we just immediately know we got it wrong, and then the next day we had people queued up, we would tweak stuff that night and try things out.
So some iterations were much quicker.
And then some were much slower where everything we were trying wasn't working or it was subtle.
We weren't sure where things were breaking and for those we ended up having to do longer term studies over months.
Hey guys, cool stuff, thanks for sharing.
You know, I've observed the same thing as you, which is a lot of people go view their own home or places they've lived as a first experience, and I think that's awesome.
And building on that, I feel like making this experience more personal to individuals would be like a really important thing to do moving forward.
And so like, I just wanna give you an idea.
I do a lot of mountaineering, and I have a decade worth of my GPS tracks that I use a lot on desktop Google VR, or Google Earth.
So I was just wondering, you know, are you thinking about supporting KML or GPX file formats and when?
Or, okay, here's another one. How about live location data from like, I want to see where my wife is right now.
Thank you.
Not sure about the, is this on? Yeah, not sure about the last one. But I think supporting KML data, it's like, have been brought up before. It's just been more of like, Prioritizing things, it's cool.
I think in some cases, like, we thought S3D was cool for the time being, and now we just have to sit down, like, okay, what's next, what do we do?
We're not sure, but I think it's great, great suggestions.
Hi guys, thanks for the talk.
Google Earth is also for me one of the best experience, super intuitive.
When I tried it last year with my colleagues, we tried one by one and we could see with the screen what is doing the person who is in VR.
So my question would be, do you have in mind to maybe make it like social directly, multi headset with two or more people at the same time?
I think that's a really cool idea.
Once again, a lot of the base of application is built in native.
So we have all these great ideas.
Some things take longer than others.
And we just have to rank them and how important they are.
I think I've heard this idea before.
And it sounds awesome.
I would love to sit in front of a computer and maybe take whoever's in the headset doing tours or something like that.
I think it would be great.
Hi, so first off, like the other people said, spectacular work.
It's a killer app.
Everybody loves it.
So Bravo.
Um, two questions on the, um, uh, the, the, uh, tunnel vision, I believe you called it, a feature that you did, you said that 15, enough, it was enough to give about 15% of the, of the visible pixels kind of view of the grid or the floor to let people ground themselves. Some of the views you showed were kind of pre distortion versus after. So was that, uh, of the total or was there also kind of a blacking out, uh, that was also being done?
Oh yeah. In the examples I showed, I did I blacked out to simulate more what it looks like when you're looking.
In the other screen recordings, they were from the view that's shown on Windows, and that doesn't have any blackout.
So it looks like a lot more, but when you're looking through, you're actually only seeing this little trim.
But effectively that's the end the end result that you were simulating. Yeah, okay, and then the the second question was with regards to the You know all of the controller schemes you showed were using the vive controller, and you talked about wanting to support oculus There's a ton of people working on other controllers and gloves and all kinds of stuff have you thought about Are you going to support these one at a time or have you thought about making it kind of, taking your set of functions and then providing a way for people to map them to other controllers you haven't thought of?
Yeah, we could provide a mapping file that people could do, but to get the, to make it intuitive for people to just grab a controller and look down and see tool tips and all this stuff, we have to do custom work for each one.
like just doing Oculus was pretty different.
We had to rethink how things were mapped.
We even had to tweak the controller model a little bit to make it more intuitive.
So we'd like to support everything, but we're just currently just on track to support the major ones that people are using.
Okay, thanks.
Yeah.
Just one more thing about the tunnel vision size.
The size we picked was a little bit larger than what we actually needed, and that's because when people have glasses in the headset, or with some headsets you could push the screen a little further away from your eyes.
So when you had it at that max setting, you couldn't see it.
But if we, you know, in the future, potentially, we can control that a little bit better, or maybe even know what setting that's at, we could even reduce tunnel vision quite a bit more.
So, yeah.
Hi, thanks for the great sharing.
So, Google released the ARCore a few weeks ago, so I'm wondering.
if you are currently designing the UX for the AR or mixed reality?
If not, so what are the main challenges or problems you are anticipating regarding to designing for that part?
It's something that we would like to do.
We have experiments where we've brought Earth into unity and tried things like this out, so it's not particularly difficult, but it's something that we're really interested in.
Cool, thank you very much for coming.
