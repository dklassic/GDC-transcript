All right.
Hi, everyone.
My name is Alexander, and I'm here to talk about the rendering of Memories Retold, or how to beep up ROS with shaders, I guess.
Before we start, I'd like you to cancel, to silence your cell phones, and to remember to fill out the evaluation forms after the talk.
So before we start talking about the Memories Retold, I am going to talk a little bit about myself.
I feel very weird giving a graphics talk, because I am actually quite sight impaired.
I only have 16% eyesight.
It's specifically because of this gene right here.
And I know that because they actually used a gene sample for me when I was one as part of the Human Genome Project.
So I guess you're all welcome for that scientific discovery.
And you might be wondering, why do I end up making games, which is such a visual medium, instead of continuing on this very promising career in genetics I had started so early?
And it is kind of ironic, but for me, I've always been attracted to video games because they're these constructed environments that are meant to be traversed and be discovered and read at a glance.
When a good level designer makes a level, they will use something called a squint test, where they will blur their vision and see if the level is still legible.
I can't stop squint test.
I do it all the time.
And let me tell you, reality doesn't hold up to it.
So having games where the environment has actually been designed to be legible, it's not so weird I ended up doing this.
More specifically, with.
painterly aesthetics and something that's not trying to be photorealistic, quite often it's a lot easier to read, because artists are thinking much more about the base shape of the objects and you don't have all the noise of the real life coming in the way.
So for example, playing a game like The Return of the Obra Dinn was something I really enjoyed, because everything had clear outlines and was easy to tell apart.
So when I got the chance to work on 11.11, I jumped at it.
It's a narrative game set in the First World War.
And it tells the story from both the German and the Allied side.
It's a co-production between Art and Animations, Didixart, and it's published by Bandai Namco.
Team size was around 30, and the team was distributed all around the world.
I think that's like a talk in itself, how we managed to do that.
We used Unity 17.3 with a modified legacy deferred renderer.
And development time was approximately six months of pre-production and 10 months of full production.
And the game was released in November to commemorate the centennial of the armistice, and hence the name.
As you can see, we kind of have the release date in the name of the game, so we could not afford any delays.
So that was a very interesting experience, but we managed to do it in the end.
Oh, in case you haven't seen the game, I'm gonna show you the trailer so you can get a sense of how this painterly art style looks.
So if we can zoom in here.
The first time you see someone die, everything shuts down.
You're left with thoughts that go round and round.
Every man has his demons.
I did what any father would do.
And I would make the same choices again.
I just did what we all did.
I followed orders.
Yeah, that music always gets me.
All right, so here is the outline of the talk.
I'm first gonna give you an overview of how the painterly render pipeline works, and I'm then gonna go over some challenges and snazzy features that is in it.
After that, I'm gonna show you how the artist used the pipeline, and following that is optimization of it.
And then at the end, I'm going to have a very short section on future improvements.
And at the end, there will also be some time for questions.
So all these interstitials you're going to see throughout the talk are actually the concept that we were working towards.
So that should give you an idea of where we were at the start and where we ended up.
So...
Whenever I start on a new project, and I have to develop new tech for it, I look at existing white papers.
Specifically when you're doing graphics programming, if you find papers that are kind of old, it means that they're quite likely to be something you can actually achieve like for a commercial project now, because you're also gonna make sure that you have enough headroom for the rest of the rendering taking place.
Thankfully, I found these two papers.
that are both using particles to approximate brush strokes.
So you get this painterly look.
Sorry about that.
There we go.
OK.
There we go.
So what you see here is two papers that are, one is completely object-based, and the other one is working full screen.
Especially the one on the right showed us that this is actually an approach that could work.
And the way that it works in the game is we have the base frame, the base frame buffer, and on top of that, we paint all these particles, and then they are combined together.
The first innovation we did was just painting these particles on top of the frame buffer.
Already it meant that it looked a lot better than what had been done before, because we could close all the gaps in the particles.
And it kind of resembles how if you're using a camera obscura, like you're basically taking a picture and then you're painting the strokes on top of it.
On the technical side, all of this is based on a technique developed by Garrett Thomas from AMD.
It's all based on compute, and it's a tile-based approach.
I will only do a very quick overview of how it works because he took a whole talk explaining in more detail how each step is put together.
But his slides are readily available on the Vault if you need to check them out.
And there's also a Git repository.
Right, so this is how a frame is constructed.
We first have the regular frame buffer rendering.
And together with that, we also render a specific brush buffer that contains extra information for the effect to place the brush strokes.
After that, we do bloom and ambient occlusion.
And after that, we start the actual rendering.
Sorry, my notes are.
Can I do this?
Nope.
So we generate new strokes, and after that we update the strokes that are currently on the screen.
Some of them are deleted and will die out.
Yes, there we go.
And then we end with calling and course calling.
All of this is basically to make the rendering of the strokes more efficient at the end.
And we also do blending with the background image as part of the brush stroke rendering.
And when all of that is done, we have the color grading and tone mapping.
This is the different aspects that the artists have control over.
And what you're seeing here is the different debug views that they have.
So they can control the size of the brush strokes, if the brush strokes should boil, and if they should flow.
If they boil, they basically die out quicker.
So you get something that kind of shimmers.
It's really good for interactive objects in the game, something that you need to stand out.
Since this game is about the First World War, a lot of the levels take place in the trenches and they're like really wet and muddy, so we needed to show something that was like running with water.
And that means that the artist can actually go in and say, like, I want this, the strokes to flow along the surface.
And they flow along the direction that the artists have also instructed the effect to use.
Something that is also quite unique is that we can actually select what brush stroke we should render with.
So that way, you can have something that approximates, like, a very dry brush being kind of, like, dabbed onto the canvas, but you can also have something that's much more...
wet and, like, long longer.
So there's a lot of flexibility for the artist to use there.
This is the format of the brush buffer.
So...
Size, flow speed, and angle each get a full eight bits because we need as much precision there as we can get.
For brush type, I'm only using six bits.
And then I have two bits left for boil speed.
The reason for this layout was predominantly that we had to integrate this into the deferred buffer.
And gamma correction caused some of these values to be kind of wonky.
So it had to be laid out this way.
So an early challenge was to make sure that the brushstrokes would actually be aligned with the objects.
And that's both, they have to respect the object's own orientation, as you see with the magenta arrows.
And they also had to work with perspective distortion.
And it took me a while to figure this out, but what I ended up doing was that I would take both the current fragments' position in world space into the fragment shader, along with the world tangent and cotangent.
And that way, I could generate an offset point and take both of them into screen space and get a direction vector.
and then take the, get the radiance from that and then store that as a normalized radian into the brush map.
This all sounds kind of expensive, and yeah, I was using an ATAN 2 for that, but it turned out to not be that big of an issue as we're gonna cover later in the optimization step.
There we go.
So.
When new strokes are created, it happens in a kernel that takes the whole screen and for each pixel generates a random number.
Then for, um, to modify this spawn chance, we look at the current brush size we want here and the flow speed along with the boil speed and also the alpha coverage of the strokes that are already there.
So if we don't have a good amount of coverage, we want to create more brush strokes.
And all of those numbers are added together.
And if there are more than one, a brush stroke is created.
And that is by taking an index from a consume buffer and use that to index into all the data arrays that contains all the information for the strokes.
So it's important that the RNG is very uniform.
To start with, I was using a noise texture.
And I kept getting strokes planted on top of each other, which was a.
Both didn't look good, and it was also not very performant.
So at the end, I used WangHash, which Cabbie Games covers in the render talk on below.
It's very easy to implement, and it is producing some very nice noise.
So I can recommend using that.
So after the strokes are created, we move on to updating all the strokes that are already on the screen.
The first thing is that the precision is updated.
And then we get the depth and color from the gbuffer.
We also get the brush type, flow speed, boil speed, and size from the brush map buffer.
And if the surface is flowing or boiling, the stroke will die out faster when we are applying the lifetime to the stroke.
A stroke is removed, that is, its index is added back to the consume buffer from before, if its alpha hits zero.
So in order to make the effect not look like you're watching it through a screen door.
It's quite important that it moves with the scene underneath.
So what you're seeing here is the same scene, but where, on one hand, they are completely static.
They don't follow the scene.
On the right side, they are using motion vectors to move along.
And especially if you're looking at the door frame, you can clearly see that there's a lot of flickering going on if you don't do this.
Thankfully, in Unity, you can just say, I want motion vectors, and you get them, and they turn out to be of a pretty good quality.
Next step is coarse culling.
This happens with the box-box intersection.
And testing revealed that 16 by 8 culling tiles was the most efficient number.
When a collision is detected, the stroke's index is added to that tile's index buffer.
And a very similar approach is used for the next step, the fine-calling.
Uh, here each tile is 32 by 32 pixels.
It's again a box-box intersection testing, but when we're done with that, we also do a step of bitonic, uh, sorting based on the depth of the particles, um, to get correct alpha blending later on.
And the bitonic sort is the same that Garrett Thomas used in his talk.
So the render kernel is what really is the most different from Garrett's approach.
It starts by loading all the stroke data into LDS.
So it's faster to retrieve.
We then set the accumulated color to zero.
And we then, for each thread, take care of one pixel.
We loop through the particles, and it's essentially a ray cast through them.
So if the ray hits the stroke, we sample its stroke texture and combine it with the stroke's color and add it to the accumulated color.
By using a custom alpha blend, we can actually go front to back instead of back to front.
We didn't see much of an improvement.
In Garrett Thomas's talk, he sees a huge performance boost from this.
But it's mainly just because he's actually doing particles in world space.
And he shows that you can go very close to them.
And then I just left it in because it wasn't hurting performance either.
All of this is rendered into an HDR-ready back buffer.
So we avoid banding.
And the alpha value stored here is is what is used in the generation kernel to determine how much coverage we have.
So strokes are stored in an alpha 8 2K texture.
We found out that distinctive strokes are actually way more important than quite diffuse and soft.
I personally thought that was actually going to work better.
And it turned out I was completely wrong.
And different sprites give impression of different surfaces.
So for example, for wood, we ended up using quite long and hard strokes.
And then for the cat in the game, we had some that were much more like hairy and bristly.
All right.
So let's cover some snazzy features.
Early on, the biggest issue we had with the effect was readability.
which shouldn't really come as a surprise, I guess.
What you're seeing here is actually a screenshot from the vertical slice a couple of months into production.
And this was a bit of a crisis moment, because if we couldn't get this to work, we would have to abandon the pencil effect and do something completely different.
You might not be able to see it, but there's actually a man crouching on the ground.
So this amount of, yeah, blurriness, that just wouldn't work.
For the vertical slice, what we ended up doing was just increasing the amount of particles so they would approximate the shape of the scene more.
But for the full release, this didn't look that great.
And it also wasn't performance, so we had to find out a different way to do it.
So the reason why this happens is that we're painting all the strokes on top of the background image.
And.
That basically means that if you have a stroke that is from the background, it's still going to cover something that is in the foreground, such as Kurt's arm here that's being obscured by the particles from the sea behind him.
So the trick was to sample the depth buffer.
in the render kernel so we know what is the absolute depth we should traverse through when we go through all our particles.
So we get like all the particles that are belonging to Kurt's arm, but we stop when we know that the particles that are behind him would just lead to like covering up the what's in the foreground.
So this works, but you see that we get these, like, quite hard edges on objects, which is not painted at all.
And in order to fix that, we end up blurring the background with a Kawase filter.
And the reason why I ended up using this is because it's quite performant.
It takes a little bit of, like, manual tweaking.
So what you do is, at each step, you sample out in a cross of four samples.
And if you use, uh, bilinear sampling, you end up with double the amount.
Uh...
So it's quite easy to use.
It does require a little bit of manual tweaking, so you don't get the banding artifacts you might be able to see in the projector.
But yeah, I recommend using it.
Something that was also important was that objects would have the correct size of strokes attached to them.
So if something recedes into the background, and it's important that it's still legible, we need the strokes on them to be smaller.
And that was quite easy by just taking the depth of the fragment shader and used that to increase the brush size.
Interestingly enough, we actually ended up having this something that the artist could control with a modifier.
And they would sometimes use it in reverse.
So if you have something where you want to guide the eye of the player, this is like a technique that's also used in regular painting, where you will use detail where you want the viewer to look.
And this meant that we could actually get something kind of similar in an interactive environment, which I think is pretty cool.
It also had to take in the camera's field of view.
So in the game, one of the characters has a camera and can take pictures.
And in order to have zooming, we changed the camera's FOV.
And it's not really because you have a different distance to the object, but they can still be smaller or bigger on the screen.
And I fixed that by just having a curve for the artist to tweak where this would be added in based on the current FOV of the camera.
Early on, it was all about getting as many strokes on the screen as possible.
We thought that this is how we're going to make it look really good.
So we had around 2 million strokes at the end of the vertical slice.
sort of like halfway through development, we started to realize, actually, it's better if there's fewer, because they look quite artificial if they're placed right next to each other.
It looks like this manic robot has been like, ah, I need to paint this as good as possible.
For longer strokes, because they will have the same height as short strokes, they will cover a large area.
So we need fewer of them.
And I basically just took that into account when generating the random probability of them spawning.
And this had the added benefit that it looked better, but we would also get better performance because there was fewer brush strokes on the screen.
So for example, here, if you look at the image to the right, the beams look a lot more painterly and human-made.
because there's fewer strokes on them.
The second artist problem to solve was flickering.
So the strokes kind of act as a...
as a downsampling of the base framebuffer.
And this causes the scene to be, uh...
become, like, very noisy.
when you're moving the camera around and when objects are moving.
And the solution to this was to sample the color of the brush stroke and across.
So we wouldn't just use the center of the stroke, but we would also look around it.
Um, here I would actually use the brush type as well to basically do a bit of rejection sampling.
So you kind of want the stroke to be colored by only the object that it's supposed to represent, but we don't have that data available.
But there's a good chance that...
it's only going to be the object itself that has the exact same brush type.
So it's like something in the background is likely going to be using something else.
So this was like a cheap and quick way to kind of fake that.
We also found out that the motion vectors from Unity are a little bit noisy.
So even when nothing is moving, there would be a little bit of drift.
So by applying a simple threshold, that also took care of that.
So the camera would have to actually be in motion, or an object would have to be in motion before you would start to have the stroke changing shape.
Oh, sorry, changing color.
So in the real world, when you're looking at a painting, you can actually pick out the individual brush strokes.
That's because each stroke will catch the light a little bit differently because of the inherent impasto.
Of course, we can't actually do that.
because we don't know the lighting conditions of each player's room that they're playing in.
But we can approximate it.
And that's simply by randomizing the color and also by randomizing the angle.
We actually get something that looks a lot more like handcrafted.
And it's also kind of like part of the pointillism, which is part of impressionism that inspired this whole art style to begin with.
So the way that this randomization is done is, um, I tried to first use HSV, but I got a lot of, uh, like, flickery and edge cases where it wouldn't work.
So in the end, I had to come up with something where, because all the other colors was in RGB, like, the randomization of them also had to be in RGB.
And by looking at the color space, like the RGB color cube, you can actually see that you, if you...
Look at it from the right direction.
It kind of resembles HSV anyways.
So if you take a vector that goes from pure black to pure white and use that to form an orthonormal set of basis vectors, you can actually use those to add color randomization.
It's by no means perfect, but it's good enough for the small amount that we needed.
And it turned out to also be much more efficient, because we didn't have to do the conversion to HSV and back again.
So we had to keep the randomization that was happening to each stroke constant from frame to frame.
Otherwise, the stroke would keep changing color.
And the only constant set of information I had for the stroke was actually its index.
So I wrote this function you see here where I get a, like, a sort of randomized, like, value out that is then used to apply all of these offsets.
So this is some of my favorite features.
We can actually handle particles in two different ways.
So one is just using an alpha cutout standard particle shader that writes into the brush buffer.
The good thing about this is that it's a workflow that the artists are familiar with.
And it also allows the strokes to be lit.
So you can see we're using that for fire quite a lot.
We can also inject shuriken particles, Unity's particle system, directly into the effect.
So they appear as brush strokes.
And I thought, personally, we were going to use this everywhere because it's damn cool.
But unfortunately, in the Unity's deferred renderer, you can't actually access the lighting data.
They required a lot of artist tweaking in order to sit well with the scene.
So then we only used them in very rare circumstances.
And most of the time we ended up using the alpha color particles instead.
They were also kind of a pain in the butt to implement because there was a lot of unique rules for them because they wouldn't get that color from the scene.
So for example, having them work with meant that I had to take that equation and actually put it into the injection code or, like, the injection compute kernel.
And also, um, it's why we had to split up our post-processing, so some of it would happen before the pencil effect and some of it would happen afterwards, which incurred a performance cost.
Um...
Yeah.
So, good idea.
Could have, um, been better if we had, uh, uh, more data available.
All right.
So how many of you are graphics programmers?
A fair amount.
OK, how many of you are artists?
Also a fair amount.
OK.
If you've ever been part of a production, maybe you can recognize this.
I will say at times it was kind of stressful.
So we thankfully had some pre-production on the game.
And I think that is what really allowed us to achieve the look we had in the end.
At the start, it was only me and the art director.
So I had all the, I could focus completely on just trying out various things and get something he was happy with.
But I also think it's like a lot of the reason why we ended up with something that worked is that I was constantly in dialogue with the artists about like, well, how should we get this pipeline to do the things that you guys needed to do.
I would also say the artists were like really nimble to adapt to how it worked because it's as you're going to see now it's quite weird workflow and it was definitely something that none of us have tried to do before.
So the artists have three levels of control.
They have globally on the camera, where they can adjust variables such as the maximum and the minimum brush size, and how long the brush stroke should live, and so on.
Then they can also go in and adjust it on a per material basis with overrides for brush type, brush size, and so on.
And finally, there's also on text level a texture that in format resembles very much what's in the brush buffer where they can go and have the maximum amount of control.
So we used Substance.
for most of our texturing work.
And it has a lot of advantages.
You can reuse graphs.
So if you create a base wood material, you can reuse that to create a lot of variations very quickly.
It also allows live editing.
And I think this is what was actually the most beneficial for us.
like I I was the only graphics program on this project so I didn't have time to do some kind of like custom solutions so they could actually see how the models would look like in the treaty modeling tool so they always had to take them into unity to see the end results and if we hadn't had this light like like live editing it would have taken so much longer.
I have an example of that.
So, what you see here is a substance material being edited in real time, and you can see that getting all these variables of like what directions are the strokes going in, and how big are they, and like how much boil do we have on them, like not having these exposed so you could like play around with it, would have taken a lot longer time to find settings that works.
There was also a lot of problems with them.
This is mainly due to the version of Substance Integration that we had to work with.
We had very long bake times, and it often broke on console builds.
It would also sometimes crash the Unity editor when we imported them.
And it actually, and then we had to.
Like our tools program had to spend a lot of time on developing this tool that would allow us to switch between substances and baked out materials for this for to allow us to actually ship the game, so Thankfully, there's like a new integration on its way, so hopefully that should Completely remove all of these problems There was also like some guidelines for creating good textures.
If we did something that was approaching real life, you have something that's very noisy.
And that noise causes the brush strokes to change color very quickly.
So you get something that flickers.
But on the other hand, if you just blur all the textures, you end up with something that doesn't really have any definition.
And the image ends up looking really flat.
So having something where you get this kind of like a posterized look turned out to be the best.
and having a bit of like smooth gradients also helped.
We would also exaggerate the main form slightly so they would read better underneath the effect.
And this is kind of like the same principles that would be used for modeling as well.
As it came into flavors, we would have hero assets such as characters and assets that were used often.
They would be hand painted in Substance Painter, whereas the rest would use substances from Substance Designer.
And the good thing about those is that the only thing we needed to do was to do a correct UV unwrap, and the model would be done.
So four characters.
and other hero assets.
This is the process.
We would do the same texture method that you saw earlier.
But we would also paint in extra highlights and shadows into the albedo map to kind of make the characters pop even more underneath, below the effect.
So.
painting the like hero assets, like as you saw earlier, the format for the brush map buffer is that's not artist friendly.
So in Substance Painter, the artist would paint into these custom defined channels that would control each aspect of the brush stroke.
And when we then imported it into Unity, a custom editor script would.
then take all of that information and crunch it down into the texture format that the pencil effect was expecting. Specifically for characters, we found that we actually needed them to pop a lot more. So we wrote this.
uh... kinda like highlight shader so it is it is uh... it is a a normal rim light but it actually takes the lighting in the scene into account so it's just uh...
so it so the character sits more with the scene but you still get like uh... them standing out more from uh...
from the background and objects around them which worked really well I think I'm doing well for time, that's good.
Okay, so, um, this was the first project where I was a graphics programmer, and I think for more experienced graphics programmers, a lot of this advice is gonna see kind of, like, basic, but in my experience, it's actually kind of hard to find this stuff online and find tutorials on it, so I hope it's gonna be beneficial for people out there that there's a place where they can actually learn about this.
So performance target-wise, we looked at GCN as our main architecture to optimize for, because it's the one used on PlayStation 4 and Xbox.
We targeted full HP on PlayStation 4 and 90p on Xbox.
And yeah, I would advise you don't give everyone on the team a 1080 graphics card, because you're sitting there.
I was sitting there as the only graphics programmer being like, oh, we should probably start optimizing.
And yeah.
It's, we had to, to get, um, about halfway through development before we started to really, um, take this into, into account.
Um...
So for, uh, optimization and for profiling, I used, uh, GPU eraser quite a lot.
There's other tools out there, but I really like the way that it's presented.
I think Sony does a really good job with their tools.
They're easy to read.
And not surprisingly, we found out that the main thing that had to be optimized was to render kernel, because in a standard HD resolution, you have around 2 million pixels.
And for a given frame, we would have around 30,000 strokes.
There.
kernel creating the strokes and updating the strokes, they were quite, like really cheap.
Like, that wasn't the worry.
And it also meant that I didn't have to worry about putting in new features into that part of the pipeline that much.
But anything that affected the rendering had to be considered quite carefully.
So something that's quite easy to stumble into when you're using Unity is that you're gonna, uh, coupling your render thread to the main thread.
And what this means is that you're not gonna be able to have your render thread, like, process the last frame while you are doing the logic for the current one.
And...
It's a lot of it. This is not really that well documented at the moment.
For example, if you use any of the methods in the graphics class, you're likely going to cause this coupling.
And the solution to that is actually to use command buffers.
which is not really explained in the documentation either.
It's presented as a way to extend the graphics pipeline, which is true.
But if you use them, you're also making sure that all the commands are executed on the render thread.
So.
Scriptable Render Pipelines was released, or like, was starting to get previewed during the production of 11.11.
And I wish we could have switched over to it, but, I mean, it was still an experimental feature.
So, originally, we were generating the BrushMapBuffer with replacement shaders.
And they are...
terrible. They basically causes the camera to call again and you end up with more draw calls and you also end up causing coupling with the main thread. So...
With scriptable render pipelines, it would likely have been easier to fix this because I would have all the calling results readily available.
But I was stuck with the deferred renderer, and I was also stuck with a non-source code version of Unity.
So there was...
Yeah, I had to be quite creative with how I went about solving this.
So what I did is that I stole a buffer in the G-buffer.
So normally, Unity stores the specular value completely separate.
But that's actually only constructed from a specular value, or a specularity value, and the albedo color.
So by using an unused channel to store the specularity, I freed up a whole.
a whole buffer where I could store all the information that I needed.
It did mean that all the lighting equations got more expensive because that color just wasn't readily available.
You had to recreate it for every light.
But in the end, we didn't have that many lights in the game anyways because we were concerned about having as much performance reserved for the painstaking effect anyways.
So this turned out to not be an issue.
But it didn't mean that all the deferred lighting shields had to be changed.
Thankfully, that's something you can actually access in Unity quite easily, as you can see here.
So it turned out to be easy.
What was not so easy is that Unity has a system for generating.
shaders that work with both the deferred and forward render pipeline and all the lighting that goes on there. But of course, they would be expecting the normal deferred color layout or like buffer layout. So in order to fix that, I wrote this like kind of dumb find and replace tool.
So I could go through the generated shader code and replace it, but it was not a great workflow because I had to hit a button on.
on the shader, you can actually access the generated code like through scripting.
So I would always have to go in and hit a button to show the generated code, copy it over into a separate file, and then run the patching system on that separate file.
So yeah, it wasn't fun.
Some objects would be forward rendered.
So if we have something like a special effect shader that we're used in certain sequences, they have their own, like, lighting going on, so they can be rendered as part of the regular deferred pass.
And this also meant that they would have to be rendered into the brush buffer separately.
The way that we would do this is that the artist would add a script to the object and instruct what materials should be used.
or should be rendered that way.
And the effect would then generate a separate command buffer that would run after the deferred pass was done.
And that way, we would get that information into the brush buffer.
This also meant that I had to inject a special shader pass into the shaders, which was also taken care of by the patching system you just saw.
All right, draw calls.
So most games I've been on, draw calls in Unity is like something you really have to look out for.
And there are more and more tricks you can use to reduce them.
So a fairly new thing is that you can use layer call distances on the camera to basically say, well, if you know that something is quite small, you can call it out at a shorter distance than the far clip plane of the camera.
We would also combine meshes when necessary.
And Unity's static batching is quite temperamental, so it also took our artists a lot of diligence and experimentation to make sure that we had everything batching as much as possible.
So occlusion calling is also something that sometimes led to better performance, sometimes not.
So we would test it on a level-by-level basis if we should have that enabled.
So data packing is a pretty cool technique.
What this allows is that you can fit more information into less space.
In our case, we used it for fitting more strokes into LDS.
And we also reduced the amount of VRAM that the effect required, which was basically freed up texture space for the rest of the rendering.
So it can also increase performance because you have less data that has to be loaded from VRAM.
So there's like less.
data that has to be transferred over.
It does increase ALU pressure.
So it's easy to make sure that you actually test to see if this is actually going to be more performance than not having it in.
The good thing is that it's very easy to implement, because you're only looking at the spots in your code where you are reading and writing your data out.
So it's something that's not going to have repercussions for the rest of your logic.
And HLSL supplies F16 to FD2 and reverse.
That allows you to do this packing of two floats into one float.
But of course, the lack of precision can be an issue.
So there was some of the information, such as the precision, where we just couldn't use this.
For optimizing ALU and VGPR usage, I used these two talks from Emil Pearson, aka Humus.
They're really good.
It's like super concrete and easy to use advice.
In our specific case, using ISP, which is an approximation of division, turned out to be really beneficial in a lot of cases.
SYNCOS is also a lot more efficient than doing SYN and COS separately.
All of these are HLSL intrinsic functions that are readily available.
I also found that pre-computing part of the intersection math, and then have that available in a constant buffer, led to a nice speedup.
And we would also pre-compute the aspect and inverse aspect radio for use in other parts of the pipeline.
And this is the aspect radio of the brush strokes.
So...
A nice feature in Unity is that you can actually see the generated bytecode by just finding the shader in the editor and clicking this button.
And what that allows you to do is to compare if changes to your shader is actually going to change the amount of instructions that you generate, which, when it takes, like, maybe 20 minutes to do a build, this can very quickly add up to a lot of time gained.
Something you can also do on some platforms is to use platform-specific defines to basically nudge the compiler to optimize your shader in different ways.
It's not available on everything, but yeah, for the consoles, that can be a benefit as well.
So when you're starting out learning compute shaders, quite often people will use structs.
It's, they're terrible.
Um, it's way better to use, uh, a structure of arrays instead of an array of structs.
And the reason for that is that, um, you get better cache coherency, and each kernel can load in the data it, uh, it needs itself.
Like, for example, for all the calling that we do, we don't need the color of the brush strokes, so there's no reason to have that, uh, sticking around.
Um, and...
I, this is a guess, and maybe someone in the audience will know this, but I also think that the compiler can rearrange the code so ALU instructions can happen while memory fetches are also being done. And if it's done as, as like one big fetch of, of a struct, I think that's a lot harder for the compiler to, to schedule.
So what you're seeing here is the debug view that I use the most, and.
This is the content of each calling tile.
So if you see a pixel in there, that is because a brush stroke has been determined to be inside that calling tile.
And something that's worked really well in terms of optimizing the effect without like a.
visual loss of quality was to actually look at how many strokes are inside the culling tile and then feed that back into the generation kernel. So if the tile was starting to fill up, the probability of new strokes spawning would be reduced.
So what's going on with all the green and blue colors?
Well, it's quite often that a stroke will not cover all the pixels in a 32 by 32 tile.
So that basically means that you end up with a lot of the threads not doing anything.
And on GCN, 8 by 8 pixels is actually the optimal size for a work group.
But that puts a lot of limitations on how much you can actually fit into your local data share.
Yeah.
So you might think, OK, can we reduce the size of the calling tiles?
And unfortunately, testing showed that if they're smaller than 32 by 32 pixels, they started being slower.
So what I did instead.
is that I would keep the calling, the fine calling, at 32 pixels, but I would then actually run three different render kernels after each other, each optimized for a different amount of strokes in that calling tile.
And I did this first as like a quick test to see if the approach would work out.
I thought I would have to do some kind of like quick, clever scheduling.
But it turns out it was actually like really, really fast.
So what happens is that the The render kernel that's optimized for 8x8 pixels will look at how many strokes are in the tile.
And if it's not fitting the amount that it can fit into LDS, it will early out.
And because all the threads in the work group are earlying out, it's very quick to do this.
So on average, executing a render kernel that wouldn't actually affect anything in the image would only take 90 microseconds.
And I thought this was going to be like a golden, or like, I guess it's a silver bullet.
Yeah, a silver bullet to performance.
But it turns out it was only a 0.7 millisecond gain we had from it.
But that's still nice.
All right, so now I'm going to talk a bit about future improvements and a conclusion.
So, one drawback of using substances is that they wouldn't take the topology of an object into account.
So, for example, these floorboards you see here, you would actually need smaller strokes on the thinner sides of the boards than on the rest of them.
In order to allow the artist to fix this, I actually had time to put in vertex painting of brush sizes into the whole pipeline.
But it came online so late, so we didn't end up using it in the end.
Unity is doing a lot of amazing things at the moment.
Scripted Render Pipelines is one of them.
And if we had had access to that, it would have helped with a lot of issues.
We could have had lit injected strokes because in the high-definition render pipeline, all of that information is readily available.
I guess it will be available in the lightweight one as well.
And the ability to create and bind buffers more freely would have meant that I would have a lot more, like, flexibility in what I would have been able to do.
Um, something that we saw at the end of the project is because the brush stroke, uh, the brush sizes are in a single 8-bit channel, that means that it's a normalized value.
And if we needed to go in and change the, uh...
like make some big strokes, for example, in one scene, we would have to do that on the global level, which meant that all the materials had to be tweaked because it was all coupled together.
So in the future, I would store all of this in a 16-bit half-precision value instead so you don't have that coupling between all the brush sizes.
That would have saved our lead artist a lot of time in the end.
So nowadays you have to make sure that you're game is easy to share.
And we found that YouTube compression was very tough on the effect.
It took us a couple of tries in recording footage for trailers before we really got a hang of it.
And it's.
It's such a shame, right?
Because when people actually see the game in real life, they can see the texture that you get from having the brush strokes there, whereas a lot of it is kind of like washed out if you're watching it on a phone.
That said, I'm really proud of what we achieved.
So what you're seeing here is a piece of concept art from early in development.
And this is the scene that you see at the start of the game.
Especially the way that the lamp posts are having this nice halo around them and how dreamy looking the background is, I think that's really nice.
And you can also see it here with other scenes in the game.
Like having this sense of a not that defined background like rushing past you, for example, when you're playing with the pigeon is something that I'm, yeah, I think looks really beautiful.
So here at the end of the talk, I would also like to thank a bunch of different people.
I got a lot of mentoring throughout the project from a bunch of very talented graphics programmers.
We also got a lot of help from the Unity spotlight team with figuring out some of the various ways to tweak the deferred pipeline to do what we needed it to do.
And I would also like to thank Cody, the lead artist on the game, for helping out with preparing the...
some of the slides in this talk.
And lastly, I would also like to thank Artman and Bandai Namco for allowing me to give this talk in the end.
So I would also like to thank the rest of the team that I worked with at Artman.
I think everyone was doing a really good job.
But maybe even more importantly, it's also one of the teams I've been on where we had the best social interactions, like everyone was gelling really well, and it meant that it was just a pleasure to work on this game.
Yeah, it's one of the best projects I've been involved in.
All right.
So to conclude on all of this, pre-production is really a must if you're trying to do something that is as weird as we try to achieve.
And it's important that the technology has a head start before the rest of production ramps up.
Also, talk to your artists and make sure that what you're developing is actually what they need.
All the disciplines have to work together in order to achieve the look that you want.
Computers have a huge potential for photo of like, for paints and rendering.
And I can't wait to see what's going to be done with them in the future.
And thankfully, I would say, be a novice when you start, because then there's a lot of easy things you can fix at the end of the project to increase your performance.
All right, that's it.
So there's microphones set up if you want to ask me questions.
I can't actually see the microphone from here, but I guess if you form a line, that'll work.
And remember to say where you're from when you ask the question.
Hey, that was a beautiful presentation.
Really awesome work.
Thank you.
I have a quick question.
How did the artist author the flow directions?
So that was by painting regular flow maps in Substance Painter and also having custom nodes for it when it was in Substance Designer, I think.
If you meet us in the wrap-up room, Cody is actually here.
And he can answer more specifically how that worked.
Thanks.
Do we have any more?
Yes.
I asked about the.
The feature about, because I look, is look the static image, I think it's very beautiful.
But what, when the camera is moving, and then we can figure out that the paint, the stroke is like a little bit flickering because they are keep appearing and disappearing.
So is that the final effect that the artist expect?
Or you try to make it more stable?
Could you try and rephrase that?
I mean, for a dynamic scene, you will find that the strobe is keep appearing and disappearing far away or something like that.
Is this the final effect that the artist expected?
So if I'm understanding you correctly, you're asking if artist was expecting the effect to behave as it did?
Yes, because you create a game with some concept art that is just static, right?
It's very beautiful, and the static scene...
Oh, I see what you mean.
So in motion, was it, like, what we were trying to go for?
Yeah.
So our art director actually made a kind of, like, a sizzle reel of things he wanted the effect to do early on, and a lot of that was inspired by...
I think I have a slide for that.
Yeah.
So...
We would look at something like the old man and the sea, which is a really beautiful oil painted animation.
And he had eight different things that he wanted to see in the pencil effect.
Some of them was not really achievable, but something like boil and slurring, some of that stuff, we kind of got in.
So I think early on, that dialogue with him is really what informed how the effect looked in the end.
I think there was definitely a learning curve for the artists in order to figure out how to create 3D art that would work because they couldn't like just look at it inside 3D Max.
They would have to export it into the game engine and fiddle around with it.
But we definitely saw like at the end of the production, they had a lot easier time creating assets quickly because they had a huge library of substances to rely on and they just knew the whole effect much better.
Okay.
Thank you.
All right.
Hello, great talk, thank you.
Did you explore machine learning style transfer to perform this effect?
Sorry, is the question to if I will use machine learning?
Did you explore the possibility of using style transfer?
Oh, do you mean the one?
No, we didn't use any machine learning for this.
So.
If we had used machine learning, the problem with that is that you give up a lot of control for how easy it is to use.
And as you can see here, we had a lot of artist control over specific objects and the scene as a whole.
So if we had used, like, machine learning, I think we would have kind of, like, thought what that approach would have given us.
So I'm seeing some quite interesting approaches at GDC this year, where they have used it, but they also build it as a way to prototype as a given look.
And I think that it's going to be interesting to see if we're going to end up with some kind of hybrid approach where we will have some parameters that are controlled, but what I'm seeing from using machine learning so far is that it is kind of like a take-it-or-leave-it situation.
You either use it or you don't.
Okay, see.
Thank you.
You're welcome.
Hello, so you mentioned a lot of, in the talk about troubles with Unity and all that.
Could you go into why Unity was chosen for a project or for maybe Unreal or a custom engine?
Yeah, that's a good question.
So.
I mean, I'm happy.
I'm a Unity freelancer.
So I think it was predominantly because admin has used it for other problems, other projects in the past.
So there was a lot of institutional knowledge on how to use the engine.
And I think for the size of team we were, it was also a fairly good fit.
So I have only worked a little bit with Unreal, so this is kind of like hearsay, but what I've heard is that if you have to modify their renderer, like, you have to delve, like, you basically have to write in C++ and, like, tear out things and, like, put things back together, whereas Unity is much more...
And I guess this is kind of like an engine design philosophy.
They use, um...
they approach it much more like a toolbox.
And there's like, you have to build more on top, but it's kind of designed that way.
And there are limitations of how far you can go with that when you're using the current deferred or forward renderer.
But what they're doing now with the SRP and the whole dot system, you're going to see that like it's probably going to be a lot quicker to do all of these things.
So yeah, that's why we ended up with Unity.
I see, thank you.
I think we have one more.
I think.
That's it, okay.
All right, well thank you for coming and remember to fill out the evaluations afterwards.
All right, enjoy.
What's left to be said?
