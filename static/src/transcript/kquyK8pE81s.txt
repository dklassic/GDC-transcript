Thanks a lot for coming along today.
I always wondered if you had to request that sort of funky music before it starts, but it just happens automatically, which is really cool.
Yeah, a couple of reminders before we get going.
I'm sure by this point in the week you all know these things, but to begin with, could you please ensure the volume's turned down on your Nintendo Switches?
And the other thing is, like, don't feel the need to record this with your mobile devices.
It's all going to be on the vault and, you know, it's being recorded.
And hopefully they're recording that 4K checkerboard upscaling rendering talk as well so we can all catch up on that at a later date.
Okay, let's get started. My name is Jamie Wood.
I'm the lead lighting artist at Playground Games.
And welcome to the ambitious HDR time-lapse skies of Forza Horizon 3.
The Forza franchise has been around since the original Xbox days and is owned by Turn 10, who make the incredible Forza Motorsport game series.
At Playground in the UK, we create.
the open world Forza Horizon games.
So to date, the studio has released Forza Horizon, Forza Horizon 2, and Forza Horizon 3.
And it's the Sky technology of Forza Horizon 3 that I'll be speaking about today.
The technique that we used involves a lot of practical photography as we shoot these high resolution, high dynamic range, time lapse image sequences which we use to project onto the sky and inform the lighting on the ground.
And I would say there's a lot of talks at GDC that will claim to speed up your workflow, improve your pipeline, save your company lots of time and money.
and this talk is definitely not one of those.
This is gonna cause you a whole lot of pain and additional work, and yeah, a whole bunch more effort.
But hopefully by the end of the talk, you'll understand why we chose to use this technique and what it brought us in the game.
So to begin with, I'm gonna play the trailer for anyone that's unfamiliar with the game and just to give an idea of what it's all about.
What a wicked game to play Don't make me feel this way Okay, so the game was released in September of last year, and everything inside this presentation, the screenshots and videos, are from the Xbox One version of the game.
We also released on Windows PC.
And what I'll go through today then is, first of all, why we chose to invest in this guy, why we felt that was an important part of our game.
I'll speak about the practical photography process of shooting these skies and how we clean up and process that data.
But if you're not into photography, don't worry, because I'll get on to the game side where I will explain the animation of the sky and all of the important lighting benefits we get from leveraging that sky data.
And then finally, I'll speak about our adventures in Australia capturing the Australian skies for our game that was set in Australia.
So why is the sky important to us?
Well, there's a number of good reasons.
First of all, the sheer screen space that the sky takes up in our particular game.
So we're a driving game.
We often talk internally about the car, the road, and the sky being what makes up the frame, and they're the things that we need to really nail.
And the sky can be a third or two thirds of the screen space.
We also, in the Horizon games, have built up a a reputation for having dynamic time of day and weather.
That is something the fans really enjoy, and that is something that we always want to push, and the sky is part of that.
The sky also informs a lot of the lighting, so we drive a lot of parameters from that base sky, so getting that realistic base is really important to us.
Finally, the sky really supports gameplay in a number of conditions.
action-packed kind of career that we have a bit more control over like dictating what times of day and weather are used in particular races But we also have the chilled out free roam gameplay that these picturesque backdrops really support So I'd like to speak a little bit about our previous system.
On Forza Horizon 2 we had a system for representing the sky that you'll see common in still a lot of open world games today or anything with a dynamic time of day cycle, I should say.
And that was a base sky simulation that is based on the sun's position and modeling the scattering of light to create the kind of gradients you'd expect as a base at different times of day.
And then on top of that, we would layer cloud sets that were particular moments of clouds, particular shapes that were created by concept artists in our case.
And that would be like the cutout shape of the cloud, the way it would fade in, different layers to dictate how we would pump in light values to change these clouds over time of day.
And in our particular implementation, we had 128 animated curves to control per.
lighting for a cloud set. So that was a lot of onerous information to handle.
It also, you know, there's room for error there where we're dictating things like atmospheric fog, ambient light, all kind of artist-driven sets of curves.
And what that gave us then, like if you had a random chance, a dice roll, to have a cloudy day, you might see the same cloud set.
throughout the whole day and there's not really any dynamic change in the shape of the clouds there and All you get is the the different artist driven Lighting that's input to that so but we achieved some really nice stuff with the system So the game was critically acclaimed everyone really liked the visuals of it and and we we got what we wanted out of it But we decided for Forza Horizon 3 that we didn't want a nice imitation of the real thing.
We wanted the actual real sky.
We wanted all the layered complexity that you can see in these images that's just really hard to model in that previous technique.
We also wanted these massive volumous clouds with sharp lighting and cloud fronts.
We wanted killer sunsets.
We wanted these one-off natural.
occurrences, and we just wanted this complexity from nature in the lighting and layering that we hadn't seen done in any simulation or mock-up. So we started to talk internally about why not just play a video of the sky, which sounds like a nice simple idea, but when you start having conversations about that, there's some very good objections. For instance, the disc space, like one hour of 4K video would...
fill up the entire Blu-ray, we need to get the rest of the game on there, and one hour is not really enough.
We also have this big open world that we drive through at 200 miles an hour, so we are already putting a lot of strain on streaming, so having a heavy video asset in the sky is not really going to make people happy.
And we also did not want to take a step backwards in image quality, so we are used to these textured assets that we can create that are sharp.
We do not want to add video compression into the mix.
And when we started to talk about a still frame sequence, it was much the same trouble.
We assumed you'd need too many frames to be able to do this, to have a smooth animation.
And so that seemed like it was impossible, too.
And then there's a really good reason, as well, in terms of production, where we've got this Sky system that worked.
We've just shipped a game that everyone loved.
Why would we add risk to the project and try and change things up?
But we were really set on this real sky.
We wanted the real thing.
So we set about going out there, despite the warnings from the rest of the team.
And we thought, right, let's go and capture some data, and we'll work out how to make it work.
So that meant.
going outside in the winter in the UK, experiencing things like fresh air and natural light that game developers don't usually get a chance to experience.
And this process was really starting out with testing individual cameras just for the suitability of shooting these long time lapses.
And then it developed to that shot where I'm standing looking quite miserable in the...
It's because I'm really miserable at the time.
And that's kind of the more developed rig.
And even at one point, they gave us some accommodation in the form of that motor home on the end.
No Lamborghinis there.
So the actual photography side of things has a lot of challenges with what we were attempting to do.
So if you're a time-lapse photographer, or if you're into that at all, you will have heard the term a holy grail shoot, which is.
what they refer to as any time lapse that goes through either sunrise or sunset.
Because of all those changing light conditions, it's called the Holy Grail because it's really difficult to do.
You've got to get good exposure settings and you've also got to kind of post-process that to give you a smooth transition.
We wanted to do two of these shoots in HDR every day so that we'd have a consistent kind of game asset.
We didn't want loads of different...
like random exposures that will come back to us in a bit of a mess and every sky be sort of different and graded differently.
So that's a big challenge.
You also have this contradictory.
thing between like trying to expose the clouds quickly, like and have nice sharp clouds, but also we wanted to capture the full dynamic range of the sun and sky.
So that meant we were using neutral density filters to reduce dramatically the amount of light coming into the camera, which directly contradicts.
it contradicts trying to take a quick shot that's well exposed.
So you have to balance these two things together.
And also things like lens flare.
So having a very tiny aperture will actually increase the amount of baked lens flare that you're getting in these shots.
So you have to find a sweet spot there.
We were also using a multiple camera rig because we wanted a big future-proof resolution.
So that meant there was more things that could go wrong.
So the.
The actual rig that we ended up with, as I say, it's a three-camera rig, and we have this portable battery.
It's just a recreational vehicle battery at the bottom of the rig that supplies mains power to these three cameras.
And we used Canon 1DX cameras.
Like, I don't want this to be like a shopping list for going out and doing this.
Like, there's many ways that you could do something similar, but we used the 1DX with some 15mm fisheye lenses.
to get our full coverage.
And one of the reasons we picked the 1DX was because it has such good internal bracketing.
So it shoots at 12 frames per second, which means you've got very small gaps between the exposures that you need.
Because we take seven exposures on each camera every 30 seconds, so that was our process.
And another advantage of the 1DX is that it has two memory card slots, which means we could shoot entire 24-hour periods without interfering really.
And you really need to get used to these dramatic changing light levels and how to handle that.
It's an absolutely amazing education for any lighting artist to just get out there with a camera, kind of experience what this planet does as it goes from day to night, and I'd definitely recommend it.
So the cameras were monitored at all times.
We didn't find a way to leave this rig in a field and come back three weeks later and pick up all the skies.
This was a very active shoot.
And the way we dealt with all the changing light levels was to have a fixed aperture of F8 which was kind of a sweet spot between that lens flare and sharpness and then we'd just ramp ISO to deal with the changing light levels so someone's There with a light meter reading how much the lights changing and then adjusting the cameras accordingly And one little trick that we did was, because we needed these neutral density filters in the day to stop down the light and get those exposures of the disc of the sun, we didn't want to fiddle around taking the filters out for a night and we didn't want to leave them in because then we'd have to use such high ISO to capture the night sky. So we actually swapped the lenses physically, which was faster than fiddling around with lenses. So we had two sets of lenses.
And I put this image in just to show how kind of like low tech this was.
Like I'm really proud of how clobbered together this looks at this stage.
So we had our art manager turned electrician for this.
There's really just like a small group of us trying to make this work.
And so he.
board all the holes in here and set up this transformer to supply mains power to all the gear. And our head of production as well is really into astronomy so he could recommend dew heaters to reduce the condensation on the lenses. And whilst we've got a picture of some exposed electronics, I should mention that we...
We didn't shoot in the rain, so we decided that rather than try to come up with something, like, because people were already laughing at us in the studio, so we didn't want to make, like, a rig that had, like, hair dryers on it, trying to, like, deflect the rain or anything, so we decided that we'd shoot skies that looked a bit rainy and then layer that stuff on digitally later on.
And I have to mention the...
the production effort here because when we started thinking about these kind of big time-lapse shoots and trying to look for a location, we really realized that we needed like a producer on this. So we spent about three months even finding like a test location that had like 360 degrees in all directions, no light pollution, which is really difficult in the UK where we're based because since the Industrial Revolution everyone's got streetlights everywhere.
And just finding things like secure land that you have permissions to use was something that really was a lot of work and not something we anticipated.
So, yeah, just to summarize the actual practical photography side of things, as I say, a big production task.
iterative process to get the rig right. It took us about six months from the first times we were out with a camera to actually having our production ready rig and it generates a huge amount of data to process which brings me on to the next section.
So we use two programs to clean up and process the data and get it ready for game.
So that's Lightroom for cleanup and Nuke, which is a compositing package.
We use that because it's like a node-based system that is quite adaptable to whatever you need.
And that kind of gets the images ready for the game format that we want to output.
And yeah, each shoot with the seven exposures every 30 seconds generates 20,000 raw photos.
So that's 1.2 terabytes of data to handle every time we do a shoot.
And this image here, you can see like the dark to light exposures.
So we take shots like that every 30 seconds.
And that's about half an hour's worth of one camera's data.
So you can imagine what a daunting prospect it is looking at all the images from three cameras from 24 hours.
And we knew at this stage that we had this challenge of getting this 1.2 terabytes down to less than 2 gig which was what we plucked out of the air as the sensible, like you can probably fit this on the disk if you make it less than 2 gigabytes. So we had a long way to go. Just to show you what one of the images looks like then, so you can see like the spherical distortion.
And down in the unused part of the image in the bottom left, we attached a diffuse light probe to the rig.
So we always have this point of reference of the lighting.
And I'll speak a bit more about that as we go in terms of how we used that.
So in Lightroom we batch process the chromatic aberration and vignette from the image.
So that's just using a lens profile that you can download to suit the lens.
And we also do quite simple color grading of this.
So all we do is we have two X-Rite color corrections, one for when we have the neutral density filter in and one for when we don't.
So all we're doing with the grading is kind of grading out that neutral filter that isn't exactly neutral.
And then we stick with daylight white balance on the cameras throughout the whole shoot.
And also we use Lightroom to clean things up, like spot removal, dust, dirt, insects on the lens, all this organic stuff that we're not used to dealing with.
And sometimes even people's faces, if you don't get out of the way in time when it shoots.
So looking miserable again.
Then we're ready to move on to Nuke.
So we bring in our cleaned up images and we have these like sets of seven image sequences per camera and we.
merge them into an HDR source for each camera.
We unwrap that spherical distortion and stitch the images together to give you something much more familiar as a lat-long map that you'd use on a sky.
And we also removed the stars here as well, so.
We found that no matter what resolution we use these in the final game, you do not want pixel stars, because we have a photo mode in the game, so you can use like a really long lens if you want and take a zoomed in photo, and you do not want those stars to turn into like big squares behind the subject.
So, we decided to remove the stars from the images in Nuke, and then we have a star field that we plot as...
the replacement for that and we also create a cloud mask which is really based to show when to show through that star simulation so we need to know where we're allowed to see that and where this cloud that should occlude that.
So, yeah, and we also use Nuke in the light probe analysis, which I will get to.
So, one other little trick that we do, which is probably quite particular to our racing game, is that we bias the resolution to the horizon.
So, we are driving along, most of the time, we are interested in that bottom half of the sky.
So here you can see the top image is traditional kind of lat long format where as you go up the vertical direction you've got an even number of pixels per degree as you go up.
We in our single down sample as we get the frame ready for game, we just bias that resolution to the horizon so that we've got almost double the vertical resolution there.
And one other trick that we do, which I don't have a slide of, but we decided that because of that importance in our particular case where the bottom half of the sky is much more important, we actually cut the image in half and quarter res the top half of the image.
So we save 3 8ths of the footprint of this as well.
So.
processing stuff is, you know, a big deal. We had a big IT effort on this as well. So we had 80 terabytes of NAS storage replicated off site. We had artists workstations that had to be equipped with large capacity SSDs for those Nuke networks where they're very read write heavy.
And as much as we streamlined the cleanup process as much as we could.
There's still a lot of manual work there just because of the nature of all these images So we we had to hire a full-time artist for sky cleanup on the project But now we have this asset ready to test so we've got this thing ready to go in game So let's have a look at the comparison. So Opus was the code name for Forza Horizon 3, so that's what that word means.
But here you see again the Forza Horizon 2 process, where the cloud shapes are very static, they're just moving gradually, and then you've got all these artist-driven values for what those clouds look like, what the...
ambient lighting on the ground looks like.
And here's our first full day test of the new system and what that looks like in game.
So you can see the moment to moment variation, the way the lighting on the ground always matches perfectly with what's going on in the sky, we've removed that kind of margin for error and you can see there's a whole lot more value moment to moment in this asset.
So at this point we were starting to convince people that this was quite a good idea.
So we still had this challenge to solve where we have this image sequence where we're going between these nice moments in the sky, but we knew...
that we needed to keep that frame rate really, really low in order to have any chance of actually shipping it on a disc.
So, the next section talks about the motion, which is really the key part of doing that.
Use a low frame rate and translate between the frames in a clever way.
The first thing that we tried then was a ...
perspective correct dominant single direction of movement on the sky.
So this debug view has longer lines overhead, where this represents like a faster distance traveled between frames.
So that's like the pixel motion that we have input to drive one frame to the next.
But this.
this motion kind of disappears to a vanishing point and slows down as it goes further away from where the camera's positioned.
So this is generally correct.
Like that's sort of how skies move if you go outside and have a look.
But.
it does rely on an artist-driven curve again.
So we have to input a direction and the speed that we want this sky to move in.
And if you get that wrong at any particular time of day, you'll get this kind of sort of zigzagging effect and that's not very natural.
And the other problem we have is that a sky has a load of contradictory motion built into it.
So, for instance, the sun, it doesn't really care what the clouds are doing, right?
It moves on its own trajectory.
It is, you know, it doesn't care about this wind direction that you've input.
The stars rotate around the pole star.
The moon does whatever it wants on an evening.
And you can't just drive it all with this single direction.
So.
We started to develop a system where we would track the sun, which is what this red circle is.
So that's like us key framing where the sun position is.
And then we would mask away that cloud movement so that the sun and everything around it in a sort of feathered blend would move along with the sun.
So what that looks like then is.
sort of a bit weird because everything around the sun is very static and you get this sort of stretching away from the sun. And yeah, like sometimes that's not really the correct movement there. These clouds could have been moving quite quickly in a different direction to the sun and having them pinned like that really didn't, um, suit what we needed. So we started looking at other methods and what we settled on was using motion vectors to drive the motion.
So what this is then is we pass in our full image sequence into Nuke, and using Nuke's built-in vector generator node, we would output this set of textures.
And for anyone who's not familiar with texture-driven motion, this is a four-channel floating-point 32-bit texture where two of the channels describe UV animation from one frame of the sequence to meet the next frame halfway, and then the other two channels with positive and negative pixel values will move the frame back in time to meet the previous frame.
They are describing actual Texture Space motion in each channel.
This is an example.
This sequence is a two-minute gap per shot.
That is a very jumpy thing that does not really look like a video that you would want to play in your game.
But the same number of frames, just with a motion vector driving the motion between them, gives you a much smoother interpolation.
It's kind of local enough that it actually describes some more of those kind of smaller clouds and how they're morphing and changing for the next frame.
And it means that we can play this at any frame rate we want, because we're making the in-betweens up.
So we can play this at 60 frames if we want, just with those one shot every two minutes.
And if I've done my maths correctly, that 0.008 frames per second was the frame rate that we have there.
So that's a significant saving from something like 30 or 60 frames per second with the number of images that we need.
And we also discovered through testing that we only needed a quarter res motion vector.
So.
We didn't see much in the way of diminished motion by court rezzing that, so that was a saving there.
And we made a further saving by, instead of storing the full 32-bit image, we would keep an XML that went along with the frame data.
So each frame would be given a maximum pixel movement, which was found per channel in the original motion vector.
We would map that 0 to 255 on an 8-bit Texture, which is the much more pretty looking one below.
Then when we are back in game, we can look at the XML, and we can remap those values to the original range of motion per channel, per frame.
That was like a really nice optimization, because that Texture compresses much better in game.
So to summarize the motion side of things then, you've got to solve those contradictory motions in the sky.
The way we did that was.
with the automatic motion vectors.
We also had the ability, if anything ever kind of didn't pick up correctly, we could easily input on that original frame data, like markers that Nuke would pick up easier.
So if, for instance, the motion of a moon didn't quite pick up, we could draw big red blobs over it, and then Nuke will kind of track that.
a bit like our original methods with the dominant direction stuff.
So the motion vectors were the way we did that.
And this is really crucial to being able to use the whole technique. Without a low frame rate, this does not happen. And I am sure that is probably something that has put other people off this kind of technique before.
So I would like to speak about the most important part of this whole thing from my point of view, which is the.
lighting benefits in the game.
It is not just about what is going on, on the pixels on the dome.
We saw some real benefits in the game lighting and rendering as well.
To begin with then, the light probe calibration that I promised.
You can see in the top of that image, you might be able to see the original gray ball there, the light probe that we have in every photograph.
And so what I've done here is just cropped in on that image like just the pixels of that ball and because in the shoots if we're in the northern hemisphere we face the The gray ball to the north and if we're in the southern hemisphere we face it to the south and that means you've always got generally normally you've always got a point of reference on this ball throughout the day from sunrise to sunset.
And so I've plotted the max luminance found on that sphere here in the graph below.
And there's some really interesting things that you can see from that.
So the sort of wavy...
break up on the first part of the graph.
So that's when the sun rises.
That's because there was like sort of broken cloud in front of the sun, and it's just sort of diminishing and changing the intensity.
And then it kind of levels out to a more consistent relationship between the RGB.
And then you've got these like huge dips in intensity, which is when the sun's actually behind like full cloud and you drop down to just the ambient levels.
That is really interesting, because that means the rig itself was in full shadow there, and that means that our player should be in full shadow when they are on that part of the image sequence.
We can use this data to model that in game and give a real kind of true idea of what the lighting was like at that time when we captured the sky.
So just to give you an example of that then, that's the original photography in Nuke on the left hand side.
And in Pix, which is one of the Xbox development tools that we use, you can see in a test bed the full intensity of the sun on the bottom and then where we've knocked it out with cloud shadow in the top.
So we don't.
We don't ever change the intensity of the sun to model this.
We do it much like real life, where you actually just knock that out with the shadows.
So this gives you, like, a really nice balance between the sun, the sky, and the lighting.
And thanks to the hard work of the environment team at Playground, you can see the balance between â€” so the images on the left are our near-final Australian track.
And the images on the right are our actual sky captures.
And you can see that that relationship really comes through and it's a realistic kind of balance.
And that happens throughout all times of day.
So the tricky kind of low light times of day where you're you really depend on that relationship between the sky intensity and the ground to look really realistic that all comes along with this kind of more scientific approach of analyzing what the lighting is doing.
And because our time of day cycle is now an image sequence or several image sequences, we can make other images to go along with those sky frames.
So one such image is the fog cube map that we have per frame.
So this is a simple Gaussian blur of the sky that is flipped to give some negative directions if you're looking down.
and you want to find a fog value.
And we actually analyze this image before creating the cube map, find the brightest point, which we assume to be the sun, and we clone that out automatically in a tool.
And this kind of blurred 360-degree cube map gives you a really nice fog color as you disappear into the distance.
So we have... that's actually a screenshot from Forza Horizon 2 where I've used the Forza Horizon 3 system.
But you can...
kind of see those distant mountains just like matching up perfectly with the sky in the distance.
And that's like a replacement for directly an artist-driven thing where we would have been picking a fog color and we would have had practically no chance of matching that all the time to whatever clouds or...
simulated gradients were in the sky.
So that's really nice.
And on a similar note, we have this diffuse sky indirect cube map.
So we take the sky image with an infinite.
black ground plane, and we convolve that offline.
So we've got the opportunity to do really nice high resolution input cube convolves.
So 180 degree cosine blur.
And that is just the indirect diffuse contribution of the sky, as if you're standing on top of a hill and there's no ground.
So it's not really.
a practical thing to have, that's never a scenario that we have, but what we do is occlude that with our voxel based GI system, so we have voxel based AO that will reduce this diffuse skylight and then we get the rest of our diffuse lighting from the voxel based radiance.
that system backs this up to ensure that at all times of day, we have this nice relationship in the shadows between the sky and the indirect lighting.
So as you can see in those images, we've got this automatic confidence that those levels match the sky.
Another texture that we have is the cloud mask, as I mentioned earlier.
So that's used to reveal where the stars are seen or not seen.
But we also use that on the distant cloud shadows.
take this black and white mask, we turn it into a cube map, and then assuming that the clouds are all two kilometers overhead, that's just a number that we used, we can look at any distant terrain pixel, you know, project up to where the clouds would be, and then look up as to whether that's in or out of shadow. And that...
We found that to work really nicely, so having a relationship clearly between the clouds that are in the sky and what's on the ground felt really natural.
Obviously, in an ideal world you'd want to model these things in 3D and ray trace through a shadow, and that always wouldn't be that what you see in the sky is projected on the ground.
But having that relationship seemed really natural to us.
this moves along with the player, like the sky has to follow the player as it moves.
That meant it was not really suitable to be used at the car, so you do not want to, as you drive, all the clouds move with you because that is really weird.
We have a scrolling texture solution for up close.
This video is really sped up so that you can see it, or maybe it is playing really slowly and slowed down.
But you can see the...
the clouds coming and going there around the car.
And that's a different system entirely.
So we have a scrolling texture such as the one that's there.
And that will move in the direction that we input that the clouds are moving in.
That is a curve that goes along with each sky.
That means that you can drive towards clouds, they can come and pass over you, and all the things you are used to seeing when you drive in the real world, you will get with this.
We can also vary the cloud shadow at car intensity based on that original graph that we draw from Nuke, where we can make sure that when the rig was in shadow, the player is in shadow using this system.
And I wanted to mention the weather which we developed for Forza Horizon 2.
The weather system was something that we worked really hard on for that game and it plugs into this new SkyTech as well.
So based on the porosity of the materials in the world, we're able to tag with the sky sequences that come along, like this point here has a chance of rain.
And then that will happen really naturally as the clouds come in, the ground gets wet, the sort of weather systems move on and then you get those nice post-rain moments as well.
So all of that stuff just kind of plugged in really nicely.
And one other benefit, suddenly having a constant with these photographic skies.
So we've got these images that we've controlled, we've taken all the exposures, we've got the metadata for how those skies were captured.
That means we've got the opportunity to take other photographic reference and see that in the global hierarchy of lighting next to the sky to give a really nice relationship.
So one example of that.
is just seen in this test here.
So this is just early pre-production stuff, and that's some images I took from my car and turned into an HDR texture.
But we.
You can see at this twilight period that things like your gauges are starting to come alive as you'd expect as things get dark.
And then headlights are blowing out, brake lights are seeming really hot.
And then as we move to a daytime condition where there's a lot more light, the game camera's exposure has had to adapt to that.
The same texture appears naturally to change as you'd expect.
you can see that those gauges recede, the headlights are less blown out in the day, and the same with the brake lights as well.
So that was like another advantage of suddenly having this kind of known quantity of the sky.
So yeah, to summarize the kind of game side stuff, it's certainly as big a part of.
this whole technique as sort of what's going on in the sky, on the sky dome, we benefit an awful lot on the kind of jump in realism just from using the sky technique based on what the other stuff that was going on on the ground.
So finally, I'd just like to speak a little bit about our adventures to Australia, where we sort of decided that once we have this rig and a process that can capture real skies and our game is set in Australia, then where better to shoot the skies?
So we shipped the whole rig out to Australia.
We went on location, me and the art manager, and we trained up two groups of photographers out there who did a really great job over...
the summer of 2015, so that's like December 2015, the year before we shipped, and over the course of the whole summer, they captured 25 full 24-hour days, and that was like our pool of data to use as our skies for the game.
They took two million photographs.
It's just a big number, not really significant, but just showing off.
So this is a video of us out there and it was really like something I'm really glad that we did, like go out to the real location.
It's like a really...
romantic idea to kind of take the lighting from the actual real world location and kind of transport that to the player wherever they may be.
And as well as kind of just that being like a cool thing to do, we got the sun arc.
included in that data as well.
So normally we would kind of go to great lengths to kind of work out the timings of the sky and where the sun would be positioned and how long sunsets would last.
And like, we got all of that stuff in the data, which was like.
just that other level of realism.
And we also had this survey of all the weather in Australia.
So we'd been there for the whole summer.
We knew how long it rained for and when it was cloudy and when we had these full clear days.
And we could use that data when it comes to setting chances of these things in our weather system in the game.
So here's some of those Australian skies in our near final Australian track then.
So these are obviously sped up to look cool as time lapses, but this stuff would roll at a natural speed in game and gives you that kind of nice feeling of things just evolving without you really noticing, much like you do when you kind of spend the day outside.
And then all of those images that I showed you at the start where I said we wanted to capture the real sky, they were, of course, things that we captured in Australia.
All of those things made it into the final game.
And that gives us this kind of wealth of options when it comes to setting up the races or online events or whatever it may be.
We've got this really nice full range of different moody, dramatic conditions.
Some shots from the DLC there.
And I think that that final shot's actually from one of our players from a community competition, which I really liked.
And yeah, just to kind of sum the whole thing up then, having been through it, you may have gathered it kind of represents a huge cross-discipline amount of additional work to do this. Like, we definitely...
touched on all departments, I think, from art, engineering, design, with the way they set up the races, the leadership team taking on this kind of crazy idea, the IT effort, the production effort, and not least my partner in crime on the rendering side.
So the rendering engineer that was assigned to this part of the project did a crazy good job of implementing all of this stuff.
And.
really wouldn't have happened without him.
So there's also this huge investment of effort on the practical capture side as well.
So it's almost like this new department we've got where it's almost like an audio team or getting into photogrammetry or whatever.
You've got to kind of understand that that's kind of a huge investment there.
But the Sky System, as I say, has really benefited the visuals and a kind of jump in realism.
And it's removed all of those artist-driven data curves that I spoke about earlier on as well.
We had barely any meetings, if any, during the project where we were sitting scratching our heads and going, oh, there's something a bit weird with the lighting.
What is it?
Like, because it was done in a sort of measured kind of scientific manner, so we had sort of confidence in it as we went along.
And finally, whilst obviously shooting natural skies and doing time-lapse photography shoots that can be reined off as adding a whole lot of unpredictability and risk on the production side of things.
That unpredictability is also the biggest benefit of this.
Like, nature is in control.
You have got this kind of wonderful performance of the clouds and the lighting, and that is really the strength as well of the system.
Like, having that stuff captured and preserved and being able to show that in the game is something that I am really pleased that we managed to do.
Yeah, that is everything. Thanks very much.
I think we have time for questions.
No one seems to know.
I'll check.
Yeah, I think we have like 10 minutes, so if anyone has a question, then...
Great talk.
Thank you.
Did you consider shooting HDR since you're using it for lighting?
So the images that we created were HDR sources, yeah.
So the exposures were later combined to give us the full dynamic range, actually, of the sun and sky.
We actually didn't use that full range in the end, so that's something that maybe we'd do differently.
Like, it's not always a benefit to have that baked, really intense sun in the image, so you're sometimes better offloading that to the key light.
But yeah, so the images were full HDR, yeah.
So had you used that HDR, would you have normalized it and then used the game camera's exposure to...
So I think just simply not having such a hot, extreme sun in the image would be actually slightly improved.
So we were very conscious not to kind of tinker with the data coming in.
We wanted a very kind of linear kind of process from the raw photography right up to getting it into the game.
So we didn't want to do any like.
grading on top of it or anything.
So I think just, you know, it'll be different for every game, but just being very conscious about how much range you actually want in there, because it depends on kind of your rendering pipeline as well, whether you're actually clipping out, like offering this huge intense hot spot on the image.
Right.
So you didn't white balance it or anything?
You just took it raw from the camera?
Just, yeah, just daylight white balance shots, and then...
white balance as a kind of process when we go through Lightroom, just assume it was shot as daylight.
Then you have got the option in-game then to white balance however you would like.
Yeah, so.
Cool, thank you.
Thank you.
Good talk. I got two questions.
One of them is like, how do you define the final value of the direction lights in the game?
And the second one would be.
How do you pick the final exposures for the in-game especially at night time where you know designers sometimes well the real life is too dark Yeah, cool Good question, yeah. So the first one was the sun intensity. So that's really taken from that light probe data, so where I'm analysing the max luminance found on that sphere. That's not directly what you would want as your sun intensity, because that obviously includes the ambient light, but we would model that not really by using the...
exact value we found in that graph, but by using a modeled, like, PBR, diffuse light probe in our game as well.
So, like, I'm able to ...
So, we have some debug options so that I can mouse over the screen and see, like, the exact kind of linear and final values on the gray ball, and I can compare that in the original photography to the values I'm seeing there.
So, it's kind of like a ...
diffuse plus key light equals this and then I'm able to balance it that way.
And what was the other question?
The final exposures for night time, because on the horizons, compared to day time, it's a completely different value.
So we actually kept the original difference in the images.
That was kind of hard to fit into the texture format.
the rendering engineer on this project actually had to do a kind of scaling factor to fit it into the Texture format because of that range being so low.
And like your night shots are very, very small values when you save it, so we actually boost them up to save out the Texture, and then we reduce them back to get the full range back.
So we did not actually ...
Cheat night? We did have a very, very dark night.
Like, if you play the game, it's really, like, headlights only that you can drive with, which was something, yeah, the art director really wanted that.
so we tried to stick to it as much as possible.
We do have some ambient controls, so we can use the underlying star simulation to actually contribute to the diffuse light, but I kept that really to a minimum.
We pretty much kept the really dark nights, actually.
Yeah.
Thank you.
Cheers.
Hi, fantastic talk.
Thanks for sharing that.
And my question is, you showed time lapses at 30 seconds intervals, but end up using two minute intervals?
Yeah, yeah, that's right.
So I think shooting at 30 seconds is a really good idea because wind speed actually really affects this.
So you can imagine like the amount that the kind of speed of clouds varies.
It's like...
sometimes it will be easily like four times faster and therefore you'd get like in terms of what you're asking the motion vector to do you would be wanting to move you know four times as much with so it's at the original kind of frame gaps doesn't always relate to the amount you're trying to move the clouds.
So our target was two minutes, that's what we found to give us the zip size we wanted.
But there were skies that had faster wind speed that we had to use at one minute intervals, which kind of the 30 seconds gave us that option to do that.
And I think there were some skies that we managed to play back at much bigger intervals.
So if you have a...
a clear sky without many clouds and all you're really doing is kind of tracking the sun we found you could use like much bigger um you know frame gaps for that so we used it variably but um, it's really down to wind speed more than kind of time lapse intervals that dictates that so Yeah, because it could save you a lot of space when you when you know the end of interval that requires certain sky.
And also my second question is, can you say about technical details, how much, what is the end of resolution for full time lapse and how much it requires on a disk space?
Yeah, so we used 4K final Textures in the end.
That's obviously with the 3.8 saving where we down-res the top portion, so it was like a 4K by 512 and then a 2K by 256 Texture, so that was our final resolution that we used.
And on disk, we ended up using up about 7 gig of the disk budget, so it's a pretty significant commitment.
Yeah.
Yeah, and my last question is, you have a dynamic weather system, and it means that you can mix different skies, like completely different time lapses?
Yeah, so we had that system, so we developed a way to put blend points, and we could have as many blend points as we like between the different 24-hour tracks.
We actually kind of reined in how much we used that, because we found that we really loved the kind of natural transitions that we were getting, and when we used our kind of manufactured ones, it felt like we were kind of...
putting part of it back to that previous system where it's like sort of made up stuff, not real stuff.
So we tend to only use that when we needed to.
For instance, like obviously at the end of the day, you have to kind of blend to a different portion.
So yeah, we had that system in place.
We didn't use it as much as we thought we would though.
Thanks.
Thanks.
Over here.
Hi.
That's great.
Thanks.
How many skies did you shoot and what was the selection process like?
Did you have reviews on the time lapse?
Yeah, so it was, so we shot 25.
Australian skies for the pool of skies that we used in Australia.
And we shipped with four, like we reduced that down to four 24-hour periods.
So but that's, you know, you can see from the video where I showed the comparison between Horizon 2 and 3, like there's so much variety in each day that four was actually quite a huge number.
Like we had a...
a whole lot of stuff to choose from in that.
And the review process was quite difficult because if you see the format that they come in.
where you've got the kind of spherical distortion and the single camera sequences, and they're all in bracketed orders as well.
Like, it's very difficult to get an idea of what that would be like to play.
So, we developed some tools to get around that.
So, we would, in every raw image, you can actually extract a JPEG, which is embedded in that raw.
So we automatically found the brightest exposures, which are normally like the well-exposed ones, and we'd extract the JPEG sequence, and then we'd automatically kind of write that to a video.
But it was still quite challenging to kind of know that you were making the right choices, because you're still kind of flicking through.
these big, big expansive skies when you're going to be looking at this kind of little part of it on the horizon.
So it's definitely something I would probably improve if we do it again.
My second question is, are you going to do this again?
I don't know, I can't really speak about anything in the future that the studio is really doing.
Sorry.
Thanks.
Yeah, I think last question then, because we're out of time.
Great.
I'm lucky.
It's the last.
Great talk, really.
The best of the week for me.
So it was, I learned a lot of things.
And I do HDRI myself.
And...
we get some kind of problems when we are making HDRI's of the skies because as you mentioned you have to have all these time-lapse, you have to make some motion vectors to make in between motion.
to get what is missing, but also in the bracketing, in the different photographs, you have 7, the time is not exactly the same, and you depend a lot on the shutter speed, and because you are using neutral density filters, sometimes the shooting time is quite long, so do you have...
You do you have some advice about this? Well, what do you use? Yeah, so we we didn't do any like offsetting of frames as sort of clouds move like which I've seen people do where they sort of try to Adapt for the the motion on each exposure. I mean our exposures because we're using the the 1DX, our exposures were less than three seconds, even though that's still quite a little bit of blur there.
But I think we kind of survived because our source resolution was so huge.
So like with the three cameras, we end up with like a 13,000 pixel source.
So by the time you...
kind of crush that down to 4K.
Those small movements are, you know...
So it's because it's just 3 seconds that it doesn't matter so much?
Yeah, I guess so. I mean, are you using longer times than that when you do your HDRI stuff?
Yeah, so probably we manage with that, yeah.
Do you change the setting considering the lighting condition, the cloud speed, the wind speed?
If there is a lot of wind, maybe you try to shoot faster?
So we didn't. I mean, that would be quite nice in one way, but also we wanted to keep it quite a structured, processed thing.
Because we were going to shoot so many, we wanted to make sure we always had like the...
the same exposures coming back and like no surprises when you're cleaning it up where someone's like maybe quickly taking the ND filters out and then done quicker shots because it's all overcast or something.
And so we opted for that consistency over doing that.
But yeah, you could definitely get a quality.
So the advice is to find the right setting for quite all the situation and try to stick to this for the best workflow?
I think that's probably quite specific to our needs on the game.
So like...
Yeah, we wanted these consistent things.
I suppose if you're taking a time lapse of fast moving clouds, and you know it's fast moving, and you have time to adapt to it, I think you'll probably get a better quality from doing that.
But yeah.
OK, thanks a lot.
All right, thanks a lot.
Cheers, everyone.
