Hello and welcome to Control Lessons in Procedural Destruction.
My name is Johannes WÃ¤chter.
I'm principal effects artist at Remedy.
I used to work in film as effects supervisor and artist for more than 10 years.
And I have moved into games in 2019.
Now, I don't really have to introduce Remedy because it's quite well-known already, but for the ones who don't, we are based in Espoo in Finland, that would be up there.
And Remedy has been known for over 20 years for iconic narrative-driven action.
So Max Payne would be a thing, Max Payne 2, Alan Wake, Quantum Break, and of course, not at least, most recently, Control.
Introducing Control. Control is a supernatural third-person action adventure and you play as Jesse Faden, director of the Federal Bureau of Control which you suddenly become right at the beginning of the game and you take over this government agency that is investigating that supernatural.
The agenda for my talk today is, well, I'm going to talk about the mission, the challenge that we have put out, and I'm going to introduce sort of our core principle for a lot of the VFX that we've been doing on controls, or the principle of granularity.
I'm going to talk about workflow and implementation, and then highlight some lessons learned and a little bit of an outlook on things that we probably would like to do better in the future.
And to start here, a little destruction breakdown of control.
To the challenges that we faced, a lot of the mission statement was about the location.
So the whole game plays inside this government agency's building, which is a bit of a supernatural building so the walls are moving and everything is a little bit different than what you expect and it's almost a character in itself.
And it's key is sort of it's very, well it's the brutalist architecture, it's very suitable for government building.
And another key was the believability, because it's a government agency, there are thousands of agents all about and it had to feel like a place that was sort of inhabited by so many agents that go about their day-to-day work.
They have telephones and coffee mugs and coffee machines.
desks in their places where they are, in the different departments, and that was quite crucial to sell that and to tell that story of that place. The lighting had a very cinematic look to it and that was the desired style. Control is very big in the whole RTX ray tracing and while obviously the lighting had to cater for that and had to be quite cinematic. And well, Brutalism is all about exposed materials. So you have that boarded concrete, you have lots of wooden glass and you know, the sort of very suitable agency building kind of style. Now, obviously, when we talk about destruction, we talk about the haptics.
So we wanted to sell a rich reactive environment conveying the sense of being able to consistently interact with anything inside it.
And obviously limitations are another challenge because we have target platforms we have to hit.
So we were after realistic, rewarding physical impact.
And we were, no, go wild, create havoc wherever you go, but please don't do that all too much.
Because obviously there's performance and memory that we have to hit and the AI requirements.
And we were a very, very small team to achieve all that.
There's sort of an overarching principle that I just coined, the principle of granularity, and that is really that we decide to represent every level of natural detail in every effect that you create, and it's actually not a thing that's specific to what we did in Control.
This is also a very, very common principle for any VFX work for film.
You try to sort of capture nature because nature isn't quantified.
Nature is sort of a continuum from very big objects down to dust and very very thin veils of smoke and those kind of things and all that you always want to encapsulate to create this richness, this rich gradient across all those different scales to it.
Now if you look at that, how to do this in a game engine, we're looking at an overlapping representation and three different layers and the first one is, well you can do rigid body simulations, you have chunks and parts of props and props and environments and all that is RBDs.
And the environments are, you know, like a big static kind of mesh you can collide with, and objects are like chairs or tables. And then you've got to start maybe representing them with something else, because the smaller you get, the more of them you will have on screen.
So we're now looking at something like mesh particles or rigid body hierarchies, or material decals even, to sort of sell a certain material richness on those layers.
So we're now moving from objects to chunks and almost into the debris already.
And then the last layer.
actually is pure particles, particles, bright particles, for embers, sparks, smoke, fires, blinters, gravel, sand, all that kind of stuff that plays a big role when filling all that kind of gradient.
Now for control, I chose this particular angle inside our research sector to demonstrate the different aspects of this gradient, how we fulfilled it, or how we filled it in our VFX work.
Now what you're looking at here is the static environment.
This is all the stuff that can't actually move.
And it looks like, okay, cool, it's a bit empty and a bit vast.
And then there is sort of another layer and it's a bit underrepresented in this particular angle, but you see like that the railings have been added for example, and this would be sort of concrete parts of your environment that you sort of can interact with, and you actually see it's actually not all that much, it's quite surprising. And then the big difference comes in when you start adding all the props.
Really for that kind of part it just shows the richness the game had to sort of have in terms of like filling everything with bits that you can actually interact with and move, which are the props and all those kind of additional items that can be placed.
And kudos to our environments team, they've done a marvelous job to sell sort of the richness of the place by using all those kind of elements.
For our workflow, it was actually quite a standard thing how you would expect it.
You have an environment build, you have environment artists that sort of provide level geometry modules, kits and props. And then that goes into the VFX department for destruction setup. And we had a lot of systemic setups and prop rigs. And also there was cinematic animation for specific aspects of what we've been doing. And that then goes into Northlight, which is our in-house engine where everything is sort of running on.
But we had to choose a certain approach to this and we went procedural.
And you're like, what does it mean? What does procedural mean?
Everyone talks about it all the time. What does it mean?
It's a rule-based processing and interpretation of real data.
That's really all it is. You get some data and you have some rules.
You apply the rules and change the data.
That's sort of the key of this and how that looked like for us and for this kind of environment process.
Well, we got the art models in the world and there was our data and the data was actually amended or augmented with a bit of extra stuff, extra metadata that basically told us what everything is made of.
So we could say, hey, those cushions are fabric and that concrete is well concrete and the plant is made out of plant.
Once you know that for an object, then you can start actually applying rules on those based on what those objects are made of.
So you could say, well, grass spawns bits of leaves when you shoot it or concrete breaks into sort of smaller concrete bits when you shoot it and spawns dust.
And you shoot a metal pipe, it spawns like a water that dribbles out of the hole and then you add like a bend decal to sort of sell a little bit of a deformation on there.
And you have a finite set of rules that sort of encapsulates that whole process and sort of makes sure that everything sort of has a reaction to whatever can happen in the world.
And then that is fed into the engine and then they impact the effects and breaking the effects and rigid body behavior that is sort of driven by the material and that sort of creates that interactable environment.
Now, why would you do that? Why would you go procedural?
Because setting all that up is a little bit more difficult, a little bit harder than just sort of kind of doing it.
But the thing is, you needed a consistent and fast turnaround.
And we also needed a predictable behavior within very clear defined metrics.
So we needed to know like how many things...
something can break into, etc. And we had hundreds of assets and variations.
So at the top, in that top image, you kind of see like all kinds of building blocks that make out rooms and walls and pillars and concrete railings and all that kind of stuff and all the lots of astral plane things in there. And at the bottom you see a selection of props, which is like desks and toilet cubicles and walls and dividers and planters and plants and computers and telephones and other kind of stuff.
And we had so much of them, but we only had a team for, well, you could say on average one to three people actually working on all of that.
So we needed something that we could just unleash on certain things that come through the pipe.
And when they're tagged in a certain way, something specific would happen to them that we have predetermined.
But you couldn't do it per item because it was just not possible.
And it doesn't scale.
So if you look at the material-driven behavior now, what you would want to do is on the left you see the concrete, you know, you shoot concrete, concrete stuff happens and you look at the middle and that's sort of like wood, when you shoot wood it breaks into chunks or it breaks into those splinter pieces and then they're like splinters on the floor and on the right you see what happens when you shoot glass and that's sort of like three different materials demonstrated here. So the material-based interactions on dynamic objects and then you see the particles and decals sort of reacting.
also to what the material is. And I'm going to go a little bit more into detail on this.
So the first part of this is the geometry layer.
We are looking at a piece of railing here and it's made out of concrete at the bottom and then a bit of metal in the middle and then there's wood on top and from the left to right it's sort of you see a hierarchy of how that stuff breaks. So you see level A that is sort of broken up the concrete. It doesn't have decals on it because you don't want to see all the cracks when you break an object.
And then you see the wood is sort of going on to longer, kind of splintery things, and the metal is a bit bent.
And then level B already doesn't have metal anymore, it doesn't break any further, but the wood does and the concrete does.
And then even level C, also optional, depending on the material, would break even further.
And now it might look a little bit funny here, because you're like, oh, that's like super regular, kind of, you know, this like screams Voronoi right there.
But you've got to imagine that you're not actually breaking all the parts of the object at all times.
So you're literally shooting a certain corner of it, and that's where it would break into all the pieces.
Now, we had different simulation entities and this is like nothing new.
We had rigid bodies that's just, you know, the physical objects that can be flying around the world.
And then we had a system that we called chunks and chunks could have bonds.
And really what that is, that is rigid bodies that sort of share a compound collision.
In some cases it's called a composite, some call it an aggregate, it depends a little bit.
And then we were using drawings as regular.
And so chunks, as I just described, chunks are sort of just a compound.
And they're created at initialization time, so like those two things belong together in the shear collider and they sort of move as one until they break.
So close proximity leaf nodes are basically bonded together.
And if you look at joins, they were just created on geometry hierarchy based on a metadata description. So this rigid body and this rigid body are connected by something like a revolute or a hinge joint to be a door or to be a drawer or you know whatever else we needed structurally to represent a prop or part of the environment in the game.
And they were dynamically breakable based again on an impulse strength.
And there's something special about the breaking of the joints because internally when you create sort of a joint between two rigid bodies what we actually wanted to do, what we actually did is that we actually connected the two lowest level chunks inside of that which essentially means that you can break a door.
And there's a big hole in the door, but the door is still attached to it on a hinge.
It's not like that the joints would actually break as soon as the parent box, like the RB1 box, as soon as that breaks into parts, it wouldn't actually fall off the hinges. It would stay on the hinges as long as the chunk that is on the hinges is still around. And that actually allowed us to have a little bit richer interaction because you can punch a hole in the door and the door still opens and closes like a door for example and that helped to not create those kind of you know fall apart on impact scenarios which is quite prominent in some games where you sort of feel like okay as soon as you touch a thing once it sort of just disassembles into all its pieces. So it retains a bit of structure and that was sort of how that was achieved.
And on the simulation side, inside the engine, it was sort of all Northlight driven, all in-house software driven basically. The data and hierarchy handling, the breaking logic, and what events and particles were spawned around all that kind of process. And then internally, it was physics that did the simulation of the rigid bodies and the constraints.
Now the destruction toolchain, how to create all that was, again, you would say, like, that's kind of pretty standard.
So we have the geometry that comes in.
We had to do a bit of model prep.
In some cases, we created some sort of bonding geometry that would say, hey, these parts here of the thing can break and these parts can't break.
And that went then into a destruction tool inside Houdini.
It's a pretty big sort of HDA kind of setup that would then do the whole material based breaking of everything and write out the data to disk.
And sometimes you had to manually patch some of the physics metadata to just make sure some of the settings are correct, especially when we did more complex things that contained joints and this kind of stuff. Certain values had to be.
set manually in there and then that went into the engine that metadata and the mesh data sort of was used by the engine to then create the world and simulate it. And the destruction tool looked a bit like this so you had an input geo that comes in like this concrete kind of block and then we were sort of defining was which areas can break which is those two wing shape kind of things on the side.
and then decided, oh yeah, it's concrete, it's tagged as concrete, so it would do the concrete breaking and operate it through and sort of create all the different hierarchies that come out of that in terms of vendor and collision geometry and make sure that this is sort of all within the metrics of the style that we kind of wanted to. It would add all the decals and would add like rebars in terms of for the concrete part, for example, etc. And it would do that for wood, glass, concrete, and all the other materials that were supported by it and they would export into Unreal Engine.
And in engine it looks like this, you end up with a hierarchy, it's literally a hierarchy of nodes, and they are, you know, ABC level, under a chunk, and the name drove what the material would be, and also other things about if they were static or not, and some things about the joints, and the type of joints, and all those kind of things.
And so the hierarchy was represented by the layers, by the depth in the hierarchy, and the physics properties were driven by naming convention material assignments.
And all the physics then would be driven off that kind of stuff by the engine when it was set up correctly as well. And that's something I'm going to get back to later on.
And then it looked like this, basically. This is the rigidbody video.
showing that scenario again, just with the rigid body simulations. So Jesse's doing the shooting and then we have this little tool, you click on things and then they just explode. So that's what I'm doing here, basically just destroying everything. And you kind of see this is sort of, you know, physics room is happening, looks a little bit empty, but this is all the rigid body stuff that is actually happening in the scenario.
So now some things about optimizations on the site, especially when it comes to rigid bodies.
Well, at any point in time, we only had about 200 active rigid bodies on screen because that's our sort of the budget we had to operate on to make it run on consoles, et cetera, et cetera.
and that meant that we sort of kind of started culling things that were off-screen and make them disappear. There was a collision delay on big events so whenever there were big forces, sort of impulse waves implemented, that would essentially disable the self-collision on objects for a bunch of frames.
to make sure that you don't do too crazy collision calculations of objects that are fast moving in very close proximity to one another because you don't really need it, because if you put an impulse somewhere, it usually means it's quite clear where things are moving and mostly in an explosion they move apart, so you're not really interested in those collisions so that's why I've used collision delay, well the term collision delay itself I know from the film world as well, we did about the same things It helps a lot with other issues like self-indetect sections and stuff as well.
And then we use the sleeping thresholds a lot.
And well, luckily if things made of concrete and drop onto the floor, they can fall asleep really quickly.
No one would expect them to bounce around a lot.
And funnily enough, the more realistic you go, the more you can actually bank on the whole sleepiness.
So you don't have a lot of bounce when you drop something on the floor.
It really has to be marbles to really have them sort of do a longer lifetime across the floor or something.
So we had a really, really high sleeping threshold.
And it's also the reason that we could reliably stack objects on top of each other, which is actually quite fun when you do that in game.
Well, and then, of course, we filled in the blanks with a lot of particles, which brings me to the next section, which is the particle simulation that we did is everything is systemic and event-driven, right?
So the particle events that we have a bullet impact, so you should think something happens, that was driven by the material, what will happen were particles to sort of play back.
We had bond breaking, so when chunks disconnect, rigid body chunks disconnect, they would have a bond breaking event where particles would be released, which is nice because then you have like cracking.
you know, cracking concrete, sort of pebbles and all that kind of stuff you can do with it.
And then the despawning, so whenever we actually made objects disappear, we didn't just pop them off, we actually made them disappear in sort of, by breaking into particles that were predetermined based on their material, which, if you would see them despawn on screen, you would actually see them sort of feel like they're breaking apart.
And that was very, very effective to sort of hide that kind of optimization.
This is here just a quick view on how we edited particles.
This was all done sort of it's live edit, it's a hot loaded live edit.
This is the game running and you can place down a specific particle system and then you can go in and change things.
And I'm really just going in here, changing like the emission frequency for the sparks.
And I think I'm like, oh, maybe that is actually too many sparks.
Maybe I go down on that.
And then I changed that, a lot more rocks to the simulation.
And the cool thing is you can literally play it back live.
So you really get an instant feedback and then you can.
go around and chew the thing and see how it feels like and then go back and keep iterating.
And that really fast-paced iteration loop really allowed us to polish that kind of stuff very, very, very much until it really sort of felt about right and then was very, very beneficial.
Another specific part about the particles is standard simulation, but we also hijacked the rendering SDF at times, so you could optionally collide with the SDF, which is, well, so much faster than colliding with anything else.
And that sort of took care of a lot of the particle collisions, not to have things either fall through the floor or look really weird.
So we use that.
This part here now would be the particles and the rigid bodies at the same time.
So now it's the same thing as before I was shooting and then I'm going around and just exploding everything and you notice instantly, oh my god, this is like so much more responsive and so much more fun because there's stuff happening, there's dust in the air.
It makes it instantly much richer and that sort of, well, it's all for the additional particle layers sort of that fills in that missing that gap in that gradient of granularity that is now covered by that.
And the last aspect is material decals.
So we have a lot of them, we dynamically spawn them.
So if something breaks, it would sort of spawn sort of ground decal of like broken shards of the kind of stuff on the floor.
If you slam the floor, it would crack the floor by decal that we had generally generated sort of in Houdini or Substance or things like that.
And we could dynamically spawn them based on the material they happened on as well.
And that sort of helped to really also include.
the quite big portion of static geometry you saw in the beginning. There's like a lot of kinesthetic stuff around. But as soon as you provide an impact on that, it really makes a difference. And this is how that looks like, right? So, you know, if you hit the floor, it still is just a single polygon, like a single quad, but it really looks much different when you start adding decals. I mean, decals are kind of awesome in general. And yeah, we could sell all kinds of effects on that. And it's quite surprising how much you get away with.
Although you don't really have a lot of texture budget, even just a few of them really instantly make something sort of connect to a destructive element, like throwing something on the wall, and then you kind of have a dent in there. Although, you know, it's concrete, it wouldn't really make a dent, but it sort of helps with making it feel just about right. And we're talking superpowers there.
So that's now when everything comes together. So we have the particles, the IBDs and the decals.
And I had to trick a little bit because just the explosion tool wouldn't actually spawn that many decals. So I think I started to launch something up in the air, like I think I take a chair and throw it. And then you kind of see that I make a dent in the floor and the floor still is just static geometry, but the decals make it so much better. And yeah, this is sort of filling the last bit on that missing gradient.
And the other part is custom props and hazards.
There are a lot of things you can throw around, like computers and lamps and stuff like that, that wouldn't be fully procedurally generated.
They needed some more custom events and custom things that artists created to make it feel rich.
So you have the fire extinguisher that obviously can't be missing, same as a computer having sparks and all those kind of things, and a cable attached to it and stuff like that.
So lessons learned. The four main things I want to talk about is the quality of geometry of input data, because procedural is driven by data, so it's all about that.
Name-based convention, kind of the same the alley really.
Monolithic destruction tools and performance monitoring. So the four items I really want to pick.
And the first thing, quality of geometry.
That is a problem I've dealt with my entire career in film and now in games.
Inconsistent incoming geometry.
And it can go, well, it's scaled the wrong way.
The orientation is wrong, but it can also be the material assignments are wrong.
You know, something that actually looks like wood, but it's sort of tagged as plant or it's tagged as flesh or something.
But sometimes also the mesh quality being too low.
And that could mean that the subdivisions aren't nice or it's sort of really single-sided and that kind of stuff.
Sometimes you break things and realize, well, there's nothing inside.
But there should be something inside because we can break it and that's sort of a difficult part of communicating standards and making that all work.
And obviously we're using procedural tools to break that kind of stuff.
So it has to be of a certain way.
And what can you do to fix it is improve the incoming data, standardize across the CCC geometry pipeline, making sure that everything is sanity checked and export.
So at export, the exporter would say, hey, this doesn't fulfill the criteria it needs to fulfill, please fix it before you export.
And that would sort of help a lot to avoid that, that constant feedback loop between different departments even and sort of put the problems exactly where they actually caused it can be fixed at the same time.
Also, what would be good to have embedded tools to simplify the setup.
So you literally have someone who's modeling a prop being able to instantly see what it would look like if it gets destroyed.
But obviously that puts a lot of strain on creating more tools with more user-friendly interfaces and all that kind of stuff, and that is a lot of work.
And then you sort of have to find the right trader for that.
But we are definitely looking into that kind of thing.
Name-based conventions, well, we name things that then get interpreted to be something specific, and the issue with that is names can be wrong.
Apparently, there are 17 different ways how you can spell concrete, for example.
And it's not really to blame anyone, because if it's a manual thing that artists have to do, it's given that there will always be a certain percentage of data that is just not correct.
And you can also not express a lot of the range that you'd need in especially physics definition stuff.
with just naming things. And yeah, and then the mesh hierarchy might not be standard conformed because you know what happens if you do things in Maya, you shuffle things around and you do all kinds of operations on it and then your hierarchy just goes bust. So while all I can say about this is don't ever do name-based conventions, abandon them, use physics authored straight in the DCC so many people tag things directly where they need to be tagged.
And for that, you probably need some kind of unified metadata API.
So from whatever tool the artists are using to creating their props, they are able to actually author that type of data and information as well.
So you don't have a translation process in between.
You literally export the data that you want to export straight where it's being created.
Monolithic destruction tool, it's almost like a Houdini specific thing.
But in general, you don't want to have the one tool that does it all.
And quite often it happens that you start with one thing and then you start adding something and you start adding something.
And the more you add, the more powerful it gets and the slower and harder it gets to maintain.
And very inflexible.
The answer to that is you've got to be more granular, but more granular comes with lots of challenges.
Because we're looking at a lot of individual interchangeable components.
And the problem is you have to maintain them somehow.
You need to make sure that you version them correctly.
You need to make sure that...
Even two years into a project, you can open something from the beginning of the project and you're still able to sort of work with that, although the tool might have changed 20 times in between.
So for us, it means an improved standardized HDA management that we're now operating on and making sure that everything is namespace and everything is properly sort of distributed within the artist.
So you will never actually lose any type of version of a tool.
You will always be able to rerun something that you did in the past, especially for long production cycles.
So it's very crucial.
And also what is important that you're making tools that you can fully automate and you just run them, but actually use exactly the same tool if you would do it manually.
So there aren't actually any differences in them and that will really, that is very flexible then for you to say, hey, actually this is automated really nice, but then I go in and tweak it and do something different to it in the places where you need it, those 10% of the cases where you kind of need to do something with it. But as long as they have the same backend, everything should be consistent and that is quite important.
Now performance and testing is almost the biggest aspect of it.
We had no automated testing of functionality so it was really like you put something in the level, run around, you shoot it a bunch of times and you see if it works and that's kind of nice. But then something changes in the engine, the backend changes, something's being optimized and all of a sudden you have to kind of retest and that is very dangerous because you will always forget something to test and the effort is quite big. So you will have a potential regression over time and that is something that needs to be improved.
to make sure that we don't run into this kind of problem.
The second part is performance testing.
So we don't have any tangible metrics.
Like what do you, it's like, you know, you can't really go by FPS.
It's not as easy as that or like millisecond spend on calculation because it's not a thing that you constantly do in every frame.
So you can optimize for a specific scenario, but it doesn't really cover what everything is.
So quite often performance issues only flare up when they were spotted.
It has to do with, well, we make the props breakable and then they're being put into the level.
And sometimes the props are so nice, they're being put into the level a lot.
And all of a sudden you end up with areas that have...
Sort of lots of props, desks, computers placed and then, well, someone in gameplay adds a bunch of enemies that throw grenades and then you earn a pickle.
But you don't want to actually make everything break a little bit less nice just because of that. So somehow you need to find a way how to deal with this.
Especially then that you can only react to the full picture very late in the project.
And the two things that we kind of can do about this maybe is better performance metrics.
So be able to sort of, even if it's magic numbers, something that falls out of the system that gives you some number, which means if it's bigger, it's worse, if it's lower, it's better.
You can then start optimizing against that kind of stuff.
And also to use those magic numbers to establish production boundaries, sort of like physics bandwidth.
Because saying, oh, it's 200 rigid bodies is too coarse.
It doesn't necessarily mean it's bad if you have more than 200.
It's only bad if you have more than 200 and three hand grenades.
And that's that kind of stuff is something that needs to be balanced out correctly.
But if you have that kind of metrics, you can push the boundaries a little bit more.
And the second part is automated testing that will help with that as well, and an automated assessment of assets through a testing environment, so you can diff outputs and see the impact of engine changes.
Another thing that can be done is just performance-issue countermeasures.
So maybe something that can adjust the physics performance at runtime, and obviously you can't change those distraction hierarchies necessarily, but you could say, hey, maybe this guy goes from breaking into level two straight into breaking into particles and despawn, and not into another level, because oh my god, mayhem is already happening, hand grenades everywhere.
And so that's one part of it.
But for that, actually, it would be good to actually have better physics performance metrics.
Because right now it's very hard to get that kind of information really out of physics, for example.
And then the second part, well, zoning of areas based on expected and load.
That's an idea sort of to say, hey, you know, you might not have known that in the beginning.
But now we have an area with lots of really awesome breakable props.
And we have guys and hand grenades.
And what do we like? Enemies and hand grenades. What do we do?
And maybe you could put a box around this and say, hey, in here, you know, heavy load physics, maybe, you know, ignore the last level of destruction hierarchy or stuff like that.
And that would sort of help us to manually zone certain things where we know things could get bad, to avoid having to downgrade all the assets for the entire game just because of a certain area.
And that could be another thing that could be done about this.
Well, to a conclusion to come to the end, while controls reception was very well, it has generally been quite liked and especially been quite liked for its destructive environments and for the feel of launching an object into a bunch of desks and enemies and have everything go to bits.
So I think we are quite happy that we achieved this.
The workflow held up better than expected almost.
So it went really, really well considering the small team that we had.
And we are quite happy that this actually worked and we are quite inspired and excited to see where we can take this one next.
And what remains for me to say is thank you to Matti Hemmellein who did the greater part of the destruction work for the destruction tool and all that kind of stuff.
And then Sami Hakkarainen on the NorthLight side doing all the physics programming and Daniel Forsberg for the particles and also the render side.
But also the entire VFX department.
remedy that sort of obviously partook in all the different moving parts of control and made it what it is today.
So thank you to them and thank you for tuning in.
