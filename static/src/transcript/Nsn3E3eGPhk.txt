Hello, audio peeps, friends and GDC goers.
How's everybody this morning?
Making it here, all right, good deal.
Well, today, we plan on taking you on an audio adventure of sorts.
An archetypal hero's journey.
A journey packed with, well, adventure, excitement, risk, but also fraught with perils and trials and adversity.
And finally, overcoming the odds, not with brute force or anguish, but with finesse and fresh as well as recycled ideas.
The setup is an all too familiar scenario for audio teams the world over.
Lack of resources, lack of understanding, lack of love.
So let me introduce you to the intrepid trackers on this journey. We have Jacqueline Shoemate, audio lead, sound creation wizard. Able to Jedi mind trick even the most stubborn producer. We have RJ Mattingly. Technical sound designer and scripting ninja whose audio systems and logic are making us wonder if those cybernetic enhancements were too much.
I'm Guy Whitmore, composer, musical trickster, dreamer, and disruptor who surrounds himself with awesome people to make myself look good.
The team is rounded out by Becky Allen, sound designer ace and organizer extraordinaire, Damian Kasbauer, wise savant and smoke jumper, and a cameo appearance by Mike Caviezel on sound design.
There he is.
Right on.
whole team is here. We are not going to waste any time. We're going to jump right into some game play. And now the basic idea in peg goes that you shoot a little metal ball and you try to hit as many pegs and clear them.
is, you know, clear them, especially the orange ones.
Now, there's a difference in mechanic between the earlier pegals and Pegal Blast, and we had to do some different design here.
Here's our metamap, and here we just set the mood.
We're just making it sound pleasant, you know? Here you are.
And a little sound seed air to get you moving.
And we're going to go to a turtle level here.
One of our tenets for all of our games is that things move and transition smoothly from one area to another.
That's one of our overall goals.
So here he's starting the game. Let's see what he's going to do.
Nice shot!
Yeah!
Notice how the bumper was going up in pitch as it was hitting.
Little tension builder there.
Turtle now has voiceover, an addition we did after launch.
So his special feature is that I can catch the ball.
Yeah, every master has their own power.
Oh, that one!
This is wasteless.
Oh, yeah!
Nice!
Wow.
Alright.
Not all shots can be excellent.
Let's do a fireball.
Alright, fireball, there's these powers that you get.
And it just clears half the room.
So notice how the music is progressing as you're moving along here.
Okay, one peg left. It always comes down to the tension shot.
And...
He's gonna get it!
No!
Okay. Near miss.
Oh my god. Plenty of opportunity here.
Alright, so there you have it.
Now we're going to jump over here.
So, just as a stage for you, we were given a 5 megabyte total asset limit for our audio footprint on a mobile platform.
To give you some comparison, we had just shipped Peggle 2 with 783 megabytes on Xbox One.
Um, yeah, thanks.
Why 5 megabytes?
Why 5 megabytes, Guy?
Well, the game as a total was only shipping with 50 megabytes.
So we got 5.
Um, it's common for freemium games and we were given a really hard limit with no wiggle room.
So we had to make it work.
Had to do a download, not RAM or any other issues.
Right, exactly.
It was the first Unity game from the studio, which meant that we got extremely limited dev support. Resources were put in other directions. Um, in addition, we really believe that mobile audio should be and can sound good and we had Peggle 2 sized expectations for the project. Um, I'm a sound designer, I really like to make content and I need space. Like I just take up a lot of room. So this was one option.
could have made a 30 second sound loop, five sound effects, ship it and went on vacation.
But then we heard a knock at the door. It was Igor Stravinsky. And he said, the more constraints one imposes, the more one frees oneself. And the arbitrariness of these constraints serves only to maintain the precision of execution.
Wow, did we feel that on this title?
And in the end, I really feel that our limitations actually gave us focus, purpose, and a drive to innovate.
And I really don't think our aesthetic, I think our aesthetic would have been wildly different had we not had these constraints.
And it probably wouldn't have matched the game as well, either.
So to start out with, we looked to our mentors, our inspirations.
I know that some of you recognize this sound.
And this one.
And I don't even have to tell you what games they're from, right?
They're fantastic audio.
So I looked to classic games and slot machines, too, actually, for examples of audio that convey a lot of emotional resonance with a very small footprint.
And for my part, when I got the five megabyte memo, I actually thought, small, what is small that could still be compelling?
And a music box came to mind, and so I wrote this demo piece to kind of try it out. I mean, you could almost fit a music box in an iPhone, if you think about it. So I wanted the idea of holding this little thing that's marionette-like.
So we put it in the game and the game team kind of had a complete freak out.
We loved it.
Yeah, we loved it.
We had a soft spot for it, but the game team didn't like it.
They wanted the orchestra back, they wanted Ode to Joy in all of its glory, so we had to readdress that.
But that's part of the exploration process.
Yeah, totally.
So from a technical standpoint, even though Peggle Blast is much different from Peggle 2 in a lot of ways, obviously there are...
a decent number of similarities. So the first thing that I did was kind of take a look at how audio worked in Peggle 2 and try to find out how we could optimize some of the functionality for the mobile platform. Obviously we had much stricter guidelines that we had to fit within. So try to figure out how we could get everything that we wanted in there. And you know, we started experimenting, looking around at what we had available, but really it came down to we just needed to make a decision, commit, and move forward.
And that decision, there was a specific moment, and it had to do with do we take on this MIDI technology that was in a beta version of Wwise.
And we knew that it was possible we might even have to ship with a beta version, although we didn't.
So there was a moment of truth.
And boy, do I remember that moment crystal clear, sitting in a room, talking about the options, and finally deciding.
of course, to take the red pill.
So we did.
We went down this journey.
And here's the rest of the story.
Yeah, and once we did that, once we decided we were going to commit, we went back and started seeing what this tech would let us do.
And we started experimenting with it.
And that kind of brought us back into the creative realm of like, oh, we can do this and this and all these different things.
which once again brought in new technical challenges.
And we kind of found that that was this cyclical nature of our entire process was creative leading to technical and back to creative just all the way through.
So I'm just gonna give you a quick run through of how we made sounds for Peggle Blast.
Our question became, how do you make sounds without taking up memory?
Our daily challenge was make something for free.
What we ended up discovering was.
of what you have on hand.
Within Wwise, we had real-time synthesis with SoundSeed Air and Impact, the tone generator.
We had lots of digital signal processors, creative playback behaviors, and a tiny bit of memory to play with that we could use real WAV files, very specially chosen assets.
So the basic process looked a little like this.
As you can see here, you have your sound source, which could be sound seat air impact or a tone generator. The power of this is that all of these features can link into realtime parameter controls on your sound. This is all within Wwise, right? This is all within Wwise. You can add effects at any layer that you want to. And again, link any aspects of the effects, or not any, but many, into realtime parameter controls.
And then play around with different playback behaviors, also within Wwise, which also link into real-time parameter controls.
So basically, it becomes layers of created sounds with various properties, combined with layer of effects and with layers of playback instructions.
The individual elements are really simple, but the combinations can get really complex to create a varied and interesting palette.
And those stay real time in the game, right?
You never mix them down, right?
Real time.
Everything is real time.
And that is the beauty of it.
So now I'm going to give you my real time demo, which is always a little dangerous in a presentation.
But in the spirit of real time, I felt like it was the right thing to do.
So within Wwise, I am going to make a sound for you.
It's sort of set up already.
This is actually based off of a sound that Becky Allen created for the game.
So the general workflow would be I would look in unity, see what needs to be made, and then I would get sort of a basic sound going, um, within Wwise that I thought would fit that and then adjust afterwards. So here we have... an explosion, which is one of the real assets that we used. So within that I can add, um, a swoosh. Let's just see what this is going to sound like.
And then I can make it squiggle a little because that'll be fun.
It was all about playing and experimenting and trying new things and turn the frequency up.
Sure. That's pretty cool.
And then I'm going to also add a sweep sound from tone generator.
I've seen this tone generator in Wwise a lot, but I'd never even thought about creating a sound with it because making assets in a DOS, really the way that we do things these days, or the way that I do things these days, I should say.
That's kind of cool, except I want it to be really short and I want it to sound kind of angry, so I'm gonna change the duration and make it a sawtooth wave.
Cool, and then you know what?
I'm gonna add an effect.
Why not?
I think this is going to sound alright.
No.
Okay, I'll take that.
And then here, I'm going to add an RTPC.
Again, everything that I'm doing I can link into a real-time parameter control to any aspect of the sound at any point because it's all generated from within Wwise.
So here what I'm going to do is utilize pitch, which is something we often played with.
This sound is for a little time bomb. I have a counter.
have the pitch of that sound raise over time, uh, the more the time bomb is hit. If I wanted to, I could add another control, um, in RTPC into this, uh, just the tone generator sound and adjust the pitch of that as those, as those time bombs go off as well. So there's really just, it's just so powerful, there's so many options. And then to, um...
to audition in Unity and continue to tweak my sound and actually make it match the game.
I could use profiling. I've always used profiling in the past for mixing and for bug tracking and things like that.
It's basically everything that I'm doing in game, which is pretty awesome.
And so now I'm going to go back to Unity, and let's say that I thought that actually this sound should be a lot longer, it wasn't doing what I wanted it to do.
I'm going to change that to .6 seconds so that the tail sounds cooler.
And there you can hear it was a little longer.
So that was basically the whole process, was just going back and forth between those.
So it was really an illuminating process to work this way.
There were a number of advantages.
First of all, there was no DAW that was necessary.
I'm mad at Avid a lot of the time, so it was really nice to not have to spend that much time in Pro Tools.
I didn't have to make a game capture, bring it into my DAW, and then spit out a sound, and then rinse and repeat that.
That meant that iteration was really quick.
animator would change an asset and I would need to change my sound. I could just go directly into Wwise, change the timing of a few different layers of my sound effects, and come out with something that matched the new animation. The biggest and most powerful part of this, I really felt like, was the easier implementation of sounds controlled by RTPCs. Part of that was because of RJ, which you'll hear about in a minute. And part of that was just because everything was already within Wwise. You just had so much control over every sound and how it was playing with the...
game. And it became this really malleable element of gameplay. It also then led me to feel like I had no barrier between me and the game because I was always working in the game. I knew it was happening all of the time. On this project I couldn't be as integrated with the team as I usually am and that was really disappointing. But because I was so integrated in the project and working within the game, I didn't feel that disconnect at all. It was really fantastic actually. And More than anything, I feel like there's so much potential to use this on games big and small.
And none of that I would have discovered had I not been faced with this limit to begin with.
There were.
some challenges. There was a limited palette. I really wanted to work with more synths and just have more things available. Having a bigger memory footprint would have helped as well so we could use more wave files. I wanted more real-time mastering tools and the ability to use more real-time mastering tools with more CPU. Limited CPU meant that I had to be a little careful with some of that and voice limiting was tight because instead of playing one wave file at a time which maybe has...
three sounds or four sounds that you've created in your DAW that takes up one voice.
With these, maybe there was eight voices and one sound at one time.
So we had to be really careful about how we were designing assets.
But all in all, it was a really fantastic way to work.
And I look forward to using it on more projects.
Which takes us to the implementation side of things.
So as Guy mentioned.
We really were lucky enough to have a lot of control and autonomy kind of across the spectrum for audio for this project, from creating sounds to getting them in and creating our own systems.
And it was really great.
I've heard a couple of other talks already this week about that kind of style and bringing more control into the audio team's hand.
And I think it's a really great direction to be headed.
So just real quick, what enabled us to do what we did for the project, we used Unity's.
Our main engine was Unity.
In case anybody's unfamiliar with that, it's a very game-object-oriented and drag scripts onto different objects kind of pipeline.
And this was great. It really kind of blurred the lines between what people call like deep engine-level code.
and the kind of high-level scripting that people are familiar with doing in audio.
And that was really crucial. It enabled us to go way beyond just simple play sound events, again, on our own. We were able to create and implement entire audio systems from beginning to end, and then manage those systems within our own code that we didn't have to burden the rest of the team with.
A real quick example of that, I remember...
Kind of early on in the project Jacqueline and I were talking about One of the character special abilities Bjorn. He has this super guide kind of a rainbowy thing that helps you aim And we were thinking wouldn't be cool instead of just having you know Obviously a static looping sound if we had it kind of dynamically react to the players swiping movement and And it seemed like a great logical way to go.
Oh, I didn't even go into that.
OK, anyways.
So I started digging around and seeing how we could do that.
And sure enough, within the game, we had this object called the shooter object, which turns out was that cannon that you control.
So once I found that, Unity has a lot of built-in functionality, like the get rotation call.
And we were able to just get the position of, get the rotational position of that.
and map that to a variable.
And then getting the speed of that was as simple as measuring the change of that position over time.
And then we just map that to an RTPC value down there.
And since we had the position in order to make the speed anyways, we might as well put that as its own RTPC value because.
RGPs are great and they come in handy all over the place. I think we ended up using the position to do some left right panning on top of that sound effect.
Yeah, one thing I want to throw in here, this was all done outside of the team scrum, the team feature list and all that. We just decided, hey, wouldn't this be cool? Walked down the hall, made it happen, it's in the game. The game team goes, wow, that's neat. No friction.
Yeah, it's a really relatively small feature and it's nice to not have to...
make a big deal out of it to be able to just, I mean as you can see it's a few lines of code, pretty easy to put in, and if I recall correctly, Mike Kviesel actually ended up making this sound, so if you hear it and it sounds especially awesome, that's why. Um, but uh, so it quickly became apparent throughout during the project that with this great power came a pretty easy ability to over step our bounds. As we offloaded the memory sounds and put them kind of in the CPU's hands we found ourselves kind of doing too much without holding it, without keeping anything back. And so we started looking for ways to come up with better ways to some performance improvements to try and get those voices down.
And over some conversations with a couple sound designers from other EA studios, Nick LaMartina and Nick Von Cannell, they kind of pointed us in the direction of this multi-tiered quality device idea where.
you know, if you're playing on an iPhone 4, it obviously has much less capabilities than say an iPhone 6. And we wanted to try and make sure that each device was getting the best sound that we could get out of it. So we created this audio quality manager that just looked at, start up about what the, what the device was and what its specs were and we mapped that into an RTPC for kind of a low, medium, high quality setting.
And from there we were able to do things like mute or kill, some of the less crucial sound events as Jacqueline was showing you.
some sound events could actually be made up of maybe like 10 sounds in a blend container which on an iPhone 6 would be great, but as you get into a 4 maybe it would make it chug. So we kind of said okay well if you could do without two or three of these would you still be able to get the point across and we were able to mute those and just save some CPU. We could do things like disable or swap out effects plugins. We had two different reverbs one for low devices and one for the medium and high.
with just some more conservative settings.
And then the obvious one, changing bus level voice limits.
I think on the high tier, we were allowing maybe like 50 voices for sound effects.
But as you get down to the low quality, we had to clamp it down to maybe 15 or so.
So that was pretty effective.
Yeah, that was really, really something.
Otherwise, I think low end Androids would have been our least common denominator, and everything would have had to have been at that bar, despite the iPhone 6's abilities.
Really be able to take advantage of what each device could do.
Yeah.
And then throughout the project, one of the biggest things, as a lot of people will say, was the ability to performance monitor in real time on device.
We could do things with the Wwise profiler, like measure the CPU of each effects plug-in.
As Jacqueline was showing you, we used a lot of effects all over the place.
And it became important to find out which ones were taking more CPU than we could spare and get those clamped down.
Also, we we used a lot of really, really short looping sounds that we found, if we had a .01 second sound looping, within one second you've got 50 voices that are stacking up.
So the profiler was able to help us find those and take care of them right away.
Hey, RJ, what did you do for easy auditioning of all of these sounds across different specs?
Ooh, that was a great question.
Yeah, so, you know, since we were doing this to test different device settings, we wanted to come up with a way that you wouldn't have to test on an iPhone for.
and an iPhone 6. Each time you made a sound, obviously at some point you would be doing that. But right away we wanted a way to kind of simulate what those settings would end up looking like. So within the Unity editor, we just kind of created a fake version of that RTPC so that you can in real time change it to low, medium and high and see which sounds are getting cut off and make sure that it's still sounding like what you want. And that is pretty much the main overview of implementation of the tech side of stuff.
Awesome. Awesome. Thanks RJ.
So now we're going to talk about the music side, the scoring aspect, and we all participated.
And I have to say, scoring Pegel Blast was definitely...
Delicious!
Nice!
So I had a lot of fun.
Imagine sitting in front of a professional choir going, It's pronounced Pegel-icious.
We know how to have fun here.
So the first part we had to think through, I did, was the palette.
A lot of times when you're starting to score, what sounds do I want?
what timbre do I go for?
And this image is apt because this is Aaron Mindendorf's art exploration of Bjorn.
And what colors do we use?
What shade?
And all that.
And once it was decided we were going with MIDI and sample banks, there were a couple of elements that became critical to helping decide this.
So those three points that I decided to work with are custom sample banks.
and instrument choice and real-time DSP.
Now, custom sample banks over pre-created ones, which don't even exist on Wwise right now, have an advantage in that by myself creating them, recording the sounds and editing them myself, I can tailor these sounds specifically to the score I want for Peggle Blast.
They're not generic samples like you would find in a typical library.
And also instrument choice was important too. After it was decided, well, we really need to bring back the orchestral sounds, how do I do that in a section of five megabytes? One of the tricks there was to use a lot of shorter sounds that were staccato or marcato or pizzicato, plucked sounds, mallet sounds, all of which have a lot of character with their attack but are very short samples. That combined with short loops created a sample bank that was really, really efficient.
And then real-time DSP is critical for anything that has multiple sounds being mixed into one.
So for the music, piping it into a real-time reverb bus allowed the whole thing to kind of come into the same world, the same sphere.
So here we're going to take a quick look at the palette.
Under here you can see basically that's my full set of sample banks for the music there.
You can see the string stuff, the music box, the choir, clarinet, brass, timpani, et cetera.
The opera bat, which is Becky doing her opera vocal.
And a little bit of synth there, which you heard in the beginning.
And here's a setup for essentially how a sample bank is made in Wwise.
And you can see after I imported the string staccatos here, you know, you set up their MIDI tracking note.
And you can see there that there are two notes per octave.
And I either did...
two samples per octave or three samples per octave or one sample per octave, because per octave, that's all I could afford.
But it worked out really well. Single layer sample banks worked really nicely in this case.
Over here are the peg hit sample banks.
Harp is the general peg hit that comes across everything.
And I chose harp partly because I knew there'd be these nice glisses across pegs that would sound like a harp.
And we also had a sound effect element that just gave it a little physical impact, no matter what happened there.
And then the marimba gets layered on if you hit an orange peg, because orange pegs are more important to the player.
So we wanted something to accent that.
And over here, we're looking at the RTPCs that are applied to any given sample bank for the peg hits here.
We have pan left and right, so if a peg happens on the left, you hear it on the left, et cetera.
Transposition, which RJ will get to in a minute.
Voice volume, which is your ADSR envelope.
And velocity sensitivity.
You did all that with RTPCs here.
That's how they're built. So it looks like Wwise, but it's a sample bank.
Here's a quick look at the.
you know, a tactic case, a stain release that you can build for any given sample bank. So, the notion of a spotting session is something that's probably familiar to folks who have done film scoring, television work, and linear media. This is something I found, you know, the Planet of the Apes, Jerry Goldsmith's Planet of the Apes thing here. And you can see each cue has a very, very specific timing, it lists the instrumentation and so forth.
and in his case, even the arranger that's going to be working on it.
And it's really common to see film cue sheets go kind of in a spreadsheet-like fashion like this.
But what about a game?
The games aren't linear.
So we can't do that.
We have to have something that's more dynamic.
And a spreadsheet will not work for this.
So the elements I needed to consider were harmonic flow, the section flow, and the progression.
the range of gameplay, fast players versus slow players, et cetera, and stingers.
Now, stingers are accents that I can drop in over the top of the music anywhere.
So those are the elements.
And here's what my spotting session looked like.
on our whiteboard.
And you notice it's not linear.
It's circular in a way.
And starting up here, the meta map, it was where you start the game.
You enter the game here.
It's in the key of A, so that's part of my harmonic map.
And loading is also in the key of A. And after it loads, here's the first thing you encounter is this section of music in D here.
Now, I wanted, again, to treat each peg.
I am sorry, each shot, each shot of the ball as kind of a new scene.
Because the player spends time, they set up a shot, they shoot it, and then OK, there's another thing.
Now, a common technique in the linear world is to do kind of harmonic shift over the course of a scene change, either right on frame or just after frame or preceding it.
And so I wanted to mimic that here.
So every time there's a shot, it moves to the next thing around.
the circle. So this circle represents a gameplay loop and it moves clockwise here and rotates around and keep going around and you can't predict how many shots are going to be fired in any given game. Sometimes it's five, sometimes it is 12, sometimes it's 24. But this circular form allows it to kind of keep continuing. You'll see it's a circle of fifths so the progression can kind of just keep going there. Now that can lead at any point from any of these.
sections of music here can lead out to...
some of these completion scenarios, and starting with last ball or last peg.
Not listed, but they're both in the key of A, and the reason is that fever is in D, so I've got my tonic-dominant relationship there when I end up hitting fever.
But you can also go to what's called a point of loss.
You don't get that last peg, and I want to hold you there for a little bit while you make a purchase decision whether to go back into the game or to just leave the game.
So these points are critical in that the player can sit there and ponder and you have to think about that and there's not a fixed amount of time, so they have to kind of continue before moving on.
And the goal is to make that as smooth as possible.
So, last peg, you hit fever, ba-da-boom, it's all great, you say yes, next level, it will load the next level and start at the top again and there's your loop.
So any game has a game flow, or what a lot of times in the business now is called the game loop.
What's your game loop?
So pay attention to what that is and make the designer your best friend so you can figure out what this is so you can map the music to that.
So compositional workflow was composing in Pro Tools for this project for the most part, but by the end I ended up switching to Nuendo.
for various reasons.
And if you haven't seen it yet, the Nuendo-Wise combo thing that they've been talking about is probably gonna be really cool to see.
I haven't played with it yet, but I can't wait because I've been experimenting with this kind of loop and I know what needs to happen.
But the first step.
is I'd create a temp sample bank in Kontakt in Nuendo or Pro Tools and create music the old-fashioned way, just kind of create all the sections and the cues, I'd write all the cues here, to gameplay captures, so I could see the game at least.
And once that was kind of in a pretty good place, I would point the MIDI tracks in Nuendo to the custom sample banks that I built.
So treating Ys as if it's a sampler right next door.
And that allowed me to edit with all the fine-tuning edit capabilities I have in Nuendo or Pro Tools, and mix it, make sure the velocity is working well.
And once that's sounding exactly the way I know it's going to sound in game, I export the MIDI tracks and import them into Ys.
and then Wwise can take over from there and I can take the MIDI files and arrange them into the segments and playlists as needed.
So that was the general workflow.
It worked really well.
So by the end you were effectively using Wwise as your sampler?
Yes, Wwise was my sampler and here it is.
This is what Wwise looks like with MIDI.
It looks like a piano roll.
like you would in a, you can do some basic adding but not a lot yet. I'm sure they'll be adding to that. Here in the profile you can see samples just flying by in real time. I have my voice count. I can see my CPU that I'm using. I can see the routing and effects that I'm using. So all this tracking that I can do in real time as I'm arranging and writing. Even when I'm piping in the MIDI from Nuendo, I can see all that stuff going on. So it was really, really valuable to have that going on there.
So any given music segment needed to be set up for the peg hits.
Because as you know, the peg hits drop down and play over the top of any given music.
And as you see, there's 12 different key centers that are possible.
How do you do that?
Well, in any given segment, I need to consider the key center, the quality, and the range.
The specifics there have to do with these custom cues I would drop into Wwise.
And the key center in this case is A-flat.
The quality is Mixolydian.
And the range is a MIDI range of 59, sorry, yeah, 56 to 99 MIDI note values.
So I could drop those in at any point in the segment, usually at the beginning, and then that would tell the Peg Hit System, which you'll hear about next, what to do.
And with that, RJ will take it from there.
Yeah, so to back out high level for a quick second, the The goal for the PEG hit system, well, we had a couple of goals.
The first one, we wanted to take advantage of the MIDI since we were using it elsewhere in the game, which inherently would allow us to use fewer WAV files.
In PEG-L2, each music segment corresponded to a separate playlist of PEG hits with a one-to-one WAV file corresponding to each PEG hit.
And then that also helped with.
caused a little more implementation time, right? Every time a new music segment would come in, you had to create a new playlist and then fill it with all of the appropriate peg hits.
Seems like it would take a lot of time. So we wanted to reduce some of the implementation time for that. Also, the other consideration was Pickle Blast being a live service game. We had no clue, you know, how much music I would have to write over the course of the game.
and we didn't want to have to keep going in and maintaining the system every time we added new content. And then the last thing was to bring the peg hits into the music system which kind of to me felt like a natural thing. The game, the peg hits are a very musical part of gameplay.
Oops, hang on, we'll go back, we'll go back. Yeah. So to go back to where Guy left us off, he He would start by dropping the markers at various points in the music segments of gameplay that corresponded to new keys that he was switching to.
Those strings were passed exactly into the Unity game engine through Wise's music callback system, where Unity would parse those out into the four separate segments, the key, the scale, and then the start note and the end note, so that we could generate the new key on the fly.
we, we also decided that we didn't want to, instead of having to create some giant lookup table for all of the keys, scale, combos that had all of the possible MIDI notes that seemed very complicated to create, maintain, again we, we wanted to avoid all of that stuff. So we created the, the core, the new keys on the fly based on the parameters that define the key.
Let me explain what that means. So basically we converted, we converted the keys to all scales into their numerical equivalents.
Everybody knows that the major scale is whole, whole, half, whole, whole, whole, half.
But if you just switch those to twos and ones or threes, it's very easy to see that you can define any scale with its numerical equivalent.
And as Guy pointed out, you could even create your own as long as they all added up to 12.
So that was.
very cool. And then this is just a little example from within Unity. So then once we had those lists, we would parse those out in a little script, which is basically, this is the part you're looking for. It's just kind of grabbing each one of those numbers and building a list out of those, so that when it came time to generate a new key, we just looked up whatever the key was, if it's major, we grabbed that.
We start on the first MIDI note.
If it's a G scale or a G key that he's calling, we know that that MIDI note is number seven.
So we start there.
Set the first note.
And then we just start iterating through those scale degrees that you saw us set earlier, either until we hit that last note or until we hit 128, which is the logical end for a MIDI scale.
So what we just did there is we built out our full scale of all the possible MIDI note numbers that the playback engine could play.
So then, once each peg was hit, all we had to do was increment to the next MIDI note number that's in that list, and theoretically fire up the MIDI event.
The one caveat there was that currently the Wwise implementation of MIDI requires that all MIDI notes, all samples be played by an actual MIDI file.
You can't programmatically say, give me an 87 MIDI note with this velocity.
So what we had to do is we had to actually point these events to real MIDI files.
Now instead of bringing in one single MIDI note 128 times for all the possibilities, we used the transpose feature.
that comes with the new implementation of MIDI. So we brought in a single C0 MIDI note and then just used the MIDI number that we assigned before in that last slide, set that to an RTPC, which you can see maps, you know, one to one right here. And then we send that off, modify the C0 MIDI note to its appropriate note.
and then fire the event.
So for example, here's a quick look again at that sample for the PEG hits the guy was showing.
What it would do is if it's an F note, a 65, it would set that ARP TPC to 65.
It would properly select the closest 62 D note, and then would move it up 3 1⁄2 steps before playing the proper note.
So.
kind of complicated to get to where we got but once we got this all set up it really was pretty much self-sufficient and every time we had a new music segment that would come in it kind of took care of it because we didn't need to fill anything out We have a perfect example of that when it came time to writing boss music the regular music is mostly in major or mixed lydian but the boss music has a lot of minor scales in it so we had to kind of create a few new scale options in there, they're super easy, and it just kind of fell together.
Didn't have to change anything in the tech. Yeah, I think Guy added something like 15 more minutes of music for that feature, and I didn't even go back in and look at this system. So, really, really helped us out again with the game as a live service to be able to kind of let this system be self-sufficient. So yeah, so that's how the PEG system worked.
So just to round out the music system here, one of the biggest takeaways was MIDI is more about the memory savings in a big way.
Of course, the insane memory savings is kind of what took us down this path in the first place.
Without that, we might not have even tried MIDI.
But it led to this big open-ended potential for what MIDI can do for us.
Obviously, the peg hit system, but also for how you can manipulate music on the fly in general.
Melodic, harmonic, and rhythmic flexibility there just doesn't exist with WAV files.
Now that said, it's not an either-or situation.
There's no battle that needs to be raged about MIDI versus WAV files.
These are two different tools in the toolbox.
And I plan on using them in conjunction with each other and even layered.
So mix your awesome DAW.
you know, set of stems and put some MIDI over the top to make sure you're hitting the precision that you need to for certain cues. And you have kind of a really dangerous system that we're looking forward to. So, once this is all done, the next, you know, every hero's journey comes with the road home, you know, so we had to come home and...
For me, the biggest takeaways, obviously the MIDI isn't just for breakfast anymore, it's kind of one of the biggest takeaways from this whole thing, but perhaps even bigger was the idea that the technical sound designer and the role that RJ played, having him be part of our team...
gave us an autonomy that I've never had on a project before.
So even if we had this tech, if he were living on the game team and we had to go through gatekeepers for any given feature, we never would have been able to do this.
So definitely huge for us.
For me, I think one of the biggest takeaways was that as we can integrate more real-time synthesis and more of these different synthesizers and everything even within Wwise like different plugins maybe third party plugins that we can use You'll really be able to take create assets quickly and effectively in new ways and just make an easier process Did you say third party? I said third party. Whoa, yeah party since maybe yeah Since then software or just less barriers between different programs in general It's frustrating you have to open this and open that and they can't talk to each other Working this way everything was talking together and working nicely and it was awesome. Yeah One of the things that I was really interested in was, you know, MIDI is kind of just this data component, right? I mean, obviously, the first use for it is to just play your music segments through MIDI, but really, you know, you've got this very complicated game engine doing nothing but just spitting data out in, like, really, really different ways, and the idea that MIDI can be used in more ways than just music, you know? I mean, really all it is is a way to interpret data, so it'll be cool to see how people...
start using MIDI for anything that they can imagine.
I think we kind of scratched the surface with this PEG hit system, but I think that it can go a long way further.
I don't know if we mentioned, but we ended up shipping with 1.3 megs of sound effects, and how much music did you put in?
Well, just the balance of the five, so like three and a half.
No, but it was even less than that.
Oh, it was less than five. It ended up like 4.5 or something like that.
Yeah.
So, yeah, I was the hog.
I took over two and a half megabytes, and she had a meg and a half.
Yeah.
But I'm the manager, that's why.
I get to see.
Yeah, you get to see.
We know our roles.
Yeah, it works out.
It was kind of good.
Yeah.
And there was hundreds of sounds and lots of music and I didn't feel like we skimped for a mobile game.
Yeah.
Oh, you know, while we're kind of adding points to this, one of the things we asked for and I think may actually happen is the ability to, you know...
compress files for download but then decompress them on load.
As you know, AUG and MP3 files take a lot of CPU, so you can't have too many of those flying around in a mobile game.
But if you could decompress those as WAV files, all of a sudden you've got this package that feels like a lot more audio.
You can pack a lot more into your five megabytes that way for looking ahead.
So with that, we would like to take any questions you might have.
Just one point or one other thing. We're going to be showing some demos at the Wwise Expo booth both today and tomorrow from 1 to 3. So if you want to see some more in action and try and break our build, come on down and the three of us will be there at various times.
Hello. I just wanted to say that the music sounds really good.
And obviously a lot of that is the effort that's gone into Nuendo in terms of sequencing and velocity and all that jazz.
But how much effort went into the sounds you actually put into your sample banks?
I mean, obviously you create your own sample banks.
I mean, obviously a lot of effort went in, but how did you go about creating them?
Did you record new musicians or did you use libraries?
Yeah, no.
The sample source, thankfully, we did live orchestral sessions for Peggle 2.
We had a budget.
Peggle 2 was a larger project, and we had orchestral.
A lot of it we recorded short little phrases, or more specifically, single note elements for the peg hits for Peggle 2.
And I was able to kind of like harvest those sessions to create sounds.
Now, when I'm doing a sound sample bank that I know needs to be memory efficient.
One way to make sure that it's going to work is that each sample has to just be rich in and of itself.
You hear that sample raw, it's got to sound rich.
So maybe there's layering that goes on.
You'll take a couple of sounds and layer them, make it sound like if in your DAW you might take a couple of string patches and layer them to thicken it up.
Well, I do that with samples to make it, you know.
little more strong sounding so that when it's in the context that it still holds up.
But the other end of that is creating custom samples.
Again, I know the music I'm going to be creating, so I tailor it in that direction.
How big were the samples you created, like for individual string hits?
I mean, I've done similar stuff before and always run into this problem where everything kind of sounds jagged because I've got these tiny little hits and I'm trying to make it sound like a rich orchestra and I've got these tails, but people want stuff to be short.
Right, the tails like end up... Right.
So the key there, you have like, let's say a pizzicato or a staccato.
Boom, it has a nice attack. It rolls off. I'll do it your way.
you try to make that as short as possible without sounding like, oop, that just cut off.
But the key there is the reverb really helps flow that through.
That was the glue.
Oh, it uses DSP and what not.
Oh, yeah, real-time reverb.
So it's all being bussed through real-time reverb.
Yeah, we didn't have any baked reverb for either sound effects or music.
Yeah, you can't make it on the fly.
But there was also, I did a lot of mastering and pre-processing to the samples themselves.
You'll notice the names on some of them, like Pultec EQ or whatever, where I'm treating them as if it's a final sound.
Because I know I don't have a Pultec in Wwise.
I can do that kind of treatment prior, but it's still a dry sample once it's in there.
And then the reverb gets added in real time.
Okay. Excellent. Thank you.
Yeah, if you want to know more about how that works.
There's a Peggle 2 talk on the Vault.
Watch it from last year.
Thank you.
And that'll kind of give you some back story on Peggle 2 and leading up to how Peggle Blast came.
Thanks.
OK, so first, we're going to.
I'm literally in tears in my eyes listening to you guys talk about this.
This is unbelievably great.
I've got a bunch of questions.
First, what about latency?
Now when you're doing stuff in Unity on the machine, everything is fine and cool.
But when you put it on a mobile device, you can get latency where, you know, and in a game like this where the peg and the sound, it's...
have to be pretty close. Did you have run into any trouble with that?
And if so, how did you deal with it?
No, not really. We ran into some issues on Android devices a little early on where there was some latency. But I think that as the rest of the game team started optimizing other stuff, everything started syncing up pretty well.
Yeah, in the early days of mobile games, Android in particular had a lot of latency. But I think the Unity-wise connection and the improved drivers on Android all helped that.
Yeah, that's a little disappointing because the Unity Fmod that I've been doing, latency is horrible.
It's like 150 milliseconds.
It's more about syncing up the two.
I don't know.
Yeah.
Well, I mean, so like when I do a, I'm doing a Unity Fmod implementation, the animation, the cannon goes, and then the sound happens.
And it was so bad that when I push a button, the sound would happen so much farther later that it was almost disconnected.
We didn't see any problem with that.
No, we never ran into that.
Interesting.
Yep, not at all.
Well, there you go.
Another question, a question about the samples in your sample banks.
Any looping in the samples?
Yeah.
So do you do bling and then have a loop?
No.
I had, I did kind of a tricky thing where...
I had like string staccatos, for example, and I had just a string loop, or a different bank that was just string loops.
So you'd play the staccato and then the loop.
And then sustain the thing underneath.
But the loop was always first to last.
Yeah.
You never had any looping in.
It's mimicking what, you know, a play in the middle loop would do, but with the strings.
Okay, cool.
Oh, and the other question was, one of the things I used to do with the Beatnik engine, which was obviously a summer, was to record chords and things that I would want to do, like I'm going to play this chord and that chord and that, so instead of having a bunch of...
That would save me voices.
Yeah.
So instead of having a bunch of string samples that were played, of course, I would record just the chord and then just play that thing, obviously you did that for the nice and the awesome stuff.
Right.
But you, did you do any of that for any of the other instruments?
No, everything else is single note samples, but I do think that's a technique, you know, obviously with the nice and all that, those were recorded as chords rather than played that way.
Um, but on.
Earlier, in the 90s, when I was doing MIDI in a previous life, I would often record little riffs sometimes.
Like if you have a guitar riff, it sounds better just to do the da-da-da-da, right?
Or to have something play a chord and just play it like a DJ would, right?
So those techniques are really valuable, though.
I didn't do that here, though, except for the exclamations.
Okay, well, I could ask questions about this for days.
We'll talk more, Peter.
We'll talk later.
Thanks. I don't know if you guys can answer this question, but it was sort of occurring to me as you were talking about building the sample library up and wise. I'm wondering if you based it on a commercial sample library, if you would run into licensing problems. Yes, you would. Okay.
Yes, you absolutely would. There in the 90s, late 90s, there was a, because direct music used MIDI and sample banks, there was a big discussion about what was legal and what wasn't. And the general rule is don't do it.
You can sample like analog synthesizers and things that are acoustic in nature, no problem, but you can't pirate other libraries is the problem.
That's why I think it's time to actually start having conversations with folks who make sample libraries and see which ones are amenable to this market, because it's a different market.
It would be much like your lease DSP for a game.
It would be that kind of a business model, and I think it's time to bring that back.
They'd have to be smaller sound sets and geared for games as well.
Right.
So anybody, any sample library makers out there?
Alright, we got two.
Nice.
Three.
There's the word.
Word is out.
First of all, amazing job guys.
Thank you.
I'm really inspired by the talk.
And I definitely think that the hybrid of real waves and maybe it's the way to go for the future.
Curious about you guys' audio output settings when you're doing your compression.
I assume you're doing selective compression between all the different areas.
Oh, like data compression?
Yeah, data compression.
Let's see.
I mean you had to go into that a lot with the sample.
We all kind of worked in our own areas.
In the samples I experimented, one time I tried all-og, because I could get more out of it.
But it just brought the system to its knees in unexpected ways, asymmetrically.
It wasn't like, oh, it's just crawling.
It's like, boom, randomly with, you know.
So we quickly took that out.
So it literally is a combination of sample rate, mono, and ADPCM where I could stand it.
And I even did some posts on some sounds where ADPCM makes things all crunchy.
I did some real-time post EQ to kind of roll some of that off.
So you kind of play around and keep...
keep massaging it or even going in and re-editing samples.
So all of those methods were combined.
It's a lot of listening and trying and seeing what works and what sounds good.
And you had to do that with your sounds as well.
You did a lot of like taking one little sample and making different uses of it, right?
How did that work?
Yeah, for one WAV file.
Yeah, one WAV file.
And manipulating it a lot.
Yeah, well for those WAV files, I think what did they end up being?
I think they were 80 PCM.
Yeah, 80 PCM, no I think we used a couple of waves.
I think we just down sampled them.
Because we were manipulating them a lot and there wasn't that much of those.
Once we added some emotes at a later time period, that's when we really had to get creative with what we were doing.
And it really just came down to auditioning in the assets, choosing what sounded best and took up the less space and trying them on device and seeing if it worked.
Yeah, I think no different than any project, only we had, you know, the added thing of we had to make sure that it was...
performing okay. We couldn't just wantonly use Vorbis anywhere we wanted.
We were pretty careful about the Vorbis stuff.
But I hear that stuff is getting more efficient.
Cool. And if you don't have Wwise, I don't have Wwise for the projects I work on, so do you know if any of the other middleware software companies are using it?
I mean, I know Fmod supports a whole slew of formats.
I think I saw in a Unity 5 talk yesterday that they just added 80 PCM and Vorbis support.
But not even just that, just the whole MIDI.
Oh, the MIDI stuff?
You know, I hear there's some plugins for Unity.
Is that correct?
But those would be kind of garage, someone in a garage making a little MIDI plugin for Unity.
There's nothing as formal as this that I've seen that's commercially available.
Obviously, the Fantasia guys use MIDI all over the place, but that's, is that wise?
Try to get your studio to use Wwise.
Maybe other engine makers will add it if they see the success that folks are having with it.
If people are calling for it, if we are calling for MIDI, the other engine makers will follow suit.
It makes so much sense for mobile.
Thanks, guys. Thanks a bunch.
That's it. Thank you so much.
Yeah.
Easy, easy.
