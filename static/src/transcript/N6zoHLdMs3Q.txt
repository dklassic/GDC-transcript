Hello, everybody.
Thanks for coming to this presentation.
My name is Cesar Romero, and I'm a machine learning engineer.
But today, I'm mostly talking to artists.
And my goal with this presentation is hopefully to maybe inspire you a little bit, maybe expose you to some tools that are becoming available, some techniques that have made a lot of progress in the recent years.
and hopefully you can come up with even better ideas of things that you might create using some of these tools.
I think it's just a matter of time before some of these tools become ready for production.
In some of the examples that I'm gonna go over, I'll point you to tools, in some cases open source, in some cases our websites you can just run directly.
Some other cases don't have tools yet.
I still believe it's just a matter of time.
So let's get started.
The first example I'm gonna go over is style transfer.
If my slides...
Okay, so just quick, who's familiar with style transfer?
Okay, that's more than half, so that's good.
You probably have seen many, many of these examples.
First paper was published around 2016.
Google even has some examples in the keynote earlier this week.
the case is basically that you have some content and you want to see what that content would look like if you paint over it.
In the case of a game, you might be exploring whether a specific scenario or setting environment, how might it feel, like how do you get the right mood or the right style, does it work or doesn't it work?
and painting over it, of course, takes some time, but you could do just style transfer and get something like this in a matter of clicks.
So remember I said I'm a machine learning engineer, which means I'm not an artist.
Even though this is not particularly mind-blowing, I would not be able to create this myself.
And I would be able to create it running a machinery model, but in this case, I didn't even have to run that model myself.
So I intentionally show this.
There's a couple things I want to highlight.
It's not production quality, right?
There's some artifacts you can tell.
But here we're just applying Starry Night from Van Gogh, which you may have seen similar examples on San Francisco, which is probably familiar to you.
At the bottom right, you see watermark.
It says deepart.io.
So the author's original paper created a site where you can upload your own image, your own style, select from existing styles, apply it.
All you need is a browser and a handful of minutes.
It took me, I don't know, a few clicks and maybe three minutes to create this example.
And if you want higher resolution, you also have that option.
You can pay to make your images private, et cetera.
So it's just one of the examples of the cheap tools that are available to check it out.
And of course, you might be interested in applying this to to video, so let's take a quick look.
We have a simple video of a beach, you have waves going from left to right, someone swimming on the left side, and you might also be interested in seeing what it might look to apply a style, so let's get the same style, Stereo Nights from Van Gogh, and apply it.
And then you get something that looks like this.
Again, it doesn't have to necessarily be production ready.
You're the artist, you decide when it's ready, but it might help you explore a lot more quickly.
In particular, this example was created using 10 lines of Python code.
You don't even need to run the model yourself.
This one was intentionally created using Algorithmia, which is a sort of a platform for hosting machine learning algorithms.
It's really a couple lines of Python code, and then off you go, and just wait for a couple minutes.
So, then, both of these examples, the style was applied to the entire image.
So, how about we want to control a little bit more what the style is applied to, using mask or something like this.
Someone had this idea, Alex J.C., some of you might know him, he's not in GDC this year, I believe, but he proposed something called Neural Doodle.
And I won't go into the details, because the intention here is not to do a deep dive into the architectures and the techniques and loss functions and things like this that are familiar for machinery engineers like myself.
The intent is to show you examples so you get the idea of what things are possible.
And this is just a sample.
There's a lot more where this came from.
So in particular, the idea of neural rules is to control a little bit more what the style is applied to, as opposed to applying it to the entire content.
So this is some quick thing that we put together at a Hack Week like about a year and a half ago.
The paper was published in 2016.
And the idea is that then I use a shape to decide what the style is applied to.
So the style comes from the left side.
the shape just controls what in the output image is the style gonna look like.
So you could rearrange, essentially, even a piece of art.
We're using a single style here for simplicity of the demo, but here you see how it actually works.
So if you have the style on the top left, the bottom left, you just have a guide or a mask that decides where to extract the style from.
Then you draw what you want the shape to look like, and then the output has the shape and the style that you wanted.
and then you can redraw it if you don't like that shape.
You get different looking trees, and then you can decide whatever you wanna do with that content.
In this case, it's just generating images.
But the point is, keep the artist in control.
These are just tools that are learning how to interpret images.
But you're the driver.
So this was just three examples of style transfer.
And I'm gonna give you here a couple of references so I can move off to the next set of examples, yeah?
So at the top here, we have the first paper.
The first two are the papers for the machine learning folks in the audience.
That's what you wanna read if you haven't already.
Then deepheart.io is a site created by the authors of the first paper where you can upload your own style and your own image.
Right below it is Neural Doodle.
It's a GitHub repo, open source.
You can grab it.
That's the one we used to create that little demo with the potato and so on.
I put the license here next to it.
In my experience, the gaming industry is not super familiar with open source licenses.
In this particular case, what you need to know is that AGP-LB3 means the code is totally there.
You can use it.
You might not want to release something that uses that code because this license would require that you also open source your code.
But at least you can just grab it and explore it, just like we did for that little demo that I just showed you.
And at the bottom is the site that I mentioned earlier, the Algorithmia.
It's kind of like a marketplace for machine learning algorithms.
And they have plenty of demos and simple, like sample Python code that you can run yourself.
And that's the one that was used to create the video style transfer.
So let's move on to another set of examples.
I want to see quickly if you can think of what these people have in common.
And if you look, well it's not gender, or age, or the hair, or facial expression, or clothing, or colors of eyes or skin, or background, so what is it that they have in common?
Someone I think knows.
They're not real.
They never existed.
These are completely generated by a model.
Which I think is pretty interesting.
What you could do if the models are now getting to the point that are generating images that look very real, like it's very, very hard to tell just from looking at the pixels that this is not real.
In particular, this is generated by a class of models called Generative Adversarial Networks.
The machine learning people in the group are probably familiar with it.
Javier here from EA had a fantastic talk on Tuesday afternoon, check it out, on the vault, explaining how generative models in general work if you're interested in the math.
want you to think at high level is that many people are familiar with how machine learning models can classify images, for example.
So think of a model that, given a face, can tell whether or not this is actually a face.
And then the insight here, 2013 or so, was what if I use that information, whether the model believes this is real, to automate the process of showing another model whether the image it's generating is real or not real.
So you have now two models, that's where the adversarial word comes from.
And it's kind of like these models are playing a Minimax game or competing against each other.
What matters to us here is that the generator can get very good at generating the content that a traditional machine learning model knows how to tell apart whether it's real or fake.
Now, in that, example that I had earlier, just faces, the generator just learns the space of human faces.
But for an artist, it might not be enough, because you want to control what the face looks like, how you're gonna use it, depending on where you're gonna use it.
So, one technique, it's a clever, simple from a machine learning point of view, but it's a clever technique that I came across some six months ago or so.
It brings that idea of trying to provide more control to the human in what other space is going to be generated.
For the machine learning people in the audience, it's a very simple linear model that learns the mapping from the input to the generator into the output of a multi-level classifier that takes the output of the generator.
If that didn't make sense, it doesn't matter.
I have a short video to show you what the user experience might look like for something like this.
The model knows how to generate faces.
And what you might get is sliders or a tool like this, where you can individually control what each of the attributes of the face.
Do you want more beard or less beard?
Or maybe blonde hair?
Generate faces at random, which the model obviously knows how to do.
You might want more hair.
You might want younger or older looking face.
You might want bangs.
Who knows?
It's up to you to create it.
And again, for the machine learning audience, one very cool thing about this technique is it does not require that the generator is retrained.
Anyone that has worked with GANs, the hardest part is training the generator, of course.
This doesn't even require that the generator is retrained.
So you can grab an off-the-shelf, pre-trained generator, in particular, in this case, it's a progressive GAN by NVIDIA, and just learn that mapping.
And then you can build a tool like this, which I think is pretty interesting.
I'll give you references all together in a couple of slides.
Let's look at another example of content generation.
This particular one is called image translation.
The original authors called it image-to-image translation.
In particular, I'm talking about the Pix2Pix model for those mission learning in the audience.
The idea is that...
The generative models that I showed you before learn a space, and then with the trick that I mentioned, you might get some control on attributes, but what about instead of turning something somewhat random into a face, get an image and turn it into a different image?
That's what they call image translation.
I won't go into details of how that works, but what I want you to take away from this is that As long as you can think and produce what this input-output pair should look like, you can train a model that does this and it's the same architecture.
And on the site that I'll give you the reference, there are multiple examples.
So I'm going to show you one of them.
This is something I created in my browser.
This is running in real time.
And again, it's not a production tool, right?
It's a tool that might allow you to explore.
So here I'm just creating facades.
And doing very, it's a very, very simple UI where I'm just selecting different colors for blocks.
I might select doors and I might want to create an entrance or I might want to create windows.
So running 100% in my browser in real time, I'm just selecting where I might want a window and whether I want to add some window sill or a trim or some shutters like a curtain.
and every time I want, I can generate a new facade, and it starts looking like a facade.
You're not gonna put this directly in the game, but it might allow you to create things, to prototype quickly, and it takes literally seconds to do something like this.
And then you can iterate as much as you want.
You can generate random examples.
You can add more windows, add a third floor, add a balcony, and then reprocess.
And at the end, you might end up with something that actually kind of looks like a facade from, I don't know, maybe 100 years ago, which might be what you're after.
And this model was trained with facades.
But if you can generate this input-output pairs, then you can just, you know, get a machine learning friend and it will train it for you.
In this site, in particular, there's also examples about going from sketches to cats, which you may have seen.
What matters is that you're translating an image into another image.
I have one more example in terms of content generation, which I think is relevant for gaming.
In machine learning, we call it super resolution.
In gaming, the closest, I mean, it's kind of like remastering.
So many people probably have enjoyed remastered games, like The Last of Us comes to mind, Spyro more recently.
Resident Evil is kind of more than a remaster, but you get the idea, right?
Some old game that you want to kind of adapt it to a newer platform, better resolution, et cetera.
So one of the things that needs to happen is that the materials need to kind of.
now grow and need to become bigger, but a simple bicubic interpolation is very naive and it doesn't work well.
So if I grab a random texture from the Ass Store, and I stretch it four times, it looks something like this.
This might be a little bit hard to see from the back.
So I'm gonna try to do back and forth and hopefully you can see it.
If I stretch this image four times, you see the pixels, you see the artifacts generated by the bicubic interpolation.
If instead we use, in particular this model is called ESRGAN, I came across it last year at some point.
You can then teach the model what an image looks like.
This model wasn't particularly trained on textures in the Asset Store.
This model was just trained on a large collection of random images from the Cocoa Data Center.
Then you can ask it to output not the same image, but a larger version of the same image that you show as input.
If you do that, let's see if you can tell the difference.
So the second image is the output of the model.
So I'm going to go back and forth.
Bicubic interpolation, ESRGAN.
Bicubic interpolation, ESRGAN.
And it's not specific to textures, so you can apply it to some random character.
Jeremy also had a talk earlier this week, Jeremy from Unity, on a few other examples.
And we have even started to see examples in the wild of somewhat recently, someone grabbed all the textures from Max Payne and remastered it using just this model without any retraining, and just put them out there.
I came across a couple of examples from Diablo 2.
And again, this could be a tool where it can help you if you're in the process of remastering a game.
My understanding from talking to a couple artists is that it's also not necessarily the most exciting thing to do.
It might be tedious, and if this can help you get that job faster, then you can get more creative.
So, a couple of references.
The first one.
is the image to image translation.
That's the one that I use for the facade example.
The first three are just the papers for the machine learning people in the audience, or whoever really wants to get into the math.
The second one is the one used to generate the faces.
The third one is for super resolution.
So things that you can look at today, even if you don't have a machine learning background at all, this person does not exist.com, is where all the spaces were generated.
It took me just, I don't know, a couple minutes.
The first GitHub repo that's called TransparentLatinGAN is the one that gives you this fine-grained control of the attributes of the content that you're generating.
The example was used with faces, but conceptually it doesn't have to be limited to faces.
It was basically built on top of Progressive GAN, which is the third repo, which was created by NVIDIA.
And then ESRGAN, so it's the second GitHub repo, it's for super resolution, and the pics to pics at the bottom with the BSD license is the image translation.
Sorry about the order, it doesn't exactly match.
But I put the license here intentionally.
Remember how before I said GPLv3 and so on?
This is MIT, Apache 2, and BST.
All three of these licenses are open source and very business friendly, actually.
You are completely free to grab this code, build with it, modify it, link it.
You don't have to pay anyone anything.
All you have to do is give credit to the creator, like mention somewhere.
The leafy licenses have a different way suggesting how to do this, but you can use this.
This is not commercial software, you can just grab it, go look at the code, if you wanna run it, you can do it.
So I wanna look at a third class of examples, which is animation.
Animation is harder than the other two classes that I've shown you.
I've seen a couple of good talks related to animation this year at UDC, which makes me happy.
So let's just look at the first example.
Let's look at smoke, in general, fluids or particle would have something similar.
Some of you may even be familiar with this paper in particular.
This is from Seagraph a couple of years ago.
Smoke here is being simulated by a neural network, but what does that really mean?
Today, what you typically have is some simulation that has...
an implementation of some complex physics formula without getting into the details of the math.
It somewhat limits the amount of smoke that you might have, the amount of particles that you can simulate, the amount of iterations that you can run the simulation for.
So someone had the idea that because machine learning models can be general purpose approximators, and someone understood the simulator enough to identify the piece that was expensive, why don't we replace that with a machine learning model?
This is trained completely on synthetic data, which means you have infinite data to train.
So then, can we train a machine learning model to approximate this function?
And the end result is that you get smoke that looks like this, but it's 10 times cheaper than what you might encounter in your engine of choice.
These kinds of things, I think, it's just a matter of time where until these kinds of tools are just in the engines, in the tools that you can use to create your content.
Let's look at character animation.
So the first example we're gonna look like for the machine learning people in the audience is called PFNN.
Fabio.
from EA earlier this week had an excellent talk comparing the next two examples, coincidentally.
Daniel Holden, who I don't think is here in the audience, is the author of the first example.
The key observation here is that animation is very complicated, and can you simplify the pipeline would be our goal.
And in particular, you can use mockup data And a machinery model that takes as input, let's say the context of the character, the direction of where it's going and where it was recently.
And that's enough for the model to decide how to animate the character in a way that adapts to the terrain.
There's no kind of handcrafted animation here for exactly where to place the hand or where to like slow down and scale as if they're steps.
The model is reacting.
to an extent, to the terrain.
And it basically simplifies the pipeline.
You know, I'm speaking in simple terms, but Daniel Holden had a talk also on exactly, not this paper, but newer work, more recent work that they have on EA that even shipped in Assassin's Creed Odyssey.
Sorry, Ubisoft.
And Fabio from EA had a talk explaining this, and he showed some examples in FIFA and so on.
This is not something that is a tool that you can grab today.
I won't give you an open source repo where this exists, but these things are coming.
And so we would take that example and we extend it to quadruplets.
With some generalization.
there's some extra tricks, but the machine learning details we don't need to get into.
Then you get something that looks like this.
So this is from SIGGRAPH last year.
inspired by the PFNN, which was that yellow soldier you saw earlier.
And so then you get a wolf in particular, who's just like the soldier, it's just reacting to the user input.
Based on the terrain and the speed, it just decides how the character should be animated.
And it learns like, you know, jumps and how to go up or down and turn and stop in a way that looks realistic.
Just from mockup data, pretty much.
And, I want to give you one more example, which some of us know as Tidmimic.
This came from Berkeley last year.
And it's also related to animation, but in particular it's related to physics-based animation.
And the key observation here is that...
If the model can learn a behavior, but not by stitching mockup data together, but by learning what force to apply to the rig, how to move the muscles, if you will, then that allows the model to react to changes in the environment, including obstacles or projectiles or things that could be thrown at the model.
So using either kind of manually animated character or mockup data, you can have a model that learns how to imitate that, hence the mimic in the name.
And then you put this character out there and affect its environment.
So let's take a look at what that looks like.
Here.
here with dragon, animated by hand, and then this is the one learned by the model.
So obviously you can't do mockup data for a dragon, but then you can start throwing things in front of the dragon, and the dragon adapts by itself in a way that looks realistic.
Like it has to, like maybe it looks like it's turning to avoid the boxes.
You can have a quadruped, in this case it's a lion, you could, you know, it could be a dog or whatever, for which you could actually have mockup data, and then learn.
how to run, and then react.
You see how it has to work to get out of the cubes.
It can even get completely bombarded and fall in a way that actually looks kind of natural.
And none of that was animated by hand.
It learned how to apply force to the body so that it would actually do what it was expected to do.
So I'm gonna give you here the references.
Unfortunately, these are...
Unfortunately for some of you in the audience, these are just papers. In this case, I don't have code that I can share with you.
But I do believe this is a matter of time.
On Tuesday, there was a machine learning talk where I learned several students.
using machine learning for animation, different parts of the pipeline, and some of which has shipped in games, some of which is shipping in games.
It's very exciting work, and I think it's just a matter of time.
So with this, one of the messages that I wanna leave with is, help us, machine learning people, help you artists.
just hopefully from this you get some idea of the things that are going to become possible or that are possible today.
And get creative on what other tools you might want, what are the things you want to create that would be very difficult to do without tools.
And there's a lot more where this came from.
So I think with that, we have a few minutes for questions.
So I'd like to, I don't know, hear from you and learn from you.
How the last part with the animation and oh great presentation by the way The last part with the animation where you're saying it was learning does that is that something that still takes a lot of?
Performance to calculate or anything or is that something that's getting really close to keyframe animation great question because I should have mentioned And I did not so um This is just reinforcement learning so what it's doing is um Applying starts, imagine that you start with random policy that just applies random force to muscles.
And it's just gonna fail to imitate the reference animation.
And then it gets a penalty or reward.
Over time, it learns to mimic that behavior by applying force to the right places at the right times.
The process of training takes a long time.
In particular.
In Unity, that's one of the reasons we released the ML Legends framework that you may have heard of.
It is for the purpose of reinforcement learning, not for the purpose of this animation, but for the purpose of reinforcement learning in general.
And it takes time, it takes some compute power, just like many other machine learning models take.
So we're talking hours, perhaps, yeah?
I don't recall off the top of my head how long it took to train each of those.
But once it's trained, then you can embed that in like a mobile device, and it can run in near real time.
Hello, are there versions of GANs or I should say any sort of machine learning approach that does image translation from say something that may be a game that's trying to look photorealistic into actual photorealism?
So like a celebrity GAN is an example where you can make a face and then make it look like a human face.
Are there also GANs like that for say, structures or real world or like?
I'm not aware of something, so just to make sure that the question's, it's like are there GANs that can be applied to examples being terrain or content in games to make it look like hyper-realistic almost?
Yes.
I can think of proxies of things that might be in that domain.
Not something that you would apply directly just to, like, especially meshes or something like this.
GANs have been very successful with 2D data, so with images, not directly with an entire scene.
But there are.
So obviously the super resolution is also a GAN, by the way, and so it can be applied to textures individually.
There are also examples from Z-Graph where this has been applied to terrain, for example.
A version of image-to-image translation would be from a sketch done by hand into something that's not a sketch, whatever that is, as long as it's an image.
In that Z-Graph example, you can sketch some mountains, and then you get a full terrain, high map type of a situation.
So it's more creation, it's not as much refinement, which is I think maybe what you're after.
But if we talk in detail about the concrete examples, we might be able to, even just in a conversation, to determine whether or not these things apply.
And this is the kind of conversation that I want to start between machine learning and artists, to see what problems we can help to solve, for example.
Cool, thank you.
Hi, great, so thank you.
When you spoke about enlarging textures and training it beyond bicubic, do you think that that could ever become a runtime solution as opposed to an offline thing to actually be optimized well enough without?
So I can say that based on my experiments, it's not real-time today.
But it takes, let's say, in the order of a second to get a 4x or an 8x outpress.
I do know of people doing research, and particularly I've talked to people at places like LG, for example, that are interested in ultimately doing this in near real time on device.
when you don't have 4K content, you want to do something better than just stretching that.
I was just thinking you could take shadow map density and every element of the game beyond it, just rather than just underlying textures.
And we're not there today.
It might be a matter of time.
Many of these things are possible because hardware has gotten better.
There's obviously improvements in the algorithms, but hardware has gotten significantly better.
To run something like that in a second, you still need a decent GPU.
So it's not something that you can do today on every frame.
Some of these things might become even more viable as more game content is streamed from the cloud, for example.
Because that provides us, machine learning people, a very controlled environment where we don't have to worry as much about the cross-platform and the different levels of hardware availability.
And you reduce the latency between where the game is being rendered and where the machine learning is happening.
but the short answer is not today.
Once per second kind of thing.
Awesome, thank you.
Alright, I think we're done with time anyways, so thank you.
