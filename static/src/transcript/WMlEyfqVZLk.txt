Hello, everybody.
My name is Robert Coddington.
I'm a senior animation director at Insomniac Games.
And the title of this, named by Lindsay, is Thwips and Hugs, and it's very appropriate because those are two things that happen quite a bit when you work with us.
But yeah, I'm an animation director.
I'm not the only one.
Or, no, that's just more me.
There's another animation director, and I'm going to talk a little bit about the structure that we set things up, and you'll see why in a second.
So Danny can't be here today, but he should be.
He should be right there.
So the way our animation team runs is broken into very specific pods.
Each one of these pods will run for pretty much the duration of the product, having anywhere from three to five people per strong leader.
So then that would be Traversal, Combat, Boss Encounters, Missions, Living World, and then Story Cinematics.
That's a really big team, so it's kind of two pods, even though I only made one icon here.
Just go with me on that, it's cool.
So yeah, like I said, in each one of those pods, you'll see that there is a very strong leader and a group that they work with.
And then today, I wish I could bring all of them here today.
That would be the perfect thing in my mind.
But we have four of them.
So Stephen Camardelle, James Hamm, Lindsay Thompson, and Bradley McLaughlin.
And they're going to tell you a little bit about what their life is like as an animator and as an animation lead or associate director at Insomniac Games in front of these pots.
And the first one up is Lindsay Thompson.
Hi, my name is Lindsay Thompson, and I'm an associate animation director at Insomniac Games for almost 11 years now.
Well, not in that role, but yes, I've been there 11 years.
And my focus on Marvel's Spider-Man 2 was our main story cinematics.
One aspect of that included the logistics behind bringing accurate representation of deaf characters and ASL to the game.
These cinematic shoots that we did and the construction of the scenes worked a bit differently than our typical Cine's So some of our key pillars for doing ASL work was we had ASL consultants and interpreters We had deaf actors and body doubles on the mocap stage a different and more extensive animation polish work Pipeline and then we also had playable Haley who is our deaf character in spider-man So, our ASL consultants and interpreters.
Josh and Jules were our consultants from Hypernova.
They're both deaf and work in entertainment to advocate, educate, and consult on ASL.
They were there for any of our questions regarding deaf culture, consulted on the scripts, and how to interpret them into ASL.
And they worked with us on stage to make sure performances were clear, and then worked with us to go over every scene and shot that had ASL in it, and make sure that our signs were correct and clear.
So for each hearing cast member, we hired a deaf body double.
We made sure we hired diverse, role-accurate actors to double as our main cast.
Here you can see some of our deaf performers.
To the left, there's Justin Folk is in the foreground there.
He's Miles' double.
John Kim, in front of him in the middle, is for our Genki double.
And of course, we have Natasha Ofili, who plays herself as Haley.
Our main cast, Najee Jeter and Griffin Pateau, are to the right.
So we had to figure out the best way to shoot our hearing cast's spoken performances, along with our deaf body doubles that would work best for Natasha to play against.
So for this, we ran them side by side so Natasha had signing people to play off of, but could still see Najee and Griffin doing their speaking performance.
Each deaf cast member had their own interpreter so we could all communicate on set, and we also had the hypernovas on stage to help interpret the scripts with the cast and plan out the signing performance.
We ultimately determined that it really depended on each scene.
Sometimes it made sense to have all of our deaf cast standing and playing against each other like you see here, but other times it was more important to have Natasha face-to-face with Najee, while his body double Justin would sign next to or in front of him, so Najee could match his performance.
So in some cases, our cast were even able to sign themselves on stage.
Seeing as they were playing a more inexperienced signer, it was authentic for them not to be signing as fluently as a deaf person would.
We had to consider each character's experience and knowledge with ASL.
So Genki, Miles's best friend, is working very hard to learn.
Miles is probably the most comfortable and experienced in speaking with Haley.
Rio, Miles's mom, is clearly just beginning to learn as Miles has just started dating Haley.
Spoilers!
So we made sure her double didn't sign as fluently and experienced.
So for example, at the end, this is a spoiler, sorry if you haven't played the game.
At the end when Albert Moon says, nice to meet you in ASL, we approached it as though he had literally watched a YouTube video before coming over and saying something to her.
So our interpreter was like, well, that maybe wasn't quite it, but that looks like somebody who's just trying to say a nice phrase to somebody.
So yeah, it was really cool.
Okay, so here, so in some instances, our main cast like Najee and Griffin were able to perform their own signs, but for more longer and more complex conversations, we relied on our deaf performers to make sure everything was accurate.
So you can see we had cameras shooting from all angles on stage, which became important for animators during polish work.
Here we have Najee capturing the emotional performance with Natasha.
Sorry, sorry, sorry.
Yes.
We wanted them to connect.
So behind Najee, you can see her interpreter is actually signing out to Natasha with Najee speaking.
And Najee is facing his double Justin so that his voice can match the cadence of the signing.
It took us a minute to find the most elegant solution for everybody's comprehension and the best performance.
And you can also see our consultant in pink behind Najee, making sure the signs were clear coming from Natasha and Justin.
You may also notice that normally mocap gloves don't have as many markers on them.
It's harder to capture intricate finger movement most of the time, and we often keyframe our fingers after the fact.
But for our ASL scenes, we added extra markers to our signing actors so that we would start off with a better, more accurate data.
So once we shoot the data for our scenes, we then move on to layout.
Our animators do everything.
They do cameras, layout, polish, key framework.
We are a department full of directors.
So this scene I personally took on myself, so I was able to direct it on stage, bring the data into Maya and set up the cameras, do the layout and edit, and I actually polished it myself.
So combining the two performances, this was our final result.
In this particular scene, we layered Justin's arm and hand movement, onto Najee's body and face performance.
So after laying it out and getting data combined, we finally worked with the hypernovas to go over this performance in video footage so that we can get clarifications on any signs that may be confusing.
We would often go through it sign by sign, and there were many times our animators had to go into the performances and add or remove signs after the fact to make it clearer.
This one was really wordy, so at times I had to hold the signs for an extra frame or two, to make sure that they really stuck and that you could read them, but without feeling too staccato.
And as much as the additional markers helped on stage, I would say that we definitely had to touch or improve nearly every sign in the game, particularly when there was quick finger spelling.
Finally, just a quick touch on our playable Hayley mission.
Our designers and writers put together a truly unique experience as a deaf character.
From the audio design, to the emotes, to the text conversation, they found unique ways to interpret emotions and a story without a voice.
And bonus points, there's a cat on this mission, so it was really awesome and it's one of my favorite missions in the game.
Look at it!
So thank you so much.
I'm going to hand over to Bradley next to talk about it some more.
Thank you, Lindsay.
Hello, I am Bradley McLaughlin.
I was an associate animation director on Marvel's Spider-Man 2, doing cinematics along with Lindsay.
Today, I am going to be talking about our process of shooting performance capture on Spider-Man 2, but more specifically, the benefits of having our main cast and stunt actors mirror each other on stage.
Why does it matter?
Keeping the timing and voice acting perfectly in tune with the body performances is a crucial aspect of putting a cinematic together.
How does it work?
Running stunt actors alongside our main cast keeps the performances synced and makes them feel more grounded and authentic.
Here's an example of our stunt actor Seth Austin mirroring our Spider-Man slash Peter Parker actor Yuri Lowenthal.
Thanks, Tao.
Miles!
I've been calling you.
For hours.
Nothing.
You know Miles has been hunting Lee.
I'll find him.
I'm calling the hospital.
I said I'd find him!
I've got this under control.
All Miles talks about is how to be a better Spider-Man.
So, the challenges of shooting superhero performance capture.
Superhero actions frequently exceed the physical limits of our main cast, but we still need to synchronize all performances.
Running two performances Two performers together introduces a greater physicality provided by our stun actors and allows subtle performances provided by our main cast.
The best part is everything is synced, making it easy to swap out the performance and keeping the timing of our video edit.
and here it is in game.
This also leads to another advantage.
Animators get to choose from multiple performances.
Sometimes an animator might want to choose from the options I mentioned in the last slide.
You might want the physicality of the stunt actor gives you, or sometimes you might really like the subtleties and performance of the main cast.
Another benefit is we can shoot more scenes throughout the day, not having to break up the stunt and main cast performances.
Here is an example where we ran, sorry, just one second.
Yeah, there we go.
Here's an example where we ran the stunt actors doing fighting choreography, then had our Craven actor, Jim Peary, do a head slam followed by the Sun actors doing a more fiscal take of the head slam.
So as stated before, running stunt and voice actor gathers creates a seamless performance that is easier to edit.
And what makes this possible is collaboration.
When the stunt and main cast actors collaborate, it helps each of them become even more invested in the performance and creates a more grounded and authentic character.
The collaboration on set is truly amazing.
It's really the best part of working this way.
The collaboration on set is, again, it's amazing.
And it really feels like we're all moving as one unit when it's time to roll.
So shooting this way can bring up some challenges.
One of the main ones being audio interference.
There are several ways we can work around this.
We can implement wild takes, which are real-time playback of the stunt performances that the main cast can do voiceover on the stage.
We also can just shoot ADR in the booth if needed.
And we can offset the performances between stunt and main cast while still getting everything in one take.
And here's an example of that, of offsetting performances.
So we're going to have the stunt actors go first.
Then we're going to have the main cast go.
And we'll capture their audio-facial performance and their body performance.
So I'm going to show a version of it on stage and then what it looked like in game.
And here's what it ended up looking like in game.
All right, that's it for me.
Thank you for your time, and I look forward to your questions.
Up next is Steven Camardella.
Thank you, Bradley.
Hello.
My name is Steven Camardella, and I'm an associate animation director at Insomniac Games.
I was in charge of leading our boss pod during Spider-Man 2's development, and I wanted to give you a quick behind-the-scenes look at one of our more complicated boss encounters, our giant Sandman, during the opening sequence of the game.
So fittingly, with a big enemy come bigger challenges.
We'll start off from the very beginning with a glimpse into some very early exploration tests we did with our giant Sandman.
This video shows some really early gameplay and cinematic spectacle moments, as well as a quick gameplay test we did with our giant Sandman moving around in the city and what it would feel like to swing around him.
Shortly after we knew Sandman would be in Spider-Man 2, which was very early in production, we quickly knew we wanted Sandman to be absolutely massive in our Spider-Man 2 encounter.
Obviously, this would bring with it some technical challenges we'd have to find solutions for.
We knew that Sandman was going to be heavy, and fittingly slow.
Once development and animation started, we arrived at about a 20% to 25% speed decrease from the captured data to give us a good starting point for his movement.
We tweaked the speed on a case-by-case basis, though.
Attacks, for example, were given an exceptionally long tell window that were slower than most of his typical movement throughout the rest of the mission, but gave us the breathing room to provide that adequate speed during the attack portion while still maintaining the weight that we needed for him during the rest of the mission.
Once we knew it would be our opening mission, though, it brought with it some additional challenges.
We knew the experience needed to feel unique, but the controls needed to be familiar to old players and teachable to new players.
That also meant that the player would have limited abilities at their disposal, so we needed to make this encounter work with basic abilities and combat.
With that ever-important wait and slow speed that we were trying to hold on to, we quickly ran into a pretty big design pickle.
Our heroes, even at their slowest speed from a brand new game, could literally swing circles around Sandman and be virtually untouchable.
After some attack tests of grabs and jabs, we knew large, sweeping attacks that covered huge areas would provide real threats to the player.
But we still had to get a little bit more technical help from our amazing programming team.
Even with the massive sweeps that Sandman was covering, the player could still easily swing to safety without much thought or even by accident.
To make Sandman's attacks pinpoint accurate, our programming team and animation collaborated on using a technique we call focus tracking to slightly aim Sandman's shoulder and ultimately his weapon arm at the player by subtly manipulating his spine and clavicle during gameplay that was overlaid over our animation.
I like to call this reversed synced aiming, since we normally sync the player to the enemy during synced moves.
But in this particular case, we have Sandman focus on, or aim at, the player's sync joint, and then capture the position once the attack portion of the animation starts.
If the player dodges, the player's sync joint, which is Sandman's current target, would remain fixed in space until the dodge performance is complete.
This allowed our Spider-Man to barely and consistently skim over the weapon without colliding and giving them the dexterity only a superhero could have.
So here in this video, what you can see is one of Sandman's main attacks, which is the arm sweep.
And you can see a debug view of this arm sweep attack.
The pink arc that you see is the damage collision moving through the sweep range.
And the focus target that's labeled on miles and the end of that white line is Sandman's intended target.
As you can see on the dodge, the white line pointing to the intended target remains stationary on player input and throughout the performance of the dodge.
All throughout the attack, dodged or not, Sandman's spine and shoulder are being subtly manipulated to ensure accuracy to the given target.
The other end of the white line, that's pointing at the focus target is at the start of Sandman's manipulated joint chain, which you may be able to see is about mid-spine on Sandman.
What this tells us is that everything from the mid-spine up to the clavicle is being subtly adjusted to make sure that that weapon arm ends exactly on that sink point.
This gives you a general idea of how we handled most of the attacks during the Sandman fight.
Now that we knew the player could get hit accurately and consistently, we needed to ensure that the hero's reactions matched the intensity and presence of our Sandman.
With Sandman being so large, we had to ensure the resulting performance from a hit felt just as large and visceral to really make the player feel like there were consequences to their actions.
To do this, animation and programming, again, closely collaborated to employ what we called super knockbacks.
These would launch the player far and away from Sandman with a quick recovery to keep the player in the fight.
This video shows a couple of variations of our super knockbacks.
These were intended to take into consideration the surrounding environment when Sandman landed a hit.
These knockbacks would target markup placed in the environment to steer the player while flying back after a hit, either into a building or a free space.
If the player was within the target cone of the markup, the player was then steered towards the target, and animation would cover up the majority of the visual oddities from any steering that we were doing through clever camera movement.
This video here is an example of a building super knockback that has our debug markup visible to help visualize the system and how it works.
The small blue and green squares dispersed across the area are our target markup, green for building and blue for open space.
The large red cone that we have highlighted is the target cone and the intended target.
Each markup target has its own targeting cone.
The large blue cone is a representation of the hit direction and where the player would be expected to go, or the acceptable steering angles to where the player could be directed.
The intersection of the hit cone and the target cones in the vicinity would determine the knockback target the player would be directed to.
There was still the very rare case, though, that these checks would still fail, and the hero animation would ultimately fall back on a simple air flail hit react that would keep the player from unnaturally colliding into the environment.
This usually happened when the player was in a bad position, usually really close to a building, where the trajectory to the intended target would have him actually penetrate through a building.
It was still rare, but we needed to have that in place just in case.
These super knockbacks ultimately fulfilled the fantasy of getting your block knocked off by our giant Sandman, so we think we hit the mark on that one.
We knew the Sandman mission, though, was going to be huge, both figuratively and literally.
But we had to keep in mind that this was still the opening tutorial mission.
This meant we had to make sure a new player would easily digest the control scheme and that they would be familiar for the rest of the game.
We knew we needed some basic inputs that would give the player spectacular results.
Basic melee combat and basic wham shot.
The inputs necessary were meant to mimic the standard webshop functionality as well as the cadence and responsiveness of a basic melee combo.
But we also needed it to fit the encounter appropriately to match the weight and size of our boss.
That ultimately led the team to what we called the wrecking ball combo, which would have the player use chunks of debris yanked out of Sandman's face to pummel Sandman during the boss fight, because that felt pretty good.
There were several challenges we faced while creating the wrecking ball combo, though.
We had to ensure the player could understand the sequence of controls, as well as the actions being performed by the hero.
And we also had to make sure that the combo remained responsive and felt very close to our base melee combat.
Due to its size, we had to make sure Sandman's blending was in perfect shape, because any pops or misalignment during these moves would move tens of meters in distance, rather than a few centimeters that we would get with a normal-sized enemy.
While in practice this is the same principle as a normal enemy, making sure Sandman's reactions didn't pop and the moves flowed together made this a bit more complex than typical combat.
Finally, making sure the characters and props remained in perfect alignment while traveling through the air took quite a bit of tweaking to get it to work correctly.
We had to switch the sync location mid-combo to ensure the player remained in relative space to Sandman rather than being stuck to follow Sandman's head reactions as during each move that would cause the player to shift back and forth with his head.
Testing and feedback obviously were a huge part of making this combo work.
We had issues with the combo being too slow, with the engagement distance, as well as the tricky note of not making it feel like we were ripping Sandman's eyeballs out of his head.
Ultimately though, these combos ended up working really well and gave the player an exciting payoff while still keeping our base combat control scheme in play.
Another goal that we set out to achieve early on in Spider-Man 2 was to showcase the power of the PS5 and what kind of Marvel hero spectacle moments we can achieve seamlessly during a tentpole boss fight.
The giant Sandman fight seemed like an opportune moment to showcase this.
It was early in the game, and we knew we had to push the streaming speed of the SSD, and what better way to do that than for Sandman to grab Miles, throw him half a mile away, through a building, all the way to Washington Square Park, stream some civilian and traffic data in, and then send him all the way back into gameplay seamlessly.
Our boss pod and design teams worked on getting the sequence to function technically, and our missions animation team stepped in to polish it up to a final product.
Ultimately, it was up to our VFX and optimization teams to handle the technical streaming aspects and put the finishing touches to bring the sequence home.
All the traffic and pedestrians seen in the sequence were streamed in on the spot using open world systems.
This moment clearly showcased the speed the PS5 SSD could handle and also gave the player a truly epic Marvel moment to witness, all while keeping them in the action.
We hope this brief glimpse into Sandman's development sheds some light onto the challenges and spectacular collaboration needed to bring our massive foe to life.
While animation was a large part of it, it was just as much a technical environment, character arts, audio, and visual effects effort to bring this massive boss to life.
Thank you.
And now I'll hand it off to James Panton.
Hello.
Thanks, Steven.
My name is James Ham.
I was also an associate animation director on Marvel's Spider-Man 2, and my focus was our mission cinematics and QTEs.
So in Marvel's Spider-Man 2, there were multiple types of QTEs.
These are a few that we wanted to highlight.
There's the fail forward, feat of strength, and play anon.
Feat of strength QTEs are a staple of the franchise, but we wanted to find new ways of incorporating them into the game.
So instead of just mashing a button, we started using them by pulling joysticks in a specific direction.
We also took advantage of the new adaptive triggers to find the sweet spot.
And here's some examples of their uses.
So on the top left, we used the sweet spot mechanic to take advantage of the triggers.
Depending on the pressure applied, it would reflect in his progression.
In the bottom left, we use the mashing setup to show progress as the player gets closer to completion.
Now the ones on the right are a bit more complex.
The top right example has moving parts on both ends.
They need to respect each other by staying in sync, as well as try to show some progression as the player can feel the change over time.
The bottom right has one of our more complex setups as the coaster has multiple moving parts, people in the coaster, our hero, webs attached to the part, to the coaster on both ends.
So everything needs to stay in sync while also showing progress.
So lots of moving parts just to get this set up that way people aren't floating outside the coaster, webs aren't flying everywhere.
So we had to make sure that everything stayed in sync.
Our fail-forward term is one of our ways to have branching moments in the game going into any QTE, we ask the question, what if the player doesn't press the button?
By failing forward, we allow the player to continue the story without having them replay the same thing multiple times.
It allows the player to stay in the moment without breaking momentum and helps give the player an experience we would like for them to have.
Granted, they take a bit more time to figure out and create, but it also allows us to tell slightly different stories, an alternate version, to move the sequence along.
So on the top left here, we have Miles getting ready to get grabbed by the lizard, and if the player succeeds, Miles is able to dodge, land on the lizard's back, and then continue moving the story.
If the player fails to do that, Lizard will then grab Miles, drag him underwater, but we still ended up getting it to the same direction to continue the story forward.
Then we have an alternate version of that, which is just multiple setups, where here the player is has the ability to grab arrows coming at him, dodge them, then you have the option to use a firework and hit enemies with it.
If the player fails to do so, Spider-Man will then get hit by arrows, and then instead of using a firework, he'll find another way to keep the story going along in this version, just yanking the enemies down.
Play Anim QTEs are one of our more powerful tools where we can do full animations on characters without branching to different shots.
This allows us to play individual animations on characters while still maintaining a cinematic feel.
These are more complex due to having a lot of moving parts.
And here's another example.
So in this mocap shoot, we have our actors, the two on the top left of the platform are our Spider-Man, Miles on the left, Peter on the right.
The person on the ground on the left is also Miles.
And then there's another person on the right that's playing as Peter, and the person on his back is our Venom.
This is spoilers, so yeah.
But in this sequence here, you're going to see our Spider-Man fighting Venom, doing a combo.
So you can see we have a breakdown of Miles throws a punch, Peter throws a punch, and we have Venom reacting.
The reason why we have people on the ground, and we have two versions, is that way we have multiple variations in case one data comes out a little bit better.
Also, our actors on the ground aren't fighting gravity in an unnatural way.
So it might give us a little bit better data to plug and play with which one's going to work the best in the overall scene.
And then to finish that sequence off, you'll see the same actors are in the same positions.
We have the two actors on the platform being Peter and Miles.
Then we also have the alternate versions on the ground, and we have Venom in the middle.
And this is to finish off that sequence.
and it looks like a mess.
But, the end result.
Thank you so much for watching our presentation.
We'll now open the floor for questions.
Yeah, if that wasn't clear, we're open for questions to the end.
Yeah, does this work?
Hello.
We have some mics to each side.
Hey.
Hello.
I've got a question about those QTEs.
I think that is like an incredibly elegant solution to like a common problem of that replayability.
Did players notice that there was sort of this, I don't have a more graceful word for it, but like unfailable sort of thing?
And if so, what was their reaction like to the sort of twofer?
Is my mic working?
Can you guys hear me?
Oh, there you go.
Check, check.
Cool.
There we go.
Yeah, I think players started to notice we started implementing that feature in Miles and we noticed it when players started finding out that if you failed the certain QTE when Miles, we called it save the baby moment in the mall.
If you don't save the baby and you like go to photo mode and you look behind, Peter saves the baby and he gives Miles like a little thumbs up.
But I think players started to notice it and it helped it with replayability where it's something new that you can find You can also see a different side of the story.
But also, you're not stuck with the, oh, I failed this.
I have to replay it again.
And then you're stuck doing that for five minutes.
And it's just going to help move the story along.
Yeah, it's less frustrating, I think.
I think it makes gameplay and progression more fun.
Agreed.
Thank you so much.
I think even if they don't notice, if they're happy.
Yeah.
If they're happy.
That's what we do.
If they're happy, we're happy.
Hi, first of all, thank you for being here.
I appreciate it.
My question was, for the workflow with ASL, do you think that this, did the people who helped you, was this a tried and true method or was it more of a work in progress?
I think it's definitely a work in progress.
I mean, it was, we've had Haley in previous games and we were certainly developing a pipeline and a system for it then.
But I think as we've worked more with our consultants, that we've done it more times.
Thank you.
We're getting better and better at it.
So even as the shoots went on during shooting our ASL, we were able to kind of get up and running a lot quicker.
Knowing everybody was coordinated, we knew which direction people needed to face, where their interpreters were.
But it was certainly a learning process and something I hope that we continue to improve on.
Absolutely.
Do you feel like you're going to increase more lip sync animation?
I'm deaf.
So to be able to lip read was a huge deal to me in the game, because I was able to not read the captions, but look at the performance.
Do you think that's going to improve?
I hope so.
I mean, we always strive to make it better and better, and that's something that did come up with our ASL consultants.
You'll notice in a lot of our ASL scenes, we tried to make sure that whoever was speaking was on screen, whereas with other cinematics, sometimes we'll take more creative license to not show their faces.
That was something in those scenes we tried to make sure people could read faces a little better.
Thank you so much.
Thank you.
Hello, thank you for the presentation.
I have two questions.
The first one is about ALS as well, and specifically about localization.
Is this something that you ever considered, at least using procedural animations or something to kind of work with the ALS in different languages?
Because, you know, obviously we have the English ALS, but also other languages and changes in how they work.
I'm English when it came to sign language for our game, but we in addition to subtitles We also had a pretty robust audio description system in the game to make sure that we Could localize that as well But yeah, we were limited in terms of signing that would have been really a lot of work to do.
So yeah, unfortunately we did have to stick with one language there, but we hope that our subtitles and audio descriptions kind of backfilled that.
Absolutely.
And the second question I have is about cinematics and seamless transitions between gameplay and cinematic.
I think Spider-Man is a prime example of really well done seamless transitions.
And I'm wondering if you have any mo-cap tips for how to shoot these and what your ideas are when it comes to seamlessness.
Hand it over to our cinematics gurus over here.
Yeah.
I'm not sure.
The most important thing you want to do is to, in this position you're between your narrative, your writers and your designers and you're going to have to really talk to them and you're going to have to basically imagine your way through that.
There's a lot of different, sometimes you need to get rid of the hero and bring them back in for it to be seamless and then sometimes you're going to hand them off.
I think at the beginning of every project, we all kind of go, they're all going to be seamless all the time.
Into and out, it's going to be amazing.
It's so much cheaper to cut, it's so much easier to cut.
Just show a reverse or show something else and have them walk back on screen.
And a lot of times we go to that because maybe it's the right call for the story or maybe it's the right call for our schedule.
But yeah, we do try to imagine what the needs are from gameplay into the cinematic and make the most interesting and seamless way possible.
There's, of course, all different ways to do it, where you can funnel the player to a certain location.
There's ways that you can sort of tag the first shot to play hero relative.
That one doesn't get used a lot, but it's super powerful.
We should probably use it more.
there's yeah and I mean The real thing is making sure you hit your core poses when you go back to... When you call cut, they're hitting their core poses.
That's like 90% of getting you there.
I think another important... We've gotten pretty good to the point where we prep these shoots as well as we can.
We prep them extensively by working with designers and going, how are we going into this and how are we going out?
and I think we always kind of cover our asses now and always plan to have a seamless transition whether we use one or not.
There should be a picture of what the gameplay out is while you're shooting and you can show it to the actors and you'll be standing here like that, you know.
Right foot forward.
But yeah, it's hard work.
But we do maintain, like, every pod or multiple pods meet, sometimes twice a week.
And everybody just talks about it just like you would now and does the, you know, it would be cool, even if you can't afford it.
And it really gets ratcheted up by everyone.
You know, there's always somebody going, you know, it'd be cool if you, like, flipped over backwards and did that.
And usually it'll be quiet and they're like, you're right.
And someone will try to, you know, Make it better.
They make each other better constantly.
So so thanks.
Thanks for that question.
Thank you Hi, I want to follow up on that thought and especially with the ASL shoot so you have your Main actor you have your stunt actor you have your voice actor And you have this poses that you have trying to match up Right?
So you have all of these layers.
As an animator, which one do you prioritize when you're working with physicality or, you know, face performance or the stunt?
Well, I think the goal is to strike the balance.
And so that's why we, I feel like we've kind of shown it in multiple of our presentations that we like to have options.
it was really important for us to shoot our main cast alongside our deaf actors as well so that we could pick and choose from those performances.
A lot of times our main cast actors would put in work to learn the signs and it was a much easier data to bring in and just use and improve upon but a lot of times we had some pretty extensive conversations and we wanted to make sure that we had the most accurate data.
So it was always striking a balance, always having options, and always choosing the best performance.
I mean, we're a performance-forward studio, so we always want the most emotional and authentic performance.
That's where it comes into collaboration.
Like, our deaf actors and our main cast, you know, they're treated equally on the set, and we want both of their performances.
So, yeah, it's a lot of... Just to follow up on that, so in your prep work, you are shooting everything you can at the same time?
Yes.
Okay.
So that we keep our timing the same.
So if I need to move in, if I take the first three seconds of this performance, I can pretty seamlessly, as long as they're looking at each other and kind of performing the same thing.
You know, Najee, when he was doing a lot of finger speaking in that scene with Haley at the end, he was looking right at Justin, our deaf actor, and he was kind of speaking to it and trying to kind of move his body at the same time so that the inflections were the same so that I would be able to later move in and out of those performances when need be.
Thank you.
Yeah.
Lots of layers and blending.
Atom layers.
And to Lindsay's point, like when you synchronize that and you're running them together, it's really easy.
I think, Bobby, you kind of set that up on layers, and you can just really dial in and out kind of between performances.
And since they're synced up, it's pretty down.
Oh, that's right.
We were animating the layer slider in Maya, just sort of bringing up, which basically just on a layer baked the ASL performer on You know, we can dub the dialogue if it was, you know, unclean or something, but was it a heartfelt performance?
Like, that first, and then we can still fix, you know, hands and body or his hands not following him because somebody's moving and the other actor didn't.
Like, we're animators, so we approach it as, what can I fix and what can I not fix, you know?
Well, my question is a kind of follow-up for that discussion.
First, Stephen, when you were recording all the actions at the same time, I saw that you might be using some real-time.
I wonder if it's Unreal.
Is it Unreal?
You're using Unreal for, like, visualize what's happening, everything at the same time.
Is that helpful for the process?
Do you do any, like, after the session, during the session, do you cut the movement and try to put it together?
I don't know what program they used to show the real-time set.
On the stage?
Yeah, I'm not sure what they used either.
Like, you could see the video on the stage?
I could, yeah, I could see something.
I was wondering if it was Unreal Engine.
They have the, they can, our studio can run Unreal.
We don't use it.
I don't know what program they use, but it's all the same to us.
Like we don't, we don't go with lighting or anything.
It's usually untextured, but a hundred percent, whatever, whatever condition the set is in at the time of shooting, we use it.
We measure everything to it.
We take the time.
It's really worth it to tape off feet and make sure people are hitting their marks.
To us, using that stage, of A cinematographer who's also being mo-capped or anything like that.
We just let them live the space as best as possible.
So yeah, we're not doing visors or anything.
They can see themselves on the TV and get used to the space, but they'll just have the gray mock-ups in the room.
But I feel like, and this is an opinion, but we get the best work and we don't complicate the day with cinematography.
We just have actors live in the space and act.
To me it works, but you know we can fight in the parking lot We do use the real-time playback a lot of times we'll we'll do a take and then you know if it's like a crowd and they're reacting to something, we'll use that for timing, so we'll take the real-time playback, play the previous take, and the actors will then act against that take, so they're using the same timing as they did in the previous.
Yeah, and they can even look at their old self, because you might have four to eight people on stage, and if they walk through themselves, we'll put a rope on the ground like, that's where you were.
Don't be there.
I was just wondering if you do any edits during the session or like right after, just to see if things are matching, since it's all the actions at the same time.
Only when we need to.
OK.
We're very cocky, I guess.
Should we take a question from this side?
I think they were like here before me.
Oh, OK.
What you got amongst yourselves.
Hi, so you talked a bit about being a performance-first studio, and another theme I'm noticing in the QTs and the knockbacks is you seem very concerned with how the animation impacts the game feel for the player.
So I was just wondering, in addition to those, if you could elaborate a bit on the direction pillars that you have as a studio, what you want to get out of the animation.
Our main pillar whenever we're working on the Spider-Man titles is always to make it talk We knew we needed to have that type of reaction.
We've seen it before in the, you know, NCU.
And so we needed to figure out a way, like, how can we make this work with the environment and also keep the player with, giving the player that freedom still of being able to move around and not making it feel too canned.
We needed something that was quick, but also gave the player the feeling of, like, I just got rocked.
So that's...
Yeah, I think a lot of the things what Steven just said is accurate.
Like, we go for the feeling.
A lot of times it's like, does this feel good?
And then does this serve the story elements of what we're trying to tell?
So going into that with pretty much every script, every QTE, we're like, cool, what are we trying to accomplish with this?
How do we make this feel good?
Does it reflect the gameplay buttons that the players normally press?
We try to make sure that's like the first thing we hit.
And then also then for us animation, we're like, how do we make this look cool?
You know, it's like, it's easy to say this goes from point A to point B, but what's all this stuff in the middle, you know, and how do we make that look good within certain time constraints?
So as long as it feels good for us, feels good for the player, that's ultimately what we're trying to get for.
Go for it.
Thank you.
Hi.
My question is about blending animation.
So when blending between animation during gameplay, during cinematics, and especially between cinematics and gameplay and vice versa, how much was relied on animation blending that already came in Unreal?
And I know that you guys also have an internal engine that you guys use.
So how much of that was custom built, and were there any specific challenges?
I mean, I can go ahead.
Yeah.
Yeah.
So we have our own internal engine and kind of what they talked about previously, like we have certain core poses that the player needs to hit.
So especially going from cinematics to gameplay assets, um, we'll either have the characters like, all right, they're getting ready to go into combat.
We'll have our aggro pose.
We need to make sure that is the correct pose.
But then especially for this game, because we had two heroes, uh, we had to make sure it's the correct pose for miles or the correct pose for Peter.
And then there, we do have blend settings to say like, if they're, We want them to be in that specific pose, but if they're close enough, we have some blend settings like blend over this duration of time.
And that helps smooth things out going from cinematics to gameplay.
And then we also have a couple of events to help out with like making sure that the camera stays where we want it to be.
That way we can go into combat setups and have these cinematic sequences to start the combat setups up.
Or if we're going to traversal, making sure that the camera is at a specific distance and going into gameplay, and that takes a lot of debugging.
saying like oh the camera needs to be five meters away with this focal length and everything so we try to sync that up with the gameplay assets as much as possible but we do have some blending duration times to help filter that out and help smooth it out as we transition.
a look But with Sandman specifically, he also took up a lot of screen real estate.
And he was massive.
So the issue was that if we used our typical blend times that we would use for any other enemy or for our heroes, you'd see him move drastically.
And it would look really unnatural.
So Sandman's blend times were usually three to five times longer than what we would do elsewhere.
And that was just kind of feeling it out and making sure that everything would perform as we wanted it to.
So we just let everything down.
Thank you.
I have just one more question.
I didn't see that in the video, but how much of the camera was mo-capped or if it was not mo-capped at all?
No mo-capped camera.
Not at all.
Well, we made a camera that has motion that's been extracted from human hips and shoulders.
And that has been put into a loop.
and I made this just to clean up my life because when I worked on a title many, many over a decade ago, where it was all mo-capped camera, and it was a nightmare.
And you know, there was like, people would give you like 40 takes of mo-capped camera, and they weren't synced up to it, and you're just like, oh, I don't even, I can't even find what this is supposed to be looking at.
So I made this thing that basically was just loops of motion capture data at different frequencies.
And so there's like Swell, which is like a big ocean.
It'll just do this to the camera, you know.
And it does it for a while.
And it just keeps going down until you get to jitter, which is like you're sitting on a washing machine.
And so you can animate these up and do little shark fin ads in the graph editor to camera shake and fade it off.
And it's quirky.
It's not a perfect thing.
You get used to how to use the rumble cam.
But at the end of the day, you could take you know, two keys.
I'll, you know, set a key at the beginning, set a key at the end, and then push in on you.
And then just add that, that drift that you get from wave or shake, or not shake.
Shake would be way too high.
And then you sort of just dial that in.
And there's like, it's just beautiful.
There's like, I mean, two, two, two keys.
you know, not a bunch of crap in your graph editor.
If someone needs you to change it, you don't look at them in horror like, I can't change it, there's a thousand keys on it.
So it's just really simplified and it looks like mocap data and it, like I said, it simplifies the shoot date not to use it and there's always that learning curve, you'll get new people, they're like, I've never, I'm not a cinematographer, but like, you will be.
You will be, by the end of this project, you're gonna be very good at it.
And I think that just, it just keeps it simple and beautiful and easy to manage, like, yeah.
All right, thank you so much.
Hi.
The final QT is really, really awesome.
Thank you.
And what is the working pipeline from the idea to the final product?
And because isn't there any suggestion that we can do at first before we shooting?
Because we can know if it works at first.
Yeah, thank you.
We did like a previs, I had like a week, and it was like, hey, previs this whole thing, and I was like, I have a week, I don't know how far I'll get, but here's an idea.
And that whole final QTE sequence changed a lot over the duration of the game, especially like the last, what, three, four months?
It was like, hey, it's this thing, and it's like, no, it's not anymore, it's gonna go with this.
Yeah.
He was trying to add even more.
Yeah.
There was, there was a lot more, there was a, there was a whole bunch of truck sequences and everything.
And then they were like, it's too much James.
And I was like, okay, so it was cool.
I was sad, but I was like, um, but a lot of it is just kind of like figuring it out.
And I kind of learned a lot on the go of like how I can manipulate this and kind of what we're saying before.
Like we wanted it to feel good.
We still want to have as much of it on the sticks as we can.
So like, even as they're like punching venom, um, a lot of that's kind of faked.
We didn't really have a system for that, but I was like, Oh, if I make his reaction to something look like in action so peter's reaction to miles was prepping for a punch and kind of alternating between the two we found out that that could work and it's kind of tweaking things to see what felt good and then just a lot of places like a does this work that i work with bobby a lot on the cinematography where it's like a put the camera here we want to be able to see this in the background we wanna see this we want to see them going in the right screen directions or bobby about like quite a bit on that just on that aspect of it but uh...
Yeah, it was fun to make, but it was very hard.
Make it up as you go and figure it out.
Figure it out.
Say yes and figure it out.
Get some help.
That's the previews combined with the gameplay or just animation?
The previews, I tried to get some of it in game, but for the most of it, it was just kind of a movie set piece of like, this is what it could look like.
Is this what you guys want?
And it was like, yes.
And I was like, crap, I got to figure out how to make it now.
But yeah, a lot of it was just in a previz of Maya, just like a play blast.
Then I exported it out into the game just so we can see where certain environments are, because trying to bring the city into Maya makes Maya super slow.
So I would do a rough draft, put it in game just to see like, oh, here's where this building is.
If I want them flying this way, trying to figure out what the path would be.
And then ultimately, we were just like, just shoot them up into the air.
And I was like, that's easier.
So we went with that.
Okay, thank you.
Hi.
In a production that's so high value with so many different disciplines coming together, I imagine that you have to concept a lot of things out ahead of time.
So how does that impact, in your experience, your artistic ability, like your freedom?
Once you have a performance, do you have to get a lot of your ideas out in the storyboards?
And then do you feel like that constricts what you can then do once you're actually animating?
You haven't talked in a while.
Yeah.
I mean, honestly, on this project, we didn't do a whole lot of previs or anything.
A lot of it is getting together with the writer, the designer, everybody getting into a room and talking things out.
And then we might pitch something.
And everyone's like, that's awesome.
Go make that.
And then, yeah.
I never really feel that stifled artistically at Insomniac.
It's great, and usually people... I think we've built up enough trust in the studio.
They're like, you make awesome stuff, just go do that.
Yeah, I think that's been one of the coolest things as we've worked on three of these games now.
as This isn't working.
We wanted, oh, this would be so cool.
Maybe we can find another piece of data that we already shot, or maybe we can get back onto the mocap stage and do this.
But generally, yeah, we kind of get to pitch our ideas.
and make them work.
So no, I have never felt more creatively fulfilled than as I have at this studio.
The trust is off the charts.
I think all of us have worked together for anywhere from 10 to 15 years in one capacity or another, or even at different companies.
So we are a family.
And people trust us now, which is nice.
Because when we kicked off Spider-Man, they were like, can you guys even do this?
And they don't ask us that anymore, which is great.
So yeah, there's a super high amount of trust.
We do still pre-vis some things if you know it's going to be a mess.
If you know you don't even see it clearly in your own head, we will send our creative people after it to sort of figure it out.
But never at any point.
There are storyboards as well, but we don't really use them.
And most of the time, just because of the way games are developed and the way missions are developed, They're kind of obsolete by the time we start to make them.
But whatever it is, we use it and we put it together.
If someone has a good idea, the whole good ideas come from everywhere thing, that's all true for us.
So we'll take as many sources as are available to try.
But even if it's concept art, they'll have a picture.
Like, that's rad.
We'll try to get that in one of the shots.
Yeah.
I mean, that's kind of the, you kind of touched on that.
We don't always have a good idea of what it is yet.
We can write it in a script as many times as we want to, and we can talk to designers and things like that.
But sometimes, if somebody has a really good visual in their head and they put that visual out, that's kind of what makes something real, is getting the rest of the team excited about it.
So, like, you know, James's awesome finale, like, he put that visual out there, and then everybody kind of, like, rallied behind it, and then you kind of push it to the final product.
Spider-Cat.
That's how Spider-Cat came to be.
That is awesome.
Yeah.
Somebody made it, and we put it in the game.
Yeah.
I mean, and even to that, like, we, especially on stage, like, we have awesome stunt performers.
We have a lot of awesome actors, and, like, sometimes we'll have an idea, and I might be like, hey, I have this idea.
Here's, I want these specific punches and stuff, but I'm also like, hey, you guys think while you're doing this, and if you come up with something better, let's shoot it because I'd rather us have the data and that way we can pick and choose because sometimes we might have data and it's like it works but it could be better and we'll filter out and a lot of it's just scrappiness like well let me look for data let me search kick what kind of kicks do we have what kind of options can I have can I is this something that we can keyframe real quick and getting a better visual so I don't feel like we really stifle ourself we try to push creativity and that's also one of the benefits I really enjoy about like not locking in to specifically having a camera going to a mocap shoot because sometimes discovering what that cinematography is comes from after putting the whole sequence together and you're like oh if I put the camera here this is dope versus if I go into the mocap shoot I might feel like oh this is cool then I realize later I'm like well I'm locked in now you know so it just gives more freedom for creativity and it's like that's something that I we really push for.
I'd say that goes with every decision we make.
You need to show up prepared, but you need to not be, that preparedness should not lock you out of all the great ideas that everyone else is going to try and bring to it.
I know we're getting close to time, but we could probably get another question.
Hello, yeah, so this is again regarding ASL.
So in the slides I saw that you used motion capture data and then refined it to create readable ASL, but in the future, or like in this project, did you consider, or would you consider building a library of isolated animations for hands and like signage that you can reuse and omit, sort of create like a ASL Google Translate for your animation pipeline, or would that affect readability since it's more mechanical and not motion captured?
I think that's something we've definitely thought about moving to the future.
I mean, that's something that we do with poses, we do it with facial poses, we create libraries so that we can quickly do handshapes.
We do it with just normal handshapes, you know, fists, this, that.
So having a pose library of sign letters and things like that could be incredibly helpful moving forward if we had more in the game.
And that's absolutely something to consider going forward.
Speeding up that process, because it is a very labor-intensive process.
So that would absolutely, if we had the time and the ability to do.
Thank you.
Oh.
like that doesn't happen in cutscenes?
Sometimes I have to check because they're, you know, like... Yeah, we have procedural events that we will drop on specific animations, so like synced moves or things like that, finishers.
There's procedural shake events that we drop on those animations, but they're not driven from Maya.
Yeah, the only times we would... There's a few certain circumstances where we would do that, and that's like finishers.
Finishers have custom cameras where we can introduce that, and it uses the gameplay camera, but lets our cameras override it.
majority of the time we have a different system for gameplay stuff.
I think we are at time.
I'm sorry, but thank you guys for all the questions.
