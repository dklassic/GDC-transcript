My name is Adrian Yu and I do have some credibility behind my name.
I've worked in the automation industry for 12 years, 11 of it has been in the gaming industry.
So from that standpoint, I guess I'll actually go back one slide.
I assume everyone here knows the benefits of continuous integration and you guys are here for Jenkins because Jenkins is in the title.
I start off as a software engineer in Tess, so I focus on automation scripts and then eventually my team took on the builds at EA.
Now I'm currently an associate technical director for the development and release engineering team.
So what is Diri?
My team is a distributed community of embedded DevOps engineers focused on continuous integration and deployment automation.
We're kind of a unique team within EA.
We serve a lot of the game teams.
And we're effectively the build engineers of EA.
Now what do we do?
Our name is kind of not that obvious, but what we do is we set up automation for your usual continuous builds, for those that are smaller startups and stuff like that.
The setup I'd recommend for new automation would be from left to right, just because that's easier.
So Kinetics Build, Static Analysis, then once you have those running, you set up your build verification tests.
Then eventually you add metrics, performance metrics, build metrics.
Then you start focusing on bug and crash reporting as your game gets more advanced and more mature.
Then you write miscellaneous tools if you need to to help with automating things.
throughout the process, and eventually you start doing custom reporting because your system's stable, the game is stable, your CI system is good.
And then at the end of the day, our team owns the end-to-end CI, so we're able to improve the productivity of development teams by taking on that task that everyone here probably understands is kind of maybe a little tedious at times.
But by having a dedicated engineer, which is not always the case, you're able to identify optimizations throughout the pipeline.
And if you're lucky enough to do multiple iterations of a product or releases, you get better and better.
And you start thinking of ideas by staying focused, not as a side job, but as a dedicated job.
Now as far as I'd like to provide a little context on the team. Our team first started in Vancouver EA Sports, I've been in Vancouver for 11 years Eventually, we did all the builds for all the sports games and then over time the past 10 years Everyone kind of heard of us within the company and they started reaching out to our team and then we eventually started I guess, adopting and helping out other studios.
So it's kind of a unique situation.
We first started off as a team of 15 in Vancouver.
We're now over 100 worldwide, just within the company, so it's kind of a unique situation.
It's kind of a double-edged sword with this kind of expansion that we're lucky to have.
By this, we were able to take over and inherit existing automation systems in different studios as EA acquired them, or at least they started reaching out to us.
Through this, my team experienced rapid growth and some of the pain points, which was the technical debt that came with it.
So a lot of these studios had custom CI solutions built around their game and their workflows.
This was good for them, but also bad for sharing amongst all the different teams.
So one of our strategies of our team was to actually adopt open source to help tackle this and help remove some of that technical debt.
So that's why I'm up here.
I'm trying to preach that adopting open source is okay for your CI system.
It's probably the same thing to do, especially if you have to grow your team rapidly.
I don't know about you, but games industry, we make games.
We're not necessarily tech companies, so writing our own CI system, people on my team made that mistake in the past, and we were finally able to deprecate that, and now the majority of our company's running Jenkins.
To give you a little scale, we have 200 plus Jenkins masters, over 2,000 build nodes, so that's a lot, a lot to handle.
I'll go over ways how we can handle that, throughout this presentation.
You know, if I were to ask any of you what the most important trait of automation is, most of you probably say speed, reliability, or flexibility. You're not wrong, but you know, I think it's everything combined.
Good reporting.
is a combination of reliability, speed and flexibility.
If you don't have good reporting, nobody knows any of that because they don't get the results, they don't understand the results, they don't see it.
So you definitely need to focus on that.
Once you have good reporting, you can use that to help manage expectations.
So for those of you that are administrating the build system, you'll probably...
have to defend the system, the reliability, and as new people come on the team, they're always questioning, that's an automation error, right?
And that gets really tedious to answer time and time again, so you wanna have good reporting to help manage those expectations.
Once you have that, you can now focus on scaling your automation responsibly, because you can now see the stability of the system, people trust the results, and you spend less time talking about it and defending it, and everyone just sees the value of it.
So since this is GDC, I decided to apply some reporting levels to what I'll be presenting a little later.
So I kind of like applied the World of Warcraft leveling system to it.
So when you have basic dashboards, I kind of call it level 60.
That was when the game first came out for those that.
weren't around then.
And then as you add log parsing and a little bit of, I guess if you want to do machine learning and stuff like that, you start to get a little bit more intelligent reporting.
And that's when you really get up to level 85.
And then ultimately, when you start getting scalable reporting solutions, integrating BI tools or OI tools, I'll start going over that a little bit later.
So first off, to start off with good reporting, I'm gonna cover four different areas.
One is log levels.
Another one is the default reporting features.
And then ways to diversify the views that you have.
And then also incorporating OI into your stack.
So first off, to start with introducing log levels, you need to introduce log levels into your system because if you don't.
your output's gonna be really, really bad, and it's gonna be really hard to read.
It just compounds the noise problem of your system and any subsequent reporting, or things that you wanna do with it afterwards.
So some of the things that you wanna do is introduce the log level.
So for things like Log4Net or Gradle or Maven, all those things have ways to toggle the log levels, so I'd definitely look into that.
For things like Unity, I'm a little less Unity-centric for me, so...
Perhaps there's ways to do that, hopefully there are.
If you do manage to reduce log noise, you can increase the performance of the system and you can ultimately have longer history retention, which everyone seems to love.
Because when you have automation, you run a lot of it, everyone wants to go back in history to make sure things are running properly.
Now that does have drawbacks.
The drawbacks include, you know, you're increasing the complexity of your system by adding different logs.
And usually, or sometimes in the rare cases actually, you don't have enough information.
And then you're forced to actually incorporate toggles into your automation to then rerun it and output more information so that you can rerun it effectively.
The second one of the things I recommend is leverage the default reporting features for this talk, Jenkins.
You should leverage it.
So I'm going to start at the lowest level.
So when I talk about configs, I think I use the Unity naming conventions here.
But you master release, debug, different code compile configurations.
You want to have separate jobs for each one, actually.
And I'll go over that a little bit more.
Then you add, you start incorporating regions.
For those of you that have multiple regions or SKUs, you want to add another layer on top of that and then have those kind of chain the jobs underneath.
And then, you know, if you support multi-platform, well, this is just an example, but you then start adding projects or schedulers for that within Jenkins.
So these jobs are nothing but shell jobs that trigger things and help summarize the results.
And then at the end of the day, if you're doing multiple branches or streams, whatever source control system you use, you add one level for that, and then this kind of gives you the layer architecture for reporting that you'll be able to leverage later on.
It also gives you vertical silos too, so if you want to look at the specific configurations, you still have the freedom to do so.
This is important to keep your automation levels modular so that you can leverage it later on.
Now, once you have that, I break down views into two different types.
One will be the dashboards, and another one will be direct feedback.
Once you've applied that layer concept, you can start diversifying your views into these sections.
Because at the end of the day, what a QA manager wants to see, what an engineer wants to see, or a producer, they're all gonna be different.
And in the past, my team tried to design an automation system that satisfied everyone at the same time, and that was horrible.
Some people were happy, there's always unhappy people.
So you want to actually break that out.
And for us, we actually broke out our dashboards into primarily two points.
One for job overview, so you look at.
all that job reporting at a massive scale on a dashboard of some sort.
And then second of all, you want to add health monitoring to your system.
As your system grows, you want to make sure that it's in good shape and that it will continue to run. Now for direct feedback for the engineers, for the content creators, for all that stuff.
We basically set apart three different ways.
Primary ways, Slack is one of them.
Setting up dedicated Slack channels is useful.
I'll go more into later.
Then email, of course, everyone's used to email.
People hate to spam, but I'll go into ways where you can control some of that later.
And then incorporating log parsers so that you don't have to manually debug automation failures as much.
So first off, going into immediately an example, open source plugin called the Extra Columns plugin.
I highly recommend it.
We use it for, it's useful for build engineers.
In this case, I've obfuscated some of the information, but the job information's there.
So the interesting columns that you can add immediately are the console output plugin or the icon actually.
So instead of doing a traditional click, you just click there and it jumps straight to the console output and saves you extra clicks.
Then you have this neat little field that you can also expose and actually dynamically populate with your Groovy.
In this case, we use it to highlight a link to a network share with published builds or published artifacts.
And then one that's really useful for build administrators is the slave allocation.
So for those of you that do incremental builds and you want to hard allocate or dedicate machines to specific jobs, this is really useful because you know immediately which job is run on which machine if there's a problem.
So it's really useful for an administrator.
Now the second one that is also open source is the Build Monitor plugin.
So this is a lot of information, but it's good.
So we use these actually on TVs all around the studio and people's desks.
They can cycle through it.
I use it with a combination of Chrome Revolver plugin or Chrome extension to revolve around different tabs.
And for each tab, this is actually reporting off automation for one particular stream.
So if you have a lot of streams, you set up one for each one.
The text is highly configurable.
In this case, you can see the job names are really bold.
You can change the text size.
The bright green is actually showing a build running.
This is, of course, a screenshot.
And at the bottom, you'll notice that there's actually failures and it shows up red.
Now my team has modified this slightly, but we haven't committed the changes back to open source.
So apologies so far, but we incorporated the ability to interpret the log parsing results.
So change the color for automation failure to be yellow, which isn't shown here, unfortunately.
But at the bottom of the red, if you can't see, it actually shows that there's end unit test failures, and that actually gives you a reason.
It also shows you who committed to that build, so you can actually shame people if you want to.
So you don't have to be a build cop, the system can do it for you.
Next, moving on, Jenkins Master Health.
So monitoring Jenkins Master Health, no matter how many masters you have, is very, very important, and especially when you're adopting different plugins.
We actually use four different methods.
So I'm just going over the stack.
So for Jenkins, the application, we use the monitoring plugin, which is open source.
We also use the Splunk plugin.
I'll go into some of these, or actually all of these in more detail later on.
Then at a different level, on the operating system level, we use Zabbix.
You can use anything else you'd like there.
And then for the hypervisor level, we use VMware.
So as you can see, we have multiple layers of monitoring.
So in case the Jenkins application becomes unresponsive, we can go down to the next lowest level.
We have many different levels to go.
Now, the first one, the monitoring plugin, this is actually my favorite screenshot.
It's a section of it.
Through this you can actually see the RAM usage and the system usage, the CPU usage.
There's 22 or more charts that you can actually look at.
I won't show you all the screenshots to save time, but it's really powerful.
It takes all of like five seconds to install and configure.
There's no extra work that you need to do, but it gives you a heap of information just for free, really.
It uses the Java melody.
on the background or underneath.
You can actually trigger your heap dumps.
So in case of all you guys installing different Jenkins plugins and you're having problems running on memory, you can do heap dumps and trigger it and then analyze it after the fact, if you'd like.
We made a lot of mistakes in the past, so that was actually really useful as we started to adopt Jenkins three, four years ago.
It helped us tremendously.
Now, at the OS level monitoring, this is just some screenshots of the Zabbix alerts that come up.
So, all these alerts are reactive, so they're just responding to actual events that happen.
They're not ideal, but they're a good place to start.
And then of course you can also get performance metrics in this case. It's a CPU utilization So if the Jenkins monitoring plugin which is rendered through Jenkins is unavailable You can fall back to something like Zabbix and the history and actually go through things that way to identify any performance impact Now at the hypervisor level, we're lucky enough to use VMware, so we're able to actually go and use the VMware monitoring or the performance monitoring in vCenter or vSphere.
This is a screen grab of some of the information.
This information is really granular, which is good if you want really quick feedback, but each of the systems I show has different...
We configure different log or history retention, so the level of detail is a lot smaller in this one, so you can get real-time, really good feedback from this one.
At the hypervisor level monitoring, you can actually look at individual CPU usage as well, for those that aren't aware, whereas Zabbix and the Jenkins Monitoring plugin will look at the OS level, so it's aggregated CPU usage, so if you want more information...
you can get it through the hypervisor.
Now, moving on to the direct, I guess, the direct notification, specifically Slack plugin.
This is, of course, open source, a common theme.
So this is a use case.
I think some of the talks earlier went about this for the LiveOps and stuff like that.
But if you hook up specific deploy jobs in Jenkins and stuff like that, you can post to a Slack channel and then people, the LiveOps team or QA or whatnot can actually monitor it.
So instead of having email all the time, you can use that.
and actually disable your emails for that.
You can have both, it's all configurable, right?
So I'd highly recommend it for those that use Slack to actually start doing this, and then that way you can start shrinking some of the email noise that comes into your inbox.
Creating multiple channels is also recommended.
Now for the job summary plugin, this is the one of the few plugins we've actually written.
This is actually, This is actually derived from the email extension plugin, which I'll go over shortly.
So in this one, it's a lot of information, but it's good information.
A majority of our Jenkins don't actually use pipeline yet, so we were using Buildflow.
And with Buildflow, we used a step, we were migrating from different CI systems in the past, and we liked the concept of steps or stages before pipeline existed.
We already had it.
The middle section, I think, well, actually, it has a link, so it's an optional view, so you install a plugin, it gives you an optional view in Jenkins.
And then this middle section here.
actually goes over, details all the steps underneath.
So I showed you the layer architecture before.
This is kind of grouping and distilling that in one place.
And you can immediately see via the log parser, which I'll go over later, you can, it highlights the failure and shows you what the failure is in that view.
And then it also shows you the detected output or the errors.
And it also shows you an icon on the side as well.
So we started playing not really a blame game, but made it more obvious that it was a game error.
So we put a joystick.
So if there's a game team compile error or something like that, it would appear right away.
So people know if they're looking natively at Jenkins that it's a specific team's fault, really.
Now, the log parser we use, we chose to use the build failure analyzer plugin.
There's a lot of different plugins out there that you can leverage that's open source that you can use.
Oh, sorry, let me go back.
This one's actually a popular one with us because of our scale.
It was one of the few plugins that allowed us to use a GUI to actually edit it, and actually you can actually connect it to a MongoDB on the backend, and then therefore when you're any Jenkins master, anybody can add a failure cause, it's regex-based.
It would then post the essential MongoDB, which we have multiple instances of in terms of to track the information.
We did some modifications to this plugin to make it a little bit more optimized for our scale because we ran into scaling issues. It was probably the only plugin that ever caused us problems on Jenkins.
We were running like 64 gigs of RAM on some of our Jenkins and when you're log parsing and you have really ugly logs, or a lot of it, it will chew up through a lot of RAM.
So that's just a lesson for everyone to be aware of.
But this is really useful in sharing it, and it's probably the biggest game changer of our plugin for Jenkins, or any log parser really, because we went from debugging things manually to just using this, and then just adding failure causes, and then basically just manually tweaking it and training it with different regexes, and we're able to share this across the entire organization.
Now the email extension plugin, which I mentioned briefly, if you guys ever were lucky enough, I say lucky with the most sarcastic terms, this is the default failure email that you get with Jenkins.
It's all text-based, it has links, and I did a Hello World, and I assume if you have a lot of output, it emailed you the entire contents of the output.
which would be horrible for very large games.
And then subsequently, when the failed build succeeds, you get this really, really useful email with a link.
So it's not really ideal, so what we actually did was...
In the email extension plugin you can configure much better notifications for different culprits and stuff like that.
We configure a lot of rules so I'd recommend that.
And then I didn't mention before about the job summary plugin but we actually use...
a Groovy script to generate the content.
So the exact same Groovy script is used to generate the email content, so it looks exactly the same.
Well, a few caveats, but at the same time, one would ask if you send emails to different culprits and stuff like that, do you have customized emails?
Yes, we do, because it's Groovy-based, you can then just condition it to send out different chunks to different people and different audiences.
This is just one example.
Now, and then following that, the failure email is also just as useful, or I guess it's improved.
It actually shows you the changelist that went into the build that made it pass again, so you can actually verify without having to click anywhere else, which saves everyone time.
And of course, this is only sent to the people that contributed to the build, so it's not gonna add, it's not gonna be noisy for everyone.
And then, yes, the changelist, I've hidden the names for, you know, to protect people.
The part I get really excited about is the operational intelligence portion of this talk and the pros and cons of it.
The scalability of it, so the way we were able to report over 200 plus Jenkins masters or at least a report across them, we had to incorporate something that was better.
We didn't write anything ourselves.
I'll show you a little bit later, but it enables you to debug the issues faster because now you can search across multiple masters, all your job logs, you're not stuck opening tabs and tabs and tabs and scrolling.
you know, that's really bad.
I did in the past, I learned from it.
It gives you greater extensibility because you're now, it gives you options to decouple the reporting stack from your actual automation system so that you don't have to have people looking at Jenkins if they don't have to.
It enables you to do metrics-driven decisions in your CI system.
And also definitely helps manage expectations, which ultimately gives everyone administering Jenkins a lot more time back when you don't have to have those lengthy conversations with every person that doubts it.
And then the cons of course are you, similar to the other ones, you have increased complexity and increased maintenance because now you have another stack to maintain.
But I think the pros far outweigh the cons in this situation.
And I guess for those that don't know what operational intelligence is, it's basically tools that help make sense of large sets of data.
It aggregates log information and makes it searchable by indexing.
And then I'll go into how the reporting stack kind of changes because of it as you incorporate it.
So you can see the familiar Zabbix and VMware layers.
I went over the Build Failure Analyzer plugin, and then it connects to a Mongo database.
The Logstash plugin, which is also open source, we use that to actually inject information from Jenkins into Elasticsearch cluster, that's on-prem for us.
And then I mentioned earlier the Splunk plugin as well.
So the Splunk plugin, I'll go into more detail later on.
Elastic is free, it's free to use and open source.
They have enterprise options just like Jenkins does as well.
So we use it primarily because it's free.
The Splunk, we have a limited license, so we use that as much as we can.
I'll go into reasons why later.
But we don't set it on one, and I think that's a key thing is never to really set it on one if you don't have to.
They have different benefits for each one.
Now, while incorporating OI tools, makes things easier. People often underestimate the effort to generate the reports afterwards.
So it's very similar to safe programming. If you don't sort your data, you're going to have to load problems writing things that handle the data afterwards.
The layer architecture, all that stuff I went before, you need to add something to it.
You need to actually define job naming conventions across all your Jenkins masters.
You know, I kind of set on something like this, so depending on the complexity of your studio, we have different projects, we call them titles.
You have different studios, you have branches, platforms, automation type in this case is gonna be like a test or a build or a deploy.
and then you have regions and configs. When you do that, some of it can seem very tedious, but it'll all make sense when you actually generate the reports later on in your systems.
You definitely need to define an infrastructure naming convention if you don't already have one, because you need to know where things are running.
If you have multiple studios around the world and you don't name it separately, it just makes, even if you report on it, it would make no sense.
Apply the layer concept I mentioned earlier.
So I gave some examples in terms of some of the names we have.
Mainline, so ML, all skew triggers.
So it's just a trigger project.
It has no real logic underneath other than chaining it to a bunch of other jobs.
You can do this a lot easier in pipeline now.
But we did in Buildflow.
We don't have to use Buildflow.
You can do it freestyle as well if you'd like.
But that's definitely very important.
When you do that, you can then report at any one of those levels, and that's really useful to create those different visualizations I mentioned earlier for all the different audiences.
Now, the Logs-plugin, as I mentioned earlier, it goes into Elastic.
The configuration's really easy.
It's about five lines.
As long as you have a cluster set up, that's basically it.
And then, there's another step afterwards.
You can configure, you have to add a post-build step to each thing.
There are scripts available online to just mass add it to the Jenkins script console, so it's really easy.
If you want to just mass log every single build log of all your Jenkins jobs in a master, you can do that.
And then this is the post-build step.
You can specify how many lines.
So you don't have to do all of them.
You can do a tail.
So in this case, it's I think 4,000 lines, which is usually way more than enough.
And then this example is for all those people in this room that's encountered random compile errors in an automation system.
I don't know about you, but if you had to debug that and track that and tally that up manually that would be really hard and really no one would want to do it.
In this case we used, we shoved everything in Elasticsearch and one of our engineers created a graph.
So the top graphs denote the frequency of the errors that happen off a particular search string.
And then the bottom charts kind of give you a visualization how widespread the problem is.
So once you know how many occurrences are happening from the top charts, you can kind of focus it a little bit more.
So the inner pie is graphing how many Jenkins masters this problem is having, or it appears on.
So in this case, I think it was like five from this graph.
And then the other one, this.
Secondary layer of power chart denotes what kind of Jenkins jobs it's been seeing this failure So not only do you have the master you have the jobs that they're occurring on and the frequency And then the last one is the specific steps, I think So or no, sorry the nodes the build nodes So you can actually find out all that information just from this just by shoving your information in and then doing a search in this case the random compiler error was caused by antivirus locking files on the build machines.
So I'm sure everyone's sort of encountered that, and it's painful.
Well, in this case, you're able to use metrics and reports to actually prove that it's a problem to people that control it.
It's able to help identify which build nodes have maybe corrupt installations of antivirus that you have to go take out, or you have to go mitigate.
So it makes it all possible without the OI tool or Elasticsearch.
I don't know if anyone would be able to get this manually or at least convince someone that it is a problem.
Now, moving on to the Splunk plugin.
I get really excited about this one because I use this one a lot.
The Splunk team, I believe, actually uses Jenkins to build their product.
So they actually, their team, if I remember correctly, actually authored the plugin for Jenkins itself and as well as the app that visualizes the data from it.
So in this case, the configuration of it is very similar to Logstash, except this one happens at a global level.
So once you configure it, all your jobs send information in.
It actually has a tighter integration to Jenkins, so it's not just log information.
I'll go over that a little bit shortly.
In the case, so you install that Jenkins Splunk plugin and it shoves the information into Splunk and then you install the app to visualize that data in your Splunk instance and then you get this.
So this is a build overview of multiple Jenkins masters.
This screenshot's taken from the plugin page so if you wanna go see it, you can see it afterwards.
I can't show you our stuff, but it's basically the same thing.
But you're able to visualize jobs across multiple Jenkins masters. It would work for one.
And here's another example of a pre-built chart that they provide.
And this one is the Jenkins Health, so it's actually taking JVM information and then graphing it in Splunk as well.
So this is your fourth method outside of the other ones I mentioned.
So there's a lot of redundancy in the system, but it's cool that way.
It actually has the master log as well.
So this case is a really short example, but you can actually search your Jenkins master log without having to SSH into it.
If you have multiple Jenkins masters, well, you can search across all of them.
So it's really cool that way.
It saves a lot of time.
And then lastly, it's kind of cool, but they actually built a way to interpret unit test results as well.
So if you have standard JUnit or stuff like that, the plugin, it'll actually be able to consume the test report and graph it for you as well.
And all of this was just install the plugin, configure it, and install the Splunk app to visualize it, and it's all there.
This is zero customization from my side.
really powerful stuff.
And if you haven't noticed, the bottom left, that was kind of like, as I go along, you're kind of leveling up your reporting.
So that's a little thing I did there.
Now, once you have that data hooked up to your OI systems, I, this is where managing expectations becomes really easy, because we're able to aggregate all the job run information from your Jenkins all across.
So instead of just having your trends view, you actually can just do all your jobs at once.
Now, yellow is bad in this case.
It's an automation error classification.
I give you the legend now, so it makes a lot more sense.
Orange is bad data, so that just means the log parser wasn't able to determine a failure cause.
And then there's no script errors that appeared.
There's also very few aborted builds.
The game failures are denoted as compile errors, linking errors, stuff like that.
And then success is just the build passed.
So immediately from here, you can see the system's either super reliable or super unreliable.
So the big yellow part actually looks a lot worse than it actually is.
Because if I hover over it in Slunk, it's actually just one.
So this was a weekend for a really small project, so nothing ran.
Only one thing ran and it failed.
So it looks really bad.
But you're able to just hover over and tell right away that there's no reason to panic.
Now, on the exact same data, so this is reporting off the build failure analyzer data that is in MongoDB that we ingested into Splunk in this case.
This is the exact same legend, same color scheme, same legend.
This is really noisy.
This is actually reliability on each project, Jenkins job.
So each vertical column is a Jenkins job, but right now you're able to tell which jobs are stable, which ones aren't from the color coding, and it's really powerful for the build as an administrator of Jenkins and for my team to actually go and address problematic builds.
Now, this example, there's full drill down capabilities.
So if you hover over something, it'll give you a number of instances.
If you click into it, you actually get to draw, not the build logs, but at least the metadata that is collected by the Build Failure Analyzer plugin.
And you're able to.
determine exactly when it happened and other information as well.
So it's really powerful that way. You can of course deconstruct reconstruct the Jenkins job link, but because this system, this data is in Splunk and not in Jenkins, if your Jenkins log history retention is really low you can configure, you can have a lot more longer history in Splunk. And the fact that Splunk and Elastic both compress the information keep a lot more.
So it's really good.
Earlier you saw the graph about the overall project reliability of a bunch of automation jobs.
Well, you can break it down by stream.
So if you have a particular team, or QA, or producers, or lead engineers focus on a particular feature stream or something like that, you build a report for them.
And then they're able to consume that information.
So you can have multiple ones.
This is just, these are all screenshots of specific reports.
You can have dashboards with multiple ones, so it makes a lot more sense.
But in this case, this is an example.
This one actually has project names that you can actually see.
So that's really good for them.
Subsequently, the same set of data, I can generate pie charts to visualize which problems are the most frequent.
In this case, the big green is actually a linking, I think a dependency error, so it's not actually our fault.
You can filter it by different failure categories within our system.
So if you're a Jenkins administrator, we can look at specific failures.
If you're a game team, you can look at other ones.
It's useful that way.
Once you have successfully managed expectations of everyone and people trust the system and they really like it, they're never happy.
They want things to be faster all the time, right?
So off the same set of data and noticing a theme, we're able to generate or calculate the mean, max, and min job times.
and plot it. Now this plot is kind of ugly to look at because it's got all the jobs or a lot of jobs. So I actually have filters at the top if you can't see that, that you can filter by job names and stuff like that. So you can filter it down across different Jenkins masters. This one I limited to just one because looking after everything would be a mistake. You wouldn't be able to see anything.
Now, that helps, we also do the, I guess, we also calculate the standard deviation, which is really useful for people that like tables and numbers.
Because job times, not just speed, but people want consistency.
You know, when QA has to plan for when, when do you start a build?
When do they have to have people in to test?
Inevitably, as much automated tests you do, you're gonna still have a manual QA come verify things.
We're able to track how consistent the build time is, so if it differs two to three minutes or something like that, that way you can get a better ballpark how long or when to schedule people to come in, so it's really useful that way.
Subsequently, you can also just pull a line chart.
Now the line charts in the middle are kind of ugly because it was a weekend, there was no data.
But many types of visualizations that were built just at a fingertip from the same set of data.
All these graphs would probably take me maybe five, 10 minutes to create, and then they're shareable across all the data.
A lot less time than talking to individuals, I might add.
And now once you have that, that was on the build data, you can also incorporate other information, not only Jenkins, but your performance metrics.
So if you want to ingest anything text-based, you just shove it into either Elastic, or in this case, Splunk, and then you will plot, in this case, I believe it's the memory, the memory high watermark.
So at a CL basis on the access, you're able to tell for a very stable game as CLs go in and out and through the automated tests.
the metrics you're collecting, you're able to detect when a performance impact has happened, and people are able to address that a lot sooner than if they did it manually.
So it keeps the performance high.
I don't know if it's clear, but I mentioned Jenkins console log, so all the build output that the system...
creates can be indexed.
Now, you want to be careful of that.
I mentioned introducing log filters.
There's a lot of data.
But if you can imagine being able to search across all the Jenkins job results in one interface without having to open different tabs, there's a lot of power behind that.
and that's what really makes me giddy when I look at the system.
I'm able to use it for multiple reasons.
I mentioned it helps improve debug or helps reduce debug time for failures.
In this case, I'm gonna go over the example of the perforce errors.
So in this case, I can't really read, I'm trying to read my screenshot, but in one search of this simple search query, I specify the index where the data is and then I specify the type of data or the source type and then I just type Perforce password.
I was able to identify 104 different Perforce connection issues over three different Jenkins masters, over 15 jobs.
and a total of 43 unique job runs from that simple search.
And that kind of highlights the thing, it summarizes the kind of power that you would have in debugging something at scale if you incorporate this type of tool.
Now Splunk does cost money, but you can do this in Elastic as well.
A lot of these things I talk about, there's a lot of stuff that's Jenkins specific, but the concepts can be applied to anything really, and not just games do.
No, the second example that I'll go over is, you know, I actually use it to optimize Java server builds with doing, with knowing nothing about Java and the server itself.
So I joined, I looked...
I became involved with the team relatively recently and I had no idea how their system was.
All I knew was the server was Java-based and I knew it was Maven so I was like, okay, great, I have a start.
And I was like, I actually used Splunk to figure out things for me and help me on-board me.
So in this case, I want to optimize the server build and I'm going, well, I know Maven, I heard rumblings from other engineers that Maven was downloading packages from the public Maven repo constantly.
So like okay, so we had an artifactory cache or an artifactory server locally, so what if we just cache this stuff locally and have a local artifactory cache?
And like okay, so which repos do we need?
So since I had all the job logs, I was able to search across all of them.
I did a little thing called extracted fields in Splunk, which is able to interpret job logs and actually isolate and create fields, unique fields.
And I created one that would give me all the unique URLs that we were downloading.
And then that kind of highlighted there, that was kind of like the extracted field that I created.
It's actually called Maven repo, you know, really aptly named.
And then I was able to construct a chart, or actually not a chart, I was, yeah, this is a chart and a line chart, but I created a job, or my engineers created a job that one, built with the default settings, nothing changed.
Second one was pointing at our local auto factory cache, and we're able to track that on average by just looking at local Maven repo, we were able to save one minute.
build time for our jobs.
And that was without looking at the code, just using the job parsing.
And I thought that was really cool.
And then I think the Maven repo went down a couple times, I guess, earlier.
So that was really useful that we were able to at least isolate the system from the public Maven repo, so that kept our build stability up even higher.
Now, the other one is, once you have that data in, Splunk offers a machine learning kit, and so does Elastic as well.
I think you have to pay for the Elastic one, and Splunk you have to pay for two.
But it just shows that.
It unlocks a lot of potential and it's kind of the next steps for my team will be to look at this stuff.
I don't have any examples.
I did, I did, uh, we were starting to experiment with, um, looking at the log or the drive space.
So whenever you're syncing and the artifacts and try to predict, uh, do more predictive reporting, uh, rather than reactive reporting.
Now once you have all that, you can now probably scale your automation a lot better.
So my recommendations for that are going to be really loose and they're going to be a really high level, but this is really important and if I spoke to a number of you before, then I apologize for ranting again, but you've got to factor in the talent.
So in our industry, CI is not the most glorious thing.
The companies that we work for may not value us as much as we think they should.
So you should factor in the talent that you have and that you can retain.
I don't know how many of us are contractors, how many of you guys are full-time, but there's not a lot.
Within the A, it's definitely improving.
To do that, you need to definitely minimize the technical debt within your stack, and leveraging open source or existing enterprise tools is definitely the way to go.
It's also transferable skills.
And then you will definitely want to apply configuration as code principles, as well as infrastructure as code principles to help scale your automation responsibly.
I won't go into too much detail for those ones because I could be up here forever.
But our team, I'll go over the high-level stuff.
I mentioned we used the Buildflow plugin before.
We still use it in some of the legacy systems, but we're migrating to the Pipeline plugin.
I know that I was there for the whole ride when they...
They kind of deprecated build flow, and they hired all the people that made it into developing pipelines, so that was kind of cool and bad at the same time.
It's extensible and powerful, the fact that you can use a groovy domain-specific language to do basically anything you want.
It offers the stage view, which is very similar to our custom CI tool that we built six, seven years ago at EA.
It actually looks very similar to it, which is kind of ironic.
And then it offers optional Blue Ocean Pipeline Editor.
Now this view is kind of cool.
It is, I think there's other CI systems with views like this as well, but Jenkins realized that the way that you view results is not the best and they started trying to improve it, so that's good for them and good for us as well.
And the next, moving on, that isn't the only thing you need though, because that only solves when you actually run a job.
Well, what about job creation?
Well, we use the job DSL plugin, so I'm not sure if any of you guys are familiar with that, but it enables dynamic job creation, and that's really good.
And once again, it uses the Groovy DSL as well.
So we're able to use the job DSL to populate our Jenkins master jobs, and then those Jenkins master jobs, when they're pipeline-free install, we use that afterwards.
So it's kind of a chicken and the egg thing.
I know that there's many areas where you can do scripted or non-scripted in pipeline.
You can put your properties in both, which is kind of confusing, but it's up to you guys and how you guys want to use it.
I don't think we've found the perfect mechanism within EA yet, because there's a lot of people with a lot of opinions, but it gives you a lot of options and that's all good for what we do.
This example is taken from the Jenkins wiki.
It's an example of the Job DSL plugin for those that aren't familiar with it.
It will...
you point it at a Git repo and it will generate a unique job for every branch you have in it.
And this is an example of, this is cool.
And if you guys are using Git, you can probably use this right off the bat.
If you're using Perforce, I don't think they have something like this, or maybe they do, maybe my team does, I don't know.
I'll have to check on that one.
But this is just an example of how powerful it can be.
And it will auto-scale your system.
Now, once you have, the ability to generate a lot of jobs, make them as complex, and manage them using code, you're gonna need things to run it on, right?
So for us, we're using in-house, we're using VMware for our VM stack, and then we use Chef to help configure the software installations on it, and I'd highly recommend doing that.
The Chef is used to configure our Zabbix configurations, so it can do a lot for you, it'll save you a lot of time.
We do use JFrog Artifactory.
We're not completely adopted it yet, but we're starting to.
And that will help us with tracking our builds better than just using network shares and Jenkins itself.
I would highly recommend you not using Jenkins to store your repository, because it's just going to blow up.
And you'll regret it later, because we do.
And then next, we're exploring with Kubernetes.
We're using Kubernetes environments to build our server, most of our server stuff right now, for what can go on it, and then of course Docker containers.
But there's one thing I'd like to mention, is we are experimenting for AAA, like the ability to use.
containers for that, but you know the jury's kind of out on that in terms of if it'll give us proper returns or not because It adds a lot of complexity to the system Not sure if the performance gains or the flexibility is worth it at the moment You know in summary your automations is only as good as your reporting You know the speed and flexibility and reliability isn't the only thing The earlier you invest in your reporting stack The more you'll thank yourself later on.
You never know when you're gonna go big.
You never know when your system's gonna go wide.
So as long as you have that flexible reporting stack and scalable, it'll just help you out in the long run.
And also, it will help reduce your technical debt by doing that and leverage your open source as much as possible or enterprise tools if your company has the money to do so.
That's really, really important.
And then, of course, once you have that.
I'm sure everyone here has had to deal with engineers, you have to manage the build system.
Use it to your advantage, manage the expectations.
And then, last but not least, I think, I said everything else was important, but this one's the most important, factoring the talent.
So, you know, if you have a big team, a small team, a permanent team, use those in deciding what kind of tech stack you want.
You know, what I said is one option of doing it.
It's our constraints and what we live with.
I understand that it's not perfect for everyone.
So definitely you want to factor in the talent that you have, that you'll retain.
If you want to stay in the job industry and stuff like that when you're making your decisions, custom tools is good for certain cases, but it depends on the situation.
So all the links I had for all the plugins are on the Pastebin link here, as well as some of the technologies I've listed.
So if you didn't have time to jot that down, you can just go to that link and then there should be links for that.
And that is it.
So if you have any questions.
I guess if you have questions, please go to the mic.
Hi.
You said you were monitoring memory, and that's something we often have problems with, just tuning the Java heap, running out of memory.
Are there any kind of tips you can give on keeping memory usage down?
Don't use build flow.
If you use Buildflow or anything like that, it does class loading.
And the class loading will bump up all your memory.
It doesn't always clean it up.
I mentioned earlier we have 64 gigs of RAM on some of our masters.
Not all of them have that.
It depends on the scale, number of jobs run concurrently.
It really depends on the complexity of your jobs and how much you use it.
So there's no master solution for it, but the monitoring plugin I definitely recommend you using and you can try to get some information from that.
If you use log parsing, it's not the perfect thing.
It's a double-edged sword.
It makes it really easy by using the build failure analyzer plugin or other log parsers, but you're using the Jenkins master to do it.
I don't think I found a Jenkins plugin that will offload that to a node yet.
Now that we're shoving more information to Splunk and Elastic, we are thinking of ways of just not using log parsing altogether inside Jenkins, just using the OI tool itself to do it, because it's powerful enough.
But we're not quite there yet.
Okay, thank you.
Hey, thanks for the great talk.
That was very good, or good, I found.
Oh, thank you.
And the question is that you said the Jenkins in the CI thing is not a glorious thing to do as an engineer.
So I'm just wondering what's the biggest motivation for you?
Oh, that's a great question.
So I say it's not the glorious thing, but I think that was more for everyone else, maybe not in this room.
For us, I find passion in helping others.
I've been doing this for 11 years and I find value in doing it.
I take pride in it and I take pride in having a stable system and for the end of the day helping the engineers, enabling the focus, the game engineers to focus on the game itself.
That brings a lot of value.
It doesn't mean I can't contribute.
I contribute ideas.
I don't have to do it.
It's kind of the best of both worlds.
We also don't pull as much on our team.
We actually don't pull very crazy hours at all.
I think within EA we have one of the highest retention, like internal retention, like we don't have no turnover rate really.
It's really low.
I mean, outside of the stuff that we can't control, like contractors and stuff like that, we try our best.
The company is warming up and doing a really good job doing it.
They recognize the value.
But I mean, I think for everyone in this room, it's really valuable that we do what we do and to share with one another.
And I think going open source definitely helps each other out because we can then share things like this.
Yeah.
Thank you very much.
Hey there. Thanks for the talk. So quickly, just two questions. I guess first, for mid-sized companies, we have enough trouble just convincing our clients just to at least use CICD in any capacity. But you didn't, I may have missed it. You may have talked a little bit about sort of your deployment best practices and...
when we're working with our clients, we're already trying to convince them to, you know, use Jenkins. But what tips might you have for deploying sort of, you know, hundreds or tens of gigs of data to, you know, servers or wherever, I guess, for companies or clients that might not be comfortable deploying them to, say, Dropbox or, you know, Google Cloud or something like that?
That is a very good question.
So I'm lucky enough to not have that problem myself personally.
With NEA we have our own team called RPM, Release and Preservation Management.
There was a talk earlier about preserving stuff.
We work directly with them.
They also manage the distribution network.
So I know that we use a dedicated hardware to do that, to securely transfer data like that encrypted to, we do it to, I think, Amazon.
So that's just an example of how you can push things to the cloud using dedicated hardware.
Now, the piece of hardware is quite expensive, but when you're looking at securing things and not increasing your software side, I mean, other than secure FTP, I mean that'd be the free stuff to try to use to start with.
Yeah, to start with, we also within the company, we use WAN accelerators, Wide Area Network Accelerators hardware, so that's also expensive.
But I mean, there's hardware options for optimizing that, and those are specifically used for, you know, your co-developed studios and stuff like that, and your big games, right?
Now as far as...
Convincing people to use CI, it's hard, it really is.
There's nothing harder than convincing people that don't want to change the quickest way.
And that's why, you know, if you have time and you can invest in Elastic, if you have money constraints, definitely go down that route first.
Hook up the data, start using it, generate the reports, get buy-in with those reports, because execs, non-technical people love those.
Even technical people love that because numbers shut people up.
So if you can do that, then that's the way to go.
And then I mentioned a slide deck about setting the automation.
Set up the builds first, and then your static analysis.
Those are the easiest to do.
And that'll start getting you some returns.
The test will be the most difficult step, I think, because you need to decide on a test framework.
Every game is different.
If you're using Unity or Unreal, I think you have a.
you know, leg up, that they have test frameworks that you can leverage.
I know some parts of our team or our company do that.
So I think you just need to analyze it, but there's a lot of free options I said there, and that was kind of my inspiration of the talk was we also use free stuff.
And we're, you know, my team, we will be looking at contributing some of our stuff back to open source, but I still need to go through most of the red tape.
Okay, so yeah, so the, um, I guess leading into the second question, right?
So are, we've thankfully convinced our clients to use CI and CD.
It's just necessarily trying to convince them to put some more money into the hardware and kind of upgrading.
Uh, but I guess, uh.
thinking sort of, stepping aside from that, do you have any tips, I guess, I don't know if you're involved sort of in the hiring process at EA, but do you have any sort of tips to try and attract talent that is in CICD because we're having issues trying to find more like-minded individuals and I guess.
you know, is there a Discord channel or something out there that you might go for to find someone who's interested?
That's a great question.
I know that the automated testing roundtable is happening at the same time.
I think there's individuals like in this room here.
Our community is quite small, especially in the gaming community.
the gaming community, so this is the first time I think there's a dedicated CI talk or somewhat like that, so I'm excited that all you guys showed up, actually.
I was like, this is a big room, I hope I have some people come, especially at this time, especially this time slot.
I don't know, if you wanna set something up, I'd be more than welcome to help join, but I was, I didn't mention, I had my slide deck, I forgot a lot, but I was involved with hiring like majority of the, like 30, 40 engineers on the team.
The thing that I look at for the most on our team is the passion.
You know, technical ability is never, for our stuff, it's nice, but the passion is the most important one to do, because passionate people, they always learn.
And my team's evolved, I've evolved with my team, and our team's evolved constantly.
So I think that's the expectation to have, to have those individuals.
I started reaching out to local universities from Vancouver, where I work, and trying to do that.
I haven't had that much time, but I think.
It's a hard one because nobody, no school really teaches it.
I did have- That's kind of part of the issue.
Yeah, there's no class about it.
So hopefully by us, by me doing a talk and by us talking more as a community, we'll start garnering that attention and actually bringing that, you know, there's actually a career path in CI within not just the gaming industry, but at large.
And it's very important, so.
Cool, thank you, Adrian.
That's it.
Thank you, everyone.
