How's everybody doing? You guys just got to survive like one more round of GDC hangovers and you're home free.
I definitely am looking forward to that.
Housekeeping. You guys are going to get the emails. You know how that works.
So just pay attention for that. Please, if you've got cell phones, silence them.
If you're not using your laptop to take notes, please close it up and let's get to it.
First of all, I'd just like to say it's a huge honor to be presenting here at GDC 2017.
It's a huge honor that all of you came to see my presentation.
I know there's a lot of good stuff going on.
I know there's a lot of important meetings, so I take it as a personal honor.
I'd specifically like to thank Dave Ranyard from the GDC board for his feedback on this presentation and while I was working on it.
This presentation is about improving efficiency, eliminating waste.
To get to the meat of the presentation, I gotta give you guys a bit of a crash course in what is known as operation science.
And that means there's gonna be some math.
So if you're math-phobic like I am, don't worry about it.
Don't freak out.
There's gonna be some formulas, some math, some equations.
And we're gonna be moving a little fast to start.
So just hang in there.
At the end of the presentation, I'm gonna give you a link to download the annotated slides.
So don't worry about keeping up with every note or getting every equation exactly right.
If you can follow the basic concepts, you're good.
And then you can download the slides today and review them at your leisure.
Fair enough?
All right, let's get to it.
So I wanna start off with a story because they say to always start these presentations with stories.
And this story takes place 350,000 years ago.
When our genetic ancestors, the hominins, made a discovery.
Fire.
And over the hundreds and thousands of years, that discovery, through iteration and repetition, became something else.
It became a process.
And really all human activities fall on this sort of spectrum of, on one extreme you have things that are processed, you have things like boiling water, there's no mystery there.
You apply energy to water and eventually it will boil.
At the other end of the spectrum you have discovery, you have the Large Hadron Collider, the cutting edge of physics, the unknown, the unknowable.
But there's a few things to recognize about this spectrum.
First of all there are very few extremes.
Most activities are a mix of process and discovery.
There are very few things that are pure process and even fewer things that are pure 100% discovery.
For example, cooking.
Lots of process around cooking, boiling water, basic combinations of ingredients, but there's also a lot of room for experimentation and discovery.
Second thing to recognize about this spectrum is that all activities start towards the discovery end and move towards process over time.
So the first time you made your own pasta sauce, Probably a lot of discovery, a lot of learning.
By the thousandth time, a little more rote, a little more routine.
And the final thing to recognize about the spectrum is that our business is discovery.
We spend a lot of time on that end of the spectrum.
And to underscore that, I would like to point out that the famous phrase, find the fun, has the word find fucking in it.
And as the bard would say, there's the rub.
Because discovery by its very nature brings with it the unknown, and the unknown brings with it risk.
Or as operation scientists like to call it, variance.
And that variance is the source of so much of our pain as we try to manage projects over long term over months and years.
It makes predictions hard, it makes management hard.
So what do we do?
Do we just abandon discovery and just make the same game over and over again to make it more routine?
Of course not.
That's not fun.
But we also don't have to be at the mercy of variance if we can acknowledge two facts.
Number one, variance compounds over sequential activities.
So if every one of these activities takes between one and five days, the total time to get through all five is between five and twenty-five days.
The second thing to recognize is that nothing we do is pure discovery.
Even the most avant-garde, experimental, in-the-weeds gameplay design is still underscored by some elements of process.
The process by which we design features, by which we code them, by which we compile them into builds, by which we upload them, by which we QA them.
And to underscore that point, I would point out that the famous phrase creative process has the word process in it.
And if that weren't true, we wouldn't need all this bullshit.
So in as much as the activities we do, Our process, our known, our understandable, our, you know, understood by us, we should seek to make those processes efficient, predictable, or consistent, and whenever possible, automated.
Both so we can minimize our overall level of variance, and so we can maximize our ability to absorb variance.
And that's what I'm gonna be talking about today.
So a little roadmap for the talk.
This talk is gonna start micro and go macro.
You're the first people to finally laugh at that fucking joke.
All right, so let's start with the bio of me.
This is about as micro as it gets.
Then we're gonna move to some fundamentals of operations science and process flows.
Then we're gonna move to a little more practical example, something called the capacity chart.
And the second half of the presentation is the real meat of it, when we get into what's called lean production and how you can apply it to games.
Then we'll have some closing thoughts, and if there's time, some question and answers.
So who am I?
I started in the industry about 10 years ago as a humble production intern at Wadlow Games in Chicago, Illinois.
I worked my way up to a senior producer studio lead position on games like Guilty Party, Avengers Initiative, and helping our sister studio out, Avalanche, with Disney Infinity.
And over that time I really developed a firm belief that the human cost of making games in terms of layoffs and crunch and studio closures and uncertainty and stress-related health issues is a little more than I could bear.
And I decided the best way for me to be able to positively impact that would be to learn as much about business and management as I could.
So in addition to working full-time and raising kids, I went to night school to get my MBA from Northwestern University.
I'm currently working as a consultant, notably with my friends at Ragtag Studio, while they work to finish Raise the Dead.
I'm also trying to get my own game development company, Agency Principal, off the ground.
And I write regularly in my blog, Breaking the Wheel.
So, if you enjoy this presentation, there's a nine-part series called Game Planning with Science you should really check out.
Shameless plug.
Another thing about me, my favorite movie of all time is Aliens.
Yes, this is cliché, I know.
I know it's cliché, but it's the truth.
You send me to Desert Island, I'm taking that one movie, if that's the only movie I get.
I'm guessing games industry, I'm guessing all of you have probably seen Aliens, so you may recall towards the end, Ripley has to go into the hive to rescue Newt, the atmosphere processor is exploding, she has to take a detour, and she and Newt wind up face to face with the alien queen.
And the queen has this gross, slimy, bulbous sack attached to her that she uses to lay facehugger eggs on the floor.
Now if you're as much of a fucking nerd as I am, you may have wondered at some point, how many eggs does she have in that thing?
So let's find out.
Let's imagine that on average the queen lays 7 facehugger eggs a day on the high floor.
Let's also imagine that it takes an average of 5 days for any single egg to gestate.
The only way the queen can sustain a 7 egg per day throughput with a 5 day per egg turnaround time is if there are 7 eggs in each day of gestation.
Therefore, the queen has 5 times 7 equals 35 eggs in the sack.
Or, to put it another way, her average egg inventory, i, is equal to her average egg throughput, r, multiplied by the time to produce a single facehugger egg, t.
Or more simply, i equals rt.
In operations science, this is known as Little's Law.
It is the most fundamental equation of operations science.
And it works for all three permutations, i equals rt, r equals i over t, t equals i over r, which is a verbose way of saying if you know two of those values, you can calculate the third.
So I'm going to belabor this point for just a second, because if you can hang with Little's Law, the rest of this presentation is going to be a lot easier to follow.
So for any ongoing process, if it's assembling cars, or manufacturing cans of soup, or perhaps creating video game assets, the amount of things currently being processed, the inventory, is equal to the rate at which things come out of the process, the throughput, multiplied by the time to create a single thing, the flow time.
Now how do you go about calculating those values?
So inventory equals throughput times flow time.
You're gonna see this equation contextually throughout the presentation, so it's gonna be our companion today.
So how do you determine those values?
Let's imagine that inside the queen's egg sac is a network of activities.
All these activities are necessary to produce a single facehugger egg.
They're all required.
Now how do you make sense of this thing?
It branches, it's not all clean, it's not everything lines up.
Well, it's actually not that hard.
First of all, let's recognize that one single path through the pipeline is longer than any other.
This is known as the critical path.
I'm sure you've heard that term in different contexts.
This is the operation science definition.
It is the longest single path from input to output.
The activities are known as critical activities, and the cumulative flow time of those critical activities dictates your flow time for one batch.
You cannot get a batch done in less time than it takes to run through every critical activity.
And that flow time is denoted as T.
Let's also recognize that one single activity is longer than any other.
This is the bottleneck.
Again, I'm sure you've heard that term in various contexts.
This is the operations science definition.
It is the single longest activity irrespective of critical path.
It can be on the critical path. It doesn't have to be.
The throughput of that bottleneck dictates the throughput of your process.
In other words, you cannot get things out of the process at a rate faster than the bottleneck will allow.
And that throughput is denoted as R.
So to summarize, your critical path determines your flow time.
Your bottleneck determines your throughput.
Now let's move to perhaps a more relevant example.
Here's a hypothetical flow chart for getting a character in the game that covers anything you might want to do.
Concept art, casting, VO, mocap, everything.
Now, this looks like SpaghettiOs, right?
It looks really complicated, but operation science makes it a lot easier to figure it out.
First of all, despite all the mess, we have a very clean critical path.
It's 29 days long.
So that means our average expected time to complete a character from scratch is 29 days.
So there's our flow time.
And here is our bottleneck.
It takes nine days.
So we should expect to get a character done out of this pipeline once every nine days or one-ninth of a character per day on average.
There's our throughput.
So with very little effort, we have just estimated the average time to complete a character from scratch, the expected time before the first character is in game, and the average rate at which subsequent characters will be in the game.
Or a visual form something like this.
Now, there is a caveat, there always is.
If you add up all your critical activities like this, just in sequence to get your flow time, what you actually have is what's known as your theoretical flow time.
It's your flow time if there is zero downtime.
So the unit, if it's a character model, never has to wait for anything.
It just moves through in one continuous flow.
In reality, that's not gonna be the case.
If you wanna get your actual flow time, you need to account for that wait time.
Theoretical flow time plus wait time equals your actual flow time.
So examples, queues, handoffs, bathroom breaks, anything that would stop an asset moving through the pipeline.
Now if you can calculate, estimate your theoretical flow time and estimate your actual flow time, you can calculate something called a flow time efficiency.
This is the ratio between the two.
And what it is, is an estimate of how much time your asset's been waiting around, not being processed, not generating value.
So if you have a flow time efficiency of 78%, your asset's spent about 22% of their time waiting around for somebody to work on them.
Your flow time efficiency's always gonna be less than one.
If it's ever greater than one, your theoretical flow time is wrong.
It means you highballed it.
You think your activities take way longer than they take.
So you need to go back and recalculate those.
That being said, the closer you get to one, the more efficient your process is.
So what if you don't know the average time to complete any activity or the entire sequence?
Well, Little's Law comes to the rescue.
If you know about the average rate things come out of that sequence, and you know how many things are currently somewhere in the sequence being processed, you can back into your actual flow time.
Your flow time's just gonna be the inventory divided by the throughput.
And that would be your actual flow time, not your theoretical.
The same thing applies to individual activities or chains of activities.
Little's Law applies to any level of granularity.
So if you're trying to figure out how long an activity really takes, figure out how many units come out of it at what rate, figure out how many things are currently being processed, and you can back into your actual flow time again.
So now we've established that fundamental bit, we can move on to something a little more practical.
So you know your throughput and your flow time, you can use something called a capacity chart to plan resource assignments.
Here's a simplified version of that same pipeline.
I just cut off some of the extraneous stuff for just clarity.
Now what you do is you go into Excel and you create a very simple seven column chart.
At the end of the slides, I'll give you a link where you can download one I've made for you.
But it's very easy to make.
So this is a seven column chart in Excel.
In the first column, you list every activity that's necessary to complete a single unit.
So for this character model, we had concept art, high poly, low poly pass, rigging, a few different sets of animation, and a QA pass.
The second column you list the average time to process one single unit.
So, concept art past takes three days, high poly model takes five days, QA takes five days, etc.
In the third column you want to figure out the throughput for one individual unit.
So, according to Little's Law, if we're talking about one unit, so inventory equals one, that means throughput and flow time are just inverse of each other.
And if you didn't quite follow that, don't worry, just trust me that this column is just the inverse of the second one.
So for instance, our average throughput from concept art is about a third of a character per day.
Average throughput of facial animations is about half a character per day, etc.
Fourth column you list how many people are actually working on the activity.
So we got one concept artist, three high poly modelers, four combat animators, etc.
Fifth column, you multiply throughput times number of people.
That gives you your combined throughput across each team.
So, across all our high poly modelers, they can do about two-thirds of a character per day.
All our facial animators can do half a character per day, that sort of thing.
In the fifth, sorry, the sixth column, you calculate the process throughput.
You find the lowest throughput of any of the activities.
That's your overall process throughput.
If we think back to the Alien Queen example, it's the bottleneck, it dictates how fast we move.
And the final column you put in, you calculate what's called your utilization.
You take the process throughput and divide it by each respective individual throughput.
What this tells you is how much of that group's bandwidth this activity is going to take.
So this character modeling path is going to consume about two-thirds of our concept artist time.
It's going to consume about a fifth of our riggers time.
And it's going to consume 100% of the cinematic animators time.
They are the bottleneck. They determine what speed this flowchart moves.
So if they're moving full-time, they're going to be 100% used up by this one pipeline.
It's going to take all their time to keep them moving.
So some takeaways here.
The number of resources you have working on an activity impacts your throughput.
100% utilization means you're a bottleneck.
And the way you staff a pipeline can shift where the bottleneck is between activities.
And shifting that bottleneck will change the throughput but not the float time.
So don't fall for the one month baby trap.
You can't throw five modelers at the same high poly model and get it done in one fifth the time.
Don't fall for that.
So now what happens if we start adding some resources?
So here's our chart.
Cinematic animators are the bottleneck, and I'm going to add one cinematic animator.
Now two things have happened.
One, our throughput has gone up to a third of a character per day, so we're going a little faster.
But now we have two bottlenecks. We have two resources that are 100% utilized by this process flow.
That doesn't mean we're worse off because we have two bottlenecks when we had one, we're going faster, but it means we can't improve throughput anymore with a single head.
So if we just add one more cinematic animator...
Throughput still capped at a third of character per day.
The concept artist is still 100% utilized, he's still the bottleneck.
And now what we did is basically we gave the cinematic animators a little spare capacity.
That gave them a little wiggle room.
That's all we accomplished with that.
If we add another cinematic animator, throughput still capped at a third of character per day, concept art's still the bottleneck, and all that extra head on the payroll did was create more spare capacity, which is probably not the most efficient use of your resources.
On the other hand, if we add another concept artist, Now our throughput goes up to about two-fifths of a character per day.
Cinematic animators and concept art are no longer the bottleneck, and QA is now the bottleneck.
So you can see how you get this little, you get a little bit of a shell game going on.
When you add people, the bottleneck jumps around, you gotta chase it.
So the big takeaway here is that process flows exhibit a weakest link phenomenon.
You only move as fast as the group with the lowest throughput.
Adding resources will shift the bottleneck between activities.
So when you're using the capacity chart, it's very important you add resources one at a time, so you can catch exactly when and where the bottleneck moves.
If we added all three cinematic animators at once, we would have seen that the bottleneck had shifted to the concept artist, but we wouldn't have known exactly when, so we wouldn't have known which person we hired is actually just creating spare capacity.
And adding resources to non-bottleneck activities won't help your overall throughput.
It just creates spare capacity, so probably not the most effective use of money.
There's always going to be a bottleneck.
This is important to recognize.
Some activity or activities will always have the lowest throughput.
So your goal is not to eliminate bottlenecks.
That's impossible.
There's always going to be one.
Your goal instead is to secure a throughput that lets you meet your schedule.
So this chart made some simplifying assumptions.
It takes a very clean view of the universe.
Your view of the universe might not be as clean.
Some of these things might not be true.
If that's so, that's okay.
You just need to use some slightly different math.
I covered it in the appendix of the slide so you can review it.
It's basic division, it's not very hard, but it's solvable.
You just need to adjust your calculations a little bit.
So on the subject of spare capacity, it's important to provide some guidance from an operations science perspective.
So it's a high pressure industry.
It's very tempting to ramp everybody up to 100% utilization.
From an operations science perspective, this is actually counterproductive.
Because remember, 100% utilization means somebody has a bottleneck.
Your bottleneck is the deciding factor of your throughput.
The bottleneck determines how fast you move.
So your goal as an operations manager is to make sure the bottleneck is always moving and never waiting on anybody else.
Wrapping everybody up to 100% utilization might make you feel good as a manager because you can be a little busy, but the reality is you're just creating more bottlenecks.
And those artificial bottlenecks might not be able to support the actual bottleneck.
So in our example, if you're tying up the rigger with a bunch of busy work just to make sure he's always working and earning his pay, the cinematic animators run out of work, they turn to the rigger and say, hey, we need another model.
He says, I can't because John has me doing this other shit.
Which means you have created a scenario where the actual bottleneck is being held up by the artificial bottleneck you created by being a taskmaster.
Which means you're failing your primary goal of keeping the bottleneck moving.
By all means, take targets of opportunity, but don't keep people busy just for the sake of keeping them busy.
From an operations science perspective, the best use of spare capacity is to alleviate the bottleneck.
So if you've got people with spare capacity who can do bottleneck activities, that's the best place to apply their talents.
All right, we just covered in about 18 minutes what took me about three to four weeks of grad school.
So if your brain's in a little singy, totally understandable.
We're gonna take a little palate cleanser, we're gonna take a little break, some story time.
This is a story I've loved ever since I was 10 when I learned it.
It's the parable of Scylla and Charybdis.
So book 12 of the Odyssey, Odysseus and his men spent 10 years fighting the war of Troy.
They're tired, they're trying to make it back to Ithaca.
They spent years trying to get back.
Most of the crew has died.
They got one ship left, one crew, everybody else is dead.
And they have to go through the Strait of Messina, between modern-day Sicily and Italy.
Now, I don't know if you knew this, but at that period in history, the Strait of Messina had two awful sea serpents living there.
It's a true story.
On one side, you had Scylla, who was a six-headed serpent, and each head would pull a man out of a boat, any boat that went past.
On the other side, you had Charybdis, who create whirlpools and maelstroms, and she might not get you, but if she did, the entire boat went down and everybody died.
And the Strait is narrow enough you can't go between them.
You have to pick.
So Odysseus has to choose, definitely lose six men, or possibly lose all of them.
Now, from a managerial perspective, this is a classic crisis, it's the test of values.
Odysseus has to pick, the needs of the many over the needs of the few, go with Scylla, or shared privations, all for one, one for all, go with Charybdis.
From an investment standpoint, his options are, stick with certainty to minimize costs.
There's no risk with Scylla, you know exactly what's gonna happen.
Or you accept some risk to maximize upside.
You take some risk on Charybdis, and you try to get everybody through.
So it's a basic advancement decision.
Now in the actual story, Odysseus decided to go with Scylla.
But in perhaps a questionable management decision, he didn't tell anybody what was gonna happen.
So boats going past and you know, six heads come out and then six poor bastards look at him and go, hey, what's that?
So you know, I'll let you decide if that was the right thing to do.
All right, break's over.
So, moving beyond art.
Little's Law applies to any sequential process flow.
And that includes features, with a very obvious caveat.
And there's that discovery again.
Feature design is more variant than art asset generation, because of scope differences, bugs, uncertainty, and human error.
Now, variance is an analysis of risk.
It can be understood mathematically.
And it adds complexity to your forecast.
You can't just deal with an average expected outcome, speed times time.
You have to account for a range of potential outcomes.
And it increases your costs.
Both your expected, what you need to be able to pay and keep in reserve, and what you actually do end up paying.
Operation science is largely concerned with minimizing the presence and impact of variants.
And one of the main ways to do that is by eliminating waste.
So then the major question becomes, who has eliminated waste better than anybody else on the planet?
For my money, You need to go back to the heady days of 1948.
Japan is still recovering from the Second World War, and a little company called Toyota finds itself competing with companies from the world's newest economic superpower, Merck.
How can it possibly succeed?
Necessity is the mother of invention.
Toyota invented the Toyota production system, and they focused maniacally on eliminating waste.
Waste of inventory, defects, meetings, even movement.
If you're somebody who puts doors on cars, they don't want you to have to move too far physically to grab the door and mount it.
The net effect is that Toyota is now the world's largest car company and established a reputation for high quality.
And TPS, Toyota Production System, came to be known more generically as lean production.
Now the moral of the story here is that what I'm about to talk about is not some squishy academic concept.
Oh, that's nice, Justin.
What else did you learn in grad school?
No, it's just that Toyota made a lot of money.
and establish a reputation for high quality, while communing with companies that have far greater access to capital and resources.
Now, lean is a Scylla approach to management.
You are spending some time now in order to possibly avoid losing a lot of time to waste later.
It's like you're going with the six steps to avoid possibly losing everybody.
You're trying to minimize your outcome variance and maximize control.
Like I said, there's no risk with Squila. You know exactly what's going to happen.
Six people, no more, no less. You know exactly what's going on there.
So when you're thinking about lean, don't just focus on what it costs.
Don't just think about the six men. Think about what it saves.
The other people who get to go home.
So let's talk about how an ounce of prevention is worth a pound of cure.
There are a lot of elements to lean, but there are five I want to cover in particular today.
Boca y que? Kanban.
Jiroka, Muda, and Haijunka.
So first up, Poka Yoke.
It literally translates as mistake proofing.
Originally it was Baka Yoke, or idiot proofing, and Toyota, I guess, decided to tone that shit down a little bit.
It means designing products so they can only be used the correct way.
So your HDMI cable will only go into your PS4 in the proper orientation.
Your ski boots will only clamp into skis with the toes facing forwards.
In a production context, it means designing parts so they can only be assembled in the correct way, so eliminate sources of human error during assembly.
Now, what we do is clearly more complicated than assembling cars to spec, but that doesn't mean you can't embrace the philosophy of polka yoke.
And the best way I've seen in my professional experience is with the humble user story from Scrum.
User stories are a form of polka yoke.
They are an attempt to establish intent.
Who wants something?
What do they want?
And why do they want it?
Now again, in my professional experience, a good user story has three key elements.
First of all, the story itself.
As some sort of user, I perform some sort of activity to engage some sort of desired outcome.
As a player, I jump so that I can traverse an environment.
As an animator, I have animation blending tools so I can provide a smooth combat experience.
As an engineer, I have continuous integration so I can have an efficient build process.
The second element are acceptance criteria.
You want to get, if you're the person requesting a feature, you want to give people clear goals as to what they're actually expected to deliver.
Give them a clear target.
And finally, technical requirements.
This feature needs to interact with these classes, deal with these objects, it needs to occupy no more than this memory footprint, it needs to accept this input and provide this output.
Try to think ahead to what a feature's gonna need to be technically sound.
Yes, this takes time.
But again, don't just ask what it costs.
Think about what it might save.
If you're routinely experiencing mis-executed features, mis-acceptance criteria, a little poke at your game might be exactly what you need.
Additionally, if you have a creative lead who has sort of impulse, synaptically driven design decisions who creates a lot of churn and feature creep, Adding a little bit of administrative cost can curtail that. It's the same thing when they say if you go to a website and one click will drop your visitor throughput by about 50%. The same kind of thing. Just adding a little bit of administrative cost can kind of curtail that feature churn. Because somebody has to ask, is it worth me writing this thing to actually request this feature that I just thought of while I was taking a whiz.
Next up is Kanban.
You would experience Agile and Scrub, you might have encountered Kanban.
Literally translates as card or sign, I've also heard billboard.
It's what's known as a pull-based production system.
What that means is, if I'm the downstream guy, if I'm the guy who puts doors on the car, when I'm out of doors, I put a card in the cart, I send it to the person who assembles the doors, he assembles a bunch of doors, puts them back in the cart, sends them back my way.
And that card corresponds to a certain number of parts.
I can't just say, give me seven doors, give me three doors.
We can only communicate numbers of requests via cards.
The team members, it means I pull work and I'm ready for it.
It's a pull-based system.
Instead of a push-based system, like a can of soup that's constantly going down the line, you're not constantly pushing work down towards me.
I'm not getting car doors piled all around me that I have to rush and keep up.
And the amount of inventory in the system is entirely controlled by how many cards is in circulation.
We don't request things other than with cards.
Over time, managers remove cards from the system in order to maintain the absolute level of inventory that's currently being processed.
Why is that helpful?
Two reasons.
Let's think back to Little's Law.
Holding throughput constant, assuming they can get cars out of the line at the same rate, if they have less inventory in the system, that means their overall flow time is faster, because the parts don't spend as much time queuing up waiting for the next process to start.
Which means your actual flow time is getting closer to your theoretical flow time, which means your efficiency is much higher.
The second reason is that excess inventory masks production issues.
It's like a protective layer of fat.
There's always stuff going around.
There's always something to work on, so it's hard to see where issues are cropping up and inefficiencies exist.
But if you pull excess inventory out, those efficiencies become far easier to spot because you look around for the person who doesn't have anything to do because they're waiting on somebody else.
That's the first place you want to look to figure out where the production imbalance is.
Now, we don't have cars to yank, typically, in game development, but we do have work in process limits.
Handsoft, Jira, a lot of those tools will allow you to set limits on Kanban flows to how much work anybody can have assigned.
So if you wanna take the Kanban approach, start loose and then dial in the restraints over time.
Next up is Jiroka.
This translates to autonomation or more literally automation with a human touch.
So at Toyota they have sensors that are on the line that automatically detect production issues and defects and will alert the workers of them.
And if necessary it will stop the entire production process to get them resolved.
The closest analog I've seen are automated testing scripts.
You have testing scripts that are written up.
You just run them and the script itself checks for any known issues, issues of experience, other predictable issues, and tries to iron them out so you don't have to consume a lot of developer and QA time finding them.
Other good examples I've seen, bots that play the game automatically.
So when WideLoad was making Avengers Initiative, we had a robot we called AutoHulk, who just played the game for us all day.
Beat up robots 24-7, and he found so many issues for us that we didn't have to find through manual regression.
Another good example is automated crash reports, game crashes, and somebody gets an email that includes any valuable data, how much, what was on the top of the stack, screenshot of what was going on at the time, anything that can help you diagnose an issue that'll save you a lot of time in investigations. And continuous integration is another great idea of streamlining processes through automation.
Now, yes, these things take time to develop and establish, but they save far more time over the length of the project.
And the earlier you get them in place, the more time they're going to be able to save you.
Next up, muda.
This literally translates as waste.
And Toyota classifies waste into seven categories.
Defects, overproduction, inventories, extra processing, motion, transportation, and waiting.
Now, defects tend to be where we experience most of our pain.
As far as waste, I'd put meetings up there pretty high, but we'll stick with defects.
They come down to generally misacceptance criteria or bugs.
Now experiencing zero defects is not a practicable goal.
We are gonna make mistakes, we are human.
So instead our goal should be to find and fix defects when it is least expensive, which is as soon as possible.
Ideally, oops, sorry.
Let out myself.
Ideally before those bugs even make it into your repository, before they make it into GitHub or Perforce or Subversion, barring that before that contaminated code gets merged or at the very least before other code gets built on top of it.
I saw a great presentation by a Bioware QA manager named Barbara Klenic on Wednesday, and she said something very similar to this.
So, if it's good enough for Bioware's QA, should be good enough for anybody's.
So a lean approach to QA would have a few steps.
The first step would be buddy testing.
You bring somebody else over and have them check your codes locally on your machine before you request it gets merged in the code base so you can catch missed acceptance criteria or other sources of error.
The second step will be automated tests.
You run those Jitaka-style tests of changes so you can find technical defects before you actually request that the code gets merged into the code base.
So you can find those technical defects without consuming a lot of dev hours.
Third step will be peer review.
Have other people review your changes in the repository before the merge actually gets approved.
So you can catch missed tech requirements, other sources of error before they can contaminate a build.
And the final step would be actual manual QA and regression.
You have dedicated QA members test the changes once they're in the build, but before they go off to leads for review.
So you can catch defects before other code gets built on top of it.
And you avoid that house of cards effect.
We've all seen it where you have a defect in the code base and you build other things around it that depend on that defect being broken in just the way it's broken.
And as soon as you fix it, you've got to fix everything else.
You want to avoid that.
You also want to avoid wasting lead and director time.
Your leads, their time tends to be the most valuable because it's the most in demand.
So a lean process, it behooves you to not send them anything unless you can have the highest possible level of confidence that you're not giving them defects.
Now, is this onerous?
Yeah, maybe.
But I've seen it in action.
And I've seen people use a very disciplined approach to QA and they have remarkably clean code bases.
They have remarkably stable code bases.
I've seen them go from six month projects, two weeks of final QA, and then it's live with a lot of stability and no crazy crashes when it launched.
So there is an upside to having discipline in your QA process.
Now some of you might think a four-step process when in the context of something called Lean and say a Lean approach.
This doesn't sound Lean to me.
That's four steps.
Because when you hear the word Lean, you think of something like that.
It's slender, not a lot of skin on them.
But when I hear the word Lean, I think of this guy.
Muscular, technically disciplined, and oh so handsome.
So when you think lean, think Jean-Claude Van Damme.
All right, hijunka.
This means leveling, literally.
Now a major misconception about operations is that batching is a best practice because it avoids switching costs.
The problem with batching is that it leaves a lot of in progress inventory, and thus increases your flow time, and thus decreases your efficiency.
So for example, you got two products, red and blue.
You're trying to minimize switching costs.
So you build up a bunch of reds and then you process them all at once.
Meanwhile, you build up a bunch of blues and you process them all at once.
You're trying to minimize how many switching costs you get.
The problem is then you have this perpetual level of inventory that's always in your system.
Inventory in the system is a liability.
It represents money that's tied up in a production process that can't develop any sort of return on investment until it's been completed.
And the longer it lingers in the system, the longer that flow time, the worse that opportunity cost is.
So to give you an example, let's say your company's cost of capital is 10%.
So for the non-finance nerds, that means that your company can on average expect to make about a 10% return on every dollar it invests yearly.
Because of inefficiencies, you have $10 million worth of bloat in your system that don't need to be there but are because you're just inefficient.
That means there's an annual opportunity cost of a million dollars that your company is absorbing.
If you could liberate that ten million, you could make a million dollars a year on it.
But you can't, because you're inefficient.
Now, the other good thing about small batches is they make it easier to find and fix recurring defects.
It's way better to fix something if it impacts five units than if it impacts a hundred.
Now, we typically don't deal in physical...
I'm sorry, the goal of operation science is not to avoid switching costs, it's to minimize them.
Ideally you want to switch in costs so low that you can cost effectively have batch sizes of one, which is what Toyota got to. They have car batch sizes of one car.
So an ideal inventory buildup diagram would look like this.
Absolute minimum level of inventory goes in and it gets out right away.
Now we don't typically deal in physical inventory, but there is an analog.
If you change your definition of a feature being complete from in the code base and functional to Q8 and ready to ship.
Because every incomplete feature represents a liability of its own, much like incomplete products.
It's dev hours you've sunk into your game that you can't actually sell yet unless it's defect-free.
So you can't capitalize on that work until you've run it through a QA process.
So our typical kind of prototypical, especially with boxed copy AAA games, build-up diagram is you go through production, you build up a bunch of features over time, and you accrue defects at some multiple of that.
Then you get to your alpha beta cert phase, you start closing defects, which lets you close down stories and everybody lives in the office and nobody sleeps because they're dealing with that house of cards and the build's constantly exploding and finally you hit a wall and you go, oh, ship it.
Yeah, we'll do a day one patch.
We've been there, we've all been there.
You want to ensure that features are defect free when they're added to the code base.
Move from the kind of prototypical build, build, build, build, build, build, build, build, build, fix, production alpha beta cert, to a build, fix, build, fix, build, fix, build, fix, build, fix, build, fix, cadence.
Right now, that sort of classical, all QA at the end model, where you're essentially batching an entire game's worth of features, which from an operations science perspective is insanity.
You want to make QA patches as small as possible, not the whole game, smaller.
Ideally, QA paths for every submission, and then you want to focus on making the cost per QA batch as low as possible, both in terms of money and time.
Now, some of you might balk at the notion of slowing down development to allow for a parallel QA cycle.
Fair enough.
Except I'm not actually advocating that you slow down.
I'm advocating that you consolidate the work.
Instead of doing 80% of work on a feature, the development, now, and the 20% of the QA in the future when you get to Alphabet Assert, I'm saying doing that 100% of the work in one sequence.
Defrag your production process the same way you would defrag a hard drive.
Now here's the way I like to think about this.
In business, you have what's called the time value of money.
It's one of the most fundamental concepts of business.
A dollar today is more valuable than that same dollar in the future.
For two reasons.
Risk and opportunity cost.
Risk because you don't know if you're going to get that dollar in the future.
And opportunity cost because you could have done something with it in the meantime.
This is why you pay interest.
You're compensating your bank or your credit card company for the risk and opportunity cost they're absorbing by providing you with money.
I think we have a direct analog in software development that I like to call the time value of fixes.
I think a fix today is more valuable than that same fix in the future for the same two reasons.
Risk, because it might not be the same fix in the future.
It might be a much larger fix in the future because it deals with other systems.
And opportunity cost, because if it is a larger fix in the future and requires more time to resolve, that's time you could have spent somewhere else making your game better.
And I'm not alone in thinking in terms of financial analogs for software development because we have the existing term of technical debt.
If you make this shitty, hacky, quick code to get something in the build fast, and then you leave it in your code base, you're essentially running up a debt on your production credit card.
And the longer that debt lies on your credit card, the bigger the payout's gonna be to finally clear it up because you're gonna have a larger and larger refactor the longer you let it linger.
So a diagram of lean production in its entirety.
When you're doing feature specs, you want to use PokéOK.
You want to carefully define specs to eliminate sources of human error before they can happen.
In development, you want to utilize Kanban.
Limit work in progress to increase efficiency and make it easier to spot production issues.
When you're actually doing testing, you want to do a Jidoka.
You want to do it with automated tests and Mura.
You want to focus on eliminating waste through disciplined QA processes.
And finally, when you're compiling, you want to observe hijinuka.
You want to minimize the level of unfinished features and avoid batching.
You want to keep continuous integration going and keep applying QA to make sure that things are ready to ship.
Now, to get started with Lean, start small.
Build up over time.
Don't be the person who shows up at the gym on New Year's Day and says, hey, I'm going to stop smoking, I'm going to cut down, I'm going to stop drinking sodas entirely, I'm going to eat nothing but raw vegan food.
Find one pain point, your biggest pain point.
Try absolving that and then move to the next one.
Gather objective data.
Don't just say like, oh, things are better.
Try to find real metrics.
I'm sure you track metrics.
Find them, track them objectively.
Apply the scientific method.
Observation, hypothesis, test, result, conclusion, repeat.
Rinse and repeat.
And then anticipate loss aversion.
So natural human psychology is that when there's a change, we focus on what it costs us before we think about what it gains.
Losses always loom larger than gains for us.
So in my experience as a manager, driving any change, I always run head first into loss aversion.
Now, loss aversion is kind of like an oil fire.
An oil fire, to get it to go out, you set off a bigger explosion next to it to choke it out.
Loss aversion is kind of the same way.
If somebody's experiencing loss aversion, the way to get them over that is to put a bigger source of loss aversion next to them.
You want to express the cost of inaction.
So yes, this change will cost you, but if we don't put this change, it's gonna cost you a lot more.
Little management psychology trick.
All right, some closing thoughts.
We don't work in factories.
Games are not widgets.
Studios are not factories.
Discovery, uncertainty, variance, experimentation, they come with the territory, and that's fine.
That's natural.
But we also don't work in performance art.
We have obligations to be responsible with the money that's provided to us in the form of backers, investors, publishers.
And we also need to take account for the fact that we have our team's livelihoods in our hands.
We have their careers in our hands.
We have their families' well-being in our hands.
And we need to be responsible about that and acknowledge that.
So I'm not proposing that we eliminate or reduce discovery.
It's clearly vital to what we do.
But I think we need to take every opportunity to eliminate waste.
Not to curtail experimentation, but to facilitate more of it and to make it more productive.
Finally.
Nothing I've talked about today is really all that complicated.
It was a lot of material, so I know it's a lot to go through, but any one instance of it is actually not that complicated.
It's really not.
So getting started on any of this stuff is not that hard.
The hard part is maintaining discipline in their use and not cutting corners when it's expedient.
And discipline is absolutely key to seeking results.
So if you take away nothing else from this presentation, I want you to take this one last thought.
which is that discipline equals freedom.
This is a saying of a man named Jocko Willink.
If any of you are fans of the Tim Ferriss Show or the Joe Rogan Podcast, you may have encountered this character.
He's a consultant and an author.
More interestingly, he's the next Navy SEAL and a black belt in jujitsu.
So I want you to imagine in your head what a guy who'd say some shit like this and have this kind of resume, like what he looks like, imagine him, probably looks like that.
I got that's his LinkedIn photo. That's where I got it. That's how severe this motherfucker is. He can't even smile. He can just frown less.
Right Discipline equals freedom is his motto If you have the discipline to eat well and get regular exercise like this gentleman right here You will have the freedom afforded to you by good health to enjoy your life If you are disciplined about studying throughout the length of the quarter like I did in grad school You have the freedom to sleep the night before the final because you won't have to cram And if you have the discipline to follow processes that reduce and eliminate waste, you will have the freedom to spend your time generating more value for gamers instead of fighting fires.
And with that, I will turn it over for questions.
But first, here's my contact info.
If you enjoyed the presentation, a lot more material on the blog.
If you scan that QR code, that'll give you my personal contact info.
There's also, I stacked a bunch of business cards right by the camera back there, and also Post-it Notes for my blog.
Just, you know, little tchotchkes.
So please avail yourself of that.
And I'll take questions. Please.
Hey, I actually really appreciated this talk. I do have a question. So in essence, this applies for process very well.
So if it's a well understood problem, you can produce and manufacture the game.
What about all that time leading up to the point where you actually not...
The requirements aren't clear. So in essence, you want to minimize the amount of waste and shit you have to throw out.
Right.
And you're going to end up making the wrong things throughout.
How do you have a good...
It's a research and experimental development process beforehand.
Sure. So there are two broad categories of activities.
Value-adding, things that make the product, whatever it is in our case, games, better for the end user.
And non-value-adding, things that don't, like meetings, rebooting PCs.
So even within your prototyping phase, you're going to have some recurring events that you're going to deal with all the time.
For anything that's recurring that you deal with multiple times in a routine fashion, can it be automated, eliminated, outsourced, or streamlined?
Yes.
And you can't contain the chaos of the prototype, but you can find those other things to streamline and you can start building in...
Start establishing things like unit tests are a lot easier to do if you establish them from the beginning and build them up over time.
So, try to make those investments you can reliably make now to facilitate that experimentation.
And then also, when you're doing prototype code, it's fine if you want to make the hacky code to get to testing features faster.
Just make sure you don't put it in the real database, put it in like the shitty hacky prototype database.
I totally understand.
Does that answer your question?
Personally.
Because one of the other things like, so are you familiar with the Bethesda level design process?
I am actually not.
And one of the main reasons why I think this seems to work is in essence that you are starting off with content that is cheap to make and cheap to throw away.
And as you get further and further and further down the line you understand what the hell you're making and what actually the final game is going to be far better.
And so in essence you should continue working on the things that you're actually going to have in your game.
doesn't quite align with this flow process. Where in essence you're talking about getting as many characters out as efficiently as possible in a final way. And so I mean I understand the whole you want to QA stuff and make sure it works, but it's also useful to be able to figure out what actually you're going to have to QA. Yeah, that's totally fair.
I don't want to make too many comments on that because all my understanding is what you just told me.
So one of my favorite quotes, I think it was Emerson, said, a foolish consistency is a hobgoblin of little minds.
So no one process, this takes kind of a clearer view of the universe.
And Bethesda's view of the universe, the reality, might be deemed for that.
That's totally fine.
And if that is the process that's most effective, do it.
I understand.
Does this cover all situations? No. Does it cover all cultures, all value systems? No.
My main thread is to say, here are the tools that exist, and here's a way you can think of the universe in terms of eliminating waste.
adapt it to your culture.
For me, it's the same thing with scrum.
Pure scrum is like pure socialism.
It sounds like a great idea until you actually try it.
You've got to adapt scrum to your culture.
So to me, it's the same thing.
Really what is most important is finding ways to eliminate waste so you can facilitate that discovery.
That discovery is totally valid.
Yeah.
But even a company as great as Bethesda, I'm sure has pain points that they can resolve.
So where can you make investments to resolve that, to facilitate even more of that?
Is that fair?
No, it's totally fair.
I was just trying to understand how do you...
How do you switch over as you go from discovery into production?
Sure.
I mean that one...
How do you switch over?
I mean, if you know what you're trying to make, it's a lot easier to say, okay, now we know what the characters are, we know what it takes to make them, now we can run through a process.
I mean, that's where having clearly delineated pre-production, prototyping, into production and post-production kind of plays in.
To me, it's like good production is each phase should have its intention.
And for me, production is the intention is, okay, we've proven the vertical slice, we've proven what the thrust is, we've proven how to build these things and what they should look like, so now how can we make them in a predictable, as predictable a way as possible.
So.
Be predictable assumes that you have that information which is what the process you're talking about would require.
And like the way I think is, one thing that I think is very valuable is to also have minimized cost of change.
Mm-hmm, totally.
And so in essence, yeah, anyways.
That's switching, minimized switching costs, totally agree.
Yeah, yeah, absolutely.
Thank you.
Any other questions? Oh, hello.
Hi, Justin. Thanks so much. Great talk.
I'm curious if you've seen the work...
You've seen Lean and Kanban work for the artist workflow in a team that's doing more batched, like Scrum?
So any hybrid, any advice for hybrid modeling?
I've never found Scrum works well for artists, because the flowchart, the critical path, never lines up cleanly with Scrum break.
So when I was doing Scrum, I was doing Scrum for the feature design, the developers, and the engineers, and then Kanban for the artists in parallel, because it just.
My problem with Scrum is I've gotten more and more experience is that Scrum kind of artificially makes these batches.
And so I've kind of moved away from Scrum into pure Kanban because I want as small batches as possible.
So I find that works better for artists, but also in my experience, I think it works better for engineers too.
OK.
Thank you.
Thank you.
Please.
Hey, so for example, when you're talking about like a sequence of work that has to happen, where they're dependent upon one another, use the art pipeline because then you could really know, you know, okay, the, you know, modeling takes this amount of time and say texturing takes this amount of time.
How do you break it up for like programming tasks which can have high variance?
You know, we tend to just group all engineering tasks all together.
Some can take a short time, long.
How do you go about, you know, breaking those up?
Or do you just say all generic engineering tasks takes, you know, on X based on, you know, your model?
So what I, I don't like to try to anticipate the time it takes to code something.
Because that just, it's too unpredictable.
What I like to do is focus on the time it takes to do all the supporting things that go to the coding.
Like, you know, writing the feature request properly so you make your life as easy as possible.
Make the QA process as understandable as possible.
Make sure that, like, continuous build integration as fast as possible.
You have a reliable source database.
Try to lay all those cards out to make sure your life is as easy as it can be.
What helps for actually dealing with code, the code development itself is A, I think it's better to have smaller, as small a feature as you can.
Like just get them on the smallest possible increments so you're not like bogged down with one thing for a while.
That's my experience.
And then the other one is.
I'm less concerned with how long a feature takes, which is hard to track and not so useful.
That's a measurement of precision. I'm not so concerned with precision, I'm concerned with accuracy.
So I'm much more interested in how much, not less in how long something takes, but more how much we can get done in a certain time.
So like story points.
I don't really care if a feature takes you 8 hours or 12 hours.
It's too hard to predict.
What I am interested in is how many points do you clear up per week.
So instead of time per issue, I'm worried about issue per time.
Does that make sense?
Yep, totally makes sense.
That's where the variance comes in.
Like, there's only so much we can control the variance of what you experience.
So I want to eliminate the variance around that so that you have the easiest time possible.
That's generally my approach to working with engineers, especially.
Great, thank you.
Cool.
Anybody else?
Any questions?
Go once, go twice.
All right, like I said, cards in the back.
You have help yourself to post notes.
Please reach out to me.
I love talking shop anytime.
And thank you very much.
You guys have been great.
