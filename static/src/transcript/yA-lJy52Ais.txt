So I'm Magnus Nordin.
I'm technical director at Seed.
And Seed is an R&D department of electronic arts.
And we try to look a bit further than the typical game team can.
They usually have a horizon of one, two, or three years.
We can look three to five years, even.
And we do a lot of game AI, deep learning.
We also do rendering, graphics, virtual humans, avatars.
And we also have a group that builds prototypes and new game experiences.
And I will talk about deep learning beyond the hype today.
And first, I'll show you a few use cases of deep learning.
And then some possibilities.
This will be the sci-fi part.
But.
what we probably can do in the future that we can't do today.
Then I'll deep dive into deep reinforcement learning and look at some results.
Because without results, it would still only be hype.
And then we'll talk about some of the difficulties of doing this deep reinforcement learning and why it's worth doing it anyway.
And yeah, one sign of the hype, I guess, is that I'm required to have this disclaimer now.
It's for the first time in the presentation.
but it says that we are doing, it's a safe AI.
Okay, so neural networks.
Deep learning is neural networks, deep neural networks.
It's just a function estimator, really.
It can take any input and produce almost any other output.
So in this case, it's 10 million pixels, and the output is the label cat.
And the nice thing, this of course is a very complex function.
The nice thing with this is that we don't have to define this function ourselves.
It's trainable.
So we train the function instead.
And that's why it's called learning.
We can get more elaborate computer vision.
We get the description of a picture instead.
And then we use one computer vision network and another language generation network.
We can recognize voice.
We can generate voice.
We can create music.
We can generate images from descriptions, in this case.
So the text is the input.
The image is the output.
And we can play games, which is, of course, the most important part for us.
So let's look at some use cases.
The first one is, like I said, playing games.
We can play board games like AlphaGo did.
And I'll come back to this.
I guess most people have heard about this one.
I'll talk a bit more later about this.
But then we have this when OpenAI's Dota agent challenged one of the best players in the world.
It's just one versus one Dota.
So it's a limited version of Dota, but it was still very impressive that they can do this.
Because no human has been able to program a bot without cheating, just using the same information as humans.
And beating a human professional.
We can also do pose estimation.
And why is this important?
We do this today in big capture studios, motion capture studios.
But this technology will soon provide anyone with a mobile phone at home to do motion capture.
Of course, the precision is not as high as a professional motion capture studio yet.
But I'm sure we'll get that.
And voice, most people know that the voice generation standard, especially the latest thing coming out of Google and Baidu and others, is amazingly good.
So I won't actually show you any voice generation samples because you already know that.
But I'll show you another interesting thing.
We are encouraged by the news.
We are encouraged by the news.
So what this was, the first was the input to a network.
The second sentence was the output of the network.
So it converted from one voice into another voice.
And this is, of course, very interesting for us.
We record a lot of voice in some of our games.
And if you want to change one line of that voice recording, which can be hours or hundreds of hours in some cases, of voice, you have to bring the actor back.
That might not be possible.
So what if we could have someone else speak and generate that actor's voice instead?
And of course, if you have your Lord of the Rings MMO, you don't want Galadriel to sound like 12-year-old Bert.
It can actually sound like Galadriel with this technique.
So let's hear two more examples.
Who was the mystery MP?
Who was the mystery MP?
It was a breathtaking moment.
It was a breathtaking moment.
This, by the way, is work by Google DeepMind.
And I will mention Google DeepMind a lot, because they, OpenAI and NVIDIA and a few others, have a very good research team that create a lot of new, cool, deep learning stuff.
We can also make neural networks sing.
So the input to this neural network is the lyrics and the musical notes.
And this is the output.
Let's see.
Like almost always, when something dies.
Nace la nostalgia, buscando un coraz√≥n.
More things we can do with voice is animating faces from voice.
This work is from NVIDIA Research and to the left here we see the fully motion captured face made in a capture studio.
To the right we have a neural network animating the face using the voice as the only input to animate the face.
Are those Eurasian footwear cowboy chaps or jolly, earth-moving headgear?
This is my reality.
And this is the reality of my people.
NBC GLAAD.
Why?
Fox TV jerks, quiz PM.
So it's not perfect, but it's very good.
It's better than most other voice-to-face solutions I've seen.
So we also have content generation.
procedural content generation.
This is an example from Anastasia in the CT.
And everything in this picture is procedurally generated so you can, you have parameters to change everything to whatever variations you want of this scene.
And of course, we can also use the same for, and maybe that's more common, for natural things like trees.
So in this case, Anastasia used biological-based rules to generate trees.
like they would actually look in nature.
And as you see, we can get a lot of variance and it actually looks like natural trees.
But of course, this requires a lot of work.
It's hard to come up with these rules and you also have to tweak the parameters to actually get the result you want.
So what if we could train something to create these rules and parameters?
So let's look at one of the best examples I've seen, and this is also from the same group that did the voice-to-face animation.
And none of these people are real.
They are created by a neural network that has learned the rules for faces by looking at 30,000 people, 30,000 celebrities, which you can probably tell.
But none of these images is in that set, the learning set.
They are all new people.
And the thing is, I said, procedural content, it's parameterized rules, so you can change the parameters.
And in this case, they glide through, they interpolate through parameter sets of the faces.
So this is pretty cool.
And of course, this is 2D generation.
And it's super impressive, but we would like 3D generation.
And 3D generation is not really there yet, but there's lots of research going on.
And I think within a couple of years, we will have useful 3D generation using generative networks.
But when you use procedural content generation to generate worlds, they are often quite lifeless, so you need to generate life in them.
So emergent behavior and life is also something we can generate with machine learning.
This is also from DeepMind.
The only goal this agent has is to move forward.
It can control the muscles in the little body.
And it has to solve the problem to just move forward, keep moving forward.
And it gets some hard.
Here's a harder body to control, and it's also harder.
And yeah, sometimes it fails.
And we actually had a guy at DICE do a more spiced up version of this, so we can take a look at that instead.
So, if anyone out there is working with sound, you know how important you are.
Sound is super important.
Possibilities, so let's look, we have seen now a lot of individual features that can be used, but what if we combine a few of these features to look at something that's pure sci-fi today, but I'm sure is possible within a couple of years.
So most games today are very violent, and why is that?
That's because violence is the simplest interaction to create in a game, actually.
It's much harder to do social interaction.
And that's a pity, because when we ask, or I asked a few people what are their best gaming experiences ever, a surprising amount of people answered something like this.
When a woman walks up to you and says, that power you hold, that's strange and ancient.
What are you?
I'm a witch hunter.
Oh!
Oh!
Oh!
Oh!
Oh!
You hear a voice.
It's the voice of the young woman following you.
An offering of outside of love is great.
Oh, I knew it!
I knew it!
It cleans the palate.
You see her flesh extend as her arms grow.
Yeah, so have you ever seen Vin Diesel this happy?
So old-fashioned pen and paper role-playing.
It's of course the social interaction you get around the table that we really can't do in games today.
But it requires a lot of imagination.
They have to visualize everything in front of them.
So that's something we should be able to help.
Some people have tried to help with live role playing, but it's definitely not for everyone.
So we in the game industry came up with this, massive multiplayer online role playing games.
However, they ended up like this, most of them.
So this is a more than social interaction is probably you can of course do social interaction in World of Warcraft, but it's mostly a highly complex exercise in coordination and intricate game mechanics.
So not much role playing there.
So we have this concept called true role playing because we think it's important board gaming is more in is more popular than ever.
More than a board games have never been more popular.
And I think one of the reasons is because the social interaction you can have around the table.
So this is an experiment we did in our lab a while back, and it's a skinning experiment in VR.
So you can control a character.
It's a Battlefield 4 character.
And we can change characters.
And this is just a prototype, but I think there's lots of animation technology going on with deep learning as well that will enable this to become really, really good soon.
And if we take this skinning and we add the voice conversion I showed you, then you can look and sound like a character of your choice.
And of course, we need the face animation as well to actually animate the skin to say what you say.
So I'll show you two scenes that we absolutely can't do today.
Imagine that this is you and your friends in virtual reality doing some role playing.
Here we go.
If you're going to play with the big dogs.
No fair.
I'm in.
That's you, Axel.
I'm in.
Oh, Hilo.
When are you going to learn?
I just want to say, it's been a while since we opened the books, and in regards to you guys, Bert, Jerry, as a man of few words, I...
Not few enough, though, huh?
No blood.
No blood.
Sallow.
No blood.
No blood.
Sallow.
So for being TV, scenes in a TV show, there's nothing special with these scenes at all.
But if we even try to imagine doing this in a game, a multiplayer setting with many players, it's impossible to do today.
But I think combining those techniques I mentioned, it will be hard, and there's a few years left before this is realizable, but I think it can be done.
And then it will open a whole new genre of games, like more amateur theater and true role-playing.
Okay, let's start looking at game AI and reinforcement learning.
So a short introduction to reinforcement learning.
We have an agent, that's the intelligent thing, and it's acting in an environment.
From the environment, it gets observations and rewards back.
So when it does something good, it gets rewards.
And it can also observe what happens when it acts.
Usually it has a goal as well.
It's typically to just optimize the rewards to get the high rewards possible.
So of course, reinforcement learning is an old technique.
But the new thing is that we combine this with the neural network and get the deep reinforcement learning.
And this is learning by doing.
It's the same way that both animals and humans learn.
So let's look at the simplest possible example of reinforcement learning.
This is a very simple game.
The blue dot here is our hero, and he or it is supposed to eat the green dots and avoid the red dots.
So first, we just drop him into the world.
And it's not going well.
The score is negative, and it's going down.
Even though he hits, I have a hard time saying he or she about this, it.
It hits the red dots, green dots sometimes.
It's also hitting red dots.
But just after a few minutes of reinforcement learning, this is what it looks like.
So it's not trying to hide in the corner and dash out to to eat some greens now and then it's it's far from optimal, but at least the score is now positive going up and after a couple hours of training it looks like this.
So now it's definitely playing this game super human will try to play it there's no way to play it with this efficiency.
So that's the simplest possible reinforcement learning game I could come up with.
And of course, my interest in this started when I saw this, that this is now more than 5 years ago, when DeepMind started playing Atari games just from the Pixel.
They just looked at the Pixel and they got the score and then they learned to play 57 different Atari games.
They didn't play all of them equally well, but most of them they played, at least nowadays, most of them are definitely played better than a human.
And this is amazing, but our games are a lot more complicated.
A lot has happened in 40 years.
So how do we go about to actually play in AAA games?
Or any game, I mean any modern 3D game at least, is more complicated.
So here's an early example of things that can go wrong when we tried to do this over a year back in.
So this is very simple.
The little guys here running on the road, they're supposed to capture the road between the walls.
And there's one single guy coming up there by the house.
He's on the opposite team, even though it's hard to see here.
And let's see what happens.
So what happened?
Of course, they're pretty stupid.
We didn't have very strong networks when we did this.
But they also don't have hearing.
Every one of them was looking in one direction.
They see this low resolution view of the world.
So what we did was add a small hearing radar, the thing you see in the lower left corner.
That's very crude.
It's very short range.
But at least it gives them an indication that someone is behind you.
And that helped a lot.
Another cool thing that speaks about power of neural networks is that they had learned using the 3D view, the vision view here.
So we just slapped this 2D radar on top of the 3D view.
We didn't do any other changes.
We just slapped it.
And it picked it up immediately, what it was supposed to do with that radar.
We had to retrain it, of course, but nothing else.
Another problem playing a real game is multi-action.
When we play real games, especially if you play a console or with a keyboard and mouse, you use a lot of simultaneous actions.
All of the game playing we have seen, this is from the Atari paper, they can only perform one action at a time.
So in this case, you see that the primitive actions, so to say, are per half, and then they have created all possible actions like as new actions, combination actions, go forward and press the button, for example.
And that's fine when you have an old Atari controller.
It doesn't work today.
PS4 controller has around 20 inputs.
And if you combine all the possible combinations of 20 inputs, it's around 2 million.
So we can't create all these new actions for all the possible combinations.
The action space is too large.
Unfortunately, you can't play, once again, this is from Battlefield.
We're in the same building as DICE in Stockholm, by the way, so that's why we do a lot of Battlefield.
battlefield you can't play without using a lot of simultaneous actions.
So we had to solve that problem because if you just start allowing simultaneous actions for the agent it will just button mash, it will press half of them on average and it takes an enormous long time to learn even the most basic things when you do that.
So what we did, and don't worry I won't go through the details here, we...
We did some imitation learning.
So we had the agent watch 30 minutes of human gameplay before starting, or actually simultaneously with the reinforcement learning.
And just for the beginning, so we decayed the amount of imitation learning it did over time, the first hour maybe of training.
And what this did, it helped the agent understand which combinations are valuable, which combinations make sense of the controller.
So when we did that, that's the green line.
So I won't go into details once again, but the higher you get there, the more score you get, the better.
So as you can see, when we added imitation learning to this multi-action agent, it behaved much, much better.
So let's look at some results, how the agent behaved.
So when we went from a target games we we started learning on the targets we couldn't jump directly into a real game so we built a very small simple. FPS game essentially so let's start the movie and look at it.
So the green guy here is the agent and as you can see is 12 different actions can perform all of them simultaneously.
And.
The goal here is this blue circle, to protect the blue circle.
We'll see here soon.
Yeah, that's the objective area.
So that's what the trade agent is trying to do, is trying to find the circles.
They move around every 30 seconds or every minute.
And it has also got health pickups and ammunition pickups.
And of course, there are opposing bots.
He's alone against 10 opposing bots.
They use classical AI techniques.
He has a much higher rate of fire though, so it's still not impossible.
So this is what the agent sees.
This is the only input it has.
It also has access actually to its health and ammo, but otherwise it's just a visual input.
And this is a low resolution input and you can also see the hearing radar.
The blue dot in the hearing radar indicates the direction to the objective area.
in case the agent can't see it.
And another thing that surprised us was the navigation capabilities.
There's no navigation systems here.
There's no nav meshes or anything.
But still, it navigates this maze of houses to find objective without problem.
And once it reaches the objective area, it has a few behaviors as well.
It, of course, starts defending the area, and it's also patrolling the area.
The hardest thing for it to learn was supplies.
So when it runs out of ammo now, it immediately, which it did now, it immediately prioritizes finding ammo before anything else.
You can see that it ignores the enemies, it ignores the objective area, runs to the green box, and then turns around.
It also, this scanning behavior was also something it discovered after a while to search more efficiently.
So all these behaviors were emergent.
We didn't say that it should do like this or that.
We only gave it the objective, protect the area.
Another cool thing about this agents, they generalize very good well so this is exactly the same agent of course it's a new action space new buttons to push, but otherwise it's the fact that the same agent.
So it's a very simple racing game, but it only took a few minutes for agent to learn how to lap this circuit.
So they are they can solve a lot of different problems of course you have to train them for each problem right now.
So what about reinforcement learning in AAA games?
So we have collaborated with dice to try to do this in Battlefield 1.
And the case study we set out to do was automated testing.
Battlefield 1 is a very complicated game.
It's 64 players.
It's four character classes.
There's infantry play.
There's vehicle play.
There's airplanes.
There's horses.
There's zeppelins.
There's lots of game modes, there's lots of maps, and this needs to be tested for every new build, essentially.
So this is a nightmare for QA.
So we thought, what if we can help by actually have self-learning agents to play, not all 64 players, but a few.
Fill them up with 50 agents, maybe, and have 10 humans or something.
That would make us be able to scale up testing a lot.
So before going further and showing you how this turned out, we tried actually to use rendered observations first.
But the visuals of Battlefield are far too complex for the small visual networks we use.
We actually use almost the same network that played Atari games.
So actually just seeing the difference of uniforms in Battlefield was too hard.
So we have a simplified observation instead.
So the blue in this observation is obstacles, and we use ray casting to find that.
So we don't render that, really.
And the red guys are the enemy.
And that's in a higher resolution.
So it's 12 by 12 for the obstacles.
It's 128 by 128 for the enemies.
And we also have this hearing radar, a 20-minute range.
The main reward for the agents was the score from the game.
So we're trying to maximize the score.
But to get them started, we also introduced a waypoint.
So both teams are trying to go to the same waypoint.
And then that also helps them meet each other on a large map.
And supplies, we also added, they don't have the actions heal and resupply.
So we added the boxes from the previous games to Battlefield.
The agents learn, the last agent we saw, that agent was alone against classical AI bots.
In this case, we have the agents learn by self-play.
So both teams are agents.
So the second team's brain is an older version of the first team's, so why don't we use the best brain for both teams?
Well, because of this.
Let's go.
Let's go.
Let's go.
Let's go.
Let's go.
Let's go.
Let's go.
Let's go.
Let's go.
Let's go.
Strange game.
The only winning move is not to play.
So this literally happened when we had the same brain for both teams.
They discovered that I don't shoot, I don't get shot.
So they stopped shooting and just went around picking up boxes.
So we actually had to, well, we froze one of the brains to be an older version, so they are not the same.
And another thing we did was actually introduce a few of the really stupid testing bots we have into each side as well, but they actually shoot.
So now the agents have to defend themselves from the beginning.
So does it work?
Yeah, well, we can look at this clip first.
The enemy is in the lead.
This is not exactly fine Battlefield gameplay.
And they also have a problem with being indoors.
Yeah, so they end up doing this when they don't have an obvious way of getting a reward.
They don't see an enemy, they are already close to the waypoint and they have no need to pick up supplies.
They have nothing else to do, so they circle until they find something to do.
So let's look at some more successful moments.
This is programmer art or programmer video recording, rather, no professional video editor has been involved in this, so apologize for the shaky camera movements.
So it's about a two minute clip, and every player you see here is controlled by neural network.
So it works.
So to my knowledge, this is the first time anyone has been able to play a first-person immersive modern game with deep reinforcement learning.
So of course, there are lots of challenges with doing this as well.
This isn't easy.
So one of the biggest problems is slow training.
So what you just saw, those agents had trained for six days.
On and we use 8 machines in parallel to to to play the game so it amounts to about 15,000 game runs for 300 days of gameplay if you count all the agents experience so it's definitely slow slow going.
It's also behavioral design.
As I talked, you do reward shaping.
You have to design these rewards to get the agents to do what you want them to do.
Of course, in this case, it was mostly the Battlefield score.
But it's hard for a game designer to actually go through reward shaping to get the behavior they need from an agent.
So that's a hard part.
You don't have a full control.
You will be surprised by the behavior.
That can be both good and bad.
It's very hard to debug a neural network, we have discovered.
And there's also the question how we integrate this with classical AI systems like behavior trees, because right now all the behavior of the agent is controlled by the neural network.
I don't think that will be the first thing that happens in real games. I think some parts of the behavior will be controlled by neural networks, and that means we need to integrate this into current AI systems.
And of course, the execution, it's actually not a huge problem, but the GPU is busy doing graphics, typically.
And so all the agents run on CPU now.
But inference, that is actually running a trained agent, is called inference.
That's much, much cheaper than training the agent.
It's a magnitude cheaper.
So right now, it's not a huge problem.
And if some of you have been at a few of our rendering talks, you might have seen this image already.
But this is a small project called Pika Pika.
It's built up on top of an R&D game engine.
So it's mostly called Halcyon, the game engine.
So it's mostly for rendering research.
While we were building a new game engine, why not make sure that it's good at training agents with that is fast and that it has all the mechanism needed to train agents.
So these little robots, the yellow robots here, I'll show you a short trailer for this project.
And the robots are trying to repair machines.
And this is a much simpler task than the battlefield task we just saw.
But this was just right now to get some life into this rendered environment.
So let's have a look.
So in this environment, we get it's fast.
We get a lot of different rendering modes that the agent can use.
It has very fast communication with the brains.
Those are typically written in Python.
The game itself is in C++.
So we'll continue making sure that this engine has good support for self-learning.
Because speed is very important.
The more data you can get when you're training, the faster you can learn.
And in this case, we see 36 agents training in the same process.
And we can actually run a few of these processes on one machine.
So we can get a lot of machines training in parallel.
So the hype.
I promised to talk a bit about the hype.
And one sign of the hype is that I counted to more than 20 talks at GDC this year that have to do with deep learning or machine learning.
And I think that's a record.
And of course, much of the hype is about artificial general intelligence.
And that's not really what we are doing, but let's talk for a short while about it.
So that is when a computer is as good as a human is on most tasks.
So, will there be artificial general intelligence?
I think that's quite easy to answer.
If you believe two things, technological progress will continue.
and intelligence is not magic, it is biologically based, it's physics, it's not supernatural in some way, then yes.
Of course, technological progress might not continue, there might be the third world war or something, but if these two things happen, then we will eventually have artificial general intelligence, according to my belief at least.
So then the question is when?
So a couple of years ago, there was a questionnaire at the conference, and a lot of experts guessed when we would have AGI.
And the median, I say average here, but it's a median.
The median guess was somewhere in the 2040s, so a little bit more than 20 years away.
Of course, this is a wild guess.
It's very hard to make these kinds of predictions.
new technology. So for example in 1902 the Wright brothers said that it would take 50 years before humans would fly.
In 1903 they flew.
In 2015 people believed it would take another 10 or 15 years at least before a computer program would beat a Go master.
In 2016 AlphaGo did it.
But of course we still don't have those flying cars that we were promised in the 50s. So it's very hard to make this kind of prediction.
But let's go with 20 years away for now.
And the hype, this doesn't necessarily have to be artificial intelligence.
But there's some negative concept.
There's lots of exaggerations going around, that AI will solve every problem.
It probably won't, not for a long time, because it's still very hard to do.
And every startup is now an AI startup, or a blockchain startup, or even the best is an AI blockchain startup.
So the label AI is very overused.
Every feature is suddenly an AI feature.
I saw a thermostat that could lower the temperature in your house during night.
That was an AI thermostat.
Another problem, not noticeable maybe if you're not in academia, but it works in practice, but not in theory.
We really don't know why deep learning works as well as it does.
It shouldn't work this well.
And we don't have the theory for it yet, which means that building a neural network architecture and tuning the hyperparameters and everything is more of an art than a science right now.
It's not a huge problem in practice because if it works, it works.
But if we have the theory, it would of course be much easier to do things.
Then we could actually calculate what the network architecture should be instead of guessing and trying.
Yes, the AI winters, there's also a lot of naysayers that we will soon have a new winter.
We had winters during the 80s and 90s where we reached a plateau, nothing, we could solve toy problems, but nothing really interesting.
And of course we might end up there again, but I'm more hopeful this time because, yeah, I'll come back to that at the end of the talk.
We have taken a big step up now and can solve a lot more than toy problems.
And of course, there's people that think that AI should be banned.
I saw that one union here in the US wants to ban all deliveries with automated trucks and drones, for example.
This is already a debate that has started.
So let's get back to narrow AI, not the general AI.
So deep reinforcement learning is hard, and there's been some great blog posts recently about something like this one, deep reinforcement learning doesn't work yet.
And...
They bring up a lot of great points that are not unsolved really.
And it only works well for games and simulations, they say, and that's very lucky for us because we are in, of course, in gaming.
So one problem is the reward shaping I mentioned before.
So this is work from OpenAI.
And the boat here is supposed to go around the track.
It's not doing that.
It's going in the wrong direction.
But it's still winning the game because it has found these green things that it gets a lot of score for.
You can see on the small radar to the upper left the other players going around like they're supposed to do.
But this boat actually wins the game.
So it's because it has much higher score than the other ones when the game ends.
So it's found an exploit.
And that's actually something these agents are very useful for, finding exploits in games.
They found bugs and old exploits that were unknown for 40 years in Atari games.
But this is also a problem because reward shaping, as I said, is hard.
We don't want to have lots of specific rewards to get a certain behavior out of the, it's too hard to do that, it takes a long time testing.
So how do we try to solve that?
So in this case, the robot, this is from a recent paper by DeepMind, and the robot here is supposed to first staple the blocks and then clean up and put them in a box, opening the lid of the box.
And this is a hard problem because usually you have to get lots of small rewards here, like first try to move the arm towards the block, then grip the block, then open the lid, then grip the block again, open and drop the block, and then you're finished.
And every one of these subtasks consists of maybe 100 actions, small actions, I mean motor actions, to actually achieve this.
So what they have done here is hierarchical reinforcement learning.
It actually learns to use these higher level actions instead.
So for example, move to block or grip, those consists of a lot of small motor actions.
But it has learned to use a sequence of those higher level actions instead.
And then the problem becomes much simpler.
Then we can simply give it a reward.
Once it has cleaned up and closed the box, you get one point in reward.
You get no other rewards.
And that's a very hard problem.
But I think we have to solve that problem to make this feasible, or at least much simpler than it is today.
Another problem is, of course, if you saw this curve, if you look at the horizontal axis, that's tens of millions of steps.
It takes a long time, like I said. So sample efficiency, this is called, we don't use the data efficiently enough.
So yeah, so they are very slow to learn.
And why is that?
So if you take a newborn kitten as an example, they are also They're not great at playing games.
Because they are born blind, they don't know how to move, really.
They know nothing about gravity until they fall a couple of times, and so on.
So they have to learn everything from scratch.
And that's also what our agent does.
Every time we train it, it starts from scratch.
It has to relearn its visual system, and so on.
So it takes a long time, and that's not strange.
So what we need to do is transfer learning.
We need to be able to take an agent that already has learned a few things and continue learning.
But that's also a hard problem.
Yeah, so this would be slow.
So there was actually recently an experiment on human priors.
So to the right here you have a simple platformer.
And there's so much we take for granted that you can climb a mirror ladder just that you can stand on the gray stuff.
And that you should avoid the pointy stuff and the monsters and so on.
And if you see here up to the right, there's a key.
And we know that a key is usually used in a door.
So the player doesn't have a problem opening the door.
To the left is what the computer sees, or an approximation of what the computer sees.
And they let humans play like this.
And then suddenly, the agent wasn't slower than the human learning.
You can imagine playing the game to the left.
It's a lot harder.
And that's because we have priors.
We have a lot of knowledge about the world.
So one trend almost, or at least a few recent papers have been starting to talking about this, we have to have our agents play.
So playing to build a model of the world.
So not try to solve the direct task that eventually we want them to solve.
Have them play around in the world instead, and let them learn a model of the world, what happens, so they can predict what happens when they do an action.
Another criticism against deep reinforcement learning is, like I said, we have to retrain it when it wants to learn something new.
So it's a one-trick pony.
And that's mostly true, until a couple of weeks ago, when once again DeepMind released a paper where they actually have one agent playing 30 games without retraining.
It's the same agent playing all 30 games.
So it doesn't necessarily have to be a one-trick pony anymore.
And finally, I want to talk more about AlphaGo.
So Go is a much harder game for a computer to play than chess.
Chess was beaten in 97, when Casparo lost to Deep Blue.
But that was completely hand-coded, Deep Blue.
AlphaGo, of course, used machine learning.
And it used machine learning first to learn the game.
It looked at hundreds of thousands of human games from archives to be able to pick up something to start with.
And then it started using reinforcement learning to become better.
And in 2016, March 2016, it beat Lee Sedol, one of the best players in the world.
And of course, that's a huge success, but AlphaGo was pretty complicated.
It had to use imitation learning to learn from humans first.
It used an enormous amount of hardware.
And if we look at what has happened since then, two years ago, we've had a few versions of AlphaGo.
So AlphaGo Master is just a continuation of AlphaLEE.
And that managed to beat 60 masters undefeated.
So that's a much better version.
But then the really amazing thing is AlphaGo Zero.
AlphaGo Zero is a much simpler version of AlphaGo.
It uses no imitation learning.
It learns completely from scratch.
It uses much less hardware.
The network architecture is much simpler.
And still, let's see.
Well, before we look at a diagram, let's talk a bit about the ELO rating.
So the best human is rated a bit below 3,700.
If you have 400 more than your opponent, then you have a 91% chance to win, and so on.
800, 99, and 1,500.
1 in 10,000 almost that you will lose.
So here is AlphaGo Zero's performance.
This is training in days that you see on the horizontal axis.
And the green line is the AlphaGo Li version.
And remember, this is using much less hardware.
It's a much simpler algorithm, a much simpler network model.
So when the line stops here, it's around 5,200 or something in Elo.
And that's 1,500 above the best human.
And that's one in 10,000 to get beaten.
So this is truly superhuman performance.
And the other thing after that, there came a version called AlphaZero only, because that can play many different games.
Because in this simple system, all you have to do is change the rules, nothing else.
So they inserted chess rules instead of Go rules.
And of course, it became the best chess-playing program in the world as well.
And a cool thing from this paper was that it discovers these standard openings of chess.
And you can see the hours of training on the horizontal axis, how it discovers an opening.
And then when it becomes more and more experienced, it abandons some of the openings.
So it's too bad of those of us that are playing car or can defense still.
You shouldn't, according to AlphaZero.
So.
But one of the most amazing things that's not visible on this graph I just showed you is this.
This is the best handcrafted Go program.
It's around 2,000 Elo rating.
So within two days, AlphaGo beat decades of software engineering and 1,000 years of Go experience with a simple learning algorithm.
And this is one of the important points here.
With learning, we can do things that we cannot possibly, we are not good enough to program this ourselves.
So deep learning is still hard.
It's still simpler to solve many problems with the conventional methods rather than DL.
So why do it?
Well, I've been in software engineering for a long time, and this is definitely the largest boost to capabilities of computers that I've ever seen.
We can now do things that were previously impossible in computer vision or post-estimation or some of the other examples I showed you.
or just learning to play one of these complex games was also impossible just a couple of years ago.
And that learning methods can quickly outperform decades of software engineering effort.
We can try very, very hard to solve a complex problem, but it's becoming more and more probable that soon some machine learning method will be able to solve it much better than you can as a programmer.
So many difficult challenges remain, but the future potential of this is enormous.
And as I said in the beginning about the AI winter, we are just starting to learn what we can do with deep learning.
There's so much left.
As I said, it's more an art than a science still, and we're still in the early days of deep learning, and that's why I'm very hopeful.
Thank you.
Questions?
Hey, so you mentioned somewhere in the middle of the talk that we don't actually have a formal understanding of why deep learning works as well as it does.
So actually tuning the parameters as much of an art form as actually having something that's formalized around that.
I'm wondering if you have any anecdotal evidence of how did you guys actually triage?
how you should change parameters between different iterations of the game, or of any sort of problem that you're actually in, because theoretically, people can make different arguments as to whether you should keep going, whether you should try to change parameters, et cetera.
Yeah, so the answer to that is we didn't triage that much, because we didn't have much hardware to do many parallel trials.
So we went with intuition and had some luck.
But I'm certain that these agents I've shown you today, they are in no way optimal attuned.
They could become much better if we put more resources towards it, or get better theory behind why they work as they do.
So yeah, we didn't do much tuning.
Yes?
Hi.
Very impressive presentation.
Thank you very much.
I was just curious about, you were talking about the network architecture, and it's very hard to tune.
And it seems like it's one of those intractable problems that machine learning is actually very good at solving.
Because we don't really understand how machine learning works, but it does work.
We don't really know how the parameters work, but it's great at finding optimal parameters to do something.
So do you think there's some sort of future for having machine learning to tune machine learning?
Yes.
Well, I don't know, but there is this topic of meta-learning, where you actually try to learn to learn.
and you have another neural network controlling the learning algorithm itself.
So I guess that's one route towards what you just described, but otherwise I don't know.
Thank you.
Hi. I'm curious if there's a reason why you chose vision as the primary input for your algorithms as opposed to more traditional knowledge representation of AI in the world.
I mean, obviously you need some element of vision, but things like ray tracing, things like, you know, things that you would give a traditional AI as knowledge in order to reason.
Why vision was the input instead of that?
One reason was because that's how the algorithms were put together when we started.
I mean, the Atari examples, they were with vision.
But as I said, in the Battlefield example, we couldn't use vision anymore because the vision was, it took too long to train and it was too complex.
So we actually used, what you said, ray tracing in the world instead.
And of course, we could use some kind of internal game state representation as well.
We don't have to use anything visual at all.
But the problem with using internal game state, you have to be very careful to not give the agent more information than it should have.
Because the game state, of course, is the perfect information of everything in the game.
You only want the partial information that the player would have to get it to play like a player.
So that's also one reason to use these simple vision-like observations.
I was wondering if your team does any work in using deep learning to analyze analytics data from online games, or is that outside your scope?
It is outside my scope, but we definitely do it within EA, yes.
Thanks.
If there's no one else.
OK, one more.
Did you do most of your experiments with very simple single hidden layers, or did you do a lot of experiments with different graph formations?
You mean the network model itself?
Correct.
We didn't do much experimenting.
We used essentially a slightly modified Atari network with some convolutional networks for the vision system.
But we did add an LSTM, the LSTM version, to get some understanding of time sequences.
But that's about it.
Thank you.
So the question was have we considered using internal game state to speed up the training?
And yes, we have considered it.
We haven't done it yet.
But yes, trying to filter the game state to actually be approximately the same information that the player has, because the problem is you have to start to do a lot of occlusion and other things to hide, and that becomes expensive.
It's actually much cheaper to give the agent all the information, but then, of course, it becomes omniscient and superhuman.
And that's no fun.
Okay, thank you.
