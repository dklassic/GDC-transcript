This talk is called Believable Humans, and I'm Mike Mumbaugh. I'm the Director of the PlayStation Visual Arts Service Group, and this is my colleague, Dr. Mark Sagar, Director of Laboratory for Animate Technologies.
I'm going to start with an interesting thing when I was putting together this talk.
I...
There's a couple quotes in here, but this one's particularly interesting to me.
You know, you can connect the dots looking... you can't connect the dots looking forward, you can only connect them looking backwards.
So you have to trust that the dots will somehow connect in your future.
And this talk really is about my experience in the last 12 years in kind of the digital human space.
And Mark's had even more experience with that, but my experience starts at...
Oh, it's not... the talk's not on. Hang on, hang on.
We didn't actually put the...
That'll be better for you if it actually shows.
There we go.
At least we didn't get too far down.
There's that.
Okay.
So again, my story starts, at least in digital character space, around 2003.
And that's when I first met Mark Sagar.
I was working on the Polar Express, and he was working on Spider-Man.
I joined PlayStation in 2007.
And that's when I started working on character stuff there, cinematic stuff.
Since I've been at PlayStation, we've worked on about 45 different titles.
And we do art and cinematic support for worldwide studios, focusing on digital characters, faces and animation.
So...
I'm going to show you here if you can see.
So since I joined PlayStation, we've done roughly about 14 animated feature, with the equivalent of about 14 animated feature films in six years, but it's been in the video game space.
Going back to Mark, just so that he, so when we do this talk, we're gonna transition, I wanna do the introduction so he doesn't have to do that.
He's a PhD in engineering and specializes in performance facial capture and advanced rendering techniques and also facial motion capture.
He's got two Academy Awards, one for his work at Imageworks on Spider-Man 2 and the other for his work at Weta on King Kong and Avatar.
So, I'm going to start talking now about some of the stuff from PlayStation and some of the story of kind of where we were, how we got there, and then kind of where we're going.
And I want to start with The Last of Us.
We're going to talk about it really briefly.
How many of you guys went to Damon Shelton from Naughty Dog's mocap talk?
Okay, cool. I'm not going to talk a lot about it, but one of the important things for me in this talk, especially in terms of believable characters, I think this is a defining moment for video games.
And this is a scene that we did in San Diego with my team.
We helped create the characters and we did.
a lot of, actually this entire scene of animation, for The Last of Us, we did about 80% of the cinematic animation for the title.
So hopefully this will play.
Is there audio?
Hang on.
How many of you guys know this scene?
Oh, this won't work without audio.
David or whatever from the old...
Sorry, y'all.
Let's see if this works now.
Yes, sir.
Somebody, we've just been through hell.
We just need...
Oh no.
Sarah.
Move your hands, baby.
I know, baby. I know.
God.
Listen to me. I know this one.
You're gonna be okay, baby. Stay with me.
Alright. I'm gonna pick you up.
I know, baby. I know you're in this.
Come on, baby. Please.
I know, baby. I know.
Sarah.
Don't do this to me, please.
Don't do this to me.
No.
No.
No.
No.
No.
No.
No.
No.
No.
Please.
Please.
Please.
Please.
Please.
Please.
Please.
All right.
We're back.
So, sorry to start in such a downer.
But this is a pretty important moment for video games, I think.
You know, Empire Called a Gaming Citizen Kane moment, we've heard a couple of things like that before.
But this really has a lot of emotional impact.
I've watched this scene, we worked on it, and I have a hard time watching it.
And, you know, I've grown up with video games since, you know, I was a kid, five years old, so really grown up with them.
And...
Maybe it's because I have two daughters of my own that I look at this and it's really hard to watch.
But I think it's more than that. I think there's empathy.
I think we can all relate to the characters here.
We can all relate to loss.
And I wondered that if we looked at this 20 years ago, if we did the same thing 20 years ago, and it looked, you know, in the old 8-bit kind of graphic stuff, could we get the same level of...
empathy. So it's the same scene, right? It's the same writing.
And that's how we would have done it and that's the same music. But I'm not crying. Why? Right? It's just, it's an interesting thing to look at because it's not just the context. It really is something else.
So these games now are getting incredibly complicated, and they're very challenging, just like making anything great, right?
It's about the quality of it.
And quality is never an accident.
It's always a result of high intention, sincere effort, intelligent direction, and skillful execution.
It represents the wise choice of many alternatives.
And this particular title is a great example of skillful execution, right?
At every level, it's got relatable characters, outstanding casting direction and pacing.
And that's really important.
It doesn't matter what kind of entertainment we're talking about.
It's using performance capture, which is really, really important.
They've made smart visual choices.
Some of the choices are related to the power of the machine and what can be achieved.
But some of it really is about the choice just in terms of the style that they want to show.
There's amazing music and sound design.
And the truth is that Naughty Dog is one of the most advanced developers in the planet in terms of the way they think about collaboration.
they handed us one of their best emotionally impactful scenes to deliver for them.
And we had the same emotional investment in the game as they did.
So, game and movies, at least in terms of digital characters, in my opinion, fall into three different categories.
Realistic, hyper-real, and animated.
And these are kind of...
you know, some game examples that are falling into these categories.
This is not an opinion of where success lies.
It's just an opinion of what they're striving to achieve and the category in which they kind of fall into.
And for the films that really tried to do digital characters well, this is where they fell into as well.
So...
I told you I started Imageworks in 2003.
One of the last films I worked there was Beowulf.
Beowulf has a significant place in digital character history.
It's one of the few films that attempt to make a realistic, believable character, human character, in a completely digital world.
But it struggled to find an audience because it struggled to find a reason to exist.
You could shoot Beowulf physically, and why did you need it digitally?
And the other issue is this, right?
The uncanny valley.
I haven't heard this term in a little while since I'm out of film, and it was a real big deal in 2003 till 2007.
But for some reason, we don't really care about it that much in games.
She's no hack of the wolf.
So?
You both know that.
But answer me.
Did you kill her?
Would I have been able to escape her had I not?
Visually, this movie still has a lot of relevance.
There's a lot of digital character stuff we would like to do.
Sticky lips, for instance.
The shaders and the lighting, a lot of stuff is still pretty advanced even by today's standards.
The issues are that you've got a fabricated digital character, Beowulf, who's not based on anyone.
acting up against realistic digital double characters like Anthony Hopkins and you know Anthony Hopkins and so when you see Anthony Hopkins as a digital character you know something's wrong if it's not exactly right.
And what's interesting to me is if this movie had been done completely with with characters that you didn't.
know were based on real life characters. Would you have accepted it more? Would it have been easier to watch? Maybe.
I still go back and watch the movie and even though I'm too close to the movie I still think it's easier and I can still watch Beowulf and don't see as creepy you know, I don't feel that creepiness but I look at Anthony Hopkins and I still get the same feeling everybody else does.
And uh...
We also were, you know, this digital character thing was a real high focus at Imageworks at that time.
And it was actually interesting because Mark was working on the image-based rendering stuff for Spider-Man, which ended up being a demo for PlayStation 3, if you guys, yeah, PlayStation 3, if you remember, during the transition.
So, the timing of the film, it was released in November 16th in 2007.
And the commentary, you know, obviously the reviews were mixed, and the commentary is kind of pretty clear. You can go, but I'm not going to read all the stuff, because we're already behind schedule.
But I'm going to tell you one down at the bottom, it says, Beowulf's a solidly gorgeous at times, borderline, stolid piece of Tolkien with a joystick mythology.
That's a fascinating commentary. Joystick mythology. It's referencing video games.
It's, it's, it's relating, these critics are relating this to games.
Even though games at that time couldn't touch this visually.
In November 19th, three days later, Uncharted 1 came out.
Much better reception, right?
This was a game that used a lot of similar ideas.
Performance capture, no, it was using motion capture with animated faces.
But the idea of doing digital characters was there.
And there was a clear change, a clear sort of chapter shift.
I started in PlayStation, this is coincidentally, one of the dots backwards theory, on November 19th, 2007.
And that was the release day of Uncharted 1.
but also three days after the release of Beowulf.
So for me personally, this was a closing one chapter, right?
And then opening another.
And this was the first thing that I personally worked on with my team, a lot of who came down from Imageworks with me.
Fully acting.
It's a two person job.
No, no, no.
Three, actually.
This is Uncharted 2.
Speak of the devil, here she comes now.
Chloe Fraser.
Nate.
Drake, Nathan, Drake.
Hello, Harry.
Chloe is one of the best drivers in the business.
She'll take good care of us.
So, Uncharted 2 had a very different reception, right?
This is one of the most, um, highest rated games on PlayStation 3 and is one of the best examples of cinematic games.
And you can see even in the commentary again, they're even saying you don't need to make a movie of this.
There's a game.
It's better than you're gonna see in any movie.
Coincidentally, that same year this came out.
And it's interesting Metacritic score, and I'm, this is not about Metacritic, and I'm not a fan of Metacritic, but it's a good reflection of this sort of view of the time.
But it's interesting to see that Avatar, billion dollar franchise, probably two billion dollars, at this point it's an 83 Metacritic score. That's a B- isn't it?
fascinating, but it's incredibly, it's an incredible, it's a landmark film, visually, and they made very smart choices to avoid digital humans because again, they had moved on at that point, and they had found better ways to do, I mean, the techniques were still very similar to the ones we used, but the output was very different.
So the, looking at the evolution of the digital character, it's kind of funny, we, kind of gave up in film.
We'd solved the problems of the monkeys and the fantasy characters and the aliens, but we kind of stopped here.
And we moved on, but that's the starting point for games.
That's where games really picked up.
So around that time, we started doing more advanced techniques.
And around 2007 to 2009, doing more advanced scanning techniques at PlayStation.
And we scanned Dennis Hopper, which should show up here, hopefully.
No?
Here you go.
This was a title that never came out, but we were in 2007, 2008, starting to look at this digital double technique.
The game was cancelled, not for any reason having to do with digital doubles, but it certainly was an interesting time to even be exploring that.
Now we're seeing even more of that with Kevin Spacey in Advanced Warfare, but again we're still, the verdict's out.
It's interesting because the whole Uncanny Valley thing isn't really being discussed.
People aren't too freaked out about it in games.
But the technology we were using for scanning went into titles like SoCom4.
And when we released Uncharted 3, it set a new quality bar for our titles and the expectation for all of our titles that needed digital characters needed to be at that level.
And frankly, SoCom4, when I was there, what the developers were doing was not at that level.
What you see on the left is what they had completed.
And on the right was when we entered in, we completely revamped their characters with no topology changes, no higher vertex counts, same texture, resolution.
It was just a technique change, honestly.
And it was a challenge to kind of convince them at the time that this was worth it, but the reality was that It PlayStation 3 toward the end of that life cycle the guy in the left just wouldn't have wouldn't have held up So not only do we have to kind of do scans, but we ended up doing, we started doing fax-based blend shapes, because their engine could support blend shapes.
We were able to get 60 blend shapes, which is pretty good for fax, although we weren't doing facial capture.
And we still use the same animation techniques.
And ended up getting reasonably believable characters for what would be digital doubles, but not digital doubles that anybody would recognize.
We started seeing neck bulge, and we were able to get wrinkle maps in.
And we started using more advanced cinematic techniques.
We also started characterizing, started pushing away from doing exact scans towards kind of hyper-stylistically moving to get to more palatable visual characters.
We also played at this time, LA Noire had come out, played with some of the ideas of the video-based scanning, which look like it was going to be very disruptive at the time.
This is also a title that didn't come out.
We did some testing.
Eyeline stuff really doesn't work.
And it works in a sense that if you have a viewer and you're using a fourth wall where it's talking directly to you, it works because the eyeline is at you.
But beyond that, it really doesn't work.
But it was interesting just to see some of the tests.
We had done some tests to see how it would hold up with characters like Kratos.
Could we put Kratos in a game?
This was not for a game.
It was just testing to see could it be done.
This is not anything other than an actor in makeup, but it was interesting to see what it would do.
And again, this is PlayStation 3.
So at the end of PlayStation 3, we knew that PlayStation 4 was going to be a higher fidelity expectation.
We were going to have to do a lot more, and we need a lot better technology, and the costs were rising, and we needed to create efficiency.
So I contacted Mark, saying we need better facial capture solutions than what we have.
And everybody, all the studios are talking about building one, or using off-the-shelf stuff, which doesn't seem to give us the results we're looking for.
So I called him, you know, he was at Weta at the time, at least I thought, and when I contacted him he'd actually left Weta a week earlier and started a laboratory for digital characters working on AI.
And we talked about building a commercial partnership immediately.
One week after he started, we started talking, doing a commercial partnership and working on the technology advancements, all the stuff we'd learned over the years, building the technology and character advancements, the techniques, all the stuff that we'd learned over the years, using the performance capture techniques and the scanning techniques to build a system which we call Parrot.
And this was literally our first test out of this system.
Beginning incision at upper abdomen.
Exposed internal organs seem to produce an ultraviolet glow.
This was again, not for a title, this was just testing to get the tech to see how far we could go with it.
When I asked Marcus, I think I hit bone or something.
What, in your opinion, is the next gen in facial?
He's been doing this since 1997, so if anybody knows about this, Mark knows about it.
So I said, what are you looking at next?
Where everybody else is looking at is a lot of stuff we talked about a decade ago and honestly did on movies a decade ago.
So, if I'm going to build something and we want to spend a lot of money and time on it, we want to do something that's more advanced and can honestly break some of the barriers that we hit.
And interestingly enough, he told me that at that time, he said, oh, I'm actually working on stuff for games.
I'm more interested in the game space.
We kind of solved King Kong, and we did Avatar, and that was really cool.
But I'm really interested in actually building stuff behind the face and using AI to actually drive facial animation.
So that's where we're going to lead to.
Really quickly, though, most recently, the stuff that we've built.
We revamped all of our photogrammetric scanners to build some incredibly high detailed scans, some of the highest you can possibly get out of photogrammetric scans for facial right now.
And we're also working on bodies.
And it was done with a partnership with Sony Electronics because we have cameras.
We have a really good camera division that we didn't have access to at the time.
And we had to build custom software, which is kind of a pain in the ass.
But we did that, and it paid off.
And we were able to immediately put that work into the order.
And so the order is a culmination of a lot of techniques, some of all of the years of these things in one place, using the best performance capture techniques, using the best scanning techniques.
We still had stuff to learn, but I think if you look at what Ready at Dawn had achieved, coming from a PSP developer to a PS4 developer, with no generation of knowledge between, it's remarkable.
And they're actually setting the bar in the industry in terms of visuals.
They're not getting recognized, I believe, as fairly as they should.
And there's certainly an interesting commentary going on that's kind of a little bit of a rebound effect.
They're getting too close to film, and now games are sort of pushing back, saying that's too film-like.
And it's sort of ironic because games have been a safe haven for innovation, especially in the digital character space.
Games like LA Noire, even though there were flaws and the disconnect between the body and the eyes were, frankly, very, very difficult to look at, but people really were OK.
And the critics were thrilled.
And the issue is that we are getting too close to film.
And for some reason, now we're getting pushback.
It's interesting.
So now I'm moving on, and then we're going to transition to Mars.
I'm going to close out with our work on Uncharted 4.
I don't have a lot to show you here because we're still working on it.
But.
When we go back to the types of techniques we were using on Beowulf, this was very interesting.
We did a base digital scan of Ray Winson who played Beowulf.
That couldn't be really utilized in any way.
The scan was only used for reference and only reference for how his face moved.
And ultimately, mocap drove the poses that we would actually model from.
So it's a remarkable difference from today for where we are with Uncharted 4.
We're actually scanning an actor that's got similar facial structure to the character we want to look at.
It's Nathan Drake. It's not an actor.
It's not based on an actor, but these facial shapes have resemblances to the character we want.
And we're using these blend shapes to influence the rig directly, which thereby make the animation a lot more...
much higher fidelity and a lot higher accuracy and also just in, you know, much better overall.
Here's an interesting example.
This is the blend shapes we used about 70 on SOCOM and this is about 180 on the order and we're about 1,000.
on Uncharted 4. So you can see the fidelity. Now how all those blend shapes, they won't all be used in the engine, they're going to drive a rig, but we're using what we call micro-expressions to define all these really, really core shapes that are really important to get the kind of fidelity we need. And you know the visual target, this is Beowulf in 2007, you know, and this is Uncharted 4 in real time in 2015.
And this is the rig just showing you kind of a range of motion.
I can't really show you any of the songs or animations.
We are using all of the techniques, the scanning techniques, the motion capture techniques, all the animation techniques that we've learned along the way.
And finally, right now, our biggest challenge in this area is a lack of domain experts.
For me, you know, I've worked in the industry between games and film for...
for over a decade, and I know very few people that can build these kinds of systems, Mark's one of the few.
The scale and complexity is enormous, and the artists and tech artists are highly specialized.
They have to completely understand how the face moves, and there's no training for that, really, and there's no schools that are focusing on that.
So it's really hard to find blend shape artists.
It's really hard to find animators with that focus.
We need better technology and automation, otherwise it's going to be impossible to scale.
That's where our focus is.
you know, and we just need more face experts.
So, I'll leave you with a quote, the reasonable man adapts himself to the world, the unreasonable one persists in trying to adapt the world to himself.
Thereby all progress depends on the unreasonable man.
I'm going to now pass it on to the most unreasonable man I know, Mark Sagar, who has some really incredible technology to show you in terms of, again, this is not being utilized right now, but this is stuff that is going to be coming.
and when we start figuring out how to use this stuff, it's going to change things.
So, I'm going to drop a few emails here.
That's mine. Feel free to contact me. I'm sorry, you know, it wasn't highly technical, but if you have any technical questions, I'll happen to answer them.
It was along with Kareem Ashoud, who is the...
one of the most important influencers on the art side, if not the most important influencer on titles like The Order and Uncharted 4.
And Tom Bland, who's been the animation director on our side for these titles since Uncharted 2.
So these are the guys to hit up and ask questions about.
Thank you, and I'm passing on to Mark.
Okay, thank you very much.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
So it's coming through this jack here.
Yeah, great.
Thank you.
OK, so thank you very much, Mike.
Thank you for the unreasonable.
characterization, that's fine.
So this is going to take that to an extreme now.
So one of the things which interests me about digital characters was, so this is going to be a slightly different tack on the whole creating human aspect.
This is really a bit more about creating behavior.
So how do you create the inner life of a character?
Now the thing is which Mike's talked about in a lot of detail is the face.
And the face is our primary means of communication and expression.
So it's critical that you get the expressions right because that communicates the information that's telling you what the character's mental state is.
And the face's identity has to be right.
So that's like the um...
Anthony Hopkins example. The face is your signature. It's very important to get right.
So this is a film that we made in 1999, a short film. It was in the SIGGRAPH Electronic Theatre. And for this I wanted to digitally age an old woman. This is ten years before Benjamin Button. And what we're doing here is sort of creating a character. It's a digital character that we want to basically make you think about what she's thinking.
So she's just supposed to be remembering things.
We're trying to make you think about the mental world of a character that doesn't exist.
And that really got me thinking about how that's done.
And so a lot of the films that I worked on, like especially King Kong and Avatar, were about how do you...
make a digital character appear like a conscious thinking being.
And, you know, that's basically...
you have to pay a lot of attention to what's going on in the face, most subtle things.
Like, characters like King Kong don't speak, so the emotional weight of that is coming through the most subtle movements.
And that really got me thinking about, okay, well, what are the symphonies of our facial movement, how are they composed?
Because that's the music of the face, which is coming across.
And...
Our behavior, what's expressed on the face, is actually the result of a very complex, multi-scale system.
It depends on how you feel at the time, whether you had lunch, what your thoughts are, what your history is, how you express that, and then somebody else's reaction.
So the whole believability of a character in a scene depends on...
the context. Does everything tie together? So, to be unreasonable we thought, can we actually model this?
So can we actually create models which integrate these different elements?
And so what we're doing in my lab is creating autonomous animation.
So we're basically simulating faces and simulating brains and putting these together.
And we're trying to do it in real time, but the motivation there is that for a computer to really simulate a human face behaviouring, you have to simulate its brain and body too.
So here's an overview of our architecture.
So we're basically creating a system to build artificial nervous systems.
So what it does is it interconnects senses and effectors with expressive displays, but through a brain.
And so the brain will do multimodal association.
It evaluates things.
In robotics, this would create movement.
We're really working on how does the computer convey how it feels?
How does it express itself?
And then how does that connect into a loop to people that it's interacting with?
So, basically I set up this lab just when Mike had called to do exactly this, because this is where I wanted to go.
I really, ultimately I want to create a conscious computer.
It might be impossible, but it's a really fun challenge.
So, really, so that's the unreasonable bit.
So what we're doing is we're combining bioengineering, neuroscience, sensing, and computer graphics, and putting these things together to basically embody models of the brain with faces that you can interact with.
There's two reasons for this.
One of it is to basically explore theoretical models of brain function.
The other thing is there's a human-computer interface technology.
And so games fit somewhere in between that.
And so the way we look at the face is as an instrument.
And so what we're doing is really building a synthesizer.
And so we want to look at what's the information that we're conveying?
You know, how does the facial information get across if you're really boiling it down?
And, you know, ultimately it's muscle activation.
So, those are the notes, the musical notes of the face, if you like.
So Paul Ekman in the 70s basically came up with a system to sort of encode these different muscle groups.
So that's been used a lot in film and games.
And so one of the things though is it's really how to generate these shapes.
So this is a particular thing I was looking at is creating a biomechanical model of the face so that we could automatically generate these blin shapes.
So what you're looking at now is a physics-based simulation of a human face.
It's a finite element model.
So it's my face because I was a guinea pig.
But basically we now digitally activate these muscles and it's creating the shapes.
These aren't being sculpted, they're being actually created by a physics simulation.
Now what we're going to do is just start activating all the muscles randomly.
And the physics simulation is doing all the flesh movements, doing all the collision detection.
And this only works if you get the anatomy really in a lot of detail and really right with all the multiple complex material properties and sliding properties of skin and so forth.
So it's not an easy job.
But this was sort of the stuff that I was doing in the past.
And what I really wanted to do was to create the control system for this.
So what we're really doing in my lab is essentially creating nerve animation.
So how do we create animation which animates the nerves?
which drive the face.
So if we do this, and I believe that it's ultimately going to be necessary to do this because this is what conveys the subtlety of what a face can really do.
So people have strokes, for example, half a face will be paralyzed, but they'll smile naturally because the emotional system driving the face is different from the cognitive system.
So there's a whole bunch of things like that.
The meat of the stuff that we're doing is what we call brain-based computing.
So it's neurobehavioral models.
So how do you create a digital character to really want to do something?
It has to have its own in life.
So for example, so what we're doing is essentially neurobehavioral animation.
So if my blood sugar level is low...
I'll get hungry and then I'll start seeking food and that motivates my behaviour.
If I get blocked in getting my food, I'll start getting angry and so forth.
So these are core models.
It's like we're trying to model what motivates a human and actually put that in a computer.
And at the same time, we're also doing models of the emotional systems, attention systems and also learning.
How do these systems learn?
Because we want them to learn in the same way that people do.
And also how do they interact socially?
So we build all kinds of different models, like for example, this is a model of basal ganglia, which is a crocodile as a giant basal ganglia, so this basically does habitual behaviour and learning, reinforcement learning, things like that.
And we also want to look at what's actually driving the emotions, so we've actually been building models of the emotional systems.
Now they're very controversial, but we look at the physiology, so we build physiology models of these emotional systems.
And then we start looking into, like, how does the brain learn?
And, you know, this is a sort of simple example of...
The brain self-organizes a lot.
You just expose it to new information, it just learns new stuff.
So I would hit a Go button, here's a bunch of random colors.
It's just self-organizing.
That could be words, it could be colors that you're seeing, part of your visual system learning, or it could be words that you're learning for the first time, and that sort of stuff.
And you build hierarchies of these types of things and you start getting essentially nature's equivalent of some of the things which, like Google, are doing with things like deep learning and so forth.
But it's all the same type of stuff.
We're taking a biological approach.
So we also look at memory.
You know, how does a computer remember its experiences?
So we're building basically models of the memory system.
So this is just an old test which is basically the computer imagining a sequence of numbers.
It knows what's going to happen next.
So this is like a computer imagining something.
So then we start looking around.
So there's been lots of interesting computational neuroscience and cognitive science models out there, but what happens if we put them together and actually embody them?
And how do you even do that?
And so to do that, we've basically built essentially a Lego system.
It's really a brain Lego system, but because of trademark stuff, we've basically called it brain language.
And what it does is it combines neural networks, sensors, graphics, all kinds of stuff, so you can just plug it all together and build very complex systems.
So I'll show you a really simple system.
So what we're looking at here is a very simple circuit down the right.
This is like the knee reflexor circuit.
So basically we'll sort of connect a few things together, whack the knee.
And so we're simulating the spinal cord circuit, which gives you a knee reflex.
And then you can kind of see all the parameters.
So what our system does is it's a continuously integrating system.
So you can basically see what each neuron's doing, you know, live.
And you can investigate that while the model's running.
Now.
Basically to give you an example of you know how to put all these things together we've we've started off as simple as we can on a human which is a baby, so this is called baby X and So baby X. Here's the real baby So this is kind of like a baby capture now Here's a real baby for the very first having solid food for the very first time so the baby's having a fundamentally new experience I think it's a whoa moment Now we want the computer to do that too.
So anyway, we start basically building up 3D models of the baby.
Had to do that when my wife was asleep, so with the scanner, because I would have gotten trouble.
And so then we start building a biomechanical model of the baby.
and activate the biomechanical baby's muscles, because you can't get real babies to do what you want, but you can simulate them.
And then we start putting in a bunch of the different types of brain systems, which are driving the various aspects of the baby.
So I'll just quit out of the presentation for a second and fire up a quick demo here.
Now, what's actually happening now is the brain is compiling itself.
It's literally compiling at run time.
So it's kind of like a Python or something like that, but it's basically building all the connections and things like that.
So, here she is.
Okay, so she can see in here.
So if I make a loud noise, she'll get a fright.
We can kind of zoom in.
Let's see if I do this.
We've just changed all the view keys.
right around like that.
So, you know, she's watching me and reacting to me.
We can kind of take a look at what she can actually see.
So you can see me here.
So I'm looking at her and I smile at her.
Hey, sweetheart.
Now she's not copying my expression.
She's actually getting feel-good chemicals and then smiling.
Now, she's constantly learning.
So she's learning online.
So if I do, like you've heard of Pavlov's dog.
So if I do a, and I'll see if we can hear this from the microphone.
Can everybody hear that?
So, okay, that's called a conditioned stimulus.
So now I'm gonna pair it, and I'm gonna go, okay, let's see, and, hey, sweetheart.
Hey, you like the bell, yeah, the bell is awesome.
Okay, so now I've kind of paired that, and let's see what happens now when I do the bell.
She's sort of learning her context, so she's basically learning online all the time.
Okay, so then let's jump through to a few other things.
And so you can see, what we're looking at up here is basically a bunch of different neuromodulators.
So she's got oxytocin, she's got dopamine, she's got all these different types of things driving her.
Now, if I abandon her, I'm just going to sort of be a bit cruel here and sort of like, hide from her.
She's going to sort of start wondering where I've gone.
She knows I've gone off somewhere to the left.
But if you start watching where the arrow pointer is, she's going to start getting stressed out.
We're actually setting off her stress system.
So, by the way, all of our animations are completely generated by neural network circuits.
and those are driving the particular muscle.
So she's getting actually upset now.
We've got her cortisol's gone up, she's stressed out.
Hey, sweetheart, it's okay.
And she's listening to my voice and everything.
So I say, hey, don't cry, it's okay.
Okay, so now we're gonna be a bit more cool and actually remove her face.
So...
And so you can see what's actually driving behind it.
So literally, what you're looking at now are the circuits actually driving the muscles, driving the face.
So you can see the eyes, you can see the neural networks, you can see the nuclei and the brain stem, which are driving the eye muscles.
And as I'm moving around, it's just kind of looking.
So now what I'm gonna do is kind of like a, whoops.
Changed all my view keys.
So we've got this whole model of the brain, you can go around, we've put the things in the appropriate anatomical place.
So the idea here is we've put our neural networks in the place where they really are.
And now we're going to sort of, we can zoom into the brainstem and do stuff, we can even do other sorts of things here.
So because the model's based on these computational neuroscience models, we can do all kinds of things.
Like if I do this, I'm actually going to, I don't know if you can see this green light pumping up there, but what I'm doing is I'm actually like...
overriding the sort of, there's a whole thing called a thalmic cortical loop, which is a cortex that sends stuff down to the thalamus, and that's a feedback loop, and if that builds up you actually do something, and if it shuts down you don't. So what we're kind of simulating now is actually Huntington's disease. So this is a case where people basically, all the muscles are moving too much, we can do the opposite and kill off those cells, and now it shuts down this feedback loop. This is Parkinson's disease, people want to move but they can't, but we can build these things into the model.
And now we're coming out of the model, and so we can even, I'll even show you the effect externally.
So I'm now going to bump up the dopamine, so watch her pupils.
So this is essentially like putting the baby on amphetamines.
So it's like, you can do all these types of things, but it actually drives the behaviour of the model, it changes its learning abilities and all kinds of things.
Now, let's just jump to another thing.
And we've been teaching her to read, so I'm gonna show you, this is her little first words book.
So I'm from New Zealand, so I'm gonna show you a sheep.
Okay, right, okay sweetheart.
What's this, what's this?
Sheep.
Good girl.
Okay, and we'll see if she can read a word.
Okay, what's this, what's this say?
What's that?
What's that say?
Come on, what's that?
Hey, look at me. Pay attention. Pay attention.
It might be the light in it.
Yeah, that could be. I'll show her some other word.
Okay, hey, what's this? What's this?
Good girl.
Okay, so she's hearing me and getting rewards and all kinds of things.
So she's basically can...
Hey, what's this?
Good girl.
Okay, now I'll show you some things which I can't, which take longer to train.
These are things which might take an hour or something to train.
So this is learning to imitate a face.
So what's happening here is the baby is now gonna just do random facial movements.
And I'm copying here.
This is what parents do to kids.
So we're basically trying to build a computer which you can teach like a child.
Now as time passes, the baby starts learning those associations.
Now she's copying me.
Good girl.
Good girl.
Yeah, clever.
Okay, let's do that again.
Okay, you ready?
So we combine, all the expressions are generated through neural networks.
It's everything synthetic.
Now this is what we've done is we've connected her brain to the bat in the game Pong.
She doesn't know this, this is just like a baby sees its arm for the first time type thing playing around.
But she gets a reward every time she hits the bat.
So she's actually going to teach herself to play Pong.
So over time, this takes about an hour.
Now she's got mental control over the bat.
So that's a sort of example of the baby.
So it's basically creating a character which is embedded in the world, which is learning from the world, which is reacting to it.
So if we kind of move forward a little, hang on.
Yeah, so this character, so I'll just show you an adult model that we're working on.
So we're also, we don't just build babies, we're also building adults.
So this is called Ziza.
So she's a work in progress, so I'm just gonna fire her up.
Now, let's see, where is this?
Okay, so Zyza's basically first job is gonna be in the airport.
It doesn't require too much intelligence, but she's gonna be like, check you in and stuff.
So, but anyway, she's just being built at the moment, so I'll just show you a bit of stuff so you can see where we're at with it.
So, there she is. I don't know if you can see it. Yep.
And then we can, let's see, zoom in on her.
Let's see, wrong button.
So she's, if we look at the types of things, she's got all kinds of controls.
Now I'm just gonna find my mouse, it's gone.
Oh shit.
I've lost my mouse.
Oh there it is, it's over there.
This is weird, okay.
This, oh no, yeah, there.
And then, I'm just trying to get this control panel.
The mouse I can't see on the.
There we go.
Okay, so she can do various behaviors.
So we'll just get her saying something.
Hello.
How can I help you?
Thank you.
So all the time she's reacting to you, she's actually watching customers and attracting you.
These are just sort of canned things that she says.
I understand.
Yes.
Now, as she's doing all this sort of stuff, what we can do is we can start like messing around with her, you know, we can change her expression, you know.
Changes the whole sort of feel of what she's doing.
Like if I do something like, you know, like a bit disgusted, you know.
Sort of, so we can do all these things live. Now this is just literally changing sort of expressions and stuff like that.
But one of the other things that we can do is we can now start activating her own sort of...
Emotional neural networks.
So if we start doing things like this, if I start saying something...
I'll just wait for her to finish.
Okay.
Okay, there we go.
So now if we now start to say something sort of disappointing happens to her.
So these are all being generated in real time and they'll never be the same.
And basically, you know, these things still...
So she starts feeling bad about it.
until she just completely shuts down.
She's completely depressed.
And now we can do other sorts of things, like say something's happening.
She's not sure how she feels about that.
And then starts getting sort of irritated by it.
So these are all sorts of things which were being generated live.
Now we can kind of overlay everything.
Hello.
So she might not be...
How can I help you?
In all these different sorts of states.
Thank you.
And because we're in San Francisco at the Game Developers Conference, we've got some cool glasses for her so she looks like a game developer.
So there she is.
And so she can hear me and do various sorts of things.
So anyway, pretty much what I will...
So basically that's the sort of system overview and it's thanks to a fantastic team that have got that we can put all these things together.
So we're working on lots of different technologies which are all going to start coming together this year, so we've got a lot of goodies to show probably in about a year.
So some of this stuff is going to be out in the public in a few months.
So yes, thanks very much.
I think somehow we miraculously made it on time.
I just went through all the hiccups.
Sorry about the hiccups, guys.
Thank you guys for coming.
I guess we have like three minutes.
You guys, anybody have any questions?
No?
Awesome.
Thanks.
