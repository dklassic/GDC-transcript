Welcome to this presentation.
My name is Anne-Sophie Mongeau, and I'm a senior sound designer at Hazelight.
And I am Joakim Enik Kruiberg, technical sound designer at Hazelight.
And today we're both here to talk about how the Hazelight audio team developed the soundscape of It Takes Two.
And we will consider both the technical and the creative challenges that were brought by the split screen, but also that were brought by the creative vision for the game.
Before we get started, let's first acknowledge and give due credit to the members of the Hazelight audio team.
So everyone on this list has been essential to everything we're going to talk about today.
Myself and Joachim are here as representatives for the team, but this project has truly been a team effort.
This presentation is divided into two parts.
In the first part, we will talk about some challenges and found solutions while designing the split screen sound of It Takes Two.
This includes managing spatialization and maintaining mixed clarity through carefully thought out sound design strategies.
The second part will focus on two selected sound design highlights from the game soundscape, where we will demonstrate our inspiration, our creative vision, as well as our sound design approaches for those two highlights.
So let's dig in. First of all, what is It Takes Two?
It Takes Two is an action-adventure-platformer-splitscreen co-op game.
That means there are two main characters, Cody and May, played by two separate players.
And the screen will always be split in two, regardless of the players playing in the same room or online.
Cody and May are two human characters who are transformed into dolls at the very beginning of the game.
And this is basically the story of how they navigate this new fantasy slash magical world, which is based on the real world, in order to get back into their real bodies.
So overall you could describe It Takes Two and its creative vision as somewhere between Toy Story and Honey I Shrunk the Kids where all the usual everyday objects become huge in comparison to the characters tiny doll size.
In all those objects they come to life, they gain a personality and they play a role in the story either helping the characters or hindering them towards their goal.
So if you haven't played the game, I want to give you a few examples of how this vision is realized by showing you a few images of some of the objects or the characters you might meet throughout the story. You have a vacuum boss and all its vacuum hose tentacles.
You have a talking hammerhead guiding you through the level and is also used as a smashing ability.
A group of gangster squirrels sending you on a mission to kill giant wasps.
The giant wasp queen who is actually an imposter in a wasp costume.
An overly protected baboon determined to trap you into an outer space realm.
A train track turning into an infernal railcar ride.
A parallel kaleidoscope dimension.
Giant evil microphone snakes.
and so much more of that.
So in short, this game presented many challenges, but also many opportunities for audio.
And among those opportunities and challenges are the following.
Concerning the split screen, how to manage ambiences and other world sounds from both sides, how to maintain mixed clarity as well as emphasis on the right things in relation to what is happening at each given moment, How to enable two spaces to coexist and be represented at the same time, or one space to be represented from two different perspectives.
And about the creative vision, how to convey this sense of bigness of everything in relation to the character's size.
How to maintain both playfulness and realistic immersion to support the tone of the game.
And of course, how to live up to the latest animation sound design standards, such as Pixar, but in an interactive world.
The solutions and the strategies that we have developed to answer these challenges are both creative and technical, and they would not have been possible without a close collaboration between those two aspects.
That is why we are both here today.
So first, to introduce our split-screen sound design strategies, let's start by clearing up a few terms.
So the first concept to have understanding of as we learn about this game is that of the listener, which is a single world-bound point of reference from which we observe sounds in a three-dimensional space.
And looking at this picture right here in front of us, it's not really that hard to imagine a more conventional single-player game where that listener will likely be attached to the character or the camera and serve as that singular point of hearing.
But it Takes Two is not a conventional game.
And part of our split screen setup means that we always have two listener presence, one for each character.
So instead of adhering to the idea of a lone listener representing one absolute truth of hearing, we have to accurately represent both player perspectives as they are summed through one shared output.
And speaking about that output, it's worth mentioning that it takes two is delivered for a channel based format of 5.1 surrounding.
So we opted to not go for an object based approach.
And what we just said here is that this is the governing principle of audio, it takes two.
It is the fundamental law.
And for the people tasked with filling this fantastic world with sound, it was of utmost importance that one learned how to live within these restrictions that this imposed.
But more importantly, how to leverage the untold opportunities for us to tell new stories through sound and take the listening experience to places that the aforementioned conventional game couldn't really do.
So next up on concepts is that of 2D versus 3D.
So a concept of audio for games that we must have a firm grasp on is that of spatial audio.
So that is to say a sound rendered to convey a point of emission within that three dimensional space.
And commonly we would address those sounds as being 3D sounds.
And as a counterpoint to this spatial audio, we have sounds that are non-spatialized, which means that they do not appear to occupy space in the world so much as they occupy space on the screen through which the world is then viewed.
And we would refer to such sounds as being 2D sounds.
So a direct contrast to those that live within the actual game world.
And tackling the sound design of It Takes Two required a very conscious approach to these two different archetypes of sounds.
Indeed. So considering all that was just said about spatialized sounds, one of our very first brainstorm sessions in the very beginning of the project was about how we were going to manage spatialized and non-spatialized sounds, so both 2D and 3D for split-screen.
There simply weren't all that many good sounding split screen games out there that followed the similar type of gameplay and creative vision as us from which we could get inspiration.
So our research basically told us we had to come up with our own way of doing things and follow our own intuition about how the game should sound.
So to introduce our split screen strategies, I will start by talking about the ambience content in It Takes Two.
Because ambiences and ambient world sounds form the bed of what players hear when entering the game.
It was very important to us that we start by clarifying how those were going to behave from early on so that we could then build from that foundation.
So in It Takes Two, each player always has their own ambience independent from the other players representing where they are in the world.
All ambiences in the game are usually made from a combination of...
a quad wave file playing the role of bass surround ambience.
So that is a one four channel looping sound that includes as many details of the ambience as possible.
We consider the quads in It Takes Two to be 2D, especially considering that they are static in the sense that all four channels of the quad will always be routed to the same four channels to the speakers, regardless of the player's movements.
The reason for that is because of the split screen.
So just imagine if we did allow the quads to be spatialized.
That means if the player on the left side is moving around, let's say they turn to the left side, that means that the whole ambience bed shifts through the surround axis, which means some elements would go from the left side to the right side, basically on the other player's side.
This could easily become confusing for the player on the right side.
We simply thought that it would feel weird for both players to have their ambience moving that way.
in the context of the split screen, that is.
And the soundscape of each text too simply didn't benefit from this added layer of complexity.
So then we have also randomly generated spot sounds.
And those are helpful to add a bit of variety and randomness to the quads and help to world-dyes the ambience elements.
So randomly generated spot sounds, or as the name says, spontaneous isolated ambience elements spawned at random 3D locations within the player's environment.
And those we consider to be 3D.
And what they include, of course, depends on the environment itself.
But common examples might be wood creaks, random animals and insects, dust, wind gusts, rustles, things like that.
On top of those random spot sounds, we have also manually placed spot sounds.
And those are of a similar nature, so also 3D in the world, but they are placed manually in the level to match the visuals with a bit more precision.
So for instance, extra wind gusts where there might be VFX to support the visual feedback.
Same for leaves rustling or water lapping around water areas.
And finally, there are objects from the world emitting their own sounds.
So those are also 3D and can include, for instance, critters or animals passing by, could be machines or fans or moving platforms, basically any object that belongs to the world and is emitting their own sets of sounds.
So all of this put together creates a consistently rich atmosphere without any dead spots.
But that was only for one player.
So then comes the question of how to convey all of this and maintain the clarity with two players.
So before getting really into our technical solutions, let's first listen to a video that will show how these described ambience layers are combined into the game and what the sounding result is.
So in this video, you will first hear the left ambience, so the quad on Mei's side, and then the right quad will be added on Cody's side.
And then we'll one by one add the other ambience layers just as described.
So the randomly generated spot sounds, you can listen in this case mostly for frogs, crickets and flies, which will be highlighted by the debug on the screen.
And then some manually placed spot sounds, we have some water lapping around May, some leaves rustling around Cody.
And finally, the critters are added.
So in this case, we have dragonflies on May's side and some ladybugs on Cody's side.
And the end result is a fuller mix of ambient sounds.
And after the video, we'll start digging into how we made this work in split screen, along with how our strategies for these ambient sounds applied to all other types of sounds as well.
So as we might begun to see, the gameplay of it takes two is incredibly varied and diverse.
The most common case though will have the screen split vertically right down the middle with both characters views getting equal screen space there. And this very rigid scene view really defined our design choices all throughout the project.
And before we had even begun putting sounds into the engine, we started having long discussions on how to manage our screen space.
And this process of learning on how to build for our viewport was ongoing for the full project.
And it became a concept that I like to refer to as sonic real estate.
For It Takes Two, Sonic Real Estate meant that we at all times adjust audio content to fit within the given visual frame by using carefully placed spatial and non-spatial sounds that each occupy compartmentalized spaces on the screen.
So in much simpler terms, it means that every sound is placed based on its visual position on the screen.
And again, for the sake of clarity, it takes two uses a channel-based format, 5.1 surround, not any object-based audio.
So now let's take a look at a video that exemplifies Sonic Real Estate by showing how the soundscapes of the individual player frames contributes to the final shared output.
So we'll watch a video now and at specific points throughout, we will transition from hearing the full mix of the game to hearing isolated layers of ambience and reverb, which will additionally also contain some voiceover.
And now we really encourage you to pay close attention to the sounds that you hear and try to think about how they are placed on the screen.
Is that water? What is this machine?
Uh, who cares? Water pillars are cool.
Help me with this wrench, Cody.
So the first method developed to help us establish that groundwork of spatial placement is found in a basic technique aimed to play sounds on the horizontal plane.
This approach, called speaker panning, is one of the most fundamental concepts of audio processing, even in linear media, but in It Takes Two, it became a very impactful tool. So the mindset we employed when we used it was that speaker panning served to impose kind of a soft ownership of sounds towards either player based on which player interacted with that moment at any, at that object, I should say, at any given moment.
And this relationship was not at all static.
Instead, we always reacted to how object player relationships change during gameplay and then pan those sounds towards the side of that player's viewport.
So in essence, May's world is a little bit more to the left and Cody's is a little bit more to the right.
And while this basic technique was instrumental in outlining the fundamentals of the mix, it was really only applicable to our non-spatialized sounds.
So again, our 2D sounds.
Taming spatialized sounds in a way that would allow us to control the sonic real estate like we wanted to would prove to be a much bigger task.
And we'll return to that topic after Anne-Sophie has dealt a little bit deeper into what world dicing it takes to meant.
So just to summarize what we've spoken about so far, what we've managed at this point is to build a foundation for a split-screen audio system.
We've established a degree of ownership of the sounds through speaker panning by sending each player's respective sounds slightly more towards their own sides.
We've also set a few ground rules regarding sonic real estate, so which sounds should be 2D or 3D and how much of the screen space they should be allowed to take in relation to gameplay.
And we have carefully designed ambiences and other world sounds with as many details as possible in order to reinforce each player's environment and their perspective in it. But with split-screen audio, due to the nature of the medium and the fact that two environments need to be portrayed at the same time, we felt like we needed to take every possible step towards greater immersion and towards selling the world in order to make it feel natural and believable.
which ultimately we think helps to minimize the distraction that the split screen might cause.
And so due to the split screen itself, we needed to do that with some degree of subtlety.
And so that meant not by trying to add more stuff, which might quickly start to compromise the clarity of the soundscape, but rather to make sure that the sounds that we do choose to include contribute to bringing the entire world to life. We do that through worldizing strategies.
For it Takes Two, our worldizing strategies were these.
Using convincing reverb, that meant convolution reverb with impulse responses that we recorded and designed ourselves.
And 100% of the game sounds, in-game sounds, are going through the environment reverb.
We also have real-time environment-based delay reflections.
So thanks to our Raycast and environment type systems, which allow the game to know in what type of environment the player is currently in.
So is it, for example, a small, a large, indoors, outdoors, and so on.
and sounds that are sent to the delay include a veal, foley, weapons, gadgets and vehicles.
And we also have sound designed baked reflection tails that are added to sounds like weapons, explosions and other large sounds and even footsteps sometimes.
So to illustrate these worldizing strategies and how they contribute to the soundscape, we'll listen to a video showing how the world sounds with and without them.
So you'll first hear the awesome weapons and explosion sound designed by Philip Erickson, where there will be initially no worldizing elements.
So there will be no reverb, no delay, and no reflection tails.
And then you will hear the same weapons and explosions with the added baked reflection tails.
And finally with the reverb and the delay added as well.
The video will also then show how the VO, foley and other environment sounds may be affected by the environment type, the reverb and the delay in real time.
I wonder where that annoying book even came from.
He said Rose bought him.
He doesn't sound like a book she would buy.
More like a pretentious self-help book you'd buy.
You know, self-help books are very important to some people, okay?
Forget about the book. Let's get to Rose.
Rosie, get inside!
Please be gentle.
It smells moldy.
Yeah, humid down here.
Did you see that, Cody?
I saw something. What was it?
I couldn't tell.
Maybe a rat.
No, it looked huge.
All right, thank you, Anne-Sophie.
So like I previously said, our speaker panning solution and our ambient system, it allowed us to bring the split screen mix and balance to a good level.
But we still experienced spatialization issues that were compromising the immersion.
And this was a result of our two listeners.
The problem was twofold.
First problem was that we suffered from an unwanted increase in volume when both players observed the same spatialized sound source simultaneously.
You see that visualized here.
The problem was in the way that WISE inherently combined channels when a single sound source was being picked up by both listeners at the same time.
The result was that for situations where both players experienced a sound from an identical or near identical point of reference to one another, we would double the signal ratio per channel, which translates to about six decibels of increased output.
So in layman's terms, two players listening to the same spatialized sound would make it louder.
compared to it being heard by only one player.
This unwanted behavior was a massive thorn in the side for us.
It would make it virtually impossible to mix this game.
And we had recognized its unpredictable implication on the mix early on in our design.
And our first response to balance the volume output was to implement and continuously update a parameter based on listener proximities to the sound source.
So slightly reducing output volume when both listeners were close.
effectively counteracting the added intensity from the double signal.
But this was very much nothing more than an ugly fix to an even uglier problem.
And we always knew that we had to go deeper to get a result that we would be happy with.
The second problem was that attempting to use one output device to represent one sound source, but from the perspective of two listeners, always results in a third perspective that is the sum of both but speaks the truth of none.
And this some perspective meant that not only was the sound of this explosion, for instance, louder than it should have been because it sums the signal from both players, but also that spatial positioning was not always truthful due to it only considering the closest player.
So, for example, if May is standing closer to that explosion, but is looking slightly to the left, the explosion will be perceived as coming from the right, even though Cody might be looking straight at it.
And we already knew that the missing piece of this puzzle here was a result of how spatialized sounds were considered in our two-listener setup.
Energy distribution between the output channels was a summed result of both listeners, which means that the represented listener position was always a compromise of both.
And we see that in this graph right here.
One listener could have a sound source dead ahead.
But if the other listener was closer to that sound and fully turned away, then that sound would still appear to have the presence in the rear speakers of the surround setup.
So we decided that we needed to rewrite the rule set of how spatial light panning worked in our game and make fundamental changes to how we rendered output based on our two listeners.
This took a bit of shifting of a mindset.
So instead of pursuing the idea of always fully representing both listeners, we instead decided that we wanted to create a soundscape where a spatialized sound was represented from the perspective of the most attentive and relevant out of the two listeners.
By constantly choosing just one out of our two perspectives and omitting the other one, then anyone's sound source would always be rendered in a way that made sense.
And this new rule of spatialized audio in multiple listeners is what we decided to implement in an approach that we've simply called spatial panning.
So we'll now take a look at a video to exemplify said spatial panning in action.
So we have a sequence here and we'll listen to a series of explosions as they're happening around the players who will observe them from two different perspectives.
We will run the exact same sequence twice, first with just the default panning, and then once more with our spatial panning engaged.
And to help you out here, also visible on the screen, it will be a volume meter showing the overall audio output.
So again, now try to listen closely to the difference in how you might perceive the locations of the explosions as they're happening on the screen, and more so how the addition of the spatial panning affects it.
So what you just saw and hopefully heard was how spatial panning serves to improve accuracy in the mix.
Its purpose is to take attenuation, so that is how a signal reduces in power over distance, take that and decouple it from the panning.
So it works in such a way that whenever more than one listener is actively mixing the same sound source, we begin to look at and compare those listeners individual relation to that sound.
The listener, the closest listener is chosen as a reference point for the attenuation and then the listener with the closest forward angle to the position of the sound source will be our observer for panning. Together these two structures are combined into our one final listener output.
So in this slide right here, we see an example of a situation that without spatial panning produced very questionable results with regards to the idea of sonic real estate.
One sound source, two listeners.
One listener looking straight at the source, one listener with its back fully turned.
But the combination of the two would sometimes produce as much signal in the rear speakers of a surround setup as it would in the speakers in the front.
even though it can only be seen by the player looking directly at it, and who of course would expect this sound to be heard as if it's playing from the front.
But spatial panning solves this for us by in effect linking to what we hear to what is on screen, and in doing so allowing those big sounds that really should fill the space provided by the surround channels to do so without being muddied by unwanted and untamed spatial sources.
To make this whole procedure as transparent as possible, we introduced interpolation to help smoothen out any drastic changes in listener sound relationships.
And a challenge in this was to make the panning update fast enough to be responsive to gameplay, but still smoothly enough to not make for any jarring jumps in panning or amplitude.
That's not something that we could allow.
But in the end of it all, spatial panning was a major benefactor to the soundscape of It Takes Two.
allowing the spatial sounds to be rendered in a coherent and world-based way, even when in range of both listeners, and at the same time, help maintaining that all-important clarity in the mix.
And the final output mix, and just pack everything together nicely.
So to summarize what we've added to our split screen strategies over the last few slides, on top of the solutions we've already spoken about, like our speaker panning solution, our sonic real estate management and our carefully designed ambiences, we have added worldizing strategies such as convolution reverb, real-time delay reflections, space reflection tails, and of course spatial panning that compensates for those spatial discrepancies.
But when it comes to developing audio for split screen, for all of those solutions to really live up to their full potential, it was important to be clever about our actual sound design strategies and to choose what were the elements that were the most important and that needed to be represented as such.
And I like to call those the elements of subjective importance.
meaning that we choose based on our own judgment, so subjective, which sounds are the most important in relation to all other game sounds and to what is happening on screen and in gameplay.
And those elements are then prioritized in both the design, sound design, and the mix, so that they come through clearly, and so that the player is getting the information, the feedback, and the story that they need from their actions and from the narrative development.
And this is where we relied on our Pixar inspiration and the idea of bigness and exaggeration as a basis for our creative vision.
Bigness was very important in It Takes Two, because in order for that contrast of the characters being tiny in a big world to come through, objects that were meant to be large in comparison to the players needed to sound even larger.
So in the end the elements that we decided were the most important were designed to be bigger, larger and more powerful than any other sound playing at the same time.
And by focusing in such a way on one important element at a time, it helps in keeping to a minimum the chaos that can come from busy sequences such as combat scenarios.
We also made an effort whenever possible to keep those important elements within the screen, and by that I mean within the left-center right speakers, rather than having them spatialized across the full surround axis, so that they really would remain in focus and in the frame of the screen space.
And that way, we believe that this important sound would be perceived as more cinematic and would be allowed to take more space in the mix.
In other words, just imagine a sound made for a movie, stuck to the screen.
So for sounding events that remain within the screen visually, we preferred to simply leave them 2D as opposed to 3D.
And finally, this bigness in the sound design was complemented with the use of HDR in Wwise to help us maintain this clarity at all times.
And just to make sure everybody understands what I'm talking about, HDR refers to high dynamic range.
And in game audio that means we continuously look at what is playing and prioritize some voices over others based on set conditions that are predetermined in the audio engine, and for us that means wise.
And this is meant to create the illusion that the dynamic range heard in the game is just as wide as the one heard in real life, even if that is not true.
So for instance, by lowering the volume of a category of voices, such as ambient sounds, while other types of more powerful sounds are playing, such as explosions, so that the explosions is perceived as being much louder than the rest. And to translate this into our game, you could say that those elements of subjective importance that I was talking about will always be given a higher HDR value than, for example, ambiences or other less important sounds.
so that the important elements are always heard clearly.
So if you want to learn more about HDR and especially about how it worked in Wwise and for this project, I recommend you simply visit Audio Kinetic's help page about it.
So now the video we're about to watch shows a few examples of how we approach the sound design and the mix for those elements of subjective importance.
You can listen carefully to the sounds that are highlighted in the text as being the most important ones and notice how everything else in the soundscape seems to disappear.
And that is the result of our conscious choice so that these elements become the true focus and so that the confusion due to the split screen is minimized.
We basically use sound for narrative purposes.
We tell the player what is important, what to look at, and what to pay attention to in relation to the gameplay.
I can see the key.
Thanks for watching!
Wow, I'm starting to like this place.
So before moving on to the second part of this presentation, let's summarize what we've said so far about the split screen sound of It Takes Two.
Two listeners sharing the same screen meant that we needed to represent both players' perspective while maintaining a clear mix at all times.
We did so by using a speaker panning system that panned sounds coming from each player's environment slightly more towards their respective sides, by compensating for spatialization discrepancies through spatial panning.
By designing detailed and rich ambiences and environment sounds that represent the world in as convincing a way as possible.
By relying on worldizing strategies to make the world more immersive and reinforce the sense of being there.
By using HDR and by considering the elements of subjective importance when sound designing and mixing.
Now that we have described our main sound design strategies when it comes to the split screen, we will enter the second part of this presentation, and this part will further our demonstration of the sound design for It Takes Two by presenting two selected sound design highlights, which have also required a very tight collaboration between technical solution and creative content.
The first highlight that we chose to share with you is called the time controllability.
So across one level called the Cuckoo Clock, where all puzzle mechanics are time-based, Cody is given the ability to control selected objects in time.
And this means that when he activates his ability, he's able to scroll through a timeline and move the selected object back and forth within this timeline, and can also pause it and basically have time stand still for that object.
So it's 100% based on player input.
So here's a short video first showing what this ability does in the game, and then we'll talk about how we made it work.
Thanks for watching!
So to make this ability work, we considered various complex solutions, including granular synthesizers and other custom tools to be able to read through a sound file in both directions.
But we eventually put aside all those grand aspirations when we realized that the best sounding solution was not to develop a system that allows to manipulate and play a sound file in all sorts of ways, but to actually sound design both the forward and the backward sounds as two separate sounds, and have full control over what ends up being the sounding result of scrolling through that timeline, as opposed to designing just one forward sound and hoping it sounds good when we manipulate it in all directions.
So in order to be able to do that, what it came down to was actually much simpler than what we originally thought would be needed for such an ability.
So all we needed to know from the game was the scrolling direction, so is the player scrolling forward or backward?
And the scrolling progress, so where in the timeline is the player currently scrolling?
And by sending those values in Y as RTPCs, so as parameters, It was then fairly easy to structure our sounds based on both the direction and the progress.
And so the layers to build a time controllable interaction in their simplest form usually look something like this.
We have one forward and one backward sound that are the exact same duration as the timeline, and those are two distinct sounds, each designed separately to fit the picture as well as possible.
We also have one forward and one backward tail sounds, and those will play at the end of the timeline in both directions, and are also designed as two separate sounds to match the visuals.
And finally the looping sounds, used when scrolling is paused within the timeline and the object appears to be frozen in time.
So there can be many looping sounds, basically as many as needed.
For instance, there can be different sounds based on the direction.
So was the player scrolling forward or backward prior to pausing?
And there can be different sounds based on progress.
For instance, to have more intense or busy sounds playing if the timeline was paused in the middle of a destruction sequence as opposed to during a quieter moment.
In addition to the sounds specific to one time-controllable object, Cody is also manipulating a clock and has his own time-controllability sounds that will always play no matter which object the player is interacting with.
The ability sounds follow the exact same logic as the object sounds, so they have forward, backward, looping sounds, and tails.
So as shown in this picture, the object sounds usually come from the actual object being time-controlled, and the ability sounds come from Cody, who is manipulating the clock.
And this helps to bring more depth to the overall time controllability sounds, as well as consistency throughout the level and through the various interactions, because it gives the ability something that is recognizable, and then becomes quickly associated with that action of scrolling through time.
So to understand this a little bit better, we'll watch this video isolating the various layers, as I just explained, and then we'll move on to our second sound design highlight.
Thanks for watching!
So the second highlight that we wanted to bring you guys is called the singing ability.
So in the music level, where all gameplay mechanics are driven by the background music, Mace singing is used to solve puzzles and progress through the game.
The challenges here were that the abilities completely interactive, just like the time control ability, it's based on player input.
and that it must always be compatible musically with the currently playing background music track.
So this is a perfect time for us to really acknowledge the composition from Gustav and Kristoffer, our two composers for the game, responsible for all the fantastic music that you hear throughout, and also Sanja Rajabi, who is the vocalist who performed May's singing for this ability.
So let's start this all by watching a short video showing you how this ability works within the game, and then we'll delve deeper into how we handle it from a technical standpoint.
Oh So obviously we knew that we had to do something spectacular with regards to this ability.
The requirements that I outlined previously and the whole way that this ability interacted with the actual music made us sit down and build glorious plans on how to make the singing always feel dynamic and musical.
So we decided that we had to build a sampler.
We started this by drawing up a sheet that outlined the kind of audio content we needed to make the sampler come alive.
The conclusion that we came to was that for each scale of each music track, for each four sub levels, we would then sample each note of the scale with three different intensities ranging from low to high, so that the like the velocity setting on an actual sampler, this would allow us to have some control over the energy and the singing based on the gameplay circumstances.
And to keep the vocals from ever feeling static or robotic, we would also record three different variations of attack and release per note.
And then lastly, to follow the key of the current music, May would need to be able to go between sampled notes whilst singing.
So to realize this in a way that felt natural, we would include additional layers of glissando covering each interval of going from every note to every note in the scale.
And if you're trying to do math in your head right now, let me just end it by saying that we ended up in source material that would have ended up in thousands of files.
And furthermore, at this point in time, we still hadn't sorted the question of who would actually perform the singing to be recorded for these assets.
And after Filip and I spent two days mocking up a track just to try and see what it would take to see our grand plans through to success, it was very clear that we had given ourselves a mammoth of a task.
And so we did something that we do not always do in the Hayslide Audio team.
We asked ourselves, are we being reasonable right now?
And so we took it back a step.
A big finding in our prototyping is how clear it became to us that for this feature to be interesting, keeping the singing in the correct key would really only get us halfway there.
In our plans to record the different type of source we thought we needed, we haven't really thought about the importance of allowing the recording singer to perform to the vocal track in a natural and musical way.
And so we quickly formulated a new plan.
Instead of building a sampler, we would have each music track of the game be accompanied by an actual vocal track.
Implementing the singing in this way means that we could allow our vocalist to perform to the music in the same way that you would approach any conventional song.
And just like that, the musicality of it all was back.
Now, to make sure that our systems could stay synced with the performance of the singer, we came up with a method where we, our composers would, I should say, for every music track, create a MIDI track that would represent the lead melody performed by the vocals.
The MIDI data then was parsed as wave markers and ingrained into the source files of the singing, readily available to be read by the game engine. So the result now is that every track has a full-fledged vocalist track with all the musical bells and whistles. And whenever the singing ability was activated by the player, this track would be introduced into the mix, playing alongside the background music.
And by using the data parsed into the markers from the MIDI track, we could design around specific parts of every vocal track to make sure that whenever singing stopped, whenever the player stopped using the ability, the singing would fade out nicely based on the current passage that was playing. So instead of an incredibly complex sampler-like system that likely would have taken us weeks, if not months, to deliver on, we built a really, really nice volume slider.
So a tricky aspect of our original plan had always been ensuring that whenever the singing stopped, it would fade out in a natural way, since that happens at a moment's notice, whenever the player decides to stop the ability.
And short notes here were especially hard to keep from sounding bad when stopping.
But our Marker Program fader really came through in solving that issue for us.
So now in the following video here, you will see the effect of these markers as we progress through the music of the singing.
And pay attention to the top of the screen, you will see gray bars that shows the markers and each of them denotes if the vocal track is currently allowed to stop when the player is no longer singing, and if so, how quickly it should fade out.
And finally, here are a few summary points and lessons learned to conclude this presentation.
The key words surrounding the story of creating the soundscape of It Takes Two include deterministic, we left as little as possible to chance, and as much as possible to remain in our control.
So for example, by designing sounds made to fit the screen space as opposed to aiming for a fully spatialized mix and just hope for the best.
We wanted to make sure both players would always experience the game as nicely as possible, and that meant taking control over a few things.
So for example, by designing important sounds in 2D, meaning leaving them in the left-center-right speakers instead of spatializing them and risking them ending up sounding from the wrong side of the screen. The same goes for leaving our quad ambience static. While a different approach might have been beneficial for a different type of game, this deterministic strategy has been essential in creating the sound for It Takes Two.
And then the simpler the better.
The design of both the sounds and the systems needed to start simple and to be built upon based on pure necessity, as opposed to designing complex systems and complex environments just because we can.
So what matters is really the result.
Our experience in It Takes Two has shown us.
that the most complex solutions are not always the best sounding result.
And both the time control ability and the singing ability are excellent examples.
Another example is choosing to deliver this game in stereo in 5.1 as opposed to object-based.
And this was based entirely on what we believed would do the greater service to the split-screen soundscape and the clarity of the mix.
And then prioritization.
In a split-screen game, if everything is audible at the same time, it just becomes messy.
So maintaining clarity is all about prioritizing and choosing what is important.
And finally, teamwork.
In our team, we rely on each other's strengths and skill sets to raise the bar to the highest level.
Our collaboration between the technical and the creative designers has been truly essential to bring this project to success.
And finally, it's important to say that all our solutions worked very nicely for this specific game, but split-screen audio is truly a Pandora's box, and a different project might require entirely different solutions, but that with those takeaways, we are confident that we are ready for the next challenge.
Thank you very much for attending this presentation, and we hope you enjoyed it.
You just stay right where you are, San Francisco. See you soon.
