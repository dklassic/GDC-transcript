Thanks everyone for being here virtually.
This is definitely a very interesting time.
And a little bit about myself.
My name is Norman and I'm the CEO of Glassbox.
We're a company that specializes in making real-time production tools.
And we have a global team with offices in Germany, Los Angeles, Australia, and in Asia.
Today I'm going to be talking about the democratization of virtual production for indie game makers.
which is brought about through a convergence of cinema and gaming technologies.
And you may have seen some of this in the works of large projects such as The Lion King, where they made use of game engine technologies and filmmaking equipment to make an animated film, almost as if they were making a real film.
And more recently, we have projects such as The Mandalorian that broke new ground on how to combine real-time.
engines that are traditionally made for video games to transform how film and visual effects are made.
But before we go into that, I kind of want to take a step back and think about the history of cinematic gaming. We've been with this term for a long time now and we as an industry doesn't really have an agreement on what cinematic gaming really is meant to be.
And here are some examples of attempts to making games cinematic.
And this includes certain things like using live action, live action cinematics, like in the case of Resident Evil and Red Alert.
But then there's also the idea of cinematic gaming being more about the content and the presentation and the subject matter, like is the case with Detroit Become Human, as well as other games like The Last of Us, Red Dead Redemption, The Witcher, and got a war.
But strictly speaking, I kind of want to think about what makes games cinematic and also the evolution of the technology that are enabling cinematic gaming.
So, adopting a very specific look at cinematics, we're just talking about game cinematics here.
So, here we have an example of Starcraft Brew War, which is pre-rendered, produced using the same tools that are available for animation studios and visual effects studios.
And this was in 1998.
And now, in 2015, for StarCraft The Legacy of the Void, we have Blizzard also using tools and technologies available to animators and visual effects studios, but they're able to create a much, much higher visual quality because of the evolution of the technology and the evolution of the tools, and also the evolution of the artists themselves.
But here's a really interesting intersection of real-time cinematics or in-engine cinematics, where we're actually making use of game engine technologies to render basically game footage, but to a quality that approximates the pre-render cinematics of yesteryear.
I want to talk about cinema.
When we talk about cinematic, we have to talk about how cinematic references the idea of cinema.
And in the area of cinema, there is the technology called virtual production.
And it really is about making use of technology to emulate certain aspects of filmmaking.
instead of using a physical camera to shoot a subject, a physical subject on a physical set, we're now using virtual camera to shoot virtual actors on a virtual set.
And the beginning of that comes from things like photo real, digital humans, virtual cinematography and performance capture, such as is the case with Final Fantasy in 2001, Polar Express, Beowulf, then also.
The film that made all of this prolific is Avatar.
And when we're looking at Avatar, we're looking at an entirely new way of making films.
And here is an example of a virtual camera.
And this virtual camera is basically virtual reality without a headset.
It'll allow the director to look into the screen as if they're looking onto a world that doesn't exist.
Then there's performance capture, where through the use of technology, we're digitizing the performance of an actor, not just how they move, but also their emotion.
And the third piece of this puzzle is real-time visualization, where in order for the director to see what is being shot, they needed a way to interactively render the images so that the director could see it as they're being generated.
And the combination of these technologies is one main avatar possible.
And this technology very much still persists today where we're using the combination of cinematography, performance capture and real time visualization to drive a new generation of virtual production.
Take for example, Battle Angel Alita, where this is very much kind of the latest in how this technology was used, where we're using performance capture technology to digitize the performance of the actor, and then we're shooting it with a virtual camera and combining with virtual sets.
And this is especially sophisticated because we're also merging real life elements into it.
So it's not just virtual actors on virtual sets, it's the virtual and the real combining together to make a seamless whole.
Now that we looked at cinema, I wanna take a step back to looking at games.
And for a while, I think people have been toying with making use of gaming technology to make film.
And this really began with machinima, where players are making use of gameplay footage or gameplay technology to record content that is not games.
So we have some really well-known examples like Red vs. Blue and the series of highly popular shorts that made by Valve for the promotion of Team Fortress 2.
As I touched on before, this technology has definitely evolved.
in the last decade.
In such that here we have two comparisons of the same scene.
Both of these games, a Meadowvano Allied Assault and Call of Duty World War II are both depicting the Normandy landing.
But on the left, we have one generation of technology, whereas on the right, we have an entirely different generation of technology capable of producing visual qualities, the likes of which that was just simply not attainable previously.
So where does that leave us?
Well, now that the technology behind real time rendering has led to an exponential increase in the computing power available to us.
And we have real-time rendering that are approaching the level of pre-rendered animations from last decade.
We're sort of looking into an area where the filmmaking tools that were somewhat originally meant for very niche productions beginning to be applicable for a broad range of users.
And here are two examples of some of the tools that were made in Game Engine, dedicated to using Game Engine as a filmmaking tool, that maybe a lot of us haven't really come across or used ourselves.
This is where it leads to kind of the next and the final topic I wanna talk about, which is the convergence of technology.
Where Technology meant for virtual reality and broadcast is coming together to create what we now call real-time visual effects.
And going further than that, we're now making use of game engine technologies to render virtual images that we directly apply to cinematic production, to animation production, to commercials, and more.
and the latest advances of using virtually rendered images on LED walls to facilitate what is now called in-camera visual effects. And some of the most recent advances in real-time visual effects is in the production of animated features directly using game engine technology and also in advertisement.
But perhaps the most influential is the use of real-time technology to render what's now called in-camera visual effects, where in combination of display technology, we are displaying the virtual world together with all the live-action components so that the virtual visual effects is a part of the camera plate.
So the crew can walk away from set with near final footage, with minimal work in post-production, in a bid to dramatically shorten the time it takes from conception to final frame.
And the best example of this is perhaps the Mandalorian.
where they use a very large LED volume to kind of create the effect of surrounding the film crew inside of this virtual world that doesn't exist.
And then when you're looking through the camera, it is almost as if everything is together in one place.
But not everyone has access to Mandalorian tech.
And ultimately, the real changes comes from the advances in latest technology, making the great ideas from a decade ago cheaper, faster, and more available.
And that leads to the topic of democratization.
And democratization, we're talking about affordability, availability, and the usability of the technology.
Like for example, we see this general trend of highly specialized hardware becoming increasingly more available and highly specialized tools becoming increasingly more available.
And generally we're kind of moving from something that would cost millions to something that would cost thousands.
Take for example over here, we have a specialist real-time renderer that was bespoke to the project all the way through to something that was running on a consumer device.
And likewise here, on the one hand, we have highly specialized camera tracking and camera robotics through to motion capture technology.
And on the low end, we have consumer-centric augmented reality and virtual reality technology that approximates the capabilities of the tools that used to cost tens of thousands of dollars.
And this is also true for performance capture, where we're talking about highly studio-specific pipelines on the one hand, professional tools in the middle, and also performance capture using consumer-centric devices like the ARKit on the lower end.
And for example, of how this new democratized virtual production technology is being used by independent studio to create content.
Here is Infinite Reality, who is working on the latest project, Morad.
MoRAV is essentially a giant robot military drama where a global arms race for building robots sparks World War III.
Even though there's a lot of these amazing giant robots, it's very important that the story remain about the people and the crews of the giant robots.
So the seamless integration between visual effects and the story including the robots is very, very important.
Since partnering up with Fonko Studios and Fawn Davis, we've been able to take some of his miniature creations and digitize those for the game engine.
The real-time visual effects pipeline in the Unreal Engine has been a game changer for us.
You know, we can build these effects and particle sims right out of the box, and we can see these things in real time.
So once we have the boards all worked out, we're going to plan our performance capture.
With a combination of Glassbox DragonFly camera system, we'll take FaceWear for our facial performance capture, Perception Neuron StudioSuit to do our motion capture.
We can put out hundreds of shots in a day to experiment with what direction we want to take our storyboards and our final edits.
This would have taken weeks before animating cameras by hand and trying to set up our shots.
But because of these virtual production tools.
That changes everything and we can do this in a day.
Big benefit of using real time for me is how fast that we can put scenes together, shots together, you know, with our basic VFX pipeline.
works the same, we get the animation out, the models out, but now we're not going into this post-processing part of the pipeline.
It's as we're creating it, we're actually putting it into the scene, and we're seeing it come to life immediately.
Special thanks to Dean and his team for allowing us to use the materials from the MORAD project.
And if you have any further questions, sorry I can't take audience questions given the circumstances, but you can visit our website and send us an email if you have any further questions.
Thank you and best of luck.
