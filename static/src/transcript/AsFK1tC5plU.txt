Hello and welcome to Loud and Clear, Improving Accessibility for Low Vision Players in Cosmonius High.
My name is Peter Galbraith.
I'm the Senior Accessibility Engineer at Alchemy Labs.
I've been with Alchemy for seven and a half years and have worked on all of the VR titles our team has shipped.
I am also the primary programmer and one of the main designers for the update we will be discussing today.
And hello, everyone.
I'm Jasmine Cano, the Senior Accessibility Product Manager at Alchemy Labs, where I've worked for two years.
I own the production on the project we'll be discussing today.
I mainly worked on design and leading plate testing.
Alchemy Labs is an amazing team with around 70 owls.
It's thanks to everyone's efforts and commitment to accessibility that we can be here today to share this with you.
Accessibility is core to what we do and is a big part of Alchemy's mission of VR for everyone.
You might recognize Alchemy Labs from our previous VR games, Job Simulator, Vacation Simulator.
While those games contain a variety of accessibility features, we're here today to talk to you about a specific update to our most recently released game, Cosmonius High.
Cosmonius High is a VR game where you crash land into your first day at an alien high school.
You unlock powers that help you compete class, or sorry, complete classes.
make friends, and save the school from cosmic chaos.
and this talk's focus will be about the game's vision accessibility update.
We'll go over the challenges we faced and the design choices we made.
We also want to acknowledge that when most people think of VR, they think of putting on a headset and being immersed in the visual experience.
It makes sense why they imagine this.
The headset goes over their eyes after all.
So why even try to add vision accessibility to a VR game?
Well, the answer to that is VR has been largely inaccessible to blind and low vision players.
But we believe it doesn't have to stay that way.
There's more to VR than simply what is on the screen.
Just like there's more to life than what can be experienced through visuals alone.
And everyone should be able to experience all of what VR has to offer.
So with that in mind, we set ourselves a lofty goal.
We wanted to allow people with partial or total vision loss to play and enjoy all of Cosmonius High without sighted assistance.
When we started this project, we weren't sure if we could make the entire game accessible to blind or low vision players.
But in our mission to make VR for everyone, we knew we had to try.
So we started by asking ourselves the big questions.
What challenges do blind players face when trying to enjoy VR?
And what barriers can we remove for them?
Before we started the project, we reached out to Steve Saylor to be our primary accessibility consultant on the project.
In our first meeting, he outlined potential challenges blind players might face, information they might need, and possibly interactions to try in VR.
This helped inform many of the early versions of features we'll be talking about today.
Since no one had ever tried something like this on the scale of a commercial VR game before, we knew theory would only get us so far.
What was most clear to us was that blind players had less access to important information than sighted players.
So the first problem we decided to tackle was the inequality of knowledge.
Blind players were missing crucial information about the world due to how much was conveyed through visuals alone.
From the type of objects and interactions available to whether or not the player was holding an item, there was a clear gap in knowledge that was going to hold players back.
If we were going to make the game accessible, we knew we needed to level the playing field.
When we started asking ourselves, what information do players need?
We also asked ourselves, how do we deliver that information to them?
The former would turn out to be a complex question.
Fortunately, though, we already had an idea for the latter.
Our proposed solution was to provide detailed information about the virtual world to the player through descriptive audio.
But unlike movies where a single, unchanging descriptive audio track could be used to aid in the experience, we were going to need audio to react dynamically to wherever the player chose to go and whatever the player chose to do.
And the obvious candidate tool to help us achieve that was text-to-speech.
Text-to-speech has been a tool used in digital accessibility for many years.
Almost every modern computer or smartphone has built-in screen reader functionality.
And more recently, we have begun to see text-to-speech used in games to help aid players through setup and menu navigation.
Impressed by the work others have done with TTS in their games, we wanted to find out if we could take these ideas a step further.
we decided to try to implement our own made-for-VR version of the Accessibility Object Model, AOM for short.
AOM is a framework designed to make webpages compatible with assistive technologies.
However, instead of the contents of a 2D webpage, our implementation would attempt to expand the concept to a fully immersive and interactive 3D world.
We first started this process by implementing a tool for working with text-to-speech.
We then made it so that we could send that tool information about the items a player could pick up.
Anytime a player hovered over, grabbed, dropped, or pointed at an object with either of their hands, that object would have its text sent to the text-to-speech tool.
This text was actually pulled from a pre-existing system in our game called the World Item System, which contained information such as the name and a comedic in-game description of the objects.
Then, we would play back the generated audio and voila!
We now had audio describing all of our interactive objects.
Well, not all of our interactive objects.
We quickly realized that we had other interactive objects that weren't able to be picked up, but still required descriptions, so we got those hooked into the tool as well.
At the same time, we also developed a new system that we called the Describable Item System that would allow text that appeared on objects to be read, such as the text on posters, book pages, and journals found around the school.
This also allowed us to describe other interactive objects where adding a world item wasn't possible.
This was our equivalent of adding alt text to images.
So now we could have TTS on nearly all interactive and important objects in the environment.
At this point, surely we were done, right?
Well, when we began internal testing, we realized that we had overlooked some major aspects of the game.
The most notable example was our tutorial pop-ups that were just animated pictures and didn't use the world item system and wouldn't work with the describable item system.
So after a bit of reworking and tweaking, we got tutorials to trigger a TTS description automatically upon being shown to the player.
and instantly ran into another blocker during testing.
We hadn't yet accounted for how the player would know where they were as they tried to navigate around their space.
We decided to make it so that while the player was aiming the teleport cursor, the system would provide information about key locations of interest.
We would also provide an additional text-to-speech confirmation after teleporting to let the player know where they had arrived and when they arrived at the desired destination.
However, once we began running external tests, it became apparent that we still hadn't provided enough location information.
Internally, we knew the layout of our game, but external accessibility testers who were not familiar with the game were getting lost because there weren't enough marked up areas.
So we went back and fleshed out even more locations throughout the game with additional teleport zones.
Unlike some of the other aspects of getting this text-to-speech working on objects, there wasn't a simple way of automating this process, and these zones had to be hand-authored by our technical designers.
Shout out.
But the good news is that after the new zones had been added, the frequency with which our testers got lost decreased.
So we took that as a win.
Finally, we had achieved a pretty thoroughly described world.
There was some content that was less detailed than we would have liked, but this part of the project took much more time than initially expected.
So, we reduced our scope to focus on what was required for the player to reach the end of the game.
By the end of this process, a huge amount of the game had been hooked up to play TTS descriptions.
We did it!
Players could learn nearly anything they wanted about the game world.
Unfortunately, merely offering descriptions on objects wasn't enough.
Players had to be able to find the objects.
We identified early on that it would be difficult or even impossible for blind players to locate objects in the game world.
We knew we were going to have to provide more information to the player in order for them to play the game.
Sightless players were a particular concern at this point, since one of the main ways sighted players found objects was by simply looking around the room.
And even if the players had some vision, it was important that they could distinguish interactive objects from the environment.
With object interactions being a core part of alchemy games, we knew that if we couldn't provide people with the resource they needed to play, then the dream of making our game work for blind players would be over before it ever really started.
Fortunately, our previous accessibility knowledge gave us a general idea on what we could do and what was needed to make this happen.
In accessibility and across all design, it's generally a good idea to convey information in multiple ways.
This increases the chance that users will be able to access your content.
In a spatial computing environment like VR, this becomes even more important.
For players to locate objects, we're going to have to take advantage of every feedback channel available to us.
And that started with auditory feedback.
Though virtual 3D environments present accessibility challenges that aren't present in other mediums, VR affords us some unique opportunities as well.
Traditional screen readers don't indicate the position of text on the screen because they don't really need to.
But for our implementation, we realized that we could provide more location context without adding more text to our spoken descriptions.
As it turns out, people are pretty decent at detecting the direction a sound is coming from.
Additionally, volume is a good indicator of how far away or close an object is.
If you've ever lost your phone and tried finding it by listening to it ringing, you've used these techniques.
Now, this isn't going to be perfectly accurate.
But, by using spatialized audio that plays from the location of an object in the world, we could give our players a general idea of the location of an object.
So, that's what we did.
And with that, players could gain a sense of the approximate position of objects in the virtual world, which meant that we could begin using other methods to help them narrow their search.
Now onto haptic feedback.
In the physical world, people can reach out and feel nearby objects.
One of the limitations of VR is that there's no physical resistance that allows you to feel that those objects are there in the virtual world.
Since we can't change that, we chose to take a lesson from white cane users.
Some blind folks use white canes to help them navigate the environment.
As they're walking, they may sweep their cane across the space in front of them, feeling for obstacles and changes in texture or elevation.
Detecting the change from one object or surface to another was something that we could do as well.
In an attempt to recreate the functionality, we added a haptic pulse for when the player moved the teleportation cursor into a different area.
By doing this, we were able to signal the boundaries of those zones to the player.
In an attempt to recreate the functionality, I just said that.
Sorry about that.
Furthermore, we made it so that when a player moved their open hand over something, it could be interacted with, and the controller would also give a short vibration.
This little bit of added context meant that players knew that their hand was near something that they could attempt to interact with.
Or get more information about it, too.
There was still one feedback channel we had yet to address.
Visual feedback.
Steve Saylor suggested we implement a high contrast mode.
Since blindness doesn't always mean sightlessness, it was important to add clear visual indicators to the world as well.
Some of the most accessible implementations of high contrast modes drastically changed the visuals of an entire game to help highlight the most important things on screen.
And though we wanted to do this, we didn't have the available resources for a feature like this.
Thankfully, a complete reskinning isn't the only way to improve contrast and object definition.
Since As early as Job Simulator, we used a system that added a light blue highlight around objects that could be grabbed.
And since there are static objects that the player needed to know about, we decided to extend that system to add a light green highlight on important environmental objects.
We chose light green as the color because it stood out well against the mostly purple environment.
But more important than the hue is making sure that the contrast is clear enough to show that something changed.
Now that players could position themselves near interactive objects that were providing different forms of feedback, we had a new problem.
Assistive descriptions were being constantly and unintentionally triggered.
And this was happening with just about any interaction that the player took.
So, for example, a player could grab their backpack when they're teleporting, or even simply reaching their hand out.
There is very little a player could do without setting off the TTS.
Now that players could learn about anything and everything, and essentially had no other choice, The synthesized speech became a dominant and cacophonous nightmare.
It drowned out subtle sounds and all the audio, and it made it sound incomprehensible.
It was an unpleasant experience, and it felt like drinking from an auditory fire hose.
It was simply too much.
It was like happening all at once, all the time.
And something had to be done to prevent players from being constantly bombarded with information that eventually just became noise.
So our plan was to improve player agency by giving them more direct control over their individual experience.
But what could we do to put the power back into the hands of the player?
What tools did we have at our disposal?
Lucky for us, we had an ace up our sleeve for just such an occasion.
For all our supported platforms and their different controllers, we had reserved one button per hand that had no existing functionality.
These reserved buttons would quickly find their new purpose as the assist button.
The assist button was designed to be a simple yet context-sensitive action button that would allow the player to be in control of what information they received and when they received it.
Now, instead of automatically playing audio when hovering over or grabbing an object, players could choose if and when to have the object's description read aloud by simply tapping the assist button.
And more importantly, the player could also now tap the same button to cancel currently playing TTS descriptions.
This meant that players no longer had to listen to the entire description before getting back to whatever they were doing.
Now we did briefly experiment with having the assist button function as a toggle rather than a tap.
But during testing, players would often forget that they had toggled it on on one hand, but not the other.
And this led to the original problem where they would be accidentally triggering audio at unwanted times again.
So we chose to revert back to using the original tap behavior instead.
Overall, though, the introduction of the assist button appeared to be a noticeable improvement to the experience.
However, this still didn't quite solve the problem of overlapping audio.
It was definitely frustrating to have a character speak while trying to listen to assistive information, but delaying a character speaking wasn't always an option for us.
And delaying the assistive audio until after all the characters were finished speaking would make the feature feel unresponsive and broken.
So, in sticking with the principle of respecting players' choices, we made it so that when assistive information was playing, we would duck the rest of the game's audio, lowering the volume so that the audio from the text-to-speech description could be heard clearly.
This allowed players to still be aware of other audio playing and cancel the assistive information if they wanted to hear what was being said, or they could choose to ignore NPC dialogue and instead listen to the information they had chosen to seek out.
And since the game already allowed players to wave at an NPC to get a reminder of what they are likely supposed to be doing, the exact audio might be missed, but important game information wouldn't be lost.
As an added benefit of using audio ducking, the player now knew when the description had finished playing because the game audio would return to normal levels.
And in circumstances where the audio could take a little longer to be generated, the player could tell that their request was actually being processed and wouldn't impatiently tap the assist button over and over.
Unsurprisingly, testers were universally on board with these new additions.
Players were now able to use assistive audio as much or as little as they wanted.
They wouldn't have to worry about accidentally missing information.
Now that players had greater control over their experience, a new issue began to reveal itself in testing.
Testers weren't activating assistance on objects very often.
At first, we were concerned that they were forgetting which button on the controller they had to press.
But during post-plate test feedback sessions, it became clear that using assistance didn't always feel beneficial.
Testers noted that it was hard to determine what information in the assistive audio was actually important to them, and it was a valid criticism.
In our efforts to bridge the knowledge gap for these players, we had overcorrected and provided more information than they really needed.
Another issue was that some testers would try to patiently wait to hear every part of the description, so that they wouldn't miss crucial information.
But it became a problem since some of the important information was sometimes mixed in with superfluous information.
That means that if a player got distracted or disinterested part way through, they'd have to re-listen to the whole thing all over again.
We figured the solution to this problem was just to make sure we presented the information more clearly and concisely.
To do that, first we had to identify what specifically needed clarifying and where we can make changes without losing information.
The first thing we identified was that the order we were presenting the information mattered.
We knew we wanted to present the most relevant and valuable information first and decided it would be helpful to standardize the order that information was read.
It made sense to put information that was the shortest and always present first.
So, interaction state information, like whether or not the object was grabbed, and the object's name were prioritized.
Next, we added a short description that contained the most commonly needed information about the object, and would play that immediately after the object's name.
Finally, the in-universe descriptions were placed at the end due to typically being the longest component of the description and including a lot of lore.
And this format was more logical and flowed more nicely.
The audio that would play, or as an example, the audio that would play You can look at the, if you see the image on this slide, it would now sound something like hovering, star shades, sunglasses with star shaped lenses, clearly the only accessory for the next ultraviolet.
Even when placed at the end, that in-universe description proved to be a source of confusion and frustration.
Some players would get hung up thinking that jokes and flavor text were important clues to beating the game.
And after talking with James Rath, who was both one of our testers and an accessibility consultant in his own right, he recommended that we follow the guidelines used for image alt text on the web and remove information that wasn't important.
At first, we were hesitant to take this suggestion, since this information was available to sighted players through other areas and means in the game.
When we took a step back to think about it, it was clear that these in-universe descriptions provided very little actual value to the player and were doing more harm than good.
So, we opted to remove them entirely.
This ended up being a worthwhile trade, because in addition to improving the signal-to-noise ratio in the assistive descriptions, this also meant that less text would have to be turned into speech speeding up the audio processing time and making everything feel more responsive.
It was a win-win.
After reorganizing the information and trimming the excess, we observed that players were interacting with the feature and the game at large with a higher degree of confidence and speed.
Knowing the information Sorry.
Knowing the information order allowed players to feel confident in canceling the audio description sooner and allowed players to stay in that flow state.
Additionally, text-to-speech processing time and audio clip lengths were now shorter, which seemed to keep players more engaged.
So all of these features seemed like they worked, and we believed that they would help our players.
But there was one major outstanding problem that had been presented from the beginning.
Turning on this assist mode, it still required sighted assistance.
And what good is an assist mode if it's inaccessible to the people who need it?
The mode was enabled by a light switch-like toggle in the game world where the player initially started, as well as one that was available in the player's virtual backpack.
But players couldn't simply feel around for this switch like they could in the real world, even if they knew it was there.
Our workaround for this during the earlier stages of accessibility testing was to provide a keyboard hotkey that could be used to turn this mode on, which worked well enough for players using VR on their PC, but we planned for this feature to ship on standalone and console headsets where it was all but guaranteed that a player would not have a keyboard.
Throughout the development process, this issue weighed on our minds.
We needed a way to turn this mode on with only things that were common across all platforms, and we wanted it to work with our existing accessibility features, like our one-handed mode.
So, we took stock of what we still had left to work with, which amounted to one spatially tracked controller and one spatially tracked headset.
It didn't seem like a lot.
until we turn the problem on its head.
If players couldn't figure out where they were in our world, maybe we could figure out where they were in their own.
We had forgotten one of the things that makes VR special.
Proprioception.
For those who are unfamiliar with the term, proprioception is the sense a person has of where their body is in relation to itself.
It's why if I asked you to close your eyes and touch your nose, you could.
Once we began considering a player's proprioception, it meant that both the game and the player could know where the controller was relative to the headset.
With this information, we could ask the player to perform an action relative to their own body, attempt to detect that action, and turn on the vision assistance mode for them.
But what kind of actions could we consistently identify?
Gestures were one option.
Around the time of the Vision Accessibility update being in development, I had been slowly playing through Horizon Forbidden West in my spare time.
And one day while I was playing, it hit me.
In the game, the main character uses a small device worn near her ear that she taps to get additional context information about the world.
The answer had been in front of my face for months.
We could infer the approximate location of the player's head and have the player perform an action near their ear.
We already were using a similar action to allow the player to pull their backpack from over their shoulder, though, so grab wasn't an option.
We could, however, have the player double-tap the assist button.
This idea worked, and the result was that players now could activate the mode without sighted assistance.
Kind of.
In truth, we should have done a better job making sure players knew right away about this method of activating the vision assistance mode.
There is explanatory audio that tells the player about this shortcut.
However, it doesn't trigger automatically upon starting the game.
So unless a player had heard about this action through word of mouth, they might still need sighted assistance.
And though this may not have been the ideal situation, blind players faced another challenge before even that one, launching the game.
Unfortunately, no matter what we did with this update, certain players would still require assistance to navigate the process of setting up a headset and launching the game.
So with complete independence still unavailable to some players, we decided that we would keep this feature as is for the time being.
We had to ship it and hope it still provided benefit to some players, which meant it was time for us to face the scariest question.
Was it all worth it?
Would this update actually help blind and low vision players who wanted to experience Cosmonia's High?
While conducting external playtests with people who were legally blind or had low vision, we sent them a survey afterwards to get their sentiment about this mode.
We were ecstatic to see that when asked if they would recommend this feature to a friend or colleague who also has low vision, their results on a scale from 1 to 5, with 5 being highly likely to recommend, were mostly 5.
No one was on the lower range of 1 and 2.
When talking to accessibility experts, we heard a lot of praise and got a lot of great feedback that would help us create even better versions and have better insights for future games.
Looking at March 14th of last year, that's when we pushed the button.
We shipped it.
We shipped the Vision Accessibility Update to Steam for PC VR, Quest 2, and PlayStation VR 2.
From that point forward, we've had a steady amount of people reach out to us over time, people who played Cosmonius High and played with the feature.
They let us know that they found the feature useful and have expressed their gratitude for making a game that they are able to play.
We received some periodic updates from Meta about how many TTS requests the system had received.
A request is made each time a player uses that accessibility button to hear a description when they teleport with that vision assistance on.
for March through September 2023, 9 million text-to-speech requests were made.
And that's just coming from Quest data alone.
I've heard other developers ask, why even do this?
If they can't see it, maybe it's not meant for them.
And I would point out to those people that, first of all, excluding people because of their disabilities is just wrong.
People who are blind also play games.
Additionally, we have proof that blind people want to play VR too.
And I'm pretty sure we are all here because we like to make games.
We want as many people to enjoy them and experience that feeling that comes with gaming.
So going back to what we said early on, we at Alchemy Labs want to make VR for everyone, and everyone includes people with disabilities.
So let's go over where we're at now with VR accessibility.
Is VR accessible to everyone?
No, absolutely not.
The good news is that it's improving and we're seeing progress all around.
We learned so much from this experience, and there's way more that we didn't have time to cover.
Our hope is that by educating you about some of the ways we approach solving the challenges that blind and low vision players face, you'll be able to build upon this work and help us make VR a more inclusive medium.
Thank you.
If anyone has any questions or wants to connect with us, we'll be in the rapid rooms outside.
