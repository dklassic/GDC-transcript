So thank you all for coming for delaying your lunch to come to my talk, Lessons Learned from a Decade of Audio Programming. My name is Guy Somburg and I'm a core programmer at Telltale Games. And so here I'm going to share my wisdom of the last 10 plus years of programming audio engines. So who am I? Why should you bother listening to me? I've been in games since 2002. That turns out it's actually 12 years so my talk should really be called Lessons Learned from a Doh Decade of Audio Programming which is totally my new favorite word.
Over the years I've owned the audio engine at every company I've ever worked at that was either the engine for the whole company or the engine just for the product I was working on.
And more importantly though I've actually shipped lots of games, everything from slot machines, Game Boy Advance, AAA PC and everything in between.
And over those games I've actually written 13 different audio engines from the ground up, from zero source code.
And some of those are more than one for each game.
For example, Hellgate London actually had three audio engines that evolved as the needs of the game evolved.
We decided it was just time to rewrite the engine from the ground up.
And so over the years, I've actually gotten pretty good at it.
So in this talk I'm going to give you six kind of pithy lessons that I've learned targeted at, you know, people who want to be audio programmers or people who are forced to be audio programmers and don't want to be audio programmers. So the first lesson is that playing audio is really easy, in fact it's one of the simplest things.
that your audio engine can do, or that your game engine does.
And to demonstrate this, we're gonna start from the very beginning.
It's like the fundamentals, how does the speaker work?
This is a great diagram.
There's a cone attached to a magnet.
There's wires coiled around the magnet.
When electricity runs through the wires, it generates a magnetic field, which moves the magnet, which moves the cone, which creates sound.
I hope you all followed that.
So you can think about, if you graph the position, or the offset of that magnet from center over time, that is kind of your waveform of your sound.
And if you actually draw that out, it'll look like this.
And then what we do is we sample it very frequently, typically 44,000 or 48,000 times per second, and that'll look like this.
And then what we do with that is we just move the magnet to wherever it is at that curve, that point in the curve, and we generate a sound.
Great, we can play one sound.
Right, big deal, that's not very exciting.
It becomes more exciting when you start to play more than one sound.
So what happens if you try and play the blue curve with the, what is that, puke green colored curve on the right?
Well, it turns out that if you play those two together, your curve looks like this orange curve.
And if you do all the math, you figure out all the Nyquist and all the voodoo behind it, it turns out that all you're really doing is adding these two curves sample by sample, just like this.
Your signal A plus signal B equals your output signal.
And because the output signal is itself a signal, it turns out that we can generalize this.
very trivially. Adding up a whole bunch of just all the signals together becomes your output. Now I can keep going, I can talk about, you know, I can introduce sub mixing and clipping and all these other things. But there's kind of one thing that I want to focus on here and that is that all of this stuff is embarrassingly vectorizable. So whatever your particular single instruction multiple data set for your hardware is, it's going to get a big workout when mixing audio and playing audios back.
So the point of all this is that playing sounds is free.
It doesn't matter how many sounds you play.
It's, you know what, I'm lying.
Playing sound isn't free.
Playing sound is really cheap.
You can play as many sounds as you want within reason and it's not gonna cause like resource problems.
Memory is still a concern.
But in terms of CPU cost, it's nothing.
So if playing sounds is cheap bordering on free, it turns out that the audio mix is actually what the hard part is.
So what I mean when I say the audio mix? Audio mix is how you combine the sounds that are playing.
It's what the game players are actually hearing. There's a lot of things that go into it.
Some examples are like how the music and the ambience will mesh together. It's how when you enter sniper mode, maybe the footsteps get louder so you can hear if somebody's sneaking up on you. It's how the environment, the reverb and the...
the background ambiance of the environment sounds.
And it turns out that one of the biggest hammers that we have is just lowering volume of sounds that are less important.
And there's lots of tools in your, whatever audio engine you're using, the audio middleware, to do many of these things.
For example, gain reduction.
sound cone so that as I turn my head I get quieter, 3D distance attenuation and ducking is the process of kind of lowering sounds whenever other sounds, lowering the volume of sounds whenever other sounds are playing. And all these things are either built into your audio engine or they're really easy to implement.
There's also more advanced features, I call them mix states, I think they're called sound moods in Crytek, they're called mixer snapshots in many other engines including FMOD studio.
And this is a much more involved subject where you're basically taking a snapshot of what you want your mix to look like and fading up to that state whenever certain game events happen and fading out.
And I wish I had more time to talk about this, I only have a 25 minute talk.
But just mix states, mixer snapshots is like a 60 minute talk on its own.
So there's a lot going there.
Now if you take the lowering volume and kind of take it to its extreme, it turns out that a big chunk of audio technology actually revolves around not playing sounds at all.
And that takes the form of things like maximum playbacks, where just the sound can't play more than so many times.
Stream limits, which are usually a restriction on your bandwidth or kind of your spinning medium, which is becoming less of a concern with more modern stuff.
But for example, the Xbox 360 often has stream limits.
Virtualized sound sources where just sounds that are less important get cut off completely so they don't become a part of the mix. And all these things are again either already built in, for example, virtualized sound sources or they're things that are pretty easy to implement in your audio engine.
But I want to talk now about one that's a little bit more involved in advance and that is Walter Merch's rule of two and a half.
So this gentleman here is Walter Merch, he is an Academy Award winning sound designer, worked on THX 1138 and a number of other great movies.
And while he was mixing, uh, THX1138, he realized, he had this realization, he said, you know, so long as, uh, I'm laying out these footsteps, if I have one person walking, I have to like, synchronize the footsteps with the, uh, with the scene. So if I have two people walking, I have to synchronize the footsteps with the scene. As soon as I get past two, the two and a half thing, all of a sudden it's less about every individual footstep, and it's more about the sound of footsteps.
So how did we take this and make it into something that was valuable for a game?
And for Hellgate London, and we did this also on Bioshock 2...
We implemented this thing which we called max within radius.
And we allowed for each sound in the game, we allowed the sound designers to define a maximum number of playbacks within a given radius.
And they defined the max playbacks and the radius for that sound.
So in this situation here, we have the blue triangle as the listener, the green dots are the sound sources.
And so we draw a circle, or a sphere really, around the sound sources at the given radius.
And then we cut out in this case, all but two of these sounds.
which was the max playbacks for the sound.
And what this does is it maintains spatialization.
So in this situation we have two sounds coming from the left, we have two sounds coming from the right, two sounds from behind.
To maintain that, but the mix isn't getting muddy.
You start to play all these sounds on top of each other, it starts to sound like noise.
And so by doing this, we are creating a situation where we still have all the spatialization, we still have the sound coming, but we're actually using less resources while still having a cleaner mix.
So if I were to summarize the audio mix in kind of one pithy sentence, it would be that you want to let the most important sounds shine.
But more precisely, since we are audio programmers, not sound designers, it's that you need to provide tools for your sound designers to allow them to make the most important sounds shine.
Speaking of sound designers, you need to learn to work with them.
They are your customers.
And they are wonderful, beautiful people.
And they will create awesome content with whatever you give them.
But really what they want to do is ship a copy of Pro Tools with every game.
And we'll come back to this, I promise.
They also have their own language, with lots of crazy terms that you have to learn.
And this shouldn't come as a surprise, we as programmers have our own language.
And so what you need to do with all these things is you need to learn their language.
You need to learn, speak it to them, and you need to love it, because your sound designers will love you for loving their language.
If you come to them and speak to them about decibels and headroom and VCAs...
It makes it much easier to communicate when you're talking to them in their own language.
There's one of these terms in particular I want to talk about, that is the decibel.
A decibel is simply defined as one-tenth of a bell.
What's a bell?
Turns out that the answer is, it doesn't matter.
A bell, the most important thing you need to know is that 6 dB is half or double volume.
So to find out what your decibels are, or what the volume percent is, you divide by 6 and turn it into a power of 2.
Easy enough.
And in fact, whenever I write a new audio engine, the first thing that I do, these four lines of code are the first four lines of code that I write.
These are conversion functions from decibels to volume percent and back.
And I'm going to stay here for about four more seconds to let people who are taking pictures of it or writing them furiously down.
These are wonderful functions.
The first thing that I do.
I hear lots of snaps.
Oh, here.
Hold on.
I can go back.
Quick, quick, quick.
Okay, alright, we got it. So I'm gonna, before we leave this subject, I'm gonna leave, uh, leave you with the biggest, best secret that I know for working with sound designers. So let's say your sound designer comes to you and he says, I want to be able to kinda play these, you know, this list of sounds randomly, kinda with a timer on it and, uh, go from there. You tell him in one word. You can say, no. No, I'm not gonna do that for you. That is not at all what you want. You're not trying to solve that problem. You're trying to solve the problem of creating a background, an ambience.
Let's see if we can find a solution for that and give you what you're actually asking for.
Because remember, sound designers will create wonderful content even if you give them trash, horrible, awful tools to do it with.
And the situation that I talked about is a dramatization of something that happened before I came on on a previous product that was actually hugely award-winning.
So, even, you know, this is true.
They will create award-winning content, even with trash.
So your goal then should be, don't give them trash.
Even if your sound designer comes to you and says, I would like a steaming pile of trash, please.
You say no.
Instead, you give them the tool, best tool to solve their more general problem in a long-term lazy fashion.
If you are long-term lazy, you spend more time up front and you give them great tools that solve exactly what they're trying to do.
And if you do this, then your product will ship on time, it will ship maybe even early, and sound will not be a problem if you spend the time up front.
So we're all game developers, we have to do things quickly.
And middleware is awesome.
There's Fmod and Wwise and various other middleware things that will make bootstrapping your audio engine really, really quick.
So the fact that I'm going to bootstrap an audio engine here in front of you.
This is all the source code for an audio engine.
I'm going to go about 99% light speed, but I'm hoping that I will get my point across.
Here we go.
This is our header for audio engine, we have some static init update and shutdown functions.
We have some functions to manage sounds and playing channels.
We're using the pointed implementation, so here's our pimple.
And then we have some... here I'm using fmod for my source data, but the principle applies for whatever audio engine you're using.
And then we have some init and shutdown functions, there is constructors and destructors, which I'm not going to show the source code here for, because they're boring, they're just copy-pasted from the documentation, and then just releasing all your assets.
Here's our update function. We're going through all of our channels, seeing what's stopped, we remove anything that's no longer valid and then we update the system.
Our load sound and unload sound are really kind of mirrors of each other. First we check our cache, then we either load or unload the sound, and then we either add or remove it from the cache.
I'm going to spend just a little bit more time on play sound here.
So here we have, we see if the sound is already loaded, if not we load the sound, and then if we still don't have the sound then we...
Then we exit early, and then finally we play the sound and set all of its parameters.
Now there's three things I want to call out here.
Number one is this parameter here.
The third parameter to play sound, it's boolean true.
What does that mean?
It's starting the sound as paused.
And what that does is if you don't pass it, if you pass false, the sound will start playing immediately or as close to immediately.
And the audio thread, the mixer thread, is going to get that.
And what can happen is there will be a brief window after you've started the sound, before you have set the 3D parameters and the volume and whatever else you're setting, while that sound can play.
And you get a little pop.
little pop and then the sound goes to whatever volume and stuff you've set.
So by starting it as paused and then setting your parameters and unpausing it, you are preventing that pop and kind of priming the pump.
Second thing I want to call it is the return value of this is an integer.
And basically this is how the external system deals with playing sounds or interacts with playing sounds.
Is through this and yes you do have to do a lookup every time to actually get the underlying object.
And it's tempting to say well let me create an object.
like a playing sound or a channel object or something that you can call functions on.
The problem is now all of a sudden you have this object that is in cahoots with the sound system.
It needs to know about the internals of the sound system, but it's not really owned by the sound system.
And so the underlying concept, so the underlying memory that this object is pointing at can disappear and then it doesn't know about it.
I have fixed a lot of crashes, a lot of weird crashes, simply by converting from sound objects to integer IDs.
The last thing I want to call out here is that even if we fail to load the sound, we still return a valid channel ID.
And this is really just to protect against one particular pattern of coding, which I've seen particularly in particle systems, where there's a sound that they want to play, and then every frame they say, is my channel ID valid?
If not, play the sound.
Next frame, is my channel ID valid?
If not, play the sound.
if you are constantly returning an invalid channel ID, if you fail to load the sound, you're gonna be wasting all this processing trying to play the sound that you're never gonna actually succeed on.
So always return a valid channel ID even if you fail to load the sound.
Last piece of this is just the set channel, whatever.
These are really boring, just follow the pattern and go from there.
So in summary, we have, it took about 300 lines of code for a fully functional audio engine.
It took me about an hour and a half, I'll grant you, I know what I'm doing and what it needs to look like at the end.
So, you know, it might take other people longer, but...
So what? Like, why did I show this to you?
Well, it turns out that you can ship on the moral equivalent of this code.
There's a few little bits and bobs other than the initial destruct that you need.
But you can actually ship on this.
And I have.
Like Minion Master, this is a game that I did as an indie at BitFlip Games.
This is more or less the audio engine that we shipped with.
But, like, remember this?
I showed you this wonderful Pro Tools that we want to ship a copy of with every game?
Does this look familiar?
Here on the left we have Wwise, on the right we have FMOD Studio.
They're very similar to this, and these tools will provide your sound designers with a lot of features just right out of the box.
Snapshots and busing and all these wonderful things.
And it's really tempting just to give them the tools and say, great, I'm done.
But you can't just throw these tools at them and call it a day.
Like off the top of my head, here's a small subset of some of the things you might have to think about.
in your game engine. Now not every game engine or audio engine needs all of these things but all audio engines will need some of these things. So I wish I had time to talk about all of them but I don't. So lesson five is that sound is always the first thing to get the shaft and I'm going to present you with some, I can see some people nodding here that's awesome, some situations that are inspired by real life events not exactly but you know sufficient close enough. So we need five megs of ram.
take it from sound, they have too much anyway. Our ship date hasn't changed, but everybody else is running late. Well, sound is post process, so guess who gets to crunch? We're staffing up, who do we need?
we don't, nobody, the audio engine, yeah, nobody gets to do work on the audio. And I want to do a little bit of, this little dude here is the flagship studio's sound caveman. The people, before I started working at flagship studios, they all knew that they were sound cavemen and they had a very primitive audio engine. So whoever kind of touched it last became the expert. They became the sound Neanderthal. And they got the sound caveman and became the expert.
So when I came in I got to keep the sound caveman because I own that audio engine.
So what can you do?
Like these are kind of fun little situations but what can you do if you're faced with these situations?
So the first thing is the best ideal case is push for a dedicated audio programmer.
I mean if you can.
It's not always possible.
Maybe you're a small indie and you can't hire anybody.
Maybe there's just no budget for a particular audio programmer.
But if you can, then there's, like, it's awesome.
It's a great position to have in your studio.
If you can't, then you can learn it yourself.
It's actually really rewarding, it's not that difficult, and it's actually quite fun to do this.
Here we got somebody.
Thank you, sir.
And the last one is a little bit sneakier.
You can ask for more resources than you actually need.
So you ask for 50 megabytes, they give you 40 megabytes, and then you plan for 35 at the end of the project when they steal from you.
But make sure if you do this that you use everything that you get.
If you actually use 35 megs of the 40 that they give you, then on your next project they'll say you only used 35, so they'll give you 35, and then they still steal from you at the end.
The last lesson here I'm going to leave you with is listen to your games.
critically important that you listen to your games as much as you can.
Over the years, I've learned to tell the difference between all these different kinds of noise just from listening to them.
And I can tell where they're coming from and why.
And this is, it's not easy, but you know, over the years I've developed this.
And so I can do this.
Can you?
Now I bet all the sound designers here are going, well yeah, but the sound programmers, you need to develop your ear.
You need to listen and know what the difference between a sample discontinuity and a buffer under it and how they sound different and why.
So, I cannot stress this enough, there is no substitute for listening to your games in as many situations and speakers and whatnot as you can.
Last thing I want to do here is talk a little bit about the history of where I've come from and where I'm going and what went right and what went wrong with a little bit of a mini post-mortem on my audio engines.
So a lot went right.
I mean, I wouldn't be here if that weren't true.
I learned a lot.
You know, I started in slot machines and I really didn't want to be doing audio, but I have embraced it and I love it now.
I also shit.
You know, shipping is a big deal.
If you can't ship or you haven't shipped, then you're just noodling around.
And so shipping is a really big feature, even if you're not complete or beautiful or perfect.
Or, well, even if your code, rather, is not beautiful.
So I've also made some wonderful tools for my sound designers.
The Bioshock 2 voice packager took a process that was just...
hours upon hours. It was literally an all-nighter if they needed to do this.
And it was error-prone and horrible and I took this into something that he could set up in five minutes before he left for home and he would come back the next day and it would be done perfectly every time.
The background sound engine for Hellgate London and its iteration rewrite slash iteration in Bioshock 2 is a beautiful piece of work that I'm really proud of.
And also the third-person camera listener tech that I wish I had time to talk about here.
in Hellgate London is where we originally developed this.
I'm particularly proud of that, and if you want to talk to me afterwards, I'll be happy to describe that to you.
Thank you.
So, excuse me.
It was not all sunshine and roses.
There were some really big missteps, and the one in particular I want to call out is the Hellgate London music engine, which was this horribly over-engineered piece of work that really didn't solve the problem at hand.
Dave, I'm sorry.
And I want to say that here.
I'm really sorry.
For all you people who played Hellgate London and never heard the action music that our awesome composers wrote, that's my fault.
I've also historically not been aggressive enough about threading.
You know, there were opportunities that have been there for a long time to thread your audio engines and to, you know, obviously you need a mixer thread, you need some threads built in, but threading the interface is a very valuable part that isolates the rest of your code from any kind of hitches that may be in your audio engine.
So, where am I going from here? Future plans.
I'm at Keltel Games right now.
I can't talk too much about what we're doing and where we're going.
But I can tell you that this is the best audio engine I've ever built.
And in fact, that's my goal, is always to make some sort of measurable improvement in the lives of my sound designers.
And before I finish up here, I want to leave you with one last bonus session, which is—bonus lesson, excuse me— which is that audio programming is fun.
You should do it, and you should not be afraid of it.
Thank you very much. We have about three minutes for questions.
Any questions? Yes, sir.
So the question is, other than FMOD and WISE and similar things, what tools can I recommend?
That's an excellent question. I'm an FMOD guy, so I've focused on FMOD throughout my career.
There are some open source audio engines. SDL has a mixer, a lib mixer I think it's called, and there's OpenAL. So there's some of those.
Sorry, say again?
Maverick if you use Unity.
Oh, fabrics. Fabric, if you use Unity. So there are some alternatives, but that's what I got. We have a question back there.
So I'm working with a sound designer on a project and historically I've been rolling a custom audio engine but he's been really interested in leveraging the features of the editor tools and fmod sound system so currently we're talking about a sort of integration stuff. I guess what's your general experience with like the level of autonomy you have with a middleware sound engine versus a ground up system.
It's a fair question. I much prefer working with middleware because I've written a low-level mixer for slot machines, really like mixing at the sample level.
And the problem is that that's tedious and error-prone and you have to deal with driver differences and you have to actually parse the sounds yourself.
So if you can get a middleware that is at whatever level abstracts the best for you.
then that's the right one.
So FMOD is, I like FMOD because it abstracts at multiple levels, but there, you know, Wwise is also great because it abstracts at a very high level.
and there are like the SDL mixer and some of these other ones abstract at a very low level.
And so whatever your mixing philosophy is, wherever you are spending your most time, that's where we want to spend it.
Wherever you're trying to solve your problems, that's the appropriate level of abstraction for your middleware.
So just as an extension of that, I find a lot of like...
potential and enjoyment honestly in writing custom DSPs. What's your experience with middleware that's easily extensible with custom DSPs? I hate to sound like a broken record but FMOD actually has a great plugin architecture for that. I believe Wwise does as well. I haven't looked at their API recently.
Beyond that I cannot speak.
I think we have about 30 seconds and then I will be at the wrap-up room at 3.07 so we'll take one more question.
As a sound designer, how much do you recommend that we dive into the code and learning audio programming for the most successful collaboration with audio programmers?
That's an excellent question. In general, you shouldn't have to be a programmer.
I went to a great talk about the audio engine and I think it was...
one of the Killzone games recently, it was just yesterday or the day before, where they did... they had kind of a boxes and lines scripting language for their audio team and that's the kind of deepest I would allow a sound designer to get.
Somebody who's kind of focused on creating sound.
If you're an indie and you want to do both, then more power to you.
But if you're more just like, I'm a sound designer, how much should I know how to program?
That's the level of abstraction I would recommend.
Cool, so I think that I will be at the, I think they're kicking us out, so I will be across the hall in 3.07 if you have any more questions.
