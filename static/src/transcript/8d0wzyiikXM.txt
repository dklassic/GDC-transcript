So welcome to automated testing and profiling for Call of Duty.
My name is Jan van Valberg and I work for Activision Central Tech as a technical director.
So I wrote a build server called Compass which is used for all the automated testing and profiling of all the Call of Duty games and I'm going to talk a bit about how Compass came to be, what it looks like and how it works.
So I actually work for a small Activision satellite tech group in the UK called Central Technology North.
And we get assigned to help out on different games as needed.
And back in 2011, we were helping out with Bungie's new game called Destiny.
So at the time, Bungie didn't have the automated testing infrastructure that they have now.
So sometimes you might grab the build, and it might not work.
And then you might not necessarily know whether it was your fault or whether someone else And this was a particular problem for us because we were in a separate office in a different time zone so it wasn't as easy to ask other people whether their version worked.
And also we were among the first people to work on the PlayStation 3 version. So that meant that it wasn't tested quite as much yet. So to help us I set up a little build server that would build and run the game continuously on PlayStation 3 to make sure that it was still working.
I've been experimenting with different build servers and one I really liked was one called Buildbot. Buildbot is an open source build server written in Python. It's used by various people like Mozilla and Chromium which is the open source counterpart of the Google Chrome web browser. And the UI that they've built for it, that Chromium built for it is really interesting. I particularly like this, which is the console page.
So at the top here, you see the kind of overall build status for all the different parts of the build.
And then down the bottom here, you can see new check-ins coming at the top.
And then all the boxes indicate whether a particular task on that check-in is currently being tested, or it's green or red if that task has failed or succeeded.
On other build servers that are used up until this point, this information would be spread across lots of different pages.
So I really like this idea of having it all on one page.
And the other thing that I liked about BuildBot was the way the build was configured.
So all other build servers that are used up until that point, you'd configure them by going onto a webpage and changing forms on a webpage.
But BuildBot was configured using code, using Python code.
and this is what's known as configuration as code and I'll get in later, I'll talk later about why that's so great. So I got this automated testing up and running on build what and um there was still a couple of things that I wasn't happy with.
So I had a little wish list of other things that I wanted to do differently.
So first of all, Buildbot didn't know about DefKit, so it didn't know about WorkerPC, so if WorkerPC would go down, it wouldn't run tests on that.
But if a DefKit would go down, it would continue to try to run tests on that, and then the page would go green and it would look like something had broken.
but actually it was fine.
So what I really wanted was a build server where def kits and workers were treated at the same level, where it would understand whether a def kit was up or down just as well as it would know whether a PC was up or down.
And also, although the configuration was done in Python, which is cool, if you wanted to update the configuration, you'd have to check it out, upload it to your build server, and then run a little batch file, which would reload it.
And what I really wanted was to be able to check the configuration into source control and have it pick it up directly from source control.
And then the last thing that I wanted was to have screenshots and performance and memory stats deeply integrated into the UI of the build server.
So we built automation with screenshots and perf stats before, but viewing the screenshots always felt disjointed from the rest of the build server.
It was like it was a separate thing.
And I had this vision of having a performance graph where you'd see a performance spike and you could hover over the spike and see which check-in was responsible for that spike.
So I wanted a build server that could do all of these things, but there wasn't one available, so I decided to write my own build server from scratch.
And this became a hobby project during Destiny.
So I would be working on it in my spare time or while we were traveling to America on the plane or at the hotel.
And that's how Compass was born.
So after working on Destiny, my group, CTN, was moved on to Call of Duty Advanced Warfare by Sledgehammer Games.
And so I set up Compass for them as well.
And at this point, Compass was still running out of our little office in Warrington, and it was basically just a couple of PCs that I'd coupled together, and a couple of dev kits next to my desk.
And Sledgehammer didn't have automated testing of the game yet, so they had all the other stuff up and running, but this was providing them that extra bit of automated testing.
And the next project we worked on was Call of Duty Infinite Warfare with Infinity Ward.
And for that one we decided to set up a compass server actually at Infinity Ward.
And we also decided that we wanted a compass to do all of the continuous integration.
So take overall stuff that was previously running in Jenkins and Team City.
And it became kind of the hub of all build server-y things at Infinity Ward.
And we quickly scaled it up to hundreds of VMs and many dozens of dev kits.
And this ended up being really popular.
And before long, all the other Call of Duty studios also adopted the Compass wholesale.
And Compass is now used, as I said, for all automated testing and profiling of all Call of Duty games.
So here are some numbers to give you an idea of the scope of what we're dealing with.
So across all of the Call of Duty studios, we have roughly 700 worker PCs.
So they could be VMs, or they could be physical PCs.
Each one is at least four cores.
Some of them are many more cores, like 20 or 30, depending on what they're doing.
We have roughly 300 dev kits, so that's all PlayStations and Xboxes across all studios added together.
And we have just shy of 1,000 unique users.
So unique users would be anyone looking at the web app to check the status of the build, or maybe producers looking at stats, or also anyone that's using Compass to test their code, to kick off pre-submits, pre-commit builds, for example.
And we run around 50,000 tasks per day.
So that's about one task every other second.
So next, I'd like to show you a bit of what it looks like.
So this is the signature page of Compass.
It's the console page, which, as you can see, it's inspired by Chromium's console page.
New check-ins.
This is a time-lapse video of what it looks like.
You can see new check-ins come in at the top.
And then the tasks appear.
And they go yellow when they're in progress.
And then they go red or green when they're finished.
All the tasks on left to right are in chronological order, so on the left-hand side you might have compiling of the game code, compiling of the tools code, then in the middle you might have the asset conversion, and then on the right-hand side you have the actual running of the game, and then we take screenshots while we run the game, and then at the far right you can see the screenshots that resulted from that.
So here's an example where you can see someone made a check-in over here and you see two things going red and then the fix comes in over here so it goes green again. So you can see exactly when things are broken and who broke them and which things exactly are broken.
So this page is perfect for looking at the current status of the build. If you want to see if something is broken you can just look at the top of the page.
and then if you want to find out who broke it, you can just scroll down and see where it started going red.
And it's also good for if you made a check in and you want to follow it through and see what's going on, you can see your check in appear and you can see all the tasks go yellow and then go green.
If check-ins come in faster than BuildServer can handle, it will skip tasks.
And these then show up on the console page as translucent boxes.
So for example, here, a bunch of check-ins came in in a very short period of time.
And the BuildServer didn't manage to test all of those things for all of those check-ins.
And so we've got these translucent boxes here, which are the skipped tasks.
We have tool tips.
So you can hover over a failed cell.
and that will then give you a little error synopsis so you can quickly see what went wrong there. So in this example you can see the games crashed and in the red error box here you can see the call stack of the crash. So having the screenshots on the console page was one of these ideas that worked out really well. It turns out that It makes a lot of intuitive sense for people to have them there.
They, even if nobody's ever explained Compass to them before, they can look at this page and they go, okay, I can see that there's a check-in and I can see a screenshot.
So that means Compass is running the game and the game is running correctly because I can see a screenshot of what it looks like.
And of course, having the screenshots is also super useful for debugging small graphical changes that maybe people might not have noticed from day to day.
You can just click on these screenshots and page through them and then quickly find where something like that might have happened.
One thing that we do struggle with with this kind of visualization is the kind of horizontal space So there's a limit to the number of boxes obviously that you can fit on screen So for big builds like the nightly build where we test every single map of every single platform in the game We run out of space and so we just use a horizontal scroll bar Another thing that we did find is that the high information density for some people initially when they see it can be off putting, can be kind of scary. Um so it's still useful to have a big overall is the whole build red or is the whole build green traffic light screen. So this is what we have for infinity ward.
The other studios have a similar thing.
So here it's the main branch and the dev branch.
And if everything is green, then this thing will go green.
And if any errors are in there, then you get this thing going red, and you get the list of all the errors that are currently happening.
So this is what we use in Infinity Ward in the common room on the TVs, for example.
We did experiment with having the console page on a TV, but it's just too much information to cram onto a little TV and to kind of absorb from a distance.
Whereas this page is perfect for that.
So this is the measurements page.
This has got the deep integration between the stats and the build server that I was talking about.
So right now this page is showing the number of shader assets in the game.
So, along the x-axis, it's the build number, it's the builds, and then the y-axis is your measurement, so in this case it's the number of assets.
And each dot represents a build, so each dot is basically a check-in.
If you scroll down on this page, you can see this, you can see the list of all of the other stats that were output by that task.
And on the right hand side you can see little spark lines that can give you a quick overview of what all these stats have been doing recently.
So you can see the assets are constantly on the rise going up.
And then in the middle you can see that all of a sudden it's dropped a little bit.
So if you were to click on that spark line, it will then show you in the big graph over here.
And you can then click on the exact dot that was responsible for this drop.
and then I don't know if you can read it, but over on the right hand side here, you can then read the description of the change list that was responsible for that drop.
So in this case, someone did some memory optimizations, they removed some assets.
So here's a related page, which we call the nightly overview page.
So every night we run a test which teleports the player to dozens of different locations on every single map in the game.
And then each location we wait for 30 seconds and we capture performance data.
And all this performance data is then aggregated and presented on this page.
So each of these rows is a different map.
You can see the map name in the top left corner.
And then each of these columns is a different map location, a different screenshot location where we've done that 30 second capture.
And then this row shows the percentage of time within that 30 seconds that the game was running on or above our target FPS, which is 60 FPS.
And then this is the average GPU time in milliseconds, average CPU time in milliseconds.
If a cell is yellow, that means it's close to going over budget or just on budget.
And if it's red, it's definitely over budget.
And we also show these up-down arrows.
So that indicates whether that measurement went up or down significantly compared to the previous night's build.
If you hover over a cell, you can see a little graph that shows the trend.
So you can kind of get an idea over the past few weeks whether this status be going up or whether it's been going down.
And you also get a screenshot so you get an idea of what that location looks like and sometimes that might give you a clue if a particular location is slow about why that might be slow.
And if you then click on this cell, that would then take you to that graph view that I showed you before and you can then dig in more into the details.
So, let's say the GPU perf has got worse, you can drill down into exactly which bits of GPU performance got worse, like whether it's...
post-processing or opaque or whatever.
So this page is used a lot by artists.
Artists will look at this page and look at their map and then look at which part of their map are slow and then they'll go and optimize them and then check in their optimizations and they'll come back the next day and look at the difference and see whether their optimizations worked and see whether maybe more needs doing.
So one thing worth mentioning is how we come up with these screenshot locations.
So we have testers play the game, and then based on those playthroughs, we kind of judge which are the slowest, the worst performing parts of those map, which are the most problem areas, and then we enter the coordinates of those locations into a text file and check that into source control.
And Compass then uses those locations to produce stable, repeatable measurements which we can meaningfully compare day to day.
So we specifically don't try to automate a play through of the whole game automatically.
Humans are better at that kind of thing.
And the way we try and do these things is by kind of...
making, automating not everything that a QA tester would do but just automating the bits that are boring or the bits that are laborious to not replace testers but empower them or make them more productive so they can concentrate on the things that humans are good at like exploratory testing.
So next I will talk a bit about how compass works on the inside.
So one of the things that's very different about Compass is the way that tasks work, or what you'd call jobs in Jenkins.
So the most common way that jobs are configured traditionally in build service is via web UI, like with these kind of forms.
So you might be able to make a new task and choose from a different set of steps like sync source control or run a shell command, and then you'd fill out different fields in the form to choose exactly how it will run.
So in Compass, all of this is done using Python.
So instead of filling out a GUI form, you take the thing that you put in the GUI form and it becomes a parameter to a function.
So in this case, the function is sh, which executes a shell command.
So here's another example, so at the top is how in Jenkins you would build a Visual Studio solution, and at the bottom is encompassed, like how you would do that.
You call the build solution function, which is a piece of code that we've written which will use MSBuild to build your solution.
It will create the output, it will classify warnings and errors, and it will do things like if there was PDB corruption, it will delete all the PDB files and retry the build.
and here is another example of how you would set up a Perforce repository in Jenkins and in Compass.
So in Compass, each task is basically just a Python function and we call these automation scripts.
So here is an example of things that you might find in a Compass task.
So at the top, the sync function, that will sync source control.
So in this case, it'll sync the dev source control repository.
And then the sh function, that runs a shell command.
So in this case, it runs echo hello world.
Measurement.new adds a measurement to the task.
So the graphs that I showed you before with all those measurements and the sparklines, each one of those is added by calling this function.
So all perf stats, memory stats, build durations, it all goes through this function.
And then store, let's say you compile an XE and you want to store that XE to be used by subsequent task, you call the store function, and then it gets stored in the central location for other tasks to get hold of.
And this is what that task would then look like in the compass UI if you run it.
So you can see all these functions have generated steps, so there's the sync step, and there's the shell step, and there's the store step.
and here I have clicked on the shell step and you can see the output for it which is hello world.
So another part of compass is Compass has an API, a Python API for talking to a dev kit.
So it's like a Python wrapper around the Xbox or PlayStation API that you might get from Sony or Microsoft.
And this allows you to do things like connect to a dev kit, reset it, upload files, download files, launch a game, take screenshots, that kind of stuff.
So here is an example of how you might use it.
You say you get hold of a PlayStation 4, you reset the PlayStation 4, then you launch your game, you wait for 20 seconds, and then you capture a screenshot of whatever is on screen at that point.
And that screenshot then gets sent to the build server, and it'll appear in the UI there.
If the game crashes, we have code for detecting that the game's crashed, and we can then capture a crash dump of that crash.
We scrape the call stack out of that crash, and that's then how we generate that call stack that I showed you in that slide before in that red box.
And we also do things like we zip up the XE and the symbols in the crash and attach that to the task as well.
So if you see a crash, if anyone sees a crash, they can just click on it and download that, open it up and then load it immediately into the debugger and start debugging that crash.
So With traditional build servers, the list of steps that make up a task would live on the server.
You might have put them in a GUI, and it might have stored them in a database.
And then when it comes time to run a task, the server goes, OK, what's the first thing to do?
Sync source control.
So it connects to the worker and tells the worker to sync source control.
And then it goes, the next thing to do is run a shell command.
So then tell the worker to run a shell command.
and this is how compass was designed originally as well. So we have this Python script and it would run on the server and whenever you called like the sage function it would connect to the worker and run like the command on the worker. But it turned out That actually made things quite complicated because within the script you constantly had to reason about the worker and the server, for example, if you wanted to read a file, read the contents of a file of the worker, you would have to first connect to the worker and download that file to the server and then you could open it up at server.
So we changed it.
And now compass works like this.
So the compass server, when it runs a task, connects to the worker and it tells it to run an automation script.
And from then on, the worker knows everything about the task.
It knows which source control repos to sync.
It knows which steps to run.
And if there is a file to store, it itself uploads it to the central repository.
and the server basically just gets told by the worker what's happened.
So if a new step comes in or there's a new measurement to register, that information gets sent to the server and the server then logs that and it makes it appear in the UI.
So it means that a job is just a simple Python script and if you want to read a file, you just use the normal Python API to read a file.
So what are the benefits of just having these simple Python scripts be the tasks?
So one benefit is that you can just debug scripts as if they were normal Python.
Like you don't actually need a build server to run them.
So you can just take your favorite Python debugger and load up your script and iterate on it like that.
And that's in fact how a lot of these things get written, just locally on your PC without a build server.
It's easy to write and easy to understand because it's just Python.
There's loads of reference on the internet and Python is an easy language to learn.
and you also have all the power of Python, Python has a huge ecosystem of packages which can do anything from image processing to reading or writing any file format that you might like or connecting to databases and you can just include, you can just import that stuff and use it straight off.
But then the biggest benefits arguably come from the fact that your configuration is checked into source control.
So the benefits are.
Of course you have history, so you can now use your source control history to see when things have changed, who changed them and why they changed them.
You have the ability to rerun old check-ins, so let's say if you were renaming a directory and you have a piece of config that points to that directory.
Maybe traditionally you would do it by renaming the directory, checking that in, then the build would break, then you would have to go onto your server and change the config to point to the new directory and the build would work again.
Then maybe if you wanted to build an old build where the directory still had the old name, then that wouldn't work.
And with checking all of your configuration into source control, these things live together and can be checked in together, so everything works much more smoothly.
It works as you would expect.
And another cool benefit of being able to have these things as check-ins is you can use your proof build or your pre-commit build system on this.
So you could make drastic changes to your configuration, have like new tasks or new dependencies, change the way resources are allocated, and all this stuff will be isolated in your little manual build.
So all the other stuff that runs in your build server is still using the normal configuration, or you can isolate it to test your changes.
Another benefit is you can branch your configuration and it just integrates alongside your code, so any time you do an integration, if you rename the directory in one branch but another branch, all that stuff will just work and be integrated as you would expect.
The last point which is possibly one of the most interesting points is that by checking your configuration into source control.
And by having these simple Python scripts that anyone can edit, you're empowering your developers to take ownership of tests and to add tests.
Like, if you...
Traditionally, you might have, like, a person that owns the build server, or maybe a team of people that owns the build server, and if your game coders want to add a test or they want to make some change to this, they would have to, like, talk to this team of people to get that to happen.
Whereas the way this works, anyone can make these changes to...
to add tasks or change things.
And it turns out that is very empowering, lots of people do that.
So people do things like adding validation tests for a new file format or tracking the hashes of converted data so they can make sure that the asset pipeline is deterministic.
Or bigger things like a physics programmer at IW added a system to make Compass generate heat map data for collision mesh.
So the artist can then use that to figure out which parts of the map have dense collision that needs to be optimized.
And you can find loads of examples of people all over the place that are like adding their own little pieces to Compass.
It's a very powerful thing.
So there are drawbacks to doing it like this, you have to, you have a learning curve, you have a learning curve in terms of you have to learn Python and you have to learn the compass API like the SH function I showed you and there is also a learning curve in terms of you have to understand how the build server works because it's different compared to other things.
And there is also a set of costs.
With Jenkins you could just click a button and say run this command and it will do it for you but with Compass you have to, you can't do anything until you've checked in some scripts into source control.
So to help people get over that learning curve and get over that setup cost I actually set up all of the automation scripts for the studios like the initial versions of it and I maintained it myself originally.
I wrote tutorials and I wrote documentation on how people do this stuff themselves.
And using that, the people at studios taught them how to do this stuff.
And over time, they've started taking responsibility for it.
And I found that once people get that it's just a Python script, they're away.
They'll build loads of stuff on top of it.
And some studios have written huge, huge systems on top of Compass.
It's a very good place to put extra things.
So a quick note on provisioning, I said that Python, it's Python tasks that run on workers so we need Python but we also need Visual Studio and PlayStation and Xbox SDK, so how do we get all that software on to the workers?
So we currently manage all software installation using something called Puppets which is open source software.
and the way it works is each worker has a Puppet service that every 20 minutes connects to a Puppet master which knows what should be installed and the worker compares that against what is currently installed and together they figure out what needs to be uninstalled or what new software needs to be installed.
It means that everything needs to be installed via scripts and installing things via command line is not always the happiest path on Windows.
Puppet comes from Linux originally.
So there is an amount of work involved in writing all of those scripts.
And in fact, it's a person's pretty much full-time job to manage all of that stuff across all of the Call of Duty studios.
There are also issues with doing it this way.
For example, if you run an install on 100 workers, you might find that on one or two workers, the install failed for some reason.
And then the build will fail on that worker, and someone has to manually go in and figure out what's wrong and apply some kind of fix.
We sometimes also have to pause Compass because it could take up to 20 minutes for everything to install this new software.
In that time you're kind of in limbo so you might need to pause it.
And once the new software has been installed, there is no way to then go back and run an older build because that would expect the older SDK to be used.
So it kind of defeats some of the benefits that you get from checking your configuration into source control.
So we're looking at the possibility of using Windows containers to solve some of those problems that I've just mentioned.
So containers are little lightweight VMs that you install all of your software into, and then you can upload those to all your workers, and the workers can then switch between containers almost instantly.
So it's still a very new technology, but we're talking to people at Microsoft and Sony about ironing out the last few wrinkles.
And we're pretty hopeful that we'll be able to start switching to containers this year.
So next, I wanted to talk a bit about two relatively unusual features in Compass, namely error bucketing and auto retry.
So an interesting thing we do in Compass is error bucketing.
So when you do loads of automated testing, like these 50,000 tasks per day, some of them are gonna fail, and they're not always going to be tied to a specific check-in.
Sometimes things just fail randomly, for example.
Def kits sometimes fail in mysterious ways, or there might be a bug in the asset conversion which left something in a funny state, which means tasks will then fail when they hit this specific worker.
and it might not always be immediately obvious what's causing these things.
So we've added this feature to track the occurrence of errors and to make it easier to figure out how often they happen and when they started happening.
So in compass, any time a task fails, we produce a single sentence summary of that error called the error key.
We get this by looking at the log file.
We look at the error logs in the log file.
So for example, this group of error lines here.
And then based on those lines, we generate this error key.
So in this example, it would be this.
It's an assert.
And the task failure is then bucketed based on that error key.
So each bucket contains all of the tasks that failed with that error key.
So whether it's an assert in the game, or whether it's a problem connecting to a defkit, or whether it's an error due to a worker running out of memory, all of these things are grouped into their own buckets. So this idea is quite common, you might maybe When you send your game out you might collect crash dumps and then you might take the call stack of those crash dumps and then bucket the crashes based on those call stacks.
This is basically the same idea but we are using log file, log messages, log errors to bucket them instead of using crash dump call stacks.
So this error bucketing gives us some very useful information.
So this is the error bucket page for unable to allocate memory error.
So as you can see, it has this little calendar view.
So each square represents a day, and the color of the square represents the number of times that this error happened on that day.
So the darker it is, the more times it happened.
So this allows you to really easily see, like if you have a build and it has an error in, you can really easily see, has it happened before and how often is it happening?
And when did it start happening?
So in this example, you can hover over here and you can see that the first recorded occurrence is on the 6th of October, which is probably when we started tracking that.
And you can see that there's been an uptick in out of memory errors since January.
And then further down the page, you can see a list of recent occurrences.
So you can see the task that it's happening in.
As you can see, it's happening a lot in the run game MP battle XP3 task.
So that means that on Xbox One, the MP battle map is out of memory.
Sometimes errors are happening on specific resources.
You can see there's a column where you can see what resource or what def kit or work or something happened on.
So you can spot that kind of pattern very easily as well.
And we also showed screenshots at the end.
So if there's a pattern in that, you can kind of quickly gauge that.
And you can also associate information with the error book itself.
So for example, this error has a bug, a link to a bug associated with it.
So this is super useful.
like if you have an error in your build, this will allow you to immediately see, has this happened before?
And if so, you can go and visit the bug and see, is anyone working on it?
What's the conversation been about this so far?
Maybe it's already been fixed, but it hasn't been integrated into your branch yet.
All this information you can get by tying errors in your build server with a bug tracker.
Another feature that we have is related to error bucketing.
It's called auto retry.
So auto retry refers to the automatic retrying of tasks that failed.
And to be honest, the idea of automatically retrying tasks used to horrify me.
I used to think that if you have errors, that's due to bugs, and you should fix all of the bugs in your continuous integration, in your build.
But it turns out that sometimes you might have bugs that don't happen very often.
Maybe they're rare bugs, maybe in your game, and maybe fixing that particular bug is not on people's top priority list at that time.
And sometimes there might even be things that are beyond your control.
And if you keep these errors lingering in your build, then it will keep coming red, and it will keep like...
sending people emails saying you have broken a build and it will keep making that big status page go red. It degrades the faith that people have in your system for detecting errors.
So what we did is for each error bucket you can specify an action.
and anytime a task fails with that error, this action will happen.
So for example, you could say, if this particular error happens, then retry that task up to three times.
And that would be perfect for some kind of rare race condition bug in your game that you don't want to pollute your console page, let's say.
Another thing that we can do is enable cooldown mode.
Let's say you have an error and it only happens on a particular resource, it's tied to a specific resource. What can happen then? Let's say your resource, your worker PC or your def kit is out of disk space maybe and every single time a task runs on this particular worker it will fail immediately. You can get into a situation where lots of tasks hit this one worker very quickly and it will generate loads of errors in your build.
and so this cool down mode is perfect for that, because it means that whenever a task hits this error, the resource, the worker or dev kit that it ran on gets put into a cool down mode, where it's not allowed to run any more tasks for a couple of minutes, and then we retry that task on a different worker.
So it kind of, you still have to go and fix that def kit, but it changes it from being like a kind of dramatic problem where everything goes red and it needs solving right now, to something that someone can deal with in due time, a couple of hours.
And then the other thing we do is we have a disable mode, so if you know that this resource, if it hits this error, it's just not going to be usable again, then you can just have it automatically be disabled and no more task will run on it ever.
And this can potentially also be useful if there was a particular error and when this error happens you want to keep the state of that worker PC exactly as it is so you can look into it.
You can use it for that as well.
So this system does turn out to be very useful.
It saves the build maintainers a lot of time.
They don't necessarily need to look into super rare intermittent failures as much as it means that.
They can spend more time on more grave things.
One thing, one piece of feedback I have had is that some people find it unnerving to see a task go red and have it retried and see it go green.
So I would recommend if you do something like this that you allow some way to surface those errors.
You want to hide the errors from people that don't care about them, like artists maybe, but there might be people that do care about them, like the build maintainer, so they do need a way still to be able to see all that stuff happen.
So what problems did we have with Compass?
What was hard about writing Compass?
Writing a fully featured build server took a long time overall, it took many years.
But I started with a very simple tool.
I didn't set out to write like a grand system.
I started with a very simple thing and then it just kind of slowly grew over time.
One definite pain point was stability.
So for automated testing itself, if a test fails every now and again, it's not the end of the world necessarily.
But if your build server is the thing that gets relied on for making gold master builds and you're trying to ship your game, it needs to be 100% rock solid.
And it is now.
but we definitely have had problems in the past, for example, at one point we relied on a Python package that had a problem with it, what would happen is if a person cancelled a task we would use this package to terminate the process associated with that task and all of the child processes, the whole process tree, but due to a bug in this package it would occasionally also terminate a random other process and very occasionally this other random process would recompose itself.
and so all of Compass would reboot and all the tasks would stop.
And that was quite tricky and quite stressful to debug while hundreds and hundreds of people are relying on your stuff to work.
We have also run into issues with scalability.
It's obviously easy to write something that runs on a couple of PCs under your desk, but when we started scaling up to hundreds of VMs at Infinity Ward, then there were definitely places where I had to go back in and optimize things.
And another thing that we struggled with, which I guess isn't related necessarily to Compass is moving data around.
When you have 300 dev kits and they're constantly running these tests, it consumes a lot of data.
It consumes a lot of game data.
So it can take a lot of network bandwidth.
We've done various things to try and reduce that so we don't Upload all of the data to the console and then run it like that. We have the console stream the data So it's not uploading textures and uploading audio files that it's not going to use And also we try and not transfer things that haven't changed, either by taking all our data and chopping it up into little bits and only transferring the bits that have changed.
Or another method that one of the studios uses is we have like a nightly build that we upload to all of the dev kits.
And then each build after that is uploaded as a patch on that base build.
And then maintenance, maintaining the automation written on top of Compass takes a lot of time.
Again, it's not necessarily specific to Compass, but if you have a lot of this stuff running, you just have churn, churn from your game and your tooling changing.
Maybe something runs out of disk space or you've overloaded your servers.
The behavior of dev kits can change as they get flashed to newer versions.
And there's just a constant number of things that you'll have to deal with.
and that's why we write stuff like the auto retry to try and reduce the workload that's associated with that.
So I've shown you various features of compass, the console page, profile view page, I ended up writing a build server from scratch but I don't expect everyone in the audience to start writing their own build server, that would be a bit crazy.
So to finish up with...
I wanted to give you tips on how you might be able to implement the features that I have shown you on top of your own existing systems.
So all the web app UI I have shown you, like the console page and the page, they are written in Python and flask.
Flask is easy to get started with, lots of tutorials on the internet.
If you wanted to write something like this, one approach would be to have your existing test code write data to a JSON file or to my SQL server and then you could write a little Flask web app that would pull that and present that to the user.
Something like a perf overview page would be straightforward to make like that.
Of course there's other ways, some build servers might allow you to write a Java plugin.
That would be another approach.
So another thing I showed you was this, the tracking and graphing of performance and memory stats.
So for this you can take a look at InfluxDB and Grafana.
So InfluxDB and Grafana are both open source software, InfluxDB is a database like MySQL but it's specifically designed to store stats, metrics.
So you can have your automated tests upload data into InfluxDB and then you use Grafana.
Grafana is like a web app which allows you to build custom graphs and build custom dashboards and it can pull data from all kinds of sources like Prometheus and InfluxDB and Elasticsearch.
We actually use this stack as well with Call of Duty.
So we have the kind of basic graphs that we use inside the build server.
But for writing more custom dashboards or for writing more complicated graphs We didn't want to like rewrite all of this kind of stuff in the build server So we also all the metrics also get piped into InfluxDB and anyone can then write their own dashboard Grafana also allows you to do alerting.
So one thing that you might be able to do is send your memory stats into InfluxDB.
And then with Grafana, you could detect when it goes over a certain memory budget, let's say, and then could send you a Slack notification or an email or something like that.
And then finally, if your build server doesn't support configuration as code, you might be able to get some of the benefits by moving some of your logic out of the build server's GUI and into scripts that you check into source control.
So instead of using a bunch of specialized steps for running different batch files or whatever, you could have a single run shell script that can then execute a Python script and that could then sync more stuff from source control.
There are drawbacks, there are trade-offs for this, for example, if you put everything into one step, the default behaviour would be that all the log outputs would appear in one place and also it means that if you use any Jenkins plug-ins, for example, the building of the Visual Studio solution, you might have to rewrite some of that stuff yourself.
So that's the story of Compass.
I hope this has given you a bit of an insight into what goes on to making Call of Duty and making all the automated testing and profiling happen in Call of Duty.
I just want to thank all my colleagues at Activision Central and all the Activision studios because automated testing at this scale is huge and without all of their help this would not be possible.
Finally, if you would like to talk to me about any of these things or if you have any questions about how we do stuff, I would be very happy to answer questions, so please feel free to get in touch.
Also, we are always on the lookout for talented people, so if you are interested in that, please get in touch as well.
Thank you very much.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
