Can we get the mic on?
All right, cool.
Good afternoon.
How's everybody doing?
Couple quick announcements GDC wanted me to make.
If everybody could turn off their cell phones so they don't beep during the presentation and so there's not a symphony after the presentation of emails.
evaluation forms will be emailed out after the session.
So if you guys could all fill those out.
GDC wants to know which sessions were good.
I'd also like some feedback in terms of if you guys enjoyed the presentation.
So if you guys could fill those out, that'd be awesome as well.
So with that, let's get started.
This is League of Legends, scaling to millions of summoners.
Little bit about me.
My name's Scott DeLapp.
I'm a scalability architect at Riot.
I've been with Riot since 2008.
How many people played League of Legends in beta?
We had a good mix of people.
Looks like about 5%, 10%.
So in League of Legends time, I was at Riot about a year before beta, to kind of put some things in perspective.
You've got my Twitter handle and my email there on the slides.
If anybody wants to catch up after the presentation, I'd be more than happy to talk to people.
So just look me up.
A little bit about Riot.
So Riot was founded in September of 2006 by Brandon Beck and Mark Merrill.
Back when I joined Riot about four years ago, I think we had about 25 or 30 employees.
Today we've got a little bit north of 500 with offices, main offices in Santa Monica, California.
We've got an engineering office in in St. Louis, Missouri.
And then we've got a player support and community office in Dublin and another one in Seoul, South Korea.
So our mission at Riot, every company's gotta have a mission.
You've gotta have some sort of core theme that you're gonna optimize on.
you know, if you've got five things, which one do you pick?
Actually, I'm gonna restart this, because it's auto-doing timings on me, which could be a bad thing.
Let's see if this will work.
So, our mission at Riot is very simple.
It's to be the most player-focused gaming company in the world.
So what does that mean?
Well, everything that we do at Riot is viewed through the lens of...
What do we do for the players?
So whether that's an engineering decision, whether that's feedback from the forums, taking that into account.
Now that doesn't necessarily mean that every new skin idea or something like that is instantly released every two weeks right after it pops up on the forums.
But we take all that into account.
I mean, we really want to focus on what's the best player experience for everything that we do.
So League of Legends, how many people play League of Legends?
Most everybody on it. So I don't need to go into everything about League of Legends.
Ah!
This thing keeps like auto-moving.
It's possessed.
Two seconds.
Let me try it another way. I'm gonna do it in rehearsal mode.
And then that should reset the, uh...
...slides for some reason.
All right, sorry about that.
So this is a scale talk.
So it's useful in a scale talk to have some perspective on what we're talking about in terms of scale.
So we released some numbers in July of last year and basically covered that we had 15 million registered users, we had about 4 million players a month, and a peak CCU of about 500,000 players a day when we hit peak.
Now.
If we compare that to November, going forward just a little bit, we went to 32 million registered accounts, 4.2 million daily players, and 1.3 million peak concurrent users.
So a little bit of a difference.
Now, how many of you went to Travis George's talk this morning on Dominion?
One of the phrases that Travis used was Riot scale.
One of the things that's interesting about the things we do at Riot Games is those are the numbers that you have to hit when something goes out the door.
And not only do you have to hit those numbers, but you have to plan in advance.
Case in point, a feature that went out in July now needs to be able to handle a much larger amount of users in November, for instance.
So for certain types of features, Dominion.
I think it was about 12 months worth of development.
So obviously, we can't go rework Dominion between July and November to hit scale targets.
So we have to be cognizant of that as we're developing new features to have runway going forward.
Now, you take the scale, and you take the mission of being the most player-focused gaming company in the world, and then you start to look at that in terms of challenges.
And the big thing is there's lots of ways that we can solve problems, but they don't always make sense from a player perspective.
So social elements, for instance, they have to be accessible from everybody.
It doesn't make sense if five of you can chat with five more people, but you can't chat with everybody in PVP.net, for instance.
There's also crafting just an enjoyable player experience.
So one of the things we like to get right is when we release a feature, we like for everybody to be able to get their hands on it.
How many people have ever gotten a Christmas gift, and you pick the box up, and it kind of feels funny, and you open it up, and there's some inkjet printer printed out thing from Amazon that says, you will receive this in two weeks.
Your mom didn't order it on time or something, so you got this stopgap.
How many people like that?
It sucks.
So it's the same thing with gaming.
We don't want to have this feature out and like, oh, well, only 1,000 people can play it right now or something.
That's not what you want to do.
And just like the Christmas gift, you'd almost wish that you didn't get the Christmas gift because now you just got to stare at this piece of paper and you're like, this isn't fun.
Well, it's the same thing when we release features.
I mean, if everyone can't play them, it's just not fun.
I mean, it actually does the inverse.
It creates negative will of the player base and people just aren't enjoying what they're doing.
So when something goes out the door, we like to make sure that everybody can get it.
The more recent example, for instance, was ranked teams.
And then back before that, with Dominion.
Dominion was in private, was in limited beta form for a couple days.
And then flash forward to Monday, and here's Dominion.
And everybody can go play it.
So how do we create a system that balances this?
We've got scale that continues to increase.
But yet we want to deliver the highest quality all at the same time to the players.
So that's what we're going to talk about today.
Essentially, what I've tried to do is, over the last four years, we've developed a number of core strategies that have kind of evolved that we use to align with player value and being able to run League of Legends as an operational service.
And I've tried to take the core things that we do and really get down to the essence of them.
So it's not just me talking about, well, yeah, we did this cool thing.
We wrote these lines of code, and here we go.
You guys can't do anything with that.
So we're going to start with the core items, and then we're going to work back and add some color commentary of how that applies to League of Legends.
But at the end of the day, whether you've got a big project or a small project, if it's mobile or if it's a PC game, all the things that we're talking about should apply.
So the first thing that we're going to talk about is we're going to talk about embracing Java and NoSQL.
Although really most of this is NoSQL, so if you don't use Java, it's not a big deal.
We're going to talk about how we think simple is the best way to go, followed up by coding a dynamic system, some scaling best practices, and then finally monitoring everything.
So.
Embracing Java and NoSQL.
So the first problem that we have is how do we develop a system rapidly, but at the same time, plan for future capacity needs?
Just like we saw with those slides a second ago.
July is here.
November is here.
A little bit of context as we're going through the session this afternoon. The back-end platform systems for League of Legends are written in a Java-based stack. So we use a lot of different technologies at Riot. We use Adobe Flex, we use PHP, we've got a lot of C++ code. But we're going to kind of focus on this portion of the stack today that does a lot of the back-end systems with matchmaking, game server allocation, things of that nature.
As far as what the stack looks like in a little more detail, we use a kind of a mix of old and new elements.
If you've done anything in the Java ecosystem, Spring and Tomcat are pretty common things in that space.
And then we use a caching technology from Oracle called Oracle Coherence.
So before I dive into NoSQL.
When we were developing League of Legends, well, I couldn't tell you that we would be where we're at today in terms of CC or players.
We didn't always have an eye towards the future.
And part of building something is not just building it, it's being able to sustain it and grow it.
So given that, we wanted to be able to continue to develop faster and faster.
So we used a lot of open source.
You can do that with any language, whether it's in Python or in Ruby.
Big is tooling.
Technology has almost always failed my five-minute test because of tooling.
You pull something up in an IDE or some editor that's supposed to support a language, and then it doesn't work.
Or you try to run the debugger, or you try to run a profiler.
So there are languages that are more concise, but that's not the entire part of the delivery pipeline.
You have to be able to debug things.
You have to be able to performance tune.
And if you don't have the right tools to be able to do that, then you run into problems.
So we always try to take both tools and language into account when we're delivering features at Riot.
We also looked at the fact that we had a large pool of developers to pull from.
There are a lot of buzzword-centric technologies right now that are awesome technologies, but you can't hire anybody to do anything with them.
You'll find one guy, and the other two guys got a job somewhere else.
So that's also a problem.
So one of the things we liked about Java is there's a large talent pool of really awesome developers that we can work with to continue to raise the bar of our engineering teams.
As I mentioned earlier, we use a product from Oracle called Oracle Coherence.
But I like to always explain it.
If you took the name off of it and threw the source code up on GitHub, it would just be classified as a NoSQL.
Oh, PowerPoint says no.
That's good.
I blame the projector.
And let me see.
I think I can just skip ahead.
There we go.
OK, try this again.
So if you put it up on GitHub, it would just be classified as a NoSQL technology.
The other thing is when we started development of League of Legends four years ago, the NoSQL space was different than it is today.
So we do use different NoSQL technologies at Riot today.
We use Redis for some things, MongoDB.
But back when we were making decisions four years ago, to start with, this was one of the more established choices.
Coherence is used a lot in the financial space.
So there's a lot of large clusters that do a lot of high-performance computing behind it.
So the big thing that coherence gives us, and you get these with a lot of NoSQL technologies, you get horizontal scaling.
That's helped us simplify absorbing CCU growth over time.
So you can add boxes, the data partitions.
So that's a pretty good combination.
The other thing that you get from a lot of NoSQL technologies, and we talk a little bit more about this later in the presentation, it's not just about the tech.
A lot of them have patterns of the way you interact with them.
And given those patterns, the real power is what they're enforcing with those patterns that then translates out in scale and other benefits later.
And I started this with the presentation view again.
One more time.
Sorry about that, guys.
It is possessed today.
OK.
As far as how we use coherence, one way that people use caching technology is how many people use memcache?
So a lot of times with memcache, we'll do a cache to the side pattern, where you have some sort of data access tier, and then you load data from a database, and then we turn around and put the data in the cache to then read from memory.
And normally, it's some sort of distributed memory.
Because you have too much data or something, it's not going to fit on a local machine.
Well, in our case, we're going to do a cache.
we actually take the cache and pull it over here.
And it's actually the first level that the DAO talks to.
In a lot of cases, I tell the engineers, just forget that there's a database even back there.
So the benefit of that is it's much more direct at one level.
But then we're also taking advantage of all the different technologies that are in the NoSQL and slash caching layer itself.
A lot of times to get real performant things, you're going to have to get down into the technology.
The idea of we're going to do everything agnostic starts to kind of break down at some point, because you need something that you're just not going to get in an agnostic sense.
Just like if you've written SQL and then you need to get down to some stored procedure technology in Postgres or MySQL, you can't write it in plain vanilla a lot of times.
So we get a lot of power from doing that, though.
The biggest thing is grid computing. I can write one line of code and run a query or run a mutation operation and it will turn around and use 200 cores seamlessly and come back with some sort of result. I don't have to worry about well will it not come back, what if it gets duplicate data coming back, it's just one line of code and that's it.
That's a really cool thing, because now engineers at Riot, we don't have to focus on, well, how do I deal with all the plumbing of 200 cores?
I just use them.
So I can go back and do what I want to do, which is focus on player value.
We also get things, as I mentioned earlier, like transparent partitioning.
So we can add machines to the cluster, and our scale pretty much goes out linearly in a very predictable fashion.
So moving a little bit away from the setup now.
The next thing is simple is best.
So how do we develop features quickly, but at the same time don't drown ourselves in bugs and technical debt?
So a modern CPU will do more than 3 billion instructions.
That's a lot.
You can throw away and mess around with 2.5 billion instructions and still have 500 million instructions to do something with.
Most code I've seen, 500 million instructions is plenty for most types of things that you're doing.
The other thing is, technology these days is fast.
I'm not saying you don't need to pay attention to it and make smart decisions, but languages are fast, memories fast, networks fast.
Used to, when I was fresh out of college, you'd have all these debates about this language is fast, this language is slow, this is interpreted, you can never use it for that.
I don't hear a lot of that these days.
If you read hacker news, most of the stuff is not about a language being fast or slow.
I mean, there's all kinds of things with JavaScript, with Node.js out there, and Python, and all kinds of new languages.
So it's really not so much about raw language speed anymore.
The other thing is, complexity at the end of the day is the enemy of quality.
The more complex things are, I can give you a laundry list of reasons why that's a problem.
uh, you know, long methods with lots of code in them, if you try to transition it to a new engineer, you know, that, that handoff has problems. You know, there's unforeseen edge cases, there's bugs that pop up, if you want to reuse code between this piece of a system over here and this piece of a system over there, it doesn't quite fit right because it's too big. So at the end of the day, our big thing is to not overdesign. So let's, let's dig into a little bit of that. So, One of the theories that we use is this thing what we call rig the game.
So you can have some sort of operation that you need to do multiple things at the same time.
And you can do a lot of coordination, but it's usually easier to kind of divide the inputs up in advance if you can kind of flip the problem inside out.
So if we look at a visualization of that.
You can have a bunch of threads and have work, and then have data that they share, and they can sit there and coordinate.
But as you can see, just looking at this picture, the picture doesn't even look clean.
You got all these lines.
It kind of takes you a second to figure out what's going on.
On the flip side, you can take the data.
kind of nicely split it up based on pairing it up with the work.
It's a much cleaner picture.
The other thing is the code behind this is much simpler.
Once the code gets a chunk of data, it does what it needs to do, and that's it.
It doesn't have to worry about edge cases.
It doesn't have to worry about dirty reads or anything else.
It just goes about its business.
And then that pattern will pretty much stack out.
So a couple examples of that in League of Legends.
Right around season one, we went through and reworked the game server allocation algorithm.
I'll talk more about that later as well.
One of the things that we did was we wrote it with the idea that it can partition itself.
And for the longest time in production, we never turned it up more than one.
There was just a level one running back there.
But then when we got close to Dominion, if you went to Travis's presentation, one of the interesting characteristics of Dominion was the fact that games are now 20 minutes shorter for Dominion.
So if the same quantity of players that used to play Summoner's Rift now play Dominion, it's twice as many people going through the turnstiles.
So twice as many game starts, twice as many champ selects, twice as many statistics being processed, twice as many games being allocated.
So when we were load testing for Dominion, we went and cranked the knob up, so to speak, validated the results.
tweaked a couple little edge cases for things. You can run into things like starvation with this model. I mean you don't want to run out of game servers for instance. So we added a little bit of work stealing. So you didn't have one pool that got full and the other one didn't. But all in all it was a very simple migration.
So the next thing I'm going to cover today is the concept of coding a dynamic system.
And this is one of the key things that we've evolved to do at Riot over the last several years.
So the big problem is how do we handle not only monthly change, but as you guys all know, every two weeks we push out a champion and use a couple skins.
So we have biweekly change.
And then we also have what I like to call hourly change.
Change is not just when we decide that we're going to have change.
Change just kind of happens.
So I usually like to use the analogy, a single box or a developer laptop or workstation is a very predictable beast.
It's got a set amount of memory, it has a set CPU on it, you run your code, it runs kind of in a predictable amount of time, and you know what you're going to get.
A large system, however, changes while it's running.
I usually say around the office, I mean, it's basically organic.
There's inputs and outputs, and you see things ripple.
But at the same time, you may not exactly know, well, why does that ripple?
I see it, and I kind of know.
But again, why is it doing that?
There's all kinds of reasons for that.
So a couple examples, just to give you guys a few that we've seen over the years.
One thing, I think it was two summers ago, we had lots of player reports on the forums of issues in the game.
And we kind of dug into our systems a little bit.
and then it kind of went away. Well, late in the day we figured out, this happened to be Europe, there was I think a Champions League playoff going on and all the people in Europe were streaming the soccer matches over the internet.
And the pipes were just a little bit full.
So we were seeing a reflection, essentially, of the internet infrastructure in Europe.
And it wasn't just us.
All kinds of other games are seeing it, too.
So that was something that didn't have anything to do with us doing a deploy, yet we had an operational change that we had to try to mitigate.
A little different example that I'll tell on myself is back when we were in a real, real early beta.
Of course, we were a 25, 30 person company.
And our beta environment was sharing a certain amount of infrastructure and data centers that our load test environment was sharing.
And I'm sitting in the office in Santa Monica.
I've got my headphones on.
I'm pounding the heck out of the system trying to raise the scale on some stuff.
And all of a sudden everybody around me starts asking, why do we have lag in the system?
What's going on here?
And the forums go crazy.
I'm like, it's probably me.
So I clicked the little button, stopped the load test, and sure enough, everything went away.
Well, it turns out I had saturated a couple switches and a few other things sitting there in the office with my little botnet of load test harnesses.
So that was another example of something that just kind of came out of the left field.
But there's much more practical things, too.
I mean, you get hardware failures.
If you do the math on large systems, you know, a hard drive will die about once a year on average.
So if you have enough boxes, you're going to get a hard drive that dies every day.
So the question is, when do we fix this?
Do we do it next release?
Do we create a downtime?
Well, if we do it next release, what if it's a big problem?
Again, player value.
Another one would be, what if we do it during a downtime?
Well, we can pick lighter downtimes, but that doesn't mean there's not people playing.
For instance, we hired a new engineer the other day, and we were talking, and he's got a young son, so he gets up at five in the morning and would play League of Legends, get a couple games in in the morning.
Well, we might be doing downtime at 5 a.m. Pacific.
So yeah, that doesn't affect the masses, but he's just as valuable as they are.
So, you know, you wanna try to not have downtime as well when you have to fix things.
So that's not going to work.
So there's a variety of ways that we come about making things dynamic.
One of them is picking technologies with elastic properties.
As we talked about earlier, like NoSQL.
You get dynamic cluster recomposition if you need to pull nodes.
We try to pick a lot of things with stateless growth patterns so we can add boxes in and put them behind the load balancer and add capacity.
things of that nature. Now, not everything has to be elastic. A lot of times you'll get force multipliers. So like for instance, our caching tier, that helps a lot of things. It's less load on things in the front. It can kind of spread out across the caching tier. You also get things in different places in the stack, maybe sometimes behind the caching tier. So like in our case, we do right behind batching for certain things.
So, that actually gives us, in some ways, a little more robustness for the database, because the database doesn't have to work as hard. There's not as many simultaneous open connections. Being able to take a larger chunk of data, the databases can optimize a little different. So, compared to what I've seen on a lot of crazy rigs and setups with 50, 100 MySQL instances, we don't have to run near as much per CCU as what I've seen for a lot of things that just go straight to a database, for example.
Now, it's also not just about technology.
It's about what you do with your code.
Your code is probably the biggest thing that you're going to run into.
It's things that you've done.
And we've done lots of these ourselves.
I'm not going to lie.
When we first rolled League of Legends out, guess what we did?
We had properties and config files.
But that's what everybody else does.
They've got their little XML or their JSON or their text file, and hey, that works.
Works real good until the system is having an issue at 5 o'clock on Friday afternoon, and you don't want to have to reboot it to change a setting.
So all of our settings are dynamic these days.
And then we use coherence to basically propagate out the data.
But you can use all kinds of different things.
I think Netflix the other day did a white paper, something that I saw that they're using Zookeeper to push out configuration changes.
Now the other thing that people get hung up on is, well, oh, when do I push those changes?
How do I make this happen?
Because I need to make sure it happens at right, this exact second.
Well, with a larger running system or just any system in general, there is no exact second.
There's people and there's still people.
So there's just before and after.
So it doesn't matter that you get lost in the minutia of it has to go out right here.
It's just before and after and it needs to be a reasonable amount of time.
The other thing is your algorithms have to be able to take advantage of this.
You can push out a new setting, but if your system doesn't ever pay attention to it, it doesn't matter.
So a lot of our algorithms essentially, they'll loop, or they'll come back around, and every so often look and see, oh, well this value went from 5 to 10, let me adjust.
So looking at some more specific examples of this, all the thread pools that we've got, we've added wrappers to them to be able to config them on the fly.
Let's just say it's not a good thing if we have a box that's running at 10% CPU, and we have some operational issue, and Mark Mill's like, well, can we fix it?
And I'm like, well, we have lots of hardware, but no, I can't get any more threads out of it right now.
I mean, that's just silly.
So all the threads we can tweak on the fly if we need to.
We've taken this up a couple levels in recent years as well.
We can change roles in machines, pull them in and out.
This helps a lot with hot fixes.
We can do much smaller hot fixes on back end server side things now.
And we can also do different things with features without just ruling them out as part of the Big Bang Deploy.
So Dominion, again, was a perfect example of that.
Dominion was out.
we were able to do some kind of real world beta testing with it and then just flip it on.
So that works out really good to be able to manage that.
So the next thing I'm going to dive into is scaling best practices.
So what happens when we follow all these rules for scaling that we see floating around the internet and we still run into issues?
So this is my modern scaling Cliff Notes.
as I like to call it, when we do engineering, onboarding at Riot.
So we start out, and scaling is hard.
So let's get rid of some things so it's easier.
What do we get rid of?
Does anybody know?
There's all kinds of options, and then there's these things that you use all the time.
You're like, I can't get rid of that.
So then we go the other way.
We're like, all right, I'll start with a blank sheet of paper.
And we're going to just put three things on there and call it a day.
And then, once we only have three things, we can optimize.
So let's say we make a choice that the minimum user that plays League of Legends has to have a two megabit internet connection.
Well, I don't have to optimize for a 56k modem anymore, as an example.
There might be some other decisions I can make.
Maybe my art assets can be a little bit bigger, because I know they can come down faster.
So if we take that and we take that concept and we look at it with some different technologies, how many people know what MapReduce is?
So a lot of the Hadoop stuff in the BI space these days, it's a concept called MapReduce under the hood.
And really, MapReduce is saying, well, we're going to solve all of our problems with a Map step and a Reduce step.
And then we can optimize the heck out of how that operates.
With NoSQL. A lot of NoSQL is, I'm going to just take your joins away.
Because the average case is awesome on your joins, but the un-average case that we need to join two big tables causes you performance problems.
So if you don't do that most of the time, you won't have problems.
Now, interestingly enough, most NoSQL technologies are putting joins back, secondary indexes, and NoSQL, and SQL now, it's kind of funny.
Another one's CapTheorem. How many people know what CapTheorem is?
So, CAP theorem is this theory that says you've got consistency, availability, and partitioning.
And you can only pick two.
So, if you want consistency and availability, it's kind of hard to partition stuff.
Whereas, if you have availability, and you can seamlessly partition, keeping things completely consistent all the time will cause problems.
So looking at this at a little more practical level, the common example, if you read a lot of big data stuff or scaling data in general, is they talk about keeping stuff together.
So they usually always use this example of a blog entry.
And the blog entry has a comment.
You can store them in the same record.
Then you do something.
I write some crazy discussion on on the League of Legends forums with a blog, one of our blogs about something about this champion is the best champion ever. Well, everybody gets on there and says, no, you're crazy. So now I have 300 comments that are all stored in the same record. And now my code that's manipulating that has a different performance characteristic. But we're not here to talk about blog comments. So let's talk about a very practical example of the same thing.
So with League of Legends, we have servers that run lots of League of Legends server processes.
And we have a root object that represents the server.
And there's a bunch of child objects that represent the game server processes underneath that.
And we use that as part of our game server provisioning algorithm.
So, the thing that's interesting is, over time, the complexity of those child objects has gotten larger.
We have more game modes now.
There's more champions in the game.
And the number of games per server, we're more efficient, and we run larger hardware than we used to.
So if you kind of dig down into the data model a little more, you've got a machine.
It might have game instances with name and player in some different state.
Well, the problem is, all that's happening at the same time.
And these child objects start to get bigger.
And I've got tons of instances on a server.
So I go from pushing 20K of data around to maybe pushing more than a half a mega data around.
And even though I said things are fast, things aren't infinitely fast.
So I start having network transfer and object serialization being bounding factors to my game server provisioning algorithm.
You've also got the fact that this algorithm performs exactly the opposite of how you would want as an engineer.
As you get more people on the system, it goes slower.
At a minimum, you want it to run at constant speed.
And even better would be to find a clever way with technologies used like ring buffers to actually run faster when there's more load on it.
But we're going the opposite.
So essentially, we've kind of got the pipe full.
I mean, yeah, we may not be saturating a 1 gig.
you know, network segment, but we're still bound by moving all that data around.
So at this point, we had to go back before season one and review and basically say, do we really have one object anymore?
We thought we did.
But when we really dug into it, there's actually, even though both objects are about a game, they have very different roles.
So we had to go through and divide those objects up with one having the players in some different state that we need for pregame and another object that represents just the state we need for game server allocation.
At that point, that object became a very consistent fixed size.
So now the pipe is much better.
So the game server provisioning algorithm will now run pretty much at constant speed.
So essentially the moral of the story is all these rules, you can't just let them go to your head.
You've got to wonder, well, OK, when do I pick a new set of rules?
When do I slightly adjust things?
And you have to continue to look at your system as it evolves to see if now you've moved into a different place that now you need to react to.
So the final thing I want to drive into is the concept of monitoring everything.
So how do we know when we have a problem?
We've got pictures.
How many people like pictures?
Few people.
Alright.
How many people like logs with millions of operations a day?
Few more people. It's about even so far.
Well, I'm going to vote like this.
There are logs right now that I will die before I can scan them with my eyes enough to find patterns in them. I've tried.
And there's also causality. Causality in logs is hard. You can, you know, a lot of times limit things down by a thread.
But when you're going across different machines, you've got to start having, you know, ways to coordinate, you know, transactions and sequences and it just kind of spirals out of control real quick. So like in this case, this happened to be a network issue.
and we can see it ripple through that organic system I talked about earlier with load averages change and the matchmaking queue has a little hiccup and then you can see it reflect in the network traffic. So you go look at the graphs and essentially you can try to go back in time and figure out what was the trigger. Because you'll see the ripple in all kinds of different cases. Maybe it was the other thing. Maybe it was the matchmaking queue.
Well, if there's an issue with the matchmaking queue, then people can't get in champ select as fast, and you'll see network ramifications that way.
So it is kind of tricky to find the timing, but at least you've got something to start with.
So we try to monitor the heck out of everything.
And one way that we do that is we automate metrics gathering.
So in our case with Java, we've got a AOP, aspect-oriented interceptor, that basically we log all the external calls for League of Legends coming into the platform systems.
So think about that for a second.
Lots of calls.
Then we sample the internal calls.
Now, really we just need a statistical sample.
So we back this out over time.
It used to be, I don't know, 5% and 1%.
Now it's probably a tenth of a percent.
Then we log all that out, and then we automate the reporting.
And as the last bullet point says, people always ask me, well, isn't this slow?
Well, you need to be cognizant of things.
You don't want to get a disk I O backup or something like that.
So you've got to make sure that that's OK.
But general overhead on this CPU and disk wise is about 1%, versus the data that allows us to triage live systems and make evaluations of changing features.
That data's invaluable.
Now, all that data is useless if I can't look at it in a good way.
So this happens to be an example of an automated report that we run that basically compares two snapshots in time.
And it runs every day on all the different environments that we run.
Now, I can scan those numbers that are there.
But that's going to take me a second.
So as you can see, we've got the color-coded red bars that pop out real quick.
And I can say, well, something's changed.
So now, what are we going to do next?
We've got those red bars.
How are we going to dig into that problem?
We can go back to our logs and grep things.
That sounds like a good idea.
However, if you're going to bother to add one layer of automation, why stop there?
So it was very obvious when we first got this report.
The engineer that worked on it, he'd email me the sample of it.
I'd be like, great.
Now, what are we going to do with the red bar?
And I'd always ask him, the next question was always the same one.
So I'm like, well, let's just automate that.
So in this case, let's say hypothetically the red bar was like a max value.
So with a max value, I'm going to want to know distribution because a max is only telling me half the problem.
So in this case, you can vary.
I don't know if you guys can read that or not.
But down at the bottom, we've got about 113 calls that took more than two seconds.
That could have been slow ISP, all kinds of things.
But on the flip side, I've got a little bit north of 542,000 that are less than 100 milliseconds.
So if I was to give you a percentage on this, 99.99-something percent are all fine.
So statistically, there's really not a problem here.
So we need to go through and automate to make those decisions be able to just happen.
Now, some other things that we've done is game server monitoring.
Originally, there's things that dump out in logs that say how many people have been in champ select, and then we transition, and you sit there and wait.
That usually is indicative of a game server having issues.
So we'll turn around, and we've automated the log gathering of that, and it goes straight to the person that's going to react to that.
And then over time, we've started to work on things like building more sophisticated rules that the servers will basically just take themselves offline.
I mean, again, it's easier to err on the side of the player experience than it is to wake somebody up or have the network operations center take time to go respond.
It's better to pull the thing from service in a peaceful transition and then react to the problem.
So to recap, we talked a little bit about embracing technologies that are elastic, like NoSQL.
We're big believers at Riot that the best thing to do is usually the simplest thing to do.
The engineers that sit at the round table and say, oh, I need this crazy threading thing, and I need this big, huge grid computing setup.
A lot of times, we don't even need that, even though we use it.
We use it for some things, but we try not to pound the fit and put everything in it.
We always start with simple.
A dynamic system is going to give you a lot more operational flexibility than having hard-coded config files and things that are very rigid.
Monitoring everything, or excuse me, scaling best practices are pretty much, you know, even though they're the 80-20 case, there are cases that they are meant to be broken.
And then finally, you need to monitor the live system.
Otherwise, you essentially are kind of flying blind in terms of what is actually happening and what's happening to your players.
So with that, I'd be happy to take some questions.
I purposely left a little bit of time because I assume people had a number of questions.
Just walk up to the mics.
If you want to talk to anybody at Riot, there's a few people floating around the audience.
I'll be available afterward.
And then we've got a booth over in the career pavilion.
If you're interested in any of the types of things that we're doing, we're always looking for more awesome engineers to join Riot Games.
So thank you.
question on your server side stuff. How do you test everything? How do you test things like failover or if somebody accidentally a server, you can't just crash a data center and see what happens. Well, actually.
One of the things that we do that I've heard is a little different than some people is we have a couple full scale environments that we try to test as detailed as we can.
There are certain features.
It takes longer to statistically model the data, dummy it up, and run the, I like to call it, it's like the high school chemistry lab experiments.
It takes longer to do that sometimes than to actually write the code sometimes.
We do a lot of that. That's not, even for us, that's not always practical for everything. So we try to spend a lot of time of what can I do to test something that will then extrapolate. So that's another thing that we do. As far as specifically your case of failure, 95% of things that I've seen that has a white paper or a sales guy that says it fails over, doesn't.
I can't tell you how many times I've seen something that, oh yeah, it's master slave and when you pull the master it's just going to pop. It never works like that. I can give you two technologies we've got in our stack that work as advertised and the rest we've had to kind of work on. So, yeah, you have to test failover. You don't just put it in and configure it and go. I know some companies, I'm trying to think, I think it's Netflix has got a tool called Chaos Monkey that they actually let it run during set hours of the day and it just runs around and destroys stuff.
because their theory is if you can't survive when something is destroyed, then you're kind of just putting your head in the sand. I know I joke with some of the guys at Riot, I'll say things have been running good and they're like shh. I'm like no, I want a system good enough that I can taunt it daily and it still won't fall over. So those are some of the things we do. But yeah, the big thing is to test the failover cases because they don't ever work out of the box.
As far as monitoring everything, a couple of charts up there look like you use a tool called Cacti.
Is that true?
And do you also use any others?
We've used Cacti for some things.
We use Nagios for different alerting.
One of the more recent things is we ended up writing a homegrown charting tool for a lot of the non.
hardware type metrics that are actually, I call them synthetic metrics of our application. And essentially what we've done is we're big on the definition of done at Riot. And it's not just code. It's unit test, it's QA. One of the definitions of done these days at Riot is every piece of code, every feature that goes out.
you have to be able to monitor it at an application level. So developers, you know, we've got libraries that will drop a couple lines of code in for their key metrics. And then when the features go out, we make sure to explain to, you know, the network operations staff and to, you know, live support and player support, different people that will be consuming that data, what does it mean? So it's not just the engineers. So it's part of the development cycle. You have to have data. Because what we kept running into was We'd write things and then they'd go to production and then we'd be like, oh, it'd be nice if we added an alert for that, or it'd be nice if we had data for that.
And you were just too far behind, you were just too much behind the curve when you did it that way.
So we've got some homegrown tools for some of those things.
Now, there's a ton of things these days that you can just pull off the shelf.
There's three or four different things on GitHub.
I know.
StumbleUpon had a time series based tool that's backed by Hadoop.
And you can basically store like finite data, like CPUs and all kinds of different things.
And basically you don't have to get rid of any of it.
Which is really nice because you need to go back to an event and correlate it from two weeks ago.
You don't just have the aggregate.
You can get down to the detail level data.
So I'd look at some of that stuff if it's something you're interested in.
Hi, I was just kind of wondering because you did a great job of explaining scaling on technology, but I'm also kind of interested in how the human resources end of this works because if you're scaling technology, I assume you need a certain amount of human resources to be able to cope with, you know, building out of your systems because, I mean, you've already described a lot of QA and testing and programming.
So can you describe a little bit how big is your team and what do you think you need to actually implement some of these techniques and maybe some other advice in that area?
Um, as far as exact numbers these days, I'll apologize. I'm probably not the guy. But I can give you a couple ballparks.
First of all, we got a 24 hour a day network operations staff.
There's usually two or three guys basically on duty all the time.
So that's just the live side of it.
Now I think back to what you were maybe hitting on.
One of the things that we invested in, uh...
several years ago because we knew it was going to come to pass is DevOps. So we have an in-house DevOps tool that basically just like the definition of done is you have to be able to monitor things. The other definition of done is you have to have the recipes to be able to push the code.
So that scales all the way up from local QA environments to larger public beta environments to different platforms of League of Legends all over the world.
So that's actually kept the people side of this much more fixed than what it would have been otherwise.
As we've grown, because we have different territories that run as different environments, instead of having to have a whole set of people for each one.
The tool will do a lot of it, and we keep trying to automate that as much as possible.
So really you need people more on the things with racking and stacking hardware and that side of it.
But can you give me an overall staff count?
I'll tell you what, if you want to hang around after the session, there's a couple people from Riot in the audience I'll be happy to hook you up with, and they can give you a more specific number on some of that.
OK, I was just looking for ballpark.
Thank you.
Hi.
I was wondering about your use of MapReduce.
Because I guess some things, for example, getting one player's stats, you don't really need MapReduce.
I guess it's just like one get.
So I was wondering what kind of situations you use MapReduce.
A lot of the stuff we're driving on the MapReduce these days are we have a big data team at Riot.
that is diving into a number of different things with statistics and BI and things of that nature.
So we're diving into a lot of that stuff with Hadoop and MapReduce these days.
Actually, it's a pretty hot thing that we're hiring for, if you've got any interest in it.
But that's where a lot of it's coming from.
On the, I call it the more pure platform side of the world.
Um, we haven't steered to that as much.
I mean, three, four years ago, a lot of the MapReduce stuff was more batch job type things, which didn't really fit with the gameplay type experience.
So although there's different things now with, you know, that are more performant with stuff like that, but back when we were looking at some of the original stuff, we tried to probably keep the MapReduce a little more on the analytics side than the actual, you know, champions-like side of the house.
So... man 2 in audience, cool. Thanks.
Yeah.
man 3 in audience Can you talk about how and how often you guys do your load testing?
Load testing is a continual thing, as I like to say.
But primarily, there's load testing that fits in the cadence of the release that goes out every two weeks.
So that's a must.
Essentially, it's similar to the tribunal for the game.
There's a load testing tribunal.
And they have to sign off on things.
And if they don't, then various people start to kind of.
What's going on?
It has a release.
It's ready to go.
Now, that being said, we also use load testing for a number of features.
Because sometimes, as we like to call it, we have this concept that we call it the treadmill.
So when code goes in, and depending on where it gets, then you have a set amount of time for it's going to go out.
So we'll actually branch off and do specific load tests for different things.
to verify different code paths and strategies before the code goes into main.
Because sometimes, just based on the risk factors of it and things like that, you have to have the data before you can just say, all right, this is ready to go out the door now.
Hey, yeah, thanks for the talk.
I was curious on your perspective on the value of owning your own hardware and the data centers and that sort of thing, especially as it relates to scalability.
Not so much when you're like where you are now at whatever, 30 million monthly active or 11 million monthly actives, but as you're building your enterprise early on and the cost benefit is less obvious than it is now.
Yeah, it's a mix of cost and performance and predictability, I guess I'd say.
I've seen lots of things that I've spent an hour digging on something, and then it's like, oh, Amazon's just having a bad day.
So it's a mix of things, especially when you're starting out.
There's a variety of cloud providers.
I know I walked through the pavilion earlier, and there's three or four of them over there that are good places to probably start.
The bigger thing I'd probably look at is, you know, we're out with consistent performance.
I mean, we've had the, uh, I'm trying to think, was that last fall when Amazon had, like, the big massive outage?
And, you know, people were running from EBS volumes, you know, running for the hills, all kinds of stuff.
So, it adds a layer of, um...
disruption in just your performance too.
And if the system can handle that, that's great, but it's not going to be quite as consistent as real hardware.
So it's a balance you've got to maintain on all those things.
Or the other thing you can do is you can have hybridized approaches.
I've talked to a lot of different people that you use a little bit of this, use that for burst capacity, and so on and so forth.
Hey, Scott, thanks.
Managing all those servers for something this big, it's pretty tough, I'm sure.
Do you guys use any configuration management, like Puppet or Chef?
And how would you get those on the new instances as they're booting up, if there's failover or you need a scale?
And how would you promote those from dev to QA to prod once they're tested?
Gotcha.
So as I mentioned a little bit earlier on the DevOps side of things, we do use a lot of Chef.
That basically is a key part of our homegrown DevOps tool.
So that handles a decent amount of that.
As far as the hardware goes, while it is smoke tested and things like that, hardware doesn't go from dev to QA.
The software does more so.
So we don't have to worry as much about that.
And then the big thing is just having these things being elastic, because you can't put something in if the system's not made to handle it.
So I mean that's been the big thing.
Or even if...
You essentially, you've got to be able to 100% trust it.
Back in, you know, years past, we've had things that are supposed to fail over.
They work most of the time.
Well...
Or, you know, you can hot, you know, deploy things and they'll work most of the time, then there might be an issue.
What happens is you end up erring, erring on the side of, well, we're not going to do a hot deploy because we don't know what'll happen.
Even if it's a 1% case, you just can't.
So, I mean, essentially you nullified the entire thing.
It's just like we have a bad, you know, a player that has a network issue, one player can cause an issue with, you know, the entire champ select process.
So you've got to get those types of hot-swappable things rock-solid so then you can start to use them as a foundation.
And if there's any doubt at all, then essentially most of the time you end up just not using them to be safe.
So...yeah.
man in audience 5 Hi. I'm curious if you evaluated any other languages for your server back end, especially C-sharp or Erlang, and what your opinions on those are.
Yeah, I mean, we're constantly looking at things.
I mean, again, the industry continues to evolve.
So we do have some things in-house with Erlang and a little bit of stuff with Scala that we've been doing recently.
So we've got some things there.
At the time, at least, when we started things with the platform, we wanted to kind of accelerate using a mix of different open source tools and other things.
So that's kind of where we arrived at with Java, plus some of the guys on the team had experience in that to start with.
You know, as we build new pieces and modules and things, we're always kind of on the lookout for what's the best thing.
And, you know, there is no one way to do this.
I mean, like I said, these days, it's not about this language being fast or something else.
There's different characteristics, you know, with, you know, actor-based models and things like that that have pros and cons to them.
And in some situations, you know, the pros are, you know, very powerful.
man 2 in audience What is your personal opinion of Erlang?
man 1 Of? man 2 in audience Erlang.
Erlang, I've seen a lot of awesome stuff done in Erlang.
You know, it's one of the things that if I had to do over, I knew a little more about it when we started, a few other things.
It would have been a very powerful thing.
I know we've got a couple engineers that are diving into some of the things with it.
I know, I think it was Facebook, there's a PDF floating around.
They built their chat system on top of Erlang as an example.
So there's a little bit of a learning curve, especially coming from some other places.
But the power of being able to swap things in and out, and it'll do in-memory migrations on different things, is really cool for these types of systems.
So.
Good.
Thanks.
Yeah.
So you had an active MQ on one of your slides.
I wonder if you could talk about messaging and how that fits in with scalability and monitoring?
I didn't dive into it, but messaging is one of those key elements that always comes up when you're reading about building scalable systems.
You've got concepts of messaging, essentially asynchronous behavior.
Because asynchronous behavior is also going to allow your system to stretch.
you know, it can only stretch so far.
But if you've got messages going back and forth, then you've got some variance on, as far as, you know, how fast things have to move.
If they get degradated by 10%, it's okay.
Depending on how you have buffering and persistence set up, that can do some things for failover.
So messaging, you know, is a key piece of the equation.
There's different ways to do that, you know, too.
I mean, some people go down to kind of a bare-bones layer these days with the ZeroMQ.
There's the Java technologies.
More recently, I know, I think it's Kessel that Twitter's got, and I think it's Kafka's the one that LinkedIn has.
They have different characteristics.
So, yeah, that's a key part of, you know, the space as well, depending on, you know, your use case.
Anybody else?
Okay.
Well, thank you guys for coming.
Again, please fill out the evaluations.
They're going to email out afterward, and I'd be happy to hang out up here and talk for a second if anybody's got any questions.
