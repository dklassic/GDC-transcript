Hi, I'm Edgar Handy from Square Enix Japan.
So today I would like to share you about our research on application of styles transfer for expressing character AI emotion.
So for today's topic, for today's talk, we will divide it into four sections.
In the beginning, I will talk about the overview about using painting to express AI emotion.
Then we will jump into the rule-based emotional mechanics that ultimately control how the neural style transfer behave.
Then finally, we will talk about the machine learning system and how it is being implemented.
And this is will be the main beef of this, the main dish of this talk.
So before jumping into the main section, I would like to give a little bit introduction.
I'm a machine learning engineer working in AI division, Square Enix Japan.
Before I joined Square Enix, I was involved in AI ML researchers on cognitive developmental science and reinforcement learning on games.
So we just started a new division called AI Division back in January.
And we are responsible for all researchers in Square Enix and within the scope of AI, such as machine learning, improving workflow for game development and then reaching game content.
And today's presentation and this whole project would not have been possible without the help of all people that are listed here.
So I would like to say thank you beforehand.
And now let's jump into the overview.
So first, I would like to summarize the whole presentation in under one video.
This is a very short video.
And here, I would like to introduce you to a character AI called Pinoc that you see in the middle.
This is our experimental testbed for this experiment.
Pinoc is a character AI equipped with emotional system that you can see on the panel on the left side.
Pinot learned how to interact with his surroundings by interaction with the player.
You as a player control the vary that you see near Pinot's legs to encourage Pinot making interaction with the world.
And you can give feedback.
And through these interactions, you will see the emotion and mood system fluctuates all over the place.
And finally, Pinot will show you what it feels by painting its surroundings.
And this will be done with neural cell transfer.
So let's take a look at how this works.
So you can see here, you can move around the fairy and P-Nom interacts with the world.
You can give feedback through the fairies and this will affect the emotion and mood system.
As you see, everything is glowing over there.
P-Nom in the idle time can decide, Hey, I want to show you what I feel.
But rather than giving you those weird panels, Pino will occasionally, okay, I would like to paint something that I see, we will put it on a canvas and we will start a machine learning system in the background to do style transfer and it will be the painting.
And this is the painting of the rubber ball that you see at the, on the floor.
So we will get into more detail about this and the full version of the video later on.
So this is summarizes everything about what we are going to do.
So now let's jump into the talk about paintings.
So generally, when we want to express character AI emotion in games, it has been limited to this triad facial expression, vocal information, and body gesture.
And we are trying, and in Square Enix, we are trying to explore a new way to let our AI express what they feel in many variations possible.
And we see painting as one of the options.
So paintings have been a natural medium for us humans to express what we feel, what we interpret, about our surroundings, and we would like to apply the same concept to the AI to give a new way for AI to interact with the player.
And all this system is implemented using neural style transfer, which we will jump into the detail later on.
So before we talk about the machine learning system, I will have to explain to you about the rule-based system of the AI called emotion mechanics.
As I mentioned before, Pinot has an emotion that controls the style transfer.
This emotion is defined inside this AI, is defined by emotions and mood, which is a short-term feeling and a long-term feeling.
The talk in depth about this topic has been done by my colleague, Goh Chebweda, back in GDC 2019.
If you have any interest in this, please refer to his talk.
So we want to put everything together, but we need a model.
First, we need a model for the emotion, and we apply a modified OCC model.
As you can see, it's on the diagram on the right side, OCC model.
This is a modified version of this.
And what the AI feels is affected by three aspects.
First is the consequence of an event that the AI observe, whether they are pleased or displeased towards that, whether the AI is approving or disapproving towards an action of the other agent or another AI, or whether the AI likes or dislikes an object.
And all the emotion that is implemented in this game is within the red box that you see at the bottom of the diagram. Multiple of these emotions can be activated at the same time. We will take a look at this mechanics later.
Now we have to model the mode system.
For this, we also use a modified system called modified path model that you can see in the diagram in the middle.
It consists of two axes, and we can divide this two-dimensional space into four quadrants.
The first is exuberant, docile, afraid, and hostile, where exuberant corresponds to all positive axes, and afraid corresponds to all negative axes.
Now we have the model for each individual system for emotion and mood.
To get the final system, we have to put everything together, as you can see on the diagram on the right side.
We put all the implemented emotions inside the two-dimensional mood space.
And you can see a red dot in the middle.
That is the current mode value of the AI.
So the current mode value of the AI is dynamic.
It changes over time.
And it changes according to the activated emotion.
For example, if the hope emotion at the top left corner of the space is activated, then the red dot will gradually move towards hope.
And if multiple of them activated at the same time, then we will have to compute the resultant movement.
So this mode value will be sent to the machine learning system later to define its final results.
Now, we have the emotion system done.
Let's take a look at the neural style transfer.
Now, this is the machine learning topic which will be the main dish of today's talk.
I think most people here already know what neural style transfer is because it's very popular lately.
It's been going for years.
But please let me give a short introduction to it just in case.
Neural style transfer is a machine learning technique for you to do stylization on a given arbitrary content image.
As you can see on the left side, it's my dog.
You put them inside the neural network and you set the reference style that you want to use.
For example, an abstract painting that you see at the bottom.
And the result is on the right side.
For our use cases, there are four types of implementations that we do, that we did.
The first one is basic style transfer, which we only transfer one style.
You already saw that in the previous slide.
And then the next one is blending style transfer, mask style transfer, and color control style transfer.
We will jump into it one by one.
However, due to the limited amount of time that we have today, we have to skip the technical detail on the color control style transfer.
But I will get into the overview just a little bit.
So blending style transfer, as the name suggests, it's a technique for you to blend multiple styles, two or more, into one content image, as you can see in the gray box.
For ease of comparison, I also put the basic style transfer inside the blue box, so you can see what the difference, what a difference when you do only one style transfer and a blending of the two styles.
But, okay, the example is only two styles, but you can actually do more.
For mass style transfer, it's similar to the blending style transfer.
However, we use a soft mass to define, to localize two style regions.
In each region, you can also do multiple blend of styles.
For example, in our use case, we only have two regions, the bright region and the dark region.
And you can see that the bright region corresponds to the blue style.
And the background, the dark regions, corresponds to the blend of two styles.
So.
Finally, this is color control style transfer, which is the one that we will skip the detail for today.
This is a technique where you do style transfer however you can control the degree between the content image or the style image color. As you can see at the bottom, 100% corresponds to basic style transfer, 0% corresponds to style transfer however you would like to you maintain the the content image color. But if you notice, you still retain the style image touch.
So it looks like a painting of a dog. Also, you can also do blending with this.
However, it will be the outside of today's topic.
To implement the style transfer, we use an architecture based on the previous work called a learn representation for R6 style.
This architecture is extremely simple.
It consists of only a bunch of convolutions, upsampling, and residual blocks.
To get into a little bit more detail inside, we can see feature extraction layer, downsampling layer of the feature maps.
Then we do bottleneck computations with residual blocks.
After that, finally, we upsample the feature maps back into the pixel space, and you get the final results of the style transfer.
The stylization computation is mostly done inside the orange block called conditional instance normalization, or I will call it SIN.
Zooming in the scene, we see two operations, normalization and f-in transformation of the given feature maps. And this is done individually for each channel. The f-in transform here is basically just a scaling and shifting operations with a learnable f-in parameters. And all these learnable f-in parameters are unique for each style, so we have to learn them one by one.
Now in order for you to blend the styles, assume that we have two styles that corresponds to two affine parameters, gamma 1, beta 1, gamma 2, and beta 2.
If you want to have 70%, 30% of the blend of the styles, then what we do is just compute the complex combination of them by weighting each.
its parameters, and then sum them over.
And you will get the new gamma and beta, which corresponds to the blend of the 70% and 30% of the styles.
So in order for you to do the mask style transfer, we have to augment the SYN module into something called mask conditional instance normalization.
Now this is almost exactly the same as before, however we take an additional input which is a soft mask to define the style localizations area.
Zooming in to get into the detail.
Mask conditional instance normalization consists again of two operations, normalize and FN transform, but this time with the mask inside there.
We give the soft region mask, as you can see on the right diagram.
What we do here first, we divide the feature maps on each channel into two regions.
So we define the region 1 as all feature maps pixels that correspond to the mask value greater than 0.5, and the rest of them into the region 2.
After that, we do normalization for each region.
We do not mix them.
And finally, we do a theme transformation with new parameters called mask shift and mask scale parameters.
And I will have to remind you that because each region has its own style, we have a theme parameter that is unique for each region.
Now, there's a little bit more detail.
If you're an artist, you would like to, okay, I don't want to look at this.
Feel free to go to sleep a little bit.
And I will go a little bit into the more engineering aspect of this one.
In order for we to do F-in transformation on each feature map because it's a 2D tensors, we can imagine that each parameter, gamma 1 and gamma 2, is also a tensor.
Gamma 1 is an F-in parameter correspond to region 1.
Gamma 2 is an F-in parameter correspond to the region 2.
Now in order to make, in order to blend this into a localization, into a stylized localization, So it's a localized stylization.
We first use the original mask to do element-wise multiplication into the tensor of the scale parameter.
And then we create an inverted mask.
And then again, we do element-wise multiplication.
to the fn parameter of the second region and then we sum them up. The final result is the mask scale parameter that we can apply to the whole feature maps to get the mask stylization.
So if you realize this is basically a complex combination as we did before for spiral blending, but this time we are doing a blending but for special blending. It is the style localization.
And finally, we also do the same thing for shift parameters.
Basically, we define again original mask and inverted mask, so element-wise multiplication.
And we sum them together to get the final mask shift parameters.
So we follow the same training setup as the previous paper to model, to train the model.
So if you would like to know the detail how to train this model, please refer to the original paper because it will give you all the awesome stuff to do this.
It's extremely simple.
Finally, the model is optimizing FIGG-16 perceptual loss, which is a very common optimization strategy for style transfer.
and we train all of this inside Cancer Flow.
Now we already know the rule-based emotion module.
We have the neural style transfer.
If we combine all of this, you can still get the diagram in the middle.
You have emotion module that outputs the mode value, where we can compute the style which, or what we call the style of impedimenters, to compute either the style localization or the blend of those multiple styles.
And then we send all of those into the style transfer module, and you get the final results on the right side.
Now, we have explained the motion module and the slice transfer module.
We get everything.
So we would like to take a look at it again at the video, the full version of the video of the previous video to look at the more detail on what's happening.
As I said before, Pinot is trying to learn how to interact with the world.
You as a fairy who control the fairy can encourage whatever it can do.
So as you can see here, Pinot is eating the ball.
and it is a bad thing to do. We can give a feedback to Finn Pino, that's a bad thing.
This will in the background affect Pino's behavior towards that ball later, but that's out of the scope of today's topic. And according to what Pino is doing and the user interaction that you give through the fairy, it will affect what Pino feels.
And you see the load system down there, it's fluctuating over time.
Now, in the idle time, Kino would like to share you what it feels.
Rather than showing you the panel that you saw before, Kino will show it by painting what it sees.
For example, the very rubber ball that they just ate before.
and we do and it is implemented using the style transfer that we saw before where the Pino's emotional system affecting the final results of the appropriate style being applied to the canvas.
So just like any other experiments, there are something that went bad and went good for this research.
However, there are a little bit of difficulty on this one because art is a subjective matter.
It's really hard to say which style is actually being represented properly.
inside our neural network. For example, for me, I think the top pictures are good and the bottom pictures are bad, but that's not really the case for other people. So in the end, we have to choose the appropriate style for each emotion, test it inside the neural network, see if it's bad, then we will throw the style. If it's good, then we will adopt the style. So it's really hard to find like a set of hyper parameters that can grant you the best representation for all the style that you want to use. Now we have to explain the machine learning system.
We would like to see how it's actually being implemented inside the game.
The game that you saw before is implemented inside UE4.
And since it's UE4, we have to implement the style transfer using C++.
First, we have to construct the style transfer pipeline.
As you can see in the diagram here, they start from the camera view, which corresponds to what the AI sees.
From the camera view, you get the texture that will be convertible to pixels.
And you can also do object detection to generate the soft mask.
From emotion module, as we have seen before, we can compute the style weights, get all of these three things, send all of them into style transfer module, and you get the final stylization results.
Finally, we apply back the results into texture, and you see what was drawn on the canvas.
To execute this system, we have two choices, obviously.
Whether we want to use CPU or GPU.
So we have to wait which one is better for this problem.
First, let's take a look at GPU.
When we are trying to implement this, V4 doesn't have GPU interference support for TensorFlow model.
So logically thinking, we can try to implement TensorFlow GPU inside UE4.
However, due to the complexities of the GPU library of TensorFlow, it is way too expensive to do.
And we also try ONNX Runtime. That one is easier to be implemented for GPU inference.
However, it is not portable to consoles due to the lack of CUDA support.
We can also try to write a GPGP core share code for a console, but we find that it's way too expensive for such a small experiment.
And finally, even though we can do everything that we want with the GPU and consoles, for our use case, the GPU budget will be prioritized for graphics, and it's not really a good idea for us to use the GPU into the table.
And when we are looking at the CPU, we try to implement the TensorFlow Lite.
So TensorFlow Lite is a sublibrary of TensorFlow that you can do CPU inference.
It was actually dedicated for microcontrollers and ARM architecture.
But yeah, we can try to adjust the source codes inside UE4 to do that.
And TensorFlow Lite is also very small.
So there are only fewer source codes to be adjusted such that it can compile nicely.
CPU, even though people say it's slow, it's fast enough for our use case because we can put animations to buffer the computation time.
And then by using CPU, there's no resource conflict with the GPU to do graphics rendering.
And finally, we can also port them into console for x86, x86-4, and ARM architecture, which is something that we really want to do.
So finally, we cross out the GPU and we go with the CPU options.
Now to fit everything inside UE4, we have to take necessary source codes from TensorFlow Lite library, put them inside UE4 as a plug-ins.
However, this is not going to compile well.
So what we have to do is first adjust a lot of source codes, trial and error, trying to make sure that UE4 and the TensorFlow Lite would compile together nicely.
After days of experiments on trying out this, we finally get everything running well.
And finally, we have a nice plugin where we can do the API call from the game codes to read the TensorFlow format model and do the CPU inference.
And after all those things for, I think, around a week, we finally get this.
This is the in-game UI of the style transfer.
On the left side, you can see the camera view of the AI.
That's what Pino is looking at.
And the corresponding mask after we do the object detection.
The final result of the style transfer is on the far right side.
And the blend is shown in the middle, where you can see the top panel corresponds to the style implemented at the background.
It only consists of 100% of one style.
And the bottom panel corresponds to the styles, the blend of styles that is applied to the box.
And for this experiment.
this style corresponds to the emotional, to the mood value that is shown in the middle diagram.
So I would like to emphasize here, is that this is a test bed where we can try multiple styles.
So for the one that we are looking, where we are showing you right now, is just like, it's just an example of how we can try to blend multiple styles by playing around inside this mood space.
So at the end of the day, we still have to choose which style is better to express the appropriate emotions of the AI.
So to summarize today's talk, we explore paintings with style transfer as a new way to express AI emotions.
The AI emotions itself is modeled using modified OCC and PET model.
The appropriate styles of the style transfer is selected according to the mood value of the AI.
And we also introduce four use cases that we did in this experiment, which is basic style transfer, blending, mask, and color control of style transfer.
Finally, we implemented the whole system inside UE4 using C++ interface from TF Lite for CPU inference.
So before we close the presentation for today, I would like to share with you one weird experiment that we did.
We tried something called G-buffer style transfer, and this is inside UE4.
This style transfer leverages the game engine GBuffer.
And we do style transfer into each component of the GBuffer before the final image rendering.
So let's take a look at the basic style transfer at the top panel.
You see in the middle, that's the UE4 scene.
And if we take a picture of that, and we do basic style transfer as usual, treating it as an image, you get the results on the right side.
But if you look at the bottom where we do the gbuffer style transfer, that's the same uniform scene.
However, what we do is that we access the gbuffer components.
We apply that corresponding yellow styles into some of the gbuffer components.
And what you get is the result on the bottom right corner, which looks really fascinating.
So thank you very much for coming to my sessions.
If you have any extra questions later, please feel free to contact me by either email, Twitter, or LinkedIn.
I also tweet a lot about machine learning that will be useful for game development.
If you are interested, please check it out.
And thank you very much for coming to the session.
