Hi everyone, welcome to Designing for AR, a post-mortem on the development of World.
So before I get started, I want to introduce you to myself, what World is, and possibly what AR is for those who are unfamiliar.
So my name is Vu, I'm a software engineer at Phenomena, I work on World and Wattam.
Before that I was a software engineer at Maxis for three years, I worked on The Sim City that came out in 2013, and a bunch of other gameplay prototypes that you probably won't see the light of day.
Before that I worked on an AR game called Nerd Herder where you try to get office workers back into their cubicles. You can kind of see it there. And even before that I worked on a Rock'em Sock'em AR game that was shown at the release of the V4ia SDK back in 2010.
Let's clarify some things about this talk. It's not going to be a technical talk where I'm going to show code or anything like that. It's going to be more about the lessons that we learned and the techniques that worked for us in the development world. So what is augmented reality? I'm sure a lot of you guys know, otherwise you wouldn't be here, but I'm going to clarify just for those that might not.
In general, I like to think that augmented reality is like the layering of virtual content onto the real world versus VR where it's more like immersing yourself into another world, taking you out of this reality. Some of the examples of AR that you might have used in your current day-to-day could be like Pokemon Go or Snapchat lenses or the line of scrimmage visualization in football. Just probably one of the most common versions. So what is world?
World is a whimsical AR game designed by Kira Takahashi. He's well known for games like Katamari Damacy and Nobby Nobby Boy. It took about three to four months to develop and a small team of about five people, an animator, a designer, producer and audio designer and engineer myself. World is built on Unity using Tango as the augmented reality platform. Here's a video of World to get you guys to have a better understanding of what it is.
Hello, I'm Takahashi.
Today, I'll be showing you an AR game called World.
At first glance, it looks like nothing much, but if you play through World and Mob2Pro, you'll see that this world is spread out.
This is the protagonist of the game.
And to teach this girl the shape of the air, I'll be mapping it.
The wall with the ? mark is green.
If the game becomes a rain pattern, mapping is success.
After mapping, the game starts.
Place the eyes on the table.
And let the rain fall from the clouds hanging in the sky.
Let the flowers bloom.
When the flowers bloom, a new item is obtained, and the next development begins from that item.
Once you reach a certain goal, the sandbox mode will be unlocked.
It's possible to do anything with this game.
It's crazy.
I hope that gave you an idea of how weird and silly this game is. Okay. So let's move on to what is Tango. What is Tango?
So Tangimo is Google's augmented reality platform that helps you do AR without the use of markers or GPS or anything like that.
It uses an infrared technology similar to like Kinect or HoloLens to scan the environment and let you do what you see here, kind of imagining surfaces rather than discrete images.
There's two devices that support Tango right now. The Tango dev kit is one of them and the Fab 2 Pro by Lenovo that just came out this week for consumer purchase. They're both Android-based devices. You hold them in your hands. They're basically normal phones. That means you have a touch screen. So that gives us interactions unlike HoloLens or Magic Leap where you have to do gestures or have some kind of controller.
When you start designing for Tango in general and AR in general, there are some constraints you should think about, such as the player having full control of the camera. A lot of people when they get it, they'll just stand there and look at the floor, for example, because they don't know what to do. They've never experienced it before.
So unlike normal games where they have, the developer has control of like the ability to show cut scenes or animations, you can't really. You have to introduce players to look at things and find ways to get them to pay attention to what you really want them to pay attention to. I'm going to guide you guys through some of the techniques that we used and that worked for us.
The next thing that you should consider is the environment itself. So unlike traditional VR games, traditional games and VR games, the content isn't completely authored. To make a really good AR game, you want to use the environment around you directly. Tango is capable of mapping everything, including the ceilings, the walls, chairs, tables, whatever.
So you don't have any control. Someone could play in a messy room like that.
I'll be talking about some visualizations and ways to use the environment to give a more meaningful AR experience.
In addition to camera controls and environments, you have to deal with the regular limitations of Android devices and mobile devices today. That means worrying about performance from physics, straw calls and things like that. But also thinking about how the tracking algorithms and scanning algorithms, meshing algorithms affect the total performance of your application.
You have to think about heat and battery also since this is a handheld device. This is someone's daily phone potentially. So if you drain their battery in like five seconds, they're not going to play your app ever again. You also have to think about comfort. These current devices are pretty big. Imagine a phablet. That's bigger than an iPhone 7 plus today. And they're heavy. So let's first talk about player attention. So the first thing In world there's a story mode that we made that players have to do specific actions and look at specific things or go to specific places to advance through the story. So I'm going to give you guys some of the techniques that we use. I break them up into two categories. One is direct techniques or methods where they're independent of where the player is looking. The other is indirect techniques where it kind of helps guide the player's view to what you want them to see.
The first technique is just displaying UI on the screen.
Since we have a screen and people are already used to looking at it, you have that ability to just show flat UI.
We try to make it more effective by showing animations as to grab players' attention.
In this case, we have the menu person, which is actually like a character that talks to you.
It kind of catches people off guard, and so people pay attention to it more.
The second method we use is attaching objects to the camera. So here we have our character called person. He's attached to the camera and giving instructions to teach people how to scan a room for the first time. And we found this to be much more engaging than just like showing a text prompt. People like really respond to the character's requests.
The next method is camera tracking. So we have objects that fly around and try to stay in front of the camera within view. In this case, we have a rocket character that once you unlock him, he flies to the camera and delivers you some meaningful dialogue about what to do next in the story. In general, these methods are used to convey really important information about how to even use Tango and devices and also advance the storyline.
The next set of methods are indirect methods.
The first one we use is teleportation into view, where as the player is doing an action, we teleport our main character next to that action within view so that they get the proper feedback they need.
Okay.
Next we also use the movement of characters to guide players. We found in our play test that people really like to look at this little guy and just follow him everywhere, like watch him climb walls and things like that. So we use this advantage to spawn, meaning like the next steps nearby. This allowed people to kind of organically discover their space instead of just being told exactly where to go. Okay. Finally we have...
VFX trails. And this is kind of interesting because since the viewport is so small you have to guide the player to look at other things within that viewport. So they could be looking at this flower, for example, and do an action. In this case, we have the cloud that they've placed earlier produce these raindrops, which actually causes them to look up and remember that they put the cloud there in the first place. So here's an example of all the techniques in combination. So here's an example of all the techniques in combination. So here's an At first, I'll let this repeat. The player is guided to this rocket by the character walking to it. When he uses the rocket, it teleports the character so that the action is really meaningful and they know something is about to happen. And then we use a VFX trail to help people see the results of their actions and then move on to the next steps.
In addition to this we use a visual metaphor to show points of interest. Like these question mark textures appear on anything that has interesting interactions that they haven't used before or forgot about to help remind them that these are interesting things that they could check out again.
So now that people are looking at the things you want them to look at, we have to teach them how to interact with those objects in real, in 3D space. The first thing we have is tapping.
Since we have a touch screen, tapping is a natural interaction that people already understand. So a lot of objects have unique behaviors such as breaking a rock to reveal a new item. Or maybe they'll play a sound or let you nudge them around the physics in the world.
We also have touch and drag which lets you move objects around really intuitively. You can just touch it and drag it.
It's this immediate response that's really effective to get people to see the virtual space that you've created for them.
Okay. And then we also have, you know, we have a lot of These magic hands that we tried early on that kind of act as indirect placement mechanisms. So you grab something by tapping on the hand and then you tap the hand again to place it down in the world. We decided not to use this in the end because it was kind of confusing for people that are used to touch and drag. They're not used to controller based games and things like that. We also have these grow and shrink rays. We wanted to explore like, you know, different mechanisms for interacting with objects. These rays let you grow and shrink things arbitrarily. It's super fun to do, just seeing a giant pyramid in your room. And then we also have this context menu that lets people easily discover new abilities and items and helped us design items so that you can have super unique behaviors and multiple behaviors.
Okay. So let's look at some environmental interactions. When you're doing an AR game, you don't want the environment to just be static. You don't want them to just float a Pokemon in space and have them not interact with the world in any way.
So, and also you want them to show like you're using the space in a meaningful way. So one of the things we did is programmatically spawn objects into the world. One of the techniques we use is relative placement to the camera. A lot of, since we know where the camera is and what you're looking at and we also have a good idea that you're looking at the person or something nearby, we can spawn another object using a ray cast from the camera into the nearby location.
This kind of ensures that the player is able to see this object.
This image did not load.
So OK, hold on.
I'm going to restart the.
OK, so this is using relative placement.
So instead of using a ray cast from the camera, we're using a ray cast from an object that the player recently placed into the world.
We know that this is a valid location, since they just did it.
And the only way to actively play something is to put it on a valid surface.
Where's my mouse?
Okay, next we have placement smoothing.
So normally if you just scan an environment, it's super bumpy, super noisy in many cases.
If you just only use one recast, you get this kind of jittered movement where the object's orienting all over the place.
So here on the bottom, we're using a multi-sampling technique where we're casting a ray from the center and also like four rays at the perimeter to average out a better position for the objects.
Okay. We also wanted to let the player feel like they're embodied in the space. Here we've attached a collider to the camera allowing players to push away physical objects such as the cloud. This also illustrates presence by having the house open up as you get closer and all the flowers actually turn to look at you wherever you are.
So we also wanted the player to feel like it's really easy to hold the game, hold the device and play the game. So one of the things we wanted to do was make sure that any orientation worked. So you could play it in portrait or landscape. And beyond that, we also wanted to teach players how to actually interact with AR in general and how to interact with our game.
At first we had all the interactions available with simple text prompts to show what did what.
This was super confusing for people and also paralyzing.
They just didn't know what to do because there's so much.
So instead we made a staggered tutorial.
And I'm going to walk briefly through what we did.
Why isn't it loading?
I'm going to open up a different slide show.
Okay.
So here we introduce the player to room scanning. We use a texture that shows question marks to kind of show where they haven't been and a grid to show where they have been and scanned. This is super effective because people really like to try to get rid of all of the question marks. We also use this percentage to give an idea of like you've scanned enough so you're ready to play.
In addition to this, we also did a first time user experience for people that have never tried AR and this is the first time they've opened the app where a person comes out and talks to you. I believe I showed an image earlier. Next we teach object placement. So we intentionally give the player only one object at the bottom of their menu and show a drag indicator to help them understand that they can touch and drag this and put it in their world. When they do it, we immediately instantiate it and it follows their finger around.
You can see here. Next we also teach players that it's safe and okay to move around. So we first try to get the player to move by just showing a cute animation of the character popping out and enticing you to come closer. But a lot of people don't move. They're totally paralyzed. They don't know that they can move for some reason. So we show the move closer pop up and that tends to always get them to move.
Next we show tapping as a common interaction. This unlocks their first object which is the cloud and then we teach them that it's okay to drag things to any surface. So part of the tutorial is we have them drag the cloud to the ceiling and this really opens up their world to more than just playing on the floor.
And then at that point they have almost all they need to know except the context menu where we teach them to use it to grow the sprout. So they tap the sprout, press the rain icon, and then see for the first time particles raining on the sprout. And it's a really nice moment.
Okay. So... I'm running out of time, but I'm almost there.
Okay. So there's, we wanted a really flexible game play. Tango is kind of iffy sometimes when you're scanning. It can be really noisy. It might miss areas and make holes in the mesh.
And then objects can be lost all the time. So we wanted super flexible and easy to approach game play. So first there's no loose condition. There's no wrong way to play. You can place anything anywhere. Next is...
Auto saving, so if you accidentally close your app or something like that, we do an auto save so you can easily recover. Then object recovery, so we have a mechanism for say if you lost an object behind a wall somehow, you can easily bring it back in front of you and then place it again in the world. And then we have a sandbox mode that's completely free and lets people explore AR for whatever they want to do.
Virtual spaces, so this is early rendering of our space. We wanted to see if we could do something.
where we generated a virtual land. This really destroys the effect of AR. This is much more VR. So here's the same image with our current technique. We use a triplanar texture shader and so we shade different materials based on the surface of the mesh. Here we During game play we only render a small radius around your cursor and around the player so that it's not so noisy when you play. We're also dynamically continuously meshing so you can actually expand your play space as you play. We also experimented with some subtraction techniques. So here you can see this hole is actually looks like it's cutting into the world. And here's a more extreme example where we have this huge cavernous space in the ceiling.
Next we also experimented with tinting the image itself to give this global lighting change effect which is really surprising to a lot of people. And further enhance the feel of the world. We make sure the 3D cursor orients properly and also when you tap it shows an effect where it needs to be. Do you want to take a picture?
And also, using the visual aesthetic that Kata designed, it's very simple and stylized, so it's really easy to accept this unrealistic movement of this character climbing walls and just going up surfaces.
We also tried to do a lot of physicality to really drive in the fact that this is in your real world.
So here we have a cloud on a spring that reacts as if you expect it, the same way you expect it to.
And we have some basic buoyancy simulation to further enhance this feel.
One thing we discovered pretty late was thinking about object permanence. So in an AR game, you kind of want things to include virtual objects to make them feel more like they're actually in the space. But this is adverse to gameplay because players will lose the objects they place. They forget where they put it. So we did this silhouette rendering to help them find things. All right.
So one of the technical limitations of Tango was saving and loading. They wanted us to use their area description file to save an area but instead it didn't really give much feedback that it was working and when it did work it just happened or not. There's no percentage or anything like that. So we came up with this technique where we take an image of the place that you're playing and then we do an edge detection algorithm to let the player self-align their world back in place. And this was actually really effective and people found it to be kind of fun.
So quickly go over some optimization techniques for just mobile and Unity in general.
So make sure you're doing pretty low poly.
This has just the same limitations of any 3D game on mobile.
Low poly share shaders and materials.
Also try to batch objects to reduce rendering.
And also, the newer devices have extremely high resolution, so we're actually rendering the game in a much smaller resolution and upscaling it to gain a lot of performance. So in Unity, we had issues with our load times. It used to take like 12, 10 seconds. And we discovered that if you set your audio to asynchronously load, it's much, much faster.
You can also crunch textures to reduce your APK size.
This really helped us.
And then Atlas sprites to get tons of draw callback, like a lot of games might not do this for, because it's not completely obvious and automatic.
And also use physics layers.
These are, these will help you filter out the hits you want and also improve performance significantly.
Okay. One thing in Unity, you can also modify it really well to kind of simulate real world scanning, different orientations and we even added like a debug console that lets you change languages and clear meshes and things like that. It might be useful for you in iteration time and things like that. So you basically can play the entire game in the editor.
And also, don't forget about localization.
This is something we kind of left to the end, but luckily we found a really good package on Asset Store to help us with this.
All right.
So some of the final takeaways.
Just remember, be aware of the player's presence and what they're looking at, and the techniques that I just talked about to guide them to see what you want them to see.
And then also be very adaptive to the environment. You're not going to know where your player is going to play. They could be in a bathroom, they could be in a plane. And whatever you're doing needs to work anywhere basically. And also remember to test. You don't know what's working until you give it to someone that's brand new to this technology and have never played your game before. You're always so close to it that you understand it intuitively but other people will not.
And that's it. Do you guys have any questions?
APPLAUSE The first slide? Okay. Oops, what?
Oh no, I have to go all the way. The unity one or the mobile one? Okay. So I'm going to So Atlas Sprites is like combining all your 2D textures into one large texture to make it so it can do basically all your UI rendering in one draw call. Yes?
So it's not super precise, but since the game is so adaptable and we do a rescan anyways, the objects usually landed close to where you originally put them.
And so because of our game's design, it was OK.
But it also allowed you to just place things arbitrarily.
So you could go load in a different room if you wanted to. Yes?
Yeah, I have a question.
When you are developing in Unity.
Do you need to have a webcam and scan every time or do you have a pre-generated?
So what we did was create a dynamic mesh that was procedurally generated within the editor and so you could walk around and do things within the editor.
So we simulated a real room instead of using a webcam or anything like that.
This isn't like Vuforia where that would work.
This uses different technology that emits infrared rays to scan the room.
Yeah, well, I mean, so you need to have something pre-generated.
And how do you do that in Unity?
Is part of your game, or?
We did it procedurally.
But you could pull in a different model and have that be on, say, a different layer or something like that.
We're not using exactly the tech when we're doing it in Unity.
We're simulating an approximation.
Yeah.
My final question is, when you load the scene, do you have to be on the same room?
Or what happens if you are in a very, very different room?
So like I said, since we're using not the area description file, you can just arbitrary load anywhere. It won't align perfectly, but it will let you have those objects again. No problem. When you load into a new scene, do you follow the physics of the room so that things will fall onto the ground right.
Yeah, so...
I'm not sure if this picture really showed it, but if you did have clouds and things, these would rise again if they were in the balloon mode or if you had balls, then they would fall into the water and things like that. So everything, I save out the physics forces and everything too. So they just kind of restore. What middleware did you use other than Unity and the standard stuff that comes with it?
I only really use lean tween and a lean tween for a lot of these UI animations and I2 localization for our localization support. It has integration of Google docs and Google sheets. Everything else is custom. We even wrote our own audio system because WI wasn't working on the devices at the time.
Even the UI orientation stuff is custom because we wanted certain elements to stay in place and didn't want the hardware to handle it. I think we're almost out of time. I don't know. I guess that's it.
