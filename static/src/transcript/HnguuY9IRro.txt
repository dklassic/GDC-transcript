Welcome to Marvel's Spider-Man Procedural Lighting Tools, GDC.
The conference associates have asked me to make a couple of reminders.
Please silence your cell phones or put them in pleasure mode.
And remember to fill out the survey that they'll send out after the presentation.
My name's X-Ray Halperin.
I am a senior technical artist at Insomniac Games.
It's my pleasure to be here with you today.
This is my second GDC, my first time speaking here.
I think it was last year in this very room I had the pleasure of watching Yoko Taro present an amazing talk on Nier Automata.
And I'm really happy to be in the same room that that happened.
Insomniac's a great place to work.
We've got a lot of current job openings, so go to our webpage, check these out.
We're looking for talented folks.
I wanna give a big thank you and a shout out to Sony Interactive Media, SideFX Software, Jeff Hanna and GDC, as well as everyone at Insomniac Games.
There's a number of talks by Insomniacs about Spider-Man at GDC this year.
Some of the related talks include Marvel's Spider-Man, a technical post-mortem by Elon Ruskin.
That's this evening at 5.30 p.m.
And tomorrow, there's a lot of talks about Spider-Man.
Doug Sheehan is gonna be presenting the traversal through the city.
Ron Peekt is gonna be talking about editing with immutable data.
Alex Previty is going to be talking about designing sound for Spider-Man.
And Jason Hickey is going to be talking about leading a large environmental team.
As a technical artist on Marvel's Spider-Man, part of my job was to identify points in the production pipeline that can be proceduralized with the goal of...
removing rote tasks from the artist's plate.
Basically, I try to keep the artist sane so they don't rage quit by throwing their keyboards across the room.
So what is procedural in this context?
Procedural refers to algorithmic methods of creating data as opposed to manual methods.
At Insomniac Games, we've developed a number of procedural workflows for generating open world content for our games.
If you attended my colleague, David Santiago's presentation earlier today, you'll have a good idea about many of these procedural systems.
This talk will cover some of the procedural workflows which apply to lighting the open world of Marvel's Spider-Man.
Lighting artists illuminate the open world by hand placing lights throughout the environment.
But how can procedural techniques augment the hard work of the lighting artists?
By identifying the rote technical tasks and automating them, we can remove the boring, repetitive stuff from the artist's workflow and allow them to focus on creating great art.
Procedural techniques assisted and augmented the lighting artists with bulk tasks across the open world.
Procedural techniques contributed to three main aspects of lighting in Spider-Man.
Light fixtures could be placed in the open world based on rules defining things like the minimum distance between lights or how far back on the sidewalk to place street lamps.
Light probes used for generating real-time reflections were placed in optimized positions using the context of the open world geometry in the immediate vicinity of each probe.
and light grid capture volumes used to generate cached light grid data for diffuse global illumination were created and optimized using procedural techniques.
We'll review each of these, but first, let's take a look at some of the open world environments from the game.
Don't you think you're a little hard on Spider-Man?
Let me tell you something, Patterson.
Spider-Man treats New York City as his own personal playground.
Just yesterday, I saw him doing back flips and 360s.
Disgraceful.
Running and crawling on buildings.
In fact, I have it on good authority that he's been doing swan dives off of skyscrapers.
It seems the more confident he gets, the more creative he gets.
Walk like a normal person.
Come on, what's wrong with a little sightseeing?
New York is super diverse and constantly changing.
Maybe he's just looking for vans.
So we can see that we've got a wide and varied open world environment that's just really dense with visual information and content.
The open world of Spider-Man is divided into 726 tile regions.
Each tile region is defined by several zone files, which are JSON that contain references to the assets within a given tile region.
Procedural systems allow iterating over the content in each tile region to add massive amounts of information to the game.
This image shows how the island of Manhattan is divided up into tiles.
We iterate over each tile as a work unit to add or adjust the content using procedural tools.
Our Manhattan is about six kilometers by three kilometers divided up into these 726 tiles measuring 128 meters per side.
Spider-Man is a streaming game.
As Spider-Man moves through the open world, the current tile and its neighbors stream into memory and the previous tiles stream out.
This poses some issues for distant reflections, but more on that in a moment.
The first step in most of the Insomniac game's procedural workflows is to construct a point cloud from the zone data.
We use Python to parse JSON from the zone files and recurse through all of the references.
Let's look at a tile called M35 in Greenwich Village.
This JSON snippet represents the region for a single file.
Each one of those zones contains data that streams in along with the tile.
The building geometry used for the context in procedural lighting tools is located in the ground zone.
I load the ground zone data into a point cloud.
Each point represents the pivot of an individual model instance within a prefabricated building asset.
The point cloud contains all of the attributes we ever need to reconstruct the ground zone and the game data in third-party digital content creation applications such as Houdini.
We flatten all of the transformation hierarchies into world space.
We encant the attributes n, up, and tangent u.
There's a lot of aligning and instancing, and packed geometry is very memory efficient because the assets are built from repeated model instances.
We've got id and asset path point attributes, which let us define a clear way to identify and address each unique asset, such as this sanctum.
Sometimes bounding boxes are handy.
A moment ago, I mentioned prefabricated assets.
Each prefabricated asset is a collection of models and other information, such as lights and shaders, grouped together in a JSON file on disk, creating a single addressable asset entity.
These assets may be browsed by using the Insomniac Vault.
Here's an example of a street light prefabricated asset.
Rather than tasking artists with placing every street lamp along the streets that make up the city, we were able to offload this repetitive rules-based task and procedurally place the street lamps into the open world of Marvel's Spider-Man.
Each prefabricated asset can be instanced multiple times.
For Marvel's Spider-Man, streetlights were placed on a metric along sidewalks at a fixed distance back from the curb and aligned according to the normals of the sidewalk upon which they are placed.
We can visualize the location of all the light sources in a tile to help find underlit areas of the open world.
Marvel's Spider-Man supports night as a time of day, so it's important to make sure there are no unintentionally underlit areas.
Doing some nearest neighbor and ray casting computations, we can detect underserved areas which are not receiving enough light.
We can then attempt to procedurally place additional light fixtures into those areas and log them for further inspection by the lighting team.
Light probes are used for reflection maps.
Light probes occur in the open world at a low spatial frequency, but capture the reflected objects at high fidelity in real time.
They are procedurally placed in optimized positions based on heuristics verbally described by the lighting team for optimal placement.
Marvel's Spider-Man had four procedurally placed light probes per tile.
Additional light probes are placed by the lighting artists where needed.
The first step is dividing the tile into four cells, one for each light probe.
We create a point at the center of each cell, and based on the rules of thumb described by the lighting artists, I analyze the ground zone geometry to identify optimal positions and move the location of the light probes.
We find where the roads and intersections exist in each cell.
And we move the light probe points to the nearest major road intersection in its cell.
If there's no major intersection, we use a minor intersection.
If there's no minor intersection, we use the centroid of the road segment that intersects the cell.
If there's no road, we leave it alone at the center of the cell.
We can then check if any probes are inside of building volumes.
We don't want them inside of buildings, we just want them outside of buildings in the open world.
If so, we use the sign distance function of the volume to push the light probe outside of the building.
Finally, we export the JSON containing light probe information for ingestion into the game engine.
Wash, rinse, and repeat 726 times.
As a procedural system, should the needing capability arise to support more light probes, we can increase the number of subdivisions per tile, thus increasing the number of light probes.
This JSON snippet represents a single light probe.
The probe is defined by a volume at a certain position within the tile, then the exact position of the light probe is defined as an offset from the centroid of that volume.
So the volume represents the extents of the cell, and the offset represents the position of the light probe within the cell.
Here's another example of a light probe in the open world of Marvel's Spider-Man.
The image in the upper left corner shows each side of the generated environment map for this light probe.
You'll notice that the quality of the assets captured in the light probe are not the full resolution assets, but rather the optimized 3D meshes used in the Insomniac Games imposter system.
I'm going to explain more about the imposter system in a few moments.
But this is what allowed us to generate distant reflections for the whole city in real time, even though the high-res assets are only streamed in for the local tile that Spider-Man is currently occupying, plus its neighbors.
Storing all the light probe reflection maps proved bulky on disks, so Tony Archulio on the core engineering team developed technology for generating the light probe captures in real time.
Since only the current streaming tile and its neighbors are available at full resolution, we leveraged the imposter system to create the content for the distant reflections.
The IG imposters informed a number of game systems, including the real-time light probes.
The IG imposters are 3D mesh models with projected textures.
As for the light probes, each face of the light cubemap is rendered at a 512 by 512 resolution.
We take advantage of mip mapping to convolve the cubemap down five levels, and each mip level ends up representing an amount of glossiness.
Regardless of how an artist sets the glossiness on the slider for the shader of the surface material, we always quantize that glossiness response down to one of the five mip levels.
All of the buildings in the island of Manhattan in this screen capture are the IG imposters.
We can load the entire island into memory.
The IG imposter geometry and lowest MIP texture remain in global persistent memory and represent the lowest LOD in the game.
This allows for reflecting distant buildings when their tile region is not streamed into memory.
The total geometry footprint for all of Manhattan was about 101 megabytes.
And the lowest MIP for texture maps is about 90 megabytes.
High quality polygons for reflective and emissive properties take up about another 30 megabytes.
So we're looking at about 220 megs of RAM in global persistent memory that hold the entire representation of the lowest LOD of the city that's accessible by any game system at any time.
The IG imposter geometry and lowest MIP texture stay in that global resident memory.
They always maintain fidelity under all circumstances.
It eliminates a far clipping plane for draw in.
It maintains geometric relief details of the source assets, supports fully emissive and reflective surfaces, and transitions seamlessly between the high-res instance geometry and the corresponding IG imposter.
Additionally, as I said, it informs many of the other game systems.
So how do we take a high-res model and turn it into one of these impostors?
This is a building from Tile H21.
This exploded view shows all of the model instances that make up the building.
Buildings are constructed from model kits containing predefined assets, such as windows, walls, and doors.
Since the prefabricated building assets are used over and over again within a building, there are actually a small number of unique models per building.
Again, here's the full resolution.
This is the lowest LOD.
To build the imposter, we identify groups of connected geometry which share primitive normals.
We iterate over each set of connected geometry which shares a common primitive normal.
and we remove shared edges from the polygons, leaving an outline of the shape.
We triangulate the result and remove occluded geometry that's on the interior of the building.
After occlusion removal.
we do a final optimization pass.
And we've brought a high-res building that had tens of thousands of polys down to about 5,000 polygons.
This is a single mesh that represents the entire prefabricated asset.
We capture the imposter geometry and generate orthogonal texture atlases.
The building in the middle is the high res asset.
The building on the left is the imposter with textures applied.
The building on the right is the imposter without textures.
We generate two texture atlases per building.
Atlas A contains the RGB albedo.
Atlas B contains spec, gloss, and emission.
We've replaced all of the model instances and texturing from a building prefab with a single model instance and a pair of texture maps.
Each building atlas is collected with the atlases from neighboring tiles in an imposter zone atlas.
The RGB albedo is stored at full size while spec gloss and emission are stored at half size.
Those half size maps are represented as green in this image.
you can see Avengers Tower on the bottom.
This is one of the nine tile imposter zones.
This is another nine tile imposter zones.
We also use the imposters to generate the light grids for caching global illumination data.
Light grids are high spatial frequency, low detail samples for diffuse light in the open world.
Using the same geometry measures as the IG Impostors, I quickly load the contents of a tile and all of its neighbors in order to determine what content would affect the diffuse samples used for diffuse global illumination calculations.
The 128 meter square tile is divided into 16 meter capture volumes.
The cached data for each volume represents the 16 meter volume divided into 4,096 samples at a one meter resolution.
The data is similar to that captured by the real-time light probes, but the resolution is much lower and compressed for the diffuse lighting computations.
The capture volumes for sampling and caching diffuse illumination were procedurally placed and optimized into each tile based on the tile contents and the contents of neighboring tiles.
The number of vertical light grids generated depends on the height of the tallest building in the tile.
Testing the individual capture volumes against the assets in the tile's zone file allows us to remove capture volumes with no contribution.
Light grid capture volumes are also removed from the inside volumes of buildings.
The final light grid capture volumes are written out as JSON and ingested into the Insomniac engine where the samples are rendered into a cached file format for runtime usage.
Light probes are real time, while light grids are pre-cached and never generated at runtime.
Light grid capture volumes removed from the insides of buildings, as you can see in this slide.
The final light grid capture volumes are written out as JSON.
This JSON snippet represents a single light grid.
And here we can see the game engine iterating through each of the light grids as it generates that point cache data.
There are 4,096 samples per capture volume.
Each sample generates a cube map at 256 by 256 pixels per face.
Each face of the cube map is then convolved down to a single HDR color value representing the diffuse term and a directional vector.
So each sample inside of that capture volume is six pixels, each with a directional vector.
The color value and direction data is highly compressible.
Thank you very much.
This concludes my talk.
And if there are any questions, I'll be happy to take them.
Hey, I saw you used JSON a lot to spit out the procedurally generated data and read it back into the game engine. Did you ever, like, load Houdini into the game engine or it was all built with JSON?
So the question is, did we load Houdini into the game engine, or did we use JSON as an intermediary format to go between the two?
The answer to that question is it's a bit of a hybrid.
JSON was certainly the way that we communicated back and forth in many cases.
We did not implement the Houdini engine in our Insomniac engine.
But what we did do is expose several tools from pull-down menus in the engine, which would spawn a non-GUI shell of Houdini that we could send requests to process data offline, which could then generate JSON on disk that could be read back into the Insomniac engine.
Thank you.
Hi, thank you for the talk.
Quick question on the imposter system.
So I noticed that I'm guessing you were marking specific prefabs for generating an imposter, and specific prefabs were not generating an imposter intentionally.
I noticed that on some of the buildings, there were benches and other things that were just completely excluded from the imposter generation.
So how did you decide what was going and what wasn't going into the imposter?
So there were a number of ways that we could include or exclude things from imposter generation.
In additional, we had another subsystem called the hibernation subsystem.
So for example, things that would contribute to the silhouette of a building that were accoutrements on top of the building, like water towers or air conditioning units.
Those were handled by a separate system.
called the Hibernate system, that basically had the full resolution asset, but with a much longer draw distance.
So we would exclude those kind of things from the imposters because that geometry tended to be a little bit more organic, like things like water towers with lots of details and struts and things like that, just wouldn't really imposter very well and would have created a lot of extra geometry for the imposter system to deal with.
So selectively, we would use meta tags to say this thing's hibernated, this thing should be skipped.
or this thing should be, well the default was to include things.
Okay.
Thank you.
Hi.
So you guys were actually in engine at game time rendering your reflection maps.
Is that to save space?
To save disk space essentially, right?
Yes, to save disk space.
So did you only have to do that when the user would kind of cross over into that region?
How often were you like refreshing those or?
they were refreshed whenever there was time to do so.
So it was a, I think it was classified as sort of a medium priority task along all of the other rendering tasks that are happening per frame.
So it's not happening 30 times per second.
It's not happening 30 times per second.
I think it's probably happening about twice a second.
the reflection maps would get updated about twice a second on average.
That seemed to be fine.
All right, thank you.
Hi, I might have misheard you earlier, but I heard you mention lightmaps, and were you speaking specifically about the GI lightmaps you were using or geometry lightmaps?
You mean light grids?
I thought you said lightmaps specifically, but...
I guess the question is, did you use lightmaps for geometry?
Or was it just the grids that were generated?
Light grids were used for generating the samples used for the global illumination.
Right.
And light probes were used for the specular and reflection.
In addition, there are real-time sources of illumination all over the game.
Right.
For direct illumination.
Gotcha, thanks.
With the light grids, how many points along the time of day did you capture those?
So there are four times, the question is for the light grids, how many times a day did we sample?
And there are four times a day in the game.
We have sort of daytime noon lighting, we have sunset lighting, we have a foggy rainy day lighting, and we have nighttime lighting.
So each of the light grids was generated at each of those times a day.
Thank you.
Hey.
You talked about reflection probes and indirect lighting data using the probes, but what about the shadows?
And I'm assuming everything is dynamic, but these buildings are so huge and they cast like dynamic shadows.
So what's the resolution?
How is the optimization for that implemented?
The question is, how did we optimize for shadows and handle the shadows?
So, that's an interesting topic I didn't really touch on in my talk, maybe I could have.
But using the light probes and the imposters, we were able to generate occlusion maps.
for the different times of day.
In addition to the light probe, each light probe has a draw list of what buildings are visible to that light probe.
So if a building is not visible to the light probe, it can be placed into the list of the occlusion map.
I'm not completely familiar with all of the technology that was used for that technique.
But again, that was Tony Archulio in our core rendering team who devised that system.
And the imposters sort of helped, the imposters and the light probes helped inform that system to make optimized choices.
Just another follow up, did you use like a mix shadowing where you used to bake the shadows too far off and then have dynamic in which were closer?
There aren't really any baked shadows that I'm aware of.
Obviously, the global illumination is baked, the samples are baked, so if something is in shadow in those samples, that is cached data.
But that's not being, that's not considering any of the direct illumination from the game.
So I think we've got time for one more question.
Yes.
So the 128 by 128 meter tiles, how many of those, on average, were you able to fit into memory?
Because the whole city was imposters, but then you had, I'm sure you had more than one of them loaded at a time.
So how many, for average, did you have?
So there would be.
I don't know about an average, but the goal was to be able to have a tile and its neighbors streamed into memory, so nine tiles.
The tile that Spider-Man would currently be in would be streamed in, or the model instance would be drawn at full resolution, and then there's, in addition to the imposter system, there's a more standard LOD system to handle neighboring tiles, for example.
I believe there were also some optimizations put in place so that when Spider-Man is moving quickly, we sort of predict where he's gonna go and we don't load the tiles behind him necessarily.
We sort of figure out what his likely frustum of travel is gonna be at any time and just load those tiles in if he's moving at a really fast clip.
And we do that to avoid load time frame dropouts and stuff like that.
Thanks a lot.
Thank you very much.
