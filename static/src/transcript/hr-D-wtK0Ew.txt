So my name is Barney Pratt.
I am Senior Audio Director at Supermassive Games.
Been there for 14 years, a long time.
I've helped build a lot of the audio systems with a lot of help from different people over time that we're going to be looking at today.
And it's that kind of cinematic audio DNA that makes Supermassive special and we're expanding on it all the time.
And this talk will focus on just some of the systems within that kind of cinematic DNA that have enabled us to release annual AAA content for the Dark Pictures.
And as you will see, a lot of these systems have as much a place in the open world as they do in.
cinematic games.
Everything I'm presenting today wouldn't be possible without a great team.
We are 18 sound designers, it's huge at the moment, plus a production manager to help us organize, with an added composer, Jason Graves, who we're lucky to have in the audience today.
So any music questions later, maybe point them in his direction more than mine, but we can share them, it's fine.
So, yeah, it's an ever-evolving team.
Quick word about Supermassive Games, we're a BAFTA-winning studio founded in 2008.
350-plus individuals, so we're pretty big now.
We're based in Guildford, UK.
We've had critical acclaim for both cinematic and VR, multi-platform, multi-format, and we support a lot of UK charities, including Games Aid.
We've been lucky enough to receive a bunch of nominations and awards, including Audio Tech, and a lot of good critical acclaim for Audio Over Time.
Standouts were BAFTA for Best Original IP and a NAVGTR for the second in the Dark Pictures anthology series for Best Audio in a Game Cinematic.
And we'll see why this award was so amazing, because a lot of the sound that we were being judged on there was systemic sound, was procedural.
So it's a huge compliment to those systems.
And also, we're up for three Gang Awards in about an hour.
So, fingers crossed.
Sorry about that. I just figured 530 on a Thursday, everyone's going to need a little pep call. So yeah, there you go. And the vampire will be making an entry later on. You'll be finding out more about him.
So Dark Pictures Anthology, what is it?
It's interactive branching horror, wildly different narratives and locations, et cetera, between each game.
There's a lot of exploration gameplay as well as the cinematic.
There's single, two, multiplayer, online, offline, couch co-op, and eight plus, sorry, eight games in total for the anthology.
Season one is made of the first four.
There's Man of Medan, Little Hope, House of Ashes, and The Devil in Me.
During these four years, we were also doing several other things.
We onboarded an outsource audio team for the Quarry, and they did an amazing job.
That's Adele from Sound Cuts, who might be here, but maybe not.
If you haven't checked out, Quarry's got a fantastic soundtrack as well.
We're working on some unannounced projects.
And last week, we released a launch title for the PSVR2, called Switchback VR.
But we are not here to talk about those.
We're here to talk about the first for the Season 1.
of the dark pictures.
Quick talk overview, we're gonna look at branching narrative design just briefly to kind of illustrate the challenges we have.
We'll look at procedural foley and character sound.
These are like really key systems for achieving the kind of cinematic characters.
We'll look at how they extend or bleed into Creature Foley, and we'll look at how we worldize sound, some of the attenuations, and we'll look at the interactive mix, how we balance out all the sounds playing in the game.
We'll also look at two unique systems.
One is an in-game runtime directional microphone, which was a gift to the sound team in the fourth of the Dark Pictures, and we'll also look at diegetic music, how we start it diegetically and then bleed it out to be full score for added drama.
So what I mean by branching narrative, I think everyone here is probably quite familiar with that concept now.
It's player agency to direct their narrative.
Here's a kind of illustration of some of the logic flows that we have.
Quick overview, these green sections are sequenced sections, Unreal Sequencer, which are essentially short linear sequences, all in-game sequences, not FMVs, not videos.
So not the kind of standard cutscene stuff.
And then there are interactive sections highlighted here in red.
These could be anything from a narrative choice, dilemma choice, combat choice, QTE, quick time event, sorry.
It's basically anything that leads to branching.
And then we have a bunch of exploration gameplay to consider as well.
This is not truly representative of how it looks in the logic flow, but it's good enough for this talk.
So take a closer look at what's contained in a single linear node or sequence.
These contain dialogue events, sound effect spots.
We also draw in RTPC curves, it's all the standard Unreal Sequencer stuff.
And the RTPC curves come in with the music later.
We also add an additional layer of linear Foley per character, which is recorded as you would do to film.
There's no adaptation there in terms of the recording technique, but it really is just a minimal addition to some of the systems.
It's anything we don't have systems for.
like a chair move, a wall scrape from a character, rope holding, doors, body falls, and some added cloth for crawls.
And then across all of the sequences, the interactive sections and the exploration is the systemic audio.
It's the same systems throughout all of the gameplay aspects.
These include character breath system, exertion of fear, responding to some of the game AI calls, or yeah, most 2D and 3D.
and all the music and sound design.
And by using systemic audio throughout all the gameplay areas, this leads to a seamless experience.
And in fact, seamless gameplay experience, it maintains the immersion for the players.
It's kind of crucial for the games.
And this systemic audio includes procedural Foley.
So all of our character Foley systems, cloth, footsteps, acutes, it's all movement based and it's fully systemic.
We invest really heavily in these systems, incrementally updating them all the time.
And this gives us consistency throughout the sequences and exploration for character sounds.
We also rely on them to finish the product because we're doing a one year turnaround.
And that gets a little bit tough towards the end if there's any edits.
So yeah, we've had some huge advancements in this area.
So our aim is to deliver cinematic audio experiences through systemic audio.
And this is going back to the award we won against other games that we're using kind of linear cutscene pipelines, video playback.
It's not a take from their excellence because there was some big games that we were up against, but it's obviously a huge compliment to the systems that we're gonna look at today.
Which brings us neatly onto the first system procedural, Foley.
Believable characters are key to our games.
We're making cinematic games.
Foley for us is a broad term.
It's essentially any contact the character makes with the environment.
So we're going to use House of Ashes, the third title in the series, to showcase some of the systems, quick back story.
It's set at the close of the Iraq War, May 2003.
American special forces are searching for weapons of mass destruction, we know how that went.
In fact, they found a buried Sumerian temple with unearthly creatures and much horror hilarity ensues.
And a quick shout out to Glenn and the crew at Molinere in London for all their amazing work on the assets that we're gonna be hearing today.
So we systemize as much as possible.
Why do we do this?
We know then that the majority of the Foley is covered.
It leads to easier filing, as I said.
It's a one-year dev cycle.
It's a time-money stress saver, and it's a win for multiple projects.
This is a long-term investment in these systems.
It's also actually essential for linear and exploration consistency.
We'll look at some gun Foley later, which is really interesting.
The main systems are footsteps.
We have a walk and run based on character speed, playing back small files.
And we have additional subtlety brought in with a walk slow.
So as the characters come to a stop, they are making really nuanced, subtle sounds all the way up to their stopping.
We've extended the system to be sideways movement scuffs, and we use high-velocity vertical movement on characters to play procedural landing sounds.
All of these are surface specific. We have something like 26-30 surfaces per game. And of course we extend this system into creature foots.
A similar setup for cloth. Short samples based on bone distance and bone pass of knee and elbow.
And we have acutes, which are the super detailed kind of costume elements. They could be combat belts, jewellery, anything that's a small addition. We don't pass up on any opportunity for added variety and subtlety.
And then there were the more bespoke elements.
Gun Foley was something we did new for House of Ashes, with three sets of samples, all velocity triggered.
They're slow, medium, and fast.
Again, we're trying to get as much subtlety as possible.
And a hand pat system, which I have to admit is still a bit of a work in progress, but we've had some success with on, off, and slide.
And of course, the creature wings Foley is an adaptation of these systems.
This is what the sound emitters look like attached to the character mesh.
We have eight emitters, there's head, dialogue, chest, left and right hands, knee, and both feet.
So the emitters are tracked at runtime, they drive RTPCs and switches in Wwise, and the idea is we're simulating natural performance.
So quick side note, we switched to spatial audio for House of Ashes.
So all the House of Ashes clips you'll see.
as you'll hear, will be with full spatial audio.
And it was an absolutely amazing system and a great project to showcase it on with huge cavernous spaces, small tunnels, portaling, et cetera, was just fantastic.
But I'm not gonna talk too much about that because it's Wwise out-of-the-box system and there's a lot of information online.
But we did learn something from this.
We have to respect the CPU because With 8 emitters per character, often multiple characters, thousands of API calls, you've been there, right?
Often like 50 enemy soldiers, 11 friendlies, 8 vampires.
The vampires actually had 12 emitters each.
All the switches, RTPC values, diffraction values, it was way too much.
So it wasn't the spatial audio that caused it, but it was definitely part of the equation, and we pushed it too hard.
So, we created a...
Well, this brings me onto the next system, really.
We created a culling system for events and emitters.
And this is a kind of a word of warning.
It's not something everyone thinks of, but it's based on distance to listener.
This is kind of an illustration from in-game.
So outside of their attenuation range, we could firstly cull the events.
in case the emitters were needed later, or then we could be even more hardcore than that.
We could just cull the emitters and stop any updates being called for them.
This was really important during the busy combat scenes with multiple characters, and also actually led to a less dense mix in some areas.
We used it to help us there.
And the biggest win and the biggest kind of learning we had from this was make this functionality platform specific So you can exempt a whole ton of stuff for gen 8 consoles and leave it dense for gen 9 So going back to procedural foley We're gonna look at footsteps first basically anim notifiers are just not a viable option with such a short cycle And all the loco animations, you know changing being updated all the way towards the end of the project So we needed a flexible system that would work with both locomotion and mocap to keep consistency.
And it's motion considerate using various RTPCs.
The core of it is emitter speed.
So we extend this, as I said before, sideways movement scuffs, vertical movement for landings.
And we have an additional layer.
We include overlay detection for sweeteners such as debris or plaster on the floor, maybe a puddle.
And let's take a listen to just the footsteps soloed from in-game.
Okay, moving on to procedural Foley handling.
As I mentioned before, we had a bespoke system for House of Ashes, which was gun Foley.
And we had prop handling for interactions and we had the hand pats.
It's easier if I just play these.
Let's listen to how they play back in game.
Very cool.
Moving on to some of the other systems, or furthering this, yeah, into the cloth systems for leg and arm movement.
Acutes, again, it's the more subtle close-up stuff.
And we use the same tech for the cloth and acutes system for the creature wings as well.
So let's see how they sound like in-game, soloed.
You see it really fills the soundscape in a very natural way.
Okay, Creature Foley.
Why create procedural foley for a creature?
Well, it gets a lot of screen time.
There's often multiple on the screen, so track lay is not an option.
Thousand plus unique sequences.
So we needed something again that gave us an infinite range of movement.
It was mainly keyframe.
There's a little bit of mocap thrown in there as well.
Here's some amazing concept art that we looked at in the early days.
And as I said, the vampire will be making an appearance later.
This is its moment.
Here is a walk cycle example.
You can see it's quadruped, not biped, but it can also walk standing up.
Hind legs like a horse with an extra joint.
It has wings.
And of course, many of these features broke the conventions of the human Foley system.
So we had to...
of Creature Solo.
Pretty cool.
So let's see how that all combines into the full mix.
And before anybody asks, no, we didn't do the vampire vocals and clicks procedurally.
We went old school on this.
It was a big job.
But it was the only way we thought we were going to really get that believable creature.
So we took the hit.
Here it is, full mix, music, everything.
Aaaaaaaah!
Aaaaaaaah!
Aaaaaaaah!
I shoulda went and get the fuck outta here.
I agree.
Let's move on to the interactive mix.
We have a full runtime mix.
It's the same assets for the characters, especially for exploration and sequenced sections of the game.
It's a sidechain mix, not HDR, although we've been looking at HDR.
We do a lot of work with attenuations to help mix focus.
We have a detachable listener to help the cinematic needs of the game.
And we do something called 50% center bias panning.
Why do we do all this?
Well, we've got a lot of systems running in the game and we want a clear, focused, immersive, cinematic mix of all of those systems.
So when we came to dialogue panning, we had a challenge.
The cameras were cinematic third person, so lots of camera cuts, often cutting in the middle of a dialogue line.
And our initial approach of full surround panning led to really harsh jumps in the dialogue there, both attenuation and panning, we've switched from side to side.
So we had two options. Option one was the old-school film option of having all dialogue through the center speaker only.
It was safe, but it was lifeless. It just lacked that directionality that game players demand.
Option two of the full kind of 5-1 surround pan had great directionality, but it led to really harsh cuts which broke the player's immersion.
So what do we do? We rewrote the panning behavior for character audio, actually for all dialogue and foliage.
And we found a sweet spot between the two options, allowing panning, but only half of what it should have been.
And this was the 50% center bias system.
So how did it work?
Well, with all the character sound constrained to the front, the LCR, we would take the character position relative to the listener as normal, and play the sound back from a newly calculated position that had only half the side panning, thus biasing the center speaker without constraining exactly to it.
So the example shown here, if a character was off screen right, rather than their dialogue and Foley panning 100% right as well, they would only pan 50% right.
Again, if the character was 50% right panned, just on the edge of the screen, it would be heard from about 25% right panned, center with center, and as you can see, it's an elastic value, just proportionate to the in-game position of the emitter.
So it's always playing the majority from the center speaker when the character emitter is on screen.
And we actually did this for all the character sound, including anything from sequences, body falls, et cetera, and of course, dialogue.
So it not only removed the uncomfortable audio jumps on the camera cuts, but it improved the player engagement compared to the sensor-only version, and led to a smoother, more immersive result.
So there's big ticks all around.
Attenuations.
We worldize character sound fairly consistently throughout all the games.
In this example here with the camera listener distant, you can see the more nuanced foley is not audible, the cloth, the acutes, and the gun foley.
Footsteps are audible over long distance to maintain that character presence and believability, that kind of connection with the world.
And dialogue, of course, is audible everywhere, never quiet because it's so important to the narrative.
Moving the listener closer, you can now, sorry, the more nuanced, subtle Foley is now audible.
And this increases that kind of sense of proximity, that up close and personal, which you'd expect from a film mix.
We can also detach the listener, as I mentioned, and place it anywhere we like in the world.
Now this gives a really cinematic feel, for example, improving on kind of intimate situations like.
listening to a conversation when you're outside a window, maybe a cafe or a car.
And we've had these examples come up.
And it also helps us solve occlusion issues with the new spatial audio system where cameras are placed outside spatial volumes.
And a quick mention of voyeurism with our standard attenuation, which puts the player in a kind of uncomfortable onlooker situation.
Such as hiding behind a bush, or maybe it's killer cam POV from behind a bush, moving around.
In this situation here, all the distant character sound is audible, but the tree rustle is also audible, adding to that kind of uncomfortable placement of the viewer or the player.
And this is a great feature for horror.
Moving on to Azimuth. This is a WI's screen grab, of course.
How do we attenuate the sound by angle of emitter from listener?
It's a kind of mise-en-scene approach.
We want to attenuate sounds when they are off screen.
And it's a kind of subtle focus pull for the on-screen sound.
So we decrease the volume, a little bit of high-pass and low-pass, nothing too clever there, but it's a good result.
So with the character on the edge of the screen, we have full volume, no filters.
to when the character is behind the listener or the camera.
There's reduced volume and maximum filtering.
So it's a slightly muffled, less focus in the mix, less clarity, and we have different settings for dialogue.
This is the Foley setup.
Dialogue is, of course, dominant with less attenuated sound.
So with interactive mix, a quick look at our sidechain bussing.
This is our basic top-down priority.
We use a combination of sidechain volume ducking, sidechain EQ band ducking, especially prevalent with dialogue, taking one K out of other elements.
And we do sidechain compression, of course, that's super smooth.
And the lower priority sounds can rise up in the gaps.
So this enables us to really focus the mix on the action we want the player or the viewer to be seeing.
And it keeps clarity, of course, within the dense cinematic mix that we're aiming for.
And we do some cool stuff, actually, with this.
In Little Hope, the second in Season 1, we added something that placed all the diegetic sound in fog and through various plugins and RTPCs we had analog control on how those sounds reacted to the fog.
And additionally for Little Hope, we had control over the pitch of all the atmospheres, 2D and 3D.
The reason for this, it was matching the kind of psychological state of the protagonist in that game.
And it was just a really effective way of preceding scares.
We could literally bend reality.
So we have a lot of fun with that.
But the example I want to show today is a scene called Boat Fight from The Devil in Me, which was the fourth dark picture.
Came out last November.
It's a high action scene.
And the challenge in this scene is to keep the intensity of the scene.
And there's a lot going on.
We want to focus on the right action.
So this is our basic mix broken out a bit to leave some space for other elements.
So unique to the scene is the boat engine revs.
There is a boat in the scene.
So unique to the scene is the boat engine revs and there's also a boat loop which help fill the gaps between those key SFX.
We had a lot of water.
Lots of incidental water splashes, characters falling in the water, etc.
And we had constant water around the hull of the boat as it was going along.
There was a lot of combat sound effects, there was a lot of punching and stabbing and kicking and all that stuff going on on the boat as well.
So we wanted these elements to be very strong, hold a lot of weight in the mix.
And we of course had a dynamic breath system for all the characters.
So we had a lot of submixes going on.
We submixed dialogue against the breath system, basically full ducking on a per character basis when dialogue played.
There was another submix with the combat sound effects ducking the main procedural Foley systems.
The water spot effects ducked the more constant water elements due to, just to allow them to stand out.
And the water spots also ducked the boat engine due to a frequency range clash in the high end.
Engine revs, of course, ducked the engine loops to add intensity.
Music and sound design had a low pass ducking on the water in the engine loops to clean up the mix when the water was pumping, was high enough in level.
Dialogue had its normal varied ducking on pretty much everything to make it audible regardless of what was going on.
And the explosions, similar to dialogue, ducked everything by a relatively high amount to make them feel bigger in the mix.
So I hope you're all still with this.
And atmospheres, they basically got ducked by everything.
Poor atmospheres.
But when they rose up in the mix, it reminded you you were out on a lake, it was windy and cold.
So, looking at the example from The Devil in Me, take note that in this scene, all of the characters can be dead, all of them can be alive.
There's massive branching.
Depending on the actions of the player throughout the game, there's different collectibles in the game which can be used as weapons.
So there's a whole variety there.
There's a heck of a lot of camera cuts and perspective changes.
So all the attenuations and azimuth and 50% center bias, that's all at work.
And there are slow-mos, which basically kind of trounce the soundscape.
Which kind of affects the pace of the scene.
So let's see how that all came together in game.
I think I wouldn't be able to do it.
What a way to go.
So one of the unique features that we created for The Devil in Me as well was the directional microphone.
And this was an absolute gift to the sound team.
Someone comes to you and says, oh, there's this listening device.
In fact, it was Tom just sitting here, came to us and said, there's this listening device we designed for this game.
It's a player-controlled...
Sorry, one of the characters in the game, Aaron, I should fill you in actually, is a sound recordist.
So there's a reason for the microphone being there.
It's actually a key gameplay mechanic, so it had to work well.
It's a player-controlled shotgun mic in third person.
No brainer, right?
So we had to create an in-game simulation.
We could decide whether it was cardioid, hypercardioid, how directional, how that affected the sense of horror and claustrophobia.
We simulated off-access mic attenuation, and we actually went for a hyper-real result.
with the off axis kind of bleeding or sending to the reverbs a little bit more.
Additionally, Erin's character or the player can take Erin's headphones on and off at will.
So we needed to be able to flip the mix.
So it was all RTPC values, all at runtime and all sort of available on an analog slider.
And on top of this, similar to the gun foley, we added mic handling.
So it's kind of all the noises you don't want, we were able to simulate.
We don't want normally, sorry, when you're recording.
So this feature reviewed really well.
A great quote from NME, which I love, and people are bored of me saying it, is like a horror ASMR, which I just think is awesome.
So how does it look in Wwise?
Well, it's actually pretty simple for audio.
We had lots of control via the azimuth system, but you'll notice the curves are a little bit more extreme here.
We've added aux send variants, as I mentioned.
There was, for the off-axis, there's a lot of increased aux send.
And worth noting, we decided in the end to have this azimuth based on camera direction rather than third-person character direction.
because the character can turn around and walk towards you.
And it just seemed to fit the kind of the player's perspective better.
So let's see how that looked and sounded in-game.
I won't be scared long.
Sing it.
And I won't be scared long.
Is that good?
I knew it was coming.
I don't know anymore, man.
I don't know.
It's like, what?
I don't know.
I'm glad you enjoyed it.
Thank you for sharing that.
So let's take a little musical interlude from all the systems.
So Jason Graves composed all the music for Season 1, and it's just a massive variety.
Completely different scores for each game, reflecting, of course, and driving the narrative, visualizing the location, the historical era.
fitting the horror sub-genre, and we have a lot of fun and experimentation with the music.
Looking back at the four games from season one, Man of Medan was all written in 3-4, representing the waves. It's kind of like a waltz.
It was based in the South Pacific, full orchestral score, very sort of traditional orchestral horror.
Little Hope was a more minimal soundscape based on research of instruments from the 1692 Salem area of North America.
House of Ashes, there were 4000 year old Sumerian kind of backdrop in Iraq with a strident signature and the whole score progressed from being orchestral to being sort of very hybrid, very synth based.
horror as we realize, spoiler, that the vampires are aliens.
And lastly, Devil in Me was harking back to 1892 Chicago.
We decided that a Bernard Herrmann kind of nod to Hitchcock's Psycho just really fitted the right tone for that score.
And in The Devil in Me as well, we also played a lot of classical and operatic pieces underscored with aleatorics.
orchestral techniques to add to some dramatic moments which we'll come to later.
So here's a two-minute medley showing some of the variety from Season 1, which is just not doing it any justice at all, but hopefully you'll get a sense of the variety. All of these tracks are available on Spotify, so you can follow up on anything you like there.
But here's a two-minute medley from Dark Pictures Season 1.
Hope you enjoyed that.
So back to the systems.
Music, diegetic music rising up into full score.
So we have a system which at runtime using RTPCs can bring diegetic music starts in the world into full score for added drama.
In The Devil in Me, these were classical operatic pieces.
They would start as a mono file being played from a gramophone.
and rise up to stereo score.
We had three RTPCs we could control.
One controlled a mix between the mono and the stereo files that you can see here.
Another RTPC controlled a kind of reverb send to, you know, effectively a kind of really bathed reverb so we could really add some tails to the moments.
And we also had another RTPC which controlled a distortion if we wanted to really nasty up some of this music at crucial moments.
So technically, it was very simple in Wwise, but the difficulty was authoring.
We were working with stereo files, so it made transitions difficult for the interactive sections.
And we were working with stereo classical pieces.
But we think we kind of got to an amazing result.
We had to make a lot of compromises along the way.
And there was the odd dodgy edit that made it to release, but it was very difficult to kind of control those.
So here's another example from The Devil and Me, fourth in the series from the dark pictures, where music by design signifies an impending murder or potential murder, depending on the branches that you choose.
In this example, the music starts diegetically outside the door.
We maintain some directionality throughout the scene, swelling it a little bit into the full score at different points for added kind of confusion and drama.
And then at the end, as you'll see, it rises to full score for a very specific reason.
Um Hey, hey hello I'm here!
I'm- Stop! Please! Stop! Please!
First you need a fireLID, then you need an ATMUG Then you need an items that gives you something to collect Finally you need ATMUG, then you need items that give you a new armor Finally you need more items that gives you a new shield Run through all the scrolls before the final 3 doors Thankfully this isn't the entire game, this was the first to me See you around Another way to go.
So what for the future of the Dark Pictures Anthology?
Well, we're already into DP5, which is the season two opener.
This one's called Directive 8020.
And now we get to take horror to space.
This is Commander Stafford of the Ford Reconnaissance Vessel Cassiopeia.
After successfully rendezvousing with the curb, this marker we have detached the booster ring and spurts a high as we make our final approach to Tal City F.
The ship has suffered a hull impact but damage is minimal and our technicians are restoring full functionality.
This is Thomas Carter. Something's wrong with Sims.
She's trying to kill me! I think...
Our next transmission will be broadcast from orbit around humanity's future home.
Stafford out.
Thanks very much for listening.
Does anybody have any questions?
You're meant to be answering them, Jason.
We actually never discussed this and I'm curious, so I'm curious with all the procedural stuff, all the choices in these games, just for the last one, the devil in me, do you have any idea if you took all of the different options that are available to the players and stack them in one long linear game, how long that would be versus how long the average playthrough is when you're making all those individual decisions?
So, yeah, so if you, so all of the.
So it's gameplay versus length of material, because of the choices that you don't take when you're playing.
Off the top of my head, I'm aiming like 20%, 30% extra material.
More?
What's the number?
It's 500%?
No, I don't know.
We don't really know.
Closer to double.
They say somewhere between 40% and 50%.
It's a lot, and actually makes QA really hard for testing.
And new branches can appear without you knowing as well, especially towards the end.
Yes, I know there's a lot of extra work we put in.
And it keeps the replayability of the games solid as well.
Yeah, absolutely.
Actually, I have a question for Jason.
Did you write that opera piece?
Oh, OK.
Because I actually played this game.
And I'm really glad I got to see him die, because I chose to go under the grate.
And that's the way that he survived.
But it was a pretty great piece.
Barney and the team kicked out the diegetic music ahead of time.
Stab at Matter.
Is it Pug a Lazy?
I'm really bad on opera.
Can anyone qualify that?
I'll go with that, then.
Yeah.
Actually, the story is that Ollie, who edited that piece, he was working from home with his partner, and apparently she cried when he showed it to her.
And, you know, so yeah, we succeeded in emotionally engaging Ollie's partner.
I have a question regarding the dev cycle that you were mentioning, and obviously the importance of working with the systemic audio to actually be able to catch up with that.
All the other teams, all the other departments, the cinematic teams, the animations and so on, were they also working on that space of one year, or was it just for the audio team, so maybe stacking it up?
Yeah, good question.
Each project has had its delays or advances every time we've rolled over.
House of Ashes, for example, there were like five or six levels ready to go.
Other projects haven't been so lucky, they've been maybe held up by the one before, which is very logical.
On the whole, the kind of cinematics team, there's a heck of a lot recorded early, and it starts hitting the game in kind of a rough form, and that can be quite varied.
It's really a kind of key year where we're all piling in, kind of give or take a couple of months basically.
Okay, good.
Thank you.
Cool, man.
Hi there. Well, that was kind of my question, but I'm going to have a second one.
Again, great job. That was fantastic. I'm actually interested, I work in production, so for me I'm actually very interested to see the pre-production moments, when you know the next theme of a game, of the same installment, how do you approach it?
What do you have in the moment to get your team started?
And where you get your references and whatever you...
So I think, yeah, so I think the question is, what do you do before the game's ready to kind of...
develop the ideas. The answer is the best thing you can.
Sometimes you've got nothing to go on, sometimes you have a film reference.
We resound sections of films to prove to ourselves how we want to make it sound.
Or you can go to simple mood board stuff, a YouTube playlist for music for example. Sometimes you have a ten minute section of a vertical slice which is okay.
And so we can take that out as a video and then just pre-vis the heck out of it.
All the departments can kind of pile in.
So I think the short answer is whatever you can do.
I mean, I've done it with like hand-drawn images from a storyboard that I've kind of thrown into iMovie, this was a while ago, and stretched them out to kind of rough timings and then sunk a sting to a cut between a distant and a closeup.
You know, storyboard.
So it's whatever you can get your hands on.
And just prove it to yourself, basically, that this is what you want.
All right.
And tough question.
It's like choosing which are you prefer the most, but which of the four games here did you have the most fun working on?
I can't say which one exactly.
I can say which one was the toughest.
It was the first of the four, because we were formulating everything.
I mean, it was, yeah, it was kind of like, you know, I think we took a lot for granted in that game.
Because we obviously had Until Dawn experience, we were like, yeah, we know what we're doing, you know.
And, no, actually, we wanted to change things, we wanted to improve things.
So there was a lot of design iteration.
I should say, actually, Tom Heaton here was the game director on both the first and the fourth.
So if you've got some production questions, you might want to tag him.
Thank you.
Yeah, no problem.
I have a small technical question, a bit more about implementation I guess, regarding the procedural Foley system.
How did you detect different kind of triggers and what information was being sent there?
I guess you had sliding and contacts and detecting materials?
Yeah, so we've changed the system recently so I'm a little bit vague on some of it.
We had before.
a distance to bone between here and also speed of movement.
So one of the kind of key features of this whole system, there's a bit of special sauce in there, but one of the key features is emitter speed.
So if it's triggering long samples or short samples, it doesn't matter.
As soon as that emitter stops moving, it's in sync.
So where the sounds end are just as crucial as where they start.
Yeah, we have like...
Surface detection, emitter speed, chest emitter speed is something which indicates walk and run and also walk slow.
Yeah, I mean it's just tweaking the settings on those.
I think a lot of games companies do very similar things to that.
We were quite advanced a few years ago, but a few different studios have caught up.
But yeah, it's just tweaking the values and poring over them.
And we use those same values for all the characters with a body variant switch to play different samples that are material consistent or shoe type consistent or character consistent.
Thanks.
Okay, there's a bit left, cool.
Okay, cool.
Well, thanks very much.
I've enjoyed it.
Thank you.
