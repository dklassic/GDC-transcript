Welcome to the knee of the curve.
I'm David Hunt Bosch.
I'm the rigging tech art lead at Bungie.
And I'm Forrest O'Donoghue.
I'm also a rigging tech artist and leader of the MyTechTools development group at Bungie.
We're here to talk about the rigging and tools work we did to help create Destiny.
So Forrest and I are both riggers, and we're also scripters.
Because in order to do rigging well, it really is necessary to also do scripting.
Rigging is complicated stuff, and there's thousands of files that you need to maintain, animation files, 3D art files, and it wouldn't be possible to do by hand to be able to keep up with this amount of work.
So we need to harness the power of computers and speak their own language and operate them with scripting.
And we've taken this beyond mere automation and taken it to a level of fully-fledged tool development because it naturally arises out of this scripted rigging that you can make tools and help people like animators be more efficient.
And we also write the tools that are the gateway for their exporting content into the game.
And at one time, long ago.
All this was doable, the leadership of this was doable by one person, me.
But the time is long gone.
We're a really big company and we have a lot to do.
So it is awesome to have Forrest here leading up MayaTek.
And Forrest, you're doing a much better job than I possibly could.
Thank you very much.
So, and at Bungie, all of this is within the umbrella of tech art.
As tech artists, we're a support crew.
We're here to help.
We enable other departments to do their work.
And...
I love approaching rigging from this standpoint because rigging is intrinsically the place where 3D art and animation come together.
It's this collaboration.
I feel that from this standpoint of a support crew, we are really able to serve that collaboration well.
So let's take a look at the table of contents.
First, I want to talk a little bit about Destiny and why we chose this approach to rigging.
And we're going to give a tour of our rigging system.
And Forrest is going to go into detail about how we've set up our code base in Python.
And then we'll talk about how we used it to create Destiny.
Starting off in pre-production, building new characters, and then later in production when they're already animated and how we can stay agile and update those rigs as we go.
And then Forrest is going to give us a tour of these tools that we've built for the animators mainly, and also tools for automatically building rigs.
So before we start, we want to add a brief disclaimer.
The rigging strategies we are presenting here are based on the needs of our project and the needs of the design, 3D art, and animation teams at Bungie.
This, of course, is not the only way or the right way for everyone to do rigging, but it has worked well for us, so we want to share it with all of you.
We chose to prioritize two main things in our rigging system.
The first one being demand.
Focusing on the need to create lots and lots of rigs is critical because our project is so large.
The second one being agility.
We are a highly play-test-driven studio.
As we make things and playtest them, we discover how to make them better.
As riggers, we need to help enable these changes at all stages of production, while continuing to maintain our high-quality bar.
So let's talk a little bit about the large scope of rigging work we had to do for Destiny.
And here they are, the combatant characters of Destiny.
So here's some screenshots of the enemy races.
And there are all the base units plus all of the variants.
And you can see how many rigs we had to build.
This is just the combatants.
There's also all the players, the cinematic characters, the vehicles, weapons, props.
And the list goes on and on.
And so there's just a really high magnitude of rigs that we had to build.
It was a lot of fun, but it was also a lot of work.
So a lot of these variants that you're seeing here, these ultra characters are, they have special bones of their own.
They need extra animation controls.
So there's often customized rigging going on in there too.
Another challenge we had was building a whole new engine.
So our tools were constantly evolving and changing as we went.
And as tools developers ourselves, we had to stay in touch with that change.
So the key to being able to do all of this was being agile with our rigging.
And I love this picture because it really speaks to me about what it felt like to be on the rigging team at Bungie.
It can be scary at times.
It feels like the boat's about to flip over and we're hanging off the edge.
But this is the kind of challenge that we live for and we love it.
And because we found that when you're making a big, giant, new IP, you can't just make things and lock them down.
You have to constantly keep updating and changing them if you're gonna make the game work and be the best it can be.
So as riggers, it's our job to support that change as best we can, and by staying agile.
So at Bungie, we make games we love to play.
These are some shots of the rigging tech art team keeping it agile on one of the old ships, just like the World Cup.
We're passionate players of our own game, so just for fun, we decided to theme a lot of these slides in this presentation to look like Destiny.
That being said, we just don't play games all day long.
We still had a number of problems to solve.
In order to help the other teams create great content, we need to help alleviate rigging production bottlenecks.
So the solution we came up with to remain agile is what we're called tools-based rigging.
We're not just talking about the scripts and tools to go with your rigs.
We're talking about a way to completely merge the rigs and tools so that they're unified from their creation and onward through their lifetime and animation.
The rig that animators interact with isn't just a static piece of content.
It's actually a user interface to a deeper set of tools and functionality under the hood.
To enable functionality even further, we have built a lot of GUI-based windows with buttons and sliders, just like all the windows you see here.
This way of animating is awesome for supporting agility, which makes it the right choice for our productions at Bungie.
A lot of this tools-based approach is driven by the freeform animation style pioneered by guys like Rick Lico, one of our animation leads.
If you haven't seen it yet, it's worth checking out his GDC talk from last year, where he went into a lot of great detail on how freeform animation works.
All right, now let's talk about rigging.
And there I am, about to solo the vault of glass.
Never mind, I'm gonna wait for my fire team, and rigging's a whole lot more fun on a team.
All right, so here's some design pillars that we often come back to when we're talking about building our rigging system, and how we like to approach it.
So, our rigs are assembled using modular components.
This really helps us share our rigging that we build with as many things as we can.
and these components are all built to support animation retargeting so that you can add and remove these on the fly in an animation scene and save the animator's work.
The resulting rig is relatively straightforward and the key to it is that it can be modified.
So it can be modified by the animators in several ways.
We provide a whole set of tools which they can use and Forrest is going to give us an awesome tour of that later on.
But we also encourage the animators to go ahead and hack the rigs.
using locators and constraints and whatever they want to do.
Because this is a natural part of the whole ecosystem where they will innovate new things and they'll come up with really clever things that we'll then sometimes build into Python tools and offer to the rest of the people.
We definitely aren't the kind of working group that is telling them what to do.
You know, we have a nice kind of balance back and forth.
And it's really important to let them keep innovating and it's a big part of our process.
So talking about rigging, I like to think of it in kind of two main categories, and one of them is the deformation rig.
And a deformation rig can be thought of as techniques for changing the shape of the model for animation.
So the most fundamental part of this is the skeleton, and the way it moves the model is with smooth bind, as skinning.
So our engine offers both classic linear skinning and dual quaternion.
We have a couple of special bones on the skeleton that are driven by the engine to automatically rotate the shoulders and the wrists to look good.
And there's a bunch of other stuff we can do with vertex shaders like blend shapes and some procedural stuff and channel-driven animation by the animators.
Plus a lot of cool stuff like physics simulation.
And if you missed it yesterday, there was a great talk by our Bungie character tech art team.
But don't worry if you missed it.
It's going to be hosted on the Autodesk area website.
So, big thanks to Tom Snocky and Natalie Burke.
Okay.
Now let's talk about the control rig.
So the control rig is what the animators interact with.
Here's a picture of it right here.
When setting out to build a control rig, it can be daunting to try to think of building the whole thing at once, because there's a lot of moving parts.
And it's also not that obvious how that would be shared between.
different kinds of characters if you were to build one.
So, I got this great inspiration from an Autodesk master class back in 2006 by Jason Schleifer called Animator-Friendly Rigging.
And it really inspired me to think of the rig in regions, where you can kind of take parts of the body, like the head, neck, and arms, and legs, and individually approach them and think about, well, what's the best way to rig this, and how would an animator want to interact with it?
How would it behave in game?
And you can develop specialized rigging.
for that part of the body.
And it really made a ton of sense.
And it's also, like, well, obvious how the left side of the body would share with the right side of the body, because you've rigged the arm once, and you can run the scripts again and rig the other side.
So I took this idea back and started working on it, and I discovered that it's not quite as modular as I would like, because if you're rigging an arm, the way the arm joins to the clavicle and that joins to the spine is very, very specific to that part of the body.
So we have all kinds of different props and rigs in our game.
Some, you know, mechanical objects, some, you know, skybox objects.
And I would love to just share that cool thing I made.
So I did notice that they have this in common.
These regions have associated bone chains.
So here they are being highlighted.
And if we look at them in terms of these bone chains, it's a lot easier to think of, like, very truly modular components.
Very, very generic that could be shared across a whole library of different kinds of rigs, whether they're a character or not.
So now it becomes important to figure out how to track these bone chains.
And the first idea is naming conventions.
So I think many of us riggers have found value in creating a consistent naming convention.
That's great. I think it's good practice.
But it becomes limiting because you see here we have the arm1, arm2, arm3.
It's real easy to think about writing a Python script to identify the start and the end of that.
But...
What if you wanted to call them something different?
Because parts of the body are like the shoulder and the wrist and the elbow.
And that's how people tend to think about it.
So, I don't like to be locked into this way of doing it.
If I want that data to track the bone chain, I wonder if there's another way.
I'm not saying don't do naming convention.
I'm saying don't rely on it too much.
So, there's another way.
Maya provides this thing called joint labeling.
And it's attributes on the joints.
It has side and region, and it does just what I said. It says right shoulder and right hand.
So that's great, but it doesn't necessarily track the bone chains in an easily understandable way via script.
So, that's not quite going to work for our chain metadata.
We came to this system called ChainMarkup.
It uses a combination of both, and it lets us call the names of the bones whatever we want.
And what this is, is just some attributes on the bones, where we say, the chain root is called arm, and the chain end is called arm.
We also identify a terminator, which can help us understand the orientation of the final joint.
And then the side, we actually use joint labeling in Maya system, because it's already there, and we like to use that for other purposes, like interfacing with MotionBuilder.
So that's great. Now we can track these bone chains, and our rigging system can easily identify how to apply these generic rig components that we're gonna be building.
You can name it whatever you want now.
And so here are some examples that are much more kind of human-friendly.
I'm not necessarily recommending this, but you can do whatever you want with that name now.
Okay, this character's ready to upgrade to level 30 and get some control rig components on it.
Okay, so on the arm, there are...
several components. There are several bone chains, but on the main bone chain, we apply this component called the FKIK component.
And it's pretty straightforward stuff. I think most rigging systems have one of these.
This is where the animators are able to do an animated switch between the forward K and the inverse kinematics systems. And they're both really useful for different things.
FK is for free-swinging animations, you get nice arcs.
And IK is great for contacting objects.
And they do need the ability to create an animated switch between them, so the rig component comes with both of them built in.
On the legs, it's a very similar structure.
And it's great that our rigging system lets us share this great work that was done on the arm down to the leg.
And we've added just a few things here for the need to contact the ground and have some remote pivots for walking and for controlling the toe.
But ultimately, it's very similar to the one we saw up there.
The animator has the choice to animate like this.
And I actually have seen animators do this.
We're in a walk cycle, do animated switch between each step.
I think it was Matt Kelly, he and I formed the Corpus Colossum, the bridge between creativity and analysis, between rigging and animation.
And all the technical animators were part of this bundle of nerve fibers, I like to think about.
Really, really, really clever folks, and we have a great collaboration.
On the chest, there are two bone chains.
We're going to look at the RFK component right here.
This is a great example of one that was actually developed and devised by Rick Lico, one of our lead technical animators.
So, he demonstrated how useful this could be with his own MEL script.
And we took it into our Python system and built what is now the standard way of controlling the chest.
And it has a top control and a bottom control that are the main ones.
And the middle controls automatically rotate to create a blended result.
So animators can get a really nice pose very quickly and easily.
And if they want to, they can go into detail on the middle one, but they don't necessarily have to.
And our animation engineers then were able to implement this at runtime.
So here, we're showing on the sparrow, the guardian is showing his chest joints.
And the middle joint is automatically derived as part of our runtime rig, thanks to Eric Brown.
And.
So this is a really helpful thing for animation retargeting so that characters with different length spines can share their animation.
And if animators had created detail animation on the middle joint, then it will come through if they specify, but otherwise, it can have a lower memory usage and it can share between different skeletons.
This is really, really useful.
Okay, the shoulders and wrists are controlled by this one called the multi-axis twist fixup.
And this is another one that's part of our runtime rig.
What it does, we have a version in Maya and a version in our runtime engine.
And this will blend the result of the parent and the grandparent to automatically twist the shoulder.
And it's great for keeping large shoulder pads from interpenetrating the body too much.
And it's also great for saving keyframe animation data in game.
We have lower memory costs because we don't have to store those keyframes.
We also have a prop component, and it's very simple on the rig.
It's a single FK control, but this is an interface to a much larger set of tools created by our rigger, Javier.
So this gives them access to a whole library of rigs that can import a whole rig of its own that exports its own animation data.
And we have utilities for aligning the hands and switching what is controlling which one is driving, whether the hands are following the gun or the guns are following the hand.
It really speeds up animators workflow to not have to go and file browse through a directory and import maybe the wrong file and then guess about the alignment of the hands. They just hit a button and it saves a lot of time, not only in their working day, but less bugs to fix at the end.
You know, that might have been our problem to fix later.
It's really good to solve it nicely.
This yellow control in the middle is what we call the cog. It is the center of gravity control.
and it is an offshoot of the pelvis.
It has single FK control.
And seen here in this animation, it gives the animators the freedom to do things like splitting the rotation and translation differently between the hips and the cog.
So right here, it's only translating, even though the character is rotating.
And you also notice that it is kind of drifting away from the body.
That's because the animator has chosen to represent what is more the true center of mass of the character, which drifts as the character changes its pose.
And we have a tool called Center Mass Tool which enables them to approximate that position automatically.
And here's the pedestal component at the base.
And this controls the topmost bone in the game skeleton.
In the control rig, though, it is actually the opposite.
It is not the topmost control of the rig hierarchy.
It's actually a sibling.
And that's very important for being able to move at a very precise speed while you can have variation movement of the hips independently.
And you can also have the feet plant on the ground because if the base of your character was moving you'd have to counter-animate.
It also has an aim vector.
As seen in this animation, it's what we call an aim screen.
These are actually a series of still poses that define where the gun's pointing.
And the whole body can be used in these poses.
And aim vector...
unambiguously tells the engine where the character is aiming.
So we can get a very precise aim with that.
We can also create them for the head, for the eyes, separately and independently, and blend those to our liking.
Okay, that's it for our tour of the control rig and our rig components.
I'd like to take a quick close-up on the chest region.
There's two components on the chest.
The first one is on the pelvis.
We apply a reverse component.
So this is a forward K component where the pivot is offset to the child.
So you get kind of a offset where very nice and tight control over the upper body.
And animators need this level of precision when they're creating game content.
And the upper chest region is controlled by the RFK, which I described before.
And as I mentioned, we have a version of this at runtime.
And this component is pretty cool, because you can apply it to an arbitrary length bone chain.
And it's pretty neat to see it blending a whole lot of bones in the spine together simultaneously.
Now, if anyone would like to hear more about how this is built, we can talk afterwards.
We didn't have time to include it here, but it's a pretty simple trick.
We were able to find the way to get without any flipping, up to 180 degrees.
It's probably good enough for our characters.
Now I'd like to do a hands-on demo of building one of these rig components.
It's kind of to demonstrate that you can put whatever kind of rigging you want into one of these components.
And Forrest will go and talk about how they can be made standardized to fit within our tool system with the Python.
So this is the FKIK component of the ARM.
So, to build this, we're going to first start off by duplicating the arm joint chain.
We're going to create two copies, one of them for the FK and the other for the IK.
Now we're going to rig the FK chain, and here I am applying some control flags, as we call them.
These are what the animators interact with, the blue polygon cubes.
You can use whatever shape you want, and some people prefer NURBS curves, and that's totally fine, it's just a shape node, it's really easy to swap.
Our animators have...
chosen to prefer these polygons, that's what we do.
Now we'll rig the IK chain.
So I'm going to add an IK handle for the shoulder and the wrist.
And that lets us translate the end effector, and it's great for contacting objects.
And I'm going to apply control flag shapes to that as well.
Now we need a way.
to switch between these two sets of controls.
So I'm creating another control flag, just another polygon shape, and onto that I'm going to add a custom attribute called fkikSwitch.
And that attribute is going to drive the visibility of these two controls, so we're only looking at the ones that we're using.
And it's also going to blend the constraint weights so that the resulting joint chain follows one or the other.
Okay, that's pretty much finished. Now I can create a test animation.
And, um, so you notice during this demo, I built this all manually, by hand.
And that's a great thing to do when you're first figuring out what you want in a rig component.
But as soon as you decide what you want, it's really important to take the next step of writing a script to generate it automatically.
Because, um, enough times of, uh, redoing this the tenth time in a row and staying up all night, and you realize there's gotta be a better way.
And, and scripting is, is definitely that.
All right, great. Now I'm going to talk about scene organization.
Hi there. That's the rig component I just built.
And take a look at the outliner.
It's a giant mess.
I wouldn't want to ship this out to the animators in this condition because it's really not easy to know what to interact with and things might get deleted or...
And look in the dependency graph.
It's also a tangled web of confusion.
And yes, it makes sense for how Maya actually functions, but it doesn't make that much sense for someone like me later coming back to find the parts that I want.
So what we do is we create these things called metanodes.
And all it is, is a network node with custom attributes and connections to parts of the rig.
And it helps us organize the scene.
So here I am organizing the outliner.
I'm going to parent the internal workings into a do not touch group.
And I'm going to parent the animator controls into a nice public group that they know to interact with.
Now it's all nice and clean.
You can collapse down to one node.
And on the metanode itself, in the dependency graph out here, I can now create custom connections to the inner workings of the rig.
And that enables our scripts to be able to find specific parts of it.
So, and that is the interface between the MyAscene and our Python code, which Forrest is going to elaborate on.
I originally presented this idea back in GDC of 2009.
It's been really exciting to see how people have innovated on this. I know that David who is at harmonics at the time and Jason Parks who is at Sony at the time and Christopher Built their own versions of this and kind of did amazing things with it so guys if you're out there if you're watching come and talk to me, I want to hear what you've been doing next and There was a student at University of Washington. I know by the name of Jason Garris Jones who?
saw the talk and decided to implement the whole thing in Python and He did such a good job. We ended up hiring him And now he was one of the key architects on our new system, which makes use of object-oriented Python in PyMEL in a really awesome way.
So we had originally even considered calling this talk object-oriented rigging because object-oriented is so useful and so powerful.
If there's ever any debate about MEL versus Python, there is no debate.
Object-oriented makes it so much more effective.
So, and Forrest will demonstrate to you exactly why in just a minute.
Okay, so I mentioned that we can add and remove these rig components.
I'm going to talk about an example of where you might want to do that.
Here's a first-person animation.
We're close up on the gun and we're looking at the fingers.
And the fingers are contacting the gun.
So here they are in IK. I can translate the end and have the middle solve and get a nice result.
And the pole vector helps me choose the orientation.
It's really how you want to be animating if you're contacting an object.
Otherwise, if you're in FK, here we switch back to FK, you're rotating.
and there's a lot of work to do to make it look like it's actually touching the gun.
So, I could have applied an FK-IK switch to each one of these fingers, but you saw how many nodes there are in construction of that rig component.
It would really bog down the Maya scene and make it slow for animators to interact with.
So, this is the perfect place for adding or removing a rig component, and that's what happened just there with our tool, the FingerIK.
So it automatically switches all of the fingers.
It removes one component and bakes the animation down to the bones and applies the other one while saving the animation.
Now I can translate the hand and the fingers will nicely stay on the gun.
All of our rig components are built to support this and it has bought us just so much flexibility in our system.
So I highly recommend it.
And if you'd like to hear more about first-person animation, Dave Helsby just did a talk yesterday about all of the art of first-person animation, and it'll be available on the GDC Vault.
All right, over to Forrest.
All right, who's ready to look at some code?
All right, yeah, I know I am.
We have about 69 pages of code to go through, but a dozen syntax errors.
Let's see, we'll file some bugs.
And yeah, we'll kick off a build.
Yeah, this is going to be great.
Actually, I'm joking.
I only have 45 pages of code.
All right, so in this animation clip, each of the fingers is now being controlled by their own IK component, just like the FP animation.
Each of the IK components is represented by a class in Python.
By using the rig as a tool, animators can add this IK to any finger or all fingers, do their work, and remove the IK when they've finished.
They could also just leave it in place in the scene for future work and all of our other tools will still work.
So the component is designed to work on any finger.
We only have to write or patch the code once.
If there is a patch involved, we can update an existing setup right in the scene and the animator can continue working.
So here's some simple code that shows the create method for the IKFinger component class.
All it needs is a start and end joint and a side and region name in order to create itself on any joint chain.
All of our components use the same type of chain system to avoid the naming convention problem.
This solidifies the idea that rigs are tools.
We can create an IK component, an FK component, or any other type of component on a joint chain just by using the markup.
Animators can switch to different components as needed.
So after the create function, we include all the functions common for any component.
By making sure all the components in the scene have similar setups.
we can perform a wide variety of tasks, knowing that each component will handle themselves appropriately.
When our code comes across the scene node, it instantiates the class as an object in my.js memory, and we now have access to all the methods within the class.
The methods do things like add and remove the component, connect to other components, switch between states, or pretty much anything else we want it to do.
So that's how the components look as code objects.
Let's take a look at the scene nodes again.
The class name is stored in an attribute on a custom metanode.
By storing information like this, we can get an all-access pass to the code by querying the node's type and get even more specific by using side and region.
XRig type is stored here, side is stored here, and region.
It became more effective to encode metadata like this as nodes with attributes and connections so that each rig knows about itself and can communicate with other rigs and components in the scene.
If we look at the finger example, we can query the scene for the active rig and get all finger components on that rig.
Or we can just query the current selection and find all currently selected finger controls.
In this case, we have the middle finger selected.
In either case, we can just attach the fingerIK component in place to the fingers and get them ready for the animator to use by using the methods and functions on the component.
So now that we've seen the scene nodes, let's dive a little bit deeper into the underlying structure of the code object itself.
This is the top-level node from which our components inherit from.
We call this an XRig node.
Earlier, we referred to it as a meta node.
The code for the class is on the left side.
Our Xorg nodes are set up like this.
We don't use init to create our nodes directly, although it makes sense to do so, but we want to control the creation ourselves.
Instead, we actually use a create method to spawn the node in the scene, which calls the init, which calls the new, and ultimately returns the correct type of node for us to use as a code object.
We can get the children of the node as correct class objects, and filter by inherited classes.
In this example, .getXRigChildren of type FKComponent returns type FKComponent instead of XRigNode, and we can access all the methods associated with the FKComponent class.
By using generic components, we have less reliance on naming conventions and can perform operations on all components of the same type, like FK switching, patch updating, or getting all the joints per component for export into the game engine.
This further allows us to remain agile and keep our rigs dynamic.
By using components that inherit from XRigNode, we get all the functionality of the XRigNode class as well.
Additionally, we chose to wrap the XRigNodes with a PyNode, so we can get PyNode inheritance for free.
In this example, .listConnections works as though it was a PyNode and returns to us a list of connections to the selected index finger flag.
Boom.
So now that we've seen how the XRig node is set up, let's take a look and see how a rig component tree is designed.
In this example, the FKIK component class tree inherits from multiple classes and interfaces.
We get all the functions from each tree to keep our code lean.
The key is we aren't just rigging an arm modularly.
We are actually rigging a class and associating the arm class to the control rig class.
It's totally based on inheritance.
This is how it works.
The component class inherits from XRigNode as soon as it's unlocked.
Once component is available, both animatable component and fixup component can use it.
FKIK component inherits from animatable component class, as well as other components that use animated controls, like the SFK component.
The fixup component also inherits from the component class.
But since this component's joints are handled by the runtime rig in-game, it actually doesn't have any user-facing animatable controls.
It's used for components like the multi-twist axis fix-up component, which is used in places like shoulders to handle better deformations.
You guys ready to file a bug?
So let's take a look at a few use cases.
Let's assume we need to get all the control flags for a character in order to set a key.
We can query the control flags on a character by asking the control rig for all of its associated controls.
To do this, we ask the control rig for all components of type AnimatableComponent, and then ask each component for its .getFlag method.
Here, we've just assigned the list result to a variable called allFlags.
So let's say that we need to switch all the IK controls on the rig to FK.
We can query the control rig for all components of a line switch interface.
and then ask each component for its .setToFK method, calling it to make the switch.
Here we've set up a batch-like process to switch the rig in the scene with FK, IK controls, and set them all to FK by using a for loop.
Just as easy as that.
So now that we've seen how class inheritance works in our system and how we can use it, let's take a look at another way that we connect nodes to classes.
To help keep the creation of extra nodes at a minimum, we use an attribute called .bungeeFlag.
This single attribute lets us interface with the flag class directly.
The flag class enables us to turn a specific type of object into a class wrapper.
In our case, we're using a joint type of object.
It just happens to have a polygon as its shape node.
This turns an object in Maya into a Python class instance.
It's much simpler than XregNode because there is only one class instead of a branching tree of classes.
but the power of this lies in the fact that we can add extra functionality to objects in the scene by only adding an attribute and gaining an entire Python class.
From there, we get functions that allow us to generate rig controls on the fly or perform functions on all rig controls with this attribute.
Each rig component knows about its own flags for easy traversal, and each flag knows about its component.
We can also do things like replace the control flags with other shapes, customize them for a specific rig, or even change them for a specific animator.
So let's see how all this comes together in rig templates.
We'll start by creating a player rig.
Rig templates are comprised of any number of rig components which use common utility functions to make up a control rig.
It could be a rig with a simple two-joint structure or something more complex, like a character with forearms and a couple of props.
By using templates, we can generate the rig quickly to deliver it to the animators.
If we need to change the character, like alter the number of limbs, adjust the skeleton, or modify the number of rig controls, we can easily do that in a template and regenerate the rig in a matter of minutes.
The rig components also draw from common utility scripts, so we can share a lot of the general purpose functionality between different rig components.
So let's create a fallen captain.
Rig components are an excellent design pattern for object-oriented programming, which is, of course, how Python works.
This native inheritance behavior lets us share code between components, and because of this componentized architecture, we can also update and add functionality on the fly while preserving the underlying mesh or animation.
So that's pretty much it.
That's how our rig components work.
We assemble them in a script that we call the rig template to construct them into a complete control rig for a character, a vehicle, or a prop.
The rig template is simply assigning components and connecting them to other parts of the body.
Thanks, little light.
Don't do that.
Awesome. So that's a tour of our rigging system and how it's built.
And now we'd love to talk about how we used it to help build Destiny.
So starting out in pre-production, we're setting out to build a whole bunch of new characters.
So that often starts from some beautiful concept art, as we see on the left, and it ends up over here in a fully-fledged game character, as seen on the right.
So here is the work that's needed to get that.
You start with a concept art, you make a model, you do some rigging, and then you do some animation.
But if you do just that and call it done, it doesn't always end up with a character that works that well.
Because what looked good on 2D concept art, once you see it moving, it might have problems.
Like the legs might not be long enough to create a good locomotion cycle.
Or the arms might not even be able to reach the gun.
We've seen things like this happen from time to time.
So, we've developed a different process, which really attempts to accelerate change at this early stage when it's easy to do, when there's not a whole bunch of data depending on it.
So, we stay closely organized as interdisciplinary group of concept art modeling, rigging, and animation.
and we go through multiple phases of development where we're re-rigging this thing every step of the way.
And it wouldn't be possible without what Forrest mentioned about the rig template script that can get us that control rig back instantaneously.
So we can move bones around, change the skeleton, and then make the rig.
And we have various ways of saving animation, which I'll talk about in the post-production section.
So how this works is it starts out with some concept art.
And here's some 2D concept art, but I think these days there's a lot of 3D concept art happening as well.
People will make an awesome sketch in ZBrush, but it's very tempting to want to call that thing done.
Not a good idea. We would put that through the same process here that we're doing.
Well, we will next go to a block model.
And I know it's not the Fallen Captain, but bear with me.
That's what a block model looks like, and it's a bunch of rigid chunks that are parented to the skeleton.
And what we're doing here is we're avoiding all of the really time-consuming parts of everybody's process.
For rigging, that's skinning.
For modelers, that's the UV layout and the high-res modeling, the texturing, normal map ripping.
We'll just wait on that. Hold on.
And then we'll test it, we'll animate it, we'll bring it back, and we'll try it again after we make some changes based on what we learned.
And the animators will give some great screenshots of like, oh, I want to make this pose.
but I'm not quite able to do it with the proportions like this.
What if we change the length of the legs?
And we really work together as a group to get the right balance.
And what finally results is a really nicely performing character in-game.
But it often takes, like, multiple rounds through this process of iteration.
Until we finally result in a nice, stable character model and rig that all the departments can go off in parallel and do all their great work.
High-res modeling.
animators can create hundreds of animations that we'll need for the gameplay systems, and so forth.
So here's a video of some pre-production.
This video was not created for GDC, it was created for an early team meeting where we're doing a sort of concept animation, and early models are being put on rigs and animated to just explore the character's performance.
And we also have early builds of the game where we're doing playtesting, and what is it like to shoot at these characters.
And often you'll see a small thumbnail in the top left, and that's so we can just make sure, is it readable from a distance?
And can the designers feel like, is that a threat?
Does the player know what to do?
So here we see the Cabal.
The Cabal is a block model.
It's a very early stage, compared to some of the other characters that were farther along.
And we're learning a lot about how these characters actually work at this stage, and we're doing what's necessary to make them really, really cool.
So big thanks to the character animation team and the character 3D art team.
Thanks a lot for letting us share your work here.
We also have a hive ogre.
Hive ogre's at the gray model stage, kind of in the middle.
Okay, so here's what that looks like with the progress meters at the bottom, showing that we've kind of started a block model like the Cabal Legionary, and move on to a gray model where we are applying skinning.
We can see how deformations work.
You get a little bit more detailed and hone in on actual bone positions.
And finally, add a production model.
So we're ready at that point with the fall-in character to go out and do real animation and modeling.
One of the keys to making this work is some tools we've built for the 3D artists.
So we have one called one-button skinning, which we've heard about from previous GDC talk.
Yes, I'm going to give that reference and include it later.
And we have animation previews.
One-button skinning imports a pre-skinned version of the model and does a Maya copy skins.
So they get rigging for free, and the modelers can see what it's going to look like when it's rigged.
And animation preview is a tool, a little tool I wrote for them to actually bring these animations into the scene that the animators have made.
And they can test it out with one-button skinning and the animation to just see exactly how everything is going to collapse and create better-looking results.
And it's awesome because when it comes to our desk for rigging...
most of these problems are already solved.
And we just connect the dots and send it the other direction.
So awesome.
I really love working with character artists who know how to work that way.
OK, so that's pre-production.
We've done our best to do all the changes up front.
But the reality of making games is a little bit of an inherently chaotic process.
And that's not bad.
That's just how it is.
And later on, we'll discover that sometimes things need to change.
So, we're going to go through a couple of scenarios, and just for fun, I've themed them as bounties.
So, the first one, for a modest XP gain, is, let's say the character model changes, and we're going to need to update that in the animation scenes.
And this is not a real problem, it's actually a really common, just part of the process.
For example, on our players, we have a lot of player gear.
And I see a warlock standing in the back.
Thank you very much.
Um, and...
you need to be able to update these independently.
So that's where it's great to keep your deformation rig and your control rig separate like this.
It lets you update them independently.
So for example, here I am doing a sweet Nova bomb.
And that's what it looks like in the animation scene where an animator would be working on it.
So if they're going to start out creating this animation, the first rig they would import is a hunter.
And that will not do if you're ranking a warlock animation.
They have totally different silhouettes.
The hunter has a cape.
the Warlock has a robe.
So we need to know what it's gonna look like.
So we use this process called X-Plan.
And what it does is it imports the render scene.
And for each bone in the old skeleton, it will get a list of the connections, and it will rewire them to a new skeleton.
At that point, it's safe to swap the namespace and purge the old one.
And it works just like that.
It's really simple, and it works really nicely.
A lot of studios will use referencing to solve this problem.
And Maya's scene referencing is where you can make changes to one file and have it automatically get propagated to multiple files.
And that works great, but it comes along with a few limitations.
Like, for example, you cannot modify the DAG hierarchy.
And that is something we need to do constantly with our Freeform rigging and animation system.
So, we've been able to overcome those limitations by this technique called Xplant.
Alright, that bounty is complete.
Let's move on to the next one.
So for a slightly higher XP gain here, let's say we need to add new bones to the skeleton.
And in that, we have to also add controls for the animators to animate those bones.
So in this example, we have the Nexus boss in the Venus Nexus strike.
And at first, he didn't have that shield.
And the designers came to us and said, look, he's just too easy to kill.
So we gotta do something about this.
And they decided to add a rotating shield.
And it adds a lot more gameplay, makes it more fun, more challenging.
So we do everything we can as riggers to make this happen.
And we have a simple technique here called patch update.
So patch update starts with an X-plant, the thing we saw in the previous bounty.
And once that imports the new skeleton with the new bones, then we run a small Python script called patch update that will just add the new rig components.
And it works. We don't even need to bake animations.
It's really easy to do.
We'll upgrade the rig version we can track and know, did you get this upgrade?
Okay, moving on to the next one.
So now let's say that we have to apply these fixes in 1,000 different files.
So I don't want to stay up all night and do that.
I would rather have the computer do it.
So we have developed a tool.
Jason Garris-Jones did. Thank you very much, Jason.
Called ProblemSpotter.
That lets us write Python script to analyze the files for known problems in our rigs.
And it can produce an XML report which will tell us where the problem actually exists.
And we might have thought there was a problem in 1,000 files, but maybe there's only a problem in 10 files.
So great, we can apply a targeted fix and only touch the files that need it.
And if we want, we can let that thing run automatically and apply the fix.
We've developed that fix in Python.
So, if it doesn't require manual QA, then we'll just let the computer run overnight.
You know, I love taking coffee breaks when this happens.
I feel like my computer is doing as much work as it possibly could.
I'm working hard by going to get some coffee.
Humans are good at that stuff, so...
We've also applied Problem Spotter here to the top of the rig toolbar.
It's that blue shield at the top.
So whenever animators are in their scenes, if they encounter one of these known problems, the shield turns red.
And they can just click that button, and it applies the fix.
It's really nice to distribute that kind of fixing.
Okay, so now we're moving on to the exotic bounty.
This one's gonna be hard.
Okay, so let's say we need to change that skeleton in ways that will totally break the animations.
And like I said, we tried in pre-production to make it all right, but we know how reality works.
We often have to change things later.
So we have this great tool called Reanimator.
And we first built it for riggers to go in and fix problems with scenes like this and do animation retargeting.
But it was so useful, we put a user interface on it and let the animators use it.
And now it has become one of the core parts of our animation pipeline.
This is how we transfer motion capture onto our rigs and a whole bunch of other great stuff that Forrest is going to talk about.
So what it is, is skeleton-to-rig retargeting.
And each one of our rig components knows how to apply itself to a baked skeleton.
So I can rip individual poses like this. We call it pose ripping.
And it might be useful if you're going to repurpose this walk cycle into something else.
Maybe I'll make a walk variant or a cinematic animation.
And with the sparse key frames, an animator can go in and really do big work on it and get a nice head start.
Or they might decide, I want to transfer the whole animation right onto this rig.
So they can do that with a single button click, transfer motion to rig.
And here we are.
That warlock is walking now.
And my work is done.
Let's go collect some rewards.
Oh, one more thing.
It can work on different skeletons.
So, Reanimator knows how to analyze these different skeletons.
Even though I have different topologies, they might even have different spine joints, number of spine joints.
By analyzing the chain markup, the joint names, and the information in the rig components, we can take this fallen animation and apply it to a player.
So, I have brought the fallen into the scene, scaled him down close to the player, and voila, I have the warlock animation now.
And it's not perfect, you can see the arms are different lengths, you might get some hyper extension, but it's a great starting point.
We have other tools like transfer scenery targeting for getting a more precise result.
I didn't have time for that one, I can talk about it later if you want.
Alright, these bounties are finished. Let's head back to the tower and see what we got.
Oh sweet, exotic chest engram and a legendary primary. I'm hoping for the heart of the Praxic fire.
Oh, I can go and reroll my Iron Banner weapons.
Okay.
Alright, Vanquisher 8.
This is a great auto-rifle for PvP.
But I already have one, so let's compare it to the one I have and see what the randomly rolled stats look like.
Oh, not as much stability, I see.
So, stability is very important for being agile.
We know how important agility is for rigging.
So I'm going to dismantle that.
Get some more Ascendant Energy.
Thanks a lot, Cryptarch.
Tough luck.
All right, so let's take a look at some more of the animation tools.
Reanimator is indeed one of the most amazing tools within our animation and rigging toolset.
We have applied it as a standard on all of our rigs, therefore they can all retarget animation from their skeleton back into their control rig.
This has saved us a ton of times.
In fact, X-Pose, the tool that you see here, this is our Pose Save and Restore tool.
This is built on Reanimator.
It has the standard features of any Pose library, Pose saving, mirroring, sharing between users, but it can also restore on a component level because of our system.
We can grab only finger rotations and apply it to the rig, or we can grab only upper body components if we want.
X-Pose can also apply a pose based on any flag's position or rotation, which is excellent for getting foot placement correct.
Xpose gets its power by using the fundamental parts of the way components work.
And because each rig is built with its functionality in mind, it is compatible with any rig.
In fact, it can transfer poses between characters as well, even if they have a different number of components.
Pretty sure I tried to put a player animation on a player ship once.
It was kind of interesting.
So with our rigs tightly integrated into our Python tools, it unlocks the door to writing all kinds of other awesome tools for animation.
We developed these tools to help the animator work with the rig easily.
If the rig is in the wrong control scheme, it takes a ton of counter-animation to make it move the way they want.
So in this case, we see an animator working away, and he wants to adjust the pose.
In his fist of havoc here.
So he's rotating the spine, but as you can see, the hands don't stay planted on the ground.
So using the context-sensitive toolbar that we've developed for them on the left there, they can easily switch both arms to IK, and the arms will stay planted.
Oh, I guess we included a character picker in there as well to make it even easier.
Great, so now they can rotate the spine, and the hands will stay planted on the ground, which is exactly what they want.
Well, let's say they want to do something slightly more drastic, and adjust the cog control here.
Well, unfortunately, the hands don't stay planted because they're actually following the parent space of the cog.
So, no problem, we can just grab the hand flags here and easily switch them to what we call F super.
This is our topmost node in the rig.
And now the hands will stay locked into that position.
And they can freely adjust the cog however they want and hands will stay planted.
That's pretty sweet. They can also still adjust the spine, and hands will definitely still stay planted.
Works for rotation too.
Excellent.
Alright, so perhaps he's going to transition back into the rifle pose.
But he doesn't have a weapon.
Okay, well, let's take a look at the armory here.
We sort of gave them a list of any kind of weapon that they're going to want.
We'll just equip the standard assault rifle for now.
He didn't win at the Crypt Arc.
So great.
He now has a rifle.
Notice everything stayed in place.
It just added on the fly.
So in this case, the animator only wants to adjust the prop here, but the hands aren't following.
So of course, we can just use our weapon toolbar widget here and have.
easy button selection to have the hands follow the space of the weapon.
It's looking pretty good right there.
All I have to do is animate that one flag. It works for rotation and translation as well.
Both hands will follow.
It looks like there's a problem. His left hand isn't in the right position.
It's kind of loading off in space.
Well, using the weapon toolbar widget here, we can easily snap to a predefined position on the weapon.
just to keep consistency.
And lo and behold, the hands are still set to follow.
So it makes it pretty easy for them.
And with a few button clicks, he now has a weapon equipped, and he's holding it in the right way.
So we've developed our component structure to make it easy for us to write advanced functionality and expose it in the window-based UI, just like the rig tool bar you see here.
Everything is constrained to the top there.
We call this the active rig.
It can be interacted with in a multitude of ways, including the multi-constraint example where the hands can be set to follow the weapon.
So multi-constraint is very useful, but since it becomes built into the rig, it can only depend on parts of its own self.
There's another way we have solved this problem, and this way is even more flexible.
The Overdriver tool lets the animators drive rig controls by anything in the scene, like in this cinematic shot where the pistol needs to change hands from one character to another.
We can tell the control to have its position and or its rotation be driven by another object or even be driven by world coordinates.
The flexibility in this system is that OverDriver is completely compatible with all the other tools so they know to treat it as a rig control just like any other rig control.
This means that it also works with our Pose Save and Restore tool, our Transfer Animation tool and our Component Add and Remove functions as well as the other tools.
What we have found is that there isn't necessarily one right way of doing a lot of these things in rigging and animation, but more that it's great to have a variety of tools for animators to mix and match, get the specific behavior they might need for a given animation.
Yeah, that's right.
Overdrive.
So it's great that all of our characters have access to all these animation tools, but our game design calls for all other kinds of things that need rigging and animation too.
There are a ton of vehicles, weapons, props, environment objects, skybox objects.
They all need rigs.
Because they are created from the same rig component system, they have access to all these animation tools.
Everything automatically inherits the functionality with zero extra work needed by us riggers because of the way we design the components.
That being said, animators still need standard rigs to work with while maintaining the core functionality of retargeting and modularity.
So there are so many rigs in Destiny that us riggers couldn't possibly have created all of them by ourselves.
We removed ourselves as a bottleneck in the pipeline by building tools for artists to create their own rigs.
The majority of the props for cinematics, environments, and skies were made by the artists in those departments using tools like this one, the Create Rig Render tool.
This is the big satellite dish from the final Cosmonaut mission, and it was automatically generated.
The tool automatically creates either a single-node rig for basic placement in the world, or a full FK rig like you see here, with components that get connected in the same order as the skeletal hierarchy.
So we initially set out to make a rig builder tool to go beyond simple rigs like the satellite dish.
But as we were designing it, we realized it's also useful as an animation tool because of the way our rig components can add and remove themselves while preserving animation.
Naturally, the next stage of our rig building tools led us to create a tool that gives animators the freedom to adjust the rigs themselves.
The animators can start with one of the default rigs to add and remove as they go, or they can start out with a skeleton and add only the components they need to craft a particular animation.
Either way, they have the complete freedom to animate comfortably, and all animation is preserved while constructing their rigs.
So this video demonstrates the power of having a modular system.
Animators can add or remove components at will, depending on the needs of the animation work.
They can strip off an FK-IK component, replace it with FK, or they can replace the reverse foot component with a standard IK.
Sometimes animation calls for a new pivot point to be set to an elbow or a knee.
They can remove the existing component, start the chain at the clavicle or the hip, to pivot from the desired location.
Some animators like to keyframe on the skeleton itself and only add IK to the legs.
All this is possible while preserving animation.
By keeping this system flexible, we can put more of the rigging in the animator's hands and give them the freedom to use the tools however they need to.
So final thoughts.
Let's summarize everything that we just learned.
The rig is thought of in terms of regions, so we can add or remove components as necessary.
This greatly increases our agility and allows us to iterate quickly.
Components are designed to be generic, so we can use them wherever we need to.
For example, an FKIK component isn't defined as an arm.
It can be used on any three-joint chain that needs the functionality.
Doesn't even need to be a character.
It could be a vehicle, spider tank.
Components are required to be retargetable and as lossless as possible.
We can update components at any time and preserve animation content along the way.
The tools complement the components and interface with classes, keeping it lean by sharing code.
Animators are totally happy with the freedom to hack away, knowing that their tools will still work.
There you go.
Go Space Magic.
Yeah, this is our whole team.
This is using our actual characters because we like to play.
Yeah.
We like to dance.
Yeah, who doesn't like to dance?
Sweet, all right.
Well, thanks to all the people that helped us out.
Yeah, we had a great time.
