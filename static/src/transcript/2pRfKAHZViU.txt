So, we're both ready to get started.
Jeez, I hope I said about, not about.
That would completely give me away.
All right, we're gonna, the lights?
Yes?
No?
Yes, we're getting the lights?
Okay, good.
Okay, so if you're attending to, intending to see my presentation, How to Build Nightly When There Is No Night, then you are in the right place.
I'm supposed to start with some housekeeping.
You know, it's pretty late in the day, so they ask you to defrag, but I suspect it's probably not that big a deal.
The surveys they send out afterwards, they encourage me to tell you to fill that out.
If you have good things to say, then I would absolutely encourage that you do.
If there's something that I don't cover as part of this or that I don't touch enough on, If you don't feel like telling me or asking about it in the Q&A afterwards, please let me know.
This build CMCI stuff is a bit of the neglected stepchild of the industry in the process.
If it's something that you're passionate about, which I'm assuming you are since you're here, let them know and like I said, maybe I can come back and do some more.
I also wanted to extend a personal thanks just before we get started.
I know everybody thanks you guys for coming, but it's 5.30, it's Thursday, it's late, we're tired, we're hungry, the Zork presentation is on, the South Park presentation is on, even I want to be at the Zork one, so I really appreciate that all you guys are here to listen to me instead.
Alright, so I'm just going to take a quick second and introduce myself.
So my name is Josh Nixdorf, I'm an Associate Technical Director up at Electronic Arts in Burnaby, which is in Canada, if I didn't already give that away.
Technical Director is just a fancy title at EA for the guy who's always in trouble when things don't work.
So I currently work with EA Sports, which doesn't really exist in terms of an organizational structure anymore.
It used to be the studio in Orlando and the studio in Burnaby that made all the sports games that have the big EA Sports when you actually start the game.
That'll still be there, that's not going away, it's just a marketing thing rather than organizational now.
They've changed the company so now we're all working together and we can do more cool stuff.
So my team is the development and release engineering team.
We're responsible for the source control, the continuous integration, the builds, the dev side automated tests, the whole pile of that kind of stuff.
As well as a bunch of the development workflows for the dev teams including FIFA, NHL, Madden, Tiger, UFC, that kind of stuff.
And now that sports doesn't exist, we'll be extending that into various other games within EA.
Alright, so...
What did I mean by no night?
Is it, you know, convincing your developers that they should, you know, forego sleep in order to work on the game?
No.
And even if I was here to talk to that, there's no way legal would have ever approved me giving you that secret.
Thank you for whoever laughed, I appreciate that.
No, seriously, I've never seen anything like that while I've been at EA.
It's...oop.
There we go.
It's also not a presentation on how to work in an environment where the sun doesn't set.
As I mentioned, I'm from Burnaby, which is in Vancouver, and if you've ever been there, you know that the sun's barely up.
If you're specifically interested in how to work when the sun doesn't set, I can get you in touch with some of the guys in DICE in Sweden, and they could probably have much more insight into that.
So what do I mean by this no night thing?
Classically, our workdays are, I say, 9 to 5, but I don't know if you guys are anything like us, it's more like 11 to 7.
And maybe that creeps even longer.
The point is, there's some set period of time beyond which there's this window, this magic window after everybody goes home, where the automation can do its thing.
It's generating artifacts, it's going back and getting coverage, it's doing things.
It's these nightly builds that we all have.
And typically when you've got your base automation set up going for a team, you've got some set of continuous builds and some set of nightly builds going on.
So the problem we encountered, and it's what a lot of this presentation is going to cover, is as we took some of our games, specifically FIFA, and we moved that development of those teams more and more global, We lost that window.
And it's really easy to take for granted that window.
How important are your nightlies?
Well, you'd find another way around it.
At a certain point, that window is actually the flexibility for a lot of your processes.
And when you lose it, you kind of find out how fragile a lot of these things really are.
So in the course of this presentation, I'm going to discuss a couple of dirty secrets that we have.
You will probably find, if you really think about it, that you have a lot of these things too.
If any of you really thinks that your build system is absolutely perfect, and that nothing I said applies, I'd really like to hear from you at the end of this, because I want to know how you've done it.
In the meantime, can I just do a quick poll of everybody?
How many people are build engineers, build managers, someone who's responsible for the actual build stuff?
Okay, I see a few hands. Okay, good.
And how many people are here because they're really concerned about the builds they have?
They're not convinced that they work well enough.
Okay, that's a lot more hands. That's what I expected. Good. Okay.
So this will cover things, a lot of best practices, even if you're not in an environment where you're going to be going highly distributed.
One of the most helpful things that you'll get from this will be evidence to help your arguments.
A lot of the stuff I'm going to talk about will be a no-brainer.
You're going to look at that and go, why did you even tell me that? That's obvious.
The point is I'm giving you an argument to take to the guys that have that money so that you can actually use that to improve your odds of getting this stuff.
Because just because it makes sense doesn't mean that they give you money for it.
All right.
So in the beginning, our dev teams start like this.
Apparently, we're all very, very young.
We're happy, we're excited, we're all working together.
We can see each other, we can talk to each other.
When there's a problem, we run over to each other's desks, we work these things out.
It's fantastic. This is the glory days.
But typically it only works for really, really small teams.
It doesn't take very long before you start organizing.
And I'm going to use organizing as a dirty word here. Before you know it, you've got some set of game development.
You've got a department for that. You've got an art department.
Maybe you've got a localization department. Of course, you're going to have a QA department.
And these guys are going to work together, but you get to the point where people don't need to talk to each other anymore.
And once you get to that point, then it becomes a very logical progression of, well, do these people even really need to be co-located?
And so I'm going to do a quick example of what ended up happening with FIFA.
So largely, the FIFA soccer game is developed in Burnaby.
And yes, for anyone who's thinking it, we get the irony of making a world-class soccer game in Canada.
If it makes you feel any better, a huge proportion of the team are all European.
So they're just a tiny little conclave in Canada.
So classically the development happens in Burnaby.
That was everything. It wasn't just the development.
The art happens there. QA happens there. Everything happens there.
Eventually these teams get to the point where, well, you know, maybe some of the stuff could get moved.
And so localization was the first one that I remember to move somewhere else.
So we moved that off to Madrid.
Why did we do that? I don't know, I didn't make the decision, but I can make a pretty good guess that it probably had something to do with money.
In this case, it's really easy to get access to people who speak a variety of languages in Europe, and Madrid was a nice low-cost place to do this.
So in the old days, we would create our build, same as we would for QA, and we would transfer that massive ISO disk over to Spain, and they would play the game and add some sort of localization data, and potentially send that back.
Now originally, they weren't really checking into the depot, but eventually someone pointed out that transferring 8.5 gig to Spain once a week was a really big waste of time, and if we were to simply get them access to the source control, they could likely build that build themselves faster than we can actually transfer it there.
And once that happened, once they had access to source control, it becomes a viable option for them to start making their localization check-ins themselves rather than through working with someone in Burnaby.
So then we added Hydrobat in India.
And this started out being reasonably simple QA type stuff.
Maybe they helped write some automation scripts that we would use for testing.
Simple stuff.
But as we worked with those guys more, we started to trust them more, we start using them for more and more stuff.
And thankfully, they're causing some check-ins.
Then we outsourced more, and we're still all doing Madrid and Hyderabad.
We didn't stop that.
We started doing more stuff in Buenos Aires and other parts of Argentina as well.
And this was the first time that we really started working with people doing actual development on the main code base, not just some QA stuff or localization, general development going on.
But still not necessarily a ton, and usually around specific features.
So maybe we'll give them this feature and they'll do that.
It's part of why I've got these guys colored in light blue.
is they're not doing a ton of interactions.
Well, and I probably shouldn't admit this, but I'm going to anyway, at some point we may have burned through all the people willing to be QA testers in Vancouver, and so we needed to do QA somewhere else.
So Baton Rouge is where we moved it to.
Probably not how it actually happened, but it's more colorful that way.
And so a lot of our games do, at least a lot of the sports games, do testing on a Baton Rouge.
And the same thing happens there. You start getting some really bright people there, and suddenly check-ins start flowing from there.
And then finally, when what we would call Gen 4, but PS4 and Xbox One came along, we couldn't really just double the size of our dev team to handle the old, the current generation and the new stuff coming out.
So we needed to find a way to get that dev team able to build for all these platforms.
So, well, we opened up a studio in Bucharest in Romania, and actually I think we already had a mobile one, but we expanded it, and a bunch of the development of the main FIFA game started happening out of there.
And that was the point where it really tipped the scales to, you know, almost half the check-ins are coming from Burnaby, and almost half the check-ins are coming from Bucharest, with small other sort of spice coming in from all these other ones.
But the point was, once we got our check-ins looking like this, we stopped having those windows where we could do things.
At any given time of the day, somebody was checking in.
So what challenges did we hit when we started doing this?
The first one, obvious, this sounds expensive.
Obviously we're saving money, presumably the reason why we would do this outsourcing, but what is this doing technically?
Integrations become...
common. It was one thing when they were all localized, you could talk to people, now some guy in a different time zone is doing this thing and he's not talking to you at all. So these became much more expensive than they usually are. We've all seen this, we all know this problem.
The works on my machine problem is a nightmare on a good day.
When it's some guy in Romania who doesn't ever work any hours that you work, telling you that it works on his machine, this got substantially worse.
I can't stress how much worse this got.
Your branch health becomes another interesting note.
As I called out before, you're working, you've got people checking in 24 hours a day, so you've just lost that window where you could do that risky check-in after everybody else goes home and just make sure that you've got things fixed by the morning.
Alternatively...
Maybe I check in, I take off for the day, and if it's broken, I'm going to log in at night, no big deal.
You can't do that anymore. If your code base is broken, you're impacting someone else's ability to work.
So let's start by talking about what were some of the things we did to address this integration cost.
The first thing was recognizing how good we used to have it, basically.
That local integrations were really, really simple, and we really wanted that back.
What were we trying to do?
At a certain point, we realized there was no way to make less integrations happen.
When we went more distributed, things just got more complicated.
It was basically not giving up, but accepting that it wasn't just integrations, there was a variety of things that got more difficult.
An example I'll give is even syncing for the very first time.
You've hired a new guy in Europe, it's his first day, what's he going to do?
Well, he's got to sync the game, right?
So we start syncing that from your North American depot and suddenly it's slowed down for everybody.
Any time that we have these type of interactions, where developers can impact other developers, and I'm going to say developers a lot, but what I really mean is content creators, so these are your developers, your level designers, anyone who's checking in.
Anything really. Anytime they're doing that, they're going to have some interactions.
These things are going to run amok, and that's what you really need to be worrying about.
And that was, I guess to reiterate this one last time, the key point.
It wasn't really integrations that were a problem, it was interactions.
So how we started addressing this was, we started thinking about our commits, our work, as a flow.
And this is maybe a bit weird, especially if there's any product managers or any of that ilk present.
You have these people, it's these people's jobs to make sure that the features get into the game, that it happens on schedule, that this thing ships and that you guys make money.
Someone is planning that.
If you're on the development side of things, you should care about that, you should be aware of that.
But if you're responsible for the build and the build health, as a few of you were, or as a lot of you were if you're concerned about that stuff, then the right mindset for this is flow.
How much work are these people able to do?
What are the things that are blocking them from doing more work?
Don't worry about the nature of that work. Think of it purely as a volume thing.
At Electronic Arts, the team that I'm with, the development and release engineering team, that's what we do.
We're the guys that are there to facilitate via CI or tools or processes.
Whatever we can do, it's our job to make sure that that developer can get his features into the game as fast as possible and spend the least overhead possible worrying about whether or not the build is healthy.
This flow analogy is really important too, as we're going to get to in a minute, because flows intersect.
That individual user thinking this is work as a flow, this is obvious. I haven't told you anything particularly important.
It's recognizing that all those flows have to meet at some point.
And so, source control is where we call that out.
So the best practice here is that recognizing that source control is the confluence of all your effort.
There isn't a heck of a lot that you can do about that.
Source control is where people's work are going to interact.
And you may have other systems that do pre-submit or other things, so that things happen before it goes right to source control, but ultimately there's some place where all this data is going.
That's probably source control.
And that's where you're going to hit all these problems.
And so recognizing what I said before, team growth makes it inevitable for the number of these interactions to grow.
Branching is hopefully something that's crossed all of your minds by this point.
Why isn't this guy talking about branching yet?
This is absolutely a solution to this problem.
For a little bit, it's a solution, not necessarily the solution.
You get all these guys working in branches or some set of appropriate branches, and it definitely reduces the likelihood that you're impacted by someone else leaving their branch in an unhealthy state.
But at some point, these things still need to get merged, and a lot of the same interaction problems still pop up.
Game architecture is another perfectly reasonable way to solve a lot of the problems associated with this.
If you can get your AI completely segregated such that it has almost no overlapping files, code, dependencies on your rendering system, which hopefully there aren't too many there anyway.
Then it just makes it that much easier.
The more dependent your code is, definitely the more you're going to hit this problem.
But it is a solution, not necessarily the solution.
So, some of what I've said so far probably sounds a little bit, you know, strange.
Why would they have all these people that sound like they're checking into one branch?
Well, this is the first dirty secret that I want to get out there.
For almost all of the sports titles...
Everybody works in the same branch. Now Madden guys don't work in the FIFA branch obviously. Madden has its branch, FIFA has its branch, NHL has its branch, but for the most part everybody, regardless of where they are in the world, is checking into mainline all the time.
There's no staging branch, there's no anything else.
Now, there's some exceptions to this.
We definitely branch for events, E3, that kind of stuff.
And there are exceptions to the rule.
FIFA has done some clever things with their AI to allow that to be off in its own branch.
But everybody who isn't working on the AI, checking into the same branch all the time.
Why in the world would we do that?
How does that work?
Some of the stuff we're gonna cover in the next little bit.
The trick is recognizing that The easier you can make those interactions that I've been talking about, and the earlier you can make them happen, the more viable you make something like this be able to work.
So I'm going to give you some fake math because I couldn't help myself.
And a graph, in case this isn't startlingly clear at this point.
Reducing those interactions is bad.
The number of interactions that you have, whether that's typically commits per day is the right kind of metric to think of for this, but it might be your team size if you don't know what your commits per day is.
Trying to hold that constant or reduce that means that you're either going to have a smaller team or you're making people hold their check-ins locally longer.
Holding those check-ins locally longer is just increasing the amount of effort you're going to have to do when it comes time to integrate.
So the nice pretty line here, this optimized one, is trying to show you that you can reduce the pain of the interactions not by reducing the number of interactions, but by reducing the magnitude of those interactions.
So what do I mean by that? It's time for some specifics.
Make source control fast from every location.
This is a no-brainer. Boy, if this is what this presentation is going to be about, you guys are going to be disappointed.
I promise you it'll get better from here.
This is one of those ones I'm talking about at the beginning where everybody knows this, but that doesn't mean that you're going to be able to get the money spent on this.
So you're going to open up a new location.
I don't know, you're doing some outsourcing again. Look at our case, Romania.
How do you make sure that those guys are working really fast?
Speed is easy when it's local, you just throw more hardware at the problem.
When you're doing this in some type of distributed global environment, you have to rely on someone else's network, which means you're paying for access, you're paying for bandwidth.
These things have a cost associated with them.
Plus, moving large volumes of data is still expensive in 2014.
I'd be interested to know what anybody's best metric is.
But I can't imagine that we can move an ISO anywhere else in the world.
Any signal... Overseas, let's put it that way.
I don't think we can move a disk image, any big disk image overseas in under probably four hours.
And even that's massively faster than it was just a couple years ago.
So I'll go back to that early example where we talk about having someone in Europe that is doing their initial sync.
Well, if he does their initial sync, he's impacting those local users.
Say that problem is taken care of, but it's slow.
He's connecting to this remote North American depot.
Every time he goes to do a commit or any type of operation, it just sits there and does nothing for five minutes.
It's from his perspective, it does nothing.
Well, how's he going to react to that? He's going to start doing it less.
And again, the less he starts doing it, the less he's integrating, the more expensive those are going to be, the more it's costing you.
And so this is where a trade-off comes into this.
Obviously, it's easy for me to say, spend all the money you can on getting that bandwidth.
But that might not be the right thing to do.
What you're going for here is to recognize that you have that option.
The more you spend on bandwidth, the more you save on time.
If you have no money to spend on that bandwidth, just recognize that you're paying for it in time.
That might be a completely reasonable way to pay for it.
So I'll go over what the architecture looks like on FIFA for this briefly.
Imagine that that's Vancouver, sort of.
All our developers are very classy. They wear suits.
So the main repository for FIFA for historical reasons is located in Vancouver, Burnaby. All the local developers work on that.
When we move to Romania, we set up a proxy in that location. All those developers work on that.
When we expand it into South America, same thing. You set up a proxy there, you have all those developers work on that.
The interesting thing here is, it goes back to the bandwidth, depending on the source control system that you're using, ours is set up such that those remote users, when they're using that proxy, when they do an initial action, that's when it synchronizes the surfer.
So the proxy allows the data to be cached locally, but it isn't necessarily synchronizing locally.
So those guys are paying some type of a time penalty when they do an initial sync.
These little recycle arrows I put on here to represent what we built on top of the source control was just a local CI job that just does nothing but sync that local repository.
And the purpose of doing that is to keep that thing always up to date so that on the end user's side, the sync is always as fast as possible.
He never has to pay the initial penalty of synchronizing the data overseas.
But again, this is a trade-off. We're paying for that in bandwidth.
But we save devs time.
And this is important for us because time is the most valuable resource on FIFA.
As you already know, this is an iterative title, we put it out every single year.
You'd think that means we have 12 months to work on it, but the reality is we have to do some amount of launch support, so we're not getting started until, thinking about the next one until a couple months, maybe six to eight weeks after launch.
By then it's Christmas time, so nobody's really thinking about what they're doing, maybe they're doing some amount of pre-pro.
The reality is in the best case we only get nine months to actually put proper development into FIFA before we're trying to get it out the door again.
So giving up even a week because we have some synchronization issue with our remote development hurts our schedules terribly.
Now, if you happen to be using Perforce like we are, this should sound reasonably familiar.
There are some benefits coming down the road.
Perforce has assured us they have new technology.
I think they call them edge servers coming out.
And the edge server should really prevent us from needing that local CI job that's keeping that stuff in sync.
That it has some intelligence I'm not entirely familiar with.
But the idea there is that it's going to do the best it can to make sure that that data is synchronized and reducing that bandwidth.
If you happen to be using a distributed source control system, you may actually have even more traffic than this.
There's a ton of benefits from doing that, I'm not slagging them.
But it's just noting that there's a tendency for those to actually transmit even more complete sets of data than something like Perforce would.
And so you may actually find that you're paying a lot more in-bandwidth.
Now, this doesn't necessarily matter a whole bunch if you have small branches, which we don't.
It leads to...
Your branch size matters.
So my original purpose for this presentation was getting back to that sort of dirty secret of why do we have all our developers work in one branch?
Anyone who joins the company will say, this is really bad CM you guys, why are you doing this?
And they usually get some type of an answer that doesn't represent reality.
And so I thought, okay, I'm going to do this presentation.
I'm going to admit to all these guys this secret.
And I'm going to answer why it was that we've done this.
And my goal, I'll admit, was to blame integrations.
I called that out earlier, integrations are expensive.
And so I sat down and I started thinking, well, what is it about integrations that we really hate, you know?
Why is this such a problem for us?
And it turns out integrations aren't a problem.
They're really not that hard in the grand scheme of things.
They're not that expensive.
They're a nuisance and developers generally don't like them, but there wasn't anything that was actually wrong with them.
So it turned out what our problem actually was, was that we were avoiding branches.
We avoided branching altogether.
And that was kind of a startling realization, like, wait, why is branching such a problem?
And so we dig into that, and it turns out it's because our branches are huge.
And when I say huge, I mean 150 gig.
They're really big branches.
You decide you want to cut a branch to do some feature in a safe way, that's a lot of extra sync that you just had to pull down to your system, that's a lot of waiting that you had to do to get that.
It's painful, so as a result, people have an aversion to doing that.
So if we continue asking why, well, why in the world do we have such large sinks?
Well, data synchronization.
We never really invented a good way to make sure that our assets were in line with our code.
How we did this was, if you change the code that would impact the assets, you're expected to rebuild the assets and check it all at the same time.
And we'll get to why that has other downsides in a minute, but that was really how we solved that problem.
Why do we do it that way?
Like, why would we do that?
Well, that was how the game team was organized.
We had an art department, we had a dev department, and those guys worked together.
There are obviously all sorts of roles that bridge that gap, but not necessarily anyone who was responsible for that whole picture.
And so this process made sense.
One team hands the process over via check-in, and it just, it makes sense.
Everybody works and nobody ever gets blocked by this.
How did we end up, you know, why would we organize that way?
Why would the system work that way?
If anyone's familiar with Conway's Law, it basically says something to the effect of any software designed by an organization will almost always end up resembling the interfaces of communication in that organization.
And this ends up being very true here.
The only reason I could find for why we did it this way was...
There was an art department.
So we handled all the art this way.
And there was one guy from the art department who talked to one guy in dev.
This was the process they came up with.
Okay, so it made sense.
Now, why are we stuck with it?
Knowing that, it's because asset synchronization was a huge problem for us.
For years, it was you could sync to head and not be guaranteed that you had some good set of data to actually go along with that.
And this caused us no amount of grief.
It was especially bad for producers or other people that need to play that game.
I mean, QA was slightly insulated because we made sure that something was blessed before we gave it to them.
But for embedded QA or production or designers, those guys need to have access to that stuff.
And there was no really good way that we could say, hey, here's that thing that's brand new.
And here's a set of data that's guaranteed to work with it.
So for us, it was absolutely huge.
It was a very important problem.
And at some point, somebody decided it was much more valuable than the integration cost.
So we stuck with it.
And over the years, we've developed around it.
We've re-architected the game so that we need to do less integrations.
We've re-architected the build system to make some aspects of that much easier.
We've built tons and tons and tons of tools and process around the pipelines just to make that stuff out a little bit faster.
So integrations become less necessary.
Looking back, would we do this again now?
No, and every time I pointed out to someone, hey, I think I figured out why we do this, it was a face palm every single time, and people just wanna sort of forget about it, but that's the way it was.
So as we're getting back on track, so I'll give you some quick details about FIFA after this.
So this is sort of a mock histogram of what commits look like over time.
And this is sort of going back to the flow point a little bit, and this window of availability for automation or process.
Odds are good if you've never created a histogram of what your source controlled commits look like.
It probably is something like this.
The leftmost is whenever your people come in.
There's typically some large volume of commits in the morning.
People have done some type of testing.
They've done something overnight.
They come in the morning to make sure that it's good.
They check in in the morning.
Typically, commits will stay reasonably high for a little while.
Some guys get in later.
Some guys need to fix a bug.
They'll get done.
They commit as well.
That first low bar, that's lunch.
The higher one after that is of course when everybody gets back from lunch.
And then that last one is right before everybody's trying to leave for the day.
This makes sense.
All that white space on either side is that window I'm talking about where you have the ability to go and do all sorts of wonderful things.
And when you have people working all over the world...
This is more what it looks like.
You end up with maybe a couple of low bars while you're transitioning from North America to Asia, because it's a big, there's a whole bunch of time zones and unless you've got guys in Australia or Hawaii.
You know, there's not a lot of time in there, but eventually it'll pick back up.
And now we've gone from this case where there really weren't going to be any chickens overnight to we maybe have a couple hours right after the guys in North America went home, and they absolutely won't allow us to do any major automation or anything risky during that time, because they've got a milestone or a gate or anything.
That's the time they want to eat into to be able to use.
So for some numbers on FIFA, just to give a scope of what I'm talking about.
I told you that our branches were in the range of about 150 gig.
They do go up to about 250 gig, depending on how careful you are with your client spec.
But if you are really good about that, you can actually get it down to about 75 gig.
This is the essential amount of data, but that's the minimum data.
I think that's the thing the build machines actually have, so they're not wasting any storage.
The number of files, if that's an optimized sync, is about 360,000 files.
If you don't put a lot of effort in determining that sync down, it's easily upwards of a million.
Which is ridiculous. Our largest file is about 700 meg.
It's a bundled audio asset. I'm sure a lot of you guys have that.
Our most edited file is... the most edited I could find was about 74 edits between now and last September.
So that's quite a few per week.
Revisions per day, I only took a look at last week.
Not including AI for any of these numbers, there was about 50 per day.
If I include the AI guys, who like I said were on a separate branch, that gets to about another 25 per day on top of that.
So even right now we're way out from final or sort of, I guess we're well into production, there's 75 commits a day.
Another interesting note while we're here is QA and localization always have current builds.
We don't give it to them once a week or once every couple of days.
Localization has the build that's absolutely as current as when we started copying it to local.
And QA is the same way.
We have every single night, or every morning, they have a build waiting for them that is the most current build as of usually 3 in the morning.
Yeah, actually that's fair enough.
Alright, so in order to make all this stuff work in one branch, there's a fair amount of discipline and automation.
We're going to get to the automation in the next second, or in the next couple slides, but for now let's finish up with source control.
So at this point I would like to make a quick apology.
At some point I developed a warped sense of humor in the process of making this presentation and almost all the slides after this attempt to be funny.
So hopefully you'll find some humor in them.
If not, my bad.
Alright, so this one's obvious, right? Limiting binary files and source control.
Binary files take up a lot of space. Source control is particularly terrible usually.
I don't know of any system that actually does diffs on binary files for these things.
So you end up in a case where the more you're putting in there, the more storage you're using up, and you typically end up using up really, really fast.
But you do want revision history on a bunch of this stuff.
As I said, in our case, we're checking in a bunch of the assets along with the code to make sure that it's synchronized.
There's not a lot we can do about limiting binary files.
In that case, we need those.
At the end of the year, or at the end of the cycle, we want to make sure that you can sync back to any changelist and have that good set of data there.
So we have to leave those files.
But what we try to use as a trade-off here is any files that we can regenerate reasonably easily, we'll try to not check in.
So an example of this is the Madden game has sort of their asset bundles that they check into Perforce.
Now they will limit these things, but the reason that they do this is so that if you happen to be at head or near head, you can get the last couple of these things, you're working very, very fast.
You don't need to generate these.
If for some reason you need to go back, say, more than a week or however many of these things they keep, You can absolutely do that, you just need to take the time to regenerate your quick load files.
FIFA doesn't do this, because unfortunately the rate at which they churn just makes it such that they'd be burning space.
So as part of your building of FIFA, you almost always need to build those quick loads, but how they got around having to spend that time was that the game almost always works with those loose assets as well.
You wouldn't do any performance profiling with that, but it's playable. Very playable.
This is NSync, in case you couldn't tell.
Protect yourself from bad syncs.
So we kind of touched on this one a little bit early.
You're a new member to a new team.
What's the first thing you're going to do?
You're going to go and sync.
What impact does that sync have on everybody else?
I don't know about you guys, but I know when I started, we had a very firm rule.
You do not do a full sync during work hours.
You set it up, you do it right before you leave, you go home for the night, so you were not likely to be impacting people.
But it always happened.
And part of this was, I don't know about your build systems, but we definitely had a couple bugs in ours where things would leak into the source tree and at some point you would go, I don't know why this isn't working, I'm just going to re-sync everything and see if that fixes it.
And you don't want that guy to say, well, 10 o'clock and I think this is the solution.
I'm going to have to go home now.
So you let them think.
But suddenly these things start taking a toll on what everybody else can do.
Not such a big deal if this is local.
If these guys are remote, this becomes a really big cost.
They're sucking juice out of your server.
This gets slow.
The more of these actions that you have, the less everybody else is getting through your source control.
So obviously the solutions to this are simple things like documentation or triggers on your source control, and that's largely what we do.
In fact, we have a trigger on the source control that points you at the documentation if you do something particularly terrible, like say, I'd like to sync all of FIFA.
You get the big denied email that makes you feel particularly upset.
And then it points you at the documentation where it says you may not sync the root.
You must only sync some of these folders.
As a protri... triggers are great.
Use them for as much as you can.
If you can kick off your CI that way, do it.
Some of the best ones that we have block you from...
If you didn't put a bug number or a feature number, some, you know, you have some system where you're doing work.
If you don't have a number indicating why you submitted this check-in, it gets blocked.
Again, continuing the warp sense of humor, keep your CI syncs separated.
So this is a really interesting one, and I don't know that you'd normally hit it unless you had a really big scale, but for a good chunk of FIFA's development during the year, we actually have more automation than we have people.
And so there may be up to 100 machines that are doing various CI jobs all at exactly the same time.
First thing in the morning, if you're the first guy to put a check in, when the system has managed to hit an idle point, 100 machines just kicked off and they're all thinking, what you just submitted.
If someone else tries to do something...
It's just, it's junk. Their experience is slow.
This typically would happen, say, first thing in the morning in Romania, or first thing in the morning in Burnaby, and it's a painful experience. First thing in the morning, people are actually excited to work.
They get in, they want to do stuff, and if their first experience with source control is, well, I hit sync and it's not doing anything, or I tried to submit my check-in and I tested all night, and it's lagging, these guys get upset, and...
They're unhappy with you.
So the solution to this was we actually set up a proxy inside of our local site that only our CI system uses.
This is a relatively cheap thing to set up.
It's all local. You just need another machine to do it.
But it makes it so that your CI never actually negatively impacts your development.
This made a huge difference during FIFA's final last year.
And that it got people to stop yelling at me.
Don't stop for backups. This is a really interesting one.
I'm going to touch on sort of how we came across something like this later.
The point is you back up your source control.
You probably need to stop your source control in order to back up your source control.
Which means you have some window where it's not usable.
Again, we go back to a scenario where you're working across the world.
Well, you still have weekends, probably.
Less of a weekend than that full 48-plus hour window, but you've still got something.
But you don't need to stop.
There's lots of technologies and approaches that are available.
For example, if you happen to be using a storage area network at all, and even if you're not using some big fancy device that you license from some global IT outfit, I've got a NAS at home that supports this functionality.
The point is that you set up your machine, you back end it off this NAS stuff, you do a tiny pause to your source control, flush any outgoing transactions, you take that snapshot, which is like an instantaneous copy, it's not really a copy, it's a redirection layer, and you pay based on how much it differs from the snapshot at the time you took it.
But at that point you can turn your source control back on, so you've been down for maybe a couple minutes, maybe not even a noticeable amount of time, but then you can do that back up off that snapshot.
Same thing kind of applies to databases.
If you need to do a backup on a database that's an operational database you're using all the time, why are you stopping that database?
You know, if we were to set up a replicant to do something, take any action that you can so that you can backup off something that isn't your live system.
I do have a side note. Oh, actually I've got a little anecdote here.
Take your backups seriously. I know everybody knows that.
Everybody's been, you know, we've all used a computer before.
We know how backups work. But stuff happens, and it always happens in the most inopportune times. Two Christmases back, we deleted Madden.
Like source control, gone.
We had an IT bug and something went wrong with one of the disks that was back in the Perforce server and some very helpful engineer thought he knew how to solve the problem and so he went in and he remounted the drive to the server.
And in the process, he wiped it. Unrecoverably.
You can imagine that that did not go over well.
So the guys came back from the Christmas break and Perforce wasn't there.
They had to just work with what they had on their local machine.
Eventually, we were able to recover from tape backups, and we were really lucky that that actually worked.
And we were able to rebuild the diffs.
There was a fortunate engineer who, at his desk, was thinking he had just sunk everything right before it got deleted.
So we were able to, you know, recover from the backup, fill in the gaps with that stuff, recreate some of the history, and get going.
But there was a better part of, I think, it was almost a week where the developers...
Just sort of sat there and twiddled their thumbs. So take this stuff seriously.
As a tip, if you happen to have source control in, say, a less potentially stable source of the world, consider those backups additionally.
We have a studio in South Korea. It definitely gets backed up more than some of our other Perforce servers.
While we're on this note, disaster recovery is obvious, and I think it's everybody's natural reaction.
Business continuity might not be something that you've heard of.
So disaster recovery I'll define as recovering from something went wrong, getting the system back exactly the way it was, which you've lost as time.
Business continuity is where you say, I don't want to be back the way I was, I want my time to be the least amount possible.
This would be like if the Apple store down the street burned down.
Disaster recovery is them rebuilding it.
Business continuity is them opening up shop next door so people can keep buying that apple swag.
Business continuity is important because there's a lot of stuff that you do and you rely on that probably can be recovered without actually needing to do all the...
The disaster recovery. So the example we've got for this is our CI systems.
We have about 150 gig, 250 gig worth of data in one of our CI systems that unfortunately flakes out about once a year.
That's five days worth of data. It's about 200 gig.
It's operational data. It's mostly logs. We're all used to that for the builds.
We don't need that data.
If that system goes down, to re-import 200 gig into SQL is going to take all day.
Usually it takes us a day to realize that the database is borked and we need to actually completely recover it.
So we've lost a day for that.
Then we're gonna lose a day to actually rebuild.
And then by the time the system gets online, it's been down for two days.
So then it needs to spend another half a day trying to purge out all the old data because it's not supposed to keep data for more than five days.
We're down for forever.
Eventually, and after, you know, seven or eight times of us suffering from this, we got used to the, you know, hope is our problem, stop hoping.
Immediately something goes wrong. Let's take those project definitions. The CI project definitions are what really matter.
Let's get them into another CI system and get everything rolling again.
And that's kind of the best practice around here is that hope is not your friend when you're in a disaster situation.
But it's where we all tend to go. We'll just give this an hour and see what happens.
It'll probably come back. If you need your system running, CI is a really good example of one where If your developers aren't comfortable working because you built up a ton of process around this thing, what are you going to do to make sure that these guys can keep working?
Okay, so we're going to get to that second problem now, where it works on my machine.
This is the one where CI is really how you solve this problem.
We couldn't come up with any other way around it.
Your machine is the problem.
So you don't get to say it works on your machine, and especially on FIFA.
If you say it works on my machine, the answer is, well, it doesn't work on the build machine, so it's your problem.
You're completely responsible for it. There's no way around it.
We've made it the gold standard.
So let's just take a quick look at what CI looks like at sports so you have a sense of what I'm talking about.
So we've got this content creator. He's working at his local desk.
He's got his feature. He's got this thing finished.
So what he starts to do, he does some local testing.
Now we don't want to burden him with a bunch of effort.
We've got five platforms that we work on, we've got four different configs.
If I ask him to do some, even, you know, nominal matrix of that stuff, he's going to spend a ton of time on this.
If our build times are down to five minutes per platform per config, he could spend a ton of time on this.
So we come up with some very, very, very minimal set that these guys need to do, and we trust them to do that at every iteration, and then check willingly into Perforce.
So that goes to the source control, which sends either there's a set of CI servers which will be polling the source control servers, or potentially they're triggered.
And then we have some set of builds that will kick off.
I'm only going to do the CI process as far as the builds actually go.
So we've got some set of quick builds, some set of complete builds.
When I say quick, we'll get into this more in a minute, but what I'm really going for is something that validates as quickly as possible that you didn't just break everything.
The complete builds are the ones that can take their time.
The quick builds are telling you that you didn't break what we care about.
The complete builds are telling you that you did not break anything.
The analysis builds are things like lint or coverity that are checking out your code quality.
Profiling are typically going to be memory metrics, these kinds of things.
And artifacts are everything else.
These are the ISOs for QA. This is potentially art.
Whenever you're actually generating a thing that some other processor will use, That falls into this type of build.
This was a particularly important one over here, and this bites us constantly.
So hopefully you've seen this XKCD.
And it's funny, we all look at it, and we all know it to be true.
The problem is just because it's true doesn't make it okay.
And so we spend a ton of time, we waste time, allowing people to compile.
And this ties back into my earlier point about the developer at his desk.
You're paying him, or your content creator at their desk, you're paying them to generate something for you.
If they have to spend time compiling, they're not working.
You can fix that with the build system. You can do all sorts of things.
I'm going to assume that you've already optimized your build system to the point where there aren't any easy gains to be had there.
So then it becomes the matrix of things that they can do.
Don't make them do everything.
Pick something that probably works, allow them to check that in, and then use the continuous integration to do everything else.
As a side note for how we come up with that, that minimal set of things we want them to build and test, that's typically going to vary by game team to game team.
It's going to be the technical directors that are going to decide that.
Some titles may favour their final build because they've been having, say, frame rate problems and that's really the one they care about.
Titles that have had 60 fps frame rates for years worry less about that and maybe they'll worry about something with more metrics in it so they can do some profiling.
So these things are highly variable, I don't have an answer for what is the one that makes sense for you.
It'll depend on your game.
Whatever one your developers are most comfortable using is probably the one that you want.
Okay, so now we've said already, well, let's bias for speed.
Let's make sure that you're always doing the fast stuff, and that we're going to do the completeness stuff can happen on the side later.
No big deal.
So that completeness stuff is coverage.
The speed and completeness are the stuff that you can do everywhere in the world.
These builds are typically the reasonably cheap ones.
They're the ones where they're not generating artifacts you need to move around.
So their stuff, in our case, we do CI in both the main development place in Burnaby and the other development in Bucharest. They've got their own CI.
The reason for doing this...
It's convenient for these guys, so we can do it nice and fast there.
The amount of time that it takes to transmit the data back over the ocean and the user experience these guys get getting report data back from us isn't worth it.
We can throw a couple machines in place over there, do that stuff locally.
But that completely breaks down if we start talking about artifacts.
As soon as we need to move files between locations...
Absolutely falls apart and it goes to that earlier point of that's really, really expensive.
So we avoid creating artifacts at the remote locations.
We tend to try and figure out where the artifacts are going and try and make sure that the continuous integration for those things are happening in that location.
So if QA is happening in Burnaby, that's when we're going to make sure that the build that's generating the disk is going to happen.
As I pointed out earlier, it happens in Baton Rouge.
We don't have hardware in Baton Rouge, so we make sure that we do it somewhere else in North America because at least we can shunt the builds over there reasonably quickly.
And again, just to drive this point home, there's no point transferring data that you can build faster.
I called this out earlier, but with our localization guys in Madrid, at some point we discovered that it did take substantially longer to copy that build to those guys, than it did just give them their own source control and let them build it themselves.
We can drive this point into the ground. We'll skip this one. Avoid copying those artifacts around the world.
Okay, don't share environments.
I did a presentation at GDC a couple years back where we were talking about how to scale your test infrastructure, using virtualization and things like that.
This was a principle topic on that.
Basically...
Sharing an environment is going to allow you to waste time.
So in this example, you do a build, and you do a test, and a build, and a test, and a build, and a test, and a build, and a test.
This is really easy to do, right?
Everybody does this because you start off with a build, because that's where you start, and at some point you added unit tests, or an integration test, or a functional test, or you added something, and it made sense to just put it on that build machine. It was already doing that. It already has the artifacts.
But as you scale those tests, you need to realize that builds and tests tend to have that one-to-many relationship.
And so suddenly you don't want to be rebuilding things you've already built in order so that you can do some other type of test on it.
So if you can, split your build environment from your test environment.
If you can get distinct environments, then you're going to get more builds out of that, which has the benefit of being more atomic.
You're getting fewer check-ins per build, assuming that your build is happening on a schedule.
Which makes debugging a lot easier, you're potentially getting more tests and more importantly, you're getting those results to people faster, which is really what this is all about.
If you've done some clever things here, you don't even really need two machines to do this.
It could be a simple matter of making sure that your environment is set up sufficiently well that you can move a build over to some other location, and start executing a test from there, and then kick off a build on that same machine at that same time.
Additionally, if you're really, really keen, that copy I just mentioned where you move it from the build environment to the test environment, there are lots of clever ways that you can completely mitigate that time.
That GDC 2012 presentation I mentioned has tons of detail about that.
So if you're interested, come talk to me afterwards or check that out in the vault.
Do your best to avoid desktop hardware.
This stuff is expensive. It'll always get you.
And it's not expensive in terms of the you go to Best Buy and you buy it. It's nice and cheap.
That's actually why we have so much of it.
We have desktop machines. We have all sorts of...
We have our developers, our content creators using these things.
Typically, you have an abundance of desktop machines.
So it's very common for your automation farm to be built out of the leftovers of that.
Eventually that's going to cost you. The desktop hardware isn't as reliable.
You start getting flaky errors, and that ends up being the real thing.
It's easy if the drive dies, you go buy a new one. When it starts to die, and it isn't quite dead, that's when it starts introducing weird things that start plaguing your results, and now you have this one machine that just does something inconsistent.
For us, it got to the point where I think we had about a farm of a hundred of these things set up in sports so that we could do the various just test automation, not even build automation, just test automation for the couple of titles in Burnaby.
And we had to have one guy who did nothing but just make sure that all these things were actually in a healthy state.
This is assuming that you're using Windows.
If you can get away with other development environments, then you will probably find greater stability.
But if you're developing for Xbox, then you're using Windows, which means that it's reasonably easy to run into problems.
So the solution that we went to for this was, if you have access to a data center, fantastic.
Get into that stuff.
Odds are good they're using server-grade hardware, which is going to have extra reliability on top of it, but ideally virtualization would be the recommendation here.
You don't want to waste your time trying to muck out environment errors. If there's something wonky happening in the machine that you don't see everywhere else, Create a new machine, move the process to there and see if it happens.
If it doesn't, fantastic, delete that old one, debug it if you've got the time, but move on.
You're trying to not have yourself dragged down by all this hardware that you're maintaining.
If you don't have an IT department and data center, don't fret about this stuff.
There's lots of services that you can rent VMs from, assuming that you don't have security or IP issues associated with that.
Desk kits, dev kits fall into the same thing.
I don't know about you guys, but...
PlayStation 4s are giving us some grief at the moment, that's all I'll say about that.
But if you're hitting the same kind of problem, devices like iBootbars really make a big difference there.
An iBootbar is a network addressable power strip.
Send a signal to that, turn that thing off, power cycle it back on.
If you can do a wake-on LAN, you can typically recover your environment.
That's not healthy for the kit, but if the kit has already given you that much grief anyway, who cares?
At least you get it working again.
And the worst possible scenario you can have is that QA didn't get a build in the morning because the automated process relied on a kit that happened to die on the test right before the important one that you needed it to do.
As an additional pro tip here, we've had great luck using PBRs attached to consoles for additional information or even just remote viewing these things.
If you're doing Gen 4 development, there's actually some neat stuff that I know we have working on the Xbox and I think we can get going with the PlayStation as well, where it's already capturing this type of video output.
We can throw it into a buffer on the side and in the event of a crash, we can just dump that and get the last couple minutes of video.
I can't recommend that highly enough.
This is one of my favorites.
So test your processes from within.
So this is a canary in a coal mine.
If you're familiar with the analogy, or if you're not familiar with the analogy, basically the idea was at some point in the past, and I really hope this isn't an old wives tale, they had canaries in coal mines, and if something went wrong and they released, I'm assuming gas, the canary would pass out, or probably die, and everybody knew to hightail it out of the mine.
We apply the same idea to our continuous integration.
One of the problems, it's really easy to erode trust with your developers.
In our environment, as I said, they check in when they've done some set of things, we take care of the rest of it for them.
If we aren't taking care of it in a very successful way, or reliable way, then we can't claim that that CI is the gold standard.
And suddenly that it works on my machine problem creeps back in.
So what we built when we called the system Canary, it's basically a fake build.
It'll never fail. Well, it fails all the time, but it's not supposed to fail.
It doesn't do a real build. It does a process, which is always supposed to pass, but it exercises almost all of the dependencies that a real build would actually exercise.
So you actually get, it's really interesting because you can map out what are all the dead spots in your network infrastructure.
When exactly does your Perforce server go offline?
When does your antivirus kick in?
Because if you have an IT department and antivirus, you'd probably be surprised how badly that's plaguing your systems.
All sorts of stuff like that.
So you can absolutely map out those windows and you use that data, you get that back in front of your...
Your developers, so that they can trust the system.
And you can show that 99% of the time these canaries are alive and sending chirps.
Fantastic.
But even for yourself, or whoever your build guys are, if only 80% of the time it's working, then you know and you can adjust accordingly.
So I have many conversations with one of our guys in Bioware, who's a big fan of, what's his motto?
AAA tools for AAA teams.
This is often not common on central teams.
We build tools, and it's not that we make them ugly on purpose.
But we don't have artists, and we don't have budgets for artists, and so we make tools that are functional.
And darn it, they work for us.
I don't understand why that guy can't use it.
I don't understand.
You really need to make compelling reports.
You want to get this in front of people.
You need people looking at this data.
Make it something they actually want to look at.
And as a final pro tip, don't report off of live systems.
If anybody is using Tableau, I don't need any hands here I suppose, we actually found a, not destroyed, but took out a couple of our test databases.
The way that it would generate data, especially if you've got it set in a polling mode, just didn't play nicely to our live systems.
And so we actually found that we were dropping our automation testbed systems.
It's a simple fix, build a replicated database and point Tableau at that, but it's really easy to do wrong and it's really easy to not realize that that's what's happening.
So that third problem, this is this branch health.
Like I said, we've already touched on this a lot with many of these other ones, but there's some additional insights that we've got that just help you make sure that you're not putting your branch into a state where it's broken or unusable by other people that depend on it.
A poll of hands, who loves their build system?
Who thinks their build system is great and they would recommend it?
Okay.
You I want to talk to after, please.
Yeah, that's kind of what I figured, right?
Nobody likes their build system.
This guy does, but nobody else likes their build system.
And it's not because we do a terrible job of developing these things.
Maybe.
But generally, it's because they're full of trade-offs.
It's the nature of these things.
They're hiding all that dirty laundry that we've done.
So they have to be nasty like that.
So how we get around this is we don't make our users actually use the build system.
Now, if they're changing files such that they need to add things, that's inevitable.
But for the most part, in order for it to work with automation, we have command line interfaces.
There's nothing worse than telling your artist, OK, in order for you to build your art, I need you to type in this 256-character command line to make that happen.
So, bundle up behind a GUI to make sure that your art guys, your level designers, your producers, your execs, if they're going to generate a build from source, make it easy for them to do that.
And just because your developers are power users and are comfortable on the command line, don't make them do that either.
If they want to use the GUI, great.
But the other tool that we built was sort of a fast version of the command line.
The automation uses the 256 character version.
The other guys just sort of type build.
And based on what they usually do, it figures out what they need to do.
Ton of time saving, and everybody loves the build system.
Telemetry is your friend.
Build system, again, same thing.
Track how it's being used.
One of the reasons people tend to hate build systems, or at least we did, is because it rotted.
And it rotted because there were workflows that were being supported that may or may not be getting used.
The build system, or having telemetry on that, allows you to identify what's getting used, what's not getting used.
And it allows you to go to that one senior guy who maintains that everybody uses a flow, to go to him and say, no, it's just you.
It's just you.
I need to kill this now.
IT is your friend. I mean this one's not too difficult. We touched on this before.
Things like virtualization, SAN, snapshots. If these aren't things that you've heard of, your IT guys definitely have. Go talk to those guys. They're going to be able to help you do stuff.
They've probably forgotten more about your network and your infrastructure than your devs will probably ever learn about that stuff.
If they're not looped into making your processes faster, you're missing out.
Validate before commit.
This is this idea of a gauntlet.
Basically, this isn't what we're doing.
I already told you guys that we just check directly in and we let the automation take care of it.
But this is what we should recommend and this is what hopefully you guys are all doing.
Have some battery of tests to get run on this stuff before it goes into source control.
And then the other one is no idling. If you can avoid letting your system actually sit idle, that's the best possible case.
You're going to have these fast builds that are running all the time. Their goal is to provide a result as quickly as they can.
Don't stop those builds, but take those coverage builds, those other builds that are slow, repurpose them.
Get them to do something else. Do anything but sit there doing nothing.
All right, so that just about wraps it.
So the key takeaways from this, think of your commits as a flow and keep them flowing.
Don't fall ever for the it works on my machine.
Gold standard's gotta be your automation servers.
And recognize that branch health is your team health.
Oops.
Thank you.
Any questions?
Hi, I was wondering, do you have a per team build system or is there a centralised EA one that covers all the titles?
That's a really good question.
Yes, yes and no.
So there is a standard build system tech stack that probably everybody uses, except for the Frostbite guys.
Hopefully none of them are in the room.
But they tend to roll their own just about everything.
Although I think they may have even merged now too.
How everybody uses it tends to vary.
And the CI system they build on top of that can be potentially different.
But they do all start from the same root tech stack.
Thank you.
All right, thank you all.
