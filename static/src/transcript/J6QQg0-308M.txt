Well, I think we should get started. So hello, everyone. First I would like to ask you to silence your phones because we have some music to play here. And also if you have questions at the end, then I ask you to walk up to the microphone in the middle and make a line if there are many that have questions.
So thank you for being here. And thank you to GTC for accepting us here with this talk about intelligent music. My name is Paltur Paltursson and I work as an audio director at GCP games, mainly working on EVE Online. In case you don't know what EVE Online is, it's a sandbox MMO.
with some 20,000 to 60,000 players playing at one time on one server. And with me here is Dr. Kerstin Oleson, composer and professor who has been working on procedural music for many years, more than 20 years. And we're going to be talking about something we like to call intelligent music.
Now let's find out what we mean by this. So first I want to do a short introduction on EVE Online and why we seek better ways of making music in this game. Then my colleague Dr. Kertan will introduce his system. He calls it Calmus.
And finally we'll do some demonstrations of the system works together with the game. Also before I start, before we start, I think it's important to stress the fact that this is very much a work in progress, so please bear that in mind.
We start with the EVE online. So the music has been part of the game from the beginning, obviously, like for most games there is music. And from 2003 to 2009, we had a thing in the game called the jukebox, which was really just a playlist that played a number of old tracks specifically made for the game.
is actually we still use this music and we have added some and we still play old tracks for part of the game. This is really, really simple. It worked well, especially since the game was very, very atmospheric and it worked really well just to have some music playing that was very atmospheric and didn't change very much.
So before 2009 we had bunch of solutions handling the audio in the game. Most of it was home grown. So proprietary solutions handling audio. This was obviously very expensive and time consuming and in 2009 we implemented twice which changed a lot. And for music we were able to add precomposed elements of music. We can stack up in every way we want it.
and playing any number of ways you want it and so on. So it was great to have Wwise as part of the whole system. So in the game, there are many different systems, lack of a better word. So we have empire space, which is basically pretty safe for players. Players start out there and they can go out and they can mine and do whatever they do when they start the game. It's pretty safe. The police is there.
You're not attacked. The music can be really easy. And you can play old tracks like we have done. And the system, the new Eden, as we call, the space in Eve, it's called New Eden.
It has more than 7,000 solar systems with I don't know how many moons and planets, just countless almost.
And most often very safe to start in Empire Space, but if you go down to low sec and down to zero sec in wormhole space, it changes a lot.
So.
Usually it's not recommended for players to wander around and go to these places like Losac and Zero in the beginning at least. So they stay in Empire Space a lot.
So the music in the game is based on these systems.
So in Empire we have it easy going. In Low Sack we have it similar to Empire Space, but it has some more difficult intervals in the music maybe. And, you know, the timbre is different in the instruments and so on. In Zero Sack we have vertical really almost sound mass music.
which plays in any way we want. We can stack up whatever we want there and it just plays sound mass kind of thing with classical instruments. And in wormhole space it's similar, it's even more sinister. So it gets worse as it gets lower. So this is fine. I mean, this works up to a point.
It sets the stage in most cases unless some players decide to change things because it's a sandbox. We don't know what's going to happen. We haven't set the story. So the players are really the ones that change the things there.
So we don't know what they're going to do. We have no idea. So, for example, in empire space, you think it's safe. Sometimes we have kamikaze pilots fly into you and you're dead.
Or some corporation declares war on you and you're just trying to mind and mind your own business. So that should change the music as well. But we still have the same kind of music there. It's easygoing and so on.
And likewise in LOSAC or CERO, if there's a big corporation there that has a station there and a lot of players, it's not very difficult for them to stay there. So maybe the music is too much in that case for them to be like it is.
So let's hear what these sound like just so you get an idea.
This is Empire Space, where everything is fine and dandy.
You can go around and mine, fly around, talk to your friends, have fun, and so on.
All the music there is like easy going.
And very nice.
You can listen to it for hours.
It's nice.
Then Losec.
Similar but a little more sinister, maybe.
It gives an idea of, well, maybe I shouldn't be here.
And it's...
Yeah, it's not very frightening, but a little bit, at least.
So it tells you something.
Then if you go to zero, it's like this. It's like clusters of things that you hear that are stacked up randomly in wires.
It's a very nice feature. You can do it any number of ways randomly. And you shouldn't hear much of repeated segments, but It still comes up, but it works really well.
Up to a certain point, of course.
And then we have wormhole space, which is, if you go there and you hear this, you probably are just frightened. You want to go back.
So that's what it tells you.
You're not flying around and having fun. It's like you shouldn't be alone there.
So we basically want an intelligent music system.
Because we can't, or maybe shouldn't.
really precompose all of the elements in a game that ‑‑ or a story that hasn't taken place? Is it really fair to compose everything in a game that doesn't have a story that hasn't happened? I think we should find ways of making this better.
Also hearing the same piece of music over and over again diminishes, causes diminishing aesthetic return and sometimes just makes us ignore it completely.
So even if we had a budget of composing thousands of minutes of music, which is like a million dollars or something, or two million, I don't know.
It will still happen. We will still hear the same thing over and over if we have a game that plays for more than ten years. That's just going to happen. We will end up with the repeated segments because of that. So that's why we want an intelligent music system. We want it to be able to create musical material by itself.
So with some help from us, of course, as composers, but it should be able to create the material according to the atmosphere of the game, though. It should evolve according to the actual progression in the game in real time. It should be possible to control various musical parameters in music simultaneously.
This is our wish list that we made. This is what we want.
So the first steps in this direction we took in 2009. This wasn't like, ah, we have to have this tomorrow. We have to have this now. That's not going to happen. And we knew that it was not going to happen. So it was a side project. We started looking at what others had done, such as EA with Spore and some other nice projects.
some really interesting things. We took a look at Max MSP and PT, if we could use that.
But we want to have a system that can create real music, not just atmosphere. So it made it a little bit more complicated. We started researching with Dr. Kertan Olavsson He was a professor at the Atlantic academy of the arts at the time to find better ways of playing music in a sandbox game.
We started at the beginnings of music with the basics of music.
Intervals, tempo, rhythm, harmony, et cetera.
And then on to methods.
systems such as probability distributions and other methods in order to realize and identify what kind of a system could work. Then we took a look at Kertan's own system, which he called Kalmus, that system he had been working on for more than 20 years, which he wrote.
to be able to make contemporary orchestral and instrumental music for himself, really.
And now I'm going to hand the mic over to Kertan, and he's going to tell you a little bit about KalMes.
Hello, everyone.
I'm going to speak a little bit about this system and why this system became a composing system.
When I was studying at the Sibelius Academy in Finland in the 80s, we got the opportunity to work with both professional composers and programmers.
And at that time it was possible for normal people to learn how to program with personal computers.
And since in music composition and mathematics has been used for at least a thousand years, as we know, but most of it was from the last century, we decided to construct a program as my thesis in my doctoral studies there.
And first of all, the idea was to compose, to make a composing program that could use musical elements and materials, as we are doing with pens and papers.
And use these new tools to make experiments with music, try to find new ways to compose music.
And in the end...
We had this program that could compose music for orchestra, for chamber music, electronic music.
And now today, we are focusing on real time.
composing, which is my interest, actually.
But when the CCP people came to me and we started to talk about this, it was incredible that we were thinking about the same things.
A composition for an orchestra is sometimes called three-level composition.
In computer games, music, it's also three levels.
Because this system is working with artificial intelligence.
it was obvious that this could be used in a game, because artificial intelligence, which I will talk about a little later more, is something that can handle new situation with new preferences every time, every step in the process.
So a composition with Kalmas is based on musical object, and We are talking about melodies, harmony, and development, polyphony, and so on, which is a common concept in composition without computers as well.
The parameters we are working with are themes, which we create melodies from, harmony that will decide what kind of connections are between the melodies, scales, which we know mostly today we are using major and minor, texture, polyphony, et cetera.
And the musical material is quite traditional, cells which creates the melody.
and harmony that will connect them together.
And polyphony structures, how they actually are talking to each other, the melodies.
The algorithms we are using, they take care of putting together tones for harmony and for melodies.
And we are using, in a very traditional way, different scales.
And the scales in our time, they are reflecting a different geographical position, like we have Japanese scales, European scales, and so on.
And the artificial intelligence system makes it possible for the system to go on, to compose music by itself.
But as we are doing it, the composer using the system is defining the framework, creating the material, and deciding what kind of color or texture it will contain.
So, one simple example about artificial intelligence, when creating a new pitch in melody construction, And as you can see on this picture, there are graphic annotation, there are four notes that are called last pitches plus a test pitch.
And these pitches, they are compared to the input cell from the composer.
And the program is deciding what are the relations between this new pitch in this new melodic line according to the actual input cell from the composer.
And what happens when it finally is accepted, after having been compared to what kind of scale, what kind of interval are preferred, then the new pitch is accepted, and there's a new situation in the middle construction.
And that means that the program has to continue to make another selection according to this new situation.
And this is called artificial intelligence in music creation.
The same things or quite similar things are happening in the computer game, like sandbox as they are working with, working on.
It is something happens in the game and a new situation have occurred.
And then the program like this can actually decide, we have to change the musical texture and prepare for another development of the music.
And to do this, we made several project just to connect this computer composing system to the actual game.
And the first one is called Kalmas Waves, which is we are using dances to control the program.
And this Kalmas program can be controlled.
Each parameter can be changed in real time, which means that we can take music from the 20th century, change the harmony and the polyphonic structure, and we can go back in time to the 15th century by doing this without changing the material.
And to do this we created, constructed a census connected to the dances and the dances are sending messages to the program that will compose according to their movements.
This is what we can use in the computer games, taking messages from the games and use the message to compose music according to what is happening.
The second thing we did is called Kalmus Play, which is kind of an app for fun to create very simple music. And what this is doing, it's this simple app is communicating with the program, a special version of the program, which is on the server in Europe, in Iceland, And this small app can compose music.
So if I press this button, it will send a simple message to the server and compose the music and send it back.
This is composed in Iceland.
Very simple.
Very primitive.
This is how we are just trying to communicate with the server, just through TCP, through the satellites and telephone system.
And then a small example from the actual use of the program.
With this program I have composed and other composers different kinds of music.
And one of the music is called Viola Concerto, which is for a full orchestra and a solo viola.
And I just play a small example from this piece.
This is the end of the piece.
It's a live recording from the Icelandic Symphony Orchestra.
And this is a screen shot of the actual program.
And...
I'm trying to change now, so I'll go into the program and show you a little bit about how it's possible to use individual parameters. So I have to leave this. So yeah, it comes here. So this blue box is actually a musical object, and it's containing a couple of melodies.
and it can be played.
Very simple things from it.
And it's possible to enter the material from the composers through MIDI files to play directly into the program and to export as well as the MIDI files.
But this interface is made in Xcode.
It's communicating with the actual program, the same program as is used for CalMOS Play and for this computer game.
It's only communicating with the numbers.
So when I press the button Play, it sends the number 50 to the actual brain, which starts to play.
And all these parameters we can see here.
a very traditional deciding what kind of intervals we are using in the harmony, what kind of polyphony we are using, and polyphony is kind of if it's canon or if everybody is playing together and this is very traditional for a composer to work with. And then we can decide in the melody.
what kind of scales we are using.
And whole tone scale is very well known from Debussy and other composers.
And here, it can be changed, the scales.
We can do this one.
When it's red, it's composing.
It's within a second, it has changed.
Now we have a different situation.
And of course, all the instrument can be selected here.
Or any electronic.
instruments. And this is the editor for the material that can be used in the computer game for the composers. The game can then afterwards use this material and use it in the game. And that's about it from me.
So, Baldur, you will continue. Thank you.
We also need to know the game that we are working with. We have to know what to look for in the game. Since we don't have a story, we have to find other ways, other means of creating music. So we don't have a single character or story to build the music on, but we have other things. In EVE we have different races of people. We have different environments.
These are things that we can use. We have wormholes. We have cosmic signatures and other anomalies. And we have other players as well as some NPC players as well. But there are no scripted stories at all. So we have to work with these things.
territories. So each when you start out, each race has a certain territory that they start in. So four territories that you get to choose from, so instead of connecting to a single character, which is usually pretty straightforward for a composer to connect to a type of a character and compose some music for it.
we connect to the whole race. So we take the character and connect this character to the whole race. And we connect this, what we created, to this whole territory in the space. So it works in a similar way. So we have four basic races that we get to choose from when we start.
In order to understand how the music can be defined for the races, it's vital to really understand and study each race on its own. We have Amar, Galente, Kaldari and Minmatar.
These are what the races are called. And all of these have a different kind of back story or history which is written.
doesn't really mean a lot, but it's there. And we can use it as composers. And as well, they look different, so we can use that as well.
And as you see, we've found some ways of defining the characters Amar, Noble, Regala, a little bit evil, they were slaveholders once. Galente, smooth and curvy, they're French.
Caldari are hard and edgy, cold, very technical. And Minmet are rusty and a bit dirty, you know, rock and roll, almost. And we also have some NPC races, Jove, they're mysterious, and they're hugely advanced to any other race. We don't really know what they can do. They They're just like gods, almost. They're there. We can use them to create some music for situations or have our system create music for situations with them in it. And we have some pirate factions as well. Many different pirate factions, actually. So we can actually have many different kinds of things for the pirate factions because they are very different. They also have a back story.
There's a lot of material to be able to base the music on, so to speak.
So six to ten races that we can define for music and we can start creating something we would like to call a character cell for each race.
And for the game we just make a specific library.
Let's look at the first one. It's regular noble, a bit evil perhaps. Some would disagree, but I don't know. I feel they're pretty bad. The principal melodies, I mean, this could be anything. It's up to us to define it as composers. So thirds and sixths scale Dorian with the raised sixth on the minor scale.
around C2, the pitch registry, and the speed around Adagio or something like that. And polyphonic structure is like this or it could be canon, horizontal, vertical distribution.
And the instrument texture also is important.
So we haven't really made ‑‑ set any of this in stone. We took a look at all of them and it also really matters how they sound. It's not enough to think this, well, yes, this guy is really sinister. It's a minor second, definitely. Well, it has to work in the game with other things as well. So we have to try it out. So we're in the process of doing this now.
And, yeah, the instrument texture mixed, meaning classical and some electronic instruments together. And now we have to find things in the game that can change things in the music.
So what can, for example, interval changes in harmony can be changed by various tensions in the game.
I mean, Kalmus can change, do interval changes in the harmony depending on various tensions in the game in real time.
This is important.
Also type of polyphony, complexity and conflicts, this could be anything again.
We are just in the process of defining it, so it's really exciting to be able to do that.
Changing tempo is tension as well, emerging threats, more tempo, there's a threat coming, there's more tempo. It's really straightforward if you think about it. It's similar to if you just compose with a story.
So we have an example. I'm going to play an example that lasts for, I think, six minutes.
But it's nice. I put some explanations and I'll talk over it maybe. It explains how the music changes while you play. And also let you hear how the program composes in real time. It's a video.
It was supposed to be plan B because we weren't sure if we were going to be able to play live here, but it sounds or looks like we are able to play live. So we're just going to play both. I'm going to do plan B first and then we're going to listen to plan A in the end.
So we'll have it realtime over the server. So this is AMAR again that we defined. You will hear the definition I made.
Melodies, thirds and sixths. I have for ‑‑ what Kalmas can do, it can forbid some intervals.
So I have forbidden intervals just for normal minor seconds and major sevenths, which is basically the same thing. But the Dorian and the Daccio and the mixed instrumental selection.
Then a danger occurs and the interval changes, change really, because it allows every interval to be played.
and it gets a little bit lower and the instrument gets a little bit changed as well. But the same melody again. Then we'll jump into another system and there you will hear a different thing. But still being composed in realtime. So let's hear what this sounds like.
This is all single-player, so there's not much happening. I will find some pirates attack me, but that's about it So there we have the intervals, there are no intervals spanned any longer and the instrumentation is changed.
So now it finishes what it is playing, it finishes the segment and starts the next one when it warps out.
But it finishes this one first.
It doesn't happen right away.
Could let it happen right away, but that's not how it happens right now.
back again to where we started.
So now we're in the Galente system.
And we are hearing a different thing altogether.
Of course, everything is done in realtime, so there's no editing done here.
So we're done with that.
Now we're going to play some realtime Plan A.
So this is also on a local server, so just one player, maybe with some NPC pirates attacking or something. But this is in real time now. So over VPN. So maybe a little bit of disturbance, I don't know. So again, Kalmus is written in Lisp.
It runs in ECL, which is an open source interpreter for Common Lisp.
And we worked with Audio Kinetic to open its API, so we could send MIDI events into the sound engine.
It was really important.
But now it's open for everyone.
They're just including it in their Wwise thing.
So CalMES feeds the MIDI events into Wwise, which hosts the instruments.
We have the sampler and Wwise play the instrument there.
The music in here, I mean, it's probably, would probably have some percussion and stuff like that there, but it's all work in progress.
But it's all in real time and the potential, you can hear the potential is there and it changes while you play the game. It evolves.
There's some traffic.
It's a local server.
But there it is.
So there we are. I don't think we have to ‑‑ if you want to hear this, take a better look at this. We'll be at the Autokinetic booth later today after this. We'll set this up and demo it.
You can listen for your own with headphones maybe. But for now I think that's it. Yeah.
So in summary, for Sandbox MMO where the story hasn't happened, we'd greatly prefer non‑ precomposed music. Some kind of a system is needed, we think. One such system is Kalmus, obviously. But no system can work without first really knowing what you're dealing with, knowing the game. You have to know the game that you're dealing with, what to look for.
And currently the system runs outside twice, but Ideally we're going to have a plug-in so we can use it with other games that we're making. So thank you. And some questions.
Hi, thank you so much. That was fascinating to hear an explanation about and to watch in action. Thank you so much. I have two quick questions. First of all, with the percussion that you mentioned just briefly, maybe you could talk some about your vision for how that could play out with the different territories or especially with the different races within the game.
I think with percussion, you're talking about percussion? Yes. I think mainly with when there is danger and you're attacked, that's what we're starting with. We're not going to tie it to a race to start with at least. So like I said, it's a game, it's an in‑progress thing. We're still trying out things. And to start with, I think we're going to tie percussion with when you're attacked and stuff like that. Yeah.
My second question is about forbidden intervals. I was maybe a little bit confused and I was wondering does that work horizontally or vertically?
Yeah. It can work for harmony. So what I was showing you was for the harmony. So it was not the melody. So you can forbid intervals in melody as well. But it didn't do that.
So this was all for the harmony.
Thank you so much.
Yeah.
Hi, you're right.
That sounds like a fantastic system, so great work on that.
I've got two very quick questions.
It sounds really fantastic to be able to have this great, sprawling soundscape that never repeats and kind of generates itself.
on its own but as an audio designer and a composer sometimes there can be a really great value in having a repetition of some sounds in terms of feeding back to the player when they've done something in the right way or feeding back to the player. This is absolute game over situation or this is you're absolutely fine and for something that shifts so much.
and can be interpreted until it's settled on what it is.
Do you think there's still space on a project like your own for a composer to still have a kind of finite, this is what we require for certain times of the game, or are you intending on having the whole thing to be open and free?
Yeah, so I totally understand your question.
I was hoping for this question, actually.
Okay.
So I think the system can be used for, as a hybrid system as well.
as you can tell it to play a certain thing and then you can compose your thing in the same scale and have it play over it. Also about repetition, we can have it repeat, but it will always change it a tiny bit. So you can tell it there's a certain slider there that you can move which is called ratio of truth. So if you move it to 100 percent it will play what you ask it.
So if you have it like 50 percent it will kind of play what you asked. So it will play something else as well.
So we plan to be able to use it, you know, if we want to use it as a hybrid system, you should be able to.
Yes.
Definitely.
Fantastic.
One last little question, just something I was wondering.
Sometimes when I'm using MIDI instruments, I find that obviously they have their limitations.
It's like a violin, it can only go so high and so low, and what have you?
Have you had any problems with that, with the MIDI instruments, where it's tried to compose something outside of the range of the actual instrumentation?
So maybe, Kertan, you can maybe answer that, that's really, the system really knows how to deal with this.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah, actually the program is made for traditional instruments and it has 16 MIDI channels and the MIDI channels are assigned to the traditional instrument of an orchestra, I mean symphony orchestra.
So number 1 is the flute and number 16 is the double bass.
So there is kind of sections in the classical symphony.
It's one to four is the woodwinds, and five to eight is the brass, and then there's a percussion, and the last five are the strings.
Two violins, viola cello and double bass.
But the thing that we are doing in this is to construct sounds that are that are similar to, for example, for a flute.
And the program can take care of the woodwinds and take care of the harmonies.
A different harmony that you use on the woodwinds than on the lower register instruments.
So that's the thing that is, we are creating artificial, well not artificial, but electronic symphonic orchestra so we can apply all the functions were actually made for the traditional symphonic orchestra.
on electronic sounds as they are creating in CCP.
So it's taking care of all the ranges for the instruments and playing with the range of each instrument together.
Awesome, great job guys.
Hi there. I was wondering, so it's obviously very early, what you're working on, but are you planning on expanding the palette of instruments that you're working on? I know you just mentioned that you're using all just like a regular orchestra and stuff like that. Have you run into any problems with the program itself, like as far as mixing like the instruments together or having like playback issues with that and having it meshed with the game audio itself when it's playing back?
Yeah, I don't think we're there quite yet.
I mean, we are playing around with instruments.
We're creating instruments ourselves and hearing what works.
I mean, we've had some problems, but we haven't really, we're not really there to tell you that this is how we fix this and this is how we fix that.
No, we're not quite right there.
But as Kertan stated before.
If you are a composer and you know how to use the system, I think you kind of need to be a composer if you want to use the system.
Not just anyone can just step in and just use it.
It's just ‑‑ it's not ‑‑ maybe Kerstin disagrees, but I think ‑‑ I can add to it.
But yeah, he wants to add to it.
But yeah, we're not quite there, but, you know, the instruments know where they should be.
But yeah, Kerstin wants to ‑‑ I can maybe add to this, all the functions in the system are made for musical purpose.
It's like controlling harmony, controlling the tension and color and melodic development.
And this is very important, even uses are not.
educated on the highest level in composition.
If you have good ears and you have a good sense for music, you can learn from the system because it's always react to what you're doing.
So it's a kind of new way of composing music.
You are dealing and communicating with a composing system and experiment with this.
Thank you.
Yes, and I also think that it's a great way to, at least for me, to get ideas.
To put something in and it spits something out in like a second or something and you're like amazed.
I didn't think of that.
That's a great idea.
That's one thing about the system that I love.
Yes.
Hello, thank you for the talk.
I was interested to know a bit more about the technical side of how you used Wwise as a sampler because sample symphonic instruments are quite complicated, right?
You have different articulations and different timbres depending on, for example, the...
the intensity of the breath, and then you should have different samples to avoid repetition.
So I guess this is a lot of work, so I wanted to know how deep did you go into that sampling aspect in WISE?
We did not go deep.
It's very short and sweet.
We just took some samples and we did not go deep.
We are thinking about this.
Okay.
And we are going to probably start a project just about this, because it's really important.
I, myself, think that it's almost impossible to play a classical instrument really fast by a machine.
It's, at least, it's very, it's not, it doesn't sound right to my ears, at least.
So I want to really work on that.
And we, no, we haven't really worked on it.
Maybe we'll work with someone that is already doing that.
We don't know yet. Like I said, it's all a work in progress.
These were the samples that we put in.
But remarkably, you know, I think it sounded remarkably well just using the basic sampler in Wwise and just playback.
Okay, and if you want to go deeper into that, would you need to redefine a bit the algorithm to integrate more parameters, like for example, dynamics and stuff like that?
Or do you think it already has everything to handle that kind of deeper sampling technology?
You mean the samples?
Yeah, I mean if you really go deeper into the, like having more samples and more parameters, we do need to rework a bit on the algorithm to integrate those new parameters.
Yeah, so we're not there yet.
Okay, yeah.
Yeah, we're not there, definitely not.
Okay, okay.
So, like I said, that's material for a whole new project, I think.
Okay.
Because it's so important to have them sound right.
Okay, thank you so much.
Yeah.
Hi, thank you.
So what's the plan for the evolution of different classical styles?
So you gave a really great example, and this is working really well for Eve.
And I'm wondering how deep do you think the technology can go in the future for, can you make it sound Baroque, or can you make it sound like John Williams, or how far do you think we can go with this AI composition?
So if I start maybe, maybe Kerstin can continue.
When we started with this, it was able to play back for us what we put in, it played back really complicated 20th century music.
That's not really what we're looking for, at least in a ‑‑ mostly we're looking for something more simple.
So we're quite, I think we have quite made a lot of success making it, make, create simpler passages of music.
As far as you're talking about maybe it can compose John Williams or something like that.
I don't see why not.
If we create the right cells for it and the input has to be right.
and it has to be, has the right tempo and so on, then I don't see why not. Maybe Kertan wants to add to it.
Yeah, maybe the methods and the algorithms in this program, they can handle different type of music and you cannot do our kind of back fugue in this or you cannot, you cannot it's not meant to imitate styles, but you can certainly get the color of the styles, like you can control the harmony, you can, as very consonant based on triads, using melodic cells that are very connected to the classical periods in the music history.
So you can get kind of color of it, but it's not meant to imitate.
And I can tell you something because I've used this for teaching contemporary music at the academy for many, many years.
And obviously the students, they can, after some time, they can actually control the system in the personal way, which is very important.
And I was once very surprised when one of the students could actually make a minimalistic music, which I didn't think was possible.
adjusting the parameters and knowing the symbol and the character of the music, you can get there.
Nick.
Hi, thank you for the presentation.
I'm a data scientist so I was pretty fascinated about the AI aspect of this.
So I was wondering if you guys ever consider using player input to train the system?
For example, if you play something during a combat and you have a button to click, if this is, if the player thinks it matches the mood, it actually feeds back to the system of the AI and train the system that way. Have you guys thought about it before?
No, we have not. But I don't see why we shouldn't be able to do something like that.
We're not there yet, but I don't see why not.
I don't see why the, so we create a cell of maybe eight notes for Kalmers to compose music to, or it reaches eight notes and creates music.
I don't see why not the system or the game would be able to create these eight notes somehow.
I don't see why not, but we haven't tried it.
CARTA wants to say something.
Yeah, very short, the system is listening to the user.
So whatever the user is giving to the system, the system will say, this is the perfect thing to work with.
But the artificial intelligence is used in that way.
It will use the material you put in and continue to compose with it.
For the memory, for the learning, we are using kind of a library, which is a memory.
And the system can save it and call back these memories.
And this is mainly for the composers to create the personal style with the library, as we all have somewhere in our minds.
And yeah, this is...
The functions in the system can actually, if we put it on and let it compose, it can continue forever.
It can select from the library by itself, but also the composers can come in and change it in real time if needed.
Okay, thank you.
So thank you all very much for coming.
It's great to see you here.
Thank you.
