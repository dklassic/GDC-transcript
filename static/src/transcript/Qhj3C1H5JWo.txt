for coming on a Friday, last day of GDC.
My name is Mike Amender.
I'm an experimental psychologist at Valve.
I'll be speaking to you guys today about brain-computer interfaces and why I think there might be one possible future for how we play.
So what does an experimental psychologist do at Valve?
Basically this.
So, maybe not.
So I'm not on the clinical side.
I'm on the research side of things.
I show this picture because it gets a laugh, makes people happy at the start of a presentation.
But I've been giving it, I use this picture at the start of basically every talk I've given for the last five or six years.
And it reliably does get a laugh.
And so I want you to think about that for a moment.
What kinds of things could we do as gameplay designers if we could elicit reliable reactions?
Or if we could determine we were eliciting reliable reactions to various things our games were doing?
Think about the notion of reliably knowing that somebody is experiencing a particular sensation and how that might change how you design games and play games.
So this is the question that motivated the talk.
How would you interact with technology in 10 years?
What is the future of computing?
What is the future of gaming?
What comes to mind when you think about consuming entertainment in the future?
Is it mobile?
Is it VR?
Is it AR?
Is it screens everywhere?
Is it something like Minority Report?
Now, you know, we're in the vision track.
This is, we're supposed to be speculative, to be forward-thinking.
And so what I'm about to cover is one possible direction that things could go.
So here's a quick overview of the talk.
We'll go quickly through current generation interfaces, and then I'll talk about three ways that I think BCIs could significantly impact us as game designers.
We'll talk about ways to improve playtesting.
We'll talk about adaptive gameplay.
And then we'll talk about what I guess I'm referring to as true BCI, but actually replacing current interfaces with interfaces that interpret your thoughts and translate them into something useful.
And then we'll end with a discussion of the challenges, the roadblocks, the things that might get in the way.
So let's define BCIs.
This is my definition, but you'll see similar definitions kind of wherever you look.
But basically, it's a communication pathway that transmits neuronal signals, so things that are happening up here, into actionable input for some external system.
Could be a game, could be a screen, could be whatever.
I also want to make the broader point that while I'm going to describe, or most of the talk will be about measuring neurological activity, truly what I'm talking about is acquiring physiological data from a player, right, and asking the question, is what changes when we can incorporate information about a player's internal state, what they're feeling, what they're sensing, what they're experiencing, what they're thinking, and incorporate that into our game designs.
So again, while we're going to focus on information acquired from the brain, there are other modalities and methodologies that will give us knowledge about a player's internal state, and we're going to cover them briefly.
Another point to keep in mind is that in one respect, every interface is a brain-computer interface.
We're always translating our thoughts into a system response.
It's just mediated by a piece of hardware.
So what differs is the medium through how we process the information that is an intent and turn it into something usable.
And so we might be able to accrue a lot of the potential benefits I'm going to describe today by not measuring brain signals directly.
And what truly matters is having a reliable way of recording information about a player's cognition or emotion or experience.
And so I'll describe other ways we could do this briefly, but really we're going to focus about brain-computer interface stuff today because I think in the long run, this will give us the most bang for our buck.
So I'd like to kind of walk you guys through an analogy.
If you think about having a conversation with somebody, there are both verbal and non-verbal parts of the conversation, right?
The verbal, the words that you say, and the non-verbal, right?
Like the change in somebody's tone of voice, right?
If they're facing you or not.
Their facial expressions, right?
And when you get verbal and non-verbal information, you put them together, you get a complete conversation.
And so I guess the point I would like to make is that right now with games, we have traditional inputs.
But we might be missing the nonverbal part of the conversation, right?
There might be other data that the player could be providing to us as game designers that we're not acquiring.
And so what happens when we add in physiological data, right?
Is it possible that we could make new kinds of games?
We're going to spend the next few minutes trying to convince you guys that yes, it is.
And I think this will be a useful analogy, a useful framework to keep in mind for the rest of the talk.
So let's talk about current generation interfaces.
You know, broadly speaking, you're transforming some intent.
I want to jump, I want to aim to the left, I want to move forward, I want to reload.
Can you combine it with current generation control, a mouse, a keyboard, and so forth.
And it turns into an onscreen action.
And so let's briefly cover what we can do with current generation technology.
All right, mouse and keyboard, this is actually my.
my keyboard, my mouse.
With a mouse you get a one-to-one mapping of hand and wrist location to a location on screen.
You can shift directions.
You have button presses as well.
Keyboards transform keyboard presses and on-screen actions.
You can navigate with WASD.
You're limited in the number of things you can do by the number of keys, but also by player memory.
Keyboards could give you, with combinations of keys, thousands and maybe even tens of thousands of possible inputs.
Humans can't remember that many.
Not to play a game.
So memory is actually a fundamental limitation here.
So how many possible combinations do you think you can remember off the top of your head when you're playing a game? 10, 50, 100?
There is a limitation.
And so a question to think about is what if you didn't have to remember everything?
What if you could just think about what you wanted to do and it happened?
Would that change how you play games?
So the gamepad.
I work for Valve, so this is the Steam controller.
There are also lots of other great game fads out there.
Again, we have buttons that mimic kind of what we get from keyboards, and we have analog sticks and touchpads to mimic what we get from a mouse.
Analog sticks, instead of being one-to-one mapping, give you a velocity vector, right?
You get a direction and a displacement, and you have D-pads for more binary actions.
We also have gestural controls.
Things that let you make more naturalistic movements.
Wii Boxing.
You can actually box.
So you get a wider array of possible movements and more naturalistic, which is kind of intuitive to, I guess, would be a useful memory cue.
It's also fatiguing.
Moving your hand and your wrist is not as tiring as moving your entire arm.
A quick summary of what we just discussed.
Again, we're mapping intent to onscreen output.
We're doing it with reasonable latency.
You have to think a thought and translate it into a finger movement or a hand movement or a wrist movement.
We have restricted bandwidth.
We can only do so much at once.
How many different keys can you press at once?
We're starting to add a widening array of input patterns and we are getting more naturalistic movements.
And these are great.
They work for current generation games and that's fine.
But I guess the question I'd like to ask you guys is, what happens if we didn't have to use them?
How would you imagine an ideal interface?
Like, think about ways to improve current interfaces, or what new kinds of interfaces might be.
What are better ways of interacting with games and pieces of technology, right?
Are we yet optimal?
Is this it?
Or can we do better?
What is your end game for an interface?
What is your dream interface?
Like, what are ways things could be improved?
So if you couldn't come up with anything in like the eight seconds I gave you to think of stuff, I made my own list.
So interfaces could respond quicker.
They could allow a much broader space of possible input commands.
We could have complex patterns of input.
We could have chains.
Think about doing thing, oh I wanna undo the last thing I did and rotate this thing that I did and whatnot.
We could also maybe get additional axes of information from the player.
And so I wanna drill into this one.
And this is going to be the focus of the talk.
With BCIs we can make progress on all of the, I guess, the top four ways we can potentially improve interfaces, but what I'm really curious about, or what I'm really fascinated by, is what happens when we get additional data from the player we're not getting with current generation interfaces?
So what could we do if we had more, or different kinds of information from the player?
This is the question, this is the question that kind of motivates this research.
What about all the data we're missing from the player's experience?
We know what players are trying to do in game for the most part.
We know if they're jumping or reloading or selecting a character, right?
Or trying to attack.
But we don't know how they're experiencing it.
So are they happy or are they sad?
Are they engaged or are they detached?
Are they challenged or are they bored?
Are they frustrated or curious?
Are they exploring or solving puzzles or goofing off or playing seriously?
Are they learning something new?
Are they learning something old?
Are they distracted?
Are they remembering something?
Are they speaking to a friend?
Are they getting angrier?
Are they getting happier?
Right, so do we actually need any of this?
You know, maybe not, but I would like to find out.
I guess like a hypothesis I'm going to make in this talk is that we would get an improved player experience and a qualitatively different player experience.
if we had access to internal states and emotions and cognitions and decisions.
And so I hope at the end of this talk, you can kind of see at least why I believe this and maybe you might feel similarly.
Okay, so let's get to the good stuff.
Here's a picture of me wearing my sweet OpenBCI EEG headset.
It's an open source company.
There are other EEG headsets that look a little bit different, but I kind of like this one because it always makes people ask me what I'm doing when they see me wearing it.
So again, broadly speaking.
A BCI, or at least as I'm using it here, is a way of transforming what happens up here into something usable for a system out here.
So we wanna interpret physiological data and turn it into something useful.
And so, well again, I am referring directly to measuring neurological signals.
Again, truly any way we could reliably detect emotions or decisions or intents or thoughts could fall kind of like under the space or under the, I guess, well, could be used for what I'm about to start describing.
So this is a cool looking picture of a neuron.
You guys might remember neurons from your eighth grade biology textbook.
Neurons are nerve cells.
They are the atomic units of the brain.
I promise this is the only, I think, biology slide in the talk.
Yeah, so neurons are the atomic units of the brain.
If you put enough of them together and you organize them in various functional and hierarchical ways, you get a brain.
And so they communicate by firing or not firing.
They send electrical signals down various pathways of processing, and they're clustered together in various brain regions and seem to provide distinct subsets of functionality, although not always.
There are about 100 billion in the brain, and connections between neurons is called a synapse.
There are about 1,000 trillion, or one quadrillion, synaptic connections in a human brain.
And when neurons fire, and this is the important part, They produce every single aspect of conscious and unconscious experience.
Every thought you have, every sensation you have, everything you perceive, every feeling you have.
They're a consequence of neurons, or bundles of neurons, or patterns of neurons firing together, or not firing.
So in some respects, we're actually already inside the matrix, like what we see and experience and feel is constructed by neurons firing up here.
And when you have a thought, neurons fire in a particular way.
When you have a different thought, neurons fire in a different way.
There is a neurological correlate or origin for every single thing that happens in our brain.
And our reality is constructed by these patterns of firings.
So if we can reliably measure them, then maybe we can start doing useful things with them.
So just to drill into that a little bit deeper, what are we actually measuring?
Patterns of activity in the brain, whether they are temporal, so happening over time, or spatial, where they are happening in the brain.
And we're also measuring responses to external stimuli, what I see and hear and feel and touch, or imagined, what I'm thinking, right?
All of these will produce, you know, all of these will induce neurons to fire.
So what we're actually recording is firing rates of neurons or firing rates of systems of neurons or blood flow to areas where neurons fire.
We simply want to understand locations of increased levels of activity in the brain and hopefully what they are.
And if we can take these patterns of activity and describe them in terms of something a player is experiencing, that's the trick.
We need to say, okay, this pattern of electrical impulses means happiness or it means sadness.
or it means I wish I had reloaded a fraction of a second earlier, or I see somebody doing something cool in a game and I wish I knew how to do that.
So how do we measure this?
The sweet BCI headset, or EEG headset, is on the lower left, it's an EEG.
I'm gonna, everything that has an acronym, I'm gonna try and say, although I think I'm gonna mispronounce them so you guys can laugh at me if I do.
EEGs are electroencephalograms, look at that.
They measure electroactivity in the brain.
The one in the middle is FNIRS, it's Functional Near Infrared Spectroscopy.
It measures light scattering of blood flow, and then you have fMRI, Functional Magnetic Resonance Imaging.
Also measures blood flow, or essentially oxidated blood flow.
And these are ways of detecting, essentially just neurons firing.
I alluded to other physiological signals we could measure.
Heart rate, galvanic skin response, essentially is a change in the pH composition of your sweat that's correlated with physiological arousal.
Pretty reliable, pretty useful.
Eye tracking, we can see where you're looking in our display, get an index of attention.
We have facial expressions which indicate emotion.
We have muscle tension, which is an EMG electromyography sensor that can also kind of give you insight into a player's emotion.
And then you have posture, which could give you guys social cues and perhaps measures of engagement as well.
All of these might be useful and all of these might get us a long way towards kind of like the things I'm about to describe.
But we are gonna stick with BCIs and I guess that's the last time I'll give you guys that caveat.
So I've also been talking primarily about the central nervous system, which refers to what's happening up here in your brain.
We also have a peripheral nervous system, right?
We have nerve cells that go to our fingers, go to our fingertips, and reach down to our toes.
People have different estimations of how long it takes a signal to travel from your brain to your finger.
One number to use could be 100 milliseconds.
What if you could intercept the signal right here at the wrist and say, yeah, we know that you, means you want to turn your hand this way, move the mouse this way.
What if we could shave off 10 milliseconds or 20 milliseconds or even 30 milliseconds off of your reaction time?
You think people who play games competitively would want that.
What if we could predict your movement before you made it?
Would that be interesting?
There are people working on this right now.
Seems possible.
Another interesting thing to think about with BCIs is I've been talking mainly about the read side of the equation.
What about the write side of the equation?
What about inducing?
So the read side is interpreting brain signals.
What about inducing them or creating them?
This already happens with perception and cognition.
When games show you something, they're inducing patterns of neuronal activity.
When they make you think something, they're inducing patterns of neuronal activity.
You can also do that directly.
People are investigating this now.
This next slide might be the most important distinction to think about when we're discussing BCIs.
You can create interfaces that communicate non-invasively, outside the brain, and you can create interfaces that communicate invasively, from inside the brain, or from underneath the skull, from inside the skull.
Lots of things we can do.
We can do non-invasively.
EEGs, electroencephalogram, MEGs, magnetoencephalogram.
But we can also go invasively with electrocorticography or stereo electroencephalograms.
A lot of the things we're going to discuss again, we can do non-invasively.
But some of the things I'm going to describe, we might only be able to do invasively.
How many of us are willing to have somebody drill a hole in our head to derive a benefit?
Maybe not too many, but in the future, who knows?
We're gonna discuss why people might be willing to do that.
So here's EEG data.
This is actual data we can turn into something useful.
This is from the very first recording I ever made.
This is my brainwaves.
You can see that electrode number eight was not seated correctly, so we'll never truly know.
what I was thinking at this point in time.
But the trick here is to turn this into something useful.
So this is data recorded over time.
One way, and I won't have too much analysis slides in here, but I do want to kind of give you guys some insight into what you can do, is just look at, essentially, the firing rates of electrical activity in various parts of the brain, and you can categorize them.
Delta waves are slow, and beta waves are quick.
And you can see where you get more or less of them in various parts of the brain.
And so you can take these patterns of activity, and again, if you reliably measure them.
you can turn them into something that says, yeah, a player is feeling this, or is thinking this, or is intending this.
There are lots of signal processing techniques to do this, machine learning and deep learning techniques are very useful in this space because we're dealing with noisy data.
And a lot of work needs to be done, but this is one way that people kind of try and make sense of what's happening.
So with BCIS, what can we actually measure today and measure non-invasively?
These are just headlines from journal articles.
Here's an incomplete list.
We could measure someone's learning, if they're learning something, and their ability or faculty to learn.
We could measure surprise or novelty.
We could measure if they're excited or relaxed.
We could measure affect, which is just what psychologists refer to as kind of the direction of emotion, whether it's positive or negative.
We could measure whether they're, if they're attending to something, or if they're engaged, if they're bored.
We could measure responses to in-game stimuli.
And we're not always getting these signals reliably, but we're starting to figure out how.
So just think about what you would want to know about your players.
There is a long list of things that we can get right now with current generation technology, current generation analysis, and current generation experimentation.
So I guess I'm gonna make the case that there is increasing BCI interest these days.
And the reasons for that would be we're getting better at imaging, detecting data, also at stimulating, which is the red side of the equation, and also the analysis, like deep learning is very helpful here.
Also, we're figuring out ways to aid people with disabilities, people with sensory impairments or motor impairments or cognitive impairments.
We're getting the ability to, yeah.
Help people see again, who can't see, and hear again, who can't hear.
And manipulate things again, if they've lost a limb or if they're paralyzed.
There's also fears about general AI.
People have those concerns and they'd like to figure out how humans can compete, and so they're looking at BCIs.
And people have a general interest in augmenting human capability.
Can we push ourselves past what we already are now?
Gets a little hairy, but again, it is driving interest.
There's also one more factor that was kind of important.
is that if you're gonna measure brain signals, you need a way to get people to wear a helmet.
So if only we, us as game designers, had a way of doing that.
Well, you know, not everybody plays VR, but one advantage that the VR and AR potentially have as well is you're getting consistent contact, or at least semi-consistent contact with the source of brain activity.
So you might be able to do interesting things if you could convince people to wear a helmet that had EEG sensors.
All right, so let's get to the interesting stuff.
That was an overview.
I wanted to get everybody on the same page just so people kind of understood what I was talking about.
What I'm gonna talk about for the remainder of the talk, I'm gonna talk about, I guess, three separate areas of interest.
How BCIs could change the way that we design games and play games and our players play games.
So we'll start with playtesting improvements, then we'll move on to adaptive gameplay, and then we'll talk about the end game, actually replacing interfaces.
Playtesting and adaptive gameplay can be done now, the interface replacements will be in the future, but in some cases are actually starting now.
So we'll begin with playtesting.
This is an example of a test subject playing Team Fortress.
So playtesting is how...
It's how Valve makes our games.
We gather data from people who are not us and then iterate and improve the game as a consequence.
I imagine it's how lots of people make games.
Playtesting is great.
Here are a few traditional approaches.
Direct observation or watching somebody play the game.
You can ask people questions afterwards, or sometimes during.
You can give people surveys.
Game metrics are how we test on the player base at large.
Look at some measurable human behavior.
And also doing usability to poke at specific aspects of what's happening.
Playlisting is great, there are downsides.
If you ask people questions, they rationalize, they invent, they confabulate and conflate, and they forget things.
Can also only measure what we record, right?
Any behavior we're not recording, we don't know if it's happening.
And it can be hard to know why sometimes.
As a psychologist, I'm happy to walk you guys through all the ways that people don't understand why they do what they do, but that's a separate talk.
We also lack moment-to-moment insight.
about what a player is experiencing.
They can tell you in the moment, but that changes the experience.
They can't tell you everything.
And large sample sizes are impractical.
We can bring in 30 people a week, maybe 100 people a week.
What if we could play this on 300,000 people a week, or three million?
Would we become better game designers?
So let's walk through six specific ways that BCIs could be interesting.
how they might help us get moment-to-moment insight to what our player is experiencing, how we might get more objective data and do it at scale, the new kinds of data we can get, how we can measure things over time, and then how it becomes, I guess, a source of, a way of giving us more confidence as we can let it kind of work with other methodologies we currently have.
So moment-to-moment insight.
To me, this is the thing that, I guess, most excites me, I guess, at the moment.
because we can do this right now and get data we haven't had before.
We can get a real-time understanding of a player's emotional state, moment by moment, second by second.
We can start figuring out responses to individual components of gameplay.
So think about that for a second.
You can understand how a player reacts to a specific enemy or a chat message or a bullet that's fired, or if they die, or if they kill a character, if they choose a character, or if they see a piece of the environment they like.
We can eliminate gross responses, like, oh, I like this.
Yeah, the game was five out of 10, or it was great, or it wasn't.
We can determine the components of a player's experience that lead to the overall impression.
In essence, we get to ask a question at every moment of the playthrough, and we get an answer without interfering with the experience.
I think that's spectacular.
So this is what I'm most optimistic about.
Now, you know, I'm presenting a rosy picture, right?
Like, work needs to be done, right?
But with current technology, this is possible.
We also get more objective data, right?
Internal sensations are not interpreted by the player, right, they're not conflated or created.
And we avoid memory, you know, memory problems, as I said.
We'd know if someone was feeling challenged instead of just hoping that they reply to our question honestly.
We could truly know if a player was feeling, we're doing game balance, right, and we ask them in a playtest, was that hard?
Some people don't like being honest.
We would know.
Think about this.
Instead of trying to induce a particular emotion in gameplay, like is a player afraid?
Are they excited?
Are they surprised?
What if we could actually know?
Right?
Like we haven't, we're designing with intent, we're trying to induce particular emotions and sensations.
We could actually know if a player was experiencing them.
We could also do this at scale.
Right?
We derive so much information from bringing people in and play testing at Valve.
But what if we could do that on everybody?
We'd have much more information about how people were playing and consuming our games.
At the moment, playtesting on the population means you look at changes in measurable player behavior.
Like, 20% of players are playing this map, whatnot.
But if we could get the advantages of direct observation and of internal state out to everybody, we'd be much better game designers.
If BCIs or any form of physiological measurement were prevalent, and all our players had access to them, and we had access to that data, we could always be playtesting.
on everyone and be asking every question we care about.
Maybe not every question, but a lot of the important ones.
We're also getting new kinds of data, data we've never had before.
We're no longer limited by the behaviors we can explicitly measure.
We can start trying to infer rationales behind behaviors, looking for positive responses or negative responses.
and whether a player is more or less engaged.
We can get more accurate measures of overall sentiment and we're getting much more granular measures of individual sentiment and more granular measures of how players are responding to individual gameplay components.
Here are a few examples.
We could predict when someone is about to quit both the session and maybe forever.
We can better understand game balance.
We can see if forum responses correlate to overall player sentiment.
This has been something I've been fascinated by for a very long time.
Is what we're seeing people write on forums and in emails, is that a loud minority or a loud majority?
We could better understand if the data we're getting from forums and from people writing in actually correlates to how people are experiencing the game out in the real world.
We can better understand impressions of a new weapon, a character, a cosmetic, a map, a game mode.
We can figure out what parts of an update somebody enjoyed and how they feel about the game's interface.
We can figure out which players are toxic, which players are good teammates.
We can find out the most satisfying elements of gameplay and the least.
What parts of the environment do they hate?
And how hopeful the tutorial ends up being.
We can also compare responses over time.
We could look at sentiments pre and post update and assess sentiment.
We could estimate a game's life cycle.
You can do this with surveys and other methodologies, but effort is required.
What if we knew how someone felt on their first session?
And how they felt on their 10th?
And how they felt on their 100th?
And how they felt on the session that was the last session of your game they ever played?
Very useful to understand how a player is experiencing your game over time and how their feelings about your game change over time.
And it becomes another useful tool in the toolbox.
We can combine data about someone's internal state with every other data source we have.
Gives us more ways to figure out the why of what is occurring.
So we become smarter game designers.
This is me trying to relax.
I wasn't relaxing, I was trying to relax.
This is data from me seeing the unusual weapon I made in TF2.
For those unfamiliar, unusuals are weapons in TF2 that have particle effects.
I made one, I like seeing it, it makes me happy.
This is me getting a kill in TF2.
So the question to ask is, where else do I have a similar response to this?
Is getting a kill the same as seeing my unusual?
There actually are some similarities here.
We won't get into the weeds on that.
I know it looks a little bit different.
But both of those are positive reactions.
And so the trick here is saying, okay, we think this is a positive reaction.
Where else do I have similar responses in game?
If we could do that, we could start saying, yeah, these are the things that I like about Team Fortress 2.
Maybe these are the things I don't.
So that was playtesting.
Let's go on to step two.
This is step two in our BCI timeline, the second major way that games could be impacted.
What if we could reliably measure various internal states?
What kinds of gameplay could we begin to create?
This is a slide from, or well, yeah, I guess a creative slide about Left 4 Dead, which is a game that came out a long time ago, but was one of our first attempts at trying to design games that were adaptive.
So the middle line here is essentially an estimated arousal level for the player.
You can see it has some peaks and valleys.
And the goal was to generate changes in that arousal level by increasing the number of zombies that were present.
The desired population of zombies is on top.
The colored zombies are boss monsters or special infected.
And the bottom is the actual zombies that were created.
So the goal here was to create a series of peaks and valleys for the player.
Is that the right pattern of arousal?
Who knows?
But what if we could reliably measure that and figure out what the right pattern of arousal was and have a game adapt to what a player was experiencing?
How would that change how you make games?
What if all you knew was a player was experiencing positive or negative emotion?
How would this change game design?
Could we start placing reactions in context?
Just think about everything you could do.
If you just knew if a player is feeling positively or negatively towards the game.
Or even better, a specific aspect of the game.
And we'll go through some examples.
And obviously we don't want to eliminate all negative examples.
Positive things are positive by comparison to negative things.
But we can definitely adjust the balance.
We can figure out ways to mitigate the impact of negative occurrences and heighten the impact of positive ones.
So think about having adaptive enemies.
What kinds of enemies do you like playing against in game?
What kinds of enemies are challenging?
What kinds are boring?
If we knew the answers to these questions, we could have the game give you more of the challenging kind and less of the boring kind.
Instead of difficulty levels, we become a thing of the past, right?
We don't have easy, you know, casual, novice, advanced, hard, hardcore, nightmare, whatever.
We have dynamic difficulty adjustment, right?
We have difficulty that's designed for you, to give you the challenge that you're craving at that point in time.
Sometimes you might want things harder, sometimes you might want them easier.
We'd be able to figure that out.
What is it like to only be able to kill an enemy while you're relaxed?
Maybe it's nothing.
Maybe it's a gimmick.
Maybe it's something different, a new kind of gameplay.
So as we walk through these examples, it may be the kinds of changes I'm discussing, they won't quite work with conventional game designs.
That's fine.
And so let's start to think about new kinds of gameplay experiences that might be better suited to make use of this data.
These are adaptive enemies.
Let's think about adaptive teammates.
What kinds of teammates do you like playing with?
Or hate playing with?
What parts of the game do you enjoy doing?
We could pair you up with other people who like doing the things that you don't like to do and form better and more cohesive teams.
If we could identify toxic players or you're feeling that somebody's a toxic player, we could automatically mute them or keep you separated.
What if you knew how your teammates were feeling?
What if they knew how you were feeling?
Again, I'm asking questions not because I necessarily have the answers, but I think that BCI technology makes them seem like interesting questions to ask.
What about adaptive rewards?
Here are examples of a few rewards in Counter-Strike.
We have new items, badges, friend requests, social rewards, XP.
We could figure out what kinds of rewards you like and the ones you don't.
We could start tailoring rewards to the individual player.
So we would avoid the things you didn't like and give you more of the things you did.
The theme I want to hammer home is that games, in some sense, become way less static.
And obviously games are not static, but on one axis they are.
They could be a lot more adaptive.
They could learn more about you, and they could respond to individually what you're doing and how you're experiencing it.
What about avatars that mimic your mood?
What if in-game characters reacted to it?
What if you had conversations and interactions that more closely mimicked reality?
Just imagine more realistic conversations.
We're not gonna solve the Turing test tomorrow, but we could change how you interact with NPCs with other players.
they could see you were feeling frustrated or if you were happy or if you were sad.
Just like, true, think about it.
But like, you know, games right now, we do our best.
Interactions could get a whole lot more realistic and engaging.
Here's an image from Dota.
Just take a look at it.
What aspects of it do you like or dislike?
Which heroes or characters in the game do you like or dislike?
How do you feel about the trees, or the flames, the health bars, the combat, the strategy, the tactics?
All of a sudden, we start becoming able to assess how you're responding to various elements in game, and maybe we can start having the game change things.
We can make small changes, we can make big changes, we can even make medium-sized changes.
So this is the question.
How does gameplay change when you know what the player is experiencing?
What aspects of the game could we change if we knew how the player was responding?
If we knew if a player was frustrated, or engaged, or feeling challenged, or immersed, what if they were in flow?
The game could respond to the player as a consequence of knowing what they were doing.
We could give you cues when you're frustrated.
We could give you challenge when you're bored.
We could figure out what keeps you intrinsically motivated and better understand when we're failing.
We can figure out where extrinsic motivation is unnecessary or unneeded.
We can figure out what leads to flow or immersion and when you're in it and when you're about to become in it.
We can improve accessibility and figure out when players needed assistance.
We can enlarge text, change audio cues, change gameplay, reduce complexity.
We can figure out what a player needed and respond accordingly.
Here's a screenshot from Valve's best known game, Alien Swarm.
Came out a while ago, it's a top-down shooter, awesome game, a lot of fun to play.
This is an experiment we did a while ago where we used physiological signals, physiological arousal, as an input to the game.
We gave you four minutes to kill 100 enemies, and the timer would tick down quicker if you got more aroused, if you got more excited, which would then cause you to get even more excited, and then it would tick down even quicker, and so you get a negative feedback loop.
And so, you know, it's one design that may or may not actually end up being suitable for consumption, but you can start thinking about novel kinds of gameplay experiences when physiological data becomes an input, a direct input to the game.
Imagine a spy game where you actually have to be a spy.
and fool people with your voice, your words, your behavior, and your thoughts.
What about a true role-playing game where you actually had to play as the character you are currently pretending to play as?
Maybe these are gimmicks, that's fine.
Maybe there's something different, something cool.
Could we teach players how to play our games more effectively?
How great are we designing tutorials right now?
Maybe we're really, really good.
We could probably be better.
We could figure out when a player was learning or in a state that was amenable to learning, right?
Maybe figure out why they learned.
And help them learn quicker.
Could better understand what are the components of skill in a game.
And teach them to other players.
And conversely, could we teach games how to play our players more effectively?
I could spend hours on this slide, but I won't.
We have a limited time remaining.
But think about, you know, game dynamics, they're no longer static if we had access to this data.
You know, we start discovering what the player enjoys or doesn't.
We're making fewer assumptions about the player's intent or preference and creating a broader range of experiences for the player.
Just as players respond to the game, games can start to respond to the player.
If these are the kinds of gameplay experiences you like, well, we can start to give you more of them, less of the ones you don't.
Games will start to learn how players want to play their games, and as a consequence, gameplay becomes adaptive and personalized.
We start modifying aspects of the game, the difficulty, the individual components of the gameplay, the rewards, the teammates, the weapons you get.
We change the visuals, the sound, the interfaces.
All of this can adapt to what you're feeling and experiencing.
So gameplay is no longer one size fits all.
It's tailored to the player.
It's no longer a game for everyone.
It's a game for you.
Step three, interface replacements, the true BCI, the endgame.
So we're gonna start getting speculative, or at least more speculative than we've been so far.
So let's go back to that question.
How will you interact with technology in 10 years?
Well, when you say be sad at people, sometimes they think, you know, you're gonna think a thing and it happens.
Removing the middleman.
Right now we have human intent, translated via physical interface, induces a response in the system.
So, what happens if we remove the physical interface?
We have human intent, it just directly leads to a system response.
If we remove the middleman, what becomes possible?
If we could reliably understand what you were thinking or feeling and have a system respond, how would that change things?
This may be decades away, but people are doing things now that imply that it might not be.
And this is one direction, human intent leading to a system response.
We have to go back the other way too.
The system has to respond to the human.
So it can do so via current display technology or Might wanna remove the middleman from that direction too.
We need to figure out how to wait for the system to send a response back to the user, directly, without mediation.
So how would we do that?
Well, we can already kind of do that.
You know, what you see and perceive, you know, what you taste and feel and hear, what you think.
All of these lead to neurons firing.
What if there was another way?
There is, I've kind of alluded to this before.
You can make neurons fire directly.
You can communicate with neurons by generating an electrical current in the brain.
Neurons will fire.
So you can make them fire all on their own.
Or all on their own.
This is one way of doing that.
This is a TMS machine.
Stands for transcranial magnetic stimulation.
It uses a magnetic field, induces electrical current in the brain, and is often used in research to increase or suppress neural activity in various parts of the brain.
And they use it for therapeutic treatments, insomnia, anxiety, depression, PDSD, and whatnot.
If you're curious what it looks like to see a TMS machine in action, there you go.
Yeah, that's exactly how it goes.
All right, so what's possible when you remove the interface entirely?
We're gonna go back to the diagram you may have seen, you remember from earlier in the talk.
Five ways that BCIs can change the way we interact.
Neural prosthetics, understanding restricted forms of cognitive intent, figuring out ways to augment perception or cognition, and then the true endgame, being able to enter the matrix.
So neuroprosthetics, these are sensory or motor replacements.
Vision, think about vision.
It's simply, it's a construct of neurons firing.
Light passes through the retina, it's transmitted to various aspects of the visual cortex.
These neurons fire in particular ways and we get the perception of sight, what we're seeing right now.
If your retina fails, we could bypass the retina.
and still give you sight if we make neurons fire in ways that you can understand and interpret.
People do the same thing with cochlear implants.
How people with auditory impairments hear again.
Think about people who are paralyzed.
Well, if we can stimulate neurons in the motor cortex, we don't need signals from the peripheral nervous system.
People are doing this right now, trying to help paralyzed people walk and blind people see.
What if we understood?
rudimentarily, what you were thinking.
We could replace a mouse or a keyboard or a gamepad.
There are probably people in this room who are working on that at this moment, being able to type with your thoughts, navigate through an environment with your thoughts, make a selection with your thoughts.
What about augmenting perception?
Could we make you see in infrared, like the predator?
Could we increase contrast sensitivity, help you see with a higher resolution, or give you access to other spatial information, like echolocation?
Could we make you navigate like a bat?
Could we add a sense?
Could we improve your sense of touch?
Let you hear at different frequencies.
Taste and smell new things you've never tasted and smelled before.
People are looking at this too.
What about augmenting cognition?
Cognition is tricky.
There's a lot we don't know.
What if we could focus attention?
This would make you better at attending to what you wanted to attend to.
What if we could stimulate appropriate areas of the brain and prevent you from being distracted?
What if we could recruit neurons for other processing tasks?
Or increase your short-term memory capacity?
Make you hold more memory at once.
That would be incredibly powerful.
What if we could improve memory retrieval, or the resolution of long-term memory?
If we could help you learn quicker and more effectively, or speed up your reaction time to various stimuli.
Or help you interface directly with machines.
If you treat insomnia, via some sort of BCI interface.
Are you augmenting cognition?
To me, the answer is yes.
That's happening now.
And then there's the matrix, the endgame, right?
Calling it like an augmentation of experiences, you know, it's underselling it.
But if neurons are truly creating every sensation that we have, well, if you can stimulate neurons directly, you can put people inside the matrix.
This is, again, decades and maybe even centuries away.
But the path forward, the path to that, I guess, reality, people can see it, and they're actively working on it.
So we've talked all about, well yeah, this is me contemplating challenges now with my BCI headset, EEG headset.
So we talked all about the possible things, why BCIs might be interesting.
So let's talk about the problems, the things we have to do to get there.
As a psychologist, I know I shouldn't put negative slides at the end of the talk.
I want you guys to go out on a positive note, and I do have a few positive slides at the end, but I absolutely want to be realistic about the challenges that lie in BCI implementation.
I'm saying a lot of optimistic things up here, and that's fine, right?
I'm trying to encourage discourse around it, but there are a lot of hard things we have to do.
I guess to enable these technologies.
So neuroscience has a long way to go.
Neuroscience is in its infancy.
We understand some things.
Sensory stuff is great, motor stuff is great, relatively great.
Higher level cognition is tough.
And how we get there is hard to tell.
There's so much we don't understand.
Like how do you think a thought?
How do you remember something that happened last week?
How do you solve a problem?
How do you play a game?
How do you feel love?
There's so much we don't understand and so much we need to figure out to get to where we want to go.
There is a common analogy that people who are doing BCRI research like to use, which is that recording neurological activity from outside the brain or outside the skull is like standing outside a football stadium with your ear against the concrete and listening to the cheers and trying to figure out what's happening.
You might be able to tell when something important is happening, loud cheer, if it's positive or negative, depending on which stadium you're at.
But data is incredibly noisy.
It's noisy inside the brain and it's way noisier if you have to pass through the scalp and the skull and hair.
Data is noisy, so neurological data is hard to parse.
We need to figure out how to do a better job of analyzing it and processing it.
And going hand in hand with noisy data, right, is figuring out the right tools to analyze and process it.
What are the most effective ways to take a look at it?
There's always gonna be lots of variability, right?
Both in an individual, like what I'm feeling now is not gonna be the same way I feel like a second from now, or how I felt three seconds ago, right?
And then obviously across populations.
We are gonna think differently and have different patterns of activity.
So we're looking for something that is stable, some pattern that is reliable and coherent.
But it might be tough to figure out.
So we could just look for spike trains, peaks, patterns of activation, whatever.
We could just throw large neural networks at the problem.
That's one approach people are taking.
But it's definitely a hard problem we need to solve.
And it goes hand in hand with a better understanding of neuroscience.
Let's go back to the stadium analogy.
If we have noisy data, well, one way of attacking it is to get a large number of samples.
It will help us get around the noise analysis issues, but we definitely need it.
So, you can build a pretty useful data set if you had lots of people giving you data, and if you knew what they were seeing and hearing, like if EEG sensors were built into a VR headset and everybody was playing the same game.
That might help you make progress.
Just a thought.
Imaging technology, this is the same slide from earlier, has limitations, right?
Obviously we can't get people to carry an fMirai machine around with them, but maybe EEG headsets or FNIRs or new technologies people are working on can get there.
But what if we need invasive approaches?
Are you willing to let somebody do this?
So we can do a lot of interesting things.
We can improve play testing, we can have adaptive experiences, maybe some rudimentary movement controls, all of this with non-invasive stuff.
If we wanna go invasive, people need to see a Lasik-like benefit.
Remember, Lasik used to be crazy, shining a laser into your eye to improve your vision.
Now it's commonplace.
Will the same transformation take place with invasive technologies, where people implant electrode arrays in your brain?
Maybe, maybe not.
So when implants get to the LASIK stage.
There's also another reason I showed you this picture.
This is the third time I've shown a picture of Gabe.
Are you laughing as much as the first time?
The answer is no.
Wouldn't it be nice to have that kind of data from our players?
Wouldn't I become a better speaker if I knew how you guys were reacting the third time you saw this picture as opposed to the first time?
So the next time I give this talk, I'll do a better job.
There are also issues with the hardware costs and dissemination, building these things, building imaging technology, getting it out to people.
Sensing technology costs money.
There needs to be a benefit to customers.
What if they're unwilling to share this data with customers, or with us, sorry, with developers?
This could be personal stuff.
We need to be able to convince them that there is utility.
and helping us make better games and creating new kinds of gameplay experiences.
That might be tough to overcome.
Some of the technology is cheap, at least on the cost side of things, but some of it might not be.
So there need to be benefits to customers.
Can't just be theory.
We need to make something that actually works.
We're also losing tactile feedback.
There's something comforting about holding a mouse, well I guess a mouse, or a gamepad, or a keyboard, right?
If I mishit a button on a gamepad, if I'm trying to press B to jump, or sometimes A, I get pretty consistent feedback.
What happens if I think a thought, I think jump, and nothing happens?
Is it my fault for thinking incorrectly?
Or the system's fault for not understanding what I was trying to do?
So this is not the largest problem we're facing, but it is an issue to consider.
And this last one is maybe the biggest.
This is Don Quixote attacking a windmill, he's attacking an enemy that does not exist.
What if BCIs are not a necessity?
What if players don't see benefit?
What if we as developers come up with ways to take advantage of having insight into internal state?
I do believe I can see clearly how this would happen.
I've laid out a variety of ways I thought we could get better data from playtesting and design gameplay experiences that don't exist.
But I absolutely could be wrong.
This is theory.
I'm describing things in the abstract, and it's easy to make things in the abstract sound cool.
So I want to acknowledge the possibility that while I am optimistic, We don't know yet, right?
We actually have to create something and see.
And again, we have some data that suggests it's there, but let's acknowledge the possibility that it might not be.
So we're at the end of the talk.
I'll summarize where we are, why we think BCIs are interesting.
Moment to moment insight into new experience, or sorry, into experience.
You have to do it at scale.
We'll get all kinds of awesome new data.
And we get to design games that are adaptive.
That probably covers a lot.
I probably should have made that, like used a larger font for that point, but hopefully I've made the case as to why I think that's interesting.
And the same for replacements for traditional interfaces, right, the end game.
New kinds of experiences become possible.
So when can all this happen?
Well, we actually have to start doing the work, but we can start making progress on playtesting and adaptive gameplay now.
Adaptive games might take a few years to get going, but we can start the process now.
Then interface replacements, well again, people are working on things now, but could be very, very far away.
So I don't know exactly when all of this will take place, but what I do know is that in the future, I very much hope this is how we play.
So thank you very much.
So I think we have a few minutes for questions.
I'll answer any now if you have them, and if not, I'm happy to go to one of the wrap-up rooms and chat as well.
Hi, so is Valve doing anything dealing with the collection of data from atypical brain structures?
So because one of the challenges is going to be that people that either have abnormal brains and don't know it try and use these devices and don't get the same responses as others.
Yeah, so at the moment, we just have to collect data from anyone who we can get data from and then start figuring out which aspects of it are reliable and if there is something different about some individual person.
So that will likely come up as a consequence.
It's not something we're looking at directly at the moment.
And also that's gonna become important later on when you're providing feedback to the player.
So for each game, you may have to do a calibration of the actual individual person.
Yes, yeah, so absolutely.
Like what this actually looks like in practice, right?
Ideally we get to a point where players could play the game without any changes on our end for a couple hours or whatnot and we would get enough data to start making, testing hypotheses about what they might like or don't like or if they're feeling engaged or whatnot.
So, and maybe we'll get a lot better at it.
It can turn it into something that's five minutes or half an hour.
But yeah, at the moment, I guess the way I see it is we probably gather a fair bit of data at the beginning.
Great.
Sure.
Nicole is from Xeo Design.
And yeah, interesting talk.
I also am really interested in the opportunity with VR and other sort of headsets, just so we get more measurements up there.
I was curious on the point, especially about adaptive gameplay, why muck around with the skull when you're looking at when emotions are so important to balancing?
if you've got even if you have frustration and you can measure it which we which we did you know 15 years ago the uh you know your your frustration curve some players play to get super cuphead frustrated yes in order to feel the feeling of fierro yes i won because you can't push a button and win yes So just saying, whether they're being positive or negative, we don't want to pull, if you pull all the negative out, then there is no actually gameplay, it's flat.
They want to feel that emotion.
And so then going back to BCIs and EEGs, you're really only measuring those systems, the data is noisy, agree.
But they also can only measure the first quarter inch of activity, which is all neocortex, where emotion happens is in the deep brain, the hypothalamus, the amygdala, and all that.
It's wires, neurons, but it's also pipes, oxytocin, serotonin, all of these chemicals.
So I was curious, why are you so optimistic about BCIs that are measuring from the external?
They're non-invasive.
When we could just measure facial expressions, like on your earlier slide, we used Pollackman's facial action coding for the four keys to fun, which is our model on emotion and how gameplay creates it.
Yeah, so it may be that we can use webcams looking at somebody's face and get almost everything we need.
I think the reason to think about BCIs is that we actually can get a little bit more, I think, than we can get.
I think we can do a better job with emotions, even if we can't get deep into the brain, although hopefully changes in imaging technology will kind of address that as well.
The purpose of the talk is not necessarily to say that BCIs, or what I was describing with EEGs, are definitely the way to go.
eye tracking and a webcam and a GSR sensor might get you everything you need.
But if we are going to think long term, and we are trying to think long term, trying to understand what's possible and what we could get that would be more granular than what webcam will give us, it seems like EEGs, I guess, they give us a lot of things and potentially a lot more.
Because there's hundreds of emotions, not just three or four.
Yeah, we're absolutely not wedded to it.
It's more like, again, I had the one slide about physiological signals.
I mean, I'm speaking about BCI, because we think in the long run, that's where things will end up.
But maybe over the next five years, it's just webcams, and we get a lot from that.
Or you can also measure the microexpressions.
You could actually measure the muscle contractions on the face.
Yeah, so that's with EMG and whatnot.
And if you have a VR headset, you can have EMG sensors as well.
Yeah, so you get curiosity, surprise, anger, contempt, a lot of stuff that's happening on muscles.
Yeah. Thank you.
Sure.
Hi, thanks.
Great talk.
So I do EEG and VR research just as part of my graduate program.
One of the problems that we have with just having VR be, well, EEG be a good control interface for VR is to reliably classify just different trials in, let's say, like an event-related potentials paradigm.
You have to average together a whole lot of trials.
And while that brings user frustration down with regards to like trying to select like different objects, it then takes that control time up.
Yep.
So is like Valve looking at any like solutions for that to help bring the, not VR, bring EEG and VR together like in in the consumer space a little bit?
Yeah, so one hope we have, which is optimistic, is that if we had 100,000 samples of the same person doing a task five times as opposed to one person doing a task 500 or 5,000 times, that we could overcome the issue that you're describing.
That may not be able to be the case, but the hope is that by looking at how people are reliably reacting to a variety of things and over time and with large enough sample sizes that we could start getting a picture.
And so we would start with, yeah, so I think in the long run, we'd like to be able to eliminate that.
At the moment, in the beginning, yeah, we realize that's a problem and we'll have to overcome it.
And the hope is that with enough samples, we will.
But that might be naive.
Okay, all right, thank you. Sure.
Hi, I was wondering, you talked a lot about the adaptiveness of changing gameplay based on the actual physical emotional response.
But I was wondering, how far along is technology in terms of actually controlling characters within a game, such as telling someone to go forward and jump?
How far along is that?
So it's right in the beginning.
So non-invasively, people have trained people using EG headsets to play Tetris, for example.
It takes training and it takes work.
So it is possible at the moment.
But actually doing complex navigation through a space, we're not there yet.
But.
And it may be that we eventually reach kind of like, we come up against a fundamental limitation of what we can do outside the skull.
And then it's like, well, we need to figure out a way to, either we stop doing the research or we figure out a way to provide a benefit where people are willing to get implants.
Okay, cool.
Yep.
Thank you.
Oh.
I guess that's all I had time.
Yeah, I will, I guess we can go to like one of the wrap up rooms over there.
So happy to chat with you.
