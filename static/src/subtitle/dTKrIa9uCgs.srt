1
00:00:06,285 --> 00:00:09,627
I assume you're all gathered here today to worship Cat Thulu.

2
00:00:11,808 --> 00:00:12,728
Welcome congregation.

3
00:00:13,989 --> 00:00:18,132
Or perhaps you're here for the Sucker Punch facial performance

4
00:00:18,172 --> 00:00:23,595
capture pipeline, as seen on Infamous Second Son.

5
00:00:32,240 --> 00:00:33,200
So, the Infamous series is...

6
00:00:37,428 --> 00:00:39,749
The second sign is the third installment of the series,

7
00:00:41,290 --> 00:00:45,732
of the Infamous series, where basically you have an open world superhero game,

8
00:00:48,393 --> 00:00:51,074
or super villain, depending on how you play and the choices you make.

9
00:00:52,975 --> 00:00:57,897
So, karma plays a big role in the game,

10
00:00:58,658 --> 00:01:01,399
not only in the choices you make and the gameplay,

11
00:01:01,419 --> 00:01:05,661
but it affects the story and the outcome and how your powers progress.

12
00:01:07,860 --> 00:01:13,963
The Infamous series, with its comic book roots, has a very strong narrative, as all superheroes

13
00:01:13,983 --> 00:01:15,204
need a strong backstory.

14
00:01:16,384 --> 00:01:22,748
And with the jump into new hardware, we really wanted to see about pushing the facial performances,

15
00:01:23,568 --> 00:01:26,590
trying to just push the quality on all fronts.

16
00:01:26,650 --> 00:01:31,052
And as you are probably familiar, if you're here, with facial animation, the higher jump

17
00:01:31,092 --> 00:01:31,893
in quality you get.

18
00:01:32,826 --> 00:01:37,247
all fronts. The more important it is to have solid facial animation, you know, because you'll quickly

19
00:01:37,627 --> 00:01:46,989
dive into the uncanny valley. So what we had before, the previous two infamous games, we had

20
00:01:47,009 --> 00:01:55,470
the real-time cut scenes, hand-keyed animations for both cut scene and in-game play, basically

21
00:01:55,490 --> 00:02:02,412
to be driven by joint-only facial rigs. So moving forward onto Second Son, we really

22
00:02:03,307 --> 00:02:05,948
We wanted to dive deeper into the story with new hardware.

23
00:02:05,968 --> 00:02:10,150
We felt like we could kind of push in closer to the characters and have a more compelling

24
00:02:13,371 --> 00:02:18,932
story, bring some actors in, actually capture, try to capture the subtle nuances that will

25
00:02:18,953 --> 00:02:26,035
help carry, connects you with both the story and the characters and hence the gameplay.

26
00:02:27,275 --> 00:02:32,857
Ultimately giving the gameplay more meaning, giving more motivation to the player.

27
00:02:33,518 --> 00:02:37,200
and just give a richer, fuller, more cinematic experience

28
00:02:37,220 --> 00:02:37,560
to the game.

29
00:02:39,221 --> 00:02:41,363
So we were exploring facial motion capture.

30
00:02:41,383 --> 00:02:43,484
It was on the table right at the beginning.

31
00:02:43,504 --> 00:02:46,085
But if we were going to go that route,

32
00:02:46,105 --> 00:02:50,488
we knew that we wanted to have bodies, faces, eyes, and audio,

33
00:02:50,528 --> 00:02:52,969
and multiple actors captured at the same time.

34
00:02:52,989 --> 00:02:58,373
There are methods where you can get some facial capture that

35
00:02:58,413 --> 00:03:01,795
really require that you're in an isolated area.

36
00:03:03,279 --> 00:03:09,041
Separate from your body capture, your face would be scanned and tracked and applied later.

37
00:03:09,061 --> 00:03:12,902
So you end up having this disconnect between your body and your face,

38
00:03:13,962 --> 00:03:17,303
as well as a disconnect between your actors.

39
00:03:20,764 --> 00:03:25,545
And lastly, it was still a requirement that this all runs in real time.

40
00:03:28,966 --> 00:03:30,326
So here are some highlights of our results.

41
00:03:42,357 --> 00:03:43,878
No spoilers.

42
00:03:43,898 --> 00:03:47,398
The game is released on Friday, by the way.

43
00:04:16,093 --> 00:04:16,753
We're all excited.

44
00:05:16,275 --> 00:05:17,236
So there you go.

45
00:05:18,037 --> 00:05:21,160
So we were looking into the various approaches

46
00:05:21,680 --> 00:05:25,284
for facial performances.

47
00:05:25,384 --> 00:05:28,026
And we could have gone the route we went before with hand-keyed

48
00:05:28,547 --> 00:05:29,067
animations.

49
00:05:29,108 --> 00:05:34,132
There's a 3D scan per frame performance capture,

50
00:05:34,192 --> 00:05:37,435
kind of like a, I guess I would call it like LA noir style,

51
00:05:38,036 --> 00:05:40,318
where you're captured in a booth.

52
00:05:42,232 --> 00:05:47,833
I think that's a method where you can really get lots of detail directly from the actor,

53
00:05:47,873 --> 00:05:51,874
but there are, like with all these things, there are their own limitations, and we did

54
00:05:51,914 --> 00:05:57,916
want to capture the body and the face and the other actors all at the same time and

55
00:05:57,956 --> 00:05:58,456
on stage.

56
00:05:58,516 --> 00:06:04,997
And using that sort of method, you're really kind of isolating, narrowing what kind of

57
00:06:05,137 --> 00:06:05,878
performances you can get.

58
00:06:05,918 --> 00:06:07,818
They're supposed to kind of keep their heads still.

59
00:06:11,423 --> 00:06:15,384
They have to use reference objects for where their eyes are tracking and it's kind of all pieced together.

60
00:06:15,424 --> 00:06:19,624
So you don't want to go big with animations because that might not necessarily come together well.

61
00:06:20,605 --> 00:06:29,687
And also there's a really consistent topology or UVs with that method without considerable work.

62
00:06:29,707 --> 00:06:34,928
So retargeting to other characters is next to impossible.

63
00:06:34,988 --> 00:06:38,968
And having animation on top of it and kind of fixing anything.

64
00:06:39,882 --> 00:06:41,643
would basically just require a reshoot,

65
00:06:41,663 --> 00:06:42,864
because you really can't do that.

66
00:06:45,326 --> 00:06:47,468
So there's other methods, muscle-based simulations,

67
00:06:47,528 --> 00:06:51,151
and things that have been done for feature film stuff

68
00:06:51,451 --> 00:06:56,735
that has relevance, and it was really more of an R&D project

69
00:06:56,755 --> 00:06:59,057
that we wouldn't really think was necessary,

70
00:06:59,097 --> 00:07:01,479
considering the last option that we did go with,

71
00:07:01,579 --> 00:07:04,942
which is the data-driven, example-based approach

72
00:07:04,982 --> 00:07:07,944
that is based on a lot of things

73
00:07:07,964 --> 00:07:08,425
that have already been.

74
00:07:08,815 --> 00:07:13,156
proven out and has worked well in feature films in the past.

75
00:07:13,196 --> 00:07:18,657
So basically, what the data-driven example-based

76
00:07:18,677 --> 00:07:23,498
solution is, is you have a bunch of data, in our case,

77
00:07:23,558 --> 00:07:28,300
a bunch of scans of your actor going through a range of motion

78
00:07:28,320 --> 00:07:33,361
to get to infer the physiology of the face.

79
00:07:33,701 --> 00:07:36,402
All the little subtleties that happen with the muscle

80
00:07:36,422 --> 00:07:37,742
sliding over bone in fashion.

81
00:07:38,282 --> 00:07:42,243
And scan, you know, when you get it from a range of scans, you can kind of get all that stuff for free.

82
00:07:45,884 --> 00:07:52,285
The scans are roughly based on what's called a FACS system, which there was a PhD named Paul Ekman that

83
00:07:53,805 --> 00:08:00,547
completely unrelated to CG, he had described all the muscle movements in the face as independent

84
00:08:02,028 --> 00:08:05,588
movements that defined everything you could do with the face. Anything else was just a

85
00:08:05,628 --> 00:08:07,869
combination of those. So it ends up being a really good.

86
00:08:08,358 --> 00:08:13,242
library for what kind of range of motion you can get out of the face that can be

87
00:08:13,262 --> 00:08:19,966
used for getting quality scans to really fill out that data set of poses that

88
00:08:19,986 --> 00:08:24,490
you're going to use to ultimately blend and match to what the actor is

89
00:08:24,510 --> 00:08:31,275
doing at any given frame of motion capture. So to make that happen you need

90
00:08:31,295 --> 00:08:35,338
a consistent facial marker at least the method that we had gone through there's

91
00:08:37,308 --> 00:08:39,868
a number of marker dots that are placed on the face.

92
00:08:39,928 --> 00:08:45,709
It's aligned with muscle regions and directions that the face moves.

93
00:08:46,310 --> 00:08:50,190
Per actor, you kind of align it in a way that it's following their muscle flow

94
00:08:50,250 --> 00:08:53,691
and kind of between the creases, so it's always exposed to the cameras.

95
00:08:53,711 --> 00:08:59,032
I think of cameras, I'll go into that a little bit later,

96
00:08:59,072 --> 00:09:03,093
but we did use head rigs that are wireless.

97
00:09:03,733 --> 00:09:04,153
Head rigs have...

98
00:09:04,632 --> 00:09:09,014
The camera's pointing back at the actor that's recording all the markers on the face, and

99
00:09:09,374 --> 00:09:13,256
they're up close enough to get that high fidelity that you want for the subtle movement of the

100
00:09:13,296 --> 00:09:13,576
face.

101
00:09:15,157 --> 00:09:22,460
So that same marker set is what it's used on a day-to-day basis for the motion capture

102
00:09:22,640 --> 00:09:23,120
itself.

103
00:09:23,180 --> 00:09:32,724
So what we do is we actually get a vacuform mask, the default pose of the actor, and there's

104
00:09:33,340 --> 00:09:37,743
where the marker set is, we basically extrude little points on that model and have it sent off to

105
00:09:38,523 --> 00:09:43,086
an engineering firm that will print out a vacuum form mask and drill holes in each one of those spots

106
00:09:43,106 --> 00:09:47,908
so we can use that as a template, you know, onset at the beginning of any day of motion capture

107
00:09:47,948 --> 00:09:52,651
and they'll correspond directly to our facts data, all of our scans.

108
00:09:52,711 --> 00:09:58,954
So as part of that, we need a method of taking that facial marker data

109
00:09:58,994 --> 00:10:02,456
and obviously coming up with a nice line of shapes that's reasonable.

110
00:10:03,083 --> 00:10:05,404
That's where the solver comes in.

111
00:10:06,765 --> 00:10:09,486
Basically just applies that data, in our case,

112
00:10:09,987 --> 00:10:13,689
to a facial rig, where it can also

113
00:10:13,709 --> 00:10:16,050
be used for animating on top of it.

114
00:10:16,070 --> 00:10:20,772
You can replace animation or just animate on top

115
00:10:20,832 --> 00:10:23,054
to tweak or change the performance as needed.

116
00:10:24,995 --> 00:10:27,916
And lastly, if your game character

117
00:10:27,936 --> 00:10:31,078
doesn't look exactly like your actor, you want to vary it up.

118
00:10:33,556 --> 00:10:36,358
you'll have to retarget that somehow onto your game character.

119
00:10:36,378 --> 00:10:39,801
So I'll go into that also in a little bit here.

120
00:10:40,562 --> 00:10:43,304
So here's just a sky view of the pipeline in general.

121
00:10:43,785 --> 00:10:47,708
The whole left column is basically about acquiring the shape.

122
00:10:47,728 --> 00:10:50,010
So I'll describe that next.

123
00:10:50,991 --> 00:10:54,133
We're just kind of floating over the points here.

124
00:10:54,173 --> 00:10:56,255
We have our facial motion capture,

125
00:10:57,556 --> 00:11:00,819
a picture of Troy Baker, the actor wearing the head rig.

126
00:11:02,905 --> 00:11:09,370
So all the scans are combined with our solver, which in our case is a post-based deformer.

127
00:11:10,071 --> 00:11:16,677
And the motion capture is applied to the rig so that it's directly connected to this solver

128
00:11:17,057 --> 00:11:24,263
and just outputs a normalized number of shapes that matches the current configuration of motion capture.

129
00:11:25,184 --> 00:11:30,809
And you can do additional things on top of that, additional deformers, simulations.

130
00:11:31,392 --> 00:11:34,314
Kind of the sky's the limit with this method.

131
00:11:35,856 --> 00:11:41,261
We actually have joints per facial marker.

132
00:11:42,362 --> 00:11:46,466
There's nearly a couple hundred facial markers, and so we've got a pretty solid skin cluster

133
00:11:46,506 --> 00:11:48,568
as a basis where this all starts off.

134
00:11:48,608 --> 00:11:53,352
And all the deformations that we have are pre-skin cluster to give you the vertex level

135
00:11:53,613 --> 00:11:56,075
changes that are inferred from all those scans that we got.

136
00:11:56,115 --> 00:11:56,235
So...

137
00:12:00,007 --> 00:12:01,828
I'll come back onto those details later.

138
00:12:03,029 --> 00:12:04,250
Just jump ahead twice.

139
00:12:05,110 --> 00:12:05,751
Okay, yeah, here we go.

140
00:12:05,831 --> 00:12:09,593
This is the fact session of this giant chart here.

141
00:12:10,094 --> 00:12:13,696
And basically we have a day, this is a one-time setup,

142
00:12:14,136 --> 00:12:17,078
you know, a lot of stuff where we'll scan the actor

143
00:12:17,138 --> 00:12:18,059
going through these series of...

144
00:12:19,937 --> 00:12:21,478
fax poses and combination poses,

145
00:12:21,518 --> 00:12:25,359
and sometimes poses that are just unique signature looks

146
00:12:25,399 --> 00:12:27,259
that he's gonna have while in character.

147
00:12:27,319 --> 00:12:29,340
So we're trying to get as much data

148
00:12:29,360 --> 00:12:31,280
that's gonna be repeat and useful.

149
00:12:31,320 --> 00:12:32,961
And the method we're using,

150
00:12:33,001 --> 00:12:35,761
it doesn't have to be quite perfectly isolated shapes.

151
00:12:35,821 --> 00:12:38,862
Like a lot of scan processes really require

152
00:12:38,902 --> 00:12:41,003
that you kind of isolate those motions

153
00:12:41,023 --> 00:12:42,103
so they kind of add up well.

154
00:12:43,143 --> 00:12:44,944
But the problem with the blend shapes,

155
00:12:44,984 --> 00:12:46,464
if you're trying to make them add,

156
00:12:47,445 --> 00:12:48,025
and not keeping them.

157
00:12:48,569 --> 00:12:52,891
normalize, you're going to end up with shapes that just don't work well together. So I've seen

158
00:12:53,552 --> 00:13:00,116
huge networks of fixer shapes that try to make combinations work together in order to kind of

159
00:13:00,676 --> 00:13:07,500
make up for where shapes don't work well together when actually they should be morphing together,

160
00:13:07,700 --> 00:13:11,183
you know, where they're not adding. They're all normalized so that you never have the weight of

161
00:13:11,223 --> 00:13:13,264
any one given shape over one. So, uh,

162
00:13:15,944 --> 00:13:20,105
just going with the process of how we get the consistent topology of all of our fact shapes.

163
00:13:20,485 --> 00:13:22,526
Each of these scans are millions of points.

164
00:13:23,526 --> 00:13:25,647
If you looked at it in wireframe view it would just look solid.

165
00:13:26,447 --> 00:13:29,948
So obviously we're not going to play that in the engine.

166
00:13:29,968 --> 00:13:34,969
We want decent topology, it has good flow, much lower resolution,

167
00:13:34,989 --> 00:13:39,110
a lot more resolution than we could have on previous games.

168
00:13:39,130 --> 00:13:39,370
The actor...

169
00:13:43,945 --> 00:13:47,848
doing these scans, you know, he can move around and he'll take breaks and come back and you know,

170
00:13:47,868 --> 00:13:51,571
they're seated. So there's a stabilization process that has to happen to make all these

171
00:13:51,591 --> 00:13:58,956
shapes relative to the default pose. As we take a low-res version of that face,

172
00:14:00,938 --> 00:14:05,741
based off the default pose, we make sure there's a tight correspondence between the markers from

173
00:14:05,781 --> 00:14:11,906
that default pose and all the other shapes on that low-res version that we make. It's basically

174
00:14:11,946 --> 00:14:12,727
extracted from our...

175
00:14:13,227 --> 00:14:14,568
our default pose to start with.

176
00:14:14,668 --> 00:14:16,549
So that's, once that's set up,

177
00:14:17,310 --> 00:14:20,131
we'll use a warp algorithm that'll take the marker points

178
00:14:20,792 --> 00:14:24,154
and basically warp the geometry and snap it to the surface

179
00:14:24,194 --> 00:14:25,314
of each one of those shapes.

180
00:14:26,375 --> 00:14:27,916
There's a subdivide and then a snap

181
00:14:28,596 --> 00:14:30,317
where it just projects itself onto the surface

182
00:14:30,337 --> 00:14:32,659
to pull those extra details back into the shape.

183
00:14:33,519 --> 00:14:33,839
And so,

184
00:14:36,601 --> 00:14:39,203
each of these shapes end up being combined

185
00:14:39,243 --> 00:14:41,044
with the back of the head.

186
00:14:41,064 --> 00:14:42,545
We have eye socks and mouth socks.

187
00:14:43,384 --> 00:14:50,746
nose socks and those basically are joined and kind of go for the ride as you run through each of those shapes almost like a

188
00:14:52,006 --> 00:14:53,746
as a blend shape initially just to

189
00:14:54,847 --> 00:14:55,067
have it

190
00:14:56,627 --> 00:15:01,729
pull the neck and mouth along for the ride so they that they're staying combined so you just kind of copy them off along the way

191
00:15:01,749 --> 00:15:08,671
You know ultimately you end up with your final topology so you have with this process as we're starting with the low-res

192
00:15:09,251 --> 00:15:11,952
You have a version. I have two versions of LOD of low-res

193
00:15:12,405 --> 00:15:13,465
and a high-res that can be applied.

194
00:15:15,926 --> 00:15:18,727
With scans, there's always problems with the eyes

195
00:15:19,067 --> 00:15:21,888
and around the teeth, where there's a lot of reflectance.

196
00:15:21,908 --> 00:15:23,728
You have eyelashes, and that causes interference.

197
00:15:23,748 --> 00:15:25,569
You can get a lot of noise in those areas.

198
00:15:25,629 --> 00:15:29,570
And the lips in particular, you have areas

199
00:15:29,630 --> 00:15:33,631
that are not always exposed, the inside of the lips.

200
00:15:33,671 --> 00:15:35,792
And you'd be surprised how much that actually shows

201
00:15:37,312 --> 00:15:39,613
with really expressive emotional performances.

202
00:15:41,148 --> 00:15:49,030
So we'll have a modeler go in and, based off of the scans, clean up that area around the mouth

203
00:15:49,090 --> 00:15:53,212
and make sure that they maintain their nice blendability across all the shapes.

204
00:15:54,672 --> 00:15:59,114
That's one area that's neglected quite a bit, which when you get more realistic characters,

205
00:15:59,514 --> 00:16:05,116
it's really important to make sure that you're maintaining the thickness of the lips,

206
00:16:05,216 --> 00:16:09,777
that the volume changes as the lips kind of scrunch together.

207
00:16:10,326 --> 00:16:14,809
And so you really need a modeler who understands anatomy.

208
00:16:14,849 --> 00:16:18,251
And ultimately, you go through and you

209
00:16:19,371 --> 00:16:20,952
can kind of run them off as a bunch

210
00:16:21,032 --> 00:16:22,213
and see how they all blend together.

211
00:16:22,253 --> 00:16:28,817
And here's more stats on our scanning process.

212
00:16:30,598 --> 00:16:33,079
You have from 60 to 70 scans per actor.

213
00:16:35,320 --> 00:16:38,402
As I described, we apply that onto consistent topology.

214
00:16:39,069 --> 00:16:42,492
The hardware we used were two linked structured light scanners.

215
00:16:42,952 --> 00:16:45,014
They can get a lot of detail out of the face.

216
00:16:45,734 --> 00:16:49,337
We had them scripted to kick off in succession.

217
00:16:49,838 --> 00:16:53,420
So there's four cameras, two per unit,

218
00:16:54,381 --> 00:16:56,002
which takes a couple seconds to a few seconds.

219
00:16:56,022 --> 00:16:57,543
We had faster machines and everything,

220
00:16:57,583 --> 00:17:03,388
but it was the cost of getting that much detail on the chief, I guess.

221
00:17:03,448 --> 00:17:06,430
But it does get a lot of detail, and you get results right away.

222
00:17:06,470 --> 00:17:08,092
So you didn't have to...

223
00:17:08,824 --> 00:17:10,465
if the actor moved a little bit at that time,

224
00:17:10,765 --> 00:17:11,566
you'd see it right away.

225
00:17:11,586 --> 00:17:13,107
You can just have it taken again.

226
00:17:13,187 --> 00:17:14,647
So that was the process we used.

227
00:17:15,968 --> 00:17:19,590
Again, that's a scanning process that's done

228
00:17:20,571 --> 00:17:22,472
in a static volume, you know, they're seated in the chair.

229
00:17:23,532 --> 00:17:26,534
It too has to be stabilized, you know,

230
00:17:26,554 --> 00:17:28,675
against the defaults that they all blend well.

231
00:17:29,576 --> 00:17:31,497
That's quite a process for the fax poses.

232
00:17:31,517 --> 00:17:32,817
So here's Troy Baker again.

233
00:17:33,478 --> 00:17:36,079
Again, hopefully, Delson Rowe.

234
00:17:36,840 --> 00:17:37,940
Series of his fax poses.

235
00:17:38,522 --> 00:17:41,984
So quite a number of shapes, mouth open and closed,

236
00:17:42,044 --> 00:17:44,566
giving it a range of motion.

237
00:17:44,586 --> 00:17:48,689
And because each of those markers has a joint there,

238
00:17:48,969 --> 00:17:51,270
and actually every joint has their own pose space deformer,

239
00:17:51,290 --> 00:17:56,033
each one of those shapes can contribute uniquely

240
00:17:56,313 --> 00:17:57,534
to any other part of the face.

241
00:17:57,574 --> 00:18:00,056
So these are all triggering independently

242
00:18:00,076 --> 00:18:03,298
to give you basically an infinite combination of what

243
00:18:03,318 --> 00:18:03,998
the face can do.

244
00:18:07,060 --> 00:18:07,260
Thank you.

245
00:18:08,537 --> 00:18:12,539
Here's a view of that consistent topology remeshing

246
00:18:12,579 --> 00:18:13,299
that I was describing.

247
00:18:13,660 --> 00:18:15,100
On the right, we have our low-res mesh.

248
00:18:16,521 --> 00:18:18,062
Same dot configuration.

249
00:18:18,703 --> 00:18:21,524
It gets warped and snapped to that dense one in the middle,

250
00:18:22,525 --> 00:18:23,866
which I think, like I said, I couldn't show that

251
00:18:23,886 --> 00:18:25,847
in wireframe, wouldn't be able to see anything.

252
00:18:27,288 --> 00:18:30,169
Then that's combined with the back of the head,

253
00:18:30,189 --> 00:18:31,610
the mouth socket, and such.

254
00:18:33,851 --> 00:18:36,113
So here's this little video of

255
00:18:37,107 --> 00:18:39,109
I'm just going to plan through some of those blend shapes.

256
00:18:42,011 --> 00:18:45,154
I just made a blend shape out of the shapes so you can see them in succession.

257
00:18:45,174 --> 00:18:51,059
You can see how they're working and decide if there's any editing that needs to happen.

258
00:18:51,119 --> 00:18:52,180
It's really important.

259
00:18:52,200 --> 00:18:57,484
This whole process, you really are following those marker points carefully through the entire pipeline

260
00:18:57,504 --> 00:19:00,407
because you want as much consistency as you can because those are going to be your drivers

261
00:19:00,447 --> 00:19:01,808
and you want them triggering the right shapes.

262
00:19:07,494 --> 00:19:09,734
So I've already described that marker correspondence.

263
00:19:11,855 --> 00:19:16,416
You can see what the custom vacuform mask looks like here on the lower left.

264
00:19:16,436 --> 00:19:20,477
So it's got this series of holes, you're just going to go on there with a marker and place the dots.

265
00:19:20,517 --> 00:19:29,400
Of course, you want to be careful doing this to make sure that the mouth is consistently in a nice default pose,

266
00:19:29,420 --> 00:19:30,600
the actor's not trying to talk or anything.

267
00:19:30,620 --> 00:19:35,201
We actually had gotten some back where I think the marker set was a little off

268
00:19:35,241 --> 00:19:36,282
and I'll animation kind of look like.

269
00:19:37,046 --> 00:19:40,228
He had a droopy mouth, but luckily we have just animation controls.

270
00:19:40,248 --> 00:19:41,569
We can tweak that right back.

271
00:19:47,294 --> 00:19:48,575
So I think I've run through all these points.

272
00:19:48,595 --> 00:19:49,596
I can move ahead.

273
00:19:53,159 --> 00:19:56,081
So now I'll be describing the motion capture process itself.

274
00:19:56,601 --> 00:20:01,705
These wireless head rigs are really lightweight.

275
00:20:02,026 --> 00:20:03,167
They're wireless.

276
00:20:04,207 --> 00:20:05,509
You can just swap in cards.

277
00:20:06,605 --> 00:20:11,568
a couple hours of motion capture if we actually would have to swap it and the batteries swap

278
00:20:11,628 --> 00:20:12,968
out and such as needed.

279
00:20:13,008 --> 00:20:25,434
So some of the other requirements of these head rigs is that, well actually I'm jumping

280
00:20:25,454 --> 00:20:27,815
ahead of myself, I'll just follow through here and follow the slides.

281
00:20:30,094 --> 00:20:33,435
It's already described that we want everything to be captured simultaneously.

282
00:20:34,515 --> 00:20:36,856
So how do we keep all that stuff in sync?

283
00:20:37,496 --> 00:20:45,258
Traditional film, you know, we just use a piece of hardware that just has cycles that's always ticking at the same moment.

284
00:20:45,438 --> 00:20:55,601
And most cameras you would find on a motion capture set are going to allow all the equipment to capture at the same time.

285
00:20:55,681 --> 00:20:58,201
So they're all in that same cycle and they're stamped with the time code.

286
00:20:58,958 --> 00:21:03,942
When you're going back and looking at the data, you'll see that they're all lining up as needed.

287
00:21:05,603 --> 00:21:12,128
Kind of a follow-up benefit of having those facial rigs is that you can get video right from any one of those cameras,

288
00:21:12,849 --> 00:21:17,492
or a combination and stitch them together to project onto a temporary piece of geometry,

289
00:21:17,512 --> 00:21:21,175
so you have basically an animated texture map of the actual performance,

290
00:21:21,255 --> 00:21:24,157
and you can have that delivered right away with the body motion capture,

291
00:21:24,738 --> 00:21:27,940
so that you can just continue setting up your scenes, setting up your camera.

292
00:21:28,597 --> 00:21:34,779
for cut scenes and while the actual tracking is taking place that can take a little bit longer.

293
00:21:34,799 --> 00:21:48,543
As far as these head rigs, there's different ones that are out there and depending on your own pipeline,

294
00:21:48,563 --> 00:21:49,423
you might have different needs.

295
00:21:52,364 --> 00:21:55,405
We use the multi-camera setup so we could actually...

296
00:21:55,928 --> 00:22:00,431
triangulate the positions of each of those markers in 3D. Other methods

297
00:22:00,451 --> 00:22:04,174
that have been used for tracking use things like optical flow and feature

298
00:22:04,214 --> 00:22:09,858
recognition to try to estimate the movement of the face.

299
00:22:09,878 --> 00:22:17,163
But then you can have motion capture. Sometimes there's noise that you

300
00:22:17,183 --> 00:22:22,767
have to deal with and try to filter your data. Luckily when you have

301
00:22:22,827 --> 00:22:25,349
cameras up that close that's not as big of a deal.

302
00:22:26,985 --> 00:22:35,010
The helmets can slip around, you know, they're on there pretty snug, but they're not, you know, bolted to their head, unfortunately.

303
00:22:35,110 --> 00:22:36,010
They didn't agree to that.

304
00:22:36,030 --> 00:22:39,712
So, they can slip around from day to day.

305
00:22:39,752 --> 00:22:41,333
They're going to be aligned slightly different.

306
00:22:41,353 --> 00:22:50,798
So, a stabilization process that can constrain those back to relative to a default is necessarily part of that process.

307
00:22:51,439 --> 00:22:51,559
So...

308
00:22:54,689 --> 00:22:57,310
You can also have missing or occluded data.

309
00:22:57,570 --> 00:22:59,711
You have wrinkles and folds in brows and stuff

310
00:22:59,731 --> 00:23:02,933
that can just hide marker data.

311
00:23:02,953 --> 00:23:04,954
So a way of dealing with the missing data.

312
00:23:05,594 --> 00:23:07,915
And then with facial rigs, the lighting conditions

313
00:23:07,955 --> 00:23:09,055
are very important.

314
00:23:09,315 --> 00:23:13,657
The stage that we were on had the entire ceiling

315
00:23:13,797 --> 00:23:16,098
was emitting light, and the walls

316
00:23:16,158 --> 00:23:17,899
and very nice diffuse lighting.

317
00:23:17,959 --> 00:23:19,680
So pretty much from any angle, you're

318
00:23:19,700 --> 00:23:21,801
going to get a decent capture of the face.

319
00:23:22,601 --> 00:23:23,122
Other units have.

320
00:23:23,785 --> 00:23:26,266
lights that are on them projecting back.

321
00:23:27,126 --> 00:23:28,647
It can be distracting to an actor.

322
00:23:28,667 --> 00:23:32,808
But again, you know,

323
00:23:32,828 --> 00:23:35,409
depending on the conditions and the stages that you're on, you'll have different needs.

324
00:23:40,411 --> 00:23:42,571
Which brings me to the retargeting phase.

325
00:23:46,513 --> 00:23:48,353
So retargeting is

326
00:23:48,373 --> 00:23:50,234
can be done in a number of ways.

327
00:23:51,439 --> 00:24:00,526
I don't think I'll go through describing all the ways we didn't do it, but basically for our method we retargeted both the shapes and the facial motion capture.

328
00:24:00,546 --> 00:24:08,471
You know, so that the facial motion capture is basically moving in the same space as your game character version, so it's triggering your game character shapes.

329
00:24:08,952 --> 00:24:17,017
And it's all applied into a rig so that since we're actually using the joints connected with that marker movement, it's all...

330
00:24:17,400 --> 00:24:21,201
incorporated into a rig that works well using the actual game character.

331
00:24:26,384 --> 00:24:30,446
So here's one of the actors you might have seen in the highlights there.

332
00:24:31,026 --> 00:24:35,328
Her name's Karen, and we have her Native American counterpart in the game.

333
00:24:36,088 --> 00:24:42,851
Thicker face, baggier eyes, wider nose, more ethnic look.

334
00:24:43,491 --> 00:24:45,672
And using these two defaults,

335
00:24:47,568 --> 00:24:49,189
and all the shapes we got from the scans,

336
00:24:49,530 --> 00:24:51,531
we can generate all the other shapes.

337
00:24:52,372 --> 00:24:54,694
So basically, it uses a proportional constraint

338
00:24:54,814 --> 00:24:57,656
to take every edge in the face.

339
00:24:58,116 --> 00:25:00,358
And that relative difference between the two defaults,

340
00:25:00,959 --> 00:25:07,203
it does a solve and iterates, basically,

341
00:25:07,244 --> 00:25:09,285
to come up with where all those would settle

342
00:25:09,786 --> 00:25:11,587
as moved through each of those other shapes.

343
00:25:16,094 --> 00:25:18,055
If Adrian stops by here, he can answer any questions

344
00:25:18,075 --> 00:25:21,076
about that specific solver that he had created.

345
00:25:22,297 --> 00:25:25,238
He explored a number of different constraint types.

346
00:25:26,498 --> 00:25:28,339
One nice thing is that with this method,

347
00:25:28,399 --> 00:25:30,920
it also was retargeting the motion data,

348
00:25:30,960 --> 00:25:31,840
which is very low res,

349
00:25:32,320 --> 00:25:33,621
relative to the really high res shapes.

350
00:25:34,001 --> 00:25:37,242
The marker points actually line up exactly spot for spot,

351
00:25:37,262 --> 00:25:39,303
so we know we're triggering the right shapes

352
00:25:39,343 --> 00:25:40,043
with the right data.

353
00:25:43,285 --> 00:25:43,625
So as far as,

354
00:25:45,583 --> 00:25:46,724
How we're applying those shapes, I mean you can

355
00:25:47,545 --> 00:25:49,706
sure have blend shapes, they're easy to animate.

356
00:25:50,447 --> 00:25:53,489
Has this, you know, possibility of having the

357
00:25:53,550 --> 00:25:55,511
over-adding issues that I was describing,

358
00:25:55,531 --> 00:25:57,733
if they're not normalized.

359
00:25:58,474 --> 00:26:03,237
Any shape-based system,

360
00:26:03,298 --> 00:26:06,060
if you have too few shapes, you can get cozy results.

361
00:26:06,820 --> 00:26:09,382
The paths, as they're blending between each other,

362
00:26:10,243 --> 00:26:11,224
if you have too few shapes, can...

363
00:26:11,665 --> 00:26:13,146
kind of take a wonky direction.

364
00:26:13,166 --> 00:26:14,407
I don't know if you've seen that in animation

365
00:26:14,427 --> 00:26:15,567
where there's something just a little off,

366
00:26:15,608 --> 00:26:18,089
it just kind of takes a weird path between poses.

367
00:26:18,129 --> 00:26:21,952
So that's a problem with any system

368
00:26:23,733 --> 00:26:24,614
if you don't have enough shapes.

369
00:26:25,274 --> 00:26:28,457
But we do have a method that we can kind of correct

370
00:26:28,477 --> 00:26:30,518
for some of that just in case there is areas

371
00:26:30,538 --> 00:26:33,620
in the pose space that you're not actually getting data

372
00:26:33,640 --> 00:26:36,222
to avoid that wonky path that it could take while blending.

373
00:26:38,164 --> 00:26:39,545
So ultimately as our solver,

374
00:26:40,385 --> 00:26:41,326
it was a pose space.

375
00:26:42,432 --> 00:26:48,674
deformer. This is basically just deriving shape weights from your marker data.

376
00:26:49,574 --> 00:26:54,715
At its heart, it uses a radial basis function, which you look at the references to go deep

377
00:26:54,735 --> 00:26:59,556
into the math on that if you're interested. But ultimately, what's really nice about it

378
00:26:59,616 --> 00:27:10,319
is that the weights that it outputs are normalized from the most relevant, the closest shape

379
00:27:11,480 --> 00:27:17,923
pose space from your, that matches your marker configuration on any one of your shapes will trigger in that section.

380
00:27:17,943 --> 00:27:20,664
Remember, we're doing a different pose space per joint. So

381
00:27:22,625 --> 00:27:26,167
if you have a configuration that exactly matches the scan, you'll get that exact

382
00:27:27,247 --> 00:27:30,589
vertex data applied in that area. So

383
00:27:34,270 --> 00:27:37,692
and this is applied pre-skin cluster so that

384
00:27:39,659 --> 00:27:43,141
The heavy lifting is done by the joints and then the extra data is on top of that which

385
00:27:44,802 --> 00:27:45,703
Helps for some efficient

386
00:27:48,725 --> 00:27:53,528
Compression because we can stream that stuff into the game where the joints are just animation that's

387
00:27:54,909 --> 00:27:58,171
played back in an engine, but the vertex data is

388
00:28:00,432 --> 00:28:03,715
Just that extra level of deformation that can happen on top of that

389
00:28:03,755 --> 00:28:06,637
But you have the joints there for a level of detail if you want as well

390
00:28:08,219 --> 00:28:11,922
So as far as training this pose space deformer,

391
00:28:12,242 --> 00:28:14,604
you have all your shapes and you have your marker configurations.

392
00:28:16,646 --> 00:28:21,630
And basically you need to fill in your deformer with all its information.

393
00:28:21,650 --> 00:28:28,355
It's basically just a matrix that's set up of rows being shapes and columns being vector data,

394
00:28:29,075 --> 00:28:32,938
coefficients are the shape weights, and a process called a single...

395
00:28:33,882 --> 00:28:39,106
a value decomposition is run on it, which gives you a transpose matrix that basically just says

396
00:28:39,126 --> 00:28:45,332
if you have a bunch of new marker data from an arbitrary pose from motion capture,

397
00:28:45,712 --> 00:28:48,455
you just have that lined up, you multiply it by the original matrix,

398
00:28:48,495 --> 00:28:54,100
and you're going to get pose weights as a result that can, within the deformer,

399
00:28:54,140 --> 00:28:58,444
it will blend those shapes together to give you an output.

400
00:28:59,788 --> 00:29:02,209
It's by this deformer node, in our case in Maya,

401
00:29:02,229 --> 00:29:04,430
so you have a direct connection into the node

402
00:29:04,450 --> 00:29:07,891
and you have a direct connection out to your shape.

403
00:29:07,911 --> 00:29:09,852
And it's just this blended result

404
00:29:09,892 --> 00:29:12,313
of all the most appropriate shapes.

405
00:29:15,955 --> 00:29:17,855
So here's just kind of a way of visualizing.

406
00:29:17,875 --> 00:29:23,098
Check up time, my timer did not work well,

407
00:29:23,118 --> 00:29:24,518
so I'm gonna try to speed up as I can

408
00:29:24,538 --> 00:29:26,219
because there's a lot to cover.

409
00:29:26,239 --> 00:29:27,239
Here's just kind of a visualization

410
00:29:27,279 --> 00:29:28,180
of what a post space.

411
00:29:28,601 --> 00:29:35,325
deformer would be if it's really simplified down to 2D. Let's say we picked a point exactly on one of those markers and

412
00:29:38,106 --> 00:29:38,486
there's a

413
00:29:41,448 --> 00:29:42,589
bunch of poses that

414
00:29:43,409 --> 00:29:49,232
where that vertices is moved around in a different spot and so that that would represent, you know, the pose space here and

415
00:29:49,252 --> 00:29:52,414
my little red circle with the fallout there is

416
00:29:53,515 --> 00:29:54,595
some arbitrary pose and

417
00:29:55,736 --> 00:29:57,757
from this, that radial basis function

418
00:29:58,344 --> 00:30:00,705
The beauty of it is that it kind of reaches out

419
00:30:01,465 --> 00:30:03,225
into a radius in that space and gives you

420
00:30:03,245 --> 00:30:05,846
a kind of a capture volume of relevant shapes.

421
00:30:05,906 --> 00:30:07,906
And so the closer a shape is to the center,

422
00:30:08,306 --> 00:30:09,826
it's gonna be driven to a value of one

423
00:30:09,866 --> 00:30:11,947
while all the other ones fall off to zero.

424
00:30:11,967 --> 00:30:13,707
Now it's kind of like a weighted average,

425
00:30:14,167 --> 00:30:16,688
but biasing towards the ones that are closer to the center.

426
00:30:16,728 --> 00:30:18,828
So it kind of cancels out any other shape

427
00:30:18,888 --> 00:30:22,709
as it gets closer to an exact match of your pose.

428
00:30:25,069 --> 00:30:26,690
So let's say you had in this scenario

429
00:30:27,837 --> 00:30:31,718
that single vertex that has a wide range of motion in y,

430
00:30:31,738 --> 00:30:33,259
but very little in x,

431
00:30:34,619 --> 00:30:38,201
you can do what's called a regularization of your parameters,

432
00:30:38,261 --> 00:30:41,922
which you basically just have an input width scale

433
00:30:41,962 --> 00:30:42,823
for each of your parameters

434
00:30:42,863 --> 00:30:44,863
that brings them down to very similar space,

435
00:30:44,883 --> 00:30:46,844
so they're similar values,

436
00:30:48,645 --> 00:30:51,746
that instead of an oblong circle

437
00:30:51,766 --> 00:30:55,628
trying to capture that really wide stretch in y,

438
00:30:56,728 --> 00:30:57,528
you really have it.

439
00:30:58,406 --> 00:31:04,791
and normalize down in space to where you're using a circular radius, you know, the radial basis function.

440
00:31:04,831 --> 00:31:13,017
So, one cool thing about your input width scales is that you can also, if you scale them all together,

441
00:31:13,037 --> 00:31:19,062
you basically are increasing the volume of your capture space for influence of shapes.

442
00:31:20,143 --> 00:31:26,708
And so if you have holes in your post space, you have kind of an even distribution,

443
00:31:28,012 --> 00:31:31,094
a little farther apart, if you don't have maybe enough poses,

444
00:31:31,675 --> 00:31:34,957
you scale that up, it's going to reach out and capture more

445
00:31:34,977 --> 00:31:36,959
and get a nice blend of those shapes.

446
00:31:39,421 --> 00:31:43,864
This is something that could actually be exposed as an animator control.

447
00:31:45,385 --> 00:31:45,846
If you have

448
00:31:47,587 --> 00:31:51,550
a nice distribution everywhere, but you just have a few spots where there are holes in your pose space

449
00:31:51,950 --> 00:31:53,912
where you just didn't have a pose

450
00:31:54,452 --> 00:31:56,654
you could actually do a scan that was relevant for that pose.

451
00:31:57,156 --> 00:32:05,661
marker set. You just scale that up until it's getting influenced by the nearing

452
00:32:05,741 --> 00:32:10,745
poses and you can either just animate it and use it like that or just copy off

453
00:32:10,785 --> 00:32:13,827
the shape after you scaled it so it's blending with those other poses and put

454
00:32:13,847 --> 00:32:19,971
that right back into the pose space deformer as a new shape. So here's this

455
00:32:20,011 --> 00:32:25,054
little test we've done all this training to get our pose space deformers where we

456
00:32:25,572 --> 00:32:29,814
iterated through our shapes we had surface locators placed at each marker and so what that would do is

457
00:32:31,275 --> 00:32:33,736
Go for they'd go for a ride as you're running through all those shapes

458
00:32:34,216 --> 00:32:39,559
And as Susan blend shapes and we're just copying off those values and it just gets fed into the deformer. And so

459
00:32:41,260 --> 00:32:43,741
We took those shapes and just randomly picked

460
00:32:44,882 --> 00:32:52,325
different shapes over a thousand, you know combinations of just picking random shapes so and blend them over over time to see that

461
00:32:53,306 --> 00:32:54,827
your interpolation of your pose spaces

462
00:32:55,266 --> 00:32:59,567
is working well and it should obviously look like your pose when it's at the same pose that

463
00:33:00,087 --> 00:33:06,008
your blend shape is at. So here we have the actor on the left, their shapes,

464
00:33:06,028 --> 00:33:10,149
the retargeted shapes on the far right. The other ones are moving joints, so it's actually a skin

465
00:33:10,189 --> 00:33:16,330
cluster, but on top of that the vertex level stuff per joint is kicking in to make it actually match

466
00:33:16,410 --> 00:33:22,211
what is happening on the right. So it isn't a blend shape, it's actually using that pose space

467
00:33:22,231 --> 00:33:22,551
to form it.

468
00:33:24,889 --> 00:33:36,278
An example of some of those poses is if they were only a skin cluster, if they didn't have that extra level of vertex deformations from the post-space deformer itself.

469
00:33:37,139 --> 00:33:41,723
So just kind of go back and forth. You can definitely see on the lips, you know, the amount of difference.

470
00:33:42,163 --> 00:33:48,028
In the lower left there, you can see the volume of the lips is totally lost by just dragging around joints.

471
00:33:49,369 --> 00:33:54,253
Obviously when you get more realistic, when you're aiming for more realistic...

472
00:33:54,713 --> 00:33:59,135
animation, the lips are a particular area that could fall apart really quickly.

473
00:33:59,195 --> 00:34:10,841
So this is a great way of just really pulling out the real physiological detail at a vertex

474
00:34:10,881 --> 00:34:11,101
level.

475
00:34:13,462 --> 00:34:19,605
So we have key drivers out of those markers that we can divide up the face into.

476
00:34:20,417 --> 00:34:26,700
regions and each of those regions use a different combination of these markers based off of distance and

477
00:34:29,721 --> 00:34:33,623
Offsets and then another layer on top of that one or you're down at the per joint level

478
00:34:34,464 --> 00:34:36,945
Uses those key markers and their own position

479
00:34:38,385 --> 00:34:40,226
So you can have more fine and control on top of that

480
00:34:40,246 --> 00:34:43,688
I'll describe that in a little bit more detail the eyes are a separate thing

481
00:34:43,708 --> 00:34:46,789
you might have noticed the eyes weren't blending in that one version in the middle it's because they

482
00:34:47,216 --> 00:34:52,019
They have different drivers, your eye rotation and your blanks, the direction you're looking,

483
00:34:52,840 --> 00:34:57,403
as well as the markers, so for squinting, as those distances change, they have their

484
00:34:57,423 --> 00:34:59,765
own set of drivers that are trained for those separately.

485
00:35:02,447 --> 00:35:06,730
So kind of like I was mentioning, part of your process is fine-tuning your pose space

486
00:35:06,790 --> 00:35:07,150
deformer.

487
00:35:07,170 --> 00:35:11,994
So you've got all these shapes in there, you're going through them and they're not quite hitting

488
00:35:12,014 --> 00:35:13,234
all the poses like you'd like them to.

489
00:35:13,255 --> 00:35:15,236
You went through that maybe blend shape test and you see that.

490
00:35:15,454 --> 00:35:17,255
Well, it's not really interpolating the way we want.

491
00:35:17,295 --> 00:35:20,737
So we have a method that kind of goes through and regularizes

492
00:35:21,337 --> 00:35:22,918
all those parameters so that they're

493
00:35:23,198 --> 00:35:24,779
working in the same space as the starting point.

494
00:35:24,819 --> 00:35:25,780
And you can kind of scale them up

495
00:35:26,060 --> 00:35:27,201
until you see that they've captured

496
00:35:27,681 --> 00:35:28,582
all the spaces you needed.

497
00:35:29,882 --> 00:35:32,444
Again, like I described, there's a whole bunch

498
00:35:32,464 --> 00:35:34,205
of really nice distribution of poses.

499
00:35:34,265 --> 00:35:37,547
There's just certain spots that don't have a decent pose.

500
00:35:40,711 --> 00:36:06,551
Again, just scale them up at the pose until it finds the neighboring shapes and copy them off, put them back in the system or even, you know, with rigging you can use a lot of things with pose space, which is really awesome is that you can actually, we didn't do this, but you know, you certainly could go through and just say wherever there's a gap in pose space, let's train it to ramp up those scales on the fly, you know, so that it's normal default scales

501
00:36:07,258 --> 00:36:11,279
But then when it gets close to an empty posed region, it'll just automatically branch out

502
00:36:11,319 --> 00:36:12,839
and kind of encompass the neighboring shapes.

503
00:36:16,319 --> 00:36:25,101
I can briefly mention weighted PSDs, but basically this separating of the joints into, or the

504
00:36:25,641 --> 00:36:33,242
pose-based deformers into different regions is basically a way of getting a similar effect

505
00:36:33,842 --> 00:36:35,823
for those that are familiar with the weighted PSD.

506
00:36:36,434 --> 00:36:38,235
So here's our example of where we had a hole,

507
00:36:38,315 --> 00:36:40,096
and our post space is not quite reaching.

508
00:36:40,136 --> 00:36:42,016
So you can imagine, if we just increased our scalars,

509
00:36:42,036 --> 00:36:45,057
we'd just increase that radius and encompass those shapes.

510
00:36:49,219 --> 00:36:51,500
So we have our one-time training phase

511
00:36:51,520 --> 00:36:54,461
where it goes through with our surface markers going for a ride.

512
00:36:54,741 --> 00:36:57,322
We have that matrix all set up.

513
00:36:58,002 --> 00:36:59,283
Once it's set up, it's just in the rig.

514
00:36:59,663 --> 00:37:02,744
The nice thing about it is that as you move your controls around

515
00:37:02,764 --> 00:37:05,165
on the rig, it's actually interpolating.

516
00:37:06,103 --> 00:37:08,025
Those same shapes so that's

517
00:37:08,986 --> 00:37:14,171
One things is particularly about doing this layered approach of using key markers is that you have

518
00:37:15,872 --> 00:37:18,835
Individual controls on the face at some of those markers you can just move around

519
00:37:18,855 --> 00:37:22,959
I'll see the example of that a little bit later how that works, but I

520
00:37:27,663 --> 00:37:29,825
Mentioned that the efficient streaming here, that's that's

521
00:37:31,208 --> 00:37:34,110
Basically, the fact that you're having skin clusters doing the heavy lifting and you have

522
00:37:34,371 --> 00:37:38,374
basically small offsets are the difference for your deformer, which you can take advantage

523
00:37:38,415 --> 00:37:42,038
of GPU decompression to stream that in really fast.

524
00:37:42,518 --> 00:37:47,723
We had about 12,000 vertices per actor.

525
00:37:47,743 --> 00:37:51,367
I think we had up to three actors at a time.

526
00:37:53,847 --> 00:37:55,588
We could do things like on some characters,

527
00:37:55,628 --> 00:37:57,908
like the Betty character, we actually did full sims

528
00:37:57,948 --> 00:37:59,949
that this would drive the face,

529
00:37:59,989 --> 00:38:02,670
but then on top of it we could do a simulation,

530
00:38:03,250 --> 00:38:05,691
loosen up the skin and just allow it to be dynamic

531
00:38:05,751 --> 00:38:08,031
and get some of that extra cool stuff out of it

532
00:38:08,071 --> 00:38:10,112
on top of it because the fact that we're just streaming it

533
00:38:10,152 --> 00:38:10,712
in afterwards.

534
00:38:10,752 --> 00:38:15,373
So one thing about doing this per joint motion

535
00:38:15,413 --> 00:38:20,054
that is really cool is the fact that you can actually

536
00:38:20,094 --> 00:38:20,735
get smoother.

537
00:38:21,317 --> 00:38:25,520
natural arts of motion in this way. I'll describe what I'm talking about here.

538
00:38:26,861 --> 00:38:29,782
So again, we'll go back to like a vertice that's

539
00:38:29,903 --> 00:38:30,823
right on one of those markers.

540
00:38:31,804 --> 00:38:38,848
On the lower left, there's a blue dot representing a pose. If you're right on that pose, you know,

541
00:38:39,889 --> 00:38:41,870
your face is going to match up perfectly with that pose.

542
00:38:41,890 --> 00:38:45,052
Along the way to that other pose, along at the top,

543
00:38:45,932 --> 00:38:46,392
you're going to get a

544
00:38:48,153 --> 00:38:52,055
another perfect match of a pose. But when you're in between there, if you don't have poses

545
00:38:53,135 --> 00:38:58,998
specifically trained all along the whole way, it's going to be basically interpolating between the

546
00:38:59,439 --> 00:39:04,661
most relevant shapes nearby in space. So you can see it's influenced a little bit more where it's

547
00:39:04,721 --> 00:39:10,484
curving over to the right. And the shapes look good in themselves, but if you didn't have quite

548
00:39:10,544 --> 00:39:14,426
enough shapes, it's going to take a slightly different path while it's blending between

549
00:39:14,466 --> 00:39:17,087
those shapes. And so having these joints...

550
00:39:19,425 --> 00:39:23,308
pulling back to the actual 3D tracked locators that you have,

551
00:39:23,649 --> 00:39:26,531
just basically warps the whole thing, gives it a nudge to it,

552
00:39:26,831 --> 00:39:31,354
so it's 100% accurate, if your track is accurate, at each of those vertices.

553
00:39:31,394 --> 00:39:35,458
And so, everything in between just goes for a ride with a nice fall-off.

554
00:39:35,538 --> 00:39:37,979
So, here's just an image of what I'm talking about.

555
00:39:38,800 --> 00:39:41,422
Just to kind of go back and forth here,

556
00:39:42,063 --> 00:39:45,025
you can see that they're not quite aligned with the surface.

557
00:39:45,825 --> 00:39:46,446
Here they are.

558
00:39:47,044 --> 00:39:49,966
And so that's just, we actually have a control per joint

559
00:39:49,986 --> 00:39:51,507
that allows the animator to say,

560
00:39:51,527 --> 00:39:54,509
well, the track was really doing something weird here.

561
00:39:54,549 --> 00:39:55,971
It's solving well, but I don't really want

562
00:39:55,991 --> 00:39:57,292
to follow the track there, but you know,

563
00:39:57,312 --> 00:39:59,133
the brows, I'm getting a lot more dynamic motion on that,

564
00:39:59,153 --> 00:40:03,497
so I'm just gonna have them attract to the markers

565
00:40:03,537 --> 00:40:06,339
and just get that more fluid dynamic.

566
00:40:06,359 --> 00:40:08,160
You know, the face moves around very fluidly,

567
00:40:08,200 --> 00:40:11,063
and having that directly tied to the actor's motion

568
00:40:11,523 --> 00:40:14,465
is really important for realism.

569
00:40:15,326 --> 00:40:15,986
So I'll just play this.

570
00:40:22,823 --> 00:40:26,785
So this isn't in the camera view, so it was used in the cut scene,

571
00:40:26,805 --> 00:40:30,526
so you'll see a hand coming in here that kind of snaps into place awkwardly.

572
00:40:30,566 --> 00:40:37,230
But you get the idea that you can see that those dots stay with the face,

573
00:40:37,270 --> 00:40:38,290
whereas this earlier version...

574
00:40:38,310 --> 00:40:43,513
Hold on, did I click the right one? Yeah.

575
00:40:44,313 --> 00:40:44,773
It's right here.

576
00:40:45,033 --> 00:40:47,534
So with this version, you can see that as he tilts his head down,

577
00:40:47,554 --> 00:40:50,896
there's a gap between his face and the actual 3D position

578
00:40:50,916 --> 00:40:51,856
that represents it.

579
00:40:52,924 --> 00:40:58,546
an inaccuracy. We didn't have enough poses to completely make it that tight, but that little

580
00:40:59,367 --> 00:41:00,867
nudge that we gave it, you know, makes it

581
00:41:02,388 --> 00:41:03,748
as accurate again as your track is.

582
00:41:03,808 --> 00:41:09,151
So that brings me to the fact that we used a 3D track versus a 2D track.

583
00:41:11,672 --> 00:41:16,434
The 3D track does require four cameras, you know, to get really good coverage we have found.

584
00:41:18,402 --> 00:41:21,983
But the benefit of the 3D track is that it allows you to verify your data.

585
00:41:22,063 --> 00:41:25,023
We have, you know, the actual track points, they're right next to the face.

586
00:41:25,083 --> 00:41:29,745
And as your face is solving, those should be lined up at least very closely.

587
00:41:30,285 --> 00:41:34,066
So you can, you know, tell right off if you're not solving very well.

588
00:41:34,086 --> 00:41:39,388
And then, you know, the additional benefit of just being able to warp right to those marking points

589
00:41:39,448 --> 00:41:46,690
and give you just that extra level of a detailed fluid motion with just taking the normal natural arcs.

590
00:41:47,852 --> 00:41:48,492
right from the actor.

591
00:41:51,133 --> 00:41:55,316
Lastly, and this is pretty key, I think, is the fact that

592
00:41:55,976 --> 00:41:58,557
I mean, the lips are the most moving part of the face

593
00:41:58,617 --> 00:42:00,378
and it actually moves in Z.

594
00:42:00,398 --> 00:42:02,379
You know, you can pull your lips out

595
00:42:02,399 --> 00:42:03,700
and your jaw forward and all this.

596
00:42:05,340 --> 00:42:07,441
With the 3D track, you can really take advantage

597
00:42:07,461 --> 00:42:09,262
of that extra dimensionality in your drivers

598
00:42:10,023 --> 00:42:12,684
so that, there's me making some funny faces,

599
00:42:12,724 --> 00:42:14,265
but it kind of shows like,

600
00:42:15,405 --> 00:42:16,646
roughly, you can make...

601
00:42:17,019 --> 00:42:20,781
the same pose from a front view and have it look very similar.

602
00:42:21,661 --> 00:42:24,703
When I'm pushing my lips out, one of them really tightening them, but

603
00:42:25,183 --> 00:42:27,944
kind of keeping that same rounded shape.

604
00:42:28,685 --> 00:42:32,406
Just moving your jaw forward even can do the same thing. And here I'm actually

605
00:42:32,807 --> 00:42:35,828
rolling my lip in and pushing my jaw forward and rolling it out.

606
00:42:36,448 --> 00:42:39,290
From the front view there's hardly a difference noticeable,

607
00:42:40,190 --> 00:42:43,291
which means if these two different shapes are getting triggered

608
00:42:43,812 --> 00:42:45,352
by your same drivers because they look the same,

609
00:42:46,539 --> 00:42:52,461
and 2D space, you're going to get wonky interpolation in your shapes because it's applying shapes that

610
00:42:52,501 --> 00:42:55,202
that are irrelevant to the actual pose.

611
00:42:59,463 --> 00:43:01,764
So that brings us on to the facial rig.

612
00:43:01,784 --> 00:43:08,446
So I was mentioning that we have key markers. They're carefully

613
00:43:10,307 --> 00:43:14,869
picked as, all the drivers are carefully picked per region to use the most relevant

614
00:43:17,567 --> 00:43:22,949
either distances or vector directions of movement of those markers so that

615
00:43:25,970 --> 00:43:29,372
basically through motion studies of seeing where the face moves independently

616
00:43:29,412 --> 00:43:32,993
and some experimentation to find the best combination of drivers.

617
00:43:35,434 --> 00:43:40,376
But once we have our key drivers, those points in the face that basically move the most,

618
00:43:40,936 --> 00:43:43,177
we can hook controls up to there and you can just move them around

619
00:43:43,617 --> 00:43:45,298
and you're actually going to get all that interpolation.

620
00:43:46,000 --> 00:43:47,981
between the shapes as you pull them around.

621
00:43:48,001 --> 00:43:49,221
I'll show you an example in a second.

622
00:43:49,661 --> 00:43:51,021
Then of course the fine level of control

623
00:43:51,622 --> 00:43:53,922
for each individual joint, if you want to expose that.

624
00:43:54,482 --> 00:43:56,323
We have the UI, as I described,

625
00:43:56,343 --> 00:43:59,743
that had a picture per scan.

626
00:43:59,803 --> 00:44:01,344
It was one of the pictures that we actually

627
00:44:01,364 --> 00:44:02,784
taken right from the moment of the scan.

628
00:44:03,224 --> 00:44:06,125
And we have a UI that exposes those with sliders.

629
00:44:06,525 --> 00:44:08,905
You can click on any picture and it'll give you sliders.

630
00:44:09,005 --> 00:44:12,226
And we can pick however many regions of the face

631
00:44:12,246 --> 00:44:14,027
we want to break it up into with sliders.

632
00:44:14,469 --> 00:44:17,491
It's basically just moving the joints in that region.

633
00:44:18,611 --> 00:44:21,153
And it moves them in a normalized way such that if you moved them,

634
00:44:21,553 --> 00:44:23,574
if you moved all the sliders to the right for that face,

635
00:44:23,914 --> 00:44:28,177
all the joints would exactly match the positions of those markers as they were in the scan.

636
00:44:28,197 --> 00:44:31,659
So I'll show you what that looks like here in a second.

637
00:44:31,679 --> 00:44:36,221
But again, if you're, because it's tied directly to this deformer, this pose-based deformer,

638
00:44:36,542 --> 00:44:41,845
and you go from pose A to pose B, I mean, it's a big distance of travel between those poses.

639
00:44:42,248 --> 00:44:43,569
what you get for free along the way

640
00:44:43,629 --> 00:44:45,529
is any other relevant shape triggering,

641
00:44:45,970 --> 00:44:47,550
because it's all in there in the post space.

642
00:44:47,590 --> 00:44:51,111
And so maybe you're doing something with your jaw open,

643
00:44:51,831 --> 00:44:53,672
it comes to a point where it's ready to compress.

644
00:44:54,692 --> 00:44:56,453
If that shape information is in there,

645
00:44:56,473 --> 00:44:57,593
it's gonna wait to the right time

646
00:44:57,753 --> 00:44:59,614
before those lips touch and start to compress

647
00:44:59,714 --> 00:45:02,695
and you get that stuff for free, which is very cool.

648
00:45:04,255 --> 00:45:06,836
So here's an example of the UI I was describing.

649
00:45:06,856 --> 00:45:10,717
We're able to slide those regions around

650
00:45:10,757 --> 00:45:11,758
and kind of pick.

651
00:45:12,248 --> 00:45:13,609
any part of the face from any of the picture,

652
00:45:13,629 --> 00:45:14,970
oh, I like what the eyes are doing here.

653
00:45:14,990 --> 00:45:16,612
I want the left side of the mouth to do

654
00:45:16,632 --> 00:45:17,412
what this one's over here.

655
00:45:17,432 --> 00:45:18,253
This gives a little,

656
00:45:18,273 --> 00:45:23,217
this gives a way of fine-tuning what you want

657
00:45:25,359 --> 00:45:26,600
if you're starting from scratch

658
00:45:26,620 --> 00:45:28,301
or if you're just tweaking motion capture.

659
00:45:28,321 --> 00:45:31,584
So with this, you're,

660
00:45:35,828 --> 00:45:38,970
you're able to slide an attribute

661
00:45:38,990 --> 00:45:41,212
that says that you want to have it take over.

662
00:45:41,832 --> 00:45:49,138
your motion capture so that basically as you dial in any shape it dials away the motion

663
00:45:49,158 --> 00:45:52,340
capture so you're not fighting with it or you can just set it to add on top.

664
00:45:52,841 --> 00:45:58,085
So here I'm just pulling around some of those gross motion controls and you can kind of

665
00:45:58,125 --> 00:46:02,708
see how like moving the chin makes the lip compress up or I'll just play that again it

666
00:46:02,748 --> 00:46:07,292
goes pretty quickly there but I'm not rotating anything I'm just translating them around

667
00:46:07,312 --> 00:46:10,695
and just because if you pull it under the mouth it's going to.

668
00:46:11,102 --> 00:46:13,203
you know, curl the lip in if you pull it over the front.

669
00:46:13,643 --> 00:46:16,405
You can try to, you know, pull the jaw forward.

670
00:46:16,445 --> 00:46:18,986
And it's just, yeah, it's fun to play with it

671
00:46:19,026 --> 00:46:20,307
when you actually see the shapes kicking in.

672
00:46:22,128 --> 00:46:23,789
So I mentioned exposing some of those attributes.

673
00:46:25,810 --> 00:46:29,412
I'll just jump into some of the ones

674
00:46:29,432 --> 00:46:30,252
that I haven't mentioned.

675
00:46:31,473 --> 00:46:33,614
Sticky lips, eye drag.

676
00:46:33,634 --> 00:46:35,415
You can kind of dial in how much eye drag

677
00:46:35,435 --> 00:46:36,156
that the controls have.

678
00:46:36,176 --> 00:46:37,997
How are we doing on time?

679
00:46:38,017 --> 00:46:40,718
Anybody?

680
00:46:43,671 --> 00:46:44,031
37 minutes in?

681
00:46:44,771 --> 00:46:45,052
OK.

682
00:46:46,672 --> 00:46:58,017
So this brings me to our wrinkle or crease maps that basically

683
00:46:58,097 --> 00:47:02,498
we're using the compression and stretch independently

684
00:47:02,558 --> 00:47:04,739
from a subset of markers on the face.

685
00:47:06,056 --> 00:47:09,137
that drive basically an alpha, a dynamic alpha.

686
00:47:09,177 --> 00:47:10,937
So wherever there's compression, it's gonna light up red.

687
00:47:11,177 --> 00:47:13,297
Wherever there's stretch, it's gonna light up green,

688
00:47:13,337 --> 00:47:16,938
and that will reveal a wrinkle or crease map

689
00:47:17,078 --> 00:47:18,258
or a stretch map.

690
00:47:20,859 --> 00:47:23,419
So what this does is you can,

691
00:47:24,459 --> 00:47:27,460
with that subset of vertices that you're using,

692
00:47:28,180 --> 00:47:31,220
you basically are defining a low-res cage

693
00:47:31,840 --> 00:47:34,281
that you can art direct where you want things to.

694
00:47:34,748 --> 00:47:36,148
to always trigger together.

695
00:47:36,469 --> 00:47:38,429
If you used every vertex in the face, it might be noisy.

696
00:47:38,449 --> 00:47:40,770
You'll have a red and green little dot.

697
00:47:40,830 --> 00:47:44,112
So this is not only good for that,

698
00:47:44,132 --> 00:47:47,533
but it makes for faster calculations, smooth falloff.

699
00:47:47,553 --> 00:47:51,615
The way we create our maps is really interesting

700
00:47:51,675 --> 00:47:54,716
is that if you look at streamed data from scans,

701
00:47:55,617 --> 00:47:57,618
if somebody's going through a range of facial motions

702
00:47:57,658 --> 00:47:58,798
and you look at it in UV space,

703
00:47:58,838 --> 00:48:00,119
where you're just seeing the color changing

704
00:48:00,159 --> 00:48:02,220
and changing and the geometry's not moving.

705
00:48:02,604 --> 00:48:08,145
What's really interesting is that 95% of those wrinkles are just coming on in the same spot.

706
00:48:08,625 --> 00:48:12,306
And this is why as you get older you start to get wrinkles,

707
00:48:12,586 --> 00:48:15,447
because it's the repeated wrinkling happening in the same spot.

708
00:48:17,087 --> 00:48:21,728
So that means we can just take the aspects that we wanted from the various maps

709
00:48:21,808 --> 00:48:27,089
and just kind of make a single map out of a combination of where those wrinkles and creases are coming out.

710
00:48:27,609 --> 00:48:31,390
And of course you can just go in and use a normal painting.

711
00:48:32,311 --> 00:48:36,254
Map to kind of add any that you want in addition if you're trying to make them older

712
00:48:41,578 --> 00:48:44,760
So here's an example that you've seen in

713
00:48:47,442 --> 00:48:50,885
In our highlights video, this is a Reggie Rowe character, and he's

714
00:48:52,106 --> 00:48:58,270
You see a corner of his eyes and his forehead you know those wrinkles kind of kicking on dynamically there

715
00:48:59,891 --> 00:49:01,032
And this is what he looks like with just

716
00:49:02,094 --> 00:49:06,338
Lambert and just the skin cluster and post-space deformers.

717
00:49:06,358 --> 00:49:09,681
So there's a little bit of that stuff there, but not that kind of detail coming out.

718
00:49:10,721 --> 00:49:18,288
And this is what the vertex color being triggered by that compression and stretch looks like.

719
00:49:18,308 --> 00:49:21,490
So when he squints the one eye, you can see that becomes red and the other one goes to

720
00:49:21,530 --> 00:49:29,056
green and gives just a nice dynamic way of having them trigger right where they need

721
00:49:29,076 --> 00:49:29,196
to be.

722
00:49:31,790 --> 00:49:33,491
So which brings us to actually getting the stuff

723
00:49:33,911 --> 00:49:35,953
into the engine in real time.

724
00:49:35,973 --> 00:49:39,876
Oh, and that wrinkle map is also applied

725
00:49:39,916 --> 00:49:42,258
as a plugin in Maya, so when you're moving the rig around,

726
00:49:42,278 --> 00:49:43,700
you see those kick in as well so that you know

727
00:49:43,720 --> 00:49:45,581
that they correspond to what's happening in the engine.

728
00:49:46,942 --> 00:49:48,263
So as far as the real-time performance,

729
00:49:49,484 --> 00:49:51,106
mentioned that we have the joint animation,

730
00:49:51,606 --> 00:49:55,229
GPU decompression, one aspect that something like Maya

731
00:49:55,309 --> 00:49:56,310
and these other programs do.

732
00:49:58,503 --> 00:50:00,543
for free is to recalculate the normals

733
00:50:00,603 --> 00:50:03,224
if you have deformations moving the vertices around.

734
00:50:03,924 --> 00:50:04,744
If they didn't do that,

735
00:50:04,784 --> 00:50:07,445
you might not see the details actually come out

736
00:50:07,465 --> 00:50:09,206
because it's not reacting to light

737
00:50:09,286 --> 00:50:10,206
at the angles that it should.

738
00:50:10,266 --> 00:50:12,646
So that's one of the things that had to be added in there.

739
00:50:13,807 --> 00:50:17,108
Some seam fixing, we had a shader applying the parts

740
00:50:17,128 --> 00:50:18,268
that are streaming of the face.

741
00:50:20,748 --> 00:50:23,129
And a different shader, very similar,

742
00:50:23,149 --> 00:50:26,010
but just wasn't streaming for the edge around it.

743
00:50:26,350 --> 00:50:28,090
Kind of ran into a funny little snag that the...

744
00:50:28,724 --> 00:50:34,428
round off that Maya was using for its skin weights wasn't matching what the round off in the GPU was so it was giving this little

745
00:50:34,608 --> 00:50:37,510
seam that once we corrected that went away.

746
00:50:42,214 --> 00:50:43,395
Some additional considerations

747
00:50:45,316 --> 00:50:46,497
Tracking issues

748
00:50:47,558 --> 00:50:47,958
we really

749
00:50:47,978 --> 00:50:49,700
kind of

750
00:50:50,520 --> 00:50:52,442
learning experience from this round was that

751
00:50:52,962 --> 00:50:54,283
it's probably beneficial if we

752
00:50:54,723 --> 00:51:01,166
have a rig version that's of the actor before any retargeting that's, that you're, the tracking

753
00:51:01,206 --> 00:51:05,488
company can, if you're doing it outsourced, they can apply it immediately to the actor rig

754
00:51:06,029 --> 00:51:11,211
and get immediate results to see if there's anything wrong, maybe camera calibration or

755
00:51:12,732 --> 00:51:17,414
misalignment from the stabilization, so they can iterate on that before it goes through the rest

756
00:51:17,454 --> 00:51:22,617
of the pipeline, gets applied into a scene, and you know, gets animated on before we realize that

757
00:51:23,017 --> 00:51:24,358
it's really not behaving well.

758
00:51:26,637 --> 00:51:30,359
Our target process, I think, you know, it has room for some improvement as well.

759
00:51:30,439 --> 00:51:36,722
So, that's something that I think, you know, led to a little bit of hand animation tweaking

760
00:51:36,742 --> 00:51:39,283
in spots where we probably didn't need to.

761
00:51:40,083 --> 00:51:42,865
We infer the jaw motion from a couple points on the face.

762
00:51:42,905 --> 00:51:46,146
We basically are aiming a joint at the center point of those.

763
00:51:46,486 --> 00:51:48,067
So, it's moving the jaw with it.

764
00:51:48,347 --> 00:51:51,088
I mean, the skin does move no matter what you do.

765
00:51:51,149 --> 00:51:53,590
So, it's doing a decent job of.

766
00:51:54,508 --> 00:51:57,670
implying where the teeth are, but we just give an extra offset control so you can kind

767
00:51:57,690 --> 00:52:04,896
of fine tune and align those as needed afterwards if it looks like it's not aligned correctly.

768
00:52:07,218 --> 00:52:11,321
Again, additional attention to the deformation point where it connects between your post-based

769
00:52:11,341 --> 00:52:13,723
deformers and maybe regular skin clusters and such.

770
00:52:18,186 --> 00:52:19,607
Which brings me to the end.

771
00:52:19,627 --> 00:52:22,370
I don't know if Adrian Bentley is here.

772
00:52:22,430 --> 00:52:23,891
He's had his own talk.

773
00:52:24,522 --> 00:52:29,884
this week. Oh, hey, Adrian. So he's going to be around, hopefully come up and help answer

774
00:52:29,924 --> 00:52:40,087
questions related to the deep engine programming and plugin writing that he implemented. So

775
00:52:40,567 --> 00:52:45,929
he's a co-contributor and a big shout out to him for all the work he did, as well as

776
00:52:47,349 --> 00:52:52,931
Josh Scherzold, excuse me, Josh Scherzold, who did additional plugin and programming work.

777
00:52:54,429 --> 00:52:56,591
Shwe Liu and Steven White as well.

778
00:52:57,651 --> 00:52:58,592
So thank you.

779
00:52:59,073 --> 00:53:00,334
Cthulhu thanks you.

780
00:53:00,654 --> 00:53:04,998
And special thanks to the GDC committee and RUGWE.

781
00:53:11,523 --> 00:53:14,766
Feel free to contact me through the email or I just got Twitter on there.

782
00:53:14,826 --> 00:53:20,031
So actually you guys are the type of people I would actually have an account for to respond

783
00:53:20,071 --> 00:53:20,291
back.

784
00:53:20,411 --> 00:53:21,412
So feel free to contact me.

785
00:53:21,432 --> 00:53:22,073
Thank you.

786
00:53:22,093 --> 00:53:22,273
Thank you.

787
00:53:23,260 --> 00:53:29,163
Connect with me that way. We do have the wrap-up room afterwards for additional questions, but we have time for questions now

788
00:53:34,285 --> 00:53:37,266
And just go ahead and step up to the microphone as you have a question

789
00:53:38,306 --> 00:53:42,548
Yeah, it looked fantastic. I was just wondering if you

790
00:53:42,768 --> 00:53:49,971
Simulated or considered simulating any of the muscles underneath you said you talked about simulating the skin. Yeah, that's that's something that

791
00:53:51,110 --> 00:53:58,051
It's certainly done in feature films that, like this example,

792
00:53:58,071 --> 00:54:02,972
they have this amazing tissue simulation that starts from muscle and bone

793
00:54:02,992 --> 00:54:04,212
and all these layers are simulated.

794
00:54:04,232 --> 00:54:09,033
It's just we don't have a giant render farm to be kicking these things off

795
00:54:09,113 --> 00:54:11,554
and it's really a matter of time and iteration.

796
00:54:11,614 --> 00:54:14,974
And animators will be able to tweak the performances

797
00:54:14,994 --> 00:54:16,575
without having to wait to see the results.

798
00:54:16,595 --> 00:54:18,855
So we didn't go that far.

799
00:54:19,604 --> 00:54:23,648
you could certainly infer that stuff after the fact and you had a farm to kind of go

800
00:54:24,048 --> 00:54:24,869
calculate that stuff.

801
00:54:25,729 --> 00:54:26,230
You certainly could.

802
00:54:26,690 --> 00:54:27,251
Yes?

803
00:54:27,671 --> 00:54:33,255
How much work did the animators have to do after the facial motion capture?

804
00:54:33,916 --> 00:54:40,982
Yes, the question was how much animation was required after the whole process was done.

805
00:54:42,643 --> 00:54:45,225
We didn't save a lot of time in the end for facial tweaking.

806
00:54:51,577 --> 00:54:54,398
There was definitely a good amount.

807
00:54:56,378 --> 00:54:59,819
Like I said, we did have tracking and retargeting issues.

808
00:55:00,999 --> 00:55:04,860
It's easier sometimes just to go through and tweak them and then to figure out.

809
00:55:04,880 --> 00:55:09,301
We'll have to postmortem to check where some of those errors came from.

810
00:55:11,621 --> 00:55:14,022
Surprisingly, we didn't have a ton of that that we had to do.

811
00:55:15,398 --> 00:55:19,181
considering we did it on the cheap, basically,

812
00:55:19,221 --> 00:55:21,543
compared to a feature film approach.

813
00:55:21,783 --> 00:55:29,129
But I can't tell you the amount of man hours that went into it,

814
00:55:29,169 --> 00:55:32,211
but it was a very tight schedule at the end,

815
00:55:32,231 --> 00:55:36,254
where animators were just going in shot for shot,

816
00:55:36,294 --> 00:55:42,039
just kind of tweaking and pumping them out.

817
00:55:42,059 --> 00:55:44,120
So there was that to it, I guess.

818
00:55:44,861 --> 00:55:45,061
What?

819
00:55:46,266 --> 00:55:48,447
What facial capture system did you guys use?

820
00:55:48,467 --> 00:55:52,248
Because it looks like it's doing a little bit of 3D and a little bit of 2D with the eyes or something.

821
00:55:52,388 --> 00:55:53,509
Can you go over that?

822
00:55:54,329 --> 00:55:56,470
The facial, the actual capture process?

823
00:55:56,490 --> 00:55:56,910
The helmet.

824
00:55:57,070 --> 00:55:57,470
The helmet.

825
00:55:58,170 --> 00:56:05,853
The helmet is, again, it's a four camera setup where you actually have to use tracking software

826
00:56:05,873 --> 00:56:09,334
that looks through those cameras and tries to triangulate the position and per frame it.

827
00:56:09,735 --> 00:56:14,076
It follows from the different views to figure out where those points are.

828
00:56:14,536 --> 00:56:14,776
Awesome.

829
00:56:15,963 --> 00:56:17,244
companies that specialize in that.

830
00:56:18,805 --> 00:56:23,969
Certainly in the film industry, we use digital domain. It was our company, we used their stage and their helmets.

831
00:56:24,590 --> 00:56:28,814
And so they did the tracking for us and would deliver us the marker points

832
00:56:29,975 --> 00:56:31,216
that we would apply into our rigs.

833
00:56:34,659 --> 00:56:34,839
Yes.

834
00:56:37,541 --> 00:56:43,506
Did you have two different runtime rigs for the cinematics versus the gameplay sections? And if so,

835
00:56:45,071 --> 00:56:47,512
roughly what was the joint count for the face for those rigs?

836
00:56:49,012 --> 00:56:52,373
The marker joints themselves, if you don't count the eyes and

837
00:56:52,734 --> 00:56:57,755
additional helper joints on the neck and such, we had 168 markers.

838
00:57:01,557 --> 00:57:05,358
We could certainly have implemented some of that within the game,

839
00:57:05,978 --> 00:57:08,959
but we really decided that you're really not up and close.

840
00:57:09,799 --> 00:57:11,840
to really need to do all that additional streaming.

841
00:57:12,341 --> 00:57:14,142
We have an open world game, very fast travel.

842
00:57:14,783 --> 00:57:16,484
So we didn't really feel that was an area

843
00:57:16,504 --> 00:57:19,206
that we really had to take up computation power.

844
00:57:21,427 --> 00:57:22,909
But since we do have the joint motion,

845
00:57:22,949 --> 00:57:25,150
that was perfect for just the LOD for the rest of the game.

846
00:57:25,510 --> 00:57:26,691
But you could drive it with the same system.

847
00:57:27,852 --> 00:57:28,933
Do you remember how many joints you had

848
00:57:28,973 --> 00:57:30,034
for the real-time character though?

849
00:57:30,294 --> 00:57:30,694
Oh, the same, 168.

850
00:57:30,854 --> 00:57:31,675
168 for the face, okay, thank you.

851
00:57:31,695 --> 00:57:31,755
Yes?

852
00:57:37,846 --> 00:57:40,588
Did you use certain markers to stabilize the face?

853
00:57:41,709 --> 00:57:43,090
For the stabilization process

854
00:57:44,411 --> 00:57:46,472
We have the stabilization from just our fax

855
00:57:47,052 --> 00:57:49,854
Session that we need to get those aligned and then there's a stabilization

856
00:57:50,875 --> 00:57:52,496
process that digital domain did

857
00:57:53,557 --> 00:57:57,219
using the motion capture to try to get that relative to the default and

858
00:57:57,239 --> 00:58:01,742
They have their own proprietary method of doing that

859
00:58:02,876 --> 00:58:06,317
constrained spring-based system that tries to keep it aligned.

860
00:58:06,397 --> 00:58:11,499
But I'm not privy to, you know, answer any of the details on that.

861
00:58:13,319 --> 00:58:13,479
Yes?

862
00:58:14,080 --> 00:58:17,521
Yeah, I was just wondering, as far as using this sort of system

863
00:58:17,601 --> 00:58:21,062
for instances where your characters are quite a bit different from the actor,

864
00:58:21,602 --> 00:58:23,303
like what are some of the challenges you see with that?

865
00:58:23,423 --> 00:58:26,484
I know you've mentioned retargeting being an issue here

866
00:58:26,544 --> 00:58:28,204
where Troy Baker looks a lot like your character.

867
00:58:28,244 --> 00:58:29,905
What if it was a creature or something of that nature?

868
00:58:29,925 --> 00:58:31,365
Yeah, you could certainly take...

869
00:58:32,836 --> 00:58:34,237
Is it one minute left?

870
00:58:34,257 --> 00:58:37,480
All right, so you might have to pull other questions into the room after this, but

871
00:58:37,881 --> 00:58:39,022
so your question was,

872
00:58:40,022 --> 00:58:43,406
if you retarget really far away from what your actual

873
00:58:43,466 --> 00:58:46,028
game character was, we had a pretty close correspondence

874
00:58:46,068 --> 00:58:47,629
with our main character, but

875
00:58:47,930 --> 00:58:49,591
let's say we wanted to animate a giraffe, you know,

876
00:58:50,692 --> 00:58:54,355
you could, instead of retargeting all the motion capture and all the

877
00:58:54,475 --> 00:58:56,838
shapes, make a correspondence between

878
00:58:57,518 --> 00:58:59,760
your giraffe for each pose.

879
00:59:00,378 --> 00:59:08,403
And even if it's a combination of other blends to give you that pose, that correlation is what can give you a path for an algorithm for retargeting those.

880
00:59:09,403 --> 00:59:14,507
If you do have an exact pose, one for one, that nicely blends on the other end,

881
00:59:15,347 --> 00:59:20,810
the output weights that are from those deformers can be just directly fed into the other one.

882
00:59:21,451 --> 00:59:23,332
And it would drive the face at the same time.

883
00:59:24,413 --> 00:59:24,633
Thank you.

884
00:59:33,564 --> 00:59:34,582
Well, thank you very much for your time.

