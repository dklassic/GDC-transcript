1
00:00:05,570 --> 00:00:06,930
My name is Omar. I'm from NVIDIA.

2
00:00:09,271 --> 00:00:14,412
I'm here to talk about something that I've been working on for the past three and a half years.

3
00:00:14,432 --> 00:00:19,133
The title of my talk is, This Machine Has No Brain, Can It Borrow Yours?

4
00:00:20,574 --> 00:00:23,835
Does anyone have any idea what this is in reference to?

5
00:00:23,855 --> 00:00:25,915
Has anyone seen these signs before, IRL?

6
00:00:27,576 --> 00:00:38,946
This is actually from Fallout, but these signs are whenever you have a machine that can kill you, this sign usually is in some form next to it.

7
00:00:41,308 --> 00:00:46,353
And I'm here to give a warning to everyone, because we're facing a crisis we don't yet see.

8
00:00:47,434 --> 00:00:51,198
In about 10 years from now, we're going to have robots everywhere.

9
00:00:52,059 --> 00:00:55,943
So we already have some form of robots in our personal space, right?

10
00:00:56,564 --> 00:01:01,189
We've had, for example, cleaning robots hang out for the past 10 years.

11
00:01:03,111 --> 00:01:06,595
But now we're beginning to see some changes in...

12
00:01:08,654 --> 00:01:15,236
the effect and the amount of robots that we have in our personal lives or doing logistics for us.

13
00:01:15,636 --> 00:01:19,537
And that's mostly because we have amazing reach nowadays, right?

14
00:01:19,837 --> 00:01:29,141
We have innovations in motors and in microcontrollers that can do stabilization.

15
00:01:29,521 --> 00:01:31,541
We also have innovation in dexterity.

16
00:01:31,621 --> 00:01:36,563
So grippers that used to be something that was very specialized.

17
00:01:37,584 --> 00:01:44,809
are now, we can now manufacture them pretty much in mass manufacturing, even for like the most specialized tasks.

18
00:01:45,070 --> 00:01:53,917
So you would have rubber and silicone in injection molding, which is a mass manufacturing technique,

19
00:01:53,937 --> 00:01:57,580
but now it's very common in robotics. So you can do soft robots.

20
00:01:58,040 --> 00:01:58,961
But you can also have...

21
00:02:00,622 --> 00:02:05,023
Robots with tendons that have a very high gear ratio.

22
00:02:05,403 --> 00:02:07,844
Over here, you are looking at a robot

23
00:02:07,884 --> 00:02:10,265
from the Cornell Organic Robotics Laboratory

24
00:02:10,785 --> 00:02:12,106
that is holding a can, right?

25
00:02:13,026 --> 00:02:16,527
It is very accurate for a machine

26
00:02:16,567 --> 00:02:18,028
to be holding a can without crushing it.

27
00:02:19,148 --> 00:02:21,629
This is done pretty much with custom motors.

28
00:02:21,669 --> 00:02:24,090
This is, you know, like we are definitely going

29
00:02:24,110 --> 00:02:25,771
in the direction of precise machinery.

30
00:02:27,071 --> 00:02:35,217
And we also have AI, as you well know, and the infrastructure is also going pretty much to personalized robots.

31
00:02:35,257 --> 00:02:46,325
We have data centers at the edge, which means you might have a data center in your next-to-your-traffic light, or just by the base station of your mobile phone.

32
00:02:48,407 --> 00:02:52,029
The economy is pretty much not going to allow it to be anything else.

33
00:02:52,329 --> 00:02:56,732
We are headed towards a completely autonomous future in several fields.

34
00:02:56,912 --> 00:02:58,013
And this is a problem.

35
00:02:58,953 --> 00:03:01,495
This is a really big problem, because what's missing in this chart

36
00:03:02,075 --> 00:03:05,657
is that we don't have good design for any of these robots.

37
00:03:05,857 --> 00:03:07,398
So no one's designing their behavior.

38
00:03:07,919 --> 00:03:10,300
And if they're going to be interacting with the elderly,

39
00:03:10,721 --> 00:03:13,783
in health care, with food preparation, with delivery,

40
00:03:14,003 --> 00:03:16,605
and logistics, they all need to be designed

41
00:03:16,645 --> 00:03:18,146
to work around humans.

42
00:03:18,246 --> 00:03:20,547
And so far, it's a hack.

43
00:03:20,808 --> 00:03:23,610
We don't have a stack to actually use

44
00:03:23,670 --> 00:03:25,971
robots interacting with humans.

45
00:03:26,432 --> 00:03:27,833
And we need to think about how we're doing it.

46
00:03:29,254 --> 00:03:32,556
And my proposal over here is that simulation

47
00:03:32,616 --> 00:03:33,516
is the right way of doing this.

48
00:03:36,615 --> 00:03:37,815
So let's start with a basic question.

49
00:03:37,855 --> 00:03:40,936
I know you all work in VR, but how many people here

50
00:03:41,276 --> 00:03:41,976
know these images?

51
00:03:45,816 --> 00:03:48,737
These were the world's first ray traced images.

52
00:03:48,777 --> 00:03:51,978
They were created in 1979 by Turner Whitted.

53
00:03:53,418 --> 00:03:54,858
That was a paper that he released

54
00:03:54,898 --> 00:03:56,018
while he was at Bell Labs.

55
00:03:56,959 --> 00:03:59,379
He called these very imaginatively

56
00:03:59,459 --> 00:04:01,519
figured six, seven, and eight.

57
00:04:03,640 --> 00:04:08,642
They were part of a paper where he's describing also how ray tracing works, which he described

58
00:04:08,662 --> 00:04:10,623
in figures one and two.

59
00:04:11,743 --> 00:04:13,344
One is a description of the actual algorithm.

60
00:04:13,404 --> 00:04:19,706
The other is a description of Snell's law, which is a way to describe how light moves

61
00:04:19,746 --> 00:04:21,247
through different media.

62
00:04:23,702 --> 00:04:31,746
We have not gone to simulate actual optics and therefore have photorealistic rendering

63
00:04:31,786 --> 00:04:32,786
until very recently.

64
00:04:32,926 --> 00:04:39,690
If you look quickly at the evolution of computer graphics, we had our first popular 3D engine

65
00:04:39,710 --> 00:04:41,370
that can run in real-time in 1993.

66
00:04:42,211 --> 00:04:44,772
On the other hand, we had our first ray-traced film in 2006.

67
00:04:44,912 --> 00:04:47,773
If you ask the people at Pixar...

68
00:04:49,389 --> 00:04:54,396
They would say that a lot of the ray tracing there that I would consider fully ray traced was a hack.

69
00:04:54,536 --> 00:04:58,842
I consider it fully ray traced. They actually managed to simulate a whole model of light.

70
00:05:00,084 --> 00:05:04,109
But something happened along the way. We have this now, right?

71
00:05:05,730 --> 00:05:10,573
With GPUs, we can suddenly do things like create game engines

72
00:05:10,693 --> 00:05:13,594
which approach photorealistic simulation.

73
00:05:14,434 --> 00:05:16,475
In this case, we are looking at Boy and His Kite,

74
00:05:16,656 --> 00:05:19,017
demoed by Epic Games, which did a lot of photogrammetry

75
00:05:19,397 --> 00:05:20,798
around the environment. But nowadays,

76
00:05:21,638 --> 00:05:24,980
when we have reached real-time ray tracing in GPUs,

77
00:05:25,400 --> 00:05:28,161
we can move between what is basically this all the time

78
00:05:29,602 --> 00:05:30,842
to this, right?

79
00:05:30,862 --> 00:05:34,144
I am going to do that again, because I love that.

80
00:05:38,101 --> 00:05:38,941
Apologies to everyone.

81
00:05:41,022 --> 00:05:47,164
What this actually means is that suddenly we have achieved several levels of abstraction throughout the years.

82
00:05:47,505 --> 00:05:52,266
We started out by being able to only make games via basically assembly language,

83
00:05:52,787 --> 00:05:57,208
and then slowly evolved into what we have now in game engines, like Unity and Unreal.

84
00:05:58,149 --> 00:06:02,569
But the same thing happened in graphics in general.

85
00:06:02,969 --> 00:06:06,190
If you worked in a film in the 1980s and 1990s,

86
00:06:06,550 --> 00:06:08,750
you'd be doing everything with command line interfaces.

87
00:06:08,790 --> 00:06:10,371
Now we're using Photoshop and Maya and Nuke.

88
00:06:10,791 --> 00:06:12,291
And this also happened in robotics,

89
00:06:12,431 --> 00:06:17,432
because when you were working on robotics in the 1970s,

90
00:06:17,772 --> 00:06:19,472
you were building all of your own circuitry.

91
00:06:20,552 --> 00:06:22,933
Around the 1990s, you started having microcontrollers

92
00:06:22,953 --> 00:06:24,633
that were very expensive and hard to program.

93
00:06:25,053 --> 00:06:25,613
And nowadays...

94
00:06:26,874 --> 00:06:29,657
The stack is almost completely solved for you.

95
00:06:29,737 --> 00:06:36,383
You have operating systems like ROS that do all of the robotic control on one hand.

96
00:06:36,423 --> 00:06:37,825
On the other hand, you have very good hardware.

97
00:06:37,845 --> 00:06:39,746
You have GPUs on the robots themselves.

98
00:06:40,687 --> 00:06:42,449
Some companies make them. I'm not going to name names.

99
00:06:46,577 --> 00:06:59,853
The underlying idea behind all of this is that from engineering, from a culture that is supposed to build machines for other people to have ideas on top of, we now have design frameworks.

100
00:07:00,855 --> 00:07:04,079
And from the idea of modeling a...

101
00:07:05,961 --> 00:07:11,867
Well, making a mathematical model to solve a problem, we're now just teaching computers

102
00:07:11,907 --> 00:07:12,607
to do this, right?

103
00:07:12,968 --> 00:07:20,675
So in a way, you can talk about machine learning being the abstraction to how you design human

104
00:07:20,695 --> 00:07:21,496
robot interaction.

105
00:07:23,337 --> 00:07:26,000
So I want to talk about machine learning a little bit so we're all on the same page.

106
00:07:28,612 --> 00:07:35,917
I think this is the first interesting component of deep learning that was ever introduced.

107
00:07:36,317 --> 00:07:42,261
I think this is the first paper from Stanford in the 50s about neural networks.

108
00:07:43,562 --> 00:07:46,064
At the time, they could deal with one.

109
00:07:47,465 --> 00:07:52,947
Basically, one solver which had a bunch of inputs and would output a linear result.

110
00:07:53,027 --> 00:07:54,808
Is this above a plane? Is this below a plane?

111
00:07:55,348 --> 00:07:57,790
As a result, we slowly evolved into these, right?

112
00:07:57,830 --> 00:08:02,972
So you can have multiple layers of many input nodes.

113
00:08:03,692 --> 00:08:06,494
And because of GPUs, we evolved to this.

114
00:08:06,574 --> 00:08:07,534
This is GoogleNet from 2006.

115
00:08:07,814 --> 00:08:08,414
No, from 2016, I'm sorry.

116
00:08:09,255 --> 00:08:09,755
2006, what happened to me?

117
00:08:14,879 --> 00:08:19,986
All of these contain subnetworks, which contains the perceptrons from the previous slide.

118
00:08:20,046 --> 00:08:21,689
This is a very, very big network.

119
00:08:23,011 --> 00:08:28,959
The problem with these is that there is an underlying thing that fuels all of them.

120
00:08:28,999 --> 00:08:29,900
It's data, right?

121
00:08:32,957 --> 00:08:35,838
We have evolved to needing data everywhere.

122
00:08:36,318 --> 00:08:40,560
So if you are building any kind of system which

123
00:08:41,860 --> 00:08:44,521
takes resources from the real world

124
00:08:45,181 --> 00:08:47,742
and makes a conclusion about something new,

125
00:08:48,163 --> 00:08:50,103
it is using some form of data.

126
00:08:50,584 --> 00:08:52,504
And there's a market for data as a result,

127
00:08:52,584 --> 00:08:55,105
because everything that used to be modeled

128
00:08:55,386 --> 00:08:58,447
is now learnable and learnable with data that doesn't

129
00:08:58,487 --> 00:09:00,247
necessarily belong to the company that made it.

130
00:09:00,828 --> 00:09:02,448
And if you're using social media.

131
00:09:03,069 --> 00:09:05,131
You are also part of that market.

132
00:09:05,632 --> 00:09:12,198
You are being used as an example in learning.

133
00:09:13,880 --> 00:09:17,044
This is a photo from my house just before I left here.

134
00:09:17,724 --> 00:09:21,108
I just got a new sofa last week, so I just removed the old sofa.

135
00:09:21,789 --> 00:09:22,850
You don't have a sofa in this photo.

136
00:09:25,082 --> 00:09:29,484
This is from a camera that is connected to the internet,

137
00:09:30,204 --> 00:09:31,725
and it helps me know if someone's at home.

138
00:09:33,306 --> 00:09:34,966
But along with powering this camera,

139
00:09:35,006 --> 00:09:40,929
I signed a user, like a EULA, end user license agreement,

140
00:09:41,989 --> 00:09:45,531
that forces me to give up all of the data from that camera

141
00:09:46,351 --> 00:09:48,152
and help the company that makes the camera.

142
00:09:49,133 --> 00:09:58,279
It can train its models on whatever it wants, so it can sell my data to other companies.

143
00:09:58,839 --> 00:10:01,181
It won't release the raw data, but it can use it for training.

144
00:10:03,429 --> 00:10:06,391
I'm not extremely happy with this.

145
00:10:07,492 --> 00:10:12,637
But the reality is, if you buy a camera-attached headset, or if you buy a camera-attached car,

146
00:10:13,437 --> 00:10:18,662
which you are doing, they'll have the same kind of end-user license agreement.

147
00:10:19,683 --> 00:10:25,528
Your data won't belong to you, and your data will be used to train newer models for the

148
00:10:25,568 --> 00:10:26,508
future of robotics.

149
00:10:27,736 --> 00:10:33,280
There are multiple problems with it, but I think the biggest problem that everyone is ignoring is that no amount of data is enough.

150
00:10:39,744 --> 00:10:48,129
When we look at data that's coming from users, we're looking at the normal behavior most of the time.

151
00:10:49,463 --> 00:10:51,104
It's very hard to catch anomalies.

152
00:10:51,345 --> 00:10:53,086
Anomalies don't appear in the data set.

153
00:10:53,467 --> 00:10:56,029
Or when they do appear, they appear very sparsely.

154
00:10:56,450 --> 00:10:58,672
So if you're trying to train a self-driving car,

155
00:10:59,833 --> 00:11:00,974
you need to train it for stuff like this.

156
00:11:03,196 --> 00:11:05,218
That's not an image that you would typically

157
00:11:05,278 --> 00:11:06,880
see driving down the street, unless you

158
00:11:06,900 --> 00:11:07,701
live in New York like I do.

159
00:11:10,657 --> 00:11:16,079
This is a very hard image for a neural network running inside a self-driving car to parse.

160
00:11:16,479 --> 00:11:18,640
It has multiple lights, it doesn't know where the sources are.

161
00:11:19,020 --> 00:11:20,221
This is a rare occurrence.

162
00:11:20,421 --> 00:11:21,682
Like I would call this an anomaly.

163
00:11:22,622 --> 00:11:24,823
But anomalies exist everywhere in the data, right?

164
00:11:25,703 --> 00:11:28,565
You're not training for this, you're certainly not training for this, right?

165
00:11:30,826 --> 00:11:34,227
So some data sets don't exist right now, but some data sets also can't exist.

166
00:11:35,645 --> 00:11:42,427
It's going to be very hard or unethical to get the data sets in advance for machines

167
00:11:42,447 --> 00:11:51,930
to actually use when you're building a product that requires a mathematical model running

168
00:11:51,990 --> 00:11:52,650
behind the scenes.

169
00:11:56,491 --> 00:11:57,091
So I have a solution.

170
00:11:57,511 --> 00:11:58,291
This image isn't real.

171
00:12:00,152 --> 00:12:01,692
It's part of our project.

172
00:12:04,043 --> 00:12:06,164
It's part of our simulator for self-driving cars

173
00:12:06,324 --> 00:12:07,145
that's called DriveSim.

174
00:12:08,706 --> 00:12:12,529
We've been using this for quite a bit at NVIDIA.

175
00:12:13,149 --> 00:12:15,831
The premise of synthetic data seems reasonable.

176
00:12:15,851 --> 00:12:17,712
There's no data in the real world, you just have to make it.

177
00:12:18,833 --> 00:12:20,034
And there's two things to attack here.

178
00:12:20,134 --> 00:12:23,056
One is generating enough imagery for ground truth

179
00:12:23,656 --> 00:12:27,999
to provide data for the networks to learn

180
00:12:28,279 --> 00:12:31,282
if you were in a scenario where you have a lot of red lights

181
00:12:31,502 --> 00:12:32,022
or if you're in a...

182
00:12:32,382 --> 00:12:34,223
in a scenario which has complexity in the road.

183
00:12:35,224 --> 00:12:37,585
But the other one is generating interesting scenarios.

184
00:12:42,047 --> 00:12:44,569
Creating images like this one,

185
00:12:45,669 --> 00:12:47,170
pretty much at random, solves for one.

186
00:12:47,210 --> 00:12:48,251
It solves for the ground truth.

187
00:12:48,731 --> 00:12:50,052
It doesn't really solve for a scenario.

188
00:12:50,572 --> 00:12:52,833
So we have to think about how to create scenarios

189
00:12:53,253 --> 00:12:55,695
for robots to learn how to behave.

190
00:12:59,652 --> 00:13:02,053
It comes to things that are hard to capture in the real world.

191
00:13:02,953 --> 00:13:04,934
This is one example of something we've been doing.

192
00:13:05,454 --> 00:13:08,815
This is a bunch of robots together in a product called Isaac.

193
00:13:08,895 --> 00:13:09,735
These are all simulated.

194
00:13:10,315 --> 00:13:13,356
And they're learning how to play hockey.

195
00:13:13,756 --> 00:13:13,896
Right?

196
00:13:14,296 --> 00:13:16,657
So all of them just have a stick.

197
00:13:17,417 --> 00:13:19,898
And all they know how to do is just move the stick.

198
00:13:20,858 --> 00:13:27,122
And they also have a score function which tracks the puck and tells them how close they

199
00:13:27,142 --> 00:13:27,622
are to the goal.

200
00:13:28,683 --> 00:13:33,426
After enough iterations, those robots learn that they get a better score if they move

201
00:13:33,806 --> 00:13:37,889
the stick such that it hits the puck and they move the stick such that it hits the puck

202
00:13:38,869 --> 00:13:43,714
And when they do, they get a super high score so that then they know how to repeat that again.

203
00:13:44,194 --> 00:13:51,201
And they get all the observation from the real world and just use that to make that one specific action.

204
00:13:51,901 --> 00:13:58,447
Now if you think that's strange, you need to get used to it because that's of how all of robotics works right now.

205
00:13:58,467 --> 00:14:02,711
And that's the most intelligent robot will be for a while, unfortunately.

206
00:14:03,672 --> 00:14:04,853
What are we actually doing here?

207
00:14:07,011 --> 00:14:09,854
That hockey thing that I showed you, that's the environment.

208
00:14:10,594 --> 00:14:14,258
Your environment as a robot playing hockey is a field.

209
00:14:14,698 --> 00:14:15,479
There's a goal in there.

210
00:14:15,579 --> 00:14:16,380
There's a puck somewhere.

211
00:14:16,680 --> 00:14:17,441
You see all of that.

212
00:14:17,881 --> 00:14:21,905
And you, as an agent, are looking at the environment.

213
00:14:22,045 --> 00:14:22,946
You're making an observation.

214
00:14:23,647 --> 00:14:25,508
After you've made an observation, time to make some

215
00:14:25,588 --> 00:14:26,349
action, right?

216
00:14:26,910 --> 00:14:27,670
So you'd be moving the puck.

217
00:14:28,996 --> 00:14:33,118
It doesn't happen just once, it happens at very, very high frequencies.

218
00:14:33,138 --> 00:14:36,940
So you've been moving the puck a little bit, then you're observing the world again.

219
00:14:36,980 --> 00:14:39,021
It's my... sorry, not the puck, the stick.

220
00:14:39,702 --> 00:14:40,562
Is my stick too fast?

221
00:14:40,602 --> 00:14:41,422
Is my stick too slow?

222
00:14:42,263 --> 00:14:45,224
Until you've reached a good speed and you've reached a good direction, it's time to hit

223
00:14:45,244 --> 00:14:45,525
the puck.

224
00:14:45,565 --> 00:14:47,205
And you've hit the puck and stopped.

225
00:14:49,559 --> 00:14:52,464
If you got something good out of it, well, what happens actually?

226
00:14:52,544 --> 00:14:53,866
There's something missing here, right?

227
00:14:54,827 --> 00:14:57,912
Someone has to tell you that you've done something good.

228
00:14:58,432 --> 00:15:00,295
So there's also an interpreter in the process,

229
00:15:01,177 --> 00:15:03,099
and if you've done something good, they'll give you a reward.

230
00:15:03,677 --> 00:15:06,399
So this is the basic idea behind reinforcement learning.

231
00:15:06,419 --> 00:15:11,102
And if you look at any kind of reinforcement learning paper,

232
00:15:11,182 --> 00:15:14,684
you'll see that there's always some system on top of this.

233
00:15:14,724 --> 00:15:15,765
But this is the basic idea.

234
00:15:16,545 --> 00:15:18,026
There's an agent in the world.

235
00:15:18,706 --> 00:15:19,547
They make an action.

236
00:15:20,928 --> 00:15:23,870
And they get a reward as a result.

237
00:15:25,491 --> 00:15:27,872
Now, I want to ask you.

238
00:15:31,034 --> 00:15:34,538
How many parts of these systems can we debug right now?

239
00:15:36,260 --> 00:15:43,309
So if you would be a reinforcement learning expert.

240
00:15:44,069 --> 00:15:45,890
What you'd be debugging is the interpreter.

241
00:15:47,631 --> 00:15:52,074
You'd be looking at what the robot actually gets rewards back from,

242
00:15:52,114 --> 00:15:53,835
and you'd be tweaking that reward function, right?

243
00:15:54,215 --> 00:15:56,377
Get closer to the goal. Cool.

244
00:15:56,537 --> 00:15:58,618
Maybe getting closer to the goal isn't enough.

245
00:15:58,658 --> 00:16:00,500
Maybe you need to hit it at high speeds.

246
00:16:00,900 --> 00:16:05,023
So you tweak the function a little bit so that the stick that's actually moving

247
00:16:07,633 --> 00:16:10,455
You get a higher reward if you hit the goal and it's a high speed.

248
00:16:10,915 --> 00:16:11,076
Right?

249
00:16:11,796 --> 00:16:13,417
You could be, for example, saying,

250
00:16:13,817 --> 00:16:15,058
as long as you got close to the goal,

251
00:16:15,398 --> 00:16:16,919
that's already high score, so do that.

252
00:16:18,000 --> 00:16:20,041
So there's a lot to tweak there to debug something.

253
00:16:20,962 --> 00:16:22,142
And that goes into the agent.

254
00:16:23,443 --> 00:16:25,384
But there's something really big that you're not debugging that way,

255
00:16:25,664 --> 00:16:27,185
and that's the environment.

256
00:16:28,066 --> 00:16:30,748
So a lot of things happening with a robot

257
00:16:30,788 --> 00:16:32,349
when they're interacting with the actual world,

258
00:16:33,529 --> 00:16:36,231
you can't solve because you can't change the environment too much.

259
00:16:36,251 --> 00:16:37,251
You don't have a debugger for that.

260
00:16:38,312 --> 00:16:39,293
So we made a debugger for that.

261
00:16:40,154 --> 00:16:43,477
This is the first example of a VR experience that I created.

262
00:16:43,497 --> 00:16:45,318
This is for SIGGRAPH 2017.

263
00:16:45,358 --> 00:16:48,941
What's happening here is you're playing dominoes

264
00:16:48,981 --> 00:16:50,763
with a fellow called Isaac.

265
00:16:51,564 --> 00:16:52,184
This is Isaac.

266
00:16:53,385 --> 00:16:54,626
He has dominoes.

267
00:16:54,726 --> 00:16:55,447
I have dominoes.

268
00:16:55,467 --> 00:16:56,988
I'm going to pick up a domino from my side,

269
00:16:57,829 --> 00:17:00,231
and I am going to place that piece of domino

270
00:17:00,892 --> 00:17:01,832
on the table in front of him.

271
00:17:02,233 --> 00:17:04,555
So as I do this, I pick this up.

272
00:17:04,575 --> 00:17:05,856
This is done with a.

273
00:17:07,742 --> 00:17:10,928
with the UE4. It's already two and a half years old, so it's been a while.

274
00:17:12,371 --> 00:17:14,976
So I'll place this piece of domino. It's got a four and a five.

275
00:17:15,557 --> 00:17:16,739
And now I'm gonna hit the button.

276
00:17:18,130 --> 00:17:31,072
In Isaac's neural networks, what's happening is that he needs to evaluate, they need to evaluate the type of domino, they need to evaluate what the legal move is, and then they need to act on it.

277
00:17:31,452 --> 00:17:40,014
And in this case, Isaac connected a 5 and a 5, so it's a legal move. Great. And it was almost geometrically legal as well, so good on him.

278
00:17:41,174 --> 00:17:46,335
Now, I put a 6 and a 6 together, so he has a few more options on his board.

279
00:17:48,551 --> 00:17:51,013
So he's going to look at it again and perform the same action.

280
00:17:51,394 --> 00:17:56,859
This time he's taking a 4 and connecting a 4 to a 4.

281
00:17:57,760 --> 00:18:01,704
So what's going on here is that this is a very controlled environment in which we can

282
00:18:01,764 --> 00:18:06,649
check or we can intervene with the environment for the robot to play in.

283
00:18:08,030 --> 00:18:10,192
And I want to talk a little bit about how we did that.

284
00:18:10,953 --> 00:18:12,294
So the first thing to do.

285
00:18:13,135 --> 00:18:14,595
It's a train-the-vision network for the robot.

286
00:18:14,635 --> 00:18:16,836
We generated a lot of images of dominoes.

287
00:18:17,216 --> 00:18:18,997
So this is also done in Unreal Engine.

288
00:18:19,017 --> 00:18:21,478
We pretty much simply threw a bunch of dominoes in the space.

289
00:18:21,838 --> 00:18:24,379
And we attached to each image, as we outputted them,

290
00:18:25,079 --> 00:18:25,940
the ground truth.

291
00:18:26,060 --> 00:18:26,820
What is this domino?

292
00:18:27,140 --> 00:18:28,721
And the neural network can then learn.

293
00:18:33,327 --> 00:18:40,756
We also added a bunch of distractors so the neural network couldn't pick up on everything

294
00:18:40,796 --> 00:18:41,437
being a domino.

295
00:18:41,718 --> 00:18:44,001
Now you'll notice that these images are not photorealistic.

296
00:18:45,483 --> 00:18:47,325
And this is another very important point.

297
00:18:48,256 --> 00:18:53,980
We don't know, we actually don't know, what neural networks consider important, right?

298
00:18:54,060 --> 00:18:54,960
This is a research topic.

299
00:18:56,281 --> 00:18:59,423
So we started from the very basic things, and we slowly improve upon them.

300
00:19:00,791 --> 00:19:02,372
The next step after we trained the Vision Network

301
00:19:02,872 --> 00:19:04,893
was to just take a very, very simple agent

302
00:19:05,153 --> 00:19:07,815
and to play a lot of games with the robot.

303
00:19:07,835 --> 00:19:11,056
We played about something like 50,000 games

304
00:19:12,077 --> 00:19:14,318
until the Robot's Reinforcement Learning Network

305
00:19:14,838 --> 00:19:17,279
learned how to make the right moves.

306
00:19:18,980 --> 00:19:21,281
And then we debugged it in VR.

307
00:19:21,482 --> 00:19:24,323
So at every step that we thought that the robot was good enough,

308
00:19:24,363 --> 00:19:27,104
we would play a few games in VR.

309
00:19:28,125 --> 00:19:29,786
And now we wanted to do something else.

310
00:19:31,695 --> 00:19:35,799
Actually, at any point in this process, did we assume that the robot was virtual?

311
00:19:35,959 --> 00:19:36,279
We didn't.

312
00:19:36,960 --> 00:19:37,120
Right?

313
00:19:37,961 --> 00:19:43,045
So, if this can work on a virtual robot, it may as well work on a real robot that has

314
00:19:43,085 --> 00:19:45,447
the grippers and the right physical configuration.

315
00:19:46,548 --> 00:19:46,708
Right?

316
00:19:46,728 --> 00:19:48,030
Do you think it's going to work?

317
00:19:51,352 --> 00:19:51,713
Yeah, it works.

318
00:19:53,128 --> 00:19:56,349
So this is a robot that we actually deployed in SIGGRAPH 2017.

319
00:19:57,850 --> 00:19:59,750
It literally is running the same networks.

320
00:20:00,130 --> 00:20:03,552
When we first put it up, nothing worked.

321
00:20:04,432 --> 00:20:05,612
And I was really upset.

322
00:20:06,873 --> 00:20:11,234
And I then realized I put the camera 90 degrees off, so I rotated the camera,

323
00:20:11,514 --> 00:20:12,575
and suddenly everything worked.

324
00:20:13,295 --> 00:20:15,656
Because the robot just had the same vision network,

325
00:20:15,696 --> 00:20:17,716
and it had the same gameplay network, so it could just play the game.

326
00:20:18,237 --> 00:20:22,320
The only thing that was different about it was that, in this case, we were not using

327
00:20:22,340 --> 00:20:28,585
the same inverse kinematics as in the control system that powers the grippers.

328
00:20:30,587 --> 00:20:32,428
But everything else was just the same.

329
00:20:33,089 --> 00:20:38,353
So there is no difference when you're actually getting it right between a physical robot

330
00:20:38,393 --> 00:20:39,174
and a virtual robot.

331
00:20:39,214 --> 00:20:40,755
And this is a very important takeaway here, right?

332
00:20:41,676 --> 00:20:45,739
So here are the goals that we set up when we started this.

333
00:20:46,895 --> 00:20:48,795
We have to use VR interactively with machine learning.

334
00:20:50,596 --> 00:20:52,337
Most of the team that we had weren't roboticists.

335
00:20:52,497 --> 00:20:53,817
At the time, I was not a roboticist.

336
00:20:55,338 --> 00:20:56,658
We can't use end-to-end training.

337
00:20:56,698 --> 00:20:58,459
We have to modularize the entire system.

338
00:20:59,659 --> 00:21:00,880
And we have to do it really fast.

339
00:21:01,000 --> 00:21:03,341
In fact, we had two weeks to do that.

340
00:21:04,821 --> 00:21:10,003
So building the system was a challenge just

341
00:21:10,043 --> 00:21:11,163
from a time crunch perspective.

342
00:21:12,447 --> 00:21:15,268
The way to solve it is we use the best component for each layer, right?

343
00:21:15,308 --> 00:21:21,712
We started with just the image recognition system for dominoes.

344
00:21:22,752 --> 00:21:25,514
Completely separately from that, after we assumed this works,

345
00:21:26,774 --> 00:21:31,498
We had separately trained a reinforcement learning network on the gameplay.

346
00:21:32,499 --> 00:21:39,807
And completely separately from that, we were testing a physical control layer, or a virtual control layer for the robot.

347
00:21:40,227 --> 00:21:46,914
So at the very end of the process, we were testing both VR and a real robot at the same time.

348
00:21:48,360 --> 00:21:54,908
The reasoning for this is that we can debug the virtual robot really quick, and the physical

349
00:21:54,948 --> 00:21:58,212
robot is a little slower, otherwise it will kill you, pretty much.

350
00:22:01,964 --> 00:22:04,106
So here are the tech stats on this.

351
00:22:04,266 --> 00:22:06,987
So you want to know that the production itself

352
00:22:07,248 --> 00:22:08,168
was two and a half weeks.

353
00:22:08,929 --> 00:22:14,072
We had separate teams doing each part of the process.

354
00:22:14,232 --> 00:22:15,533
There was the vision, the policy.

355
00:22:15,573 --> 00:22:18,295
Policy is how the gameplay is played.

356
00:22:18,815 --> 00:22:20,656
The physical robot, the HRI, and the art.

357
00:22:20,996 --> 00:22:23,958
I led the HRI and the art and the VR.

358
00:22:27,619 --> 00:22:32,441
And we trained a vision network with 30,000 domino images

359
00:22:32,741 --> 00:22:37,143
with our own simulator called Isaac Data Studio.

360
00:22:39,064 --> 00:22:41,185
The policy network used, I'm sorry, I thought it was 50,000.

361
00:22:41,265 --> 00:22:44,807
It's 80,000 games to get the robot to the right position.

362
00:22:45,207 --> 00:22:47,048
I tested it after about 50,000 games.

363
00:22:48,368 --> 00:22:49,029
It wasn't good enough.

364
00:22:49,649 --> 00:22:52,290
We also had to redo some of the components.

365
00:22:52,790 --> 00:22:54,871
At the time, this is about two and a half years ago,

366
00:22:54,891 --> 00:22:58,553
at the time the training was quite, quite slow.

367
00:22:58,973 --> 00:23:02,375
As in, like, it took 24 hours to play 80,000 games.

368
00:23:03,556 --> 00:23:05,597
Which, when you're in a time crunch, actually matters.

369
00:23:05,797 --> 00:23:09,199
So testing it early, virtually, really mattered to us.

370
00:23:11,220 --> 00:23:13,381
And the components were never integrated until deployment.

371
00:23:15,101 --> 00:23:17,323
The other thing that's really important to say here

372
00:23:17,523 --> 00:23:18,983
is that all the teams were separate.

373
00:23:19,083 --> 00:23:20,984
I was in my home office in New York.

374
00:23:22,425 --> 00:23:26,349
Some of the AI people were in Toronto,

375
00:23:26,909 --> 00:23:29,051
and some people were in Austin,

376
00:23:29,351 --> 00:23:32,614
and the physical robot was in LA, I think?

377
00:23:32,934 --> 00:23:33,234
Not sure.

378
00:23:34,315 --> 00:23:36,236
So we were all completely separate,

379
00:23:36,337 --> 00:23:37,177
and we could get it done.

380
00:23:37,778 --> 00:23:37,938
Right?

381
00:23:38,618 --> 00:23:39,419
I'm going to skip this.

382
00:23:39,799 --> 00:23:42,562
So, the other takeaways here is that...

383
00:23:44,460 --> 00:23:50,702
We built this in such a way that people know what to expect when they're testing the robot.

384
00:23:51,523 --> 00:23:56,385
This is not a smart machine so far. One day we'll be at a point where this is a smart machine,

385
00:23:56,445 --> 00:24:00,467
but so far it's kind of like a five-year-old child learning how to play dominoes. So you

386
00:24:00,487 --> 00:24:06,129
have to be patient. So by putting this inside the story, people wouldn't make

387
00:24:06,549 --> 00:24:09,991
smart moves at first. We can actually debug using the simple cases.

388
00:24:11,171 --> 00:24:12,893
If you're in VR, you get bored, right?

389
00:24:13,673 --> 00:24:18,357
But if you're in VR thinking you're playing against a five-year-old child, you get bored less quickly because you're thinking,

390
00:24:18,377 --> 00:24:21,179
what's going on in their head, right? Is this a good move? Is that a bad move?

391
00:24:21,199 --> 00:24:24,642
So that's really important to keep in mind when designing things like this.

392
00:24:26,663 --> 00:24:30,727
The other thing that we realize is really important is that

393
00:24:31,774 --> 00:24:34,055
Players don't remember if you tell them,

394
00:24:34,175 --> 00:24:38,636
oh, just don't put one of those tiles sideways,

395
00:24:38,817 --> 00:24:40,837
because that's an advanced move.

396
00:24:41,778 --> 00:24:44,799
But they will remember game mechanics really well.

397
00:24:45,339 --> 00:24:48,620
So we've been playing games all of our lives,

398
00:24:49,320 --> 00:24:53,862
so we get to know what a game of dominoes looks like.

399
00:24:54,602 --> 00:24:56,543
And we kind of listen to that, or we tune into that.

400
00:24:57,523 --> 00:24:59,604
We don't really tune into instructions like, oh, no, no,

401
00:24:59,644 --> 00:25:00,264
just don't touch that.

402
00:25:01,145 --> 00:25:05,014
But if you make something in the game that makes you not touch that, it really helps

403
00:25:05,034 --> 00:25:05,635
in the simulation.

404
00:25:07,580 --> 00:25:08,963
Obviously, I talked about it.

405
00:25:09,324 --> 00:25:11,007
When you modularized all components...

406
00:25:12,426 --> 00:25:16,609
We could really speed up because we weren't dependent on each other at any point.

407
00:25:17,250 --> 00:25:22,274
I could generate fake data in the first week and have everyone test with the fake data

408
00:25:22,775 --> 00:25:26,738
and then I could bring in real data and replace the fake data and it just worked.

409
00:25:27,458 --> 00:25:31,362
And the other thing we realized as we were doing this, what we were actually building

410
00:25:31,422 --> 00:25:31,902
is a debugger.

411
00:25:33,083 --> 00:25:40,947
So we need the ability to cache previous games, and we need the ability to switch between a later version of the framework to an earlier version and test which one is better.

412
00:25:41,707 --> 00:25:42,708
Do A-B testing on that.

413
00:25:43,688 --> 00:25:47,390
So, using this knowledge, you can actually go very far.

414
00:25:48,931 --> 00:25:52,772
You can train very advanced networks to do things like self-driving cars.

415
00:25:53,913 --> 00:26:04,939
These aren't real images. Again, what this is, is output from a neural network looking at synthetic data that we have inside DriveSim, our self-driving car simulator.

416
00:26:04,959 --> 00:26:11,862
I want to talk about it a little bit, just so you know what we're actually building when we're building gameplay environments for robots.

417
00:26:13,884 --> 00:26:23,714
In this case we have this car model that has a bunch of cameras. All these cameras need to inform the car on how to make decisions in an autonomous driving situation.

418
00:26:24,055 --> 00:26:27,098
So cars would be driving around and they get some anomalies.

419
00:26:28,960 --> 00:26:33,965
So we need to emulate all the sensors in the car and we need to recreate dangerous and rare scenarios to do that.

420
00:26:35,367 --> 00:26:38,710
And so we can manually inject inside the environment.

421
00:26:38,750 --> 00:26:41,132
We can inject fog, snow, rain.

422
00:26:41,192 --> 00:26:44,515
We can make some arbitrary traffic congestions.

423
00:26:45,115 --> 00:26:47,377
We can add really backlit scenes with fog.

424
00:26:48,859 --> 00:26:51,281
So the sensors barely do anything, but they can still

425
00:26:51,321 --> 00:26:53,082
react properly because we trained them

426
00:26:53,122 --> 00:26:54,083
on many extreme cases.

427
00:26:55,754 --> 00:27:02,379
And we get this and we also apply some distributed computing to this because we don't have to train one scenario.

428
00:27:02,759 --> 00:27:05,982
We can train a bunch of scenarios at the same time and kill the ones that we don't like.

429
00:27:06,542 --> 00:27:09,845
The ones that we don't like didn't perform well in a rainy street.

430
00:27:10,765 --> 00:27:13,067
Okay, redo that. We'll use another.

431
00:27:15,049 --> 00:27:17,150
And this approach should work, right?

432
00:27:17,170 --> 00:27:18,671
Do you think it works?

433
00:27:20,130 --> 00:27:20,550
Yeah, it works.

434
00:27:21,711 --> 00:27:25,635
So this is a test that we have from about a year ago.

435
00:27:25,655 --> 00:27:26,896
Yeah, it's about a year ago.

436
00:27:27,757 --> 00:27:30,640
We did a test with one of our autonomous vehicles.

437
00:27:31,380 --> 00:27:31,901
This one is in.

438
00:27:34,075 --> 00:27:37,155
In the South Bay, we're running a track of like 80 miles.

439
00:27:38,096 --> 00:27:40,076
And this is real data that you see here.

440
00:27:40,136 --> 00:27:42,937
So what's actually happening is the car's driving around,

441
00:27:43,257 --> 00:27:46,117
and it's picking up on all the road marks, and so on.

442
00:27:46,818 --> 00:27:49,538
In order to validate this information, what we're doing

443
00:27:49,898 --> 00:27:52,739
is we're recreating the same track virtually.

444
00:27:52,859 --> 00:27:54,399
So you see on the left here, you see DriveSim.

445
00:27:54,699 --> 00:27:56,700
DriveSim is our self-driving car simulator.

446
00:27:57,080 --> 00:27:58,600
And it's using all the information.

447
00:27:59,801 --> 00:28:03,824
to recreate that ride and validate it to see that we can inject more dangerous scenarios.

448
00:28:03,944 --> 00:28:08,607
In that case, maybe the car wouldn't make it. So we want to validate all these, we add this layer

449
00:28:08,647 --> 00:28:14,352
to it, right? We're just changing the environment and we're not telling a real environment from a

450
00:28:14,392 --> 00:28:19,135
virtual environment as far as the evaluation network is concerned, because they're not different.

451
00:28:21,497 --> 00:28:27,181
So the layers that we have here are simulation on one hand, emulation of sensors on the other hand,

452
00:28:28,385 --> 00:28:31,743
And at the bottom we also have imitation. We're adding more data.

453
00:28:32,967 --> 00:28:39,752
For the machines to be able to use without exactly understanding what a human is doing

454
00:28:40,572 --> 00:28:45,656
or what other players in the driving simulator are doing, but still be able to do something

455
00:28:45,676 --> 00:28:45,956
with it.

456
00:28:46,036 --> 00:28:46,216
Right?

457
00:28:46,276 --> 00:28:52,380
Like if you don't know why a machine has done a certain thing, but you're still able

458
00:28:52,420 --> 00:28:55,502
to navigate around that, well, mission accomplished pretty much the same way.

459
00:28:58,444 --> 00:29:00,146
This is one example of what I mean.

460
00:29:00,166 --> 00:29:00,786
So this is a...

461
00:29:01,767 --> 00:29:04,448
This is a demo by a colleague of mine named Madeline Gannon.

462
00:29:04,948 --> 00:29:07,270
She built this playful robot.

463
00:29:07,290 --> 00:29:11,572
Now, this is an assembly robot from ABB Robotics.

464
00:29:12,452 --> 00:29:14,994
This thing can definitely kill you.

465
00:29:15,574 --> 00:29:17,535
It's very big, and it's very powerful.

466
00:29:17,995 --> 00:29:20,977
And if you get too close to it, just one wrong move

467
00:29:21,537 --> 00:29:22,278
can break a few bones.

468
00:29:24,059 --> 00:29:25,840
But in this scenario, she made it

469
00:29:26,100 --> 00:29:30,042
look cute and inquisitive, mimicking humans in a way.

470
00:29:31,003 --> 00:29:33,786
This field has grown quite a bit in recent years.

471
00:29:33,826 --> 00:29:37,910
This is a work by Daniel Holden from about a year and a half ago, or two years ago.

472
00:29:38,731 --> 00:29:45,979
He made a neural network that combines a bunch of different motion-captured humans to recreate motion.

473
00:29:46,039 --> 00:29:52,206
Over here, what you see is that this character is getting a bunch of terrains that it hasn't seen before and still reacting properly.

474
00:29:52,547 --> 00:29:58,373
Because the neural network is mixing all of the different terrains that were available in training time,

475
00:29:58,393 --> 00:29:59,934
it's just doing it intelligently.

476
00:30:00,275 --> 00:30:03,979
So the thing looks like it's acting, reacting pretty normally.

477
00:30:04,219 --> 00:30:06,641
The same university also released about a year ago,

478
00:30:08,984 --> 00:30:13,569
pretty much the same type of network with some modifications, but done on a wolf, right?

479
00:30:15,676 --> 00:30:18,877
There is some improvement there. Also you see that the motion is very, very natural.

480
00:30:19,377 --> 00:30:21,617
Please don't ask me how they motion captured a wolf.

481
00:30:23,058 --> 00:30:27,019
We have been doing the same sort of things only with reinforcement learning.

482
00:30:27,079 --> 00:30:33,220
So over here you have Isaac Gym. It's our own training system just for reinforcement learning

483
00:30:33,260 --> 00:30:37,381
where we're training virtual characters to just get up and tap each other.

484
00:30:38,322 --> 00:30:40,084
There's definitely a video game in here somewhere.

485
00:30:42,087 --> 00:30:44,129
There's no mocap here, just physics.

486
00:30:45,130 --> 00:30:46,472
We can do this with imitation.

487
00:30:47,473 --> 00:30:52,299
One character here, the physics simulation that you see on the left,

488
00:30:52,800 --> 00:30:53,080
it has...

489
00:30:54,326 --> 00:30:56,047
All the characteristics of a human body.

490
00:30:56,087 --> 00:30:58,248
So it has the biomechanical model of a human body,

491
00:30:58,749 --> 00:31:01,150
and it's observing another motion-captured clip,

492
00:31:01,350 --> 00:31:02,431
and it's trying to imitate it.

493
00:31:03,231 --> 00:31:05,753
So what we're doing is we're providing real-world data,

494
00:31:05,793 --> 00:31:07,694
real human data, into the process,

495
00:31:08,074 --> 00:31:10,375
and then another machine is just imitating that,

496
00:31:10,395 --> 00:31:12,477
and it's getting a good score for doing it right, right?

497
00:31:12,977 --> 00:31:15,418
It's not actually using the motion-captured data.

498
00:31:16,339 --> 00:31:21,622
It's learning to activate its own actuators and motors.

499
00:31:22,362 --> 00:31:24,524
To move like the motion-captured human.

500
00:31:24,804 --> 00:31:28,807
And it's getting a score on the end result, not on the joints.

501
00:31:29,767 --> 00:31:30,588
Right? Is that clear?

502
00:31:31,608 --> 00:31:31,829
Cool.

503
00:31:32,729 --> 00:31:33,890
And we can make masked games with it.

504
00:31:33,930 --> 00:31:38,213
So in this case we motion-captured a bunch of different positions.

505
00:31:38,273 --> 00:31:40,774
And we also inserted some synthetic data into it.

506
00:31:41,075 --> 00:31:43,996
In this case they're trying to do backflips.

507
00:31:44,557 --> 00:31:46,658
Over here they're trying to do some masked games.

508
00:31:47,138 --> 00:31:50,681
You see how this evolves into the training of a character in a video game.

509
00:31:51,963 --> 00:31:54,910
Next time you kill a monster in Doom, I want you to feel a little guilty.

510
00:31:55,752 --> 00:31:58,278
It's done so much for you.

511
00:31:59,850 --> 00:32:03,313
OK, so now we can create imitation-based data sets.

512
00:32:03,793 --> 00:32:05,815
And we can actually do useful things with them.

513
00:32:05,875 --> 00:32:08,457
So this is one other VR experiences

514
00:32:08,617 --> 00:32:10,738
I created in which you're using the same robot

515
00:32:10,778 --> 00:32:13,581
that you had before, but now you're making pretzels.

516
00:32:14,721 --> 00:32:18,324
So in this case, the user gets the chef's hat,

517
00:32:18,644 --> 00:32:20,746
and they get to transform themselves into the robot.

518
00:32:21,707 --> 00:32:25,230
and control its actuators, control its grippers.

519
00:32:26,050 --> 00:32:27,692
And they'd be folding a pretzel.

520
00:32:27,732 --> 00:32:30,034
So they're grabbing a pretzel from both ends.

521
00:32:30,294 --> 00:32:31,715
This is sticky dough.

522
00:32:32,396 --> 00:32:35,238
It's also soft, so you have to shake it off your hands.

523
00:32:36,519 --> 00:32:38,000
You'll notice here something strange.

524
00:32:39,301 --> 00:32:42,184
The robot doesn't have the same joint structure as we do.

525
00:32:43,305 --> 00:32:44,225
So we need to think...

526
00:32:45,104 --> 00:32:46,884
about what it looks like when we make a motion.

527
00:32:46,905 --> 00:32:47,865
And sometimes we're wrong,

528
00:32:48,285 --> 00:32:50,285
and that's when you see this thing turning red,

529
00:32:50,745 --> 00:32:52,126
like this virtual gripper turning red.

530
00:32:52,386 --> 00:32:55,647
There's the real world, which is that virtual gripper,

531
00:32:56,027 --> 00:32:57,327
and then there's the virtual world,

532
00:32:57,667 --> 00:32:58,687
which is where you see the robot.

533
00:32:58,727 --> 00:33:00,108
That was a legal move.

534
00:33:00,828 --> 00:33:05,009
So this simulator is only made to make legal moves

535
00:33:05,529 --> 00:33:07,889
inside the VR experience.

536
00:33:08,409 --> 00:33:10,530
And whenever my hand is green, I can do this.

537
00:33:10,730 --> 00:33:12,970
Now, I ring the bell when I'm done,

538
00:33:13,511 --> 00:33:14,211
and now you have...

539
00:33:15,351 --> 00:33:18,874
The robot imitating my exact motion.

540
00:33:19,935 --> 00:33:22,217
So after I've recorded it, the robot is going to try again,

541
00:33:22,997 --> 00:33:26,560
and they're going to fold the dough, which is fully simulated here,

542
00:33:27,281 --> 00:33:28,962
the same way that I had just done.

543
00:33:30,724 --> 00:33:36,048
So this is training a virtual robot to do a completely manual human task.

544
00:33:38,390 --> 00:33:42,133
And now that it's done, I have my little helper robot do a salt bae move here.

545
00:33:44,271 --> 00:33:46,854
This is a year and a half ago. I was very proud of it at the time.

546
00:33:48,376 --> 00:33:50,899
Now it's going to bake the pretzel. Everyone's happy.

547
00:33:54,143 --> 00:34:01,548
I think this is a very significant process in, it's going to be part of the future of

548
00:34:01,608 --> 00:34:02,889
labor in many ways.

549
00:34:02,929 --> 00:34:09,053
We're going to train robots to do this small action that we are so good at doing, but hate

550
00:34:09,093 --> 00:34:09,353
doing.

551
00:34:10,134 --> 00:34:10,294
Right?

552
00:34:10,314 --> 00:34:11,735
It's going to happen over and over again.

553
00:34:12,135 --> 00:34:16,038
So I want you to notice a few things that we found along the process that might be interesting.

554
00:34:16,238 --> 00:34:16,958
So one is...

555
00:34:18,335 --> 00:34:22,458
It is totally non-trivial the way you choose to control the robot.

556
00:34:23,219 --> 00:34:26,741
We controlled the robot from first person because we thought it was interesting enough

557
00:34:27,222 --> 00:34:35,107
to make it such that you would learn how the robot can move from the perspective of a human.

558
00:34:35,327 --> 00:34:36,568
Some moves are legal.

559
00:34:37,669 --> 00:34:40,951
But the other options that we tried are grabbing things by the object.

560
00:34:41,052 --> 00:34:44,874
So instead of grabbing the actual gripper, I could grab some virtual point.

561
00:34:45,455 --> 00:34:50,578
So, we had to make sure that we had the robot stand away from the gripper, where the actual object, the end point, should be, and have the robot attempt to grip it.

562
00:34:52,300 --> 00:34:59,705
That had limitations. We also tried standing at a distance from the robot so we can see what's going on and complete the task from our perspective.

563
00:34:59,725 --> 00:35:05,809
Because there's nothing actually limiting the robot from doing the entire task upside down, mirrored.

564
00:35:07,670 --> 00:35:10,772
We just happened to be piloting the robot from first person.

565
00:35:11,132 --> 00:35:13,093
We could be piloting the robot from third person,

566
00:35:13,433 --> 00:35:16,454
and it would be completing the same task from our perspective.

567
00:35:16,654 --> 00:35:17,815
So there's no difference, really.

568
00:35:18,215 --> 00:35:18,835
We tried that, too.

569
00:35:18,855 --> 00:35:21,736
At this point, you see this kind of preview

570
00:35:21,996 --> 00:35:24,777
where there's a bit of a rope between you and the robot doing that.

571
00:35:26,403 --> 00:35:30,245
We also tried following the actual gripper with a

572
00:35:30,265 --> 00:35:30,665
controller.

573
00:35:30,685 --> 00:35:33,446
That turned out to be weird.

574
00:35:33,466 --> 00:35:36,268
It definitely has some advantages, but you still run

575
00:35:36,308 --> 00:35:39,269
into the same problems of legal moves as you had before

576
00:35:39,309 --> 00:35:41,670
from the first person perspective, only it's harder

577
00:35:41,690 --> 00:35:42,030
to track.

578
00:35:43,667 --> 00:35:45,769
And we also tried doing that at a distance.

579
00:35:46,249 --> 00:35:46,450
Right?

580
00:35:48,472 --> 00:35:49,933
I think the main conclusion out of that is

581
00:35:50,313 --> 00:35:53,176
just don't make controls that are special for the game

582
00:35:53,677 --> 00:35:55,058
because you'll end up making them anyway.

583
00:35:55,398 --> 00:35:59,062
And as a result of thinking that way,

584
00:35:59,302 --> 00:36:01,644
we ended up making the actual controls that we needed.

585
00:36:02,005 --> 00:36:06,048
So we had visible motion bounds for anywhere that the robot couldn't reach.

586
00:36:06,528 --> 00:36:12,673
We also had the legal state indicator to see when the inverse kinematic system that solves for where the robot should be

587
00:36:13,573 --> 00:36:16,635
actually doesn't hit a singularity or doesn't hit a motion bound.

588
00:36:19,497 --> 00:36:23,580
We also placed all of our scenarios inside the environment

589
00:36:24,101 --> 00:36:27,583
so people can get used to doing the action inside where they should be.

590
00:36:29,748 --> 00:36:34,911
But that wasn't enough, because actually folding a pretzel itself, that task, requires some training.

591
00:36:36,031 --> 00:36:40,993
And so when people started doing it, we thought this would be enough to train them,

592
00:36:41,033 --> 00:36:43,554
by just showing them a how-to somewhere in the game.

593
00:36:43,854 --> 00:36:47,396
But it wasn't enough, so when we actually set this up, we had a rope on the table,

594
00:36:47,716 --> 00:36:49,316
so people could train on the real material.

595
00:36:49,737 --> 00:36:54,000
And this really matters because what you're actually doing when you're inside a simulation

596
00:36:54,060 --> 00:36:58,944
is you're trying to use all your proprioception, your natural senses of where the world might

597
00:36:58,984 --> 00:37:03,448
be, and you end up relying on some defaults that suck.

598
00:37:04,529 --> 00:37:10,114
So by having good intuition to what you're doing, this is helpful.

599
00:37:13,234 --> 00:37:20,959
The other thing that's super important when building a robotic simulation is to make sure you understand the difference between you and the robot.

600
00:37:20,979 --> 00:37:26,343
In this case, we had a robot that couldn't cross its arms, right?

601
00:37:26,743 --> 00:37:31,706
So this motion was illegal. Whenever I would fold a pretzel like this, the simulation would fail, right?

602
00:37:32,167 --> 00:37:35,449
And the arm would actually stop somewhere here because there was a motion bound.

603
00:37:38,297 --> 00:37:41,124
However, the robot is more robust than us in other cases.

604
00:37:41,204 --> 00:37:45,173
For example, there's no limitation that the robot could fold the pretzel when it's facing

605
00:37:45,414 --> 00:37:45,775
outside.

606
00:37:48,445 --> 00:37:54,849
So make sure you understand what the limitations of the robot are, and make sure that you understand

607
00:37:55,870 --> 00:37:59,112
how fast the system can evaluate alternative cases.

608
00:37:59,132 --> 00:38:03,355
In this case, the reinforcement learning system can search in a very limited space very quickly,

609
00:38:03,655 --> 00:38:06,897
but as soon as you move outside that space, it's actually not intuitive.

610
00:38:07,277 --> 00:38:10,419
So doing things like going above a hand.

611
00:38:10,979 --> 00:38:17,585
This is not something that a robot can intuitively do. You have to make it very, very specifically perform the one task.

612
00:38:18,666 --> 00:38:21,229
Yeah, and you have to validate your assumptions.

613
00:38:21,529 --> 00:38:22,810
Other takeaways that we had from that...

614
00:38:25,142 --> 00:38:27,963
Users will default to illegal states.

615
00:38:28,204 --> 00:38:32,166
So you have to tell them that a state is illegal.

616
00:38:32,206 --> 00:38:33,907
We used the red grippers to do that.

617
00:38:34,468 --> 00:38:35,869
We also used motion bound to do that.

618
00:38:36,689 --> 00:38:38,790
I saw some people do something like, ah, I

619
00:38:38,810 --> 00:38:39,391
don't want to do this.

620
00:38:39,911 --> 00:38:42,813
And then a hand would be stuck over here because the robot

621
00:38:42,873 --> 00:38:45,935
couldn't solve moving the hand from here down back to where

622
00:38:45,955 --> 00:38:46,275
it should be.

623
00:38:49,968 --> 00:38:56,072
The scale and the torque of the grippers and also the mass affect the user behavior.

624
00:38:57,573 --> 00:39:00,574
So you really have to have visualization and feedback in place.

625
00:39:02,676 --> 00:39:05,898
We never got it right the first time whenever we tried it, so you have to make tutorial

626
00:39:05,918 --> 00:39:06,598
levels like we did.

627
00:39:08,940 --> 00:39:12,724
And the last thing, super important, that we didn't implement and I really want to implement,

628
00:39:13,144 --> 00:39:15,607
is making the last action that you did easy to undo.

629
00:39:16,328 --> 00:39:20,793
Because you are performing an action on a robot that's affecting the environment.

630
00:39:20,873 --> 00:39:22,034
As far as you care, it's real.

631
00:39:22,054 --> 00:39:26,199
Okay, so last slide, I swear.

632
00:39:29,578 --> 00:39:32,579
When you're building human in the loop systems, here's what I

633
00:39:32,619 --> 00:39:33,299
want you to think about.

634
00:39:33,939 --> 00:39:34,919
Make the system modular.

635
00:39:35,299 --> 00:39:36,960
The more you can replace stuff in it, the

636
00:39:37,000 --> 00:39:37,760
quicker you can move.

637
00:39:38,940 --> 00:39:42,601
You have to state clear goals, because any system that you're

638
00:39:42,621 --> 00:39:45,141
making that's human in the loop has people from different

639
00:39:45,161 --> 00:39:45,682
disciplines.

640
00:39:46,742 --> 00:39:49,062
If they know what you're trying to solve, they will

641
00:39:49,102 --> 00:39:51,203
work around what you can't solve.

642
00:39:51,863 --> 00:39:54,184
So always communicate well between people in the team.

643
00:39:54,984 --> 00:39:55,884
Avoid false empathy.

644
00:39:55,924 --> 00:39:57,864
I just covered that, but it's really important to remember.

645
00:39:59,125 --> 00:40:01,566
And remember that you're building a debugger, right?

646
00:40:01,726 --> 00:40:04,207
You have to expose parameters, you have to make agents interchangeable,

647
00:40:04,487 --> 00:40:06,007
and you have to make states rewindable.

648
00:40:06,368 --> 00:40:09,469
And remember that you're building a debugger and you're also building a game.

649
00:40:10,029 --> 00:40:12,770
So you have to exploit all the game mechanics that you can,

650
00:40:13,370 --> 00:40:14,751
you have to move as fast as possible.

651
00:40:16,452 --> 00:40:16,712
Thank you.

652
00:40:16,732 --> 00:40:25,535
APPLAUSE

653
00:40:25,555 --> 00:40:25,835
Questions?

654
00:40:33,511 --> 00:40:33,811
All right.

655
00:40:33,831 --> 00:40:35,732
Oh, question.

656
00:40:35,792 --> 00:40:35,972
Yes.

657
00:40:36,932 --> 00:40:37,272
I'm sorry.

658
00:40:39,533 --> 00:40:39,773
Hi.

659
00:40:40,493 --> 00:40:43,654
How do you come up with all of those test scenarios?

660
00:40:44,234 --> 00:40:49,516
So basically it's a test scenario that you're pushing through the whole process, right?

661
00:40:51,577 --> 00:40:52,797
So I don't...

662
00:40:53,063 --> 00:41:01,846
I don't necessarily think that I can call every single of these situations a full test scenario.

663
00:41:02,206 --> 00:41:06,047
We were trying to validate if a robot can complete a certain task.

664
00:41:06,428 --> 00:41:10,289
So we're like, what is good for a human to perform that a robot can copy?

665
00:41:12,190 --> 00:41:16,431
What is a simple task in the case of Dominoes? What is a simple task that we can play with?

666
00:41:18,072 --> 00:41:19,572
Is simple for a human to understand?

667
00:41:20,293 --> 00:41:24,755
And uses the limitations of a gripper that the robot has.

668
00:41:25,055 --> 00:41:28,656
We pretty much brainstormed until we had something that works.

669
00:41:28,796 --> 00:41:32,217
There's no guidelines except for user creativity, I guess.

670
00:41:33,358 --> 00:41:35,759
I'm sorry if there was supposed to be a higher insight there.

671
00:41:35,779 --> 00:41:36,819
No, just user creativity.

672
00:41:39,540 --> 00:41:45,286
Hi, I just have a question. When you were showing the tapping Isaacs and they were

673
00:41:45,366 --> 00:41:52,412
imitating the motion capture files, you said it's important to note that their

674
00:41:52,433 --> 00:41:56,156
score was not based on joints but their overall performance. I'm just

675
00:41:56,196 --> 00:41:58,899
wondering how you scored that?

676
00:42:00,133 --> 00:42:08,200
So there are many variations on the reward function, but the end result is overall pose.

677
00:42:08,420 --> 00:42:11,503
So you can think about overall pose in many ways.

678
00:42:12,464 --> 00:42:17,689
One result would be to project from the perspective of the camera and get a least squares approximation.

679
00:42:18,049 --> 00:42:23,074
Another one would be to test the end effector positions, but not the actual joint hierarchy.

680
00:42:24,875 --> 00:42:30,498
You might have a small offset in the root joint, but still down along the hierarchy there are some corrections.

681
00:42:30,899 --> 00:42:38,643
If you were to judge joint by joint, you'd create an overfitting in the system that is going to be hard to overcome in later steps.

682
00:42:38,863 --> 00:42:43,926
So there's a preference in some composite score that's undefined, I'm aware,

683
00:42:44,746 --> 00:42:51,550
but some composite score that would be more helpful for letting the machine be more creative later on, I guess?

684
00:42:52,834 --> 00:42:55,396
That makes good sense. I've done the same thing in dance.

685
00:43:00,361 --> 00:43:01,942
Um, okay.

686
00:43:02,963 --> 00:43:14,793
So I do a lot of simulation work as well, and one thing I find is with the old guard, if you want to call them that, the physical engineers, there's a little bit of skepticism and pushing back.

687
00:43:14,853 --> 00:43:19,117
I'm curious if you've had to deal with that and how you've dealt with that.

688
00:43:24,866 --> 00:43:27,428
I think the proof is that we're getting results a lot quicker.

689
00:43:31,088 --> 00:43:37,970
I have no pretense, especially when it's like 20 or 30 year experienced roboticists,

690
00:43:38,890 --> 00:43:46,792
to claim that I am in any way improving their techniques, improving upon their techniques.

691
00:43:47,513 --> 00:43:51,874
I'm producing a new perspective that they couldn't do up until now because tech wasn't ready.

692
00:43:53,054 --> 00:43:57,355
VR just lets you do that quicker, and you know, like game engines.

693
00:43:58,136 --> 00:44:00,998
And simulation frameworks allow you to move very fast.

694
00:44:01,178 --> 00:44:02,459
And they never could.

695
00:44:02,559 --> 00:44:05,921
So if you present it that way, it's a nice carrot.

696
00:44:06,101 --> 00:44:08,743
And suddenly, your skills become complementary.

697
00:44:09,323 --> 00:44:12,306
And they begin inquiring very quickly.

698
00:44:12,686 --> 00:44:13,707
Oh, and how do you do this?

699
00:44:13,767 --> 00:44:16,208
And can we also add some stickiness to it?

700
00:44:16,328 --> 00:44:16,729
And so on.

701
00:44:17,109 --> 00:44:21,492
So I haven't found anyone who hates me for doing this so far.

702
00:44:26,625 --> 00:44:33,451
I just had a quick point of clarification. In the Dominoes example, you said there was 80,000 games.

703
00:44:33,792 --> 00:44:39,136
There were 80,000 games played in succession to train the reinforcement learning.

704
00:44:39,156 --> 00:44:40,217
But not all in VR.

705
00:44:40,638 --> 00:44:41,338
No, no, no, no, no.

706
00:44:41,955 --> 00:44:54,120
So we had a scripted agent make legal moves, and we had a reward function reward legal moves from Isaac's side.

707
00:44:55,200 --> 00:44:59,462
But the scripted network was dumb. All it could do was play legal moves.

708
00:44:59,922 --> 00:45:02,303
When it couldn't, the assimilation would just stop.

709
00:45:02,523 --> 00:45:04,764
So very early on, you'd get punished for making...

710
00:45:05,805 --> 00:45:07,347
I'm sorry, you got cut off.

711
00:45:21,138 --> 00:45:25,282
So it's a little bit, I guess, unclear to me in that example,

712
00:45:26,403 --> 00:45:30,267
what the what was the VR adding that you couldn't have done with your

713
00:45:30,327 --> 00:45:31,909
sure standard scripted methods?

714
00:45:31,969 --> 00:45:35,773
Yeah, so the scripted method only examines the scripted method only

715
00:45:35,833 --> 00:45:36,293
examines

716
00:45:37,174 --> 00:45:39,496
What is a legal move to make in front of the robot?

717
00:45:39,816 --> 00:45:43,198
But what you're actually trying to teach the robot to do

718
00:45:43,759 --> 00:45:46,260
is to play a game of dominoes like humans would play them, right?

719
00:45:46,601 --> 00:45:50,003
So I could be placing the domino not exactly in the right place,

720
00:45:50,723 --> 00:45:52,565
and I still want a legal move out of it.

721
00:45:53,165 --> 00:45:56,487
And I could be placing the domino sideways,

722
00:45:56,928 --> 00:45:59,109
and the robot would then know how to do something like...

723
00:46:00,488 --> 00:46:03,372
That was one interaction that we had.

724
00:46:03,772 --> 00:46:05,154
Say that you don't have a legal move.

725
00:46:06,255 --> 00:46:13,484
The other thing that we're trying to do is to make the robot not make extreme motions outside their motion range.

726
00:46:13,845 --> 00:46:17,990
So all these things have to be trained in a scenario that breaks the rules.

727
00:46:20,183 --> 00:46:27,225
And so the last thing is then, so how many VR games were part of that training set for the domino example?

728
00:46:27,285 --> 00:46:31,647
So you put the number of scripted games, and I guess that's what I'm trying to tease out here.

729
00:46:37,308 --> 00:46:39,289
Altogether, probably low hundreds.

730
00:46:39,589 --> 00:46:40,349
Okay.

731
00:46:41,329 --> 00:46:41,709
Maybe like 100, 150.

732
00:46:41,769 --> 00:46:41,869
Okay.

733
00:46:41,909 --> 00:46:42,009
Yeah.

734
00:46:42,069 --> 00:46:42,149
Okay.

735
00:46:42,169 --> 00:46:42,430
Thank you.

