1
00:00:07,915 --> 00:00:14,599
Hello and welcome to Loud and Clear, Improving Accessibility for Low Vision Players in Cosmonius High.

2
00:00:15,920 --> 00:00:17,441
My name is Peter Galbraith.

3
00:00:17,902 --> 00:00:20,724
I'm the Senior Accessibility Engineer at Alchemy Labs.

4
00:00:21,404 --> 00:00:26,768
I've been with Alchemy for seven and a half years and have worked on all of the VR titles our team has shipped.

5
00:00:27,988 --> 00:00:32,892
I am also the primary programmer and one of the main designers for the update we will be discussing today.

6
00:00:34,614 --> 00:00:35,475
And hello, everyone.

7
00:00:35,735 --> 00:00:41,398
I'm Jasmine Cano, the Senior Accessibility Product Manager at Alchemy Labs, where I've worked for two years.

8
00:00:41,959 --> 00:00:44,560
I own the production on the project we'll be discussing today.

9
00:00:44,920 --> 00:00:47,182
I mainly worked on design and leading plate testing.

10
00:00:49,774 --> 00:00:53,118
Alchemy Labs is an amazing team with around 70 owls.

11
00:00:53,678 --> 00:00:59,104
It's thanks to everyone's efforts and commitment to accessibility that we can be here today to share this with you.

12
00:00:59,925 --> 00:01:05,912
Accessibility is core to what we do and is a big part of Alchemy's mission of VR for everyone.

13
00:01:07,301 --> 00:01:12,884
You might recognize Alchemy Labs from our previous VR games, Job Simulator, Vacation Simulator.

14
00:01:13,644 --> 00:01:23,189
While those games contain a variety of accessibility features, we're here today to talk to you about a specific update to our most recently released game, Cosmonius High.

15
00:01:24,030 --> 00:01:28,992
Cosmonius High is a VR game where you crash land into your first day at an alien high school.

16
00:01:29,533 --> 00:01:33,915
You unlock powers that help you compete class, or sorry, complete classes.

17
00:01:34,436 --> 00:01:37,135
make friends, and save the school from cosmic chaos.

18
00:01:38,742 --> 00:01:42,485
and this talk's focus will be about the game's vision accessibility update.

19
00:01:43,065 --> 00:01:46,448
We'll go over the challenges we faced and the design choices we made.

20
00:01:47,409 --> 00:01:55,875
We also want to acknowledge that when most people think of VR, they think of putting on a headset and being immersed in the visual experience.

21
00:01:56,596 --> 00:01:58,057
It makes sense why they imagine this.

22
00:01:58,237 --> 00:02:00,319
The headset goes over their eyes after all.

23
00:02:01,379 --> 00:02:04,622
So why even try to add vision accessibility to a VR game?

24
00:02:06,158 --> 00:02:11,580
Well, the answer to that is VR has been largely inaccessible to blind and low vision players.

25
00:02:12,380 --> 00:02:15,341
But we believe it doesn't have to stay that way.

26
00:02:16,161 --> 00:02:18,782
There's more to VR than simply what is on the screen.

27
00:02:19,202 --> 00:02:22,843
Just like there's more to life than what can be experienced through visuals alone.

28
00:02:23,624 --> 00:02:27,845
And everyone should be able to experience all of what VR has to offer.

29
00:02:28,996 --> 00:02:31,918
So with that in mind, we set ourselves a lofty goal.

30
00:02:32,598 --> 00:02:40,243
We wanted to allow people with partial or total vision loss to play and enjoy all of Cosmonius High without sighted assistance.

31
00:02:41,244 --> 00:02:48,588
When we started this project, we weren't sure if we could make the entire game accessible to blind or low vision players.

32
00:02:49,389 --> 00:02:53,792
But in our mission to make VR for everyone, we knew we had to try.

33
00:02:54,987 --> 00:02:58,549
So we started by asking ourselves the big questions.

34
00:02:59,549 --> 00:03:03,071
What challenges do blind players face when trying to enjoy VR?

35
00:03:03,811 --> 00:03:05,932
And what barriers can we remove for them?

36
00:03:07,755 --> 00:03:15,056
Before we started the project, we reached out to Steve Saylor to be our primary accessibility consultant on the project.

37
00:03:15,676 --> 00:03:24,698
In our first meeting, he outlined potential challenges blind players might face, information they might need, and possibly interactions to try in VR.

38
00:03:25,778 --> 00:03:29,799
This helped inform many of the early versions of features we'll be talking about today.

39
00:03:30,679 --> 00:03:37,260
Since no one had ever tried something like this on the scale of a commercial VR game before, we knew theory would only get us so far.

40
00:03:39,277 --> 00:03:48,248
What was most clear to us was that blind players had less access to important information than sighted players.

41
00:03:49,270 --> 00:03:53,115
So the first problem we decided to tackle was the inequality of knowledge.

42
00:03:54,985 --> 00:04:00,847
Blind players were missing crucial information about the world due to how much was conveyed through visuals alone.

43
00:04:01,568 --> 00:04:10,351
From the type of objects and interactions available to whether or not the player was holding an item, there was a clear gap in knowledge that was going to hold players back.

44
00:04:12,085 --> 00:04:16,206
If we were going to make the game accessible, we knew we needed to level the playing field.

45
00:04:17,267 --> 00:04:21,708
When we started asking ourselves, what information do players need?

46
00:04:23,109 --> 00:04:26,170
We also asked ourselves, how do we deliver that information to them?

47
00:04:27,090 --> 00:04:29,471
The former would turn out to be a complex question.

48
00:04:29,991 --> 00:04:32,712
Fortunately, though, we already had an idea for the latter.

49
00:04:34,512 --> 00:04:41,593
Our proposed solution was to provide detailed information about the virtual world to the player through descriptive audio.

50
00:04:42,434 --> 00:04:57,377
But unlike movies where a single, unchanging descriptive audio track could be used to aid in the experience, we were going to need audio to react dynamically to wherever the player chose to go and whatever the player chose to do.

51
00:04:58,157 --> 00:05:03,078
And the obvious candidate tool to help us achieve that was text-to-speech.

52
00:05:04,209 --> 00:05:08,672
Text-to-speech has been a tool used in digital accessibility for many years.

53
00:05:09,393 --> 00:05:14,096
Almost every modern computer or smartphone has built-in screen reader functionality.

54
00:05:14,757 --> 00:05:21,702
And more recently, we have begun to see text-to-speech used in games to help aid players through setup and menu navigation.

55
00:05:22,702 --> 00:05:31,289
Impressed by the work others have done with TTS in their games, we wanted to find out if we could take these ideas a step further.

56
00:05:32,289 --> 00:05:39,636
we decided to try to implement our own made-for-VR version of the Accessibility Object Model, AOM for short.

57
00:05:40,797 --> 00:05:45,561
AOM is a framework designed to make webpages compatible with assistive technologies.

58
00:05:46,342 --> 00:05:56,912
However, instead of the contents of a 2D webpage, our implementation would attempt to expand the concept to a fully immersive and interactive 3D world.

59
00:05:58,766 --> 00:06:03,288
We first started this process by implementing a tool for working with text-to-speech.

60
00:06:04,248 --> 00:06:10,611
We then made it so that we could send that tool information about the items a player could pick up.

61
00:06:11,731 --> 00:06:21,255
Anytime a player hovered over, grabbed, dropped, or pointed at an object with either of their hands, that object would have its text sent to the text-to-speech tool.

62
00:06:22,152 --> 00:06:34,541
This text was actually pulled from a pre-existing system in our game called the World Item System, which contained information such as the name and a comedic in-game description of the objects.

63
00:06:36,082 --> 00:06:39,204
Then, we would play back the generated audio and voila!

64
00:06:39,484 --> 00:06:42,746
We now had audio describing all of our interactive objects.

65
00:06:44,147 --> 00:06:47,910
Well, not all of our interactive objects.

66
00:06:49,323 --> 00:06:58,828
We quickly realized that we had other interactive objects that weren't able to be picked up, but still required descriptions, so we got those hooked into the tool as well.

67
00:06:59,708 --> 00:07:12,955
At the same time, we also developed a new system that we called the Describable Item System that would allow text that appeared on objects to be read, such as the text on posters, book pages, and journals found around the school.

68
00:07:14,093 --> 00:07:19,597
This also allowed us to describe other interactive objects where adding a world item wasn't possible.

69
00:07:20,757 --> 00:07:24,180
This was our equivalent of adding alt text to images.

70
00:07:25,160 --> 00:07:32,085
So now we could have TTS on nearly all interactive and important objects in the environment.

71
00:07:32,805 --> 00:07:34,907
At this point, surely we were done, right?

72
00:07:36,782 --> 00:07:44,046
Well, when we began internal testing, we realized that we had overlooked some major aspects of the game.

73
00:07:44,927 --> 00:07:54,813
The most notable example was our tutorial pop-ups that were just animated pictures and didn't use the world item system and wouldn't work with the describable item system.

74
00:07:55,533 --> 00:08:03,178
So after a bit of reworking and tweaking, we got tutorials to trigger a TTS description automatically upon being shown to the player.

75
00:08:05,053 --> 00:08:08,214
and instantly ran into another blocker during testing.

76
00:08:08,934 --> 00:08:14,256
We hadn't yet accounted for how the player would know where they were as they tried to navigate around their space.

77
00:08:15,316 --> 00:08:23,059
We decided to make it so that while the player was aiming the teleport cursor, the system would provide information about key locations of interest.

78
00:08:23,859 --> 00:08:33,482
We would also provide an additional text-to-speech confirmation after teleporting to let the player know where they had arrived and when they arrived at the desired destination.

79
00:08:35,096 --> 00:08:42,259
However, once we began running external tests, it became apparent that we still hadn't provided enough location information.

80
00:08:43,080 --> 00:08:52,984
Internally, we knew the layout of our game, but external accessibility testers who were not familiar with the game were getting lost because there weren't enough marked up areas.

81
00:08:53,924 --> 00:08:59,267
So we went back and fleshed out even more locations throughout the game with additional teleport zones.

82
00:09:00,499 --> 00:09:12,427
Unlike some of the other aspects of getting this text-to-speech working on objects, there wasn't a simple way of automating this process, and these zones had to be hand-authored by our technical designers.

83
00:09:13,427 --> 00:09:13,828
Shout out.

84
00:09:15,529 --> 00:09:22,913
But the good news is that after the new zones had been added, the frequency with which our testers got lost decreased.

85
00:09:23,554 --> 00:09:25,355
So we took that as a win.

86
00:09:27,915 --> 00:09:31,598
Finally, we had achieved a pretty thoroughly described world.

87
00:09:32,158 --> 00:09:39,564
There was some content that was less detailed than we would have liked, but this part of the project took much more time than initially expected.

88
00:09:40,545 --> 00:09:45,949
So, we reduced our scope to focus on what was required for the player to reach the end of the game.

89
00:09:46,829 --> 00:09:52,514
By the end of this process, a huge amount of the game had been hooked up to play TTS descriptions.

90
00:09:53,275 --> 00:09:53,655
We did it!

91
00:09:54,376 --> 00:09:58,118
Players could learn nearly anything they wanted about the game world.

92
00:09:59,619 --> 00:10:05,143
Unfortunately, merely offering descriptions on objects wasn't enough.

93
00:10:06,764 --> 00:10:09,446
Players had to be able to find the objects.

94
00:10:10,187 --> 00:10:11,608
We identified early on

95
00:10:12,288 --> 00:10:17,471
that it would be difficult or even impossible for blind players to locate objects in the game world.

96
00:10:18,211 --> 00:10:23,054
We knew we were going to have to provide more information to the player in order for them to play the game.

97
00:10:24,475 --> 00:10:32,440
Sightless players were a particular concern at this point, since one of the main ways sighted players found objects was by simply looking around the room.

98
00:10:33,260 --> 00:10:39,524
And even if the players had some vision, it was important that they could distinguish interactive objects from the environment.

99
00:10:40,284 --> 00:10:54,793
With object interactions being a core part of alchemy games, we knew that if we couldn't provide people with the resource they needed to play, then the dream of making our game work for blind players would be over before it ever really started.

100
00:10:56,314 --> 00:11:05,220
Fortunately, our previous accessibility knowledge gave us a general idea on what we could do and what was needed to make this happen.

101
00:11:06,994 --> 00:11:13,497
In accessibility and across all design, it's generally a good idea to convey information in multiple ways.

102
00:11:14,518 --> 00:11:18,240
This increases the chance that users will be able to access your content.

103
00:11:19,080 --> 00:11:23,702
In a spatial computing environment like VR, this becomes even more important.

104
00:11:24,603 --> 00:11:30,506
For players to locate objects, we're going to have to take advantage of every feedback channel available to us.

105
00:11:32,767 --> 00:11:35,368
And that started with auditory feedback.

106
00:11:36,570 --> 00:11:46,954
Though virtual 3D environments present accessibility challenges that aren't present in other mediums, VR affords us some unique opportunities as well.

107
00:11:47,834 --> 00:11:53,916
Traditional screen readers don't indicate the position of text on the screen because they don't really need to.

108
00:11:54,617 --> 00:12:03,760
But for our implementation, we realized that we could provide more location context without adding more text to our spoken descriptions.

109
00:12:05,007 --> 00:12:09,811
As it turns out, people are pretty decent at detecting the direction a sound is coming from.

110
00:12:10,631 --> 00:12:15,195
Additionally, volume is a good indicator of how far away or close an object is.

111
00:12:16,016 --> 00:12:22,521
If you've ever lost your phone and tried finding it by listening to it ringing, you've used these techniques.

112
00:12:24,302 --> 00:12:27,585
Now, this isn't going to be perfectly accurate.

113
00:12:28,359 --> 00:12:37,827
But, by using spatialized audio that plays from the location of an object in the world, we could give our players a general idea of the location of an object.

114
00:12:38,508 --> 00:12:39,809
So, that's what we did.

115
00:12:40,830 --> 00:12:50,038
And with that, players could gain a sense of the approximate position of objects in the virtual world, which meant that we could begin using other methods to help them narrow their search.

116
00:12:52,530 --> 00:12:54,111
Now onto haptic feedback.

117
00:12:54,852 --> 00:12:58,635
In the physical world, people can reach out and feel nearby objects.

118
00:12:59,356 --> 00:13:06,802
One of the limitations of VR is that there's no physical resistance that allows you to feel that those objects are there in the virtual world.

119
00:13:07,422 --> 00:13:11,226
Since we can't change that, we chose to take a lesson from white cane users.

120
00:13:12,338 --> 00:13:15,663
Some blind folks use white canes to help them navigate the environment.

121
00:13:16,364 --> 00:13:24,014
As they're walking, they may sweep their cane across the space in front of them, feeling for obstacles and changes in texture or elevation.

122
00:13:24,795 --> 00:13:29,321
Detecting the change from one object or surface to another was something that we could do as well.

123
00:13:30,262 --> 00:13:38,545
In an attempt to recreate the functionality, we added a haptic pulse for when the player moved the teleportation cursor into a different area.

124
00:13:39,265 --> 00:13:43,067
By doing this, we were able to signal the boundaries of those zones to the player.

125
00:13:44,207 --> 00:13:48,129
In an attempt to recreate the functionality, I just said that.

126
00:13:48,349 --> 00:13:49,049
Sorry about that.

127
00:13:49,549 --> 00:13:58,873
Furthermore, we made it so that when a player moved their open hand over something, it could be interacted with, and the controller would also give a short vibration.

128
00:13:59,855 --> 00:14:06,818
This little bit of added context meant that players knew that their hand was near something that they could attempt to interact with.

129
00:14:07,778 --> 00:14:09,479
Or get more information about it, too.

130
00:14:11,980 --> 00:14:14,661
There was still one feedback channel we had yet to address.

131
00:14:15,461 --> 00:14:16,301
Visual feedback.

132
00:14:17,642 --> 00:14:20,803
Steve Saylor suggested we implement a high contrast mode.

133
00:14:21,423 --> 00:14:28,686
Since blindness doesn't always mean sightlessness, it was important to add clear visual indicators to the world as well.

134
00:14:29,879 --> 00:14:38,665
Some of the most accessible implementations of high contrast modes drastically changed the visuals of an entire game to help highlight the most important things on screen.

135
00:14:41,587 --> 00:14:46,370
And though we wanted to do this, we didn't have the available resources for a feature like this.

136
00:14:47,030 --> 00:14:52,314
Thankfully, a complete reskinning isn't the only way to improve contrast and object definition.

137
00:14:53,475 --> 00:14:53,715
Since

138
00:14:54,739 --> 00:15:02,526
As early as Job Simulator, we used a system that added a light blue highlight around objects that could be grabbed.

139
00:15:03,506 --> 00:15:13,935
And since there are static objects that the player needed to know about, we decided to extend that system to add a light green highlight on important environmental objects.

140
00:15:14,696 --> 00:15:19,940
We chose light green as the color because it stood out well against the mostly purple environment.

141
00:15:22,928 --> 00:15:30,752
But more important than the hue is making sure that the contrast is clear enough to show that something changed.

142
00:15:32,733 --> 00:15:39,777
Now that players could position themselves near interactive objects that were providing different forms of feedback, we had a new problem.

143
00:15:40,557 --> 00:15:45,320
Assistive descriptions were being constantly and unintentionally triggered.

144
00:15:46,318 --> 00:15:51,760
And this was happening with just about any interaction that the player took.

145
00:15:52,340 --> 00:15:59,082
So, for example, a player could grab their backpack when they're teleporting, or even simply reaching their hand out.

146
00:15:59,503 --> 00:16:04,004
There is very little a player could do without setting off the TTS.

147
00:16:06,465 --> 00:16:12,467
Now that players could learn about anything and everything, and essentially had no other choice,

148
00:16:13,623 --> 00:16:17,796
The synthesized speech became a dominant and cacophonous nightmare.

149
00:16:19,215 --> 00:16:23,956
It drowned out subtle sounds and all the audio, and it made it sound incomprehensible.

150
00:16:24,636 --> 00:16:29,598
It was an unpleasant experience, and it felt like drinking from an auditory fire hose.

151
00:16:30,218 --> 00:16:31,238
It was simply too much.

152
00:16:32,098 --> 00:16:34,319
It was like happening all at once, all the time.

153
00:16:35,139 --> 00:16:43,381
And something had to be done to prevent players from being constantly bombarded with information that eventually just became noise.

154
00:16:45,200 --> 00:16:51,965
So our plan was to improve player agency by giving them more direct control over their individual experience.

155
00:16:52,725 --> 00:16:56,048
But what could we do to put the power back into the hands of the player?

156
00:16:56,968 --> 00:16:58,850
What tools did we have at our disposal?

157
00:17:00,411 --> 00:17:03,813
Lucky for us, we had an ace up our sleeve for just such an occasion.

158
00:17:04,674 --> 00:17:13,120
For all our supported platforms and their different controllers, we had reserved one button per hand that had no existing functionality.

159
00:17:14,238 --> 00:17:18,559
These reserved buttons would quickly find their new purpose as the assist button.

160
00:17:19,459 --> 00:17:28,521
The assist button was designed to be a simple yet context-sensitive action button that would allow the player to be in control of what information they received and when they received it.

161
00:17:29,461 --> 00:17:41,684
Now, instead of automatically playing audio when hovering over or grabbing an object, players could choose if and when to have the object's description read aloud by simply tapping the assist button.

162
00:17:43,179 --> 00:17:51,102
And more importantly, the player could also now tap the same button to cancel currently playing TTS descriptions.

163
00:17:52,063 --> 00:17:58,165
This meant that players no longer had to listen to the entire description before getting back to whatever they were doing.

164
00:17:59,606 --> 00:18:04,388
Now we did briefly experiment with having the assist button function as a toggle rather than a tap.

165
00:18:05,252 --> 00:18:11,777
But during testing, players would often forget that they had toggled it on on one hand, but not the other.

166
00:18:12,678 --> 00:18:18,543
And this led to the original problem where they would be accidentally triggering audio at unwanted times again.

167
00:18:19,083 --> 00:18:23,207
So we chose to revert back to using the original tap behavior instead.

168
00:18:24,327 --> 00:18:30,953
Overall, though, the introduction of the assist button appeared to be a noticeable improvement to the experience.

169
00:18:32,741 --> 00:18:37,644
However, this still didn't quite solve the problem of overlapping audio.

170
00:18:38,064 --> 00:18:48,611
It was definitely frustrating to have a character speak while trying to listen to assistive information, but delaying a character speaking wasn't always an option for us.

171
00:18:49,491 --> 00:18:58,337
And delaying the assistive audio until after all the characters were finished speaking would make the feature feel unresponsive and broken.

172
00:18:59,295 --> 00:19:15,888
So, in sticking with the principle of respecting players' choices, we made it so that when assistive information was playing, we would duck the rest of the game's audio, lowering the volume so that the audio from the text-to-speech description could be heard clearly.

173
00:19:16,789 --> 00:19:24,215
This allowed players to still be aware of other audio playing and cancel the assistive information if they wanted to hear what was being said,

174
00:19:25,207 --> 00:19:33,492
or they could choose to ignore NPC dialogue and instead listen to the information they had chosen to seek out.

175
00:19:34,513 --> 00:19:48,081
And since the game already allowed players to wave at an NPC to get a reminder of what they are likely supposed to be doing, the exact audio might be missed, but important game information wouldn't be lost.

176
00:19:49,542 --> 00:19:59,150
As an added benefit of using audio ducking, the player now knew when the description had finished playing because the game audio would return to normal levels.

177
00:20:00,011 --> 00:20:12,502
And in circumstances where the audio could take a little longer to be generated, the player could tell that their request was actually being processed and wouldn't impatiently tap the assist button over and over.

178
00:20:14,692 --> 00:20:20,418
Unsurprisingly, testers were universally on board with these new additions.

179
00:20:20,999 --> 00:20:26,005
Players were now able to use assistive audio as much or as little as they wanted.

180
00:20:27,086 --> 00:20:29,789
They wouldn't have to worry about accidentally missing information.

181
00:20:30,890 --> 00:20:36,096
Now that players had greater control over their experience, a new issue began to reveal itself in testing.

182
00:20:37,430 --> 00:20:40,512
Testers weren't activating assistance on objects very often.

183
00:20:41,253 --> 00:20:45,836
At first, we were concerned that they were forgetting which button on the controller they had to press.

184
00:20:46,377 --> 00:20:53,042
But during post-plate test feedback sessions, it became clear that using assistance didn't always feel beneficial.

185
00:20:54,812 --> 00:21:03,974
Testers noted that it was hard to determine what information in the assistive audio was actually important to them, and it was a valid criticism.

186
00:21:04,714 --> 00:21:12,716
In our efforts to bridge the knowledge gap for these players, we had overcorrected and provided more information than they really needed.

187
00:21:13,736 --> 00:21:19,837
Another issue was that some testers would try to patiently wait to hear every part of the description,

188
00:21:20,549 --> 00:21:22,891
so that they wouldn't miss crucial information.

189
00:21:23,691 --> 00:21:30,956
But it became a problem since some of the important information was sometimes mixed in with superfluous information.

190
00:21:31,596 --> 00:21:39,642
That means that if a player got distracted or disinterested part way through, they'd have to re-listen to the whole thing all over again.

191
00:21:41,474 --> 00:21:47,839
We figured the solution to this problem was just to make sure we presented the information more clearly and concisely.

192
00:21:48,780 --> 00:21:56,527
To do that, first we had to identify what specifically needed clarifying and where we can make changes without losing information.

193
00:21:58,731 --> 00:22:04,414
The first thing we identified was that the order we were presenting the information mattered.

194
00:22:04,914 --> 00:22:14,118
We knew we wanted to present the most relevant and valuable information first and decided it would be helpful to standardize the order that information was read.

195
00:22:15,433 --> 00:22:19,697
It made sense to put information that was the shortest and always present first.

196
00:22:20,238 --> 00:22:28,165
So, interaction state information, like whether or not the object was grabbed, and the object's name were prioritized.

197
00:22:28,906 --> 00:22:38,716
Next, we added a short description that contained the most commonly needed information about the object, and would play that immediately after the object's name.

198
00:22:39,889 --> 00:22:49,598
Finally, the in-universe descriptions were placed at the end due to typically being the longest component of the description and including a lot of lore.

199
00:22:51,880 --> 00:22:55,423
And this format was more logical and flowed more nicely.

200
00:22:58,206 --> 00:23:02,990
The audio that would play, or as an example, the audio that would play

201
00:23:04,961 --> 00:23:20,437
You can look at the, if you see the image on this slide, it would now sound something like hovering, star shades, sunglasses with star shaped lenses, clearly the only accessory for the next ultraviolet.

202
00:23:24,421 --> 00:23:30,913
Even when placed at the end, that in-universe description proved to be a source of confusion and frustration.

203
00:23:31,534 --> 00:23:37,425
Some players would get hung up thinking that jokes and flavor text were important clues to beating the game.

204
00:23:38,630 --> 00:23:54,140
And after talking with James Rath, who was both one of our testers and an accessibility consultant in his own right, he recommended that we follow the guidelines used for image alt text on the web and remove information that wasn't important.

205
00:23:55,021 --> 00:24:05,808
At first, we were hesitant to take this suggestion, since this information was available to sighted players through other areas and means in the game.

206
00:24:07,830 --> 00:24:16,894
When we took a step back to think about it, it was clear that these in-universe descriptions provided very little actual value to the player and were doing more harm than good.

207
00:24:17,654 --> 00:24:19,635
So, we opted to remove them entirely.

208
00:24:20,695 --> 00:24:31,340
This ended up being a worthwhile trade, because in addition to improving the signal-to-noise ratio in the assistive descriptions, this also meant that less text would have to be turned into speech

209
00:24:31,918 --> 00:24:36,542
speeding up the audio processing time and making everything feel more responsive.

210
00:24:37,203 --> 00:24:37,923
It was a win-win.

211
00:24:40,226 --> 00:24:49,754
After reorganizing the information and trimming the excess, we observed that players were interacting with the feature and the game at large with a higher degree of confidence and speed.

212
00:24:50,515 --> 00:24:51,616
Knowing the information

213
00:24:53,262 --> 00:24:53,522
Sorry.

214
00:24:53,763 --> 00:25:03,992
Knowing the information order allowed players to feel confident in canceling the audio description sooner and allowed players to stay in that flow state.

215
00:25:05,033 --> 00:25:11,078
Additionally, text-to-speech processing time and audio clip lengths were now shorter, which seemed to keep players more engaged.

216
00:25:12,496 --> 00:25:17,101
So all of these features seemed like they worked, and we believed that they would help our players.

217
00:25:17,581 --> 00:25:22,086
But there was one major outstanding problem that had been presented from the beginning.

218
00:25:22,846 --> 00:25:26,770
Turning on this assist mode, it still required sighted assistance.

219
00:25:27,651 --> 00:25:32,216
And what good is an assist mode if it's inaccessible to the people who need it?

220
00:25:34,866 --> 00:25:43,549
The mode was enabled by a light switch-like toggle in the game world where the player initially started, as well as one that was available in the player's virtual backpack.

221
00:25:44,269 --> 00:25:50,312
But players couldn't simply feel around for this switch like they could in the real world, even if they knew it was there.

222
00:25:51,412 --> 00:26:13,707
Our workaround for this during the earlier stages of accessibility testing was to provide a keyboard hotkey that could be used to turn this mode on, which worked well enough for players using VR on their PC, but we planned for this feature to ship on standalone and console headsets where it was all but guaranteed that a player would not have a keyboard.

223
00:26:14,860 --> 00:26:18,202
Throughout the development process, this issue weighed on our minds.

224
00:26:18,762 --> 00:26:30,249
We needed a way to turn this mode on with only things that were common across all platforms, and we wanted it to work with our existing accessibility features, like our one-handed mode.

225
00:26:31,930 --> 00:26:40,675
So, we took stock of what we still had left to work with, which amounted to one spatially tracked controller and one spatially tracked headset.

226
00:26:41,496 --> 00:26:42,737
It didn't seem like a lot.

227
00:26:43,655 --> 00:26:45,216
until we turn the problem on its head.

228
00:26:46,256 --> 00:26:53,158
If players couldn't figure out where they were in our world, maybe we could figure out where they were in their own.

229
00:26:55,219 --> 00:26:57,820
We had forgotten one of the things that makes VR special.

230
00:26:58,720 --> 00:26:59,480
Proprioception.

231
00:27:00,361 --> 00:27:07,703
For those who are unfamiliar with the term, proprioception is the sense a person has of where their body is in relation to itself.

232
00:27:08,763 --> 00:27:13,065
It's why if I asked you to close your eyes and touch your nose, you could.

233
00:27:14,694 --> 00:27:22,919
Once we began considering a player's proprioception, it meant that both the game and the player could know where the controller was relative to the headset.

234
00:27:23,760 --> 00:27:34,246
With this information, we could ask the player to perform an action relative to their own body, attempt to detect that action, and turn on the vision assistance mode for them.

235
00:27:35,687 --> 00:27:38,389
But what kind of actions could we consistently identify?

236
00:27:39,329 --> 00:27:40,570
Gestures were one option.

237
00:27:42,008 --> 00:27:50,592
Around the time of the Vision Accessibility update being in development, I had been slowly playing through Horizon Forbidden West in my spare time.

238
00:27:51,332 --> 00:27:53,433
And one day while I was playing, it hit me.

239
00:27:54,693 --> 00:28:02,616
In the game, the main character uses a small device worn near her ear that she taps to get additional context information about the world.

240
00:28:03,537 --> 00:28:06,058
The answer had been in front of my face for months.

241
00:28:07,048 --> 00:28:12,970
We could infer the approximate location of the player's head and have the player perform an action near their ear.

242
00:28:13,810 --> 00:28:20,812
We already were using a similar action to allow the player to pull their backpack from over their shoulder, though, so grab wasn't an option.

243
00:28:21,652 --> 00:28:24,712
We could, however, have the player double-tap the assist button.

244
00:28:26,333 --> 00:28:32,675
This idea worked, and the result was that players now could activate the mode without sighted assistance.

245
00:28:33,995 --> 00:28:34,395
Kind of.

246
00:28:35,840 --> 00:28:42,545
In truth, we should have done a better job making sure players knew right away about this method of activating the vision assistance mode.

247
00:28:43,385 --> 00:28:47,108
There is explanatory audio that tells the player about this shortcut.

248
00:28:47,849 --> 00:28:51,031
However, it doesn't trigger automatically upon starting the game.

249
00:28:52,279 --> 00:28:58,082
So unless a player had heard about this action through word of mouth, they might still need sighted assistance.

250
00:28:59,062 --> 00:29:07,526
And though this may not have been the ideal situation, blind players faced another challenge before even that one, launching the game.

251
00:29:08,566 --> 00:29:17,730
Unfortunately, no matter what we did with this update, certain players would still require assistance to navigate the process of setting up a headset and launching the game.

252
00:29:19,330 --> 00:29:26,332
So with complete independence still unavailable to some players, we decided that we would keep this feature as is for the time being.

253
00:29:27,493 --> 00:29:35,955
We had to ship it and hope it still provided benefit to some players, which meant it was time for us to face the scariest question.

254
00:29:37,456 --> 00:29:38,596
Was it all worth it?

255
00:29:39,216 --> 00:29:44,278
Would this update actually help blind and low vision players who wanted to experience Cosmonia's High?

256
00:29:45,530 --> 00:29:53,653
While conducting external playtests with people who were legally blind or had low vision, we sent them a survey afterwards to get their sentiment about this mode.

257
00:29:54,314 --> 00:30:06,538
We were ecstatic to see that when asked if they would recommend this feature to a friend or colleague who also has low vision, their results on a scale from 1 to 5, with 5 being highly likely to recommend, were mostly 5.

258
00:30:07,779 --> 00:30:09,720
No one was on the lower range of 1 and 2.

259
00:30:12,500 --> 00:30:21,708
When talking to accessibility experts, we heard a lot of praise and got a lot of great feedback that would help us create even better versions and have better insights for future games.

260
00:30:26,032 --> 00:30:29,495
Looking at March 14th of last year, that's when we pushed the button.

261
00:30:29,595 --> 00:30:30,276
We shipped it.

262
00:30:30,956 --> 00:30:37,342
We shipped the Vision Accessibility Update to Steam for PC VR, Quest 2, and PlayStation VR 2.

263
00:30:38,430 --> 00:30:46,238
From that point forward, we've had a steady amount of people reach out to us over time, people who played Cosmonius High and played with the feature.

264
00:30:47,199 --> 00:30:52,645
They let us know that they found the feature useful and have expressed their gratitude for making a game that they are able to play.

265
00:30:55,781 --> 00:31:01,369
We received some periodic updates from Meta about how many TTS requests the system had received.

266
00:31:01,930 --> 00:31:10,663
A request is made each time a player uses that accessibility button to hear a description when they teleport with that vision assistance on.

267
00:31:15,052 --> 00:31:20,375
for March through September 2023, 9 million text-to-speech requests were made.

268
00:31:21,396 --> 00:31:23,497
And that's just coming from Quest data alone.

269
00:31:24,557 --> 00:31:27,419
I've heard other developers ask, why even do this?

270
00:31:28,019 --> 00:31:30,601
If they can't see it, maybe it's not meant for them.

271
00:31:31,521 --> 00:31:37,505
And I would point out to those people that, first of all, excluding people because of their disabilities is just wrong.

272
00:31:38,677 --> 00:31:40,518
People who are blind also play games.

273
00:31:41,379 --> 00:31:45,102
Additionally, we have proof that blind people want to play VR too.

274
00:31:45,843 --> 00:31:51,127
And I'm pretty sure we are all here because we like to make games.

275
00:31:51,767 --> 00:31:56,491
We want as many people to enjoy them and experience that feeling that comes with gaming.

276
00:31:57,332 --> 00:32:04,958
So going back to what we said early on, we at Alchemy Labs want to make VR for everyone, and everyone includes people with disabilities.

277
00:32:07,469 --> 00:32:12,052
So let's go over where we're at now with VR accessibility.

278
00:32:13,092 --> 00:32:14,493
Is VR accessible to everyone?

279
00:32:14,813 --> 00:32:16,274
No, absolutely not.

280
00:32:16,995 --> 00:32:20,577
The good news is that it's improving and we're seeing progress all around.

281
00:32:24,428 --> 00:32:30,314
We learned so much from this experience, and there's way more that we didn't have time to cover.

282
00:32:31,195 --> 00:32:46,810
Our hope is that by educating you about some of the ways we approach solving the challenges that blind and low vision players face, you'll be able to build upon this work and help us make VR a more inclusive medium.

283
00:32:51,054 --> 00:32:51,555
Thank you.

284
00:32:51,896 --> 00:32:57,929
If anyone has any questions or wants to connect with us, we'll be in the rapid rooms outside.

