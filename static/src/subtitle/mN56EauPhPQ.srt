1
00:00:07,285 --> 00:00:08,606
All right, let's get right into it.

2
00:00:09,406 --> 00:00:10,507
Here's the agenda for this talk.

3
00:00:11,307 --> 00:00:12,528
We're gonna introduce ourselves.

4
00:00:12,869 --> 00:00:14,850
We're gonna talk about the audio direction for Division 2,

5
00:00:15,811 --> 00:00:16,631
the Snowdrop engine,

6
00:00:17,292 --> 00:00:19,733
the Raycast-based runtime audio systems in Snowdrop.

7
00:00:20,454 --> 00:00:21,735
We're gonna cover procedural reverb

8
00:00:21,795 --> 00:00:23,176
and ambient systems for interiors.

9
00:00:23,196 --> 00:00:25,677
We're gonna talk about immersive weather sounds

10
00:00:26,518 --> 00:00:27,539
and Dolby Atmos support.

11
00:00:28,579 --> 00:00:30,581
And we'll finish off with a short conclusion.

12
00:00:32,053 --> 00:00:33,915
So my name is Simon Kudryavtsev.

13
00:00:34,475 --> 00:00:36,117
I'm an audio director for the Division series.

14
00:00:37,318 --> 00:00:39,880
I've worked on Division 1, 2, and Far Cry,

15
00:00:39,960 --> 00:00:41,301
Assassin's Creed, and World in Conflict.

16
00:00:42,662 --> 00:00:43,924
Hi, my name is Robert Banton.

17
00:00:44,524 --> 00:00:46,326
I'm an audio programmer on the Snowdrop team.

18
00:00:47,247 --> 00:00:49,368
And similarly, I worked on Division 2 with Simon.

19
00:00:50,049 --> 00:00:51,610
And previously, I've worked on other games.

20
00:00:51,871 --> 00:00:54,753
I list here Dirt 4 and Guitar Hero Live as examples.

21
00:00:56,876 --> 00:01:00,119
So we work at Massive Entertainment, which is a Ubisoft studio.

22
00:01:00,139 --> 00:01:05,264
It's been open since 1997, located in the south of Sweden in MalmÃ¶,

23
00:01:05,984 --> 00:01:08,427
and became part of the Ubisoft family in 2008.

24
00:01:09,748 --> 00:01:14,052
Last year we grew beyond more than 670 staff, including 50 different nationalities.

25
00:01:14,997 --> 00:01:17,300
Currently we're working on Division 2 and Avatar.

26
00:01:18,402 --> 00:01:20,024
The audio team during peak production

27
00:01:20,064 --> 00:01:22,127
were around 25 people on Division 2,

28
00:01:22,548 --> 00:01:24,892
collaborating across Ubisoft Studios in the UK,

29
00:01:25,132 --> 00:01:25,973
States, and Europe.

30
00:01:26,254 --> 00:01:28,898
But the core team is around 12 people at Massive.

31
00:01:30,640 --> 00:01:35,602
So this talk is obviously based around the game we shipped last year called The Division 2,

32
00:01:36,142 --> 00:01:39,444
and I wanted to kind of first show the kind of core pillars of the game,

33
00:01:40,124 --> 00:01:46,227
which is that it is a modern clancy shooter RPG. It's grounded in reality, set in a collapsing

34
00:01:46,247 --> 00:01:50,489
world where you are agents of change, and it's always online and code-friendly.

35
00:01:51,529 --> 00:01:57,393
In simpler terms, it's a third-person online co-op shooter in a huge open world where you get sent in as a last resort

36
00:01:57,833 --> 00:02:02,375
to try and save the city and fight the enemy factions that seek to take control amongst the chaos.

37
00:02:03,056 --> 00:02:08,519
So whilst the first game was set in New York, we decided to have the sequel to be set in Washington DC.

38
00:02:09,399 --> 00:02:13,322
And why is that? Well, we wanted to give our players a fresh play field.

39
00:02:14,762 --> 00:02:18,446
DC is obviously vastly different compared to New York City.

40
00:02:19,566 --> 00:02:25,011
It's an iconic location with plenty of recognizable landmarks that helps ground things in reality.

41
00:02:25,732 --> 00:02:29,515
And timeline where seven months later after the events of the first game.

42
00:02:30,175 --> 00:02:32,918
It's also hot summer instead of winter in New York.

43
00:02:33,278 --> 00:02:39,283
And it's also has vastly different nature and wildlife, which also gave us a lot of opportunities with sound.

44
00:02:40,084 --> 00:02:41,885
So here's a small teaser from the second game.

45
00:02:56,030 --> 00:02:57,271
How could this have happened here?

46
00:03:01,214 --> 00:03:04,796
Of all the capitals, of all the nations on Earth...

47
00:03:09,239 --> 00:03:11,040
None more vigilant.

48
00:03:14,322 --> 00:03:15,062
Safeguarded.

49
00:03:16,603 --> 00:03:17,064
Prepared.

50
00:03:20,866 --> 00:03:21,507
Was it vanity?

51
00:03:26,880 --> 00:03:30,563
Bad luck, or something more sinister?

52
00:03:34,447 --> 00:03:35,769
One thing is beyond question.

53
00:03:36,709 --> 00:03:38,411
This is where we push back.

54
00:03:42,335 --> 00:03:44,317
This is our defining moment.

55
00:03:47,820 --> 00:03:50,062
If we fail, our nation falls.

56
00:03:54,704 --> 00:03:57,538
We are the free world's last line of defense.

57
00:04:08,664 --> 00:04:11,026
So the audio direction for Division 2.

58
00:04:11,546 --> 00:04:12,907
Obviously we're a Tom Clancy brand,

59
00:04:13,448 --> 00:04:16,831
so we wanted to keep things as grounded as possible

60
00:04:17,011 --> 00:04:19,493
and where we can't really go too much into sci-fi land.

61
00:04:20,334 --> 00:04:22,176
The world in our game plays a huge part,

62
00:04:22,236 --> 00:04:25,038
so making it dynamic, responsive, and immersive was key.

63
00:04:26,360 --> 00:04:28,101
We really wanted to have a polished mix

64
00:04:28,321 --> 00:04:30,483
that adapts to player actions and location,

65
00:04:31,845 --> 00:04:33,526
so the player can really tell what's going on

66
00:04:33,566 --> 00:04:34,487
just with help of audio.

67
00:04:35,151 --> 00:04:39,694
The goals for the team were to improve on the established identity that we did in the first game,

68
00:04:40,254 --> 00:04:42,895
but with better assets, improved workflow and tools.

69
00:04:43,736 --> 00:04:46,137
And again, the worlds that we're building are huge,

70
00:04:46,357 --> 00:04:52,601
so we needed to come up with a way, a smart way to implement and utilize the world that's already built in our advantage.

71
00:04:53,582 --> 00:04:57,564
We also wanted to increase variety and quality of our assets.

72
00:04:59,725 --> 00:05:00,486
And the Snowdrop engine?

73
00:05:02,047 --> 00:05:04,048
So the Snowdrop engine is developed.

74
00:05:05,462 --> 00:05:07,703
in Melmo by Massive.

75
00:05:08,924 --> 00:05:10,005
It's a cross-platform engine.

76
00:05:11,125 --> 00:05:13,707
It has a graphical editor, and most of the behavior

77
00:05:13,727 --> 00:05:14,347
is node-based.

78
00:05:15,268 --> 00:05:15,908
It's flexible.

79
00:05:15,949 --> 00:05:17,369
It has a modular architecture, which

80
00:05:17,389 --> 00:05:18,870
allows you to make different types of games with it.

81
00:05:20,211 --> 00:05:22,032
And we also have an abstraction that

82
00:05:22,053 --> 00:05:24,234
connects us to the Wwise Audio middleware, which allows

83
00:05:24,274 --> 00:05:27,096
us to do all our sound design.

84
00:05:28,577 --> 00:05:30,098
And as an example of the different types of games

85
00:05:30,118 --> 00:05:32,219
that you can make with Snowdrop, here are some things

86
00:05:32,239 --> 00:05:32,920
that are already shipped.

87
00:05:33,595 --> 00:05:35,297
Obviously, the first two divisions are very similar,

88
00:05:35,357 --> 00:05:39,000
but then you have Mario Rabbids versus Kingdom Battle,

89
00:05:40,001 --> 00:05:44,685
and Fractured But Whole, and Starlink Battle for Atlas.

90
00:05:46,746 --> 00:05:50,429
These are many other soft titles that are in production

91
00:05:50,510 --> 00:05:51,390
that are using Snowdrop.

92
00:05:51,911 --> 00:05:53,952
So the question is, why use Snowdrop?

93
00:05:55,654 --> 00:05:57,115
One of the main things that we try to maintain

94
00:05:57,155 --> 00:05:58,276
is a quick iteration speed.

95
00:05:58,636 --> 00:05:59,977
We want to empower the end user,

96
00:06:00,278 --> 00:06:01,759
which is why we have the node-based editing.

97
00:06:02,918 --> 00:06:06,022
better real-time processing based on the physical properties of the game world,

98
00:06:06,843 --> 00:06:08,705
leverage of offline background processes,

99
00:06:08,785 --> 00:06:11,489
so we have nightly builds that do a lot of data processing for us,

100
00:06:12,630 --> 00:06:15,614
and better reuse of, in our case, custom DSP algorithms,

101
00:06:15,814 --> 00:06:18,037
so we're not constantly authoring new plugins for every game.

102
00:06:19,897 --> 00:06:22,820
And so before we go into the meat of this talk,

103
00:06:23,060 --> 00:06:25,282
I wanted to go back and mention the two problems

104
00:06:25,302 --> 00:06:27,904
that we wanted to solve going into the Division 2.

105
00:06:29,225 --> 00:06:32,007
It is how do we increase the quality

106
00:06:32,107 --> 00:06:33,969
and variety of the audio in our game?

107
00:06:34,850 --> 00:06:37,292
And it is how do we decrease manual labor?

108
00:06:38,453 --> 00:06:42,656
We found that Raycasts for audio was a good solution for us.

109
00:06:42,896 --> 00:06:45,178
So before we go into the different systems

110
00:06:45,238 --> 00:06:47,280
based around that, let's learn about the basics

111
00:06:48,341 --> 00:06:48,861
for Raycasts.

112
00:06:50,222 --> 00:06:53,022
So, raycasts, what are they used for?

113
00:06:54,163 --> 00:06:56,123
This diagram just demonstrates a plan view

114
00:06:56,423 --> 00:06:57,684
of a very simple L-shaped room,

115
00:06:58,044 --> 00:07:00,004
and the black dot is where the player is supposed to be.

116
00:07:00,304 --> 00:07:02,485
And I'm demonstrating eight raycasts pointing out,

117
00:07:02,665 --> 00:07:04,806
and where they hit tells us where the walls are.

118
00:07:05,926 --> 00:07:06,846
So the raycast can be used

119
00:07:06,866 --> 00:07:08,187
to determine space around the player.

120
00:07:09,607 --> 00:07:14,408
Acoustics is about spaces of air between materials.

121
00:07:15,689 --> 00:07:17,869
Estimating those spaces helps us make sonic decisions

122
00:07:17,929 --> 00:07:18,910
that match the player's view.

123
00:07:20,465 --> 00:07:23,746
So in the case where you have different types of materials,

124
00:07:24,547 --> 00:07:26,047
I've just adapted this diagram to show you

125
00:07:26,067 --> 00:07:28,148
that we have metal, we have concrete with brick,

126
00:07:28,289 --> 00:07:29,409
and we have some chain link fence.

127
00:07:30,049 --> 00:07:31,470
And here we have the same ray cast,

128
00:07:31,510 --> 00:07:34,832
but ideally we want to ignore the chain link fence

129
00:07:34,912 --> 00:07:36,493
because that won't actually have an effect

130
00:07:36,693 --> 00:07:37,813
on the way the sound is propagated.

131
00:07:37,833 --> 00:07:40,755
So the important thing is we have to contextualize

132
00:07:40,835 --> 00:07:41,575
the ray cast.

133
00:07:44,156 --> 00:07:45,257
Concrete walls block sound,

134
00:07:45,657 --> 00:07:47,178
chain link fences do not block sound.

135
00:07:48,318 --> 00:07:49,699
How do we differentiate materials?

136
00:07:50,232 --> 00:07:56,437
So, we have this concept of single hit raycasts, piercing hit raycasts, and material IDs.

137
00:07:57,878 --> 00:08:02,001
So in the example where we are doing piercing raycasts, what happens is, is that we go from

138
00:08:02,021 --> 00:08:05,824
the point of origin, so again where the black dot is, where the player is standing, we're looking

139
00:08:05,864 --> 00:08:11,849
at him from above, and then we raycast out towards the brick wall. But first we hit a chain-link

140
00:08:11,909 --> 00:08:16,372
fence. We look at the material ID and we go, we don't care. So we then have to re-raycast.

141
00:08:17,424 --> 00:08:20,907
We hit another chain link fence, we don't care, and so on, until we eventually hit something we do care.

142
00:08:21,867 --> 00:08:25,130
So the thing about heavy use of raycasting is it's very CPU expensive.

143
00:08:25,610 --> 00:08:27,091
So it's vital that it's done efficiently.

144
00:08:27,251 --> 00:08:30,394
As you can see from this diagram, we've done a bunch of raycasts,

145
00:08:30,434 --> 00:08:32,856
when in actual fact, ideally, we would have wanted to do just one.

146
00:08:32,876 --> 00:08:37,759
We have to be very conservative about how much we use them, because we're working towards CPU budgets.

147
00:08:38,520 --> 00:08:41,802
Piercing raycasts and material filtering adds a huge performance hit.

148
00:08:42,383 --> 00:08:43,303
This is a problem.

149
00:08:44,880 --> 00:08:49,181
So I just want to show you this is a screenshot I just took from inside Washington DC.

150
00:08:49,521 --> 00:08:53,862
The left hand side is the normal rendered view with some gizmos from the editor,

151
00:08:54,702 --> 00:08:58,003
and the right hand side is actually showing the triangles that are actually drawn in the renderer

152
00:08:58,303 --> 00:09:02,203
marked up according to material ID. So those different colors mark up whether it's concrete,

153
00:09:02,243 --> 00:09:07,525
glass, steel, that kind of thing. The triangles are marked up with a very specific bit mask.

154
00:09:08,805 --> 00:09:14,888
So it's material-based, bit masks used by Raycast to ignore certain mid-triangles.

155
00:09:15,628 --> 00:09:19,170
So as the Raycast goes through the world, we can basically check the triangle and go,

156
00:09:19,210 --> 00:09:23,891
do I care about this triangle or not? What we did instead was, was that we ended up

157
00:09:24,212 --> 00:09:27,893
essentially marking up the world according to what we collide with, being whether it's

158
00:09:28,713 --> 00:09:32,915
actually acoustically opaque or not. So here we have the same diagram again, only now,

159
00:09:33,535 --> 00:09:36,356
I'm now demonstrating what actually blocks sound and what doesn't.

160
00:09:37,942 --> 00:09:40,943
Snowdrop's audio raycasts only see acoustically opaque materials.

161
00:09:40,963 --> 00:09:42,964
This helps reduce complexity.

162
00:09:43,765 --> 00:09:45,505
Most of the time, single hit raycasting

163
00:09:46,086 --> 00:09:48,106
replaces the piercing raycast that you saw before,

164
00:09:48,667 --> 00:09:50,267
and there's no need to use material IDs.

165
00:09:50,648 --> 00:09:52,328
There's no material ID filtering anymore.

166
00:09:53,049 --> 00:09:54,649
So by reducing that complexity,

167
00:09:54,669 --> 00:09:57,270
it enables us to do more raycasts than we could do before.

168
00:09:58,611 --> 00:10:00,552
It's available as a filter in the editor for debugging,

169
00:10:00,572 --> 00:10:01,792
which is how I made the screenshot.

170
00:10:04,928 --> 00:10:09,309
So one of the first systems we designed around raycasts

171
00:10:09,490 --> 00:10:10,350
was Bubble Space.

172
00:10:11,150 --> 00:10:15,811
And the idea is to basically determine the average width

173
00:10:16,491 --> 00:10:17,351
around the player.

174
00:10:18,192 --> 00:10:20,452
It's a real-time geometry-based raycast system

175
00:10:20,492 --> 00:10:22,953
that determines not only the width,

176
00:10:23,033 --> 00:10:26,394
but also if there is anything obstructing above the player.

177
00:10:26,974 --> 00:10:27,894
It's used to blend.

178
00:10:29,678 --> 00:10:35,904
It's basically to ensure that the audio experience of the player is varied based on the surroundings of the player.

179
00:10:36,464 --> 00:10:42,450
And it's used to blend between different type of ambience pads, reverbs, random effects and even switching weapon tails.

180
00:10:44,512 --> 00:10:51,719
So we have a video to show how the system works. Keep in mind that this was really early in development.

181
00:10:58,219 --> 00:10:58,679
of death

182
00:10:58,719 --> 00:10:58,859
of

183
00:10:58,919 --> 00:11:00,240
death

184
00:11:00,400 --> 00:11:00,520
of

185
00:11:00,540 --> 00:11:00,801
death

186
00:11:00,921 --> 00:11:01,101
of

187
00:11:02,062 --> 00:11:02,262
death

188
00:11:02,402 --> 00:11:02,522
of

189
00:11:02,542 --> 00:11:02,782
death

190
00:11:02,902 --> 00:11:02,942
of

191
00:12:02,032 --> 00:12:03,912
So let's get into the core module.

192
00:12:05,412 --> 00:12:07,273
So we have these high level requirements.

193
00:12:07,853 --> 00:12:09,153
We need to give the sound designers

194
00:12:10,273 --> 00:12:12,534
a metric for the free air around the player's horizon,

195
00:12:13,214 --> 00:12:14,775
a metric for the free air above the player.

196
00:12:15,175 --> 00:12:16,575
Bearing in mind in Division 2,

197
00:12:16,635 --> 00:12:18,295
the player can't leave the ground

198
00:12:18,375 --> 00:12:19,856
unless they go through an animation.

199
00:12:21,356 --> 00:12:22,656
The metrics must be consistent.

200
00:12:22,676 --> 00:12:25,257
This is very important because the sound designers

201
00:12:25,277 --> 00:12:27,818
need to hook into those metrics how they mix sound

202
00:12:27,918 --> 00:12:28,838
and they need to be reliable.

203
00:12:29,955 --> 00:12:34,521
Metrics must not be unstable or appear glitchy as the player moves around the world.

204
00:12:34,661 --> 00:12:36,103
We don't want any weird edge cases.

205
00:12:38,826 --> 00:12:45,353
The only active when the metric is invalidated by the player's new position.

206
00:12:45,614 --> 00:12:47,055
So we measure and then we stop.

207
00:12:47,155 --> 00:12:49,098
Until the player moves again, we don't remeasure.

208
00:12:49,278 --> 00:12:50,920
That way we can save a bit of CPU time.

209
00:12:52,852 --> 00:12:56,534
So here is another plan view, and I just want to explain four phases.

210
00:12:57,135 --> 00:13:04,540
So one, two, three, four. The black dot is the player's position, the green dots are a raycast

211
00:13:04,560 --> 00:13:07,723
that's ended and hit nothing, and a red dot is where a raycast has hit something,

212
00:13:08,043 --> 00:13:11,365
and the black outline is the wall. So what we're having here is that

213
00:13:12,297 --> 00:13:16,798
At each of these phases, the raycasts are going out in three evenly spaced directions,

214
00:13:16,918 --> 00:13:20,939
and then they rotate a little bit. So over the course of four phases, they've covered

215
00:13:21,479 --> 00:13:27,500
a full 360. So we've actually got not just three points, but we actually have 12. The raycasts

216
00:13:29,061 --> 00:13:31,782
horizontally around the player, it occurs once per frame.

217
00:13:33,082 --> 00:13:36,663
Three raycasts outwards, evenly spaced around the player position.

218
00:13:38,000 --> 00:13:40,922
the distance of the two shortest raycasts are averaged.

219
00:13:41,362 --> 00:13:42,423
So if we have hits, that is.

220
00:13:44,205 --> 00:13:45,526
New average is then added

221
00:13:45,566 --> 00:13:47,227
to a four element short history buffer.

222
00:13:47,367 --> 00:13:48,608
So the idea is we're recording

223
00:13:48,628 --> 00:13:49,849
the last four measurements we took,

224
00:13:49,949 --> 00:13:52,451
and then we're constantly working out what that average is.

225
00:13:54,033 --> 00:13:55,414
Now then, the result of that is,

226
00:13:55,514 --> 00:13:57,195
is that this diagram is showing

227
00:13:57,255 --> 00:13:58,897
how the player can move around the world.

228
00:13:59,057 --> 00:14:00,398
So one, two, three, four,

229
00:14:00,418 --> 00:14:01,539
this is the player moving around,

230
00:14:01,659 --> 00:14:03,680
and you can see the resulting horizontal space

231
00:14:04,561 --> 00:14:05,822
growing and contracting,

232
00:14:06,162 --> 00:14:07,383
depending on the space around it.

233
00:14:09,677 --> 00:14:13,799
Target value updates from an average of three smallest values in the short history buffer.

234
00:14:14,940 --> 00:14:20,362
The raycast directions rotate 1 12th a turn as I demonstrated in the diagram beforehand.

235
00:14:21,483 --> 00:14:26,025
Raycasting begins again and then the final metric smoothed out over 16 frames,

236
00:14:26,545 --> 00:14:30,287
approximately half a second, until it reaches the new target value.

237
00:14:31,477 --> 00:14:36,038
The moving average updates the real-time parameter control in Wwise if the delta is significant.

238
00:14:36,278 --> 00:14:42,241
If it's a very small value, less than 5%, we don't bother changing it, and we also spare some CPU on that.

239
00:14:44,141 --> 00:14:48,503
Final metric keeps updating while the player is moving, or until 16 frames have passed.

240
00:14:48,663 --> 00:14:53,085
The reason why we do this is that if the player stops moving, we still have a history buffer to work through.

241
00:14:53,205 --> 00:14:57,987
So essentially, once the player stops moving, we keep processing for 16 frames, and then it stops.

242
00:15:00,420 --> 00:15:02,581
For the zenith plane for above us,

243
00:15:03,141 --> 00:15:04,121
we do something quite different.

244
00:15:04,721 --> 00:15:06,602
So here we raycast vertically above the player.

245
00:15:06,882 --> 00:15:08,242
As you can see, there are three raycasts.

246
00:15:09,102 --> 00:15:12,243
Because once per frame, three raycasts upwards

247
00:15:12,343 --> 00:15:13,703
evenly spaced around the player position.

248
00:15:14,143 --> 00:15:16,124
All three height measurements have to approximately agree

249
00:15:16,164 --> 00:15:16,764
with their average.

250
00:15:17,424 --> 00:15:18,404
If you see in this diagram,

251
00:15:18,544 --> 00:15:20,325
I've placed a cube above the player,

252
00:15:20,465 --> 00:15:22,305
which means that one of the raycasts hit it

253
00:15:22,325 --> 00:15:23,205
and the other two did not.

254
00:15:24,285 --> 00:15:26,946
In this instance, the red dot represents the raycast

255
00:15:26,966 --> 00:15:27,886
that we don't agree with

256
00:15:28,006 --> 00:15:29,287
because it doesn't match the average.

257
00:15:30,214 --> 00:15:34,779
So therefore, we would only be looking at the two green ones to actually use as our height metric.

258
00:15:35,560 --> 00:15:37,502
When they do agree, the target value is updated.

259
00:15:38,383 --> 00:15:42,087
The final metric smoothed out over four frames until it reaches the new target value.

260
00:15:44,903 --> 00:15:49,684
Yes, and here are some examples of how we've been using the value that we get from the system.

261
00:15:50,644 --> 00:15:55,525
At the bottom there you see our switch container which contains three random containers with our

262
00:15:55,565 --> 00:16:02,027
weapon tails for a weapon. And we have large street, narrow street and open field. And as

263
00:16:02,067 --> 00:16:06,608
you can see that switch container is actually controlled by a game parameter which is called

264
00:16:06,668 --> 00:16:07,668
GP bubble width.

265
00:16:08,248 --> 00:16:17,752
So when the value is 0 to 0.25, I think, roughly, we use the narrow street tails.

266
00:16:18,313 --> 00:16:23,395
If the value is bigger than 0.25, then we use the large street tails.

267
00:16:23,575 --> 00:16:28,277
And if the value is larger than 0.8, we switch our weapon tails to the open field tails.

268
00:16:28,797 --> 00:16:35,040
And this gives a nice effect where the weapons sound like they adapt to the environment in which they're being shot.

269
00:16:35,961 --> 00:16:45,129
Another example of us using the value is modifying the random effects in the ambience to be louder and triggered more often when in a more narrow space.

270
00:16:47,911 --> 00:16:52,154
Finally also here's an example of us blending between different exterior reverbs.

271
00:16:53,295 --> 00:17:02,503
So depending again on the width value we fade in and out different convolution reverbs, everything from alleys to streets to crossroads etc.

272
00:17:04,317 --> 00:17:06,417
So the next system we're gonna talk about is Slapback.

273
00:17:06,998 --> 00:17:10,719
And this is a player-centric outdoors system.

274
00:17:11,759 --> 00:17:14,020
And the goal here was to really add more variety

275
00:17:14,060 --> 00:17:15,221
to our weapon tails.

276
00:17:16,201 --> 00:17:19,882
It creates emitters on building walls around the player,

277
00:17:20,302 --> 00:17:21,843
and it replicates the player's short pattern

278
00:17:21,883 --> 00:17:24,304
with a few distance-based effects and full surround.

279
00:17:26,607 --> 00:17:32,939
So this gives the player more spatial awareness, it gives the guns kind of a nice feel, again

280
00:17:33,380 --> 00:17:35,824
helping with spatial awareness.

281
00:18:23,095 --> 00:18:29,958
Okay, so what's actually going on? The following diagram is a plan view again. So the black dot is

282
00:18:29,978 --> 00:18:34,379
the player position and we're ray casting in six directions horizontally around the player.

283
00:18:35,239 --> 00:18:39,381
The red dots represent when we hit something and those blue lines are the surface normal we get

284
00:18:39,421 --> 00:18:44,403
back from that that raycast hit. The green dots when we hit nothing and then blue dot is where

285
00:18:44,423 --> 00:18:49,704
we think there is a reflection point. So the rule that we use is similar to optics where we essentially

286
00:18:49,784 --> 00:18:52,345
use the surface normal to infer where the reflection point is.

287
00:18:52,998 --> 00:18:59,580
should be. So what we do is we use a 3D emitter pool. We compute once per frame if the player's

288
00:18:59,640 --> 00:19:02,922
movement exceeds a certain threshold. That's important so we don't keep processing.

289
00:19:04,162 --> 00:19:10,504
Raycasts around the azimuth and upwards of the player position. We use simple 3D geometry to

290
00:19:10,544 --> 00:19:14,306
infer new candidate reflection points from raycast positions and normals.

291
00:19:15,098 --> 00:19:17,560
So what we're actually doing is we're creating candidates.

292
00:19:18,320 --> 00:19:21,182
We first of all ray cast out those six candidate positions

293
00:19:21,343 --> 00:19:24,485
and we may get some reflection points, some blue dots.

294
00:19:24,865 --> 00:19:28,108
We then compare the ray cast position again.

295
00:19:28,408 --> 00:19:30,570
So we ray cast out from the black dot to the blue dot

296
00:19:30,650 --> 00:19:32,371
and see if we actually get the same result.

297
00:19:32,971 --> 00:19:34,673
If we do, then that candidate's good.

298
00:19:35,153 --> 00:19:37,114
If we don't, that means it's just anomalous.

299
00:19:37,134 --> 00:19:38,856
So we basically remove it from the candidate list.

300
00:19:39,716 --> 00:19:40,857
Any duplicates also,

301
00:19:40,877 --> 00:19:42,198
because you might end up with blue dots

302
00:19:42,879 --> 00:19:44,780
overlapping each other, they also get removed.

303
00:19:46,277 --> 00:19:48,598
And this is what we call basically a series of sanity checks.

304
00:19:49,158 --> 00:19:51,839
There are existing reflection points that we had from the last time we processed.

305
00:19:53,879 --> 00:19:55,300
If they are updated, they get culled.

306
00:19:55,800 --> 00:20:00,502
And the idea is that they're streaming something, so they fade out with a 200 millisecond fade out.

307
00:20:01,362 --> 00:20:03,743
And then any candlelight reflections left over become a new point,

308
00:20:03,883 --> 00:20:05,784
and they start playing again, so they start streaming.

309
00:20:08,645 --> 00:20:12,346
It computes a series of sanity checks, as I said, to remove erroneous reflection points.

310
00:20:13,421 --> 00:20:17,922
duplicates or near duplicates based on threshold. So sometimes they don't line up exactly but they

311
00:20:17,962 --> 00:20:24,724
might be close enough to cause a flamming effect so remove them anyway. So what is actually causing

312
00:20:24,744 --> 00:20:31,145
the streaming to occur? So we have here the idea of a double buffer. So the broadcaster module is

313
00:20:31,185 --> 00:20:37,047
this place where sound is recorded and played back. It supports multiple broadcast channels up to eight.

314
00:20:37,922 --> 00:20:42,564
each broadcast channel consists of up to eight audio channels. So when I talk about a broadcast

315
00:20:42,584 --> 00:20:49,207
channel I just mean a broad like a radio channel essentially or some kind of not sound. So what's

316
00:20:49,227 --> 00:20:54,749
happening is is that we've registered so every time Wwise finishes its render it calls back to

317
00:20:54,789 --> 00:20:59,251
the system and it says please swap these two pointers we have one pointing at the record

318
00:20:59,271 --> 00:21:03,133
buffer and one at the playback buffer and they're constantly swapping frame by frame.

319
00:21:03,573 --> 00:21:06,935
That's what I mean by double buffer system. Some of you will be familiar with this some of you will

320
00:21:06,975 --> 00:21:07,195
not be.

321
00:21:08,676 --> 00:21:14,740
In a sense, it's like a digital tape loop. So we have a record head and a playback head. So whilst

322
00:21:14,940 --> 00:21:21,644
one pointer is recording in, another one is playing out. The loop gets moved on from a callback,

323
00:21:21,684 --> 00:21:26,127
we register it in the rendering chain as I already said. This actually adds a little bit of latency.

324
00:21:26,347 --> 00:21:32,751
So if our buffer size is 1024 samples, that means the latency we add by creating this recording loop

325
00:21:33,111 --> 00:21:37,373
is 1024 samples. But we're not so bothered by that because we're going to add a bigger delay

326
00:21:37,413 --> 00:21:38,154
in a moment anyway.

327
00:21:38,811 --> 00:21:40,673
So we just take that on the chin.

328
00:21:41,995 --> 00:21:44,778
So how do we take sound into the record buffer?

329
00:21:45,179 --> 00:21:47,281
We have a sync plugin inside Wwise.

330
00:21:47,822 --> 00:21:50,605
This is a fake audio device, so it looks like an audio device,

331
00:21:51,306 --> 00:21:54,130
but in actual fact it's communicating with the broadcast module.

332
00:21:54,991 --> 00:21:56,713
What it's doing is, is essentially...

333
00:21:57,546 --> 00:22:04,050
taking when Wwise is essentially completing its render passes and all the master mixer graph

334
00:22:04,491 --> 00:22:09,634
endpoints, it then sends sound that has gone to that particular bus to the record buffer,

335
00:22:10,574 --> 00:22:13,837
based on the record channel that's been set in the plugin, of which there are eight.

336
00:22:15,278 --> 00:22:17,659
Sends audio to the broadcast record head, as I said,

337
00:22:18,119 --> 00:22:22,322
and then controls what broadcast channel it uses and how many audio channels it needs.

338
00:22:23,880 --> 00:22:25,761
It's called once at the end of each render pass.

339
00:22:25,901 --> 00:22:28,043
So every time we render out 1024 samples,

340
00:22:28,103 --> 00:22:29,083
this gets called at the end.

341
00:22:30,204 --> 00:22:32,165
Now, the receiver plugin is reading out

342
00:22:32,625 --> 00:22:35,927
from the broadcaster system.

343
00:22:37,848 --> 00:22:39,889
It gets audio from the broadcaster's playback head.

344
00:22:40,732 --> 00:22:43,875
It's triggered by the core module play event.

345
00:22:44,135 --> 00:22:48,659
So essentially, when you have this actor mixer graph,

346
00:22:48,879 --> 00:22:51,601
every time one of those nodes that has this plugin on it

347
00:22:51,661 --> 00:22:54,183
is called to basically play out, it starts rendering a bus.

348
00:22:55,784 --> 00:22:57,926
It's faded out by the core module stop event.

349
00:22:58,206 --> 00:23:00,148
What I said, that was that 200 millisecond fade out.

350
00:23:00,288 --> 00:23:01,089
That's what causes it.

351
00:23:02,049 --> 00:23:05,272
So the control is what broadcast channel it reads from.

352
00:23:06,018 --> 00:23:08,860
So the idea is that you can record out to one channel,

353
00:23:09,000 --> 00:23:11,201
but you can have multiple things reading from that.

354
00:23:11,241 --> 00:23:13,682
So we could actually have multiple receiver plugins

355
00:23:13,962 --> 00:23:16,404
reading out from the same playback head.

356
00:23:18,084 --> 00:23:20,125
It gets its audio channel layout from the broadcaster

357
00:23:20,225 --> 00:23:21,206
by way of the sync plugin.

358
00:23:21,346 --> 00:23:23,747
So whatever the channel layout was,

359
00:23:23,787 --> 00:23:26,348
if we said it was 7.1, that's what we receive.

360
00:23:28,309 --> 00:23:30,030
It's called once at the beginning of each render pass.

361
00:23:30,977 --> 00:23:35,841
And as you see from the diagram, what I mean is that when the active mixer graph causes

362
00:23:35,881 --> 00:23:37,663
nodes to render, that's when this is called.

363
00:23:39,365 --> 00:23:42,187
The actual effect is then applied to the receiving end.

364
00:23:42,288 --> 00:23:46,752
So after we get sound back from the broadcaster, we then apply a delay effect.

365
00:23:47,292 --> 00:23:49,494
So we had to make a custom Wwise effect plugin.

366
00:23:49,955 --> 00:23:54,179
The reason why we did this is because we need to solve the problem of flexible delay time.

367
00:23:55,668 --> 00:24:00,411
So just to explain that, we have a circular buffer, so what happens is as each sample

368
00:24:00,491 --> 00:24:06,695
time progresses, we have two pointers chasing each other, and the gap between them is the

369
00:24:06,735 --> 00:24:07,616
delay we actually get.

370
00:24:07,656 --> 00:24:11,598
In this case it's about two seconds of delay, because that's what we need to actually have

371
00:24:11,638 --> 00:24:14,280
a realistic delay effect in a world.

372
00:24:16,089 --> 00:24:21,797
The input sample buffer is constantly being updated, and then as the output samples are pulled out,

373
00:24:22,138 --> 00:24:25,082
we actually do a power complementary interpolation.

374
00:24:25,543 --> 00:24:30,289
Basically, because we are moving so quickly, we have to be able to essentially move out any artifacts.

375
00:24:31,109 --> 00:24:32,429
and this is what deals with that problem.

376
00:24:32,709 --> 00:24:34,571
We also have phase control because reflections

377
00:24:35,351 --> 00:24:37,192
usually come back the opposite phase that they went in.

378
00:24:37,352 --> 00:24:38,793
This is actually an oversimplification,

379
00:24:38,913 --> 00:24:40,914
but it's a good optimization.

380
00:24:41,295 --> 00:24:42,936
And we also have some filter EQ in the plugin

381
00:24:42,956 --> 00:24:44,016
so that we can actually modify

382
00:24:44,337 --> 00:24:45,477
the sound that gets reflected back.

383
00:24:45,857 --> 00:24:47,218
And then that becomes the output sample,

384
00:24:47,258 --> 00:24:48,319
which goes to the output buffer.

385
00:24:48,379 --> 00:24:50,760
And that's what gets sent on from the source plugin

386
00:24:51,181 --> 00:24:52,662
into the Active Mixer node.

387
00:24:54,863 --> 00:24:56,944
As I said, it has a built-in EQ and phase inversion.

388
00:24:57,204 --> 00:24:58,985
There's one instance per slapback emitter.

389
00:24:59,025 --> 00:25:00,646
So as we play through a slapback emitter.

390
00:25:01,609 --> 00:25:07,894
this effect is at the end of its DSP chain before it gets sent back into the mix engine of Wwise.

391
00:25:09,375 --> 00:25:16,060
Wait one second, okay. So just to demonstrate here, the top left one is the, that's the sync plugin.

392
00:25:16,200 --> 00:25:20,863
So when I said before, you set the draw cross channel and the number of audio channels we

393
00:25:20,923 --> 00:25:22,805
actually need to describe and that basically gives us.

394
00:25:23,860 --> 00:25:24,601
what gets recorded.

395
00:25:24,621 --> 00:25:27,282
The bottom left is the source plugins.

396
00:25:27,302 --> 00:25:29,183
That's what's coming back from the broadcaster.

397
00:25:29,263 --> 00:25:30,483
And then all we do there is select

398
00:25:30,503 --> 00:25:31,484
what channel we want to read in.

399
00:25:32,504 --> 00:25:34,805
And then the actual emitters of course are 3D.

400
00:25:35,645 --> 00:25:37,966
So therefore we position them accordingly.

401
00:25:37,986 --> 00:25:44,029
And then what we do is then we have the delay time

402
00:25:44,049 --> 00:25:46,730
is predicated by those Raycasts we were making.

403
00:25:46,870 --> 00:25:48,010
The distance from the player

404
00:25:48,050 --> 00:25:49,351
to the Raycast reflection point

405
00:25:49,451 --> 00:25:51,332
is what we compute for the delay time.

406
00:25:52,500 --> 00:26:00,189
And as you can see here, the actual conversion scale that we use isn't actually a realistic one.

407
00:26:00,369 --> 00:26:05,695
We wanted to have a kind of like a movie type dramatic effect, so that's why it's curved like that.

408
00:26:09,011 --> 00:26:15,393
And here's an example of us using the slapback bus here on a weapon shot sound.

409
00:26:17,254 --> 00:26:21,216
At first we tried it on the whole weapon sound and quickly realized that we maybe don't want

410
00:26:21,256 --> 00:26:27,558
to have the weapon mechanics being reflected off of walls so that's why it's applied to the shot only.

411
00:26:28,999 --> 00:26:32,083
So the next thing we're going to talk about today is procedural ambience.

412
00:26:33,045 --> 00:26:38,592
And before we go further into this, I just wanted to do a quick shout out to Ubisoft Reflections audio team,

413
00:26:38,692 --> 00:26:45,902
especially Seb Thomas and Andy Muchu, who were integral for basically designing and coding the system.

414
00:26:47,324 --> 00:26:57,035
But basically the idea here was to kind of add more variety and remove the manual labor from the sound designers for covering all of our interior spaces.

415
00:26:58,016 --> 00:26:59,458
In Division 2 we had over 2.5 thousand...

416
00:27:03,482 --> 00:27:08,908
rooms, what we call interior rooms. And the problem that we wanted to solve is that we didn't really

417
00:27:08,968 --> 00:27:15,115
have enough time for applying manual sound design and reverb design for each of those spaces.

418
00:27:16,476 --> 00:27:20,640
So we thought about how can we use the already existing geometry and data

419
00:27:21,842 --> 00:27:23,203
and basically designed a system

420
00:27:24,424 --> 00:27:29,931
a procedural offline system to generate impulse responses for each room. Those impulses would

421
00:27:29,971 --> 00:27:36,480
be based on each room size and the materials of those of those rooms. And also coupled with that

422
00:27:36,580 --> 00:27:42,427
we designed a custom Wwise Convolution Reaver plugin to load those impulse responses at runtime.

423
00:27:44,194 --> 00:27:53,242
And we even took it one step further and used the IRs that were generated for each room to create an ambient sound for each of those rooms as well.

424
00:27:54,623 --> 00:28:01,269
We did this using the IRs combined with brown noise and the wet signal of the props of that room.

425
00:28:01,970 --> 00:28:04,412
But I'll go into more detail about that shortly.

426
00:28:07,575 --> 00:28:08,917
So the convolution implementation.

427
00:28:11,080 --> 00:28:12,381
is active when the player is indoors.

428
00:28:12,501 --> 00:28:14,722
We know the player is indoors because of terrain markup,

429
00:28:14,802 --> 00:28:16,483
which is also used by the graphics people.

430
00:28:18,565 --> 00:28:20,526
It's instance partition convolution engine.

431
00:28:20,746 --> 00:28:22,207
So we have multiple engines

432
00:28:22,507 --> 00:28:24,689
and then we can appropriate them as needed.

433
00:28:25,329 --> 00:28:26,950
The actual processing occurs in Snowdrop.

434
00:28:26,970 --> 00:28:28,852
So when we talk about a custom convolution plugin,

435
00:28:29,032 --> 00:28:30,993
the actual plugin part is just feeding sound

436
00:28:31,033 --> 00:28:31,774
back from Snowdrop.

437
00:28:33,195 --> 00:28:35,036
The total number of convolution engines is scalable.

438
00:28:35,979 --> 00:28:37,720
as it happens that we used four this time,

439
00:28:37,760 --> 00:28:40,702
but that's according to how much CPU bandwidth

440
00:28:40,803 --> 00:28:41,583
we had to play with.

441
00:28:42,604 --> 00:28:44,425
The Customize effect calls into Snowdrop

442
00:28:44,446 --> 00:28:45,546
via a dedicated interface.

443
00:28:46,167 --> 00:28:46,928
So that's how we've done it.

444
00:28:46,968 --> 00:28:48,629
We have basically a callback interface

445
00:28:48,669 --> 00:28:51,111
to pull out the audio that we processed inside Snowdrop.

446
00:28:52,612 --> 00:28:52,933
Okay.

447
00:28:53,953 --> 00:28:55,995
And this is a debug view of the kind of thing

448
00:28:56,015 --> 00:28:58,897
when you start ray tracing inside a large space like that,

449
00:28:59,158 --> 00:29:01,179
all of those white markers are reflection points

450
00:29:01,219 --> 00:29:02,200
generated by the raycast.

451
00:29:04,660 --> 00:29:08,421
What we do is that we generate an impulse response that is static for the room space.

452
00:29:08,441 --> 00:29:10,382
So this would be one single impulse response.

453
00:29:11,143 --> 00:29:15,485
But because we purposely filter out the early reflections first, all you hear is the late

454
00:29:15,505 --> 00:29:18,606
reflections and in actual fact, they don't really change that much.

455
00:29:18,646 --> 00:29:21,547
So it doesn't really matter that the player moves around that space without it really

456
00:29:21,587 --> 00:29:21,947
changing.

457
00:29:23,008 --> 00:29:23,428
Let's hear it.

458
00:29:32,281 --> 00:29:33,402
He's trying to get behind you!

459
00:29:33,442 --> 00:29:35,064
Cover that target!

460
00:29:37,966 --> 00:29:38,587
Get back!

461
00:29:39,688 --> 00:29:40,248
Fuck off!

462
00:29:46,254 --> 00:29:47,595
Cover won't save you!

463
00:29:49,777 --> 00:29:50,878
Go! I got your back!

464
00:29:54,180 --> 00:29:57,583
Important thing to mention is that we didn't use this system everywhere.

465
00:29:58,243 --> 00:30:03,788
Of course, where we wanted to use ambience to kind of help the narrative of the game

466
00:30:03,968 --> 00:30:09,853
or if the space was a very particular space, we of course designed manual ambiences and

467
00:30:09,873 --> 00:30:14,517
reverbs for those spaces and simply did an override for whatever was generated there

468
00:30:14,577 --> 00:30:15,537
by the procedural system.

469
00:30:16,238 --> 00:30:21,827
But we were really happy with the way the reverbs were sounding and so we wanted to basically find

470
00:30:21,907 --> 00:30:28,778
out if there was any way of using those reverbs to generate an ambience pad for those rooms as well.

471
00:30:31,354 --> 00:30:35,595
Since our game basically takes place in a scenario where most spaces are abandoned,

472
00:30:36,075 --> 00:30:41,456
we wanted to avoid making create the sound of an empty office tasks for our sound designers.

473
00:30:42,216 --> 00:30:47,698
So we're thinking about what actually defines a room tone and we're experimenting in Wwise with

474
00:30:48,078 --> 00:30:53,559
noise and reverbs and here's a little example of how we reached the conclusion that

475
00:30:54,360 --> 00:30:57,546
Reverb is actually a very integral part of a room tone.

476
00:30:58,007 --> 00:31:01,894
Here's just using noise in Wwise, a short example of this.

477
00:31:30,363 --> 00:31:30,424
you

478
00:32:27,244 --> 00:32:33,410
So, whilst researching this topic of acoustics and rooms and room tones, another interesting

479
00:32:33,450 --> 00:32:35,712
concept we found is critical distance.

480
00:32:36,153 --> 00:32:40,537
And this is basically where the level of the direct sound equals the level of the reflected

481
00:32:40,577 --> 00:32:40,838
sound.

482
00:32:41,758 --> 00:32:46,543
So in the graph here you can see the black sound pressure line flattening out as it passes

483
00:32:46,743 --> 00:32:47,744
the critical distance point.

484
00:32:48,565 --> 00:32:53,928
This knowledge gave us more confidence to use generated IRs from the rooms to create the actual ambient sounds.

485
00:32:54,468 --> 00:33:02,892
So in an offline process, there were two virtual microphones placed in each room, 22 centimeters apart, which is roughly the average head size.

486
00:33:04,233 --> 00:33:11,837
And they recorded the wet sound of the props, blended it with brown noise and produced perfect 15-second loops for each of those spaces.

487
00:33:12,938 --> 00:33:16,961
hear how the results actually sounded in the game.

488
00:33:17,201 --> 00:33:19,243
And obviously they're not meant to be played this loud.

489
00:33:43,209 --> 00:33:46,452
While a juror is in theroom, the jury-in-waiting works a special effect on the statute ofende of the juror.

490
00:33:46,492 --> 00:33:50,636
The jury-in-waiting updates that the jury has given the evidence of this specific juridical aspect of the legal procedure.

491
00:33:50,656 --> 00:33:52,377
The jury is exactly what the juror did.

492
00:33:52,397 --> 00:33:53,258
The jury will remember the juror.

493
00:33:53,278 --> 00:33:56,601
The jury will remind us of the jurors' predictions, but also remind us of the jurors' mistakes.

494
00:34:26,057 --> 00:34:28,842
So now we're going to talk a bit about immersive weather sounds.

495
00:34:29,122 --> 00:34:34,552
We knew that weather was going to play a huge part in our game and so therefore I wanted us

496
00:34:34,632 --> 00:34:38,960
to see if we could create a system that gives the weather sounds more depth.

497
00:34:40,481 --> 00:34:46,823
So we created an emitter manager to control all of the emitters on all of the props in our game,

498
00:34:47,243 --> 00:34:55,906
and this allowed us to basically play back the sounds of rain or wind playing on the actual props.

499
00:34:57,706 --> 00:35:01,387
It's of course material-based, so depending on what material the prop is,

500
00:35:01,767 --> 00:35:04,668
that would be taken into account when playing back the sound.

501
00:35:05,909 --> 00:35:12,335
And it's also a cool thing because it's props that otherwise don't make any sound like tables or road signs

502
00:35:12,775 --> 00:35:17,799
But we can actually play sound on them now if it rains or if there's a heavy wind

503
00:35:17,819 --> 00:35:33,012
Nice to see you agent

504
00:35:42,391 --> 00:35:42,802
Hello there.

505
00:36:22,365 --> 00:36:28,532
So, it was actually when I first heard what we did in the prototype for the first weather

506
00:36:29,733 --> 00:36:33,577
system experiment that we did, when I heard sounds of rain above me, that I was thinking

507
00:36:33,597 --> 00:36:36,980
that Dolby Atmos would be a really nice fit for our game.

508
00:36:37,421 --> 00:36:42,546
And as we kept developing the game, the systems that we designed, whether that's bubble space,

509
00:36:42,826 --> 00:36:44,067
slap back, or...

510
00:36:44,708 --> 00:36:49,250
as I mentioned the weather sounds or even like the in-game things like helicopters and drones it just

511
00:36:49,290 --> 00:36:55,353
made sense to kind of try out this technology and the first time I tried it I felt not only that it

512
00:36:55,413 --> 00:37:00,435
was enhancing the mix in the vertical plane but also in the horizontal one where I could feel

513
00:37:02,056 --> 00:37:05,238
better perception of enemies behind me and things in front of me,

514
00:37:05,258 --> 00:37:08,761
it just felt like we gave the mix that much more clarity.

515
00:37:09,301 --> 00:37:15,145
So it was a pretty obvious choice, and Dolby were also very helpful with integrating this into our game.

516
00:37:15,685 --> 00:37:18,928
And of course, not everyone has Dolby Atmos Theater at home,

517
00:37:19,188 --> 00:37:23,110
but most of us have headphones, which Dolby supports with Atmos for headphones.

518
00:37:23,310 --> 00:37:25,792
So, yeah, I thought it was a really good fit.

519
00:37:26,313 --> 00:37:30,115
And this brings us to the quick summary of the talk today.

520
00:37:30,876 --> 00:37:35,039
So we talked about us wanting to create a dynamic audio experience for our players,

521
00:37:35,899 --> 00:37:41,143
and we utilized already existing geometry and raycasts to analyze the surroundings.

522
00:37:42,244 --> 00:37:46,927
We then used that output of the raycast to modulate and change the soundscape.

523
00:37:46,968 --> 00:37:52,532
The procedural system helped us with greater variety, more quality, and less manual labor,

524
00:37:53,252 --> 00:37:56,194
and Atmos was a great match for the systems that we designed,

525
00:37:56,214 --> 00:37:59,217
and it opened up for more space in the mix.

526
00:38:00,398 --> 00:38:01,399
Thank you very much for listening.

527
00:38:02,039 --> 00:38:02,380
Thank you.

528
00:38:04,842 --> 00:38:13,511
Oh, by the way, if you want to find out more about the sound of The Division 2, you can search for it on YouTube.

