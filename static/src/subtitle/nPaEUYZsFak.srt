1
00:00:05,468 --> 00:00:10,930
Hi everyone, we are very glad to have you at our speech today.

2
00:00:10,930 --> 00:00:17,432
Our topic is real-time data processing for multiplayer online games.

3
00:00:17,432 --> 00:00:25,035
I'm Lei Xia, a senior data mining engineer from Thunderfire UX team of NetEase Games.

4
00:00:25,035 --> 00:00:33,218
This is my colleague Zhechen Xu, a senior project manager also from Thunderfire.

5
00:00:35,172 --> 00:00:40,853
Before entering today's topic, let's think about such a scenario.

6
00:00:40,853 --> 00:00:43,994
A new game is launched today.

7
00:00:43,994 --> 00:00:50,536
Your boss is eager to know today's in-time new players and the cash flow.

8
00:00:50,536 --> 00:00:53,557
Can you give him this data in real time?

9
00:00:53,557 --> 00:00:58,018
Or a batch of new items have been launched.

10
00:00:59,283 --> 00:01:05,927
Can you let the boss knows which are the 5 best sellers now?

11
00:01:05,927 --> 00:01:12,211
Finally, these new items were copied illegally by some players.

12
00:01:12,211 --> 00:01:19,196
Can you detect it in the first time before the boss goes crazy and yells at you?

13
00:01:19,196 --> 00:01:22,438
Why didn't we find them in time?

14
00:01:24,738 --> 00:01:32,084
To solve these problems, we need a real-time data processing system.

15
00:01:32,084 --> 00:01:39,050
Therefore, we will mainly discuss three questions in today's presentation.

16
00:01:39,050 --> 00:01:46,897
First, how to build a real-time data processing system for online games?

17
00:01:46,897 --> 00:01:52,882
Second, how do we process real-time data in NetEase games?

18
00:01:53,772 --> 00:01:58,976
And third, how to optimize real-time data processing.

19
00:01:58,976 --> 00:02:03,059
Most of you probably have had little knowledge of big data.

20
00:02:03,059 --> 00:02:08,364
So I'd like to introduce some big data tools

21
00:02:08,364 --> 00:02:12,167
that are frequently used in practice

22
00:02:12,167 --> 00:02:15,569
before digging into today's topic.

23
00:02:15,569 --> 00:02:16,070
Firstly,

24
00:02:16,658 --> 00:02:27,986
Kafka. Kafka is a distributed streaming platform which can be used for log transferring and temporary

25
00:02:27,986 --> 00:02:38,674
storage. Storm and Flink are systems for real-time data computations. Android is a real-time analytics

26
00:02:38,674 --> 00:02:45,520
database. The specific functions of these tools will be explained later.

27
00:02:46,658 --> 00:02:55,585
In addition, there is a commonly used ELK system in order to search and query the logs.

28
00:02:55,585 --> 00:03:04,072
And ELK is actually the abbreviation of the three words Elasticsearch, Logstash, and Kibana.

29
00:03:04,072 --> 00:03:12,118
They are tools of log search, log collection, and visualization.

30
00:03:13,328 --> 00:03:19,130
This picture presented here shows how ELK works.

31
00:03:19,130 --> 00:03:23,732
Logstash collects, passes, and filters the data,

32
00:03:23,732 --> 00:03:28,973
and then sends them to Elasticsearch for indexing.

33
00:03:28,973 --> 00:03:32,814
Elasticsearch is used to index, store,

34
00:03:32,814 --> 00:03:33,554
and extract your data.

35
00:03:34,456 --> 00:03:41,918
Kibana allows you to visualize your data stored in Elasticsearch,

36
00:03:41,918 --> 00:03:47,079
helping with data searching and data analysis.

37
00:03:47,079 --> 00:03:52,460
By far, we have had an overview of the big data tools.

38
00:03:52,460 --> 00:03:56,561
So now, let's begin our first topic.

39
00:03:56,561 --> 00:04:02,422
How to build a real-time data processing system for online games?

40
00:04:03,838 --> 00:04:12,400
Corresponding to the real-time data processing system is the offline data processing system.

41
00:04:12,400 --> 00:04:19,322
So you may ask, what is the difference between these two systems?

42
00:04:19,322 --> 00:04:24,363
A main difference is timeliness.

43
00:04:24,363 --> 00:04:32,785
Offline data processing system is not immediately and usually causes delaying for hours or a

44
00:04:32,785 --> 00:04:33,045
day.

45
00:04:33,844 --> 00:04:36,826
as we often call it T plus 1.

46
00:04:36,826 --> 00:04:42,530
While real-time processing system reduces the delay

47
00:04:42,530 --> 00:04:47,853
to minute level, second level, or even millisecond level.

48
00:04:47,853 --> 00:04:52,156
Secondly, the processing modes are different.

49
00:04:52,156 --> 00:04:55,818
Offline system uses batch calculation,

50
00:04:55,818 --> 00:05:00,901
while real-time system uses micro batch calculation

51
00:05:00,901 --> 00:05:02,602
or streaming calculation.

52
00:05:03,605 --> 00:05:12,852
In terms of resource consumption, offline data processing has a higher demand for disk and network

53
00:05:12,852 --> 00:05:21,398
I.O. due to the relatively large amount of data processed. Real-time data processing requires

54
00:05:21,398 --> 00:05:27,982
higher CPU and memory usage for processing real-time data.

55
00:05:29,042 --> 00:05:35,128
Finally, they each have different representative technology.

56
00:05:35,128 --> 00:05:44,757
For offline data processing, these are Hadoop, HDFS, and MapReduce Hive.

57
00:05:44,757 --> 00:05:53,426
While for real-time data processing, they are Spark, Streaming, Storm, Flink, and so on.

58
00:05:55,013 --> 00:06:02,676
Now, let's look at a typical real-time data processing system as shown in the slide.

59
00:06:02,676 --> 00:06:12,719
The source of it is log collection and transmission. Then transferring and storage

60
00:06:12,719 --> 00:06:21,202
stands at the middle of the process, while computation and application is the final step.

61
00:06:22,278 --> 00:06:28,903
The corresponding tools for this step are Logstash, Kafka, and Flink.

62
00:06:28,903 --> 00:06:39,131
This is a brief overview of the data processing pipeline, and we are going to have a more

63
00:06:39,131 --> 00:06:42,194
detailed analysis of it.

64
00:06:42,194 --> 00:06:46,237
Players in the game will have a variety of behaviors.

65
00:06:46,831 --> 00:06:53,872
These behaviors will be recorded by the log system on a local game server.

66
00:06:53,872 --> 00:07:00,074
For example, some typical behaviors such as logging in and log out,

67
00:07:00,074 --> 00:07:06,575
truncating a character, paying for items, and the use of items, etc.

68
00:07:06,575 --> 00:07:16,377
In addition to server-side behavior, some client-side click behavior can also be recorded.

69
00:07:17,947 --> 00:07:22,750
such as the operation of starting the app.

70
00:07:22,750 --> 00:07:26,273
In order to achieve real-time processing,

71
00:07:26,273 --> 00:07:31,396
these logs need to be transmitted in real-time.

72
00:07:31,396 --> 00:07:34,759
We can use log collection tools,

73
00:07:34,759 --> 00:07:39,081
such as Logstash or Flume,

74
00:07:39,081 --> 00:07:42,284
to transfer logs from local

75
00:07:42,284 --> 00:07:45,646
to a stream processing platform like Kafka.

76
00:07:46,747 --> 00:07:54,391
Kafka is a transfer station for the entire real-time data processing system.

77
00:07:54,391 --> 00:07:56,973
It plays a key role in the system.

78
00:07:56,973 --> 00:08:07,940
On the right side of the slide, we can learn that Kafka works as a transfer station for game data of all kinds.

79
00:08:07,940 --> 00:08:09,961
Data are written into Kafka.

80
00:08:11,033 --> 00:08:20,058
from producers, after which various applications start consuming data from Kafka.

81
00:08:20,058 --> 00:08:30,663
Through this, the whole data processing pipeline is built and the real-time data analysis is

82
00:08:30,663 --> 00:08:31,264
achieved.

83
00:08:32,197 --> 00:08:39,340
Kafka's advantage lies in its higher throughput and low latency.

84
00:08:39,340 --> 00:08:44,122
It can process millions of messages at the same time,

85
00:08:44,122 --> 00:08:48,724
and has local storage and data backup.

86
00:08:48,724 --> 00:08:52,825
So it has a certain degree of fault tolerance.

87
00:08:52,825 --> 00:08:56,247
No failure is allowed.

88
00:08:56,247 --> 00:09:01,489
As a distributed system, Kafka is very easy to scale.

89
00:09:02,694 --> 00:09:07,376
so you can add new nodes without stopping the server.

90
00:09:07,376 --> 00:09:16,621
The last step of the real-time data processing system is computing and application.

91
00:09:16,621 --> 00:09:24,285
Earlier in our presentation, we have introduced the ELK system.

92
00:09:24,899 --> 00:09:29,301
which is a typical log search application.

93
00:09:29,301 --> 00:09:33,183
However, in daily BI applications,

94
00:09:33,183 --> 00:09:38,386
we will more often deal with database systems

95
00:09:38,386 --> 00:09:43,468
and encounter the following two processing modes.

96
00:09:43,468 --> 00:09:47,010
One is to process the data through the engine

97
00:09:47,010 --> 00:09:53,333
like Flink or Storm, and then write into the database

98
00:09:53,333 --> 00:09:53,893
system.

99
00:09:54,818 --> 00:10:13,405
The other is to directly transfer the data from Kafka to real-time data analysis database like Druid for online analytical processing, so-called OLAP.

100
00:10:13,405 --> 00:10:15,186
What is Flink?

101
00:10:15,186 --> 00:10:17,066
Flink is a framework

102
00:10:17,562 --> 00:10:26,288
and distributed processing engine for stateful computations over unbounded and bounded data

103
00:10:26,288 --> 00:10:36,175
streams. Flink has been designed to run in all common cluster environments,

104
00:10:36,175 --> 00:10:41,498
perform computations at in-memory speed and at any scale.

105
00:10:42,770 --> 00:10:53,312
For the advantages of Flink, first of all, it has the flexible windows, such as tumbling and sliding window.

106
00:10:53,312 --> 00:11:00,674
This will be explained to you in detail in the following cases with my colleague.

107
00:11:00,674 --> 00:11:09,996
Secondly, there is another important feature, which is the exact ones.

108
00:11:10,688 --> 00:11:20,174
The exactly once means that all data flows over and the data will be processed only once.

109
00:11:20,174 --> 00:11:31,301
This is very important for streaming systems to avoid data loss and repeated data calculation.

110
00:11:31,301 --> 00:11:36,464
Finally, in order to deal with possible service failures

111
00:11:37,334 --> 00:11:41,117
we may need recover to specific time or data points.

112
00:11:41,117 --> 00:11:47,823
The checkpoint and save point mechanism provided by Flink

113
00:11:47,823 --> 00:11:50,925
can solve such problems very well.

114
00:11:50,925 --> 00:11:54,508
The Flink programming models.

115
00:11:55,214 --> 00:12:01,038
as presented in the slide, can be divided into three modules.

116
00:12:01,038 --> 00:12:05,781
The source module receives various kinds of data source.

117
00:12:05,781 --> 00:12:10,964
The sink module is to output all kinds of storage.

118
00:12:10,964 --> 00:12:14,906
The transformation module in the middle

119
00:12:14,906 --> 00:12:20,009
refers to various operations of data transformation,

120
00:12:20,009 --> 00:12:24,032
such as map, reduce, filter, and so on.

121
00:12:25,565 --> 00:12:34,347
Now I would introduce you my colleague Zhechen Xu, a senior project manager in Thunderfire

122
00:12:34,347 --> 00:12:45,710
for the next part. He will show you a lot of use cases in NetEase games.

123
00:12:45,710 --> 00:12:49,711
Firstly, let's start with a common using case as an example.

124
00:12:51,013 --> 00:12:54,635
how to calculate key metrics such as new accounts,

125
00:12:54,635 --> 00:12:56,076
DAU, and revenue.

126
00:12:56,076 --> 00:12:59,398
Following what we have discussed before,

127
00:12:59,398 --> 00:13:03,460
the first step is to build a pipeline.

128
00:13:03,460 --> 00:13:07,583
We use LogDash to collect log, Kafka to transfer it,

129
00:13:07,583 --> 00:13:10,344
and Flink to calculate and analyze it.

130
00:13:10,344 --> 00:13:14,166
The result of computation is written

131
00:13:14,166 --> 00:13:15,707
into MySQL database.

132
00:13:15,707 --> 00:13:18,128
Then we can get a desired result

133
00:13:18,128 --> 00:13:20,210
through querying the database.

134
00:13:21,344 --> 00:13:24,985
In order to calculate these indicators in real time,

135
00:13:24,985 --> 00:13:29,787
the first thing we have to do is to record some log behaviors.

136
00:13:29,787 --> 00:13:32,268
And to give you an intuitive feeling,

137
00:13:32,268 --> 00:13:37,310
we have listed some log examples for log in and log out

138
00:13:37,310 --> 00:13:38,190
and pay in.

139
00:13:38,190 --> 00:13:42,272
The very first step is to transfer logs to Kafka.

140
00:13:42,272 --> 00:13:45,213
Here we take logstash as an example.

141
00:13:45,213 --> 00:13:49,434
A standard logstash configurations

142
00:13:49,434 --> 00:13:50,915
include input and output.

143
00:13:52,357 --> 00:13:56,720
On the slide, the input is a local file and the output is Kafka.

144
00:13:56,720 --> 00:14:02,763
With such straightforward configuration, we can achieve our goal.

145
00:14:02,763 --> 00:14:07,106
Finally, we use Flink to consume the data in Kafka and write the

146
00:14:07,106 --> 00:14:13,669
result in three MySQL data tables, new accounts, DAU, and revenue

147
00:14:13,669 --> 00:14:18,332
respectively, based on which we calculate the data we need.

148
00:14:19,739 --> 00:14:26,080
The example hopefully has clearly presented you with the way a real-time system is built up,

149
00:14:26,080 --> 00:14:30,301
along with how it serves for our application.

150
00:14:30,301 --> 00:14:36,723
In the next part, I will introduce some more complicated technologies and cases.

151
00:14:36,723 --> 00:14:42,244
Based on Flink programming models, we can achieve real-time ETL very conveniently.

152
00:14:43,353 --> 00:14:49,175
The so-called ETL here refers to a condition that extracts the transform load

153
00:14:49,175 --> 00:14:53,956
is in the exact match with the source transform sink in the Flink.

154
00:14:53,956 --> 00:14:57,137
Through sinking the data into HDFS,

155
00:14:57,137 --> 00:15:01,157
the creating external tables in Hive,

156
00:15:01,157 --> 00:15:05,138
we can create the tables for analysis in a very short time.

157
00:15:06,871 --> 00:15:15,833
The example we gave before is a typical real-time BI application combined with Flink's tumbling time window function.

158
00:15:15,833 --> 00:15:23,914
We can easily know some key elements and monitor some in-game items and currency follows.

159
00:15:23,914 --> 00:15:34,776
For instance, if we want to monitor the total coin acquisition of each account every minute, we can use a Flink programming model to achieve our goal.

160
00:15:36,300 --> 00:15:41,462
First group the data by account, then use tumbling time window and set the time window

161
00:15:41,462 --> 00:15:45,344
at 1 minute, and finally, we calculate the sum of the coins.

162
00:15:45,344 --> 00:15:52,727
However, if the time window changes to the latest hour by now, then we should choose

163
00:15:52,727 --> 00:15:56,568
sliding time window rather than tumbling time window.

164
00:15:56,568 --> 00:16:01,410
And then we calculate the total number of each account and make a top key.

165
00:16:03,162 --> 00:16:05,463
So far, we have clearly understanding

166
00:16:05,463 --> 00:16:09,124
of how Flink's programming models works.

167
00:16:09,124 --> 00:16:11,685
However, we wonder whether we can achieve

168
00:16:11,685 --> 00:16:16,427
a real-time data analysis and query without Flink.

169
00:16:16,427 --> 00:16:19,669
Here, we will introduce another tool, DREAD,

170
00:16:19,669 --> 00:16:23,290
to give you another way to solve problems.

171
00:16:23,290 --> 00:16:28,433
The data can be transformed directly from Kafka to DREAD,

172
00:16:28,433 --> 00:16:30,133
and then query-based on DREAD.

173
00:16:31,208 --> 00:16:32,568
So what is DREAD?

174
00:16:32,568 --> 00:16:36,109
DREAD is a real-time analytics database

175
00:16:36,109 --> 00:16:40,050
that can store much larger data volume than MySQL

176
00:16:40,050 --> 00:16:42,770
while still maintains very fast query performance.

177
00:16:42,770 --> 00:16:47,131
With DREAD, we can no longer need to write Flink codes

178
00:16:47,131 --> 00:16:51,672
over and over again to implement the previous case,

179
00:16:51,672 --> 00:16:55,133
but only to use SQL to query after the data

180
00:16:55,133 --> 00:16:57,733
is loaded in DREAD.

181
00:16:57,733 --> 00:17:00,574
So analysis process become more flexible.

182
00:17:01,498 --> 00:17:07,861
In this screenshot, if we want to check all payment data in the latest day by far,

183
00:17:07,861 --> 00:17:12,223
all we need to do is just to query the database directly,

184
00:17:12,223 --> 00:17:18,546
and we don't need to worry about issues such as performance.

185
00:17:18,546 --> 00:17:25,549
For this more complex top-k account problem, here we put the corresponding DREAD query,

186
00:17:25,549 --> 00:17:27,590
which is fast and direct.

187
00:17:28,698 --> 00:17:31,681
and has no need for programming ability,

188
00:17:31,681 --> 00:17:34,084
it is suitable for data analytics

189
00:17:34,084 --> 00:17:37,027
to analyze the data quickly.

190
00:17:37,027 --> 00:17:40,250
Finally, let's look at a more special case.

191
00:17:40,250 --> 00:17:44,615
We need to monitor some special keywords in the game.

192
00:17:44,615 --> 00:17:47,358
Let's call them bug words here.

193
00:17:47,358 --> 00:17:50,121
Firstly, we can monitor the real-time chat data.

194
00:17:51,105 --> 00:17:57,287
Then, with the word segmentation techniques and bug word dictionary we set,

195
00:17:57,287 --> 00:18:00,948
we can quickly monitor the bug message in the game.

196
00:18:00,948 --> 00:18:14,613
Thanks for Zerton's wonderful speech. By far, we have solved two problems I mentioned in the very beginning.

197
00:18:15,377 --> 00:18:23,585
how to build a real-time data processing system for online games and a real-time data processing

198
00:18:23,585 --> 00:18:31,352
in net-based games. In the section to follow, I will present a more technical problem,

199
00:18:31,352 --> 00:18:38,959
how to optimize the current system, how to tackle some detailed technical issues.

200
00:18:39,893 --> 00:18:47,060
An important indicator of distributed services is high availability,

201
00:18:47,060 --> 00:18:55,388
which means how to automatically restore services even after some kind of node failure,

202
00:18:55,388 --> 00:19:02,755
such as configuring Flink on HL young and setting up the system checkpoint, etc.

203
00:19:02,755 --> 00:19:04,937
Also,

204
00:19:06,005 --> 00:19:10,328
Flink can guarantee processing exactly once,

205
00:19:10,328 --> 00:19:14,111
but if the upstream data is duplicated,

206
00:19:14,111 --> 00:19:19,274
Flink itself cannot avoid dealing with duplicated data.

207
00:19:19,274 --> 00:19:24,477
We need to pay attention to the logical deduplication,

208
00:19:24,477 --> 00:19:29,380
which is the so-called idempotent calculation.

209
00:19:29,380 --> 00:19:31,822
In addition,

210
00:19:31,822 --> 00:19:34,744
there are some more details.

211
00:19:35,352 --> 00:19:46,323
questions. For example, what if flink's parity zones exceeds the number of partitions in Kafka?

212
00:19:46,323 --> 00:19:56,332
Well, we should use rebalance option. Also, how to ensure the usability if we restart

213
00:19:56,332 --> 00:20:00,757
or update flink? Then we should use save point.

214
00:20:01,778 --> 00:20:14,485
The future of real-time data processing system might, from my perspective, focus on the following three points.

215
00:20:14,485 --> 00:20:14,905
Firstly,

216
00:20:16,023 --> 00:20:25,165
Real-time OLAP is a very promising direction as it can reduce the complexity of programming.

217
00:20:25,165 --> 00:20:35,527
Secondly, we would continue to improve the performance and efficiency of the real-time

218
00:20:35,527 --> 00:20:36,708
processing system.

219
00:20:37,525 --> 00:20:46,515
Last but not the least, we could work on monitoring the real-time processing system

220
00:20:46,515 --> 00:20:54,965
so that abnormal alarming is issued in time. Thanks you all for listening our talk.

