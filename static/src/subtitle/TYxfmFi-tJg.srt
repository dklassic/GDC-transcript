1
00:00:06,481 --> 00:00:11,365
Hello, I'm ClÃ©ment Duquesne, Technical Audio Designer and Audio Programmer at Pixel Riff,

2
00:00:11,685 --> 00:00:15,568
and I will be talking about the audio systems for our VR game Paper Beast.

3
00:00:16,248 --> 00:00:20,932
I'll be addressing several different topics ranging from simulation and physics to audio-based

4
00:00:20,952 --> 00:00:23,234
synchronization and VR spatialization.

5
00:00:25,215 --> 00:00:29,078
If you're not familiar with the game, Paper Beast is a VR adventure game where you're

6
00:00:29,158 --> 00:00:33,581
accidentally thrown into a poetic world in which new life is born from big data.

7
00:00:34,317 --> 00:00:38,285
and you will explore this world and slowly discover and interact with its ecosystem.

8
00:00:39,027 --> 00:00:41,091
Here's a short trailer to get a better idea.

9
00:01:39,739 --> 00:01:42,721
Ok so let's go now into the details of what we've seen.

10
00:01:43,182 --> 00:01:47,046
The game relies on a custom physics engine, and everything is physics driven.

11
00:01:47,526 --> 00:01:49,468
Creatures, plants, terrain, weather...

12
00:01:51,710 --> 00:01:55,974
The creatures have procedural locomotion, this means their movements are not written

13
00:01:56,014 --> 00:01:57,936
in advance but calculated in real time.

14
00:01:58,496 --> 00:02:02,300
They can slide if they are ascending slippery slopes, they can be carried away by river

15
00:02:02,340 --> 00:02:04,963
streams or struggle if their limb is caught by a predator.

16
00:02:07,168 --> 00:02:12,734
The terrain is handled with a fluid simulation. Water and sand flow dynamically. There's erosion.

17
00:02:12,854 --> 00:02:15,837
Creatures leave their tracks. They can dig and create small dams.

18
00:02:17,859 --> 00:02:22,824
The weather is dynamic as well. You can have rain, sandstorms, and wind affecting the physics.

19
00:02:23,325 --> 00:02:27,969
It ranges from a small breeze making plants rustle to a big storm blowing creatures away

20
00:02:28,049 --> 00:02:28,790
and flooding the land.

21
00:02:31,099 --> 00:02:33,520
And all these elements can be interacted with.

22
00:02:33,700 --> 00:02:36,021
You can grab the creatures, lift them in the air,

23
00:02:36,141 --> 00:02:37,542
help them cross obstacles.

24
00:02:38,202 --> 00:02:40,243
You also have tools to manipulate the terrain

25
00:02:40,283 --> 00:02:42,384
and weather directly or indirectly.

26
00:02:44,325 --> 00:02:46,706
All these elements and features had many implications

27
00:02:46,966 --> 00:02:49,147
and were the starting point of the audio tech design.

28
00:02:49,887 --> 00:02:52,769
Paper Beast being a VR game with immersion at its core,

29
00:02:53,009 --> 00:02:56,010
in an unfamiliar setting, we had to put a lot of energy

30
00:02:56,050 --> 00:02:58,511
to make sure runtime audio behavior was accurate.

31
00:02:59,303 --> 00:03:02,485
I mean synchronization, spatialization, propagation,

32
00:03:02,945 --> 00:03:06,707
they were all key elements for making the player truly believe in this universe,

33
00:03:06,807 --> 00:03:11,950
no matter its weirdness. Then there's this highly dynamic game context where situations

34
00:03:11,990 --> 00:03:17,373
get very unpredictable and granular with many moving parts. This means audio has to be very

35
00:03:17,413 --> 00:03:23,016
dynamic as well and react accurately to every possible situation, even when it gets a bit chaotic.

36
00:03:24,190 --> 00:03:28,794
And on top of that, there's the VR medium, which gives the player much more focus and discernment

37
00:03:28,814 --> 00:03:34,638
about what they're hearing with 360 immersion, head tracking, and we felt this really raised

38
00:03:34,698 --> 00:03:40,802
the bar for interactive audio. So because of all these things, sound has been an early focus in

39
00:03:40,822 --> 00:03:45,826
the development, and indeed the audio team was quite large compared to similar sized projects.

40
00:03:46,885 --> 00:03:51,671
For 12 people into the development team, 3 worked exclusively on audio, so that's a quarter.

41
00:03:52,252 --> 00:03:57,018
We had Florianne Pochon, she's a radio artist who specializes in pseudo-natural sounds.

42
00:03:57,438 --> 00:03:59,501
She's fully into original sound creation.

43
00:04:00,322 --> 00:04:05,508
We had Rolly Porter for the original soundtrack, a British composer of electronic ambient music.

44
00:04:06,247 --> 00:04:10,729
They both brought a very fresh vision as it was both the first time they worked for video games.

45
00:04:11,569 --> 00:04:15,631
And finally myself, I am a sound designer and C-sharp programmer and I took care of

46
00:04:15,771 --> 00:04:20,753
the audio tech from integration to system design and coding. We worked with Unity and

47
00:04:20,793 --> 00:04:24,595
Wwise from AudioKinetic which was a big help to get many advanced audio features.

48
00:04:25,275 --> 00:04:29,477
We also used the binaural audio features of the PSVR as it was our main target platform.

49
00:04:31,725 --> 00:04:35,727
We'll now go through the major audio features of Paper Beast and we'll use them to discuss

50
00:04:35,767 --> 00:04:39,729
the more general strategies and lessons we learnt about interactive audio and VR.

51
00:04:40,489 --> 00:04:44,351
First we'll talk about the creature sounds and more globally how we handled the physics

52
00:04:44,411 --> 00:04:45,031
engine's audio.

53
00:04:45,791 --> 00:04:50,393
Then we'll go with the vocalizations and how we used audio to drive animation and other

54
00:04:50,433 --> 00:04:51,934
things throughout the project.

55
00:04:52,794 --> 00:04:57,696
And finally we'll talk about the environments of Paper Beast and how we handled VR spatialization.

56
00:05:00,190 --> 00:05:01,451
Let's start with the creatures.

57
00:05:02,913 --> 00:05:05,215
Here are a few of the creatures inside Paper Beast.

58
00:05:05,495 --> 00:05:09,879
As you can see, we have many very different creatures with different number of legs, different

59
00:05:09,919 --> 00:05:11,980
sizes, different behaviors and materials.

60
00:05:12,561 --> 00:05:16,264
They were designed in a custom editor within Unity which we called PolyTool.

61
00:05:16,965 --> 00:05:20,628
Creatures can be created and edited on the fly, adjusted very quickly.

62
00:05:20,808 --> 00:05:22,029
It's a very flexible tool.

63
00:05:24,730 --> 00:05:29,352
As we said, these creatures just have a code behavior, but they have no traditional keyframe animations.

64
00:05:29,772 --> 00:05:34,835
They have intents of movements, but unexpected results can happen anytime if the creature slips,

65
00:05:35,095 --> 00:05:37,936
if another creature interferes, if the player grabs it.

66
00:05:38,657 --> 00:05:43,399
And the terrain is also possibly soft, so creatures can be buried, the ground can fall underneath them,

67
00:05:43,979 --> 00:05:47,101
and there's also water and streams, making it even more complex.

68
00:05:47,981 --> 00:05:52,924
So for us in audio, the only reliable source of information is the actual physics engine's data.

69
00:05:54,345 --> 00:05:59,208
Within these features and constraints, what we wanted for the audio was something as flexible

70
00:05:59,268 --> 00:06:04,210
as the physics editor, something that reacts accurately and closely to the physics simulation,

71
00:06:04,710 --> 00:06:08,792
but most importantly we wanted to give a real sonic identity to each creature.

72
00:06:10,974 --> 00:06:15,056
So how did we approach this? Well, physics, fluid simulation,

73
00:06:15,316 --> 00:06:20,378
and all small granularity, very dynamic systems are a big challenge for interactive audio design.

74
00:06:21,244 --> 00:06:26,526
When working with physics, on one hand, you have these unpredictable continuous streams

75
00:06:26,586 --> 00:06:33,829
of data coming from everywhere in the simulation, and what you need is to trigger a very limited

76
00:06:33,889 --> 00:06:37,171
number of discrete sounds at a given time, maybe a dozen.

77
00:06:38,171 --> 00:06:41,132
Also, you can't anticipate anything that's happening next.

78
00:06:42,533 --> 00:06:47,535
So the work here is really to interpret this data, to translate it, and make it usable

79
00:06:47,655 --> 00:06:48,915
and meaningful for audio.

80
00:06:51,082 --> 00:06:55,365
As said earlier, we could rely only on the physics engine's data, which are positions,

81
00:06:55,505 --> 00:06:59,288
velocities, collision information, and none of this data is clean.

82
00:06:59,548 --> 00:07:03,811
Even when you see what seems to be a clean collision, it's often a succession of frames

83
00:07:04,151 --> 00:07:07,974
where many collisions occur with various velocities, there's a lot of noise.

84
00:07:08,775 --> 00:07:13,958
So we built small code units to analyze specific physics interactions, which we call sound sources.

85
00:07:14,768 --> 00:07:19,090
Their job is to filter the noise and translate the data into sound information.

86
00:07:19,811 --> 00:07:25,313
This sound information is events for triggering and stopping sounds plus runtime audio parameters.

87
00:07:26,534 --> 00:07:30,395
The sound sources are instanced and attached to specific points on our creatures.

88
00:07:31,156 --> 00:07:34,157
And there's one sound source type for each different physic interaction.

89
00:07:35,984 --> 00:07:42,931
For example, our most common sound source types were move, impact, slide, tension, immersion,

90
00:07:43,391 --> 00:07:44,653
shake and bind.

91
00:07:45,313 --> 00:07:50,038
And each was tasked to trigger a specific set of audio events and to control specific

92
00:07:50,078 --> 00:07:51,019
parameters in Wwise.

93
00:07:53,486 --> 00:07:56,329
Now let's take the case of the Move sound source as an example.

94
00:07:57,110 --> 00:08:00,534
So the Move sound source is used for putting sound on simple movements.

95
00:08:00,954 --> 00:08:06,100
For this one, we used three inputs from the physics engine, which are speed, acceleration,

96
00:08:06,380 --> 00:08:07,021
and position.

97
00:08:07,882 --> 00:08:11,185
Each one is fetched on the points that are attached to the sound source instance.

98
00:08:11,998 --> 00:08:17,400
First, speed and acceleration are mixed into a custom intensity value, which is computed per

99
00:08:17,440 --> 00:08:23,562
point. Then, this value is averaged between points and with a system of thresholds, it triggers and

100
00:08:23,602 --> 00:08:29,605
stops a sound loop in Wwise. This same value is smoothed and sent to our RTPC in Wwise,

101
00:08:30,105 --> 00:08:32,586
affecting volume and filters depending on the case.

102
00:08:33,595 --> 00:08:37,377
The point positions are weighted by the intensity into a centroid.

103
00:08:37,918 --> 00:08:42,261
This centroid is smoothed over time and used as the sound emitter position by Wwise.

104
00:08:43,602 --> 00:08:47,605
Now we'll explain a bit more each stage of the process, beginning with the mixing of

105
00:08:47,665 --> 00:08:50,206
speed and acceleration into this custom intensity.

106
00:08:51,787 --> 00:08:55,650
So it's actually a basic tweakable sum and product of speed and acceleration.

107
00:08:56,211 --> 00:08:59,233
The coefficients are set by the audio designers for each creature.

108
00:08:59,813 --> 00:09:02,675
This allows us to tweak the weight of velocity and acceleration.

109
00:09:03,440 --> 00:09:08,321
In some cases, we wanted the acceleration to affect more, for example the sounds of paper creasing.

110
00:09:09,062 --> 00:09:13,903
Or sometimes we wanted the velocity to affect more, for example for a parachute whooshing around.

111
00:09:15,644 --> 00:09:21,466
Then, from this intensity, how do we get to the actual triggering of events and setting of parameters in Wwise?

112
00:09:22,466 --> 00:09:27,748
We start from this noisy intensity input and we add a threshold above which sound starts playing.

113
00:09:28,574 --> 00:09:32,679
But as we see, this would result in sound starting and stopping very quickly, which

114
00:09:32,739 --> 00:09:35,342
is not aesthetically or technically viable in audio.

115
00:09:36,275 --> 00:09:40,517
So we add a hysteresis behavior, which means there are two threshold values.

116
00:09:41,017 --> 00:09:43,358
When going above the upper one, we start the sound.

117
00:09:43,498 --> 00:09:45,939
When going below the lower one, we stop the sound.

118
00:09:46,399 --> 00:09:50,822
This makes much cleaner starts and stops for the sound, while still being very reactive.

119
00:09:51,422 --> 00:09:55,464
Then we add temporal thresholds too, a minimum duration, and a cooldown,

120
00:09:55,664 --> 00:09:58,225
which ensure that Wwise is never spammed with instructions

121
00:09:58,505 --> 00:10:01,126
and that the sounds have time to fade in and out properly.

122
00:10:02,130 --> 00:10:06,855
In parallel, this intensity value is also smoothed and sent to Wwise as a RTPC.

123
00:10:08,937 --> 00:10:12,881
About spatialization, each sound source is specialized independently.

124
00:10:13,501 --> 00:10:17,846
This has been key for us in VR. The moment we put one sound emitter per leg

125
00:10:18,006 --> 00:10:20,889
to play the footsteps was the moment they started to really work.

126
00:10:21,952 --> 00:10:23,994
In flat screen games, it's often more permissive.

127
00:10:24,094 --> 00:10:29,097
For example, you can specialize a whole humanoid voice for each footsteps on a single position

128
00:10:29,217 --> 00:10:29,938
and it sounds fine.

129
00:10:30,498 --> 00:10:32,600
In VR, you have to be more accurate than that.

130
00:10:32,920 --> 00:10:37,423
And in Paper Beast, I would say any creature bigger than a basketball had to be specialized

131
00:10:37,483 --> 00:10:38,304
on several emitters.

132
00:10:39,525 --> 00:10:43,428
So the emitters were positioned on the points that were attached to the sound sources.

133
00:10:44,168 --> 00:10:49,032
And when there were many points, we use centroids weighted by our custom intensity value to

134
00:10:49,072 --> 00:10:50,212
get as precise as we could.

135
00:10:50,773 --> 00:10:51,453
Here's an example.

136
00:10:53,495 --> 00:10:58,099
On this creature, the metallic rustle is moving along the body as the centroid follows movement

137
00:10:58,139 --> 00:10:58,660
in density.

138
00:10:59,400 --> 00:11:03,064
The small points are the ones assigned to the sound source, and their centroid is the

139
00:11:03,084 --> 00:11:03,444
big point.

140
00:11:20,373 --> 00:11:24,876
So the idea was that to each creature we could add any number of sound sources assigned to

141
00:11:24,896 --> 00:11:25,776
the points we wanted.

142
00:11:26,377 --> 00:11:28,658
Here is CAGE, one of our biggest creatures in the game.

143
00:11:28,978 --> 00:11:32,440
It has already 3 sound sources just for the sound of the body crackling.

144
00:11:33,261 --> 00:11:37,523
One for the head and neck, one for the left part of the body, and one for the right part.

145
00:11:38,183 --> 00:11:41,905
This was for when you go between its legs, you can hear the two sets of legs rumbling

146
00:11:42,025 --> 00:11:42,586
all around you.

147
00:11:43,446 --> 00:11:48,369
Also you can see the sound emitter for immersion being positioned in the middle of the immersed

148
00:11:51,167 --> 00:11:54,849
And here, for a simpler, smaller creature, you have a different sound source set.

149
00:11:58,971 --> 00:12:03,093
The locomotion sounds were a specific type of sound source that managed foot slides and

150
00:12:03,153 --> 00:12:03,553
footsteps.

151
00:12:04,173 --> 00:12:05,394
We used one emitter per leg.

152
00:12:06,074 --> 00:12:09,095
One challenge we faced was to characterize the gait of each creature.

153
00:12:09,936 --> 00:12:14,137
Often when you hear a real animal moving, especially when it's fast, you don't hear

154
00:12:14,258 --> 00:12:16,078
individual steps, you hear a rhythm.

155
00:12:16,945 --> 00:12:20,906
In theory that should have been automatic with our audio parameters being driven by the

156
00:12:20,946 --> 00:12:26,348
strength of each step but we wanted to accentuate that to avoid the typing machine effects when

157
00:12:26,388 --> 00:12:32,890
footsteps sound like tac tac tac tac tac tac tac. That's why we added static wise parameters giving

158
00:12:32,950 --> 00:12:38,712
to each foot different offsets of volume pitch and filters. So each footstep sound would be affected

159
00:12:38,752 --> 00:12:44,934
by these parameters giving a rhythm to the global gate. Now let's listen a bit to the result of all

160
00:13:40,773 --> 00:13:43,154
In the end, we would say these are the benefits of the method.

161
00:13:43,514 --> 00:13:45,295
The workflow is rather efficient.

162
00:13:45,595 --> 00:13:47,755
It was pretty quick to put sounds on new creatures.

163
00:13:48,656 --> 00:13:50,736
There's a lot of room for user intervention.

164
00:13:50,996 --> 00:13:55,358
It's easy to author the creatures, give them sound signatures, and adjust things when they

165
00:13:55,418 --> 00:13:56,458
didn't work right away.

166
00:13:57,498 --> 00:14:00,199
And its modular structure made it very flexible.

167
00:14:00,479 --> 00:14:03,520
In the end, it was used for all physical objects, not only creatures.

168
00:14:04,160 --> 00:14:08,021
And it was easy to add new sound source types for new behaviors that would follow the same

169
00:14:08,061 --> 00:14:08,442
pipeline.

170
00:14:09,978 --> 00:14:13,138
But there's also one particular drawback, it is CPU heavy.

171
00:14:13,839 --> 00:14:17,579
As each object can trigger many sounds, the voice count can easily go too high.

172
00:14:18,179 --> 00:14:20,280
And there's a limit to what voice can do on that level.

173
00:14:20,720 --> 00:14:24,120
So we had to do a system for custom high-level voice management.

174
00:14:24,701 --> 00:14:28,921
It was a bit of work, but it was essential, and after that, it made our life much easier.

175
00:14:31,382 --> 00:14:35,703
Now let's talk about the creature's vocalizations and how to drive animation and other things

176
00:14:36,063 --> 00:14:36,443
with audio.

177
00:14:37,575 --> 00:14:42,497
Creatures in Paper Beast vocalize when they are manipulated, when they are afraid or hurt or curious.

178
00:14:43,138 --> 00:14:48,641
So the vocalizations were a vital element in making the creatures feel alive and having the player connect with them.

179
00:14:50,602 --> 00:14:55,824
Our intention was to make them sound like nothing known and also nothing too distinctively vocal.

180
00:14:56,705 --> 00:15:01,167
Florian used a lot of non-vocal material, glitches, cardboard, flutes, pencils.

181
00:15:01,888 --> 00:15:03,689
Here are a few examples from the game.

182
00:15:04,809 --> 00:15:05,490
The Predator.

183
00:15:09,040 --> 00:15:09,400
Cage,

184
00:15:13,001 --> 00:15:13,481
Cotillon,

185
00:15:17,142 --> 00:15:17,902
the Papyvore,

186
00:15:21,163 --> 00:15:21,823
and the Tape.

187
00:15:25,944 --> 00:15:28,785
The challenge with these sounds was to connect them to the visuals.

188
00:15:29,245 --> 00:15:32,446
As the creatures both look and sound a bit out of this world,

189
00:15:32,806 --> 00:15:36,787
we had to give a little push so the player would recognize and accept these sounds

190
00:15:37,007 --> 00:15:38,747
as their voices without questioning.

191
00:15:41,889 --> 00:15:45,652
As the creatures are very flexible because entirely physically simulated,

192
00:15:46,293 --> 00:15:49,375
and as there are no keyframe animations and no animators,

193
00:15:49,935 --> 00:15:52,037
why not drive their movements with audio?

194
00:15:54,118 --> 00:15:58,181
Our strategy for this was to bake sound analysis data from the vocalization assets.

195
00:15:58,762 --> 00:16:01,544
We made this choice so we could tweak the analysis data

196
00:16:01,704 --> 00:16:04,266
and so we could anticipate by knowing the data in advance.

197
00:16:05,072 --> 00:16:07,652
It also allowed us to ignore runtime attenuations

198
00:16:07,912 --> 00:16:10,773
and base the analysis only on the original sound files.

199
00:16:12,213 --> 00:16:13,733
Here's an overview of our workflow.

200
00:16:14,214 --> 00:16:15,874
First, we have the offline analysis,

201
00:16:16,014 --> 00:16:18,174
where we extract pitch and volume information.

202
00:16:18,935 --> 00:16:21,775
Then audio designers tweak and mix the analysis curves

203
00:16:21,855 --> 00:16:23,636
with custom tools inside Unity.

204
00:16:24,616 --> 00:16:26,616
Then at runtime, when a sound is played,

205
00:16:26,816 --> 00:16:28,237
the game fetches the matching curve

206
00:16:28,457 --> 00:16:30,337
and feeds it back to the animation,

207
00:16:30,617 --> 00:16:31,737
the nodal script module,

208
00:16:32,037 --> 00:16:33,778
or anything else through the game code API.

209
00:16:37,077 --> 00:16:39,299
For the analysis, we used open source software,

210
00:16:39,499 --> 00:16:41,621
Sonic Annotator and Sonic Visualizer.

211
00:16:42,361 --> 00:16:44,743
With these two, you can visualize and batch extract

212
00:16:44,883 --> 00:16:46,924
almost any kind of data from audio files.

213
00:16:47,865 --> 00:16:50,627
In our case, we extracted amplitude, pitch centroid,

214
00:16:50,927 --> 00:16:51,968
and spectral centroid.

215
00:16:52,769 --> 00:16:55,851
The analysis batch was launched automatically from Unity.

216
00:16:56,691 --> 00:16:57,392
Here's how it looks.

217
00:16:57,732 --> 00:17:00,734
First, you select sound files, you run the analysis,

218
00:17:01,435 --> 00:17:03,977
then you can preview the sounds while visualizing

219
00:17:04,157 --> 00:17:04,977
and tweaking the curves.

220
00:17:05,805 --> 00:17:09,327
And you can create mix curves by combining pitch and volume together,

221
00:17:09,788 --> 00:17:12,850
with a bunch of tweakable parameters to get to the desired result.

222
00:17:13,791 --> 00:17:18,714
We also have additional options using Alternate Sounds for analysis,

223
00:17:18,774 --> 00:17:22,837
if you want to analyze only one layer of the sounds or get rid of parasite frequencies.

224
00:17:23,678 --> 00:17:26,480
And you can even draw on top of the curves if you don't get what you want.

225
00:17:29,167 --> 00:17:32,950
Here you see the raw pitch and volume curves and two custom mix curves.

226
00:17:33,430 --> 00:17:38,393
The mix curve exists in two modes, the absolute mode and the speed modifier mode, that I'm

227
00:17:38,453 --> 00:17:39,574
going to explain a bit more.

228
00:17:41,275 --> 00:17:45,958
Here's an animation with the absolute mode, where the movement follows a combination of

229
00:17:46,018 --> 00:17:46,638
pitch and volume.

230
00:17:56,149 --> 00:18:00,871
And here's an animation with the speed modifier mode, where it's the speed of the movement

231
00:18:00,911 --> 00:18:03,112
that follows a combination of pitch and volume.

232
00:18:12,397 --> 00:18:15,518
Sometimes we use these two modes combined on single animations,

233
00:18:16,078 --> 00:18:20,480
each curve driving different axes in the animation. Here's a basic example.

234
00:18:29,592 --> 00:18:32,835
And finally, some in-game examples. Here is the tape howling.

235
00:18:37,299 --> 00:18:39,861
Then, the baby vore.

236
00:19:07,699 --> 00:19:09,000
And finally, paper strip.

237
00:19:33,815 --> 00:19:35,996
In the end, this method helped us in several ways.

238
00:19:36,416 --> 00:19:41,118
It helped us making the vocalizations work by creating a strong audiovisual bond.

239
00:19:41,898 --> 00:19:46,799
It offered us free diversity in the vocalization motion as each sound file produces its own

240
00:19:46,879 --> 00:19:48,360
unique animation variation.

241
00:19:49,080 --> 00:19:53,362
And finally, it was very flexible and easy to adapt for various applications.

242
00:19:53,922 --> 00:19:58,963
And it could be enriched with new analysis methods, new offline processes for new uses.

243
00:19:59,724 --> 00:20:03,185
We didn't use it as much as we would have liked, but there's lots of potential.

244
00:20:05,220 --> 00:20:09,942
Now to the final part of this talk, where we'll speak about environment and VR spatialization.

245
00:20:11,803 --> 00:20:16,065
The environment of Paper Beast also had its share of challenges, essentially because of

246
00:20:16,105 --> 00:20:17,265
the VR specificities.

247
00:20:18,065 --> 00:20:22,968
In flat screen experiences, audio environment can be a background illustration that freely

248
00:20:23,008 --> 00:20:27,830
evokes what you want the player to imagine, but in VR the player has the environment all

249
00:20:27,870 --> 00:20:30,191
around them and they need to think they are truly there.

250
00:20:31,183 --> 00:20:37,387
Audio plays a major role in this VR immersion and it needs to be accurate and to behave in a natural and consistent way.

251
00:20:38,728 --> 00:20:42,030
Then you have also less choices when designing audio VR environments.

252
00:20:42,610 --> 00:20:45,753
With flat screen you can suggest a lot of things that aren't really there,

253
00:20:46,333 --> 00:20:49,935
but in VR you have to be very careful because there is no off-screen space.

254
00:20:50,836 --> 00:20:53,998
If something is heard the player can just look and see if it's there.

255
00:20:54,658 --> 00:21:00,202
You can still suggest something that is in an unseen space behind obstacles, but it has to be done with caution.

256
00:21:01,153 --> 00:21:03,815
And finally, there's the challenge of outdoors in VR.

257
00:21:05,717 --> 00:21:09,921
Making natural-sounding environments is somewhat easy when you are dealing with point sources,

258
00:21:10,041 --> 00:21:14,125
for example in interiors with a lot of machinery, electric devices.

259
00:21:14,945 --> 00:21:18,889
That's because binaural solutions provide good spatialization for point sources.

260
00:21:19,638 --> 00:21:24,001
But without those, you have to deal with ambience pads, which is a bit less easy in VR.

261
00:21:24,581 --> 00:21:29,244
There's the ambisonic format, which works quite well if you are able to record your environments,

262
00:21:29,724 --> 00:21:34,607
but to design ambisonic pads from scratch for non-realistic environments requires a

263
00:21:34,647 --> 00:21:38,710
very specific setup, and it isn't very fast nor intuitive.

264
00:21:39,630 --> 00:21:42,572
So we searched for other solutions that we're going to introduce.

265
00:21:44,178 --> 00:21:47,481
Our strategy for Ambience Pads was to blur the spatialization.

266
00:21:48,082 --> 00:21:51,605
The goal was to make sure the player wouldn't feel like they are listening to just sound

267
00:21:51,645 --> 00:21:52,246
files playing.

268
00:21:52,946 --> 00:21:57,391
We wanted to trick the ear into believing the sounds were coming from the VR world itself.

269
00:21:58,271 --> 00:22:02,355
To achieve this, we combined several spatialization techniques, trying to find the right blend

270
00:22:02,435 --> 00:22:04,097
for each scene and each asset.

271
00:22:05,785 --> 00:22:08,787
First, we use static, non-specialized binaural recordings.

272
00:22:09,187 --> 00:22:12,889
They sound very natural and are easily accepted by the ear as real.

273
00:22:13,630 --> 00:22:18,912
When rotating the head, they can sound awkward, especially if there are prominent elements in the sound.

274
00:22:19,413 --> 00:22:23,995
So we use them for indefinite, continuous textures like air tones or sand rustling.

275
00:22:24,455 --> 00:22:27,316
Then we used spatialized quads, so four-channel sounds.

276
00:22:27,597 --> 00:22:32,118
They are easier to author than ambisonic and they give a good sense of 3D depth to the scene

277
00:22:32,278 --> 00:22:36,960
when rotating the head. We had four emitters at high altitude for the four channels,

278
00:22:37,260 --> 00:22:40,001
each one with a bit of spread to smooth the rotation effect.

279
00:22:40,842 --> 00:22:44,783
Sometimes we use them with the binaural plugin, sometimes with the default spatialization.

280
00:22:45,715 --> 00:22:50,178
We also had an audio billboard mode to easily convert a stereo file to a quad pad.

281
00:22:50,859 --> 00:22:55,202
We would play the stereo file twice, with the second one seeking at 50% of the file.

282
00:22:56,343 --> 00:23:00,566
They would play on orthogonal facing emitters, giving the same result as a quad file.

283
00:23:01,667 --> 00:23:07,152
We used additional random effects to strengthen the 3D immersion and diversify the result,

284
00:23:07,692 --> 00:23:11,695
but we did this very carefully not to suggest world elements that should have been visible.

285
00:23:13,072 --> 00:23:16,314
Now, an important part of Paper Beast audio environments is the wind.

286
00:23:16,935 --> 00:23:21,317
The world of Paper Beast being quite barren, apart from the ecosystem, wind was a natural

287
00:23:21,377 --> 00:23:24,139
choice for being the main environmental audio character.

288
00:23:25,159 --> 00:23:29,582
On top of that, it was also a part of gameplay, so it really made sense to give it some sonic

289
00:23:29,602 --> 00:23:29,982
presence.

290
00:23:30,923 --> 00:23:36,666
But then the question was, how do we convincingly specialize something that is immaterial, everywhere,

291
00:23:36,906 --> 00:23:38,287
and yet constantly moving?

292
00:23:40,032 --> 00:23:44,836
Here again, our strategy was to combine several specialization techniques to try and make it feel real.

293
00:23:45,576 --> 00:23:51,380
We used specialized quads for the bass with a lot of asset variations for the different intensity across the game.

294
00:23:52,781 --> 00:23:55,423
Then we added the sound of wind buffeting in the ears.

295
00:23:55,743 --> 00:24:00,106
This was done with two voices playing in each ear, full left and full right.

296
00:24:00,984 --> 00:24:06,010
We made these voices react to head tracking with two individual EQs changing the filtering

297
00:24:06,110 --> 00:24:10,936
depending on the head orientation to simulate the wind entering differently in the ear depending on

298
00:24:10,956 --> 00:24:18,184
the direction. We also added 3D wind gusts with moving point sources circulating around the player

299
00:24:18,645 --> 00:24:20,167
and following the direction of the wind.

300
00:24:20,977 --> 00:24:25,599
When the wind is soft, they are almost inaudible, they only amplify the buffing sound and other

301
00:24:25,619 --> 00:24:26,620
wind-related sounds.

302
00:24:27,320 --> 00:24:31,722
But in high winds, they are very loud and they give a nice physicality to the air rushing

303
00:24:31,762 --> 00:24:32,162
around you.

304
00:24:33,503 --> 00:24:36,564
Another feature is the global dynamic equalizer on the wind bus.

305
00:24:36,904 --> 00:24:41,706
The idea is that the frequency and gain parameters of the EQ are defined by the ambient zones.

306
00:24:42,306 --> 00:24:45,208
So they specially interpolate when going from one zone to another.

307
00:24:46,053 --> 00:24:49,696
We use this, for example, to muffle the wind when going in a covered space,

308
00:24:50,356 --> 00:24:54,358
or on the contrary, to boost the bass and gain when going on an exposed ridge.

309
00:24:55,259 --> 00:24:58,540
So we create subtle local changes that are very cheap to do,

310
00:24:59,141 --> 00:25:01,502
and they give an impression of continuous change.

311
00:25:01,742 --> 00:25:05,624
This prevents the player from feeling a monotony that would break the illusion

312
00:25:05,905 --> 00:25:08,366
and makes him believe that the wind is actually making this noise.

313
00:25:11,308 --> 00:25:13,689
Okay, now let's listen to all of this in-game.

314
00:25:59,878 --> 00:26:03,819
Now for the reverbs, we tried again to convey a sense of space as much as possible.

315
00:26:04,400 --> 00:26:07,961
We used convolution reverbs and we added specialized reflections.

316
00:26:08,581 --> 00:26:12,962
The reflections enhanced the 3D impression of the scene, which is very valuable in VR,

317
00:26:13,402 --> 00:26:17,583
but they also made it easier to highlight the different locations and different acoustics,

318
00:26:18,023 --> 00:26:21,424
which isn't always obvious with the limitations of impulse responses.

319
00:26:22,643 --> 00:26:26,908
We placed these reflections on the prominent topological features of the levels, big cliffs,

320
00:26:27,088 --> 00:26:29,630
big rocks, and there are plenty of those in the game.

321
00:26:31,513 --> 00:26:36,017
On the technical side, we used auxiliary 3D positioned buses in Wwise and we put them

322
00:26:36,118 --> 00:26:38,120
as children of the convolution reverb bus.

323
00:26:38,901 --> 00:26:41,744
Each of these buses carried an EQ effect and a delay effect.

324
00:26:42,684 --> 00:26:46,687
We would have liked to put variable delays instead of static ones to have a realistic

325
00:26:46,727 --> 00:26:48,609
behavior when going near the reflectors.

326
00:26:49,209 --> 00:26:53,192
We tried with Heavy from Enzyne Audio, but we didn't have time to get to the end of

327
00:26:53,232 --> 00:26:54,373
that, but it's doable.

328
00:26:56,755 --> 00:27:00,898
Still on propagation, we did a basic sound occlusion system that would measure the thickness

329
00:27:00,958 --> 00:27:03,440
of obstacles between the listener and the emitter.

330
00:27:04,061 --> 00:27:08,965
It was difficult to do more in an open outdoor environment, but this simple solution was

331
00:27:09,005 --> 00:27:09,945
a lot better than nothing.

332
00:27:10,726 --> 00:27:15,088
In VR, it's very confusing to hear very clearly and loudly something you don't see at all.

333
00:27:15,628 --> 00:27:17,149
That's why occlusion is essential.

334
00:27:17,689 --> 00:27:22,392
On the audio processing side, we used Wwise's occlusion pipeline to filter independently

335
00:27:22,492 --> 00:27:27,834
pre-fader and post-fader, allowing us to filter only the dry signal and keep the reverbs and

336
00:27:27,874 --> 00:27:29,595
reflections when there is little occlusion.

337
00:27:30,396 --> 00:27:32,056
And we found this gives nice results.

338
00:27:33,859 --> 00:27:38,122
And finally, we wanted to make a last note about something very specific to the VR medium.

339
00:27:38,782 --> 00:27:42,625
When we were still looking for a composer, we made some tests with atmospheric music

340
00:27:42,665 --> 00:27:43,046
we liked.

341
00:27:43,786 --> 00:27:48,730
And even when the tone was 100% matching the game universe and scene, most of the time

342
00:27:48,750 --> 00:27:50,071
it didn't work in VR.

343
00:27:50,692 --> 00:27:55,475
It felt like our VR character was listening to music on headphones within the game, instead

344
00:27:55,595 --> 00:27:59,418
of having the magical blend that happens so easily in flat screen experiences.

345
00:28:00,515 --> 00:28:04,278
We think this comes from a dissonance between the acoustic space of the music,

346
00:28:04,538 --> 00:28:08,342
the space of the recording studio, or the space of the reverbs used by the composer,

347
00:28:09,003 --> 00:28:10,524
and the acoustic space of the game.

348
00:28:10,544 --> 00:28:12,065
A desert, hills,

349
00:28:12,817 --> 00:28:14,037
a canyon, a cave.

350
00:28:14,737 --> 00:28:19,558
So we thought of many ways to counteract this effect by specializing the music inside the

351
00:28:19,578 --> 00:28:20,519
game and so on.

352
00:28:21,019 --> 00:28:25,700
But Gladly, our composer, had a good intuition about this and he worked on the soundtrack

353
00:28:25,740 --> 00:28:30,721
by alternating all along with the VR headset and listening and tweaking it while inside

354
00:28:30,781 --> 00:28:31,102
VR.

355
00:28:32,022 --> 00:28:36,243
By doing this, he always found where to place his music acoustically in the scene, which

356
00:28:36,283 --> 00:28:37,343
was a big relief for us.

357
00:28:38,200 --> 00:28:42,601
The result is a soundtrack that is often melded into the sound effects, making it difficult

358
00:28:42,641 --> 00:28:45,482
to know where the world's sounds end and where the music begins.

359
00:28:46,402 --> 00:28:49,983
We still specialized the music a couple of times to integrate it even more in the sound

360
00:28:50,023 --> 00:28:50,603
environment.

361
00:28:53,344 --> 00:28:55,104
So we are coming to the end of this talk.

362
00:28:55,364 --> 00:28:56,804
Let's try and sum up all of this.

363
00:28:57,685 --> 00:29:02,366
We've discussed about the interpretation of a simulation's data into audio information

364
00:29:02,506 --> 00:29:04,466
with some examples of our implementation.

365
00:29:05,514 --> 00:29:10,778
Then we've seen how we can use the simulation's plasticity at our advantage and use audio

366
00:29:10,798 --> 00:29:13,280
to drive other game elements such as animation.

367
00:29:14,801 --> 00:29:19,385
And finally we've talked about some of the VR spatialization techniques we used and how

368
00:29:19,445 --> 00:29:24,329
by combining them and adding dynamic behaviors and details we tried to trick the player into

369
00:29:24,369 --> 00:29:26,090
believing into our VR illusion.

370
00:29:30,887 --> 00:29:35,196
Okay well, thank you very much for listening to this talk and feel free to comment this

371
00:29:35,236 --> 00:29:39,885
video or DM me on Twitter if you have questions or if you want to start a conversation.

372
00:29:40,827 --> 00:29:40,987
Bye!

