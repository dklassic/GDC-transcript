1
00:00:06,197 --> 00:00:10,623
Oh, there we go. I'm super excited to talk about this, you have no idea.

2
00:00:11,905 --> 00:00:15,349
OK, I'm going to talk about motion matching in the future of games animation today.

3
00:00:16,591 --> 00:00:17,332
But before I get to that.

4
00:00:18,063 --> 00:00:22,425
Let's see. My name is Christian Jean-Diouc. I'm the animation director from Ubisoft Toronto.

5
00:00:22,886 --> 00:00:27,128
As you can see, I've been in the industry since 2000 with various different styles of game under my belt.

6
00:00:27,648 --> 00:00:32,490
Then in 2004, I decided to lean more towards animation-driven games, such as Assassin's Creed,

7
00:00:32,970 --> 00:00:36,712
which led me to a couple of studio moves before Ubisoft Toronto tempted me back

8
00:00:37,172 --> 00:00:40,714
and gave me the animation director position on Splinter Cell Blacklist, which was shipped in 2013.

9
00:00:41,874 --> 00:00:46,301
I've since been working on various Codev projects and a couple of unannounced titles and some

10
00:00:46,321 --> 00:00:48,705
of the cool tech that you'll see in here.

11
00:00:48,905 --> 00:00:52,271
You will also notice if you're keen that Dan and I work together at Bazaar.

12
00:00:55,000 --> 00:00:57,041
Okay, so, quick disclaimer before we get started.

13
00:00:57,541 --> 00:00:59,923
This presentation demos a new animation prototype.

14
00:01:00,343 --> 00:01:02,665
This is tech that the team is very, very passionate about.

15
00:01:03,005 --> 00:01:07,148
And because of the potential applications Ubisoft is keen to invest in and push our games forward.

16
00:01:07,588 --> 00:01:12,952
The tech is being considered for future games, but the assets seen here do not represent any game currently in development.

17
00:01:12,972 --> 00:01:13,913
Is that clear?

18
00:01:14,873 --> 00:01:15,874
Good, alright, okay.

19
00:01:16,875 --> 00:01:19,176
As long as everyone's clear, we've all signed NDAs, we're fine.

20
00:01:19,196 --> 00:01:20,397
Alright, so.

21
00:01:21,733 --> 00:01:27,257
Okay, so I'm going to give you a short overview of what I'm talking about. I'm going to give you a quick explanation of what motion matching is.

22
00:01:27,818 --> 00:01:30,981
Then I'm going to talk about the process of how we work with motion matching.

23
00:01:31,501 --> 00:01:35,324
Then I want to describe quickly ways that we can manipulate the system.

24
00:01:35,724 --> 00:01:39,528
Then I'm going to show you a couple of cool tests that we've tried out, just because we can.

25
00:01:41,009 --> 00:01:47,474
And then I'm going to kind of wrap up with a conclusion, talking about successes, failures, things we want to do for the future, and various different things.

26
00:01:48,475 --> 00:01:48,695
Okay.

27
00:01:50,654 --> 00:01:54,357
Motion matching is a concept that's been around for a while and has been known as a few different things.

28
00:01:54,737 --> 00:01:57,139
It's been known as things like motion graphs or motion fields,

29
00:01:57,560 --> 00:02:01,323
but it's only really been this generation of consoles that has made it possible for practical use.

30
00:02:04,245 --> 00:02:08,849
So, to understand motion matching, we really need to first understand what the current standard process is.

31
00:02:09,530 --> 00:02:11,472
So, I'm massively oversimplifying this, but...

32
00:02:12,465 --> 00:02:14,147
Once we know what we want to do, we'll go to MoCap.

33
00:02:14,707 --> 00:02:16,788
We'll capture loads of individual cycles and movements,

34
00:02:16,848 --> 00:02:20,531
making sure that we can carefully capture as many actions as we need for our systems.

35
00:02:23,753 --> 00:02:27,135
So then once we get the data back, the animators will painstakingly cut up the clips,

36
00:02:27,455 --> 00:02:29,977
tidy up the poses, create the loops and polish the animation,

37
00:02:30,417 --> 00:02:31,798
done a ton easier by Dan's tech.

38
00:02:33,797 --> 00:02:38,100
But for years, games animation has been using things like state machines as standard.

39
00:02:38,160 --> 00:02:40,662
With node-based options becoming the option favoured by most projects.

40
00:02:41,362 --> 00:02:44,024
So, if you're here, chances are you know what one of these is,

41
00:02:44,404 --> 00:02:46,185
and you know how difficult it can be to use correctly.

42
00:02:46,666 --> 00:02:51,689
So we have to add nodes, we have to set up rules, we have to add blend times, loop points, etc. etc.

43
00:02:52,690 --> 00:02:56,933
And then, if you get all that working, then you should hopefully get something that goes into the game,

44
00:02:57,393 --> 00:02:58,254
if you're very, very lucky.

45
00:02:58,734 --> 00:03:02,196
So, of course that's oversimplifying the process, but what if there was another way?

46
00:03:05,595 --> 00:03:10,336
Okay, so the idea of motion matching is really to find the best possible way to go from A to B

47
00:03:10,877 --> 00:03:16,078
with as little fuss as possible, allowing animators more time to focus on what the actions actually are

48
00:03:16,178 --> 00:03:21,479
instead of having to worry about losing fidelity of their work and manipulation during implementation.

49
00:03:22,959 --> 00:03:29,401
So to highlight one of these issues we wanted to address, here is an example of weight shifting due to a desired change of direction.

50
00:03:30,481 --> 00:03:31,762
So let's see if this works.

51
00:03:33,210 --> 00:03:33,571
There we go.

52
00:03:35,032 --> 00:03:38,575
This seems like a simple enough move, but is incredibly hard to recreate in a state machine

53
00:03:39,135 --> 00:03:40,196
and to remain responsive.

54
00:03:40,977 --> 00:03:42,978
So, if we break this move down in slow motion,

55
00:03:44,239 --> 00:03:46,521
you can see the moment I decide to change direction,

56
00:03:47,702 --> 00:03:50,084
where the entire body will move and will bend,

57
00:03:50,444 --> 00:03:53,827
and it'll just, to remove momentum, shift towards that desired direction.

58
00:03:54,542 --> 00:04:00,045
Solving this in a state machine would be difficult, as we would need to likely use a transition animation to replicate this actual move.

59
00:04:01,146 --> 00:04:06,089
But the problem is, is we would lose responsiveness, as I said, and we'd probably lose quite a lot of fidelity.

60
00:04:06,889 --> 00:04:12,272
And it also wouldn't look grounded, because we would probably have to do it with blends, and it'd just look a bit weird.

61
00:04:12,812 --> 00:04:15,794
Anyway, this is kind of the core of what we wanted to fix.

62
00:04:16,735 --> 00:04:19,956
But it would also be a good example of the sort of quality that we were trying to achieve.

63
00:04:22,005 --> 00:04:26,767
So, none of this would be possible unless we assembled a team of animation experts passionate about gameplay animation.

64
00:04:27,307 --> 00:04:34,689
Starting with the animation programmer, Marco Butner, whose idea of motion matching had been in the first place, along with Simon, who's here as well.

65
00:04:35,429 --> 00:04:43,171
We were just looking for the right opportunity to push things further, and this seemed like coming to Ubisoft Toronto was the perfect time.

66
00:04:44,210 --> 00:04:47,451
We then added gameplay programmer Mike Wazalewski

67
00:04:47,531 --> 00:04:49,811
and technical art director Alexander Brezniak,

68
00:04:49,831 --> 00:04:52,392
who I think is also in the room somewhere.

69
00:04:52,972 --> 00:04:55,673
Then we added Bettina Marquis,

70
00:04:56,373 --> 00:04:59,334
and she's the perfect example of the kind of animator you want

71
00:04:59,434 --> 00:05:00,434
on a crazy project like this.

72
00:05:00,935 --> 00:05:03,475
And of course, holding it all together is me.

73
00:05:04,636 --> 00:05:06,096
So, with a nice beard there as well.

74
00:05:06,996 --> 00:05:10,397
OK, so we set out a clear mandate of four goals

75
00:05:10,417 --> 00:05:11,418
that we wanted to improve upon.

76
00:05:12,282 --> 00:05:14,723
Realism, control, simplify and variety.

77
00:05:15,944 --> 00:05:20,026
So, with many Ubisoft games leaning more towards realism, we wanted to improve core locomotion

78
00:05:20,286 --> 00:05:22,207
and allow for biomechanically correct human movement.

79
00:05:23,588 --> 00:05:26,389
Ultimately, this is the feel of the character whilst retaining animation quality

80
00:05:26,689 --> 00:05:28,350
and allowing the player to always be in control.

81
00:05:29,911 --> 00:05:31,612
Simplify the way we want to implement data.

82
00:05:31,632 --> 00:05:35,714
This would be allowing animators to focus on animation, creation and rapid iteration,

83
00:05:36,434 --> 00:05:39,255
giving them the ability to test or change the style of their character quickly.

84
00:05:40,913 --> 00:05:44,718
So, and also we need to be able to add high quality variety quickly and easily,

85
00:05:45,038 --> 00:05:49,363
and this isn't just about replacing a walk cycle, we wanted to look at replacing entire core locomotion.

86
00:05:51,105 --> 00:05:51,646
Excuse me.

87
00:05:52,307 --> 00:05:56,853
So, which brings me to what motion matching actually is, albeit a very, very high level description of it.

88
00:05:58,446 --> 00:06:02,450
So we describe a small amount of characteristics on what we want the character to do over a certain amount of time.

89
00:06:02,770 --> 00:06:04,092
Let's just say one second.

90
00:06:04,472 --> 00:06:06,994
We then take into account things like root position and velocity,

91
00:06:07,515 --> 00:06:08,876
the past and present trajectory,

92
00:06:09,377 --> 00:06:12,079
joint positions such as feet and hands and their velocity,

93
00:06:12,520 --> 00:06:14,041
as well as any tags that we may add.

94
00:06:15,383 --> 00:06:18,706
We take all this and basically find an appropriate matching section.

95
00:06:20,994 --> 00:06:23,814
And it would be like in an unstructured library of poses,

96
00:06:24,174 --> 00:06:26,414
effectively meaning we could jump anywhere in a piece of data

97
00:06:26,674 --> 00:06:28,555
and it matched our current pose at the time of input.

98
00:06:29,035 --> 00:06:29,275
Okay?

99
00:06:30,375 --> 00:06:30,775
Stay with me.

100
00:06:31,575 --> 00:06:31,735
So...

101
00:06:33,556 --> 00:06:35,436
Okay, which brings me to our first test.

102
00:06:36,176 --> 00:06:37,516
So I'm going to skip forward a few months,

103
00:06:37,776 --> 00:06:39,957
and with that I wanted to share our very first playable test

104
00:06:40,277 --> 00:06:43,197
that proves that we thought this was something special.

105
00:06:43,237 --> 00:06:44,998
And as Dan said in his presentation,

106
00:06:45,018 --> 00:06:48,018
it was quite hard to demo this live,

107
00:06:48,278 --> 00:06:49,998
so I've pre-recorded a video for you.

108
00:06:53,532 --> 00:06:56,293
So here's some of the first tests that Michael and I worked on for Motion Fields.

109
00:06:56,774 --> 00:07:02,076
This involved me getting into a motion capture suit, running around our studio, walking, jogging, running,

110
00:07:03,396 --> 00:07:10,099
going through various different movements, plant turns, stopping, running around in circles and sneaking.

111
00:07:11,139 --> 00:07:16,941
We felt like this would give us a good, widespread base for everything that we needed.

112
00:07:17,481 --> 00:07:23,183
So this is about as real as a simulation to a human as you can get.

113
00:07:23,823 --> 00:07:27,365
And it's very weird for me sometimes because I can see that this is me.

114
00:07:28,166 --> 00:07:32,688
So, the arrow at the bottom is the player input.

115
00:07:33,208 --> 00:07:41,871
And as you can see, it's relatively responsive with just, I think it was maybe 10 minutes worth of data at the time.

116
00:07:42,011 --> 00:07:43,471
We added the ability to jog.

117
00:07:44,652 --> 00:07:51,718
On the left button, you can see there's a very smooth transition and the camera would very easily move behind you.

118
00:07:52,539 --> 00:07:57,223
And you can see that there would be some quick change of direction in there if it fitted.

119
00:07:59,145 --> 00:08:03,889
So sometimes the plant turns would work, sometimes it wouldn't and that was something we worked on a little bit better.

120
00:08:04,629 --> 00:08:16,212
All this was in one file, so we didn't separate anything out and we felt that if everything was in one file at the time, that we would get a better combination of all the different moves.

121
00:08:16,252 --> 00:08:22,993
So as you can see, if I let go of the left shoulder button, it slows down to a nice walk.

122
00:08:24,333 --> 00:08:30,094
So conversely, there you go, and you can see that I rubbed my nose, which became kind of a running joke on the team.

123
00:08:30,234 --> 00:08:31,455
If I hold the right shoulder button...

124
00:08:32,514 --> 00:08:34,876
Then I quickly move into a sprint.

125
00:08:35,697 --> 00:08:43,384
And the cool thing about this is that it completely recreates the weight shifting from the snaking that we put in there.

126
00:08:43,485 --> 00:08:45,186
Also there was quick changes of direction.

127
00:08:47,048 --> 00:08:49,711
So you could see the quick change from left to right.

128
00:08:49,731 --> 00:08:50,692
And if I wanted to go right...

129
00:08:51,502 --> 00:08:54,786
You could very quickly, if I wanted to double back on myself, I could.

130
00:08:54,806 --> 00:08:58,249
And also, something that we found that was really nice was,

131
00:08:58,730 --> 00:09:03,395
if I was running around in a circle, I could take my finger off the button and...

132
00:09:04,699 --> 00:09:07,041
It would actually slow down and decelerate in a circle.

133
00:09:07,641 --> 00:09:12,065
So, and also what I could do is I could start Sprinty again and I could completely let go of everything.

134
00:09:13,426 --> 00:09:15,888
And you see it would come down to a nice quick stop.

135
00:09:16,169 --> 00:09:19,492
This works really well in an open gym obviously with anything out there.

136
00:09:19,732 --> 00:09:23,996
But then it would probably cause more problems if there was going to be any obstacles in there.

137
00:09:24,036 --> 00:09:26,458
But as a first test we felt that this was really successful.

138
00:09:29,000 --> 00:09:29,180
Okay.

139
00:09:30,483 --> 00:09:36,066
So, after seeing this first test it immediately raised a few questions with the team, a few fears and a lot of assumptions.

140
00:09:36,587 --> 00:09:43,710
We understood that no one had tried to ship a game with this before and we would need to lead the way to convince everyone not only was it worth the investment, but that we were worth following.

141
00:09:44,271 --> 00:09:46,712
But as an animator, it absolutely terrified me.

142
00:09:47,432 --> 00:09:52,995
So, what does this mean for animators to work with this? Can animators work with this? How would we implement it?

143
00:09:55,047 --> 00:09:56,828
What if we can't get access to the MoCap facility?

144
00:09:57,128 --> 00:09:59,828
How do we prototype new ideas if it only works in MoCap?

145
00:10:00,148 --> 00:10:01,729
Exactly how much data will we need?

146
00:10:02,089 --> 00:10:03,829
And would producers absolutely hate me

147
00:10:04,129 --> 00:10:05,270
because of how much it's going to cost them?

148
00:10:07,030 --> 00:10:10,051
So, with our initial success it appeared that all we'd need to do

149
00:10:10,091 --> 00:10:12,491
would be to plug in the data, just leave it at that.

150
00:10:13,052 --> 00:10:14,572
So would animators even be needed anymore?

151
00:10:15,072 --> 00:10:17,753
So, as you can see, this is why I didn't want to tell anyone about it.

152
00:10:19,289 --> 00:10:25,431
Okay, but this brings me to the process. We need to find solutions to some of these problems, so I'm going to break it down.

153
00:10:26,491 --> 00:10:33,253
So first step for us was to create a routine, which we called dance cards. These would be in order so we could capture data and would help us capture more efficiently.

154
00:10:33,933 --> 00:10:38,834
These would have everything we needed to create core locomotion set, and we would repeat for like walk, jog and run.

155
00:10:40,140 --> 00:10:53,451
But it could only really, but it could be really tiring doing everything in one shot, and as one shot could last upwards of four minutes, you would need to remember a lot of moves, you'd need to be extremely fit, which took the actor basically out of character, i.e. me.

156
00:10:56,229 --> 00:10:59,331
So we split up the dance card into smaller files to make them a little bit more manageable.

157
00:10:59,551 --> 00:11:00,852
We broke them into starts and stops.

158
00:11:01,052 --> 00:11:05,156
We broke them into plan and turns, circles, acceleration, deceleration.

159
00:11:05,616 --> 00:11:08,818
And the acceleration, deceleration, we'd have like a longer walk, jog and run cycle.

160
00:11:09,199 --> 00:11:12,461
We'd also have the transitions between all speeds of locomotion.

161
00:11:13,102 --> 00:11:14,803
And the key for me was snaking.

162
00:11:15,804 --> 00:11:17,605
So, let's see.

163
00:11:18,980 --> 00:11:22,442
The goal of these dance cards was to find a way to capture the minimum amount of moves

164
00:11:22,482 --> 00:11:25,764
possible needed to create the maximum amount of coverage to create a basic locomotion set.

165
00:11:26,364 --> 00:11:28,645
We would start off with start, forward and stop.

166
00:11:29,739 --> 00:11:34,542
Then we would basically be moving back, we'd go over the left shoulder, we'd go over the right shoulder,

167
00:11:35,322 --> 00:11:38,404
we'd go 45 degrees right, we'd go 45 degrees left.

168
00:11:39,285 --> 00:11:44,348
We'd try and do everything we could for that core locomotion set, trying to cover us, basically like scribbling on a piece of paper.

169
00:11:46,089 --> 00:11:49,131
Then we'd do large and small circles to cover different types of banking,

170
00:11:50,351 --> 00:11:54,254
longer circles to give more consistent loop coverage for like walk, jog and run,

171
00:11:54,714 --> 00:11:56,915
and then snaking to handle smooth changes in direction,

172
00:11:57,696 --> 00:11:59,057
which goes back to the thing I was talking about before.

173
00:12:02,305 --> 00:12:05,746
So, once we had all this data back, the files would look a little something like this.

174
00:12:06,626 --> 00:12:08,686
And you've got this example here of circling.

175
00:12:09,486 --> 00:12:13,787
We would then export these FBX files straight into the engine without any touch-up, just to see what happened.

176
00:12:14,948 --> 00:12:15,328
So, you ready?

177
00:12:19,169 --> 00:12:22,810
OK. So, here is the result.

178
00:12:23,210 --> 00:12:27,491
Completely untouched, in-engine, 100% player-controlled and responsive.

179
00:12:28,491 --> 00:12:30,451
Sure, it was an open gym and nothing in the way.

180
00:12:31,242 --> 00:12:35,584
But we just created a better looking and feeling locomotion system in four hours.

181
00:12:36,325 --> 00:12:41,147
That would take the best teams in Ubisoft at least three months. Awesome and terrifying in one video.

182
00:12:43,228 --> 00:12:44,469
So as you can see, there you go.

183
00:12:46,287 --> 00:12:52,454
So we'd have things like the snaking there, and you can see the biomechanically correct human movement.

184
00:12:53,535 --> 00:12:58,180
And there's no IK yet on this, so foot sliding is still minimal at this point.

185
00:12:58,821 --> 00:13:02,785
We get proper weight transfer, acceleration, deceleration, the whole nine yards.

186
00:13:03,386 --> 00:13:05,228
And this was also using the split dance card method.

187
00:13:06,356 --> 00:13:08,420
We couldn't always be sure it would pick the move that we wanted,

188
00:13:08,881 --> 00:13:13,391
and the system tended to pick a move that was always the best match for the pose it was at at the current time of input,

189
00:13:13,812 --> 00:13:16,899
providing some happy accidents, but equally unpredictable results.

190
00:13:21,516 --> 00:13:25,619
So we made a few changes to the routine and adapted it to work with what we call second-person movement.

191
00:13:26,059 --> 00:13:30,882
We wanted to know how well the system would handle more complicated locomotions such as strafing.

192
00:13:31,402 --> 00:13:36,986
This was much the same dance card as before, except we would face the same direction during the capture as if the camera was always behind us,

193
00:13:37,827 --> 00:13:40,328
which resulted in complex foot crossovers and torso twists.

194
00:13:42,890 --> 00:13:46,172
So again, we processed the data, exported it, giving us this.

195
00:13:47,517 --> 00:13:51,699
The player movement was connected directly to the camera, placement controlled by the

196
00:13:51,719 --> 00:13:57,281
player. We had eliminated the need for complex foot crossover transitions and added biomechanically

197
00:13:57,321 --> 00:14:01,463
correct weight transfer when changing direction on top of what was connected to the previous

198
00:14:01,503 --> 00:14:05,484
dance card data to give us a really interesting transition between second person and third

199
00:14:05,524 --> 00:14:10,246
person movement. Just let it run for a bit.

200
00:14:19,650 --> 00:14:22,473
So we approve that adding raw data could get really decent results.

201
00:14:23,654 --> 00:14:26,957
But for us moving forward, we'd need to learn how to manipulate this data.

202
00:14:28,218 --> 00:14:31,941
Which brings me to some of the most frequently asked questions by animators about motion matching.

203
00:14:32,541 --> 00:14:36,685
So, how can animators control quality versus responsiveness?

204
00:14:37,486 --> 00:14:39,347
Can this be integrated into our existing system?

205
00:14:40,068 --> 00:14:41,469
And how can animators work with this system?

206
00:14:43,826 --> 00:14:47,388
We wanted to push the boundaries of what was possible, so we decided to try and add different

207
00:14:47,408 --> 00:14:51,290
types of locomotion using human as a base, but with varying degrees of success.

208
00:14:56,874 --> 00:14:59,315
So, here we have what we called ApeNav.

209
00:14:59,936 --> 00:15:02,577
We thought it was good enough for Andy Serkis, it's good enough for us.

210
00:15:04,940 --> 00:15:08,602
The interesting thing about this particular type of locomotion is that we actually attempted to motion capture it.

211
00:15:09,523 --> 00:15:12,725
But it was so complicated and the data was so absolutely useless

212
00:15:13,466 --> 00:15:16,448
that we used this as an opportunity to deconstruct how the system worked.

213
00:15:16,828 --> 00:15:18,630
So it turns out Andy Serkis does actually know what he's doing.

214
00:15:19,891 --> 00:15:21,572
So we brute force keyframed the dance card,

215
00:15:21,652 --> 00:15:24,434
stitching together keyframed clips that would match the dance card format.

216
00:15:25,034 --> 00:15:26,415
Then we exposed the debug settings.

217
00:15:26,435 --> 00:15:28,977
Let's see. Oh, sorry. Here we go.

218
00:15:29,237 --> 00:15:30,458
Then we exposed the debug settings.

219
00:15:32,148 --> 00:15:36,631
So we could see frames and which files that the system was using to adapt accordingly.

220
00:15:37,111 --> 00:15:40,594
It was a bit long-winded, but it was like taking something apart to understand how it works

221
00:15:41,154 --> 00:15:43,916
and then putting it back together again, which gave us an invaluable insight.

222
00:15:48,659 --> 00:15:49,600
So that's a bit slow to catch up.

223
00:15:53,374 --> 00:15:56,697
For each move to be read by the system, we would need to add about a second to either

224
00:15:56,757 --> 00:15:58,418
side of the move.

225
00:15:58,598 --> 00:16:01,801
That way the trajectories of each move could be accurately seen by the prediction model.

226
00:16:03,062 --> 00:16:05,203
So this is what a start forward and back would look like.

227
00:16:06,124 --> 00:16:09,667
By adding a longer pause at the end of the move, it would highlight that this was a stop,

228
00:16:10,067 --> 00:16:12,529
even though we wouldn't see that part of the animation.

229
00:16:13,759 --> 00:16:16,901
So removing this pause highlighted the left and right plant turns.

230
00:16:17,501 --> 00:16:20,182
So, so far nothing too crazy, this is kind of standard stuff.

231
00:16:21,483 --> 00:16:26,586
Then once we knew how the system was being used, basically the basics,

232
00:16:27,366 --> 00:16:29,227
we could add more natural, complicated motions.

233
00:16:29,287 --> 00:16:33,029
So we had the key to knowing where to polish, a character.

234
00:16:33,669 --> 00:16:36,251
This also meant that we could start adding all manner of interesting motions

235
00:16:36,271 --> 00:16:39,152
depending on how we read the prediction model

236
00:16:39,232 --> 00:16:40,513
and fill in the blanks of player control.

237
00:16:43,007 --> 00:16:45,027
So this basically gave us a good insight.

238
00:16:45,107 --> 00:16:45,927
Oh, frame rate's terrible.

239
00:16:46,627 --> 00:16:48,408
This gave us an insight of how the system worked.

240
00:16:55,329 --> 00:16:55,609
Okay.

241
00:16:56,090 --> 00:16:58,750
So another way to control this was by simple tagging

242
00:16:59,890 --> 00:17:00,971
of interested areas or waste.

243
00:17:01,571 --> 00:17:03,691
It meant that we could highlight areas we wanted to favour

244
00:17:03,791 --> 00:17:06,612
in files such as idles, garbage, or interesting walks

245
00:17:06,972 --> 00:17:07,812
depending on our needs.

246
00:17:08,352 --> 00:17:11,013
This would mean that we could get more use out of a single file

247
00:17:11,093 --> 00:17:12,493
and we could add extra elements of character.

248
00:17:13,142 --> 00:17:14,823
Like, that's it.

249
00:17:15,083 --> 00:17:19,646
This added an element of predictability to a system that can be at large, largely at times, unpredictable.

250
00:17:20,266 --> 00:17:24,408
To tidy this up, we would need to implement a simple IK solution to remove foot sliding,

251
00:17:25,088 --> 00:17:26,529
but the results were a step in the right direction.

252
00:17:26,549 --> 00:17:29,711
I myself don't have many videos in one slide.

253
00:17:30,411 --> 00:17:31,272
Okay, so...

254
00:17:34,779 --> 00:17:38,460
So I owe a huge debt of thanks to Michael Butner for the next two technical slides,

255
00:17:38,520 --> 00:17:41,001
so you're going to have to excuse me if I get this wrong.

256
00:17:41,701 --> 00:17:46,603
So, motion shaders were created as a node graph system to allow animators and designers

257
00:17:46,923 --> 00:17:48,743
more control over how the moves would be seen.

258
00:17:49,304 --> 00:17:53,045
It was a way to define the cost functions of how well we wanted to match current, future

259
00:17:53,085 --> 00:17:56,566
and past trajectories of limbs or root nodes when we were looking for a match.

260
00:17:57,226 --> 00:18:00,829
So essentially we could say that a position was 10% important,

261
00:18:01,189 --> 00:18:03,511
then when the velocity was 30% important,

262
00:18:03,851 --> 00:18:06,032
and hands or feet would be 80% important.

263
00:18:06,513 --> 00:18:09,155
Meaning that we could change the look and feel of the locomotion

264
00:18:09,615 --> 00:18:11,236
as it would use the available poses differently.

265
00:18:12,837 --> 00:18:14,779
This could be in the form of sliders between 0 and 1,

266
00:18:14,939 --> 00:18:17,341
and it would be used to say what was more important.

267
00:18:17,741 --> 00:18:19,742
So respecting the positions in the animation,

268
00:18:20,203 --> 00:18:21,263
or favor players' input.

269
00:18:22,384 --> 00:18:24,946
This could be created using the same animation pool as a base.

270
00:18:26,553 --> 00:18:32,039
We'd also look to adjust the amount of dampening on input of stick resulting in longer or shorter acceleration or deceleration.

271
00:18:32,059 --> 00:18:39,767
It means we could make something feel like a car if we respected the animation more, or we could make it respond immediately if we respected the input more.

272
00:18:43,103 --> 00:18:53,906
OK, to bridge the gap between this system and existing systems, we added the use of motion matching as a traditional animation node, which can sit inside of a state machine.

273
00:18:55,267 --> 00:19:02,629
So even a motion matching node, which would become active, it would understand what your current pose is and find a relevant match to connect to.

274
00:19:03,889 --> 00:19:11,612
Meaning it would be great for things like transitions in and out of existing systems, as you wouldn't need to set up any complicated rules, and you could just use a motion matching node instead.

275
00:19:13,157 --> 00:19:17,100
And it can also be used as a straight-up replacement for your existing movement system.

276
00:19:17,480 --> 00:19:20,182
We've only really started to scratch the surface of what this is capable of,

277
00:19:20,242 --> 00:19:21,703
but we really want to push this a little bit further.

278
00:19:23,664 --> 00:19:25,145
So what else can be done with motion matching?

279
00:19:29,408 --> 00:19:32,630
Using the dance card, we thought it would be fun to try out a few different types of locomotion.

280
00:19:33,110 --> 00:19:35,352
So we spent some time capturing off-balance movements.

281
00:19:36,452 --> 00:19:39,114
When we added this to the system, it gave the appearance of pretty successful

282
00:19:39,154 --> 00:19:42,516
off-balance stumble movement, where we would be in complete player control.

283
00:19:43,359 --> 00:19:47,662
So we could be pushing the character around everywhere and it would be completely responsive.

284
00:19:48,222 --> 00:19:51,765
You can imagine this being used for things like persistent injury locomotion,

285
00:19:52,825 --> 00:19:56,468
where we could manipulate the inputs to define just how responsive a player was.

286
00:20:08,328 --> 00:20:14,030
So using the dance card, sorry, Michael and I thought that it would be fun to try out something other than locomotion.

287
00:20:14,390 --> 00:20:21,454
So we captured a bunch of impact motions involving being pushed in all sorts of different types of direction with all different types of force.

288
00:20:23,034 --> 00:20:28,757
As we didn't know how well this was going to work, we attempted a few different methods to see what kind of results we might get.

289
00:20:29,217 --> 00:20:33,339
Here you can see some of the team being just a little bit overzealous with me.

290
00:20:33,379 --> 00:20:35,000
I think I might have pissed a couple of people off that day.

291
00:20:37,390 --> 00:20:39,931
Yeah, I got him back at the end, it's fine.

292
00:20:43,053 --> 00:20:45,735
But with the system being pose-based,

293
00:20:45,855 --> 00:20:47,356
if we could create an impact pose

294
00:20:47,656 --> 00:20:49,957
using a combination of physics, momentum and motion matching,

295
00:20:50,498 --> 00:20:53,480
we could use that pose to connect to a sort of impact animation.

296
00:20:54,623 --> 00:20:58,627
This would give us a seamless transition from locomotion to the sort of impact locomotion

297
00:20:58,968 --> 00:21:00,830
that could actually be based on physics impulses.

298
00:21:02,231 --> 00:21:06,055
As this would also be a part of motion matching, we used the poses in this range of motion

299
00:21:06,095 --> 00:21:07,777
to connect directly back to locomotion.

300
00:21:08,217 --> 00:21:12,741
This would effectively close the loop between animation and physics, and back to animation,

301
00:21:13,202 --> 00:21:15,724
whilst retaining complete player control of the character.

302
00:21:17,208 --> 00:21:20,329
So this is something we were kind of proud of, kind of cool.

303
00:21:20,369 --> 00:21:24,730
So as you can see here, the results were pretty successful.

304
00:21:25,550 --> 00:21:28,731
And again, it's something we plan on exploring a lot further.

305
00:21:30,651 --> 00:21:32,972
OK, which brings me to some of our early conclusions.

306
00:21:35,212 --> 00:21:36,992
So remember our initial fears and assumptions.

307
00:21:38,213 --> 00:21:39,273
How will animators work with this?

308
00:21:39,653 --> 00:21:41,253
Well, for me, this is a mentality shift.

309
00:21:41,633 --> 00:21:43,454
This is potentially the biggest transition

310
00:21:43,494 --> 00:21:44,774
since the introduction of motion capture

311
00:21:44,794 --> 00:21:45,734
to the pipeline in the first place.

312
00:21:47,899 --> 00:21:51,060
Will this only work with motion capture?

313
00:21:51,721 --> 00:21:57,883
Keyframing within the structure, small amounts of data to set up basics and then fill them out from there.

314
00:21:57,923 --> 00:22:01,084
So you can use keyframing with this system, you just have to use a lot of keyframing.

315
00:22:03,960 --> 00:22:05,821
Can you just plug in the data for the finished result?

316
00:22:06,241 --> 00:22:13,587
Well, you can plug in data for a finished result, but originally I felt that this would yield the best results by just throwing all the data at it.

317
00:22:14,368 --> 00:22:16,189
But it seemed to muddy the waters a little bit more.

318
00:22:16,750 --> 00:22:17,931
So I have a little analogy.

319
00:22:19,952 --> 00:22:21,814
Okay, so take Chris Pratt.

320
00:22:22,454 --> 00:22:25,817
When he was in Parks and Recreation, all he wants is burgers.

321
00:22:28,522 --> 00:22:29,684
He clearly loves burgers.

322
00:22:30,485 --> 00:22:33,268
And why not? In-N-Out burgers are particularly delicious.

323
00:22:34,389 --> 00:22:38,434
Um, let's see. Oh, come on. Timing. There we go. Perfect. Okay.

324
00:22:39,055 --> 00:22:43,240
Um, so, he needs to work out. He needs to start eating right.

325
00:22:44,800 --> 00:22:49,122
Then he became nice and lean and starts getting roles in films like Guardians of the Galaxy, Jurassic World.

326
00:22:50,302 --> 00:22:54,543
But what does this have to do with motion matching except for me wanting to show a bunch of pictures of Chris Peck with his top off?

327
00:22:54,563 --> 00:22:57,104
Which is weird. But anyway.

328
00:22:59,404 --> 00:23:05,246
Well, basically we found the system performed better if we fed it the right data as opposed to all the data.

329
00:23:05,906 --> 00:23:12,768
If we were efficient with the type of data that we fed into it, it became a lot more manageable, it would perform a lot better, and would actually look a lot better as a result.

330
00:23:15,726 --> 00:23:18,787
There you go. So, which brings us to some of our successes.

331
00:23:21,448 --> 00:23:27,751
Minimal setup is required. Once initial setup is created, it's possible to just throw entire locomotion at the system and just see what happens.

332
00:23:29,616 --> 00:23:39,540
We get higher quality motion, and we can get to see actual character in the actor, so you can imagine it being used for things like sports games, or if there was someone that had particular nuances.

333
00:23:39,580 --> 00:23:46,783
I mean, you can see from the demo earlier on, and if anyone knows me, I walk like that, it looks like me, I scratch my nose, and yeah, it's weird.

334
00:23:47,103 --> 00:23:53,826
But anyway, by using the idea of dance cards, we were able to easily break down all the moves we needed in the smallest amount of time and space.

335
00:23:54,770 --> 00:23:59,454
And it was super simple to add a wide variety of locomotion styles to the system.

336
00:24:01,415 --> 00:24:02,456
But it wasn't all successes.

337
00:24:02,876 --> 00:24:05,038
There was a steep learning curve to working with motion matching.

338
00:24:05,418 --> 00:24:08,820
So it's only fair that I highlight some of these areas that we feel like we can improve upon.

339
00:24:10,582 --> 00:24:15,426
So, editing data can be tough. The system gets used to about 70% really, really quickly.

340
00:24:15,846 --> 00:24:20,510
But the problem is that last 30%, which is kind of where Dan's tool comes in, which is cool.

341
00:24:22,071 --> 00:24:26,014
It can be tough to manage. We found that sometimes removing the data from a file,

342
00:24:26,575 --> 00:24:29,156
even if we felt it wasn't being used, would actually cause more problems.

343
00:24:31,772 --> 00:24:33,533
The system was very data heavy.

344
00:24:34,674 --> 00:24:37,737
We would basically put large amounts of wasted data into the system,

345
00:24:38,117 --> 00:24:40,660
but then of course the cost and availability of motion capture facilities.

346
00:24:41,680 --> 00:24:43,862
And currently the system is restricted to human-like rigs.

347
00:24:44,483 --> 00:24:49,788
Everything we had to do with this system was on the same rig,

348
00:24:50,328 --> 00:24:52,830
even when we did different types of locomotion.

349
00:24:53,991 --> 00:24:56,133
And as well, all the animation on there was full body,

350
00:24:56,153 --> 00:24:57,835
so we didn't use anything like layers or anything, yeah.

351
00:24:59,547 --> 00:25:02,550
It was originally quite difficult for designers and animators to work together using motion

352
00:25:02,590 --> 00:25:07,674
matching, as design would alter the data, the needs of precision, and alter the numbers

353
00:25:07,694 --> 00:25:09,836
that we used to actually see the poses in the animation.

354
00:25:10,196 --> 00:25:13,599
This would result in really undesired results and would actually break the animation.

355
00:25:14,579 --> 00:25:16,541
For some reason, mirroring didn't work.

356
00:25:16,581 --> 00:25:20,584
We thought that we'd try that, but again, it seemed to muddy the waters more than it

357
00:25:20,604 --> 00:25:21,165
seemed to fix it.

358
00:25:23,168 --> 00:25:25,134
So we felt like it needed a clear choice.

359
00:25:25,675 --> 00:25:29,306
And currently, as I said, we've had a few tests.

360
00:25:29,346 --> 00:25:31,311
But our main focus at the moment has been locomotion.

361
00:25:33,983 --> 00:25:36,345
Which brings me to the next steps, at least as we see it.

362
00:25:36,885 --> 00:25:39,267
So far, we've only really used this base for locomotion.

363
00:25:39,547 --> 00:25:41,649
We've touched on a few of the systems, such as traversal,

364
00:25:41,709 --> 00:25:43,150
cover, and various AI.

365
00:25:43,370 --> 00:25:44,871
But there's plenty more work for us to do here.

366
00:25:45,392 --> 00:25:47,653
We would also like to make improvements to a hybrid system,

367
00:25:47,733 --> 00:25:49,835
more to kind of stop animators from freaking out

368
00:25:49,875 --> 00:25:50,896
and to kind of bridge the gap.

369
00:25:52,517 --> 00:25:55,479
But it's to help bridge the gap, and we try and add things

370
00:25:55,519 --> 00:25:57,941
like layering, which is also something

371
00:25:57,961 --> 00:25:58,822
that we've been looking at.

372
00:25:59,182 --> 00:26:01,504
So it wouldn't be completely reliant on four-body motions,

373
00:26:01,704 --> 00:26:02,785
but this is something early stages.

374
00:26:03,953 --> 00:26:07,456
With a greater understanding of what the system is reading, we can make improvements to the dance cards,

375
00:26:08,397 --> 00:26:12,700
splitting files up, making them easier to manage and allowing us to get cleaner, more readable data.

376
00:26:13,420 --> 00:26:18,784
If we have looked into improving the routine, sorry, we've also looked at improving the routine,

377
00:26:19,185 --> 00:26:20,746
so we get better data from that as well.

378
00:26:21,567 --> 00:26:26,050
And to get more from the data, we need to combine it with other tech being developed at Ubisoft and their engines.

379
00:26:26,450 --> 00:26:32,255
Things like Alex Bresnik's IK rig, which I'll talk about in a second, and of course Dan's automation tool.

380
00:26:34,138 --> 00:26:38,921
So, for more information on some of the really cool animation innovations that are happening

381
00:26:39,021 --> 00:26:43,204
at Ubisoft as a group, I suggest you check out these two talks in particular on Wednesday

382
00:26:43,224 --> 00:26:47,927
and Thursday. Ubisoft has been really good with letting us talk about stuff like this

383
00:26:47,987 --> 00:26:53,411
so early on, and who knows, after today they may not let me do it again. So, get it as

384
00:26:53,451 --> 00:27:01,276
you can. But, for us, it's super exciting, and it really is just the beginning. And thank

385
00:27:01,296 --> 00:27:02,056
you, that's my talk.

386
00:27:13,298 --> 00:27:16,321
I don't know whether there's any time for questions or anything.

387
00:27:16,921 --> 00:27:18,483
Three minutes. Oh yeah, I got it under.

388
00:27:20,604 --> 00:27:22,246
Okay, anyone have a question?

389
00:27:23,847 --> 00:27:27,971
If you do, please approach the mic that is there and I will do my best to answer.

390
00:27:29,732 --> 00:27:32,274
You mentioned mixing it with state machines.

391
00:27:32,354 --> 00:27:37,318
It's all full body. Can you get it so you can do upper body stuff like shooter stuff?

392
00:27:38,599 --> 00:27:40,241
That is something we could definitely look at.

393
00:27:41,001 --> 00:27:41,902
And it's something that...

394
00:27:43,359 --> 00:27:48,421
I think it would work very well for a system like this, but we haven't done much on that

395
00:27:48,861 --> 00:27:49,262
just yet.

396
00:27:50,102 --> 00:27:54,724
I know it's really early on, but do you think this would ever make it to like third party

397
00:27:54,744 --> 00:27:55,164
software?

398
00:27:55,764 --> 00:27:56,205
Into what, sorry?

399
00:27:56,465 --> 00:27:59,466
Like third party software where everybody else could use it someday?

400
00:28:00,006 --> 00:28:04,708
I mean, it's certainly, if Ubisoft kind of feels it's successful, then who knows what

401
00:28:04,728 --> 00:28:05,368
they might do with that.

402
00:28:05,488 --> 00:28:08,730
But that's not really, it's so early on that I don't think we'd know.

403
00:28:08,750 --> 00:28:09,570
Right.

404
00:28:09,910 --> 00:28:10,411
Thanks, man.

405
00:28:10,491 --> 00:28:10,931
No problem.

406
00:28:12,371 --> 00:28:12,731
Anyone else?

407
00:28:14,592 --> 00:28:15,254
Please approach the mic.

408
00:28:16,138 --> 00:28:17,122
Or say the question and I'll repeat it.

409
00:28:21,997 --> 00:28:31,521
Hey, what's up? How long was the whole R&D process and overall time that it took for you guys from start to the current stage?

410
00:28:31,902 --> 00:28:38,685
Well, one of the programmers that was working on this, he kind of had it as a seed in the back of his mind for a while.

411
00:28:38,985 --> 00:28:46,049
And then when he came to Ubisoft, it was kind of an opportunity for, we gave him some time for some R&D, and then he came up with this idea.

412
00:28:47,129 --> 00:28:49,750
So to get the initial demo that you first saw,

413
00:28:51,030 --> 00:28:53,411
we had that working in I think it was about two months,

414
00:28:54,691 --> 00:28:55,691
the initial stick figure demo.

415
00:28:55,911 --> 00:28:57,452
And then obviously it was just kind of iteration,

416
00:28:57,472 --> 00:29:00,272
getting into various different engines and then trying it in there.

417
00:29:01,093 --> 00:29:06,154
So it was actually quick to get to a point where we got the data playable,

418
00:29:06,714 --> 00:29:10,515
but then working out how to work with it was the hard thing after that.

419
00:29:10,615 --> 00:29:12,835
So we had to work out the process and what we were going to do with it

420
00:29:12,895 --> 00:29:14,956
to get it into a state that would be shippable.

421
00:29:18,693 --> 00:29:23,335
You said you thought you had problems with abandoning data you thought was useless.

422
00:29:23,975 --> 00:29:26,276
Could you go into what you had happen?

423
00:29:26,296 --> 00:29:32,819
Well, it would be the thing is when you do the dance card, you have things like if you're walking, you'll repeat the same walking action.

424
00:29:33,708 --> 00:29:36,908
like in between each transition because what it's really looking for is those transitions

425
00:29:37,469 --> 00:29:42,089
and then it will find, and then when it comes out of one of those transitions it will go into a walk.

426
00:29:42,630 --> 00:29:46,430
So what was happening is like my walk, so unless I change my walk every single time

427
00:29:46,990 --> 00:29:51,571
then it would be the same walk. So we've kind of find that if we had eight transitions in there

428
00:29:51,591 --> 00:29:56,992
you're going to have eight or nine similar walks that we didn't really need. So when we added the

429
00:29:57,032 --> 00:30:01,633
tagging we kind of found ways to not use that which meant that at runtime it wouldn't see that data

430
00:30:02,053 --> 00:30:03,293
which meant that it would process a lot easier.

431
00:30:06,450 --> 00:30:13,215
I think that's it. Thank you very much.

