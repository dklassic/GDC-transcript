1
00:00:07,338 --> 00:00:11,880
Hello, everyone. My name is David, and I'm going to talk to you about game server performance on The Division 2.

2
00:00:13,781 --> 00:00:15,001
A little bit of info about me.

3
00:00:15,701 --> 00:00:18,042
I've worked at Massive for about 12 years now,

4
00:00:18,903 --> 00:00:24,905
on a bunch of different games that you can see here, from World in Conflict to The Division 2, most recently.

5
00:00:26,305 --> 00:00:29,426
I've had a couple of different roles, but mainly in gameplay and AI.

6
00:00:31,048 --> 00:00:33,809
But the last couple of years have really been focusing on server performance,

7
00:00:34,350 --> 00:00:36,771
both the workflows that we employ on the team

8
00:00:37,031 --> 00:00:39,313
and the optimization parts themselves.

9
00:00:41,634 --> 00:00:45,777
So the contents of today's presentation will be focused around

10
00:00:45,877 --> 00:00:48,178
how we maintain a large player base on our servers.

11
00:00:48,878 --> 00:00:50,860
And I'll be going through this in three main parts.

12
00:00:51,320 --> 00:00:54,682
One is the code architecture that's required to maintain this.

13
00:00:55,801 --> 00:01:01,305
And the second part will be about the tools and processes that we have to be able to debug

14
00:01:01,365 --> 00:01:06,508
performance problems and monitor performance over time. And the third part will be some examples of

15
00:01:06,668 --> 00:01:10,931
problems that we ran into and the solutions that we found. And then we'll round off with

16
00:01:10,971 --> 00:01:16,875
some conclusions and takeaways as well. So let's start with a video to show you a little bit of

17
00:01:17,215 --> 00:01:19,017
the game and what it's basically about.

18
00:01:19,037 --> 00:01:23,880
...reports that the peacekeepers are trying to take a Rikers control point.

19
00:01:54,143 --> 00:01:54,684
So there you go.

20
00:01:55,764 --> 00:01:58,825
The Division 2 is a third-person shooter, as you can see.

21
00:02:00,026 --> 00:02:02,267
It has a typical client-server architecture,

22
00:02:02,967 --> 00:02:04,567
and it has an authoritative server.

23
00:02:04,667 --> 00:02:07,869
So we use the server to validate all kinds of player actions,

24
00:02:08,509 --> 00:02:12,170
like shooting and movement, using animations, physics,

25
00:02:12,490 --> 00:02:13,051
and mav mesh.

26
00:02:14,671 --> 00:02:16,432
The servers are fairly large scale.

27
00:02:16,512 --> 00:02:18,213
It's about 1,000 players per server.

28
00:02:18,313 --> 00:02:21,374
And on that kind of server, we have around 20,000 NPCs

29
00:02:21,434 --> 00:02:22,094
at any given time.

30
00:02:24,100 --> 00:02:29,042
It runs at 20 FPS, except in some dedicated PvP modes where we run at 30 FPS.

31
00:02:31,123 --> 00:02:35,485
So we took a decision early on to maximize the number of players that we have on any given server.

32
00:02:36,045 --> 00:02:37,406
And this helps us in a couple of ways.

33
00:02:37,986 --> 00:02:43,969
One of them is that we wanted this kind of seamless transitions between your own personal part of the world

34
00:02:44,089 --> 00:02:48,351
into the more social spaces, like the dark zone and the hubs, where you can meet other players.

35
00:02:49,443 --> 00:02:54,127
And having a lot of players in that case also helps us populate those areas in a good way

36
00:02:54,187 --> 00:02:57,749
so that whenever you go into a dark zone, for example, it's usually filled up with people

37
00:02:57,809 --> 00:02:58,950
or should be most of the time.

38
00:03:00,752 --> 00:03:04,455
And lastly, it also helps us on the hardware side because we don't really have to do any

39
00:03:04,495 --> 00:03:06,797
streaming from disk once we've started up the server.

40
00:03:09,879 --> 00:03:13,782
So what does the game server really do if we're focusing specifically on the server?

41
00:03:14,924 --> 00:03:20,088
Well the server updates worlds and the worlds have a bunch of stuff in them like all the

42
00:03:20,208 --> 00:03:26,393
AI and all the players and all the different kinds of systems like the mission script system

43
00:03:26,533 --> 00:03:30,556
and the skill script systems which are used to implement things like player skills and

44
00:03:30,576 --> 00:03:30,816
so on.

45
00:03:32,051 --> 00:03:34,812
So each world is updated once per frame,

46
00:03:35,372 --> 00:03:37,333
and runs through updates of all those systems.

47
00:03:38,093 --> 00:03:40,754
And each single update is also single threaded.

48
00:03:40,934 --> 00:03:43,155
So each world update is single threaded,

49
00:03:43,195 --> 00:03:45,356
and it doesn't have any cross references

50
00:03:45,396 --> 00:03:46,716
to other worlds during the updates.

51
00:03:47,517 --> 00:03:50,058
And since we have at least one world per player,

52
00:03:50,418 --> 00:03:53,199
this means we have a thousand worlds or more

53
00:03:53,219 --> 00:03:53,859
at any given time.

54
00:03:53,899 --> 00:03:55,460
So this makes it a really good candidate

55
00:03:55,520 --> 00:03:57,560
for sort of parallelizing process.

56
00:04:01,955 --> 00:04:03,355
So let's take a look at the frame structure.

57
00:04:04,556 --> 00:04:07,838
The way we structure the frame is that in the beginning of the frame,

58
00:04:07,878 --> 00:04:10,019
we have this small non-parallelized segment.

59
00:04:10,599 --> 00:04:14,201
And after that, we have tried to parallelize world updates

60
00:04:14,241 --> 00:04:15,502
as much as we can, basically.

61
00:04:16,082 --> 00:04:17,263
And then at the end of the frame,

62
00:04:17,303 --> 00:04:19,984
we have a small non-parallelized segment again.

63
00:04:21,425 --> 00:04:23,626
So we have two types of tasks,

64
00:04:23,706 --> 00:04:26,308
mainly what we call short tasks and long tasks.

65
00:04:26,708 --> 00:04:29,870
And the short tasks are what we use to update the worlds.

66
00:04:31,472 --> 00:04:32,753
They run on high priority threads.

67
00:04:33,253 --> 00:04:35,953
The long tasks are used to update more background kind

68
00:04:35,973 --> 00:04:38,574
of data like caching save game data to disk

69
00:04:38,914 --> 00:04:41,174
and instancing nav meshes and so on.

70
00:04:42,155 --> 00:04:44,455
And also deleting worlds if a player leaves the server

71
00:04:44,495 --> 00:04:46,676
and creating new worlds when a player joins the server.

72
00:04:49,656 --> 00:04:51,697
We're looking at this, you can see here that in the middle

73
00:04:52,517 --> 00:04:54,698
we have short tasks doing all these world updates.

74
00:04:54,738 --> 00:04:57,258
And we really want to parallelize this as much as possible

75
00:04:57,738 --> 00:05:00,259
to get the most performance out of the system.

76
00:05:04,155 --> 00:05:07,738
So how do we distribute this work on a typical machine?

77
00:05:08,959 --> 00:05:12,662
On a 40-core server, we would use perhaps one or two cores

78
00:05:12,842 --> 00:05:14,303
reserved for the operating system.

79
00:05:15,144 --> 00:05:17,726
And the remaining 38 cores would be available to all threads.

80
00:05:17,986 --> 00:05:20,648
So we don't use any thread affinities to set any limitations here.

81
00:05:21,008 --> 00:05:22,810
We just let the scheduler do the work for us.

82
00:05:24,288 --> 00:05:28,229
And on such a 40 core machine, we would spawn 36 short task threads,

83
00:05:28,549 --> 00:05:30,989
which take care of this world updates that I mentioned before.

84
00:05:31,609 --> 00:05:35,190
And this leaves us with about two cores worth of CPU

85
00:05:35,350 --> 00:05:37,491
to run long tasks as well.

86
00:05:39,891 --> 00:05:45,393
So that should give you a rough idea of how we do the CPU work, basically.

87
00:05:45,413 --> 00:05:49,053
So let's take a look at the actual tools and the pipeline we use for this.

88
00:05:52,239 --> 00:05:55,961
So on a high level, we want programmers to always have

89
00:05:56,601 --> 00:05:58,922
all kinds of performance data available to them

90
00:05:58,982 --> 00:06:00,602
for debugging server performance issues.

91
00:06:02,203 --> 00:06:04,104
The data generation, however, in collection

92
00:06:04,484 --> 00:06:06,044
can vary a little bit with context.

93
00:06:06,104 --> 00:06:07,765
So for example, if it's the live environment,

94
00:06:07,825 --> 00:06:09,966
or if it's the production environment.

95
00:06:10,746 --> 00:06:12,687
And this is something we're looking into fixing further.

96
00:06:13,447 --> 00:06:16,368
So Björn Törnqvist is also going to do a talk here at GDC about

97
00:06:17,491 --> 00:06:20,072
unifying this, we always have the same kind of environment,

98
00:06:20,112 --> 00:06:22,712
both in development and when we go live.

99
00:06:23,332 --> 00:06:24,652
So this can be further improved.

100
00:06:24,752 --> 00:06:27,193
But at least for Division 2,

101
00:06:27,453 --> 00:06:29,894
we made sure that all the data is always available

102
00:06:29,954 --> 00:06:32,734
for debugging server performance issues.

103
00:06:35,135 --> 00:06:37,055
So let's start with a tool we call Grafana.

104
00:06:37,535 --> 00:06:40,876
This is basically a graph viewer on top of a database.

105
00:06:41,216 --> 00:06:44,737
And what we do on the game server is that we send…

106
00:06:45,423 --> 00:06:48,205
data every frame from the game server to this database.

107
00:06:50,026 --> 00:06:51,388
And due to its high-level nature,

108
00:06:51,448 --> 00:06:53,950
this becomes the point of entry for most investigations

109
00:06:54,010 --> 00:06:54,910
into performance issues.

110
00:06:58,193 --> 00:07:00,975
We keep tracking this for all servers at all times as well.

111
00:07:01,475 --> 00:07:04,778
So this data is basically always available for programmers to look at.

112
00:07:06,439 --> 00:07:08,000
The second tool we're going to talk a bit about

113
00:07:08,281 --> 00:07:09,882
is the Snowdrop Profiler.

114
00:07:11,583 --> 00:07:13,685
The Snowdrop Profiler basically has two parts.

115
00:07:14,880 --> 00:07:17,381
It has the runtime component, which constantly records

116
00:07:17,601 --> 00:07:19,122
profiler events and performance data,

117
00:07:19,542 --> 00:07:23,084
and it has a UI part, which you can see here in the picture.

118
00:07:23,404 --> 00:07:25,545
And the UI part can be used either directly live

119
00:07:25,805 --> 00:07:28,807
as we are debugging a running executable

120
00:07:28,947 --> 00:07:31,248
or debugging the process for performance,

121
00:07:31,848 --> 00:07:34,469
or it can be used as a offline tool

122
00:07:34,489 --> 00:07:36,670
where you load up old performance data

123
00:07:36,690 --> 00:07:38,571
that you've saved to disk in some previous run.

124
00:07:39,152 --> 00:07:40,272
We use it for both cases,

125
00:07:40,332 --> 00:07:43,594
but it's quite nice to have that sort of live debug ability.

126
00:07:44,515 --> 00:07:46,316
If you want to tweak some values, for example,

127
00:07:46,376 --> 00:07:49,217
while you're running, you can tweak a little bit

128
00:07:49,317 --> 00:07:52,398
and look immediately and see if this had any effect on performance.

129
00:07:52,898 --> 00:07:55,659
You don't have to restart or save something to disk

130
00:07:55,699 --> 00:07:58,321
and then load it up in an external viewer, for example.

131
00:08:01,022 --> 00:08:03,123
And what you see here in the profiler UI

132
00:08:03,163 --> 00:08:06,184
is basically a single game frame in our internal profiler.

133
00:08:07,504 --> 00:08:09,905
These colors represent the different game systems.

134
00:08:10,946 --> 00:08:12,927
So you have the green for pathfinding, for example,

135
00:08:12,967 --> 00:08:13,927
and you have the blue for...

136
00:08:14,529 --> 00:08:17,892
AI updates in general, and you have the pink here is agent updates.

137
00:08:19,433 --> 00:08:22,135
And just to break this down in terms of what we talked about before

138
00:08:22,175 --> 00:08:23,076
with the frame structure,

139
00:08:23,657 --> 00:08:26,139
this pink part here in the beginning, the begin frame,

140
00:08:26,619 --> 00:08:29,401
you can see that it's very small and needs to be very small

141
00:08:29,761 --> 00:08:32,664
so that we because we can't run anything else in parallel with that,

142
00:08:32,804 --> 00:08:33,805
at least not world updates.

143
00:08:35,486 --> 00:08:37,047
And then you can see I've marked here as well,

144
00:08:37,107 --> 00:08:39,710
the different worlds and how they are sort of updated

145
00:08:39,850 --> 00:08:42,031
one by one on each on each short task.

146
00:08:44,850 --> 00:08:47,852
So how do we use this in a day to day?

147
00:08:49,433 --> 00:08:52,736
Well, every day we have automated bot tests running on the latest builds.

148
00:08:53,556 --> 00:08:56,278
And the bots are very lightweight AI agents.

149
00:08:56,818 --> 00:08:58,800
And they try to replicate behaviors

150
00:08:59,020 --> 00:09:00,801
that the players have as closely as possible.

151
00:09:01,341 --> 00:09:03,883
Not necessarily from a functionality standpoint,

152
00:09:04,023 --> 00:09:07,245
but at least from a performance standpoint.

153
00:09:08,266 --> 00:09:12,229
So we really want to make sure that they are similar to players

154
00:09:12,329 --> 00:09:14,050
in terms of the server load that they have.

155
00:09:15,067 --> 00:09:16,930
And this is something we verified during production

156
00:09:17,010 --> 00:09:20,454
by having QC tests, directed QC tests, for example,

157
00:09:21,115 --> 00:09:23,178
that would, so we would test with a hundred bots

158
00:09:23,238 --> 00:09:24,339
and then a hundred QC players

159
00:09:24,399 --> 00:09:26,442
and make sure that the server load was roughly the same.

160
00:09:28,770 --> 00:09:30,550
The bots themselves are part of the game solution,

161
00:09:30,770 --> 00:09:34,792
and this really helps us implement additional functionality

162
00:09:34,812 --> 00:09:35,352
into the bots.

163
00:09:35,532 --> 00:09:38,273
It's a small side note, but having that,

164
00:09:38,353 --> 00:09:41,174
make sure we can use server code and client code

165
00:09:41,294 --> 00:09:43,135
and data directly in the server bots.

166
00:09:43,155 --> 00:09:45,716
And it's very quick to iterate over features using the bots.

167
00:09:48,216 --> 00:09:49,957
The main point here is that we use the bots

168
00:09:50,037 --> 00:09:54,078
to fully load test our target hardware every day.

169
00:09:55,101 --> 00:09:57,862
So we run a thousand of these bots on the actual hardware

170
00:09:57,882 --> 00:10:00,183
that we're shipping the server on every day.

171
00:10:02,224 --> 00:10:03,825
Each test generates a report,

172
00:10:04,485 --> 00:10:07,107
and we get links to Grafana that I mentioned before.

173
00:10:07,727 --> 00:10:09,808
There's also a link to some binary profiler data

174
00:10:09,828 --> 00:10:12,629
that was output during the test if there was any issues.

175
00:10:13,509 --> 00:10:15,570
And we get links to crash reports and so on as well.

176
00:10:17,791 --> 00:10:19,472
So some source code coming up.

177
00:10:20,393 --> 00:10:22,754
It's been heavily modified just to make it readable

178
00:10:22,894 --> 00:10:24,054
and understandable here.

179
00:10:24,174 --> 00:10:24,294
So...

180
00:10:25,682 --> 00:10:28,504
So, but I hope it's the concept comes across anyway.

181
00:10:30,926 --> 00:10:32,707
So the profiler data export that I mentioned,

182
00:10:32,727 --> 00:10:34,828
the sort of binary profiler data that's available,

183
00:10:35,649 --> 00:10:40,573
we output that from the server from a couple of different cases.

184
00:10:41,293 --> 00:10:44,976
One of them, the most simple one is that when the frame time

185
00:10:45,096 --> 00:10:46,397
exceeds the target frame time,

186
00:10:46,917 --> 00:10:49,059
in our case, 50 milliseconds on a regular server.

187
00:10:50,498 --> 00:10:54,021
The other one is that an individual system has actually gone over its own budget.

188
00:10:54,661 --> 00:10:59,585
So each one that each system has its own CPU budgets like AI system,

189
00:10:59,625 --> 00:11:02,807
for example, and the dark zone and so on.

190
00:11:03,348 --> 00:11:07,090
And if either of those go over their own budgets, we also save profiler data to disk.

191
00:11:08,171 --> 00:11:12,334
The third one is when an individual scope actually takes longer than anticipated.

192
00:11:14,022 --> 00:11:18,225
In this case, we create, first of all, a regular profiler event,

193
00:11:18,545 --> 00:11:21,207
but then we also create this timer object that you can see here,

194
00:11:21,247 --> 00:11:23,908
which tracks if the scope actually took too long

195
00:11:23,968 --> 00:11:26,610
and does a profiler save in that case.

196
00:11:29,251 --> 00:11:32,153
The data export itself is nearly lock free,

197
00:11:32,613 --> 00:11:35,055
and it runs on a low priority thread in the background,

198
00:11:35,315 --> 00:11:38,217
so it has almost no effect on frame rate whatsoever,

199
00:11:38,717 --> 00:11:40,498
and can basically be running constantly

200
00:11:40,518 --> 00:11:41,959
during normal operation of the server.

201
00:11:43,779 --> 00:11:45,480
It allocates memory on the first use,

202
00:11:45,660 --> 00:11:48,581
which is not super relevant for the server,

203
00:11:48,621 --> 00:11:50,901
but since this is available for the Xbox

204
00:11:50,941 --> 00:11:52,442
and the PS4 as well, for example,

205
00:11:53,002 --> 00:11:56,503
we need to make sure that we don't use this memory,

206
00:11:57,143 --> 00:11:58,844
if we don't allocate this memory,

207
00:11:58,864 --> 00:11:59,764
if we're not going to use it.

208
00:12:01,224 --> 00:12:03,825
And you can see here as well that in this task,

209
00:12:03,905 --> 00:12:05,406
which is actually the code that is running

210
00:12:05,446 --> 00:12:06,486
on the low priority thread,

211
00:12:06,906 --> 00:12:11,688
we call into the runtime of the profiler to save its state.

212
00:12:13,037 --> 00:12:15,759
And the way we do that without interrupting the execution

213
00:12:15,999 --> 00:12:21,281
of the process or the threads is that the profiler at any given time

214
00:12:21,321 --> 00:12:24,643
has a number of used blocks that contain profiler data

215
00:12:24,863 --> 00:12:25,643
that has been recorded.

216
00:12:26,304 --> 00:12:28,725
And it has a number of free blocks as well available for use.

217
00:12:30,166 --> 00:12:33,907
And when a specific thread, so this is organized per thread.

218
00:12:33,987 --> 00:12:37,189
So each row here is per thread performance data,

219
00:12:37,229 --> 00:12:38,670
basically profiler data.

220
00:12:39,595 --> 00:12:42,457
When a thread fills up the current block,

221
00:12:42,717 --> 00:12:45,378
it's going to the profiler grabs a free block

222
00:12:45,458 --> 00:12:47,820
and puts it in the front and starts filling up this block again.

223
00:12:49,160 --> 00:12:51,222
And then once per frame, at the end of the frame,

224
00:12:52,302 --> 00:12:54,743
we do a sort of a cleanup where we go through

225
00:12:54,763 --> 00:12:57,865
all of the oldest memory blocks here in use by the profiler

226
00:12:58,045 --> 00:13:00,346
and move them to the free blocks pool.

227
00:13:02,027 --> 00:13:03,508
And what happens when we do a save

228
00:13:03,568 --> 00:13:07,650
is that we're going to lock this per thread data one by one.

229
00:13:09,524 --> 00:13:10,805
and copy it basically.

230
00:13:12,285 --> 00:13:15,467
And if that thread is currently recording events,

231
00:13:15,567 --> 00:13:16,668
which is usually the case,

232
00:13:17,708 --> 00:13:20,649
then we simply take a new block from the free blocks

233
00:13:20,709 --> 00:13:22,730
and put it in the front and continue to record data.

234
00:13:23,351 --> 00:13:25,752
This way execution can continue on the thread without,

235
00:13:26,252 --> 00:13:28,593
even though we're currently saving down the state as well.

236
00:13:30,554 --> 00:13:31,875
There is one edge case here where

237
00:13:33,210 --> 00:13:35,072
we actually could run out the free blocks.

238
00:13:35,352 --> 00:13:38,555
And in that case, the current thread that is trying to record something

239
00:13:38,615 --> 00:13:42,838
actually has to grab the oldest memory block for that thread

240
00:13:43,259 --> 00:13:44,079
and move it to the front.

241
00:13:44,600 --> 00:13:46,101
And if that happens during a save,

242
00:13:46,782 --> 00:13:49,164
then we actually end up locking execution of that thread.

243
00:13:49,784 --> 00:13:51,205
And that is something we really want to avoid.

244
00:13:51,245 --> 00:13:54,428
So we need to make sure we keep the number of free blocks available

245
00:13:54,468 --> 00:13:56,450
needs to be big enough to never run into that example.

246
00:13:59,432 --> 00:13:59,892
Okay, so.

247
00:14:00,916 --> 00:14:03,798
We store, we save the internal state of the profiler

248
00:14:03,998 --> 00:14:04,998
and we store it to disk.

249
00:14:05,859 --> 00:14:08,361
We store it to disk based on the source trigger

250
00:14:08,861 --> 00:14:10,802
that did the save in the code.

251
00:14:11,342 --> 00:14:13,424
This makes it easy to find for the programmer

252
00:14:13,444 --> 00:14:14,464
who's looking for something,

253
00:14:15,145 --> 00:14:16,846
for example, the dark zone in this example,

254
00:14:17,326 --> 00:14:18,947
debugging performance issues in the dark zone.

255
00:14:20,150 --> 00:14:22,451
The data will also always be there because we don't allow

256
00:14:22,911 --> 00:14:25,712
sort of these different game systems

257
00:14:25,752 --> 00:14:27,012
to overwrite each other's data.

258
00:14:27,032 --> 00:14:29,673
They don't share a single disk budget, for example.

259
00:14:29,693 --> 00:14:31,133
They have individual disk budgets.

260
00:14:34,234 --> 00:14:36,574
Using Grafana from code as a programmer is also very simple.

261
00:14:36,975 --> 00:14:38,495
Here you can see, for example, we have two,

262
00:14:38,515 --> 00:14:40,195
a couple of simple one-liners

263
00:14:40,235 --> 00:14:42,436
to report load factor and update rates.

264
00:14:44,456 --> 00:14:46,497
And this is something we really encourage people to do

265
00:14:46,557 --> 00:14:46,837
as well.

266
00:14:47,575 --> 00:14:52,560
For programmers who write the system, if they can think of any type of value in that system

267
00:14:52,620 --> 00:14:57,124
that they would like to track over time to be able to compare it with performance, for

268
00:14:57,164 --> 00:14:59,906
example, they are encouraged to do so.

269
00:15:00,307 --> 00:15:04,891
And this helps with, at the early stage of most investigations, to narrow down what the

270
00:15:04,911 --> 00:15:05,571
problem could be.

271
00:15:08,074 --> 00:15:11,977
Here's an example of that, where we see, for example, we have all the different...

272
00:15:13,003 --> 00:15:16,905
The numbers of AI in each particular load level, for example,

273
00:15:17,005 --> 00:15:18,567
and the number of raycasts we've done,

274
00:15:18,607 --> 00:15:20,868
and the number of state syncers for interactions,

275
00:15:20,948 --> 00:15:22,529
which have this sort of runtime data,

276
00:15:22,729 --> 00:15:24,631
runtime state of interactions in the world.

277
00:15:28,333 --> 00:15:31,595
Just briefly, going into how it works

278
00:15:31,635 --> 00:15:33,156
on the live environment is very similar.

279
00:15:33,336 --> 00:15:34,317
We still have Grafana,

280
00:15:35,078 --> 00:15:37,779
and we constantly monitor all of our live game servers.

281
00:15:38,550 --> 00:15:40,791
And if we see an issue like the spike here in this case,

282
00:15:41,412 --> 00:15:43,092
instead of going to Windows Explorer,

283
00:15:43,153 --> 00:15:46,154
we just go to Google buckets and it's structured in the same way.

284
00:15:46,174 --> 00:15:48,595
So you'll find your dark zone data there, for example,

285
00:15:48,815 --> 00:15:49,976
if that was the problem you were having.

286
00:15:51,316 --> 00:15:53,457
And then you simply download and open the profiler data

287
00:15:53,477 --> 00:15:54,318
and can start debugging.

288
00:15:57,399 --> 00:15:59,780
So that's it for the tools and process side of things.

289
00:16:00,400 --> 00:16:02,341
Now I'm going to go into a couple of examples of things,

290
00:16:03,362 --> 00:16:06,643
of issues we had during development and the solutions we found.

291
00:16:07,965 --> 00:16:11,147
The first one was that at some point we were seeing that

292
00:16:12,128 --> 00:16:14,430
some short task threads were not running

293
00:16:14,470 --> 00:16:16,171
for the entire frame, basically.

294
00:16:18,132 --> 00:16:20,894
So this means that several cores

295
00:16:20,955 --> 00:16:22,956
are not really updating worlds for that entire frame.

296
00:16:22,996 --> 00:16:26,519
And this is with the sort of performance model we have,

297
00:16:26,539 --> 00:16:29,081
that means that we've lost CPU work

298
00:16:29,181 --> 00:16:31,743
that was supposed to go into updating worlds, right?

299
00:16:33,484 --> 00:16:35,265
So the first question is, do they not have work?

300
00:16:35,365 --> 00:16:36,006
Is there nothing to do?

301
00:16:37,712 --> 00:16:41,054
But looking at what we do, we spawn a number of tasks

302
00:16:41,234 --> 00:16:42,695
equal to the number of threads.

303
00:16:43,515 --> 00:16:45,676
And then what each of those does

304
00:16:46,857 --> 00:16:49,778
is to grab a world from a global list,

305
00:16:50,038 --> 00:16:51,799
update that world, and then grab the next one.

306
00:16:53,740 --> 00:16:55,741
So there's, on that global list,

307
00:16:55,801 --> 00:16:58,442
like these different tasks that will do that

308
00:16:58,502 --> 00:16:59,743
until that global list is done

309
00:16:59,783 --> 00:17:00,803
and we've updated all the worlds.

310
00:17:00,903 --> 00:17:01,984
So there's definitely work to do.

311
00:17:03,585 --> 00:17:06,587
You can also see here in the above section of the code

312
00:17:06,627 --> 00:17:10,269
that we list we sort the world models based on their estimated costs,

313
00:17:11,350 --> 00:17:12,550
the estimated update costs.

314
00:17:13,031 --> 00:17:16,493
And the reason we do that is to avoid frame time spikes.

315
00:17:17,474 --> 00:17:20,896
So looking at this example here, which you saw before as well,

316
00:17:21,536 --> 00:17:23,998
we actually have one world that's taking a very long time

317
00:17:24,018 --> 00:17:25,278
to update at the end of the frame.

318
00:17:25,559 --> 00:17:27,500
And this delays the entire frame execution

319
00:17:27,940 --> 00:17:29,041
or the entire end of the frame.

320
00:17:30,675 --> 00:17:35,137
And it also creates this pocket of low CPU utilization here at the end, which is also really bad.

321
00:17:35,677 --> 00:17:41,199
So the solution here is to make sure that the system understands that this world will be expensive to update and then it will be

322
00:17:41,259 --> 00:17:44,321
sorted and be updated at the beginning of the frame instead.

323
00:17:46,742 --> 00:17:47,662
But back to the issue.

324
00:17:50,003 --> 00:17:55,505
So each task and thread, there's definitely work to do. So next we

325
00:17:56,180 --> 00:17:58,842
enable the context switch data in the profiler.

326
00:17:58,902 --> 00:18:01,784
And this uses the Windows event tracing API.

327
00:18:02,765 --> 00:18:05,827
And we can see clearly that three threads are never

328
00:18:05,907 --> 00:18:08,029
switched in during the entire frame.

329
00:18:10,110 --> 00:18:12,052
And then two of them are switched in after half the frame.

330
00:18:13,473 --> 00:18:16,475
At the same time, we have five long task threads running.

331
00:18:16,695 --> 00:18:18,897
So long tasks are doing something.

332
00:18:19,037 --> 00:18:21,559
At the same time, short tasks are not doing anything.

333
00:18:22,593 --> 00:18:24,935
And the only conclusion here is that the low priority threads,

334
00:18:25,495 --> 00:18:27,616
they get to run while the high priority threads are waiting.

335
00:18:28,877 --> 00:18:31,738
And the explanation is that for Windows Server 2016,

336
00:18:31,798 --> 00:18:34,179
at least the highest priority guarantee

337
00:18:34,239 --> 00:18:37,021
is really only valid within a single core group.

338
00:18:38,001 --> 00:18:40,182
And core groups have up to four cores in them.

339
00:18:41,283 --> 00:18:45,585
And each core group only has its own ready queue of threads waiting to run.

340
00:18:46,405 --> 00:18:48,226
So I've given here's an example in the picture

341
00:18:48,246 --> 00:18:48,966
where you can see that.

342
00:18:50,279 --> 00:18:52,400
Core 7, for example, is running a,

343
00:18:52,960 --> 00:18:54,561
is executing a low priority thread

344
00:18:55,162 --> 00:18:56,883
while there's a high priority thread waiting

345
00:18:56,943 --> 00:18:58,123
in on another core group.

346
00:18:59,744 --> 00:19:01,966
And core group 2 is not really gonna steal any work

347
00:19:01,986 --> 00:19:04,187
from core group 1 here until the ready queue

348
00:19:04,787 --> 00:19:07,369
for core group 2 is empty.

349
00:19:07,629 --> 00:19:08,850
So this can go on for quite some time.

350
00:19:09,550 --> 00:19:12,012
And with 48 cores, we're gonna have 12 of these groups.

351
00:19:12,052 --> 00:19:13,753
So we can expect this pattern to happen.

352
00:19:14,333 --> 00:19:17,115
And you should expect that using this schedule.

353
00:19:18,872 --> 00:19:20,793
We made one attempt to sort of get around this.

354
00:19:22,455 --> 00:19:24,677
Using the function called setThreadIdealProcessor,

355
00:19:24,717 --> 00:19:27,420
we tried to map each short task thread

356
00:19:27,620 --> 00:19:30,382
onto its own core, basically,

357
00:19:30,943 --> 00:19:33,765
without using affinities and making it a hard requirement.

358
00:19:33,845 --> 00:19:35,867
But this didn't have any real effect.

359
00:19:36,688 --> 00:19:38,610
And it mentions that in the documentation as well,

360
00:19:38,770 --> 00:19:41,733
that this is more of a hint to the scheduler

361
00:19:41,773 --> 00:19:42,333
than anything else.

362
00:19:43,816 --> 00:19:47,900
So generally, oversubscribing to the CPU is good for throughput.

363
00:19:47,920 --> 00:19:50,443
You get a lot, you get the most work done that way usually.

364
00:19:51,164 --> 00:19:56,069
But in our case, we could spot the 5 to 10% perceived performance drop

365
00:19:56,169 --> 00:20:00,193
because the threads we want to run are not running.

366
00:20:01,514 --> 00:20:02,996
But it made us rethink the role of priorities a little bit

367
00:20:03,016 --> 00:20:03,857
because we were not sure how to do that.

368
00:20:03,877 --> 00:20:04,698
So we decided to go with the standard

369
00:20:05,675 --> 00:20:08,877
Even if we managed to succeed and enforce that short tasks

370
00:20:08,957 --> 00:20:10,378
are the only ones running at this time,

371
00:20:10,458 --> 00:20:12,459
or at least all short tasks are running,

372
00:20:13,040 --> 00:20:16,382
then you would have long tasks that need to run,

373
00:20:16,742 --> 00:20:17,962
wouldn't be running anymore instead.

374
00:20:18,283 --> 00:20:20,824
So it could lead to some sort of starvation in those systems.

375
00:20:20,984 --> 00:20:24,566
And this is something we saw as well in development.

376
00:20:25,587 --> 00:20:29,429
So I'm going to go into some examples around that.

377
00:20:30,350 --> 00:20:34,212
Yeah, so we spotted basically overnight, this server was crashing.

378
00:20:35,102 --> 00:20:35,903
we ran out of memory.

379
00:20:36,663 --> 00:20:38,844
And looking in Grafana, we could see that this was

380
00:20:40,145 --> 00:20:42,586
connected, probably connected to a list of

381
00:20:43,066 --> 00:20:46,447
pending deletions being built up over time.

382
00:20:46,748 --> 00:20:49,569
So what this is, is that when a player leaves the world,

383
00:20:50,329 --> 00:20:53,211
we take their world, when a player leaves the server,

384
00:20:53,251 --> 00:20:55,192
we take their world and put it in a delete queue.

385
00:20:55,612 --> 00:20:57,873
And then we have background tasks running to

386
00:20:57,913 --> 00:20:59,474
delete that over time.

387
00:21:00,074 --> 00:21:01,234
But they weren't able to keep up

388
00:21:01,374 --> 00:21:03,235
because they were starved for CPU.

389
00:21:06,027 --> 00:21:10,151
So long tasks, they are, long task threads are low priority,

390
00:21:10,171 --> 00:21:12,873
but it's crucial that they run within a given time.

391
00:21:14,415 --> 00:21:15,716
Remember that they are doing things

392
00:21:15,796 --> 00:21:17,317
like caching save game data

393
00:21:17,357 --> 00:21:19,159
and instancing nav mesh around the player

394
00:21:19,860 --> 00:21:22,282
and deleting worlds in this case, of course.

395
00:21:23,263 --> 00:21:26,646
The Windows scheduler isn't really built to handle this,

396
00:21:26,746 --> 00:21:27,687
and this is by design,

397
00:21:27,807 --> 00:21:29,729
especially under high amounts of pressure.

398
00:21:31,968 --> 00:21:34,290
And in fact, the better CPU utilization

399
00:21:34,970 --> 00:21:37,172
means that we have a larger risk of running into this issue.

400
00:21:37,652 --> 00:21:40,315
So the better we are at packing sort of the main part of the frame

401
00:21:40,335 --> 00:21:43,998
with world updates in a perfectly nice, parallelized way,

402
00:21:44,638 --> 00:21:46,460
we run into this issue instead.

403
00:21:48,582 --> 00:21:51,264
The solution for us was to add an internal wrapper

404
00:21:51,404 --> 00:21:52,786
around long tasks on the server.

405
00:21:53,646 --> 00:21:56,289
And what it does is basically it adds an estimated cost

406
00:21:56,589 --> 00:21:59,211
to a global value whenever we start a new long task.

407
00:21:59,973 --> 00:22:01,294
And as soon as the long task is done,

408
00:22:01,394 --> 00:22:04,536
we remove that estimated cost from that global value again.

409
00:22:05,376 --> 00:22:08,979
And this gives us a nice sort of, yeah,

410
00:22:09,119 --> 00:22:11,340
estimated cost of the total amount of work

411
00:22:11,420 --> 00:22:13,301
that is currently queued up to be done

412
00:22:13,442 --> 00:22:14,963
or being done on long tasks.

413
00:22:15,943 --> 00:22:18,265
And the way we use this is that if it

414
00:22:18,285 --> 00:22:19,926
goes above a certain threshold, we

415
00:22:19,966 --> 00:22:21,987
increase something called a starvation level.

416
00:22:23,508 --> 00:22:27,170
And if it goes below another threshold,

417
00:22:27,210 --> 00:22:28,791
we reduce the starvation level again.

418
00:22:29,849 --> 00:22:32,390
And then when we spawn tasks to update the worlds,

419
00:22:33,231 --> 00:22:35,652
we basically, instead of just spawning a number of tasks

420
00:22:35,772 --> 00:22:36,773
equal to the number of threads,

421
00:22:37,193 --> 00:22:38,914
we also subtract the starvation level.

422
00:22:40,174 --> 00:22:42,135
We don't want to change the number of threads

423
00:22:42,175 --> 00:22:44,757
we're actually running, we just spawn less tasks.

424
00:22:44,837 --> 00:22:46,838
So those some of those short task threads

425
00:22:46,918 --> 00:22:48,599
are just going to be idle during the frame.

426
00:22:52,020 --> 00:22:53,821
Here you can see how that works in practice,

427
00:22:54,081 --> 00:22:54,902
looking in Grafana.

428
00:22:55,342 --> 00:22:55,942
So you can see as.

429
00:22:57,075 --> 00:22:58,476
As work here in the top graph,

430
00:22:59,057 --> 00:23:01,619
estimated work for long tasks starts building up.

431
00:23:02,340 --> 00:23:05,242
In the middle graph, you can see that we are starting to sort of yield

432
00:23:05,823 --> 00:23:09,867
short tasks in order to allow more CPU

433
00:23:09,907 --> 00:23:11,588
to be used for the long tasks instead.

434
00:23:13,370 --> 00:23:17,955
And eventually, we can see that the effect of that

435
00:23:18,075 --> 00:23:20,077
is that quite quickly,

436
00:23:20,137 --> 00:23:22,038
the amount of long task work drops off again.

437
00:23:22,786 --> 00:23:25,288
And in the bottom here, you can see this is from a workstation,

438
00:23:25,328 --> 00:23:27,809
so it has a slightly more exaggerated effect here,

439
00:23:27,829 --> 00:23:31,631
but you can see that frame time goes up a little bit

440
00:23:32,232 --> 00:23:34,213
when we yield short tasks to long tasks.

441
00:23:34,253 --> 00:23:36,214
But this is expected, and that's the sort of trade-off

442
00:23:36,234 --> 00:23:39,296
you want instead of complete starvation for some systems.

443
00:23:41,998 --> 00:23:45,860
Another example we had was with regards to memory allocation.

444
00:23:47,781 --> 00:23:50,963
So overnight, we were spotting a lot of contention in our graphs.

445
00:23:52,840 --> 00:23:56,723
These graphs show aggregated execution time for every system

446
00:23:57,984 --> 00:23:58,985
per frame basically.

447
00:23:59,025 --> 00:24:01,447
So during a single frame here, we

448
00:24:01,487 --> 00:24:04,149
had 306 milliseconds spent in contention,

449
00:24:05,310 --> 00:24:06,951
which is an enormous amount of waste, of course.

450
00:24:08,192 --> 00:24:10,233
And again, this data is available for basically

451
00:24:10,313 --> 00:24:11,814
all game servers all the time.

452
00:24:14,636 --> 00:24:17,539
Just a side note on this is that the data here is per frame,

453
00:24:17,579 --> 00:24:20,341
but the resolution sample in Grafana is one second.

454
00:24:21,086 --> 00:24:24,409
And the reason I bring this up is that if you look into this kind of thing,

455
00:24:24,869 --> 00:24:26,891
be very careful with data compression,

456
00:24:26,951 --> 00:24:28,712
how it's handled in every step of the way.

457
00:24:29,633 --> 00:24:31,494
So in the game server executable itself,

458
00:24:31,554 --> 00:24:36,238
we have a report interval of one second and there are different ways to deal with

459
00:24:36,278 --> 00:24:36,458
that.

460
00:24:36,678 --> 00:24:40,762
And what we did was to make sure we preserve the maximum value through all of

461
00:24:40,802 --> 00:24:45,726
those steps. So if there is any, at any point on any frame, there is a spike,

462
00:24:46,006 --> 00:24:49,949
we will, we made sure that it will reach Grafana. We will see it in the graphs.

463
00:24:50,887 --> 00:24:53,289
But yeah, if you use the wrong kind of averaging methods

464
00:24:53,369 --> 00:24:54,970
on any of those steps in the middle, for example,

465
00:24:54,990 --> 00:24:56,171
you can actually squeeze the data

466
00:24:56,211 --> 00:24:57,852
and miss a lot of important information.

467
00:25:00,634 --> 00:25:02,315
Back to the issue, contention.

468
00:25:04,236 --> 00:25:08,019
Here's what the profiler data looked like

469
00:25:08,059 --> 00:25:09,860
when we downloaded the file for it.

470
00:25:11,413 --> 00:25:13,754
So this, what we're looking at here is a full game frame

471
00:25:14,135 --> 00:25:17,076
of about 74 milliseconds in this case, so too long.

472
00:25:17,796 --> 00:25:19,777
And one of the short tasks is stuck in contention

473
00:25:19,977 --> 00:25:21,898
for 42 milliseconds, which is a very long time.

474
00:25:22,758 --> 00:25:24,859
What's worse is that it's waiting for a long task

475
00:25:24,919 --> 00:25:27,260
to actually release the mutex.

476
00:25:28,940 --> 00:25:31,101
And that's so it's a priority inversion issue.

477
00:25:31,161 --> 00:25:33,282
It's the worst, we don't know when this mutex

478
00:25:33,302 --> 00:25:35,483
will actually be available again, especially under heavy load.

479
00:25:38,224 --> 00:25:39,325
And what it's hitting is a.

480
00:25:40,769 --> 00:25:49,758
global mutex inside the allocation system. So all allocations above 256 kilobytes hit that mutex.

481
00:25:50,379 --> 00:25:55,785
So here's how you can see sort of how the allocator looked. We have different heaps

482
00:25:55,865 --> 00:26:00,770
for different sizes of allocations and this heap system is used basically for all game systems.

483
00:26:03,807 --> 00:26:07,448
The smaller sized heaps also had some locks in them,

484
00:26:07,508 --> 00:26:09,249
but we had less contention here,

485
00:26:09,289 --> 00:26:10,629
but it was still a bit problematic.

486
00:26:11,069 --> 00:26:14,330
But you can see here if I go back in the page heap allocator here,

487
00:26:14,370 --> 00:26:17,972
you can see the sort of the main global mutex that we were hitting.

488
00:26:20,072 --> 00:26:22,193
So just to give an example of what kind of allocations

489
00:26:22,213 --> 00:26:24,394
we were having that were 256K,

490
00:26:26,134 --> 00:26:30,055
when we create a new AI, a new NPC in the world,

491
00:26:30,255 --> 00:26:31,796
we give them a behavior.

492
00:26:32,447 --> 00:26:34,369
And these are generally pooled and reused,

493
00:26:34,469 --> 00:26:35,490
so we don't create new ones.

494
00:26:36,410 --> 00:26:39,413
But there is a small part of the runtime data

495
00:26:39,713 --> 00:26:44,477
that is actually reallocated on every new NPC that is created.

496
00:26:45,618 --> 00:26:47,760
And since behavior graphs are about 10,000 nodes,

497
00:26:47,840 --> 00:26:49,641
and many of them have some small runtime component,

498
00:26:49,661 --> 00:26:51,182
this actually ended up being a lot of data.

499
00:26:51,543 --> 00:26:53,464
And the reason that we have to have to reallocate it

500
00:26:53,604 --> 00:26:57,708
is that we use these very highly compressed data structures

501
00:26:57,808 --> 00:26:59,129
inside the node graph system.

502
00:27:00,230 --> 00:27:01,551
So it was always, even on the...

503
00:27:02,510 --> 00:27:04,391
It was always reallocating the memory, basically.

504
00:27:05,812 --> 00:27:11,796
We managed to fix this eventually by optimizing just the size of the runtime data.

505
00:27:12,256 --> 00:27:16,098
So we got below 256 kilobytes and then it was less of a contingent problem.

506
00:27:17,699 --> 00:27:23,043
Another example we had that ran into this large allocation was NavMesh sector deletion.

507
00:27:24,284 --> 00:27:27,025
In some specific cases when we were

508
00:27:31,713 --> 00:27:35,075
going to delete the damaged sector. Usually that's done on a background thread,

509
00:27:35,775 --> 00:27:39,697
but in some specific cases when the player had run out of range before the

510
00:27:40,178 --> 00:27:44,100
the sector was fully streamed in for example we would actually end up deleting

511
00:27:44,360 --> 00:27:48,103
ending up with the last reference to it in the game code and we would delete it

512
00:27:48,183 --> 00:27:48,843
in the short tasks.

513
00:27:51,505 --> 00:27:55,167
This was also fixed. This was basically a bug so we just made sure that

514
00:27:55,587 --> 00:27:58,669
during the deletion we can detect that it's about to get orphaned and then we

515
00:27:58,689 --> 00:27:59,269
would spawn a

516
00:28:01,972 --> 00:28:05,655
a specific short or long task just to do that deletion instead,

517
00:28:05,775 --> 00:28:07,417
making sure that it runs on a background thread.

518
00:28:09,478 --> 00:28:11,100
So both of these examples were fixed

519
00:28:11,360 --> 00:28:14,402
to avoid doing these big allocations in the world updates.

520
00:28:14,883 --> 00:28:18,306
But we weren't able to fix all of these cases at the time.

521
00:28:18,926 --> 00:28:21,448
This was a bit later in production, we didn't have that much time left,

522
00:28:21,548 --> 00:28:24,471
and some of the systems were quite complex to try to refactor.

523
00:28:25,532 --> 00:28:26,913
So we had to take a look at the memory system.

524
00:28:29,273 --> 00:28:31,554
So the first step was to refactor the heap to be lock-free.

525
00:28:31,714 --> 00:28:32,675
Remove the mute access,

526
00:28:32,975 --> 00:28:35,037
make sure there is no locks left in there.

527
00:28:35,997 --> 00:28:37,238
But we could still see that,

528
00:28:37,858 --> 00:28:39,319
as you can see here in the picture as well,

529
00:28:39,339 --> 00:28:40,440
the blue highlighted parts,

530
00:28:41,401 --> 00:28:43,282
memory allocations were still taking a lot of time.

531
00:28:44,523 --> 00:28:46,404
And there was some lock still in effect,

532
00:28:46,464 --> 00:28:49,546
because as you can see in the second frame there,

533
00:28:49,926 --> 00:28:50,426
when everything,

534
00:28:50,967 --> 00:28:52,868
when all the allocations are happening at the same time,

535
00:28:52,888 --> 00:28:54,989
we would see this prolonged effect basically.

536
00:28:55,009 --> 00:28:56,370
So some kind of lock was still in effect.

537
00:28:57,502 --> 00:29:01,883
It turns out Windows Server 2016 had an internal global lock

538
00:29:02,003 --> 00:29:03,703
in the soft page fault implementation.

539
00:29:05,384 --> 00:29:08,665
And we were lucky to detect this at allocation time

540
00:29:09,705 --> 00:29:12,786
by having memory stamping in our debug builds.

541
00:29:13,586 --> 00:29:15,707
So we would see this as soon as you allocate memory.

542
00:29:16,027 --> 00:29:18,748
But if we were running this on one of our release builds

543
00:29:18,768 --> 00:29:20,508
that don't stamp memory, we would see this

544
00:29:20,588 --> 00:29:24,670
whenever a specific page was touched the first time by the application.

545
00:29:24,730 --> 00:29:27,150
It would have been a bit trickier to find it.

546
00:29:29,480 --> 00:29:31,381
So the solution was to have the lock free heap

547
00:29:31,802 --> 00:29:33,783
and combine it with a commit cache.

548
00:29:34,844 --> 00:29:36,746
So what that means is once we commit memory

549
00:29:36,786 --> 00:29:39,408
from the operating systems, we from the operating system,

550
00:29:39,428 --> 00:29:40,990
we basically don't decommit it again.

551
00:29:41,851 --> 00:29:43,432
We keep it in fixed size buckets,

552
00:29:44,012 --> 00:29:45,834
and buckets based on the allocation size.

553
00:29:46,455 --> 00:29:48,196
And these are global, they're shared by all threads.

554
00:29:48,396 --> 00:29:50,758
So we have a lock free access pattern to them.

555
00:29:53,225 --> 00:29:57,306
And we still run into this Windows lock the first time we touch a page.

556
00:29:57,846 --> 00:29:59,606
But since we don't decommit the page again,

557
00:30:00,327 --> 00:30:02,467
we basically only do that on startup

558
00:30:02,687 --> 00:30:04,828
and at the very beginning of execution

559
00:30:04,868 --> 00:30:07,208
and then all those occurrences taper off very quickly.

560
00:30:10,329 --> 00:30:12,470
So looking a bit at the new implementation,

561
00:30:12,510 --> 00:30:16,691
the new page heap allocation doesn't have a mutex anymore.

562
00:30:16,711 --> 00:30:19,632
And each bucket entry commits just as many pages

563
00:30:19,692 --> 00:30:22,212
as it needs for that allocation. It doesn't decommit them again.

564
00:30:24,000 --> 00:30:28,884
Here's a look as well at the bucket allocation implementation,

565
00:30:28,924 --> 00:30:31,686
if you're interested in that. I won't go through it in detail here.

566
00:30:33,668 --> 00:30:36,650
It took some time to find a good solution to this problem.

567
00:30:36,690 --> 00:30:38,692
And the main reason was that the code here

568
00:30:38,732 --> 00:30:41,294
is shared between all Snowdrop supported platforms.

569
00:30:42,135 --> 00:30:44,917
So the PS4, the Xbox One, all these types of clients

570
00:30:45,177 --> 00:30:46,458
and the PC server as well.

571
00:30:48,320 --> 00:30:50,200
And this needs to run on all of them.

572
00:30:50,220 --> 00:30:51,941
And we can't really introduce any regression

573
00:30:52,001 --> 00:30:55,162
on any of these other platforms when we do this rewrite.

574
00:30:55,322 --> 00:30:58,443
There's not a lot of room for server specific changes here.

575
00:30:59,523 --> 00:31:01,684
And the second thing to note really is that

576
00:31:02,204 --> 00:31:04,605
it may seem obvious that it's bad to have a mutex.

577
00:31:05,555 --> 00:31:07,537
on a global level in the allocator,

578
00:31:07,577 --> 00:31:10,580
but this was never visible on a 12-core workstation,

579
00:31:10,600 --> 00:31:11,721
basically, on the PCs.

580
00:31:12,342 --> 00:31:16,706
It was only started appearing and actually quite significant

581
00:31:16,886 --> 00:31:18,308
on 40 cores or more.

582
00:31:21,311 --> 00:31:24,294
Next example is a crash that we had in open beta.

583
00:31:25,655 --> 00:31:27,457
So basically, on the first day of our open beta,

584
00:31:27,477 --> 00:31:28,017
we had a crash.

585
00:31:29,372 --> 00:31:35,336
hundreds of crashes, I think. Certainly, game servers were crashing every few minutes.

586
00:31:36,296 --> 00:31:40,079
It was affecting thousands of players and we didn't get any crash dumps or any call

587
00:31:40,099 --> 00:31:44,561
stacks whatsoever. There was no information. Machines just went down and then automatically

588
00:31:44,601 --> 00:31:49,204
restarted. So we drew the conclusion that we have to catch this with a debugger attached.

589
00:31:50,868 --> 00:31:53,889
So we had to go and talk to the infrastructure team

590
00:31:54,349 --> 00:31:58,851
and sort of convince them to deploy a debugger, WinDBG,

591
00:31:58,951 --> 00:32:04,373
to a live game server machine and give us remote access.

592
00:32:04,393 --> 00:32:07,314
And we did that, and we attached the debugger to the game server

593
00:32:07,354 --> 00:32:07,735
process.

594
00:32:08,595 --> 00:32:10,716
And we immediately saw that every single player

595
00:32:10,776 --> 00:32:12,457
was off the server.

596
00:32:14,284 --> 00:32:16,986
So it turns out that WinDBG breaks by default

597
00:32:17,126 --> 00:32:18,267
when you attach to a process.

598
00:32:18,808 --> 00:32:22,190
And goodbye to those 1,000 players, basically.

599
00:32:24,012 --> 00:32:26,714
So we added the G flag to WinDBG,

600
00:32:26,774 --> 00:32:29,736
which makes it not break on attaching by default.

601
00:32:30,137 --> 00:32:32,899
And we sat four programmers around the laptop,

602
00:32:33,239 --> 00:32:35,661
remoting into this game server machine and waiting.

603
00:32:36,442 --> 00:32:37,242
It took a couple of hours,

604
00:32:37,302 --> 00:32:38,643
but we got the crash eventually.

605
00:32:39,880 --> 00:32:46,135
Turns out one of the UDP channels was deleting a linked list by recursion, basically.

606
00:32:46,275 --> 00:32:48,060
So the stack was running out of memory.

607
00:32:49,580 --> 00:32:51,622
And the problem here was that the crash handler

608
00:32:51,642 --> 00:32:53,463
was dependent on stack allocation itself.

609
00:32:54,864 --> 00:32:56,646
So there's a couple of solutions to this.

610
00:32:57,306 --> 00:33:01,409
You can either use a set threads that guarantee call

611
00:33:01,890 --> 00:33:03,571
to make sure that you have some space left

612
00:33:03,751 --> 00:33:04,812
when you get this problem

613
00:33:05,513 --> 00:33:07,614
and then use the crash handler can use that,

614
00:33:07,994 --> 00:33:10,436
or you can just use static memory instead,

615
00:33:10,977 --> 00:33:14,039
either thread local or locked with some sort of mechanism.

616
00:33:16,161 --> 00:33:17,862
Okay, time for some conclusions.

617
00:33:20,600 --> 00:33:24,841
The first one is that we managed to reach our performance targets fully.

618
00:33:25,361 --> 00:33:29,463
And this was due to the sort of heavy-duty testing

619
00:33:29,503 --> 00:33:30,623
that we did throughout the project,

620
00:33:31,444 --> 00:33:33,144
always running on the target hardware,

621
00:33:33,304 --> 00:33:36,085
always testing with a full server every day in production.

622
00:33:37,706 --> 00:33:40,807
We had very few performance issues actually slip past release.

623
00:33:42,228 --> 00:33:42,488
This was...

624
00:33:44,366 --> 00:33:47,627
And the ones that we actually had, they were fixed easily,

625
00:33:47,707 --> 00:33:48,828
found and fixed very easily,

626
00:33:48,868 --> 00:33:51,589
thanks to all of this data being available,

627
00:33:51,629 --> 00:33:53,329
like I said, in live as well.

628
00:33:53,349 --> 00:33:55,210
You can just go in and look at any server

629
00:33:55,250 --> 00:33:56,750
and see if it has any problems.

630
00:33:57,511 --> 00:34:00,592
And then you can find the binary profiler data

631
00:34:00,612 --> 00:34:01,652
for that specific server

632
00:34:01,692 --> 00:34:03,293
if you need to look into something specific.

633
00:34:05,197 --> 00:34:07,058
The third one is that the team and the management

634
00:34:07,078 --> 00:34:09,660
were always aware of the performance work

635
00:34:09,700 --> 00:34:11,181
that we had to do before shipping.

636
00:34:11,641 --> 00:34:14,343
So this relieved a lot of stress, I think, from the team

637
00:34:14,883 --> 00:34:17,485
and made sure that we knew the work that was coming up.

638
00:34:18,065 --> 00:34:20,447
We still did most of the optimizations quite late,

639
00:34:20,527 --> 00:34:22,268
but we knew exactly what we had to do.

640
00:34:25,210 --> 00:34:27,191
The next one is that this actually helped us

641
00:34:27,251 --> 00:34:28,832
find a lot of rare server crashes.

642
00:34:29,841 --> 00:34:32,904
So just looking at an example, if it takes a single dev tester

643
00:34:32,964 --> 00:34:36,427
about five hours to find a single bug or like a rare crash,

644
00:34:37,288 --> 00:34:41,091
and using a full server with bots,

645
00:34:41,592 --> 00:34:43,674
we can find it usually within 20 seconds.

646
00:34:43,754 --> 00:34:46,176
That's sort of the scale of how much faster we can find it.

647
00:34:48,378 --> 00:34:50,220
One thing that we need to improve, I think,

648
00:34:50,360 --> 00:34:52,942
is our tools for regression testing.

649
00:34:54,104 --> 00:34:57,328
So today, what most programmers have to do, if they make a change and they want to test

650
00:34:57,668 --> 00:34:59,690
if their change has any effect on performance,

651
00:34:59,730 --> 00:35:02,413
they have to test with and without their changes a couple of times.

652
00:35:03,435 --> 00:35:07,239
I want to improve this so we have a more kind of deterministic test.

653
00:35:09,481 --> 00:35:11,243
And should generally it should be available

654
00:35:11,323 --> 00:35:13,306
as some sort of pre-flight mechanism as well.

655
00:35:13,326 --> 00:35:15,468
So you just click a button somewhere and.

656
00:35:16,655 --> 00:35:20,779
your change list gets sent off and it gets tested by bots in a good deterministic way.

657
00:35:21,560 --> 00:35:25,023
And then you get a report back which systems were affected by the change,

658
00:35:25,083 --> 00:35:26,165
how are they affected and so on.

659
00:35:29,127 --> 00:35:30,489
So takeaways for you.

660
00:35:32,190 --> 00:35:37,996
My primary tip is to make sure you test your absolute worst cases continuously in development.

661
00:35:38,556 --> 00:35:41,860
This helped us find basically all of the examples that I've shown you before.

662
00:35:42,758 --> 00:35:46,819
with the thread priority issue and the memory allocation issue.

663
00:35:46,859 --> 00:35:49,739
We found them in development early on using bots

664
00:35:50,200 --> 00:35:52,360
because we were testing on target hardware

665
00:35:52,640 --> 00:35:53,740
with the max amount of players.

666
00:35:56,361 --> 00:35:57,601
This is really key.

667
00:35:59,482 --> 00:36:01,362
And the second one is to have some people

668
00:36:01,462 --> 00:36:04,243
or at least one person dedicated solely to looking at performance.

669
00:36:04,863 --> 00:36:07,744
This also helps because that means that that person will continuously

670
00:36:07,784 --> 00:36:11,044
also can improve the processes for the other programmers

671
00:36:11,104 --> 00:36:11,925
and how they work with this.

672
00:36:15,337 --> 00:36:18,739
The third one is to plan, implement and test your debugging pipeline

673
00:36:18,759 --> 00:36:20,440
for the live environment early on.

674
00:36:21,601 --> 00:36:24,183
This is sort of self-explanatory why this is a good idea,

675
00:36:24,223 --> 00:36:27,825
but it really helped us be very reactive once we went live

676
00:36:27,925 --> 00:36:31,107
and immediately analyze and fix some performance issues.

677
00:36:32,368 --> 00:36:35,710
But again, this is something that preferably you would want

678
00:36:35,770 --> 00:36:38,071
to have the same debugging pipeline in development.

679
00:36:38,831 --> 00:36:42,915
and when after you go live and this again is something that that Björn will talk

680
00:36:43,175 --> 00:36:45,318
talk about here at the GDC like I mentioned before.

681
00:36:47,840 --> 00:36:51,764
And that's all I had for today. Thank you very much for listening everyone and if you have any

682
00:36:51,804 --> 00:36:56,549
questions or any any comments please don't hesitate to to contact me on the email address

683
00:36:56,569 --> 00:36:58,170
you can see here. Thank you.

