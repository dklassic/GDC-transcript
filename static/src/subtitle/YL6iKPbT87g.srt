1
00:00:10,184 --> 00:00:18,229
My name is Chris Evans. I'm one of the lead technical animators at Epic. I kind of specialize in special projects.

2
00:00:20,592 --> 00:00:24,214
which may make me an odd choice to be giving this talk.

3
00:00:26,235 --> 00:00:29,817
So over the years, since my university days, I've been a

4
00:00:29,857 --> 00:00:31,358
bit of a Michelangelo-phile.

5
00:00:32,719 --> 00:00:36,441
I've been told that I have more books on Michelangelo

6
00:00:36,501 --> 00:00:38,782
than they have in the Stadel Art Library in Frankfurt.

7
00:00:39,823 --> 00:00:43,405
This is actually a plaster cast of David's face that I

8
00:00:43,445 --> 00:00:43,925
have at home.

9
00:00:45,266 --> 00:00:48,048
I've studied Michelangelo for a long time, and I'm a

10
00:00:48,388 --> 00:00:55,415
big fan and big geek and part of that was, you know, 1999 I was at SIGGRAPH because I'm

11
00:00:55,495 --> 00:01:00,820
old and I saw some of the work that was done scanning the David and I've always been

12
00:01:00,840 --> 00:01:05,704
interested in that. So I thought it would be an interesting thing to take a look at and

13
00:01:05,724 --> 00:01:06,905
that's what we'll be talking about today.

14
00:01:08,853 --> 00:01:14,699
At Epic we really like complex challenges so we have this kind of 20% time where we can go and

15
00:01:14,719 --> 00:01:21,245
do our own little endeavors. I was the games chair at SIGGRAPH last year and we thought, hey,

16
00:01:21,546 --> 00:01:25,610
this might be a really cool thing to do if we can work it out with the Italian government and

17
00:01:25,630 --> 00:01:26,571
all the parties involved.

18
00:01:28,225 --> 00:01:35,010
In the end when it comes to complex challenges, I didn't just want to get the statue in but I

19
00:01:35,070 --> 00:01:41,554
wanted to recreate the tribute and allow you to ride a scaffold to any part of the statue and

20
00:01:41,574 --> 00:01:47,878
really look at it up close. So to give you kind of a background.

21
00:01:48,812 --> 00:01:55,657
on the scanning of the David. The David was scanned in 1998 by Mark Lavoy and his team

22
00:01:55,697 --> 00:02:02,441
from Stanford. It was a huge undertaking. It took them 30 nights of hard work. So basically

23
00:02:03,742 --> 00:02:09,426
every night the museum would close and then they'd wheel in their like laser scanner.

24
00:02:10,327 --> 00:02:16,030
and set up these trusses and work on scanning the statue piece by piece by piece

25
00:02:16,951 --> 00:02:20,773
up until like 4 a.m. and then they'd pack their stuff away and go to sleep

26
00:02:20,813 --> 00:02:23,234
and the museum would open again in the morning.

27
00:02:23,574 --> 00:02:24,935
But it was a true labor of love.

28
00:02:26,276 --> 00:02:27,536
It's interesting.

29
00:02:28,057 --> 00:02:33,079
They scanned so much data that it wasn't really pieced together until about 2009.

30
00:02:34,821 --> 00:02:41,993
So that was a different paper. But that kind of tells you how high fidelity the data set is. It's

31
00:02:42,033 --> 00:02:45,459
down to pretty much the quarter millimeter the scans are.

32
00:02:46,510 --> 00:02:48,872
It's been presented over the years at SIGGRAPH multiple times.

33
00:02:49,752 --> 00:02:56,017
You may have seen many geometry processing papers that come out use the David data set

34
00:02:56,298 --> 00:03:00,221
or at least attempt to use one of the versions of the data set because it's very hard to

35
00:03:00,261 --> 00:03:03,403
do anything with the billion point data set.

36
00:03:05,166 --> 00:03:13,268
It is a billion points. When I was in university, they released something called scan view.

37
00:03:13,348 --> 00:03:16,988
Stanford released something called scan view. What scan view allowed you to do was tumble

38
00:03:17,028 --> 00:03:23,730
around like a 300 polygon representation of the David or of one of Michelangelo's other

39
00:03:23,750 --> 00:03:29,231
statues that they scanned. And then it would freeze. And when you stopped tumbling it, it

40
00:03:29,251 --> 00:03:32,612
would clear their server and send you back like a high res JPEG of what you're looking at.

41
00:03:33,135 --> 00:03:38,299
So I was there in my dorm room eating all my bandwidth, looking at every nook and cranny

42
00:03:38,359 --> 00:03:39,160
of the David.

43
00:03:39,180 --> 00:03:42,242
You could see the chisel marks, you could see the unfinished top of the head.

44
00:03:42,722 --> 00:03:43,703
It was really, really cool.

45
00:03:43,723 --> 00:03:50,227
I think back then, it was I think 1999 or 2000, back then I even tried to set up a little

46
00:03:50,268 --> 00:03:54,010
photogrammetry rig, but they could detect if you were taking the same view and just

47
00:03:54,050 --> 00:03:55,671
rotating a little bit or something.

48
00:03:56,092 --> 00:03:57,312
But I just thought it was so cool.

49
00:03:57,473 --> 00:03:58,813
I really wanted to see that data.

50
00:03:59,134 --> 00:04:00,475
I was really into sculpture back then.

51
00:04:00,575 --> 00:04:03,697
And even back then, he was just one of my heroes.

52
00:04:06,078 --> 00:04:08,960
So the data itself, it has no UVs.

53
00:04:09,321 --> 00:04:10,962
It was basically laser scanned.

54
00:04:11,262 --> 00:04:12,202
There's no UVs.

55
00:04:12,463 --> 00:04:13,764
It's all range data.

56
00:04:14,604 --> 00:04:17,086
There's multiple representations of that data.

57
00:04:17,486 --> 00:04:19,207
There's a 7 million representation.

58
00:04:19,247 --> 00:04:20,308
There's a 50 million.

59
00:04:20,588 --> 00:04:22,890
And there's a billion representation, which is the

60
00:04:22,950 --> 00:04:24,611
one that took about 10 years to piece together.

61
00:04:25,724 --> 00:04:32,109
So these different representations, they don't really match each other. They're warped in

62
00:04:32,149 --> 00:04:38,033
different ways. So being able to use one and then reach up into another one is very

63
00:04:38,053 --> 00:04:42,916
complicated because the data sets aren't aligned. You can't just bake the billion down onto

64
00:04:42,936 --> 00:04:48,180
the 50 million or vice versa. A billion points is a lot of data.

65
00:04:49,320 --> 00:04:54,723
I am one of my close friends, Mark Galant, we were having a discussion just about data

66
00:04:54,743 --> 00:05:00,965
sets this size and he was one of the original architects of Houdini and he was saying, oh,

67
00:05:00,985 --> 00:05:03,787
we're going to work on some geometry processing stuff, maybe we can talk about this.

68
00:05:04,287 --> 00:05:05,768
And he's like, I just thought about it.

69
00:05:05,788 --> 00:05:08,509
A billion points, that's like a gigabyte.

70
00:05:09,569 --> 00:05:15,633
just if they're bulls. And he's like, let's make them vertices. Okay, now it's 32 gigs of RAM.

71
00:05:15,653 --> 00:05:22,438
And he's like, connectivity? Yeah, I think you're on your own. But it's definitely interesting

72
00:05:22,699 --> 00:05:29,023
that data that was captured in the 90s, today we still just don't have a way to visualize. It's

73
00:05:29,043 --> 00:05:30,545
just so much data.

74
00:05:32,152 --> 00:05:35,994
So I kind of talked about how the different representations that people had pieced

75
00:05:36,014 --> 00:05:42,217
together over the years didn't really match. We weren't going to, so it started as myself and

76
00:05:42,237 --> 00:05:46,960
then I slowly started building this ragtag bunch inside of Epic to take a look at this stuff.

77
00:05:48,881 --> 00:05:54,063
one of our awesome character modelers on special projects. You may have seen him talking just

78
00:05:54,163 --> 00:05:59,865
the other day about the Andy Serkis work we did with 3Lateral or the Siren demo. I kind

79
00:05:59,885 --> 00:06:04,987
of roped him into this. What sculptor wouldn't want to see the data at such high res?

80
00:06:08,168 --> 00:06:12,033
We knew from early on we're not going to be piecing together the billion point set. I could

81
00:06:12,113 --> 00:06:19,922
load individual voxelized chunks of it. We needed to figure out how we're going to cross back and

82
00:06:19,962 --> 00:06:21,744
forth between these different data sets.

83
00:06:23,952 --> 00:06:29,516
There are lots of scanning artifacts. So the lower res datasets, people had already written

84
00:06:29,796 --> 00:06:34,720
test algorithms and stuff to fill a lot of the holes, to figure out a lot of the artifacts. As

85
00:06:34,740 --> 00:06:40,044
you got more, as you got higher and higher res, there were some pretty gnarly scanning

86
00:06:40,064 --> 00:06:44,127
artifacts and stuff that I also didn't want to start cleaning up by hand and I'm not, you

87
00:06:44,167 --> 00:06:44,267
know,

88
00:06:44,988 --> 00:06:45,588
I'm a technical enemy.

89
00:06:45,608 --> 00:06:47,770
I'm not going to write an algorithm to help me with that.

90
00:06:48,570 --> 00:06:51,712
Just to give you an example on the 50 million data set,

91
00:06:52,252 --> 00:06:56,375
this is how many issues one of the geometry processing

92
00:06:56,475 --> 00:06:57,976
programs found with that.

93
00:06:58,356 --> 00:06:59,817
Some of the holes are larger than others.

94
00:06:59,877 --> 00:07:01,237
Some of them are very small.

95
00:07:01,578 --> 00:07:05,500
But it's all a bunch of issues that normal geometry

96
00:07:05,540 --> 00:07:08,462
processing pipelines that we use in games would definitely

97
00:07:08,502 --> 00:07:09,242
have a problem with.

98
00:07:11,826 --> 00:07:13,787
So I'm going to talk a little bit about the pipeline

99
00:07:14,768 --> 00:07:18,029
that we went through to get to kind of shoehorn this

100
00:07:18,089 --> 00:07:19,430
into Unreal Engine.

101
00:07:21,031 --> 00:07:24,993
So we started with one of the lowest approximations.

102
00:07:25,793 --> 00:07:28,235
Well, I think, so this one came from the 50 million,

103
00:07:28,695 --> 00:07:30,576
like a down res to 10.

104
00:07:30,736 --> 00:07:32,077
And then we took that into ZBrush.

105
00:07:33,136 --> 00:07:37,979
From there, in ZBrush, we did a UDIM layout in ZBrush, so we tried to give it UVs.

106
00:07:39,860 --> 00:07:44,083
And again, all of this work we're giving back to the Digital Michelangelo Project at Stanford.

107
00:07:44,504 --> 00:07:47,706
So, whereas it had this representation without these things before,

108
00:07:48,166 --> 00:07:55,471
now Stanford's going to have a nice ZBrush model with UDIM layout and stuff for future people if they want to mess with the data set.

109
00:07:56,232 --> 00:08:01,014
So the ZBrush sub D3 was 2.7 million but it was pretty much

110
00:08:01,134 --> 00:08:04,055
indistinguishable from the 50 because we had vector displacement.

111
00:08:04,435 --> 00:08:08,277
Like with ZBrush, as you go down under the hood, it retains a lot of those details.

112
00:08:08,797 --> 00:08:11,898
And for me, because it was a very difficult

113
00:08:13,319 --> 00:08:18,323
rigid mesh alignment problem. I wanted to get into that 50 billion data set. We had to

114
00:08:18,363 --> 00:08:24,308
really align the meshes somehow. The ways we did that was by creating a lower res cage with

115
00:08:24,528 --> 00:08:27,350
C brush but also loading the 50 million data set.

116
00:08:29,212 --> 00:08:38,682
It's just like magic. Very rarely does a paper come along and the authors decide, hey, we're

117
00:08:38,702 --> 00:08:42,406
going to make an application and hey, that application works better than most standard

118
00:08:42,426 --> 00:08:43,988
software packages that I have to use every day.

119
00:08:44,726 --> 00:08:48,388
But instant meshes allows you to artistically paint.

120
00:08:48,648 --> 00:08:50,309
Well, first off, it picks a beautiful edge flow.

121
00:08:50,689 --> 00:08:52,510
This is the sample edge flow that it

122
00:08:52,570 --> 00:08:53,651
generated for the David.

123
00:08:53,871 --> 00:08:56,212
And you can see, by default, clicking one button, it's

124
00:08:56,272 --> 00:08:59,834
already realized that along the sling it's going to put

125
00:08:59,854 --> 00:09:01,295
the edges here, along the face.

126
00:09:01,355 --> 00:09:02,116
It's done a pretty good job.

127
00:09:02,556 --> 00:09:04,997
You can go and you can artistically draw exactly what

128
00:09:05,057 --> 00:09:05,978
edge flow you want where.

129
00:09:06,558 --> 00:09:08,939
And then it kicks out an example of exactly what it's

130
00:09:08,979 --> 00:09:12,661
going to build with quads and placed pole polygons.

131
00:09:13,202 --> 00:09:13,302
So.

132
00:09:14,202 --> 00:09:20,105
This is kind of an example of down resing the 50 into something that I wanted to go and then

133
00:09:20,485 --> 00:09:28,008
use to wrap. And so for wrapping, I originally tried and then we finally got to work.

134
00:09:28,834 --> 00:09:32,076
RAP3, which I don't know if you guys have used RAP3.

135
00:09:32,776 --> 00:09:36,659
It's a really great software for wrapping one mesh to

136
00:09:36,699 --> 00:09:36,999
another.

137
00:09:37,219 --> 00:09:39,861
It's for kind of non-rigid mesh alignment.

138
00:09:40,221 --> 00:09:42,022
You pick the different correspondences.

139
00:09:42,103 --> 00:09:48,047
We use this all the time to cast different head scans into

140
00:09:48,087 --> 00:09:49,227
different facial topologies.

141
00:09:50,308 --> 00:09:52,489
You can just pick a couple correspondence points on the

142
00:09:52,530 --> 00:09:55,311
two meshes, and then it will warp one mesh into the other.

143
00:09:56,473 --> 00:10:04,955
Um, so it's really made for taking a low res mesh and wrapping another. Uh, the first time I fed

144
00:10:05,015 --> 00:10:12,377
it, uh, different kind of decimated versions of the David, it was just like, no, not, not

145
00:10:12,397 --> 00:10:17,779
gonna work. Uh, and I, I contacted them and we walked through some of the issues and it's

146
00:10:17,819 --> 00:10:24,621
just, um, it's, it's very, very difficult to non-rigidly align one scan to another scan. Um,

147
00:10:26,215 --> 00:10:31,020
So in the end what we did is as I was mentioning on the previous

148
00:10:31,060 --> 00:10:38,367
slide, we ended up wrapping the Z brush, the Z brush sub D3

149
00:10:38,507 --> 00:10:42,011
itself and then once that was wrapped we bring that back into

150
00:10:42,051 --> 00:10:45,414
Z brush and reapply the vector displacement and then we had

151
00:10:45,474 --> 00:10:48,437
kind of a version that matched all of the

152
00:10:49,197 --> 00:10:51,298
I don't want to say they're warped.

153
00:10:51,318 --> 00:10:56,319
It's just the way it's scanned, they can be slightly off, just very, very slightly.

154
00:10:56,679 --> 00:11:02,021
But when you're talking about a 20-foot high statue, I mean, slightly is the toes five

155
00:11:02,061 --> 00:11:06,222
inches off at the base by the time you get there, because there are a bunch of different

156
00:11:06,262 --> 00:11:08,043
scans all the way up the length of the statue.

157
00:11:08,063 --> 00:11:08,103
So

158
00:11:08,123 --> 00:11:08,203
it's

159
00:11:08,283 --> 00:11:08,383
a

160
00:11:08,483 --> 00:11:08,583
game

161
00:11:08,603 --> 00:11:14,605
development

162
00:11:14,665 --> 00:11:14,965
transcript.

163
00:11:16,030 --> 00:11:25,863
This was the UV layout here. We used UDIMS and UE4 to kind of maximize the UV space. So you

164
00:11:25,883 --> 00:11:31,130
can see here how it's broken up. We gave a lot of UV space to the face. And then we

165
00:11:33,297 --> 00:11:37,121
The body and you can see the legs and the base down there is like just a little bit

166
00:11:37,561 --> 00:11:38,863
of UV space compared to the face.

167
00:11:39,283 --> 00:11:45,490
But that was because we really wanted to scan at least the face, get the representation

168
00:11:45,510 --> 00:11:50,255
of kind of the head and shoulder area in UE4 down to the quarter millimeter.

169
00:11:50,775 --> 00:11:55,861
So we were able to get the statue represented in UE4 baked to a millimeter.

170
00:11:58,802 --> 00:12:04,625
But when it came to the face, we looked up into the billion data set and we now had our data

171
00:12:04,685 --> 00:12:12,509
set, our model in ZBrush matching the billion model. So here we are allowed to go in and now I

172
00:12:12,549 --> 00:12:18,271
can piece together all these chunks that make up the face and then I can bake those across.

173
00:12:19,325 --> 00:12:23,407
But to bake those across, I just couldn't find a program that

174
00:12:23,447 --> 00:12:28,609
could load up, what is this, load up just 20 million

175
00:12:28,689 --> 00:12:32,630
polygons just for the face and decimate that down into

176
00:12:32,670 --> 00:12:34,671
something that we could ingest into ZBrush.

177
00:12:36,471 --> 00:12:41,493
So one of my friends, Kevin, a guy on our team, Kevin Vassey,

178
00:12:42,013 --> 00:12:45,235
she was like, hey, so why don't we try Blender?

179
00:12:45,395 --> 00:12:46,315
Because Blender's awesome.

180
00:12:46,615 --> 00:12:48,496
And I was like, look, if you know.

181
00:12:49,196 --> 00:12:55,398
If the standard mesh tools like mesh lab and all these things that people that write these

182
00:12:55,458 --> 00:12:59,039
papers use can't do it, Blender is not going to be able to do it.

183
00:12:59,279 --> 00:13:03,480
I remember one weekend we had one software package going all the way over the weekend

184
00:13:03,640 --> 00:13:07,041
and then it crashed like three days into the down resing of the face.

185
00:13:07,561 --> 00:13:10,002
And then in 12 minutes Blender just popped out of mesh.

186
00:13:10,922 --> 00:13:12,143
It was indistinguishable.

187
00:13:12,643 --> 00:13:14,323
I think it was just a couple million.

188
00:13:15,304 --> 00:13:20,770
It was significantly lower, low enough that we could load it into ZBrush and use it to transfer the

189
00:13:20,830 --> 00:13:25,456
details across to our mesh. But that was amazing. I mean, you can see here, the meshes are

190
00:13:25,496 --> 00:13:29,120
indistinguishable, but on one side I think it's like 20 million, on the other side it's like

191
00:13:29,380 --> 00:13:29,681
three. So...

192
00:13:33,298 --> 00:13:37,659
We had this high res face, we had everything aligned, but my

193
00:13:37,759 --> 00:13:41,400
topology was from instant, or no, it wasn't from instant

194
00:13:41,420 --> 00:13:44,220
mesh at that point, it was just from the ZBrush down res.

195
00:13:44,700 --> 00:13:48,941
But to recapture all the details in ZBrush, we wanted

196
00:13:48,961 --> 00:13:51,982
the even on the cage mesh, we wanted the face to be higher

197
00:13:52,022 --> 00:13:52,542
resolution.

198
00:13:53,222 --> 00:13:55,783
And around this time, we were also working on the digital

199
00:13:55,803 --> 00:13:58,664
Mike Seymour, and that's our in-house face topology.

200
00:13:58,764 --> 00:14:02,044
So we used RAP3, and we just wrapped Mike Seymour's face

201
00:14:02,184 --> 00:14:02,805
onto the David.

202
00:14:03,665 --> 00:14:04,866
So we were kind of running out of time.

203
00:14:05,647 --> 00:14:06,888
And that worked really, really well.

204
00:14:07,168 --> 00:14:10,431
So Adam was able to bake the details across.

205
00:14:11,031 --> 00:14:13,634
And as you can see here, at the quarter millimeter

206
00:14:13,674 --> 00:14:17,617
resolution mesh, in the experience, you can ride the

207
00:14:17,637 --> 00:14:19,298
scaffold right up to David's face.

208
00:14:19,699 --> 00:14:22,141
And you can see the chisel marks from the rasp chisel

209
00:14:22,161 --> 00:14:25,524
that he used here above the brow where he left it a bit

210
00:14:25,624 --> 00:14:26,104
unfinished.

211
00:14:26,504 --> 00:14:29,026
I mean, he was told that the statue was going to be seen

212
00:14:29,087 --> 00:14:29,667
on the top of.

213
00:14:30,589 --> 00:14:38,884
Unfortunately he did such a good job, it's a curse of competence, you know, that they wanted it

214
00:14:38,944 --> 00:14:41,208
down where everybody could see it really up close.

215
00:14:42,148 --> 00:14:48,130
This is a screenshot from the experience showing Peter

216
00:14:48,190 --> 00:14:52,112
Sumaseni, he did a really good job setting up lighting.

217
00:14:53,372 --> 00:14:55,713
So I had modeled the inside of the Tribune, which is where

218
00:14:55,753 --> 00:14:56,734
it's stored in Florence.

219
00:14:56,794 --> 00:14:59,795
And he did a great job lighting it to where it looks

220
00:14:59,855 --> 00:15:03,036
almost exactly like it does there in Florence.

221
00:15:03,536 --> 00:15:05,157
Here you can see the rasp chisel here.

222
00:15:08,160 --> 00:15:13,961
There are so many details of the statue that come across. It's kind of an annotated experience.

223
00:15:14,021 --> 00:15:18,642
So you can click different, you enter a mode and then there's little spheres you can click on

224
00:15:19,002 --> 00:15:23,943
and then you hear my voice kind of droning on about the statue. But little things like the

225
00:15:23,983 --> 00:15:29,884
cracked arm, like in 1527 they were riding and some people entered the Palazzo Vecchio.

226
00:15:30,244 --> 00:15:33,705
Like some dude threw a bench out a window just because he was angry. Well the bench just

227
00:15:33,725 --> 00:15:37,686
like lopped off the David's arm outside. Imagine being that dude that's like...

228
00:15:39,086 --> 00:15:44,189
Oh, I'm about to hit the dusty trail.

229
00:15:44,209 --> 00:15:48,512
So yeah, you can see all of those fixes that

230
00:15:48,532 --> 00:15:49,552
were made over time.

231
00:15:49,652 --> 00:15:50,593
It's really interesting.

232
00:15:51,754 --> 00:15:55,456
Even that's with the body at the one millimeter resolution.

233
00:15:56,216 --> 00:15:59,018
This was the final topology that we ended up using.

234
00:16:01,319 --> 00:16:04,941
And then this is the Tribune here.

235
00:16:04,961 --> 00:16:05,001
So.

236
00:16:08,052 --> 00:16:11,414
We tried doing an irradiance bake from the skylight itself.

237
00:16:12,714 --> 00:16:14,755
Pete set up, I don't know how many lights.

238
00:16:14,955 --> 00:16:18,076
As soon as I unhit his layer in Unreal, my world just became

239
00:16:18,536 --> 00:16:20,497
wire frames of lights pointing everywhere.

240
00:16:21,497 --> 00:16:25,559
But it's using the dome itself to bake a lot of the

241
00:16:25,619 --> 00:16:26,239
illumination.

242
00:16:26,279 --> 00:16:29,681
And then there's a bunch of little spots that they have

243
00:16:29,741 --> 00:16:33,422
set up in Florence that just highlight the statue itself.

244
00:16:34,968 --> 00:16:37,693
So looking at a couple things that could probably be better.

245
00:16:37,713 --> 00:16:42,362
The experience we use of Vive and we were

246
00:16:43,851 --> 00:16:45,912
It was really awesome that they gave us this extension box.

247
00:16:46,433 --> 00:16:48,294
So basically, the extension box would come out into the

248
00:16:48,314 --> 00:16:52,077
middle, and then we had full five chord length around the

249
00:16:52,117 --> 00:16:54,139
middle so someone can walk around the entire statue.

250
00:16:54,679 --> 00:16:58,682
But the experience is not a teleport around experience.

251
00:16:58,763 --> 00:17:02,145
It's very much you're on a wooden scaffold, and you can

252
00:17:02,165 --> 00:17:05,688
completely walk around the entire statue in 3D at life

253
00:17:05,708 --> 00:17:08,350
size and ride a scaffold up to any.

254
00:17:09,770 --> 00:17:16,433
to anywhere you want, even way above his head and look down. We only really had time to look

255
00:17:16,513 --> 00:17:23,077
up into the billion dataset to do a quarter millimeter bake on the face. But, um, we

256
00:17:23,571 --> 00:17:26,174
Like I said, we're going to give this stuff back to Stanford.

257
00:17:26,214 --> 00:17:30,798
And if in the future someone wants to do that kind of bake

258
00:17:30,959 --> 00:17:33,841
on the entire body, that would be a pretty interesting

259
00:17:33,982 --> 00:17:34,762
science project.

260
00:17:34,782 --> 00:17:38,346
It would be nice to see some of the unfinished marks on the

261
00:17:38,386 --> 00:17:40,869
tree behind his leg and different areas

262
00:17:41,309 --> 00:17:42,490
at that resolution.

263
00:17:44,011 --> 00:17:48,712
It would be great to see some more statues. The team when they were there, they scanned

264
00:17:49,512 --> 00:17:56,254
Dusk and Dawn and some of the Medici chapel statues and there were some artifacts and

265
00:17:56,274 --> 00:18:01,235
stuff. There's three or four statues you can look at on scan view even to this day. But it

266
00:18:01,255 --> 00:18:07,876
would be cool to see some more of the statues in VR. And I could envision in the future

267
00:18:08,797 --> 00:18:14,199
With other data, like scan the world, a bunch of guys go out and try to scan statues around the

268
00:18:14,239 --> 00:18:19,920
world. It would be cool to have a sculpture loader that would load it at the exact size that it's

269
00:18:19,940 --> 00:18:22,861
supposed to be viewed as the sculptor originally intended.

270
00:18:26,044 --> 00:18:31,934
I just wanted to thank the people that were able to help me create the experience. It was like a

271
00:18:32,014 --> 00:18:37,163
learning process. It was the first level I ever made in UE4 so that was good. It got me out of

272
00:18:37,203 --> 00:18:38,084
Maya for a couple of days.

273
00:18:40,278 --> 00:18:47,044
That's about it. If you guys have any questions, be sure to fill out the little email they send you.

274
00:18:47,284 --> 00:18:53,149
I think it would be really great to see more people talking about kind of working to get

275
00:18:53,270 --> 00:18:58,334
sculpture or fresco or any ways to experience things in VR.

276
00:18:59,595 --> 00:19:03,399
At a high quality that you can't really experience without traveling there.

277
00:19:03,619 --> 00:19:07,322
I mean, it's pretty interesting to be able to also see things from different views.

278
00:19:07,523 --> 00:19:10,285
Maybe the different views the way the sculptor originally intended it.

279
00:19:10,666 --> 00:19:14,169
We can put the David up on the Duomo and you can check him out from there.

280
00:19:14,910 --> 00:19:20,675
He changed a lot of the proportions just to be seen from that angle originally.

281
00:19:20,895 --> 00:19:23,858
But, sure. Questions?

282
00:19:26,937 --> 00:19:30,719
more about people's feedback, like artists, sculptors,

283
00:19:31,359 --> 00:19:33,901
sculptors, when they see David in VR?

284
00:19:35,081 --> 00:19:38,923
It was, I almost feel bad telling you how awesome it is

285
00:19:39,003 --> 00:19:41,985
when, originally we had planned to have it here, but it didn't

286
00:19:42,025 --> 00:19:46,507
really work out. There were some, I never thought, I mean

287
00:19:46,527 --> 00:19:48,809
I'm a Michelangelo geek, so I thought like, oh, this is

288
00:19:48,849 --> 00:19:51,150
pretty cool, maybe some people think it's cool. There were

289
00:19:51,190 --> 00:19:53,111
some people that took the goggles off and they were like

290
00:19:53,151 --> 00:19:55,112
crying, and I was like, wow.

291
00:19:56,951 --> 00:20:03,175
It's pretty crazy because you just don't expect it to be so large when you put the goggles on

292
00:20:03,215 --> 00:20:05,896
and you're there on a scaffold and you kind of look up at the statue.

293
00:20:06,437 --> 00:20:12,860
And you don't expect the detail and kind of the hair and areas around the face.

294
00:20:13,801 --> 00:20:18,043
And a lot of people didn't realize certain things he did with the eyes to cast shadows and stuff.

295
00:20:18,063 --> 00:20:19,924
Some people say, oh, it's little hearts or something.

296
00:20:19,984 --> 00:20:23,546
But it's just really interesting to be able to see the statue up close.

297
00:20:25,072 --> 00:20:28,052
What did you do for materials? Did the Stanford data have any color?

298
00:20:28,112 --> 00:20:31,433
And did you try to capture any of the, or make any of the subsurface stuff?

299
00:20:32,033 --> 00:20:37,595
Yeah, so it had a whole bunch of color. I think there were like 40,000 photographs of color

300
00:20:37,955 --> 00:20:42,496
that we had to use as reference because they were all, I think it was film stuff,

301
00:20:42,636 --> 00:20:46,037
and we had access to it, but we just didn't have time.

302
00:20:46,197 --> 00:20:48,658
So Pete went in and he lit it.

303
00:20:49,098 --> 00:20:51,640
And then we used the images that we had.

304
00:20:51,800 --> 00:20:54,523
So that's probably the thing that is the most, everything

305
00:20:54,543 --> 00:20:56,385
is super ground truth when it comes to the mesh.

306
00:20:56,765 --> 00:20:59,647
But when it came to, if I can go back to my first slide,

307
00:20:59,908 --> 00:21:04,031
when it comes to the lighting or the surfacing, we really

308
00:21:04,071 --> 00:21:09,296
had to kind of try as light-handedly as possible to

309
00:21:09,376 --> 00:21:09,777
add something.

310
00:21:11,458 --> 00:21:16,402
One of the, most of the stuff that we used as reference was pre-cleaning. So you saw like, you

311
00:21:16,442 --> 00:21:23,327
can really see the chisel marks because it's a bit gritty. They have since cleaned him with like a

312
00:21:23,487 --> 00:21:28,411
fancy process and he doesn't really look like that anymore but that was the data set we had so we

313
00:21:28,431 --> 00:21:32,174
just tried to keep, kind of adhere to that. Right, so it was just photographs, it wasn't like

314
00:21:32,234 --> 00:21:35,817
registered to the points or anything. No, no. I couldn't even get a Macbeth chart.

315
00:21:37,259 --> 00:21:51,974
So I wanted to just white balance everything. Anybody else? You can either walk over there or

316
00:21:52,034 --> 00:21:54,217
yell and I can repeat your question into the microphone.

317
00:21:57,678 --> 00:22:07,046
I'm just wondering if Stanford or other people have continued to do this with more statues and sort of what the time frame is now to scan versus the 30 days that it took to do data.

318
00:22:07,086 --> 00:22:19,096
So, Mark Lavoie and his team scanned this stuff in the 90s. He's now at Google. He is the licensor of the data in the US if someone wants to do something with it.

319
00:22:19,676 --> 00:22:25,721
He told me, he's been in the experience and he took the goggles off and he was like, holy crap.

320
00:22:26,896 --> 00:22:30,533
He's like, I sure am glad I replied to that email from the animator at Epic.

321
00:22:31,748 --> 00:22:38,792
But he was blown away and they did a lot of scanning of different statues and it's in the

322
00:22:38,912 --> 00:22:44,775
archive but there's not really anyone using those at the moment for anything like this. There

323
00:22:44,875 --> 00:22:50,858
are other statues like St. Anthony and there's other statues that are more life size that are

324
00:22:50,878 --> 00:22:56,901
interesting to walk around but there's no plans that I know of unless people's enthusiasm in

325
00:22:56,921 --> 00:23:00,182
this has kicked some people into gear.

326
00:23:05,240 --> 00:23:10,804
I was wondering what are your thoughts on larger scale virtual tourism like the Pyramids

327
00:23:10,844 --> 00:23:13,567
of Giza or the Great Wall of China or something like that.

328
00:23:13,947 --> 00:23:19,712
Would it be possible in today's or future technology to have that kind of level of detail

329
00:23:20,072 --> 00:23:23,335
for those kind of super large projects?

330
00:23:28,124 --> 00:23:34,527
I've been taking stereo photos for like 20 years. I love stereo 3D. I do feel that,

331
00:23:34,648 --> 00:23:38,810
especially just like stereo photography, I mean you feel the parallax. It feels like, you

332
00:23:38,830 --> 00:23:45,253
know, almost like you're there just visually. So I do think that could be possible. I think

333
00:23:45,273 --> 00:23:47,935
you'll always get some kind of resistance from the people.

334
00:23:48,435 --> 00:23:54,099
that sell all of the mouse pads and tourist stuff at those places. I don't think it will ever

335
00:23:54,219 --> 00:23:57,762
supplant the actual experience of going there but some people just can't.

336
00:24:00,007 --> 00:24:07,949
It's really interesting. This also raises a lot of questions in terms of just IP. Because it's

337
00:24:08,489 --> 00:24:14,310
stuff that's public domain. If you go into a museum and you take a picture of something that

338
00:24:14,370 --> 00:24:19,912
is in the public domain, it's a slavish reproduction. You could then go print mouse pads.

339
00:24:20,432 --> 00:24:23,152
It was really, really great for the game development transcript.

340
00:24:23,292 --> 00:24:30,559
for Mark and for the team back in Italy to allow us to show the demo. But it had the caveats of

341
00:24:31,540 --> 00:24:38,726
everything being encrypted and it was really super on lockdown. So it's with data sets like

342
00:24:38,746 --> 00:24:44,171
that, I mean, to be able to capture some of the experiences to the level to replicate them

343
00:24:44,672 --> 00:24:48,215
means you're going to have to live there for 30 days. And the people that own those things

344
00:24:48,235 --> 00:24:49,756
aren't going to let people come in.

345
00:24:50,577 --> 00:24:55,198
Mark had said that it was an amazing thing that they let him and his team go in and scan it

346
00:24:55,498 --> 00:24:58,759
because it was at the time, even newspapers in Florence, it was like,

347
00:24:59,319 --> 00:25:02,260
America is just going to make a digital David and nobody is going to visit.

348
00:25:02,300 --> 00:25:07,782
You know, it was, he definitely said it was kind of just an amazing thing that they let that happen

349
00:25:08,022 --> 00:25:15,304
and they've had to be very respectful of the data and of the Digital Michelangelo project itself,

350
00:25:15,384 --> 00:25:18,585
has to be very respectful of that stuff and keep it on lockdown.

351
00:25:22,733 --> 00:25:27,678
Hi, so I'm wondering if this project is actually available publicly for people to experience

352
00:25:27,738 --> 00:25:28,399
with or...

353
00:25:28,939 --> 00:25:30,601
That goes right into the next question.

354
00:25:30,621 --> 00:25:30,661
No.

355
00:25:30,681 --> 00:25:38,528
Because I'm actually with Stanford, I work for Stanford University, and I'm wondering

356
00:25:38,688 --> 00:25:42,512
also like, do you know which department that Mark was in when he...

357
00:25:43,513 --> 00:25:44,414
process of the scan data.

358
00:25:44,434 --> 00:25:45,114
I should have put a link.

359
00:25:45,174 --> 00:25:52,441
If you just Google Digital Michelangelo Project, there's the whole web page as it was in 99.

360
00:25:52,641 --> 00:25:53,562
It's still there.

361
00:25:54,983 --> 00:25:56,464
And you can get a lot of information.

362
00:25:57,325 --> 00:26:02,109
As you guys know, as soon as something hits a video card, I mean, with DirectX RIP or

363
00:26:02,129 --> 00:26:04,631
anything, it's so easy to rip meshes out of stuff.

364
00:26:05,392 --> 00:26:06,913
No, we would never be able to.

365
00:26:08,181 --> 00:26:13,907
to have it, when we want to show it as an experience at a single place like at a conference, we

366
00:26:13,947 --> 00:26:20,274
have to ask for, to be granted the right to show it and then it has to be kind of on lockdown

367
00:26:20,454 --> 00:26:28,704
with encryption and it's pretty, yeah, unfortunately it would never work putting it online. The

368
00:26:28,744 --> 00:26:29,985
Sistine ceiling, however.

369
00:26:30,901 --> 00:26:31,801
Yes, absent that.

370
00:26:32,001 --> 00:26:36,843
Yeah, the Sistine Ceiling, there's enough slavish reproductions of images out there

371
00:26:36,863 --> 00:26:41,204
that someone could piece together Sistine Ceiling and its public domain by themselves,

372
00:26:41,944 --> 00:26:42,784
maybe in their bedroom.

373
00:26:44,385 --> 00:26:49,306
But, yeah, in this case, the data is only, you know, you're only going to find the data

374
00:26:49,967 --> 00:26:52,487
in that data set, and that's not something that's shared.

375
00:26:52,547 --> 00:26:58,949
But for researchers, he grants the data all the time to be used in papers and things.

376
00:26:59,809 --> 00:27:00,110
Thank you.

377
00:27:00,510 --> 00:27:00,610
Yep.

378
00:27:07,483 --> 00:27:14,171
I see that the data was taken in 98 and nothing was done until 2009.

379
00:27:15,512 --> 00:27:16,754
When did you complete the project?

380
00:27:19,097 --> 00:27:25,204
I worked on it for about six months with Adam and Peter and Shane helped me with some of

381
00:27:25,244 --> 00:27:26,566
the learning the engine and the UI.

382
00:27:27,587 --> 00:27:30,429
But it was about six months from start to finish.

383
00:27:30,609 --> 00:27:34,492
But just in spare time, like we call it, like a 20% time project.

384
00:27:34,552 --> 00:27:39,956
I mean, when you... in 2009, you mean it was completed in 2009?

385
00:27:40,116 --> 00:27:44,460
There was a paper at SIGGRAPH, I had mentioned the author of the paper,

386
00:27:44,920 --> 00:27:48,123
where it was a new mesh alignment algorithm, and they said,

387
00:27:48,243 --> 00:27:54,428
and to show how well this works, we're going to piece together the whole billion point data set for the first time.

388
00:27:55,208 --> 00:28:00,331
I'm wondering if you had the choice to do your own scans,

389
00:28:00,832 --> 00:28:03,333
would you, do you think if you had the choice to do

390
00:28:03,353 --> 00:28:05,695
photogrammetry, would you be able to get the same results?

391
00:28:06,595 --> 00:28:08,436
No, no way, not even close.

392
00:28:08,516 --> 00:28:09,417
Well, I mean, who knows.

393
00:28:09,437 --> 00:28:12,219
With photogrammetry, you're just limited by how many

394
00:28:12,259 --> 00:28:13,880
cameras and what sensors you're dealing with.

395
00:28:15,841 --> 00:28:17,262
The data is really, really high res.

396
00:28:20,123 --> 00:28:25,026
Yeah, I don't, I love photogrammetry and I do a lot of photogrammetry, but the details

397
00:28:25,146 --> 00:28:31,248
that are captured in this data set, yeah, it's just hard to compare.

398
00:28:31,768 --> 00:28:38,631
Even in VR, because in VR it's really, you really, the quality, you don't get the same

399
00:28:38,671 --> 00:28:42,653
quality that obviously the scans are able to capture.

400
00:28:46,102 --> 00:28:46,743
with doing the scan.

401
00:28:46,763 --> 00:28:51,506
You mean like normal maps in stereo view or which, like baking it to normal maps versus

402
00:28:51,706 --> 00:28:52,747
parallax occlusion or?

403
00:28:52,767 --> 00:29:00,892
No, I mean getting the final, putting the final in VR using photogrammetry you would

404
00:29:00,912 --> 00:29:04,895
be able still to tell the difference between the scans and photogrammetry.

405
00:29:06,455 --> 00:29:13,220
If you, like I said, it depends on your budget. I guess I can totally picture a photogrammetry rig

406
00:29:13,240 --> 00:29:19,526
that could do as good of a job with a lot of macro lenses and a big structure. I do a lot of

407
00:29:19,546 --> 00:29:22,388
photogrammetry and I think it could get there.

408
00:29:23,349 --> 00:29:29,213
It's just the laser scan picked up even the smallest detail, but it's a bunch of swatches.

409
00:29:29,253 --> 00:29:31,154
You go like, brrr, and then you have a swatch.

410
00:29:31,855 --> 00:29:35,497
So for many years at SIGGRAPH it was, how do we align all these swatches?

411
00:29:35,677 --> 00:29:38,559
Because that's how cyberware laser scanners worked back then.

412
00:29:39,560 --> 00:29:43,162
It didn't just auto-align it like photogrammetry where it just figures out all the views and

413
00:29:43,182 --> 00:29:45,684
then does a pretty good job of making one mesh out of them.

414
00:29:45,744 --> 00:29:47,105
It's a whole bunch of little pieces.

415
00:29:47,125 --> 00:29:47,185
Yep.

416
00:29:48,246 --> 00:29:48,526
Thank you.

417
00:29:48,926 --> 00:29:49,046
Yep.

418
00:29:51,623 --> 00:29:55,945
All right. I think we're out of time. Thank you guys very much. Thanks for coming out.

