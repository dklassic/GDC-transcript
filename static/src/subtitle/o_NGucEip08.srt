1
00:00:07,861 --> 00:00:08,902
Thank you so much for coming.

2
00:00:09,162 --> 00:00:11,624
It means a lot to us.

3
00:00:12,785 --> 00:00:17,768
So this is a talk called Little Learning Machines, A Game About Training Neural Networks.

4
00:00:18,569 --> 00:00:26,795
Fundamentally, this talk is about reinforcement learning in games and why training agents is actually fun.

5
00:00:27,455 --> 00:00:29,036
And we made a small game about it.

6
00:00:30,277 --> 00:00:36,522
And a lot of the second half of the talk is how to make training fast, which is a very important part of this.

7
00:00:37,967 --> 00:00:43,889
But first off we need to kind of like recognize some prior work Black and white the 2001 classic.

8
00:00:43,929 --> 00:00:50,671
I hope you all know about it Creatures 1996 which was even bigger influence.

9
00:00:50,751 --> 00:01:05,095
I hope everyone knows what that is both of these games Use forms of reinforcement learning actually while you play and then more like more recently modern machine learning has been used in games such as Project Nero and

10
00:01:05,465 --> 00:01:11,907
While True Learn, ArtBot, which was made, or we found out just as we entered early access.

11
00:01:12,187 --> 00:01:12,787
Incredible game.

12
00:01:13,547 --> 00:01:20,209
And then LightBot, which doesn't use machine learning, but focused on teaching kind of like simplified models of programming.

13
00:01:21,830 --> 00:01:31,033
But actually our main influence when going into making Little Learning Machines was a reinforcement learning short film we made called Agents.

14
00:01:31,273 --> 00:01:34,974
And it was directed by Pietro Galliano, who is in the crowd and he's the best.

15
00:01:36,266 --> 00:02:02,169
And both of these projects were created by transitional forms Transitional forms was founded with a goal of kind of like human machine empathy and getting machines to understand humans and humans to understand machines I'm Nick my background is in animation and illustration like primarily focusing on kind of like General science education and I helped co-direct this game

16
00:02:03,133 --> 00:02:04,254
and we're co-directing this talk.

17
00:02:04,474 --> 00:02:11,677
So I am Dante, I was technical director on this game, and my background is in research and emerging technologies.

18
00:02:12,358 --> 00:02:17,020
One of which, at the time, was reinforcement learning, specifically deep reinforcement learning.

19
00:02:18,101 --> 00:02:26,405
Quick overview of reinforcement learning, don't want to get too far into it, this is the AI Summit after all, but you have an agent, you have an environment, they need to do things together.

20
00:02:26,985 --> 00:02:38,258
Usually, in the form of the agent taking some sort of action that changes the environment, and then that environment changing, and those changes being reflected in what the agent can observe.

21
00:02:38,338 --> 00:02:41,601
So the agent has some information about the environment that it observes.

22
00:02:41,882 --> 00:02:42,663
And the way that the agent

23
00:02:44,024 --> 00:02:47,066
changes its own behavior is through reinforcement.

24
00:02:47,806 --> 00:02:52,129
Reinforcement happens through the provision of rewards that can either be positive or negative.

25
00:02:52,249 --> 00:02:59,914
You give an agent positive rewards for doing the things that you want it to do, and negative rewards for doing the things that you don't want it to do.

26
00:03:00,274 --> 00:03:09,079
Over time, the agent optimizes and then develops a policy or a behavior that allows it to optimize this amount of reward.

27
00:03:10,280 --> 00:03:10,440
Cool.

28
00:03:10,900 --> 00:03:15,344
So one of the places I'd like to start is with this.

29
00:03:16,005 --> 00:03:25,773
And this was one of the primary inspirations for our director, Pietro, when he was coming up with what we saw in RL as being inspirational.

30
00:03:25,813 --> 00:03:33,140
This idea that within just these pixels, you can see that the blue and the red pixel are collecting these green apples.

31
00:03:33,800 --> 00:03:36,061
And they're shooting yellow lasers at each other.

32
00:03:36,101 --> 00:03:39,263
But even within this little 2D simulation, there's conflict.

33
00:03:39,303 --> 00:03:40,483
And it's kind of interesting.

34
00:03:40,503 --> 00:03:41,284
You can root for one.

35
00:03:41,464 --> 00:03:42,924
Blue is my favorite, by far.

36
00:03:44,545 --> 00:03:47,927
And this is a pattern that repeats during reinforcement learning.

37
00:03:48,187 --> 00:03:54,049
You can see this is an example of a clean RL where it's trying to learn how to walk in the Mojoko environment.

38
00:03:54,069 --> 00:03:56,250
And yeah, there's strife there.

39
00:03:56,911 --> 00:03:59,112
The only reward function for this one was go right.

40
00:03:59,632 --> 00:04:01,453
And RL doesn't have the intuitions we have.

41
00:04:01,913 --> 00:04:02,954
just does what it can.

42
00:04:03,815 --> 00:04:05,316
And we thought that this was very cute.

43
00:04:06,837 --> 00:04:11,180
And then finally, we also saw RL at the time kind of exceed our expectations.

44
00:04:11,521 --> 00:04:16,124
Again, this is an old paper, but OpenAI trained a team of five players to play Dota, which is

45
00:04:17,285 --> 00:04:20,767
If you've played Dota, you know that's really hard to collaborate at Dota.

46
00:04:21,147 --> 00:04:25,989
So yeah, RL kind of created a model that played better than we did.

47
00:04:26,069 --> 00:04:27,130
So that was amazing.

48
00:04:27,190 --> 00:04:33,933
So all this kind of came to a head when in 2019, 2020, 2019, we released this film called Agents.

49
00:04:35,254 --> 00:04:40,096
Yes, and so Agents was, like I said, the main inspiration going into Little Learning Machines.

50
00:04:40,336 --> 00:04:43,938
It's a short film about reinforcement learning agents.

51
00:04:45,245 --> 00:04:45,426
as well.

52
00:04:50,033 --> 00:04:54,876
move these kind of creatures around this planet, try to balance it out.

53
00:04:55,797 --> 00:04:57,738
And agents live on this place.

54
00:04:58,058 --> 00:04:58,699
It's weird.

55
00:04:58,819 --> 00:04:59,379
It's spinning.

56
00:04:59,459 --> 00:05:04,502
It's kind of like being influenced by your interaction and the weight.

57
00:05:04,822 --> 00:05:07,404
But actually, gravity goes down, which is weird.

58
00:05:07,944 --> 00:05:09,866
But also, that means the agents can fall off.

59
00:05:09,926 --> 00:05:17,490
So it's quite a dangerous place that they're trying to kind of coexist, cooperate, but also kind of create a bit of conflict around this.

60
00:05:20,098 --> 00:05:26,435
And what's special about Agents is that the characters, the actors, were actually trained through reinforcement learning.

61
00:05:27,560 --> 00:05:29,781
And this was back in 2019, like we just said.

62
00:05:30,441 --> 00:05:31,801
They're just neural networks.

63
00:05:31,821 --> 00:05:32,821
They're just neural networks.

64
00:05:33,181 --> 00:05:40,302
And more importantly, users could actually interact with the pre-trained models in VR or literally on their smartphone.

65
00:05:41,343 --> 00:05:45,744
And it was special for me at the time, because I had no idea what any of this was.

66
00:05:47,444 --> 00:05:52,645
And it kind of inspired, I guess, the next six years after.

67
00:05:54,182 --> 00:05:56,185
So the process was very classic.

68
00:05:57,126 --> 00:05:59,028
In this video you can kind of see agents training.

69
00:05:59,809 --> 00:06:11,163
There's a simplified environment, all running in parallel, and basically developers at the time, Dante and Alex, would set up these trainings and leave the agents training overnight

70
00:06:11,603 --> 00:06:13,324
and hoping for some sort of success.

71
00:06:15,084 --> 00:06:21,487
Often that took 16, 18 hours plus with our small team, and for a long time we didn't see anything.

72
00:06:22,587 --> 00:06:28,169
But one day we all came into the studio in the morning and we saw the runner.

73
00:06:29,472 --> 00:06:35,855
He might not be that impressive to any, like, AI enthusiasts in here, but to us this was, like, a big deal.

74
00:06:36,955 --> 00:06:38,156
And we love the runner.

75
00:06:38,196 --> 00:06:40,197
He's kind of, like, turned into the meme of the office.

76
00:06:40,757 --> 00:06:48,060
And it was really the first time that the team, like, really started feeling a connection and kind of, like, a love for this kind of, like, digital being.

77
00:06:48,180 --> 00:06:53,382
If you can see what he just did, he tricked the other four agents to fall off by going to the other side.

78
00:06:53,835 --> 00:06:57,059
and then just continue to run infinitely.

79
00:07:14,490 --> 00:07:21,832
And the more behaviors we trained, the more we kind of like developed a lot of like love and pride and attachment for these like little beings.

80
00:07:22,292 --> 00:07:36,855
But more than anything we found that watching intelligence and behavior kind of emerge out of the randomness they start with is in itself a uniquely rewarding experience and unlike anything we kind of like previously worked on or kind of experienced in games.

81
00:07:37,215 --> 00:07:41,976
So we started to think about how we could use this in a project in the future.

82
00:07:43,307 --> 00:07:47,188
Yeah, so we were a small team.

83
00:07:47,648 --> 00:07:53,210
And once we started coming together and seeing these things happen, the mornings were just cool.

84
00:07:53,370 --> 00:07:54,790
It was like, oh, what did they do last night?

85
00:07:55,190 --> 00:07:59,311
One time, we gave them a reward to get as close to the middle of the planet as possible.

86
00:07:59,531 --> 00:08:03,292
And they learned how to clip through the geometry, which was crazy.

87
00:08:04,813 --> 00:08:08,814
But the problem we were having is, OK, we show up and we show the simulations.

88
00:08:09,314 --> 00:08:10,954
And then the whole team has ideas.

89
00:08:11,414 --> 00:08:15,015
Like Nick had ideas, Pietro had ideas, but everyone.

90
00:08:15,395 --> 00:08:19,216
I think at one time our marketing manager, Michelle, came and she's like, oh, you want them to fight more?

91
00:08:19,276 --> 00:08:21,977
Just increase the scarcity or have less flowers.

92
00:08:22,017 --> 00:08:23,137
And then we did that, and it worked.

93
00:08:23,297 --> 00:08:24,677
And the film became more interesting.

94
00:08:25,097 --> 00:08:31,119
So the whole idea is Alex and I were gatekeeping anyone from running whatever simulations they wanted.

95
00:08:31,719 --> 00:08:32,559
And that wouldn't work.

96
00:08:32,719 --> 00:08:33,599
So we wanted to share it.

97
00:08:33,759 --> 00:08:36,500
We wanted to make it so that they can run their own simulations.

98
00:08:36,540 --> 00:08:37,380
But we had some problems.

99
00:08:38,720 --> 00:08:40,842
So the first problem is that it was slow.

100
00:08:41,622 --> 00:08:42,222
16 hours is a lot.

101
00:08:42,923 --> 00:08:43,603
It's a long time.

102
00:08:43,963 --> 00:08:46,645
I can't even wait for my crock pot to go that long.

103
00:08:46,725 --> 00:08:49,286
I usually just eat raw meat.

104
00:08:49,546 --> 00:08:52,688
And then the other thing was the hardware.

105
00:08:53,068 --> 00:08:54,829
Alex and I had big thread rippers.

106
00:08:55,610 --> 00:08:57,070
Back in the day those were beefy.

107
00:08:57,151 --> 00:08:59,232
And we ran them on that and we had to run

108
00:09:00,052 --> 00:09:02,633
We had to run parallel simulations, and that was expensive.

109
00:09:03,113 --> 00:09:11,035
So we either had to pay for cloud compute or pay for expensive hardware, and it wasn't something we could easily do or provision, at least with a small team.

110
00:09:11,856 --> 00:09:20,178
The other thing is, this may not seem like a big deal, but coding is a big barrier of entry, especially for something that we saw as accessible as reinforcement learning.

111
00:09:20,618 --> 00:09:25,600
So having to just code your own reward functions or change the physics of the world or whatever was a big barrier of entry.

112
00:09:26,420 --> 00:09:32,383
It was also non-visual, which is difficult if you have disabilities that prevent you from engaging with textual formats.

113
00:09:33,624 --> 00:09:36,005
It is sometimes difficult to work in a non-visual format.

114
00:09:36,545 --> 00:09:37,866
And then finally, it was just very complex.

115
00:09:37,906 --> 00:09:39,487
This is reinforcement learning, after all.

116
00:09:39,527 --> 00:09:40,867
It's deep reinforcement learning.

117
00:09:40,887 --> 00:09:43,068
There's a lot of Bellman equations and all that stuff.

118
00:09:43,429 --> 00:09:45,269
So having an understanding of

119
00:09:47,010 --> 00:09:50,592
of kind of optimizability was a bit difficult to kind of convey well.

120
00:09:50,612 --> 00:09:51,633
It was like, oh, that won't optimize.

121
00:09:52,113 --> 00:09:52,733
It won't converge.

122
00:09:53,274 --> 00:10:01,898
So just to give you an example of how complex things were when we wanted to help someone set things up, it's like, OK, well, you have to install Unity, install Python, set up your virtual environment.

123
00:10:02,179 --> 00:10:04,680
OK, not that one, that other virtual environment.

124
00:10:04,760 --> 00:10:05,400
Set up TensorFlow.

125
00:10:05,440 --> 00:10:07,922
No, not that version of TensorFlow, the one that's not

126
00:10:08,762 --> 00:10:12,083
Make sure it supports CUDA, but not for that video card.

127
00:10:12,763 --> 00:10:15,783
OK, get your environment running, do your reward functions, whatever.

128
00:10:15,843 --> 00:10:16,343
Run the simulator.

129
00:10:16,443 --> 00:10:17,143
Oh, you have a bug?

130
00:10:17,984 --> 00:10:20,144
Ah, yeah, you might want to try running it again.

131
00:10:20,504 --> 00:10:20,764
You know what?

132
00:10:20,804 --> 00:10:23,265
I hope you like graphs, because that's what you're going to be looking at, just graphs.

133
00:10:24,025 --> 00:10:26,885
And oh, and command lines, and more graphs, and more command lines.

134
00:10:27,245 --> 00:10:28,365
Shout out to ML agents.

135
00:10:29,026 --> 00:10:29,746
Their tooling was great.

136
00:10:31,846 --> 00:10:34,847
But we wanted to make it way more accessible.

137
00:10:35,187 --> 00:10:37,327
And by way more accessible, I mean really accessible.

138
00:10:37,867 --> 00:10:38,627
So we did it.

139
00:10:39,508 --> 00:10:40,028
We made this game.

140
00:10:41,189 --> 00:10:42,189
We made a whole game out of it.

141
00:10:42,630 --> 00:10:46,912
So yeah, the rest of the talk is going to tell you about Little Learning Machines, where we... Oh, sorry.

142
00:10:47,372 --> 00:10:48,012
We made this game.

143
00:10:48,213 --> 00:10:48,413
Sorry.

144
00:10:48,653 --> 00:10:49,673
I'm looking at the wrong slides.

145
00:10:51,374 --> 00:10:55,736
Yeah, we made this game called Little Learning Machines, which we're going to talk about.

146
00:10:56,777 --> 00:11:05,201
And in this game, we implemented something called real-time reinforcement learning, which is not a new AI concept, but rather a game mechanic.

147
00:11:06,782 --> 00:11:10,925
One that you can hopefully put into your game if you want your players to experience it.

148
00:11:11,366 --> 00:11:13,907
So let me just quickly run you through a live demo.

149
00:11:13,988 --> 00:11:16,810
It was supposed to be a live demo, but Nick doesn't trust me with the computer.

150
00:11:18,451 --> 00:11:20,092
This is an agent that's been trained to water flowers.

151
00:11:20,132 --> 00:11:21,373
You can see it's kind of walking around.

152
00:11:22,852 --> 00:11:25,795
It's kind of walking around, looking at flowers that it wants to watch.

153
00:11:25,935 --> 00:11:26,375
All right, Nick.

154
00:11:26,415 --> 00:11:26,875
Nick moved it.

155
00:11:27,596 --> 00:11:28,036
Put it up here.

156
00:11:28,496 --> 00:11:31,919
We can see that it kind of navigates the area, looks for flowers to water.

157
00:11:32,179 --> 00:11:35,721
But what we're going to do is we're going to reset it, just so you know that it's not pre-trained.

158
00:11:35,982 --> 00:11:39,084
And that's what it looks like when you reset your brain, I know firsthand.

159
00:11:40,505 --> 00:11:42,086
So this agent is now reset.

160
00:11:42,186 --> 00:11:42,727
It's brand new.

161
00:11:42,927 --> 00:11:47,891
Even though it previously was watering flowers, the same network is now completely randomized.

162
00:11:47,931 --> 00:11:50,814
So the actions that it takes are kind of at random, anything.

163
00:11:52,636 --> 00:11:57,220
And what we're going to do now is we're going to give it something completely new to do.

164
00:11:58,501 --> 00:12:05,588
And I want to show you how quick and easy it is to kind of give it a new thing to focus on.

165
00:12:06,069 --> 00:12:10,614
So in order to do that, we have to go to this space called the training cloud.

166
00:12:11,034 --> 00:12:14,818
And in the training cloud, we can set up an environment for our agent to learn.

167
00:12:16,619 --> 00:12:18,080
The first thing we need is obviously our agent.

168
00:12:18,260 --> 00:12:20,121
This one is affectionately named Chungus.

169
00:12:20,721 --> 00:12:21,862
So it gets put right in the middle.

170
00:12:22,042 --> 00:12:23,843
And we're going to remove its old rewards.

171
00:12:23,863 --> 00:12:24,623
It doesn't need them anymore.

172
00:12:25,564 --> 00:12:27,405
And we're going to give it something to do.

173
00:12:27,525 --> 00:12:31,547
So the easiest thing that I can show people in a talk is just collect crystals.

174
00:12:31,587 --> 00:12:36,729
Let's put down, I don't know, 14 crystals and see, can it collect them?

175
00:12:37,330 --> 00:12:39,731
And we don't have to code any sort of A star.

176
00:12:39,791 --> 00:12:42,332
And this agent is not pre-programmed, OK?

177
00:12:45,513 --> 00:12:48,354
We could have trained it to, I don't know, chop dogs with axes.

178
00:12:48,734 --> 00:12:51,835
But instead we're telling it that it loves collecting crystals.

179
00:12:52,175 --> 00:12:53,636
And that's put in there.

180
00:12:53,936 --> 00:12:56,457
We set up the end conditions so that it doesn't go forever.

181
00:12:56,657 --> 00:12:58,577
We saw what happens when you let agents go forever.

182
00:12:58,657 --> 00:12:59,738
And boom, that's it.

183
00:13:00,098 --> 00:13:00,758
We're running a training.

184
00:13:01,781 --> 00:13:03,742
It's reinforcement learning happening on your computer.

185
00:13:04,142 --> 00:13:14,148
Now, that doesn't sound super impressive, but to us, at the moment, this was like, oh yeah, this was, especially since in undergrad, I did a lot of this in labs, it's like, this is the smoothest it's gone in a long time.

186
00:13:14,868 --> 00:13:18,430
And as it trains, you can see right now, it's starting to take random actions.

187
00:13:18,970 --> 00:13:26,715
But little by little, you can see, right now it didn't want that crystal, but little by little, it develops a desire for crystals.

188
00:13:27,655 --> 00:13:30,238
It can develop a desire for blood, too, if you train it that way.

189
00:13:31,520 --> 00:13:33,322
But yeah, you can see that there's a little graph on the left.

190
00:13:33,342 --> 00:13:33,983
There's still graphs.

191
00:13:34,183 --> 00:13:35,825
We couldn't get rid of the graphs, at least not yet.

192
00:13:36,246 --> 00:13:37,848
But it's a little bit more intuitive.

193
00:13:38,769 --> 00:13:40,751
You can visually see the behavior that's happening.

194
00:13:40,811 --> 00:13:42,794
You can get a sense of how the agent is improving.

195
00:13:43,214 --> 00:13:45,697
And within the span of two and a half minutes,

196
00:13:46,457 --> 00:13:47,177
We have an agent.

197
00:13:47,258 --> 00:13:53,301
Oh, well, the other thing is you can't really see the behavior that well develop in real time unless you kind of speed it up.

198
00:13:53,661 --> 00:13:55,503
Collecting crystals is like a macro behavior.

199
00:13:55,563 --> 00:14:01,966
So if you speed it up, you're going to get a better sense of kind of the macro decisions that your agent is doing, which is why we added that little speed up there.

200
00:14:02,327 --> 00:14:05,429
But yeah, ta-da, a trained agent in a matter of minutes.

201
00:14:05,989 --> 00:14:08,591
Now, obviously, this is a very simple monotonic problem.

202
00:14:08,871 --> 00:14:11,092
But let's recap.

203
00:14:11,752 --> 00:14:12,193
Was it slow?

204
00:14:12,673 --> 00:14:13,433
No, it was pretty fast.

205
00:14:14,895 --> 00:14:16,276
The setup is what took the most part.

206
00:14:16,336 --> 00:14:20,401
It took like a minute and a half, but the whole training happened in a matter of like a minute or so.

207
00:14:22,824 --> 00:14:23,985
Did we need a special hardware?

208
00:14:24,025 --> 00:14:28,850
No, this was trained on an M1 Mac, but if you have a Surface Pro 3 from 2019, it'll work just fine.

209
00:14:31,032 --> 00:14:31,833
Did we have to do any coding?

210
00:14:32,554 --> 00:14:32,714
No.

211
00:14:33,094 --> 00:14:39,019
All we did was select the reward functions visually, which clearly the whole process was a visual one.

212
00:14:39,439 --> 00:14:40,860
And finally, was it complex?

213
00:14:41,000 --> 00:14:42,902
Well, there we have a bit of a caveat.

214
00:14:42,922 --> 00:14:50,848
And for the rest of the talk, we'll talk about some of the complexity that arises in trying to set up a world and behaviors like this.

215
00:14:51,489 --> 00:14:52,790
So I'll let Nick introduce that.

216
00:14:53,653 --> 00:14:57,976
Yeah, so first of all, our approach requires kind of like four steps.

217
00:14:58,636 --> 00:15:03,739
The first being designing an actual world for real-time reinforcement learning.

218
00:15:03,999 --> 00:15:06,080
The second being designing the agents.

219
00:15:06,701 --> 00:15:11,904
The third, designing the training experience itself and what kind of like training loop the players will go through.

220
00:15:12,444 --> 00:15:15,386
And then fourth, like packaging it all into a game.

221
00:15:16,827 --> 00:15:25,733
But before we start that, I just need a few key learnings from agents that really kind of like influenced early decisions on how we kind of like set those systems up.

222
00:15:27,034 --> 00:15:41,884
First off, there's in agents, there was a fundamental mismatch between how our team experienced training the actual reinforcement learning agents, seeing them learn and seeing their behaviors kind of emerge out of nothing versus audiences who like

223
00:15:42,404 --> 00:15:47,726
viewed these pre-trained models for the first time and may have not actually understood anything about machine learning.

224
00:15:48,086 --> 00:15:57,649
So it was kind of like, it's difficult to understand machine learning in general, or what a pre-trained model is, what they see, what they're doing, and fundamental, how they're trained.

225
00:15:59,029 --> 00:16:04,771
It's a lot easier to understand them when you're actually there in the room experiencing and watching them learn.

226
00:16:05,651 --> 00:16:15,698
Second, agents had this really beautiful kind of story with a lot of different story states, which each story state kind of required different behaviors from the agents themselves.

227
00:16:16,139 --> 00:16:25,885
So agents were expected to kind of like hit specific kind of behaviors at specific different story beats, which made it kind of really difficult to craft them.

228
00:16:27,026 --> 00:16:29,968
And it didn't necessarily allow them to flourish on their own.

229
00:16:31,798 --> 00:16:40,403
And on top of that, as you probably noticed, the environmental conditions we placed the agents in, in agents, were extremely difficult and very complicated.

230
00:16:41,203 --> 00:16:44,745
Any action they took could easily, like, lead to their death.

231
00:16:46,206 --> 00:16:51,349
On top of that, even their actual actions they could take to survive were quite limited.

232
00:16:51,409 --> 00:16:51,969
They could move,

233
00:16:52,382 --> 00:17:06,290
They could consume the flower, and they could also headbutt each other, all which combined to making it quite difficult to find really beautiful behavior out of them.

234
00:17:06,650 --> 00:17:13,213
And that led to the main thought, which is the agents can only be as smart as their environment lets them be.

235
00:17:14,174 --> 00:17:18,596
So that's the main thing going into designing a world for little learning machines.

236
00:17:19,837 --> 00:17:21,698
So when designing this world,

237
00:17:22,072 --> 00:17:22,572
like this.

238
00:17:37,158 --> 00:17:37,218
we

239
00:17:56,125 --> 00:18:00,347
like in this design process were kind of like based on playfulness.

240
00:18:00,968 --> 00:18:06,391
We want agents to interact with each other, whether it be through competition or cooperation.

241
00:18:06,931 --> 00:18:13,955
And so because of that, the world itself needed to be able to support many agents running at the same time.

242
00:18:14,276 --> 00:18:18,238
We ended up shipping with nine very diverse kind of brains in the experience.

243
00:18:18,618 --> 00:18:23,421
But the system actually allows up to 100 agents operating in real time

244
00:18:23,945 --> 00:18:24,145
at all.

245
00:18:40,247 --> 00:18:41,929
Like, inherently, they are pretty dumb.

246
00:18:42,129 --> 00:18:45,573
In fact, they start off only taking random actions.

247
00:18:46,254 --> 00:18:53,902
So the environment needed to be simple enough for a small brain to understand it, and diverse enough that it's not easily optimizable.

248
00:18:54,543 --> 00:19:01,331
But most importantly, we want to encourage more generalized behavior over, like, over-optimized zombies.

249
00:19:03,060 --> 00:19:05,442
So how do you encourage generalized behavior?

250
00:19:05,502 --> 00:19:06,822
How do you create a dynamic world?

251
00:19:06,862 --> 00:19:08,223
The same way you do it for your players.

252
00:19:09,004 --> 00:19:14,947
You add the dynamism to kind of a component of the world and then you stretch that out.

253
00:19:15,328 --> 00:19:20,331
So what we ended up doing is we stole a page from the book of Minecraft and we created items.

254
00:19:21,572 --> 00:19:31,482
which are these kind of things that can exist on one of the blocks within the world and they allow the agent to perform dynamic behavior without the agent having to be dynamic itself.

255
00:19:32,102 --> 00:19:35,386
You can have the same agent perform different types of behavior depending on the item that it's holding.

256
00:19:36,867 --> 00:19:38,148
Yeah, here's a couple examples.

257
00:19:38,168 --> 00:19:39,470
There's like a bat, an axe.

258
00:19:40,230 --> 00:19:42,031
Whatever that is at the bottom, there's a dog.

259
00:19:42,451 --> 00:19:44,672
These are all items within the world of little learning machines.

260
00:19:45,132 --> 00:19:47,653
And then the world itself is supposed to be editable by the user.

261
00:19:47,693 --> 00:19:57,698
The user journey is to create these environments in which your agents learn, and we wanted to make the environment as editable by the player as possible.

262
00:19:57,718 --> 00:20:02,760
So we took that concept of voxels and items and stretched it to as much as we could.

263
00:20:03,680 --> 00:20:14,788
But there was another reason we did this and this is Probably one of the trickiest parts of the talk is because making the world this way lets you train really fast The the world is actually not 3d.

264
00:20:14,988 --> 00:20:28,498
It's a 2d type soco bound tile type grid Little modifications in that there's like one item per cell and then the agents kind of move around within those within those 2d cells There's height, but the height mostly is used for navigation

265
00:20:29,220 --> 00:20:30,861
And there's no tunnels allowed.

266
00:20:30,881 --> 00:20:39,364
And the reason we did this is so that we could, A, discretize the space, and discrete observations for neural networks are much faster to train than analog ones.

267
00:20:39,924 --> 00:20:45,747
And then the other thing is that, or continuous ones, the other thing is that it allowed us to

268
00:20:46,768 --> 00:20:49,129
to reduce the dimensionality of the space.

269
00:20:50,110 --> 00:21:02,840
Instead of having to store 3D information and having to do a lot of the reasoning of the agent having to do with 3D information, it allowed the agent to make decisions on a 2D space, which is, again, trains a lot faster.

270
00:21:03,420 --> 00:21:07,123
And there was no end problem to the user.

271
00:21:07,163 --> 00:21:13,588
The user still sees a 3D world, but the agents and the AI train a lot faster because they're able to work on reduced dimensions.

272
00:21:14,588 --> 00:21:16,430
The representation drove the design.

273
00:21:17,331 --> 00:21:21,014
But not only did we discretize space, we also discretized time.

274
00:21:22,495 --> 00:21:31,083
Having an agent make a decision every couple of frames was not possible for the kind of scale... Again, don't make an MMO.

275
00:21:31,483 --> 00:21:36,944
the kind of scale that we were hoping to reach with this kind of game, and also having it run on the players' computers.

276
00:21:37,124 --> 00:21:40,465
And the more we kind of discretized time, the faster it trained.

277
00:21:41,305 --> 00:21:45,326
So what we ended up doing is we ended up dividing actions into turns.

278
00:21:45,506 --> 00:21:55,589
So every time anything in the world has an effect on other parts of the world, any item has an effect on an item, or an agent has an effect on an item, these all happen within turns.

279
00:21:55,649 --> 00:22:00,570
And that lets the evaluation of the simulation happen much faster.

280
00:22:02,172 --> 00:22:08,464
Yeah, but not only that, but the key kind of focus on kind of making training fast was also the design of the agents themselves.

281
00:22:09,919 --> 00:22:14,142
So the design of the agents, the agents in Little Learning Machines, we call them Animo.

282
00:22:15,243 --> 00:22:23,869
And we wanted to create a goofy character that could demonstrate the transition from non-intelligence to intelligence, semi-intelligence.

283
00:22:25,390 --> 00:22:35,077
The design is directly influenced by the networks themselves with their actions representing their body, which you can put some gloves, shoes on, or some eggshell.

284
00:22:35,817 --> 00:22:37,859
Their network is their crystal brain.

285
00:22:38,883 --> 00:22:42,626
And their observations are represented by their kind of like see-through bulbous head.

286
00:22:42,686 --> 00:22:50,032
It was really important in the design that we like made sure you could see the brain at all times because it's kind of like the kind of like core consistency of them.

287
00:22:50,993 --> 00:23:03,903
But the main goal was converting the discretized 2D space and the discretized kind of time space into a 3D representation that the audience could kind of like understand and learn from.

288
00:23:04,862 --> 00:23:12,106
For a long time we didn't actually know how smart these agents could become, and that was a huge barrier and a huge struggle.

289
00:23:12,366 --> 00:23:16,909
So we created a style that kind of showed that they're fallible machines and quite dumb at times.

290
00:23:17,469 --> 00:23:22,472
They need to be cute, silly, and most of all, forgivable, because they're going to spend a lot of time running into walls.

291
00:23:23,813 --> 00:23:29,177
and leaving the heavy lifting, basically portraying their intelligence through the animation system.

292
00:23:29,818 --> 00:23:47,393
Basically filling the gaps of the 2D Soccuban style brains and their turn-based decisions, like filling in those gaps with kind of like animations that could communicate the intent behind those decisions so that players could actually observe what they're doing and learn from what they're doing.

293
00:23:49,100 --> 00:23:54,763
So I'm going to break down some of the agent's architecture, just really quickly.

294
00:23:55,743 --> 00:24:00,445
It's going to get a little technical, but hopefully we can get through it in some quickness.

295
00:24:01,946 --> 00:24:03,007
The first thing is the vision.

296
00:24:03,267 --> 00:24:06,368
As I showed you earlier, the agents can see an area around them.

297
00:24:06,708 --> 00:24:10,530
Some agents can actually see infinitely far away, but they can only see a certain number of items.

298
00:24:11,410 --> 00:24:15,652
And the way that gets processed is that each of these observations is this stack

299
00:24:17,934 --> 00:24:20,335
of specific features that they can observe.

300
00:24:20,815 --> 00:24:38,906
So, for example, this agent that you see on the top left can observe a range of 5x5 around them, and within that range they can observe maybe the terrain, whether the medium that cell has, any items, any creatures, and anything that the creatures themselves are holding.

301
00:24:39,666 --> 00:24:47,050
And then each one of these features turns into a set of Boolean values.

302
00:24:47,170 --> 00:24:57,955
There are still some kind of floating point values that get passed in, but we found that the more we stuck to, again, Boolean values, the easier it was to train the networks.

303
00:24:58,716 --> 00:25:07,708
Originally, we had a one-hot encoding for every single item in the game, but that quickly blew up as the number of observations needed to grow with everything that was in the game.

304
00:25:08,109 --> 00:25:13,577
So eventually, we landed on this 20 questions model, which are these handcrafted embeddings where

305
00:25:14,417 --> 00:25:17,419
items that are similar can share similar embeddings.

306
00:25:17,860 --> 00:25:28,027
And this also actually sped up training again, because it allowed an overlap in the types of behaviors that certain agents were able to develop.

307
00:25:28,067 --> 00:25:37,914
So if you want an agent to not step on flowers and not step on fire, then they can generalize that behavior more easily through finding commonalities in the features.

308
00:25:39,354 --> 00:25:47,637
So all of these bits get stacked into a big stack and they get passed into the network.

309
00:25:47,837 --> 00:25:48,757
These diagrams are in the game.

310
00:25:49,057 --> 00:25:58,080
We tried to explain this to the users to some degree to the point where they can be confidently curious about some of the agents that they train with, but we really don't think that this is that important.

311
00:25:58,980 --> 00:26:14,063
Really, the point of adding these different kinds of learning to the game is so that the players get a sense of a different taste of the kind of representations of these different approaches at collecting observations for an agent.

312
00:26:14,303 --> 00:26:16,704
And that's kind of how the whole game plays out.

313
00:26:16,784 --> 00:26:21,245
Different agents have differences that just make them feel a little bit different.

314
00:26:22,685 --> 00:26:30,007
As I said, all of these observations get tacked onto a network, and this network is just a standard neural network.

315
00:26:31,188 --> 00:26:31,808
It's very small.

316
00:26:31,988 --> 00:26:36,789
You'd be surprised at the level of intelligence that these kind of little networks are able to show.

317
00:26:37,449 --> 00:26:41,591
Our smallest networks, for some of the first agents, we really want to prioritize training.

318
00:26:41,991 --> 00:26:43,371
There's two metrics that we want to prioritize.

319
00:26:43,391 --> 00:26:46,072
The first metric is time-to-first intelligence.

320
00:26:46,132 --> 00:26:49,933
As you saw when the agent was collecting crystals, it happened really, really quickly.

321
00:26:50,433 --> 00:26:54,719
And we need that to hit fast so that players are confident in the fact that they're actually learning.

322
00:26:55,399 --> 00:26:58,663
But then the second metric that we try to balance against is maximum intelligence.

323
00:26:59,044 --> 00:27:06,013
So smaller networks are not going to be able to do some of the more complex tasks in the game, for which you'll have to use some of the more complex networks.

324
00:27:06,893 --> 00:27:19,982
So our agents that we shipped vary from 128, it's two fully connected layers of 128 neurons, and then some of the later agents just have 512 neurons in four layers, and again, fully connected.

325
00:27:20,262 --> 00:27:26,786
There's nothing too special, but just with this alone they're able to do some fairly complex behavior if they're trained appropriately.

326
00:27:27,746 --> 00:27:31,689
One interesting thing we do is that these agents are trained using proximal policy optimization.

327
00:27:31,789 --> 00:27:33,210
We actually use the ML Agents Trainer.

328
00:27:33,270 --> 00:27:35,551
We kind of stripped it out and reused it for our own purposes.

329
00:27:36,371 --> 00:27:42,215
So one of the things that we do is we reset the critic every time we restart the training.

330
00:27:42,535 --> 00:27:52,461
You would think that this would result in slower trainings, but actually as long as the policy is accurate enough, the samples kind of converge fast enough that you don't notice a drop that much.

331
00:27:53,081 --> 00:27:59,806
and it allows you to kind of fine-tune your policy towards the direction which you wanted to take more easily.

332
00:28:00,647 --> 00:28:10,114
A lot of tech words, but finally, after we take all of those observations, we put them into this kind of like network that you can either reset or train continuously.

333
00:28:10,594 --> 00:28:13,876
They result in actions, which is what your character actually does.

334
00:28:14,377 --> 00:28:18,800
So in kind of deep reinforcement learning,

335
00:28:19,381 --> 00:28:19,461
of

336
00:28:34,603 --> 00:28:36,524
the set of things that we're training for.

337
00:28:36,884 --> 00:28:40,546
In agents, for example, we had multiple outputs, but here we're actually just looking for one.

338
00:28:40,566 --> 00:28:50,390
And the way I like to explain this is you kind of have a controller, and you give it to your little sibling while you're playing with them, and instead of holding the controller with both hands, they press one button at a time with their finger.

339
00:28:50,810 --> 00:28:52,052
That's basically what an agent is doing.

340
00:28:52,112 --> 00:28:53,773
It's pressing one button at a time.

341
00:28:54,334 --> 00:28:56,737
So every turn it's taking one of these seven actions.

342
00:28:57,397 --> 00:28:58,278
That's what it wants to do.

343
00:28:58,699 --> 00:29:10,812
And here's where another kind of trick in optimization happened that let us speed up the training, which was converging actions to have different meanings depending on semantics.

344
00:29:11,372 --> 00:29:18,815
Originally we had more actions, but we found that that was really one of the big limiters on the kinds of behavior that we can train.

345
00:29:19,215 --> 00:29:35,961
And we found that by having actions perform similar types of, have similar results, depending on contextual situations, it allowed us to put the weight of training not on the action decisions, but on the recognizing the different situations, which meant

346
00:29:36,461 --> 00:29:41,309
that we could reuse the latter parts of the network to optimize for specific actions.

347
00:29:41,810 --> 00:29:43,512
Sorry, to reuse specific actions.

348
00:29:43,532 --> 00:29:48,761
Like, for example, the same kind of part of the network could be used for, like, watering a flower than it is for chopping down a tree.

349
00:29:49,001 --> 00:29:50,764
And it allows you to train an agent to do multiple things.

350
00:29:51,905 --> 00:30:01,657
And the movement actions were also something where we found quite a bit of success by using these kind of tank controls instead of kind of like what's called cardinal directions.

351
00:30:02,578 --> 00:30:06,002
It allowed us to exploit symmetries in a grid.

352
00:30:06,923 --> 00:30:13,730
So you only need to train in one direction, and then all actions are kind of relative to that direction, which again was exploiting another symmetry.

353
00:30:15,252 --> 00:30:21,478
It actually sped up training by a factor of three, which was pretty useful.

354
00:30:24,302 --> 00:30:27,527
it let agents kind of develop egocentric views of the world.

355
00:30:28,068 --> 00:30:37,745
It's a little bit harder to debug because when you're looking at kind of the agent's observations, you have to take into account which direction they're looking at, but it does result in better, like faster training times.

356
00:30:39,366 --> 00:30:43,150
Yeah, so those were all kind of little tricks in designing your agent so they can train faster.

357
00:30:43,531 --> 00:30:48,636
But now that we have this kind of world and we have this kind of agent, collecting crystals isn't going to do it.

358
00:30:48,777 --> 00:30:52,621
What's kind of like some of the things you can expect out of an agent that's built like this?

359
00:30:52,981 --> 00:30:55,104
And I think Nick can cover the first time we saw that.

360
00:30:56,897 --> 00:30:57,958
So this is Sneaky.

361
00:30:58,058 --> 00:31:00,161
He's my all-time favorite agent.

362
00:31:00,341 --> 00:31:03,526
This was trained during the prototype.

363
00:31:03,946 --> 00:31:06,250
I love him a lot, but he sadly no longer exists.

364
00:31:07,031 --> 00:31:12,739
He's an agent that was trained originally to use multiple items to solve a puzzle.

365
00:31:12,859 --> 00:31:13,600
As you can kind of see,

366
00:31:14,500 --> 00:31:18,563
what he's doing is his goal is to collect all of those flags.

367
00:31:19,584 --> 00:31:22,705
And he has access to two tools at once.

368
00:31:22,765 --> 00:31:29,209
He has a block maker to make bridges and an axe that can break down those blocks.

369
00:31:30,790 --> 00:31:34,613
And if you notice it, I think he doesn't make a single mistake.

370
00:31:34,693 --> 00:31:37,034
He does it in the most optimized way.

371
00:31:37,354 --> 00:31:39,155
And this is why I love him.

372
00:31:39,716 --> 00:31:43,538
But sadly, when I put Sneaky into a different environment,

373
00:31:43,658 --> 00:31:51,420
with the same puzzle, he does nothing, and he's basically overwhelmed with indecisiveness or something.

374
00:31:52,260 --> 00:31:55,561
At the time, I had no idea what had gone wrong.

375
00:31:55,901 --> 00:31:56,741
What did I do wrong?

376
00:31:56,961 --> 00:32:02,743
But obviously, it became clear that he had over-optimized for that previous environment.

377
00:32:03,063 --> 00:32:04,603
And so rather than understanding

378
00:32:05,085 --> 00:32:10,108
anything about the environment, understanding kind of like the IDs of the different kind of items and stuff.

379
00:32:10,948 --> 00:32:20,594
Sneaky had purely basically remembered a set of actions in a row, basically kind of like a cheat code from a Sega game or something.

380
00:32:20,854 --> 00:32:26,337
So obviously we would have to address this and make this clear in the design of the training experience itself.

381
00:32:27,458 --> 00:32:29,959
So on to the designing of the training experience.

382
00:32:31,020 --> 00:32:34,362
Our main goal was to kind of like faithfully recreate training

383
00:32:34,902 --> 00:32:34,942
of

384
00:32:56,071 --> 00:33:03,517
separate from basically the actual world they live in, which is the sandbox main island and the quest islands that you kind of put them in.

385
00:33:04,458 --> 00:33:08,461
And this meant that we would have a training set versus an evaluation set.

386
00:33:08,701 --> 00:33:20,631
And this was inspired directly by what we saw with Sneaky, is that I was training Sneaky in a very specific environment for like a long time, and at no point did I take Sneaky out of

387
00:33:21,007 --> 00:33:22,327
and just test him somewhere else.

388
00:33:22,667 --> 00:33:24,628
At no point did I evaluate his behavior.

389
00:33:25,548 --> 00:33:28,669
So we basically created a training loop from this.

390
00:33:30,169 --> 00:33:35,311
We kind of like based it on a process of tweaking, and observing, and tweaking, and observing.

391
00:33:36,051 --> 00:33:47,213
Starting with tweaking, which is creating a simulation space, setting up an environment, setting the rewards, setting the end conditions, starting the training, and then watching the training happen.

392
00:33:47,474 --> 00:33:49,154
Like watch the animal,

393
00:33:49,654 --> 00:33:49,755
and

394
00:34:04,556 --> 00:34:04,796
I'm

395
00:34:21,334 --> 00:34:23,296
training becomes actually quite intuitive.

396
00:34:23,996 --> 00:34:24,557
Fun!

397
00:34:24,597 --> 00:34:28,400
We didn't know that this was the fun part of the game until very late into production.

398
00:34:28,520 --> 00:34:29,861
Exactly, exactly.

399
00:34:30,842 --> 00:34:34,965
So in this slide I can just talk about like basically setting rewards.

400
00:34:35,025 --> 00:34:39,769
A lot of this experience and a lot of part of the tweaking is setting appropriate rewards for the appropriate environment.

401
00:34:40,089 --> 00:34:47,835
So basically we created this kind of emoji-like system where it's kind of easy to kind of set different very specific rewards based on

402
00:34:48,175 --> 00:34:48,415
the kind

403
00:35:06,703 --> 00:35:07,784
they're going to break.

404
00:35:08,684 --> 00:35:10,685
They're just going to get confused, obviously.

405
00:35:11,225 --> 00:35:17,008
And that's not actually a big deal, because you can always play it, observe it, remove a reward.

406
00:35:17,768 --> 00:35:21,530
Their networks keep their shape, so you can constantly change them while you're tweaking and observing.

407
00:35:22,030 --> 00:35:29,594
And also, fundamentally, you can get some of the most complex behavior with a single reward in a really well-made environment.

408
00:35:31,279 --> 00:35:33,302
And yeah, so that's setting rewards.

409
00:35:33,862 --> 00:35:41,393
And while you're actually observing, we wanted to make it important that you could see the rewards being kind of accumulated at the right time.

410
00:35:41,733 --> 00:35:45,699
In this case, this little guy is receiving a lot of rewards because he's receiving

411
00:35:46,237 --> 00:35:57,628
just one for holding onto the blockmaker to make sure he doesn't drop it, and then a lot when he's kind of using the blockmaker to make a block at the very specific height to gain crystals or to collect crystals.

412
00:35:57,968 --> 00:36:05,195
So you can kind of see and debug the experience just by watching them and seeing them gain these rewards.

413
00:36:06,455 --> 00:36:27,125
And it lets you kind of like figure out if you need to change anything and then so after that you gain these checkpoints So all the simulations are kind of being combined and kind of crushed in the background We got lots of running in the background And so we're averaging out kind of like all the different rewards and we're sending them back in so that you can see kind of like the average growth and progress of

414
00:36:27,573 --> 00:36:29,894
of your Anemo at the same time.

415
00:36:31,234 --> 00:36:33,975
So in this video, you can kind of see me just scrolling over.

416
00:36:34,295 --> 00:36:35,836
This is a lot of checkpoints collected.

417
00:36:35,896 --> 00:36:41,498
And you can see the averages of the rewards being collected changing over time.

418
00:36:41,838 --> 00:36:51,621
And most importantly, you can kind of see the ratio of the rewards changing over time, which actually became kind of like that's top tier players looking at the ratio change.

419
00:36:52,781 --> 00:36:54,922
But on top of that,

420
00:36:55,582 --> 00:37:03,565
Just going over end conditions really quickly again, we really needed to allow players to control how the simulations were being reset.

421
00:37:04,625 --> 00:37:08,287
Basically the first way to do that is how many turns in a single simulation you can run.

422
00:37:08,647 --> 00:37:14,089
You can run it at 50 turns or up to 200 turns, and this really depends on the kind of behavior you want out of it.

423
00:37:15,011 --> 00:37:21,093
And two, we wanted to end a simulation if a specific item of your choosing no longer exists.

424
00:37:21,373 --> 00:37:24,494
And this is basically to waste no training time.

425
00:37:24,834 --> 00:37:26,595
Training time is precious when we're doing this.

426
00:37:27,135 --> 00:37:36,779
And we also didn't want you to have a chance or the animal to have a chance to collect straggler rewards when it already completed its task.

427
00:37:37,339 --> 00:37:39,239
And that's a no-go as well.

428
00:37:39,259 --> 00:37:40,920
That can lead to awful things.

429
00:37:41,495 --> 00:37:41,615
and

430
00:37:59,086 --> 00:38:03,368
If you remember Sneaky and the optimization problem, that's still potentially a problem here.

431
00:38:04,228 --> 00:38:18,733
Fundamentally, you can kind of start a training and move an axe a bit further or tweak the environments to add a bit of noise on your own, but by starting and observing and stopping and starting, that actual process becomes quite a lot quite quickly.

432
00:38:19,522 --> 00:38:33,338
So we designed kind of the ability to kind of create a bit of noise in between resets Sadly, we call this wiggle and we can't ever turn back But basically players can control how much they want if they want their environment to change in between resets

433
00:38:33,755 --> 00:38:33,795
if

434
00:38:49,717 --> 00:38:56,948
not able to memorize that I need to walk forward, pick up this block maker, and then walk forward again and use it.

435
00:38:57,149 --> 00:39:04,500
So just really trying to get them to operate in the training space, but then operate in the real world with actual integrity.

436
00:39:08,077 --> 00:39:20,108
And just to kind of break down what's happening while you're seeing these kind of loops happen, even though you're seeing kind of the training happen in front of you, the training doesn't run in Unity itself.

437
00:39:20,488 --> 00:39:29,937
We actually started by doing that, but there was a lot of problems, the least of which was we had to write our own trainer, and that's harder than you may think.

438
00:39:30,938 --> 00:39:38,184
The other thing is like, sharing resources within the same process can often get in the way of the game itself, and we wanted to keep the game kind of running smooth while the training happened.

439
00:39:38,644 --> 00:39:40,766
So we ditched the whole thing and sent it to another process.

440
00:39:42,347 --> 00:39:48,132
So this, what happens when you hit play, it actually starts a Python process in the background, which actually does the training.

441
00:39:48,813 --> 00:39:54,178
And originally we were, as I said, we were using the ML Agents Unity trainer, but turns out running Unity is really expensive.

442
00:39:54,798 --> 00:39:57,439
So we got rid of Unity, too.

443
00:39:58,419 --> 00:40:04,222
Well, we didn't get rid of the Unity that you're seeing when you're playing the game, but we got rid of the actually Unity for running the simulation.

444
00:40:04,802 --> 00:40:12,125
What we ended up doing is we ended up taking all of the logic of the game and shipping it to Python so that the logic can run in Python completely uninterrupted.

445
00:40:12,765 --> 00:40:15,766
making use of all of those optimizations we've been mentioning throughout the talk.

446
00:40:16,506 --> 00:40:20,848
This made it so that the training runs in, like, blazingly fast.

447
00:40:20,888 --> 00:40:25,949
When you're watching a single, like, animo do a motion, it's performing hundreds and hundreds of steps.

448
00:40:26,790 --> 00:40:29,670
It takes, like, a hundred steps for every step it takes in-game.

449
00:40:30,011 --> 00:40:34,272
And that varies by your computer, but even on the slowest computers we were able to get a significant speed up by doing this.

450
00:40:34,592 --> 00:40:53,565
It's actually extremely laborious, and I go into detail later on in the paper you'll see at the end of the talk, but it took a lot of engineering to embed C Sharp into Python and be able to run the same logic that we use to run the game within the Python backend.

451
00:40:53,865 --> 00:41:02,731
And the other thing that this does too is that if we ever wanted to ship to mobile, it wouldn't be too difficult to take that architecture and just send it to the cloud and then just

452
00:41:03,151 --> 00:41:08,212
you don't have a mobile game where the training happens in the actual cloud, not just the training cloud.

453
00:41:08,612 --> 00:41:13,713
So yeah, this kind of architecture let us really speed up training.

454
00:41:13,954 --> 00:41:17,794
And I should have put a metric slide, but I'm just going to tell you.

455
00:41:18,495 --> 00:41:28,517
That sneaky behavior that you saw that Nick trained, it took 30 minutes to train that, to get someone to build a block maker and then use an axe to chop down either trees or obstacles.

456
00:41:29,398 --> 00:41:31,300
And that wasn't even a generalized behavior.

457
00:41:31,840 --> 00:41:36,465
In the launch version of Little Learning Machines, you can get that kind of behavior in like five minutes or less.

458
00:41:36,725 --> 00:41:39,068
And all the time, you're kind of watching your agent kind of get stuck.

459
00:41:39,348 --> 00:41:41,731
You have to change a couple things, and you have to run it again.

460
00:41:42,031 --> 00:41:47,076
And it creates a lot more of an interactive experience, which actually makes it viable as a game mechanic, in our opinion.

461
00:41:48,117 --> 00:41:56,040
So just to kind of summarize all of the tricks, the big bag of tricks on how to speed up your RL so you can put it in your game and have your players experience it while you're playing.

462
00:41:57,000 --> 00:41:58,180
Number one, discretize your space.

463
00:41:58,561 --> 00:42:01,121
It's much easier to make decisions about a discrete space.

464
00:42:01,782 --> 00:42:05,283
For example, if this was a standing room, would you know where to sit or stand?

465
00:42:05,363 --> 00:42:07,423
It's much easier if there's chairs that are discretized.

466
00:42:07,463 --> 00:42:08,644
You can make that decision for yourself.

467
00:42:09,925 --> 00:42:11,466
Number two, reduce your dimensions.

468
00:42:11,546 --> 00:42:23,255
As much as possible, if you can flatten it, do so, because making choices on multiple dimensions increases the arity of the computation you have to do, which uses more neurons.

469
00:42:24,156 --> 00:42:25,157
Discretize your time.

470
00:42:26,318 --> 00:42:27,639
The less decisions you have to make, the better.

471
00:42:27,939 --> 00:42:28,799
Discretize your actions.

472
00:42:32,262 --> 00:42:41,809
Leaving some of the intelligence to the animation and navigation systems, whether you're using a nav mesh or a grid, it really reduces the work that your network has to do.

473
00:42:42,349 --> 00:42:53,257
Exploit symmetries in your world, or design a world that has symmetries, so that your agents can reuse learning that they've done when facing north, and they don't have to relearn it when facing south.

474
00:42:53,437 --> 00:42:56,319
This is especially useful if you have visual observations that use lighting data.

475
00:42:58,340 --> 00:43:00,101
Next, exploit overlapping actions.

476
00:43:01,062 --> 00:43:03,903
For example, this is contextual actions in video games.

477
00:43:04,303 --> 00:43:07,705
Just like players, networks have a limited amount of verbs.

478
00:43:08,625 --> 00:43:12,688
So if you can use those very wisely, it really reduces your training time.

479
00:43:13,028 --> 00:43:14,348
And finally, just optimize your code.

480
00:43:14,929 --> 00:43:20,231
That's not really something that I can give you advice for, but Steve here has a lot of good advice for you if you need that.

481
00:43:20,612 --> 00:43:21,632
And then finally,

482
00:43:23,513 --> 00:43:24,434
a whole deck of cards.

483
00:43:25,174 --> 00:43:32,360
But yeah, that's all the tips and tricks on how to make a game that has RL happening in real time in front of the players.

484
00:43:33,581 --> 00:43:35,863
The last part of the talk is about making it into a game.

485
00:43:36,183 --> 00:43:41,026
This is just a little design indulgence where we talk about some of the reasons we made it into the game and why.

486
00:43:41,086 --> 00:43:45,810
We have this little training experience that's very packaged and novel, but what we were really

487
00:43:47,251 --> 00:43:53,155
Struggling with especially an early prototype is giving players a sense of well Why should I train or what should I train?

488
00:43:53,736 --> 00:44:00,921
So we kind of decided to go both take two approaches on this and kind of we were divided at the beginning But we decided I'm doing both.

489
00:44:01,482 --> 00:44:11,409
The first one is kind of like a player driven in Intrinsic motivation where we give the players a bunch of items and tools and we create interesting interactions between them and

490
00:44:11,709 --> 00:44:17,412
And we leave it up to the players to explore those interactions out of curiosity or a desire to see their machines grow.

491
00:44:17,992 --> 00:44:23,355
But then also to kind of challenge the players and kind of give them a sense of, hey, you can do this with an agent.

492
00:44:23,475 --> 00:44:23,915
We've tried.

493
00:44:24,255 --> 00:44:24,836
We've been able to.

494
00:44:25,556 --> 00:44:26,837
We give them these things called quests.

495
00:44:26,997 --> 00:44:28,237
So we kind of divide the game into two.

496
00:44:29,078 --> 00:44:31,079
So both of these are provided through islands.

497
00:44:31,259 --> 00:44:34,821
And it kind of creates an extension to the training loop we mentioned earlier.

498
00:44:35,901 --> 00:44:46,149
There's this idea of a player progression, where a player reaches a new island, they get new items, new animo maybe, new rewards to train with, and then they kind of get issued a challenge, whether that's intrinsic or extrinsic.

499
00:44:46,309 --> 00:44:48,611
Intrinsically, they're like, oh, there's a skull here.

500
00:44:48,951 --> 00:44:50,132
I wonder how this got here.

501
00:44:50,552 --> 00:44:51,893
Or, hey, this thing makes fire.

502
00:44:52,174 --> 00:44:53,034
What can I light on fire?

503
00:44:53,815 --> 00:44:58,778
And then to be able to use that item, the player can't directly interact with the world, they have to train their agents.

504
00:44:59,419 --> 00:45:04,643
This is similar to what Black and White did for some of their creatures, that they're the only ways in which you can achieve certain elements in the game.

505
00:45:06,245 --> 00:45:11,991
But then also we kind of provided these quests, which is like, hey, do this thing, which we know you can do.

506
00:45:12,332 --> 00:45:23,865
And that gives players kind of the motivation they need to kind of not get like, oh, I don't think the networks are smart enough to learn this, by guaranteeing basically that they are.

507
00:45:25,125 --> 00:45:26,406
And then this kind of ties into the training.

508
00:45:26,426 --> 00:45:29,807
So the whole thing is like the player goes into an island, figures something out.

509
00:45:29,907 --> 00:45:31,047
They're like, oh, I want to do that.

510
00:45:31,507 --> 00:45:32,628
And then they go into the training loop.

511
00:45:32,668 --> 00:45:35,288
And then they're like, oh, I got to tweak, observe.

512
00:45:35,528 --> 00:45:36,749
Is my agent doing the thing I want it to?

513
00:45:37,169 --> 00:45:37,689
Tweak it more.

514
00:45:38,049 --> 00:45:38,449
It's ready.

515
00:45:38,809 --> 00:45:39,930
Bring it down to the island.

516
00:45:40,830 --> 00:45:40,950
have

517
00:46:02,575 --> 00:46:11,600
do reinforcement learning in an academic setting, there's a set of things that you have to understand that aren't necessarily immediately intuitive from how you would, for example, train a dog.

518
00:46:11,980 --> 00:46:18,744
You have to realize that, oh, your agent can kind of get stuck doing a certain thing, or you have to teach it to do something before you teach it to do something else.

519
00:46:19,244 --> 00:46:28,068
Some of these are kind of commensurate with training an infant or a dog, but there are specific things that we think are unique to neural networks.

520
00:46:28,369 --> 00:46:28,749
For example,

521
00:46:30,149 --> 00:46:34,730
This is the first quest, and we don't want to overwhelm players.

522
00:46:34,750 --> 00:46:36,551
We don't want to introduce randomization right away.

523
00:46:37,051 --> 00:46:40,431
So we introduce the dog, and the dog kind of has randomization built into it for free.

524
00:46:40,731 --> 00:46:42,132
You pet the dog, and the dog runs away.

525
00:46:42,152 --> 00:46:42,932
It's like, come chase me.

526
00:46:42,952 --> 00:46:43,612
Come pet me over here.

527
00:46:44,032 --> 00:46:49,213
And that's basically recreating the same problem for your agent, and it's doing the randomization for you.

528
00:46:49,313 --> 00:46:50,874
So you don't have to modify the environment.

529
00:46:51,154 --> 00:46:53,514
You don't get stuck in the same trap that Sneaky got stuck in.

530
00:46:53,914 --> 00:46:55,015
And then the dog does that.

531
00:46:58,255 --> 00:47:00,877
that skill for you, and you don't have to learn it as a player.

532
00:47:01,677 --> 00:47:12,642
Later on, what's interesting is the second quest, you have to pet the dog in those cacti over there, and the first thing you notice is that your animal, that you have just trained to pet dogs, is starting to pet cacti.

533
00:47:13,102 --> 00:47:13,563
Guess what?

534
00:47:14,003 --> 00:47:17,905
It can't distinguish between either of them, because the cacti were not in the training data.

535
00:47:18,245 --> 00:47:22,107
You didn't train an animal that can pet dogs, you trained an animal that can pet anything in front of it.

536
00:47:23,267 --> 00:47:26,689
So it's gonna start petting the cacti and being like, I can't do anything with this.

537
00:47:27,869 --> 00:47:28,930
So, in conclusion,

538
00:47:49,408 --> 00:48:04,364
Basically, what we've learned and what we've been trying to tell you is something you might already know, that watching something learn is an amazing experience, whether it's watching you guys learn or a family member or a pet rat or something, or an ML agent.

539
00:48:04,505 --> 00:48:11,172
It's fun to watch something learn, especially when it's happening in front of you, and that by using careful design to train, you can actually

540
00:48:11,512 --> 00:48:14,274
train agents in real time as a gameplay mechanic.

541
00:48:15,355 --> 00:48:17,576
And we believe audiences are capable of this.

542
00:48:18,036 --> 00:48:19,417
It doesn't have to be a niche thing.

543
00:48:20,097 --> 00:48:23,419
It can be understood like any other game mechanic in time.

544
00:48:23,800 --> 00:48:28,102
It's kind of like what we always hope Pokemon kind of like promised us when we had children.

545
00:48:28,162 --> 00:48:28,723
Or Tamagotchi.

546
00:48:28,743 --> 00:48:29,563
Tamagotchi.

547
00:48:29,603 --> 00:48:30,844
Something that could actually live and learn.

548
00:48:30,864 --> 00:48:31,605
Watch your thing grow.

549
00:48:31,645 --> 00:48:32,365
Watch your thing learn.

550
00:48:32,425 --> 00:48:33,105
It actually learns.

551
00:48:33,165 --> 00:48:33,746
It actually grows.

552
00:48:35,013 --> 00:48:37,414
But we have a few reflections as well.

553
00:48:37,514 --> 00:48:39,516
Like, what did we learn from making this game?

554
00:48:40,977 --> 00:48:46,201
First off, learning should have not been just real time, but it should have been real time and continuous.

555
00:48:46,841 --> 00:48:52,585
We wanted to mimic the experience generally used in reinforcement learning that we talked about, the simulation space and the real world space.

556
00:48:53,406 --> 00:49:02,172
But in doing so, we limited them to kind of perform tasks of the user's desire, rather than having the agents actually potentially

557
00:49:02,720 --> 00:49:04,461
discover behaviors on their own.

558
00:49:05,341 --> 00:49:13,464
In the future, it would be nice to aim towards letting the agents teach us as much as we were inflicting our agents on them.

559
00:49:14,064 --> 00:49:22,987
And fundamentally, one of the problems that was in agents was that we talked about was you were just watching these pre-trained models.

560
00:49:23,527 --> 00:49:31,390
And in fact, we kind of repeated this mistake in little learning machines to some extent, because all the action really happens when you're training them in the cloud.

561
00:49:31,430 --> 00:49:32,510
And that's where you really find

562
00:49:32,850 --> 00:49:33,390
talk about

563
00:49:56,205 --> 00:50:06,335
These days when people talk about AI, and I'm sure lots of people are talking about AI here, they're often talking about large language models with billions, trillions of different neurons and parameters.

564
00:50:06,836 --> 00:50:12,982
But we believe that understanding the foundations of AI can actually happen with a lot of different scales of models.

565
00:50:13,222 --> 00:50:18,067
And we believe that there is a beauty to the small micro-examination of it.

566
00:50:18,567 --> 00:50:26,294
and that you don't need to use the newest tools, you can actually just use the tool that works best for you and just go deep into it.

567
00:50:27,495 --> 00:50:35,123
And then finally, while working on Little Living Machines, Dante heard me talk about this a lot, it was always the idea of the fruit fly.

568
00:50:35,503 --> 00:50:42,990
And if any of you have taken a biology course in university or anything, you know that we study the biology of a fruit fly or of a small worm,

569
00:50:43,290 --> 00:50:54,359
and we kind of like look at the neurons and look at all the factors of it in order to kind of project into a larger scale and teach us kind of like the fundamentals of, you know, what's some pretty amazing cool science.

570
00:50:54,840 --> 00:51:08,051
And in this way, I think that like playing with AI at these smaller scales actually creates a literacy and intuition about AI systems that are kind of like becoming ubiquitous, all-encompassing and somewhat scary.

571
00:51:08,411 --> 00:51:17,739
Yeah, like last week I was talking to someone about embeddings, and it was so much easier to say, this is how an animal sees a flower, and this is how your large language models see your words.

572
00:51:17,759 --> 00:51:19,321
They're just bits.

573
00:51:19,621 --> 00:51:20,102
Exactly.

574
00:51:20,442 --> 00:51:26,347
And using real-time learning as a game mechanic makes this sort of understanding and experience a lot more accessible.

575
00:51:26,547 --> 00:51:31,712
And fundamentally, I think we believe that this base level of understanding is more important

576
00:51:32,142 --> 00:51:32,542
than ever.

577
00:51:49,981 --> 00:51:51,942
Also, thanks to all the people that are listed here.

578
00:51:52,783 --> 00:51:54,544
We couldn't have done it without any one of them.

579
00:51:55,745 --> 00:52:00,610
And to the organizers of the AI Summit, a good first day, I would say.

580
00:52:00,690 --> 00:52:01,470
Very interesting talk.

581
00:52:01,590 --> 00:52:03,292
Yeah, great stuff.

582
00:52:12,460 --> 00:52:16,043
I know everyone wants to just get out of here, but do we have time for questions?

583
00:52:16,866 --> 00:52:19,244
Maybe, yeah, five minutes maybe.

584
00:52:21,339 --> 00:52:22,179
Six, seven minutes.

585
00:52:22,199 --> 00:52:23,059
That's what we're aiming for.

586
00:52:26,660 --> 00:52:28,301
And if there are none, that's OK, too.

587
00:52:28,381 --> 00:52:28,901
Go get a drink.

588
00:52:30,381 --> 00:52:31,321
I think we have one over there.

589
00:52:31,921 --> 00:52:32,521
Oh, awesome.

590
00:52:32,541 --> 00:52:32,881
Sorry.

591
00:52:33,302 --> 00:52:34,362
Thank you for the wonderful talk.

592
00:52:35,322 --> 00:52:38,303
When you're doing your simulations, are you under the hood?

593
00:52:38,603 --> 00:52:40,103
Are you doing many worlds in parallel?

594
00:52:40,543 --> 00:52:42,283
Or are you just doing one single world?

595
00:52:42,423 --> 00:52:44,324
So we started with doing many worlds in parallel.

596
00:52:44,464 --> 00:52:46,384
And we tell the players we do many worlds in parallel.

597
00:52:46,864 --> 00:52:49,565
It's much better for cache locality if you do one world at a time.

598
00:52:50,365 --> 00:52:56,010
I'd recommend you check out the mini-world simulation paper at SIGGRAPH 2023 from my lab mate, Brendan Shacklett.

599
00:52:56,951 --> 00:53:00,734
There's some work on high-performance GPU simulation of games using ECS systems.

600
00:53:01,175 --> 00:53:10,763
Yes, so when we first designed the system, the reason why it's grid-based is because we wanted to do a GPU-based simulation, but man, engineering is hard, man.

601
00:53:10,783 --> 00:53:11,563
Well, let's talk.

602
00:53:11,664 --> 00:53:12,004
Let's talk.

603
00:53:12,384 --> 00:53:12,925
Cool, cool, cool.

604
00:53:13,125 --> 00:53:13,465
Thank you.

605
00:53:14,426 --> 00:53:16,508
Yeah, let's alternate.

606
00:53:16,628 --> 00:53:16,808
Sure.

607
00:53:17,602 --> 00:53:18,403
Really wonderful talk.

608
00:53:18,663 --> 00:53:26,149
I think this is wonderful work, and maybe one of the largest repositories of RL behavior or trained RL models in the world.

609
00:53:26,729 --> 00:53:29,551
Well, I actually think it's true, and it's beautiful.

610
00:53:29,972 --> 00:53:31,633
You mentioned old techniques.

611
00:53:31,713 --> 00:53:34,915
Actually, about 20 years ago was a game called Nero.

612
00:53:36,537 --> 00:53:37,197
Oh, you guys are aware.

613
00:53:37,237 --> 00:53:37,698
Yeah, so.

614
00:53:39,119 --> 00:53:39,739
Oh, it's on the slides.

615
00:53:40,640 --> 00:53:41,520
It was on the first slide.

616
00:53:41,761 --> 00:53:42,601
Oh, I missed the first slide.

617
00:53:42,681 --> 00:53:43,142
Sorry about that.

618
00:53:43,962 --> 00:53:44,503
Was that you guys?

619
00:53:45,328 --> 00:53:45,929
No, no, no.

620
00:53:46,349 --> 00:53:50,473
My advisor, Ken Stanley.

621
00:53:54,496 --> 00:53:55,997
Julian's also done a lot of work in evolution.

622
00:53:56,077 --> 00:54:09,849
I'm wondering if you tried something like that because 512 by 4, even fully connected layers that large, neuroevolution techniques kind of go from the ground up kind of approach where you take as few nodes as possible and grow them.

623
00:54:10,733 --> 00:54:12,675
you know, many ways to skin a neural network.

624
00:54:13,356 --> 00:54:15,317
So I'm going to try to get ahead of a couple questions here.

625
00:54:15,658 --> 00:54:18,961
Often when I talk about this game, people ask, hey, why not tabular RL?

626
00:54:19,301 --> 00:54:20,282
Why not neuroevolution?

627
00:54:20,803 --> 00:54:30,212
And so one of the things that we really wanted to emphasize is not capturing the whole problem within the network itself.

628
00:54:30,913 --> 00:54:35,017
And this allows for the network to develop more complex behavior, but more specialized behavior

629
00:54:35,397 --> 00:54:37,899
that is not tuned to the whole problem.

630
00:54:38,419 --> 00:54:44,723
Tabular RL grows in dimensions really fast, and then neuroevolution has a problem where you kind of have to start from basic principles.

631
00:54:44,743 --> 00:54:58,251
You have to teach it how to navigate first, whereas neural networks just kind of learn everything at once, which, well, at least in our experience, and it turned out to be the best implementation for us to kind of follow in this.

632
00:54:59,071 --> 00:55:03,054
Hopefully that answers your question, but hopefully it matches your research as well.

633
00:55:06,360 --> 00:55:14,344
Like Black and White, you've got RL as part of the gameplay.

634
00:55:15,845 --> 00:55:17,406
Your QA team might want to talk to you.

635
00:55:17,626 --> 00:55:19,827
Other people want to put learning in games.

636
00:55:20,148 --> 00:55:31,834
Please check with your QA team about learning after ship, and make sure that it's the player that controls it and not the game, so your game doesn't go stupid or they accuse it of cheating because it got too smart because they figured them out.

637
00:55:33,475 --> 00:55:34,756
Any comments on that at all?

638
00:55:37,096 --> 00:55:37,176
I'm

639
00:55:51,479 --> 00:55:58,966
Which is a good thing for the technology, because then your QA team won't leave you in the parking lot one night when it goes stupid in the field otherwise.

640
00:55:58,986 --> 00:56:00,187
Because a player did it.

641
00:56:00,587 --> 00:56:04,490
And it's the player's problem if they teach it stupid or if they teach it too smart.

642
00:56:04,810 --> 00:56:05,291
I see.

643
00:56:05,331 --> 00:56:05,991
I see.

644
00:56:06,051 --> 00:56:09,414
Do you want to talk about the problems QAing this game?

645
00:56:10,175 --> 00:56:11,096
Yeah.

646
00:56:11,336 --> 00:56:12,397
I think you're bang on.

647
00:56:12,637 --> 00:56:17,121
And creating a game like this is extremely difficult to QA.

648
00:56:17,721 --> 00:56:29,368
and we were QA-ing it a lot ourselves before we could even pass it on, but fundamentally that so much can go wrong, and it does go wrong, that you kind of can't catch it.

649
00:56:29,469 --> 00:56:44,358
And I think there's a lot more work to do in this space in order to find where these behaviors actually break, and more than just QA-ing it, communicating properly why they broke,

650
00:56:44,778 --> 00:56:45,579
and at what time.

651
00:56:45,659 --> 00:56:53,105
And I think we tried our best in some elements of this, but we could have spent five more years doing that.

652
00:56:53,125 --> 00:56:58,309
One funny story is that one time, for a month, the agents could not see deep water.

653
00:56:58,610 --> 00:57:02,393
So the agents kind of disabled when they fall into deep water, and they couldn't see where deep water was.

654
00:57:02,433 --> 00:57:06,416
And they still worked, because they were able to kind of memorize a lot of the maps that you train them on.

655
00:57:07,057 --> 00:57:10,019
So there was a moment where we were telling QA, like, oh, no, they should learn.

656
00:57:10,059 --> 00:57:10,960
You're just training them wrong.

657
00:57:11,380 --> 00:57:13,342
And then QA is like, no, they're really stupid.

658
00:57:13,843 --> 00:57:17,086
And then we go in there, and we look at them, and they're like, oh, yeah, they're broken.

659
00:57:17,346 --> 00:57:17,466
Sorry.

660
00:57:17,486 --> 00:57:18,627
They can't see water.

661
00:57:18,647 --> 00:57:20,029
They can't see water.

662
00:57:20,249 --> 00:57:20,669
Our bad.

663
00:57:21,951 --> 00:57:23,672
But yeah, good call.

664
00:57:23,933 --> 00:57:27,176
I think one more question, and I think we've got that side next.

665
00:57:28,397 --> 00:57:28,798
Hey, guys.

666
00:57:29,758 --> 00:57:31,280
Great talk, great work.

667
00:57:32,461 --> 00:57:33,802
First of all, there's lots of like, you know,

668
00:57:34,802 --> 00:57:57,445
Particular technical things I'd like to comment on from having done some very similar stuff myself but I'm gonna refrain from that so we can take a whole long discussion over the overfitting problems and so on and and and GPU paralyzation, but first point is like not a question I think it's awesome that this happens and it must happen because this is the kind of work that

669
00:57:57,877 --> 00:58:04,983
If we're ever going to use machine learning in runtime in games in an interesting manner, we need to design around it.

670
00:58:05,023 --> 00:58:09,567
We need to leave old paradigms that are designed for the lack of AI.

671
00:58:10,107 --> 00:58:13,149
And we need to leave the thinking that we can ever QA it.

672
00:58:13,190 --> 00:58:20,936
We need to be able to step out into the bold new future and do things that are built on abilities, not on lack of abilities of AI.

673
00:58:21,436 --> 00:58:22,457
So I think it's great work.

674
00:58:24,367 --> 00:58:29,229
The question here is like, shouldn't this be much more of a social game?

675
00:58:29,549 --> 00:58:49,115
And why should, and the question is, and the reasoning for this is that there's always going to be a tension between wanting things to learn fast and that learning something that is interesting is always going to take a long time, at least in interactions.

676
00:58:49,496 --> 00:58:53,477
And isn't this something that should happen over kind of a social timescale between players?

677
00:58:54,993 --> 00:58:57,934
I love that question, because I think that's just a great idea.

678
00:58:58,234 --> 00:59:06,917
And why we want to give this talk, we framed a lot of this game over teaching fundamentals.

679
00:59:07,177 --> 00:59:09,898
And maybe we did our best at teaching that.

680
00:59:09,918 --> 00:59:18,181
And I think if there is more of a base level understanding of reinforcement learning, you can quickly get to games and experience social games that you're talking about.

681
00:59:18,401 --> 00:59:19,521
Because I totally agree.

682
00:59:19,541 --> 00:59:23,543
I think that is the best use scenario for this kind of project.

683
00:59:25,036 --> 00:59:27,265
And I gotta end it, but thank you so much.

684
00:59:27,847 --> 00:59:29,252
I totally agree with you, and thank you.

