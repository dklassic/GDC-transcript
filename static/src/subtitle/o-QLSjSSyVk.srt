1
00:00:07,973 --> 00:00:12,735
Okay, so these are two Assassin's Creed games,

2
00:00:13,075 --> 00:00:15,156
Syndicate and Origins, the most recent ones.

3
00:00:15,176 --> 00:00:16,376
So Syndicate came out in 2015,

4
00:00:16,416 --> 00:00:19,837
and Origins came out in 2017.

5
00:00:20,598 --> 00:00:23,558
And if we look at the size of the maps in this game,

6
00:00:24,139 --> 00:00:26,379
we'll see that it's grown incredibly.

7
00:00:26,459 --> 00:00:30,561
So it's grown about 250 times larger

8
00:00:31,241 --> 00:00:32,501
in the span of a couple of years.

9
00:00:33,001 --> 00:00:34,402
So this is like the kind of,

10
00:00:35,322 --> 00:00:40,006
increase in scope which players expect from Ubisoft games.

11
00:00:40,967 --> 00:00:45,030
But this talks about animation, so does anyone want to hazard a guess how many

12
00:00:45,210 --> 00:00:47,592
animations were in Assassin's Creed Origins?

13
00:00:49,333 --> 00:00:54,217
So, there was roughly 15,000 animations in this game.

14
00:00:54,677 --> 00:00:58,440
So, it had like a 3 year development cycle.

15
00:00:59,101 --> 00:01:03,344
So that means on average there was about 20 animations added a day.

16
00:01:05,026 --> 00:01:08,709
So that's 20 animations that need to be recorded in the motion capture studio,

17
00:01:09,170 --> 00:01:11,272
cleaned up by motion capture technicians.

18
00:01:11,832 --> 00:01:15,616
They need to be processed by animators, edited, tweaked.

19
00:01:16,076 --> 00:01:18,178
And then they need to be put in game by a designer.

20
00:01:19,059 --> 00:01:21,502
And they need to be tested and QC'd by testers.

21
00:01:21,542 --> 00:01:23,884
So that's an incredible, incredible amount of work.

22
00:01:24,585 --> 00:01:28,706
And the question is, what are people going to expect in a few years time from Ubisoft

23
00:01:28,726 --> 00:01:30,086
games in regards to animations?

24
00:01:30,266 --> 00:01:34,507
Are they going to expect 100,000 animations or a million animations?

25
00:01:35,427 --> 00:01:37,348
What is the sort of scale we're talking about?

26
00:01:37,488 --> 00:01:45,610
So I work for La Forge, which is Ubisoft's main R&D department in Montreal.

27
00:01:46,270 --> 00:01:50,031
And one of the initiatives I'm working on is basically trying to tackle this question.

28
00:01:50,071 --> 00:01:54,372
So how can we prepare for this scaling up, which is bound to happen?

29
00:01:55,332 --> 00:01:59,177
And I've been working with a bunch of people there, in particular Simon Clavet,

30
00:01:59,657 --> 00:02:01,919
who did some previous work on motion matching.

31
00:02:02,780 --> 00:02:07,625
And so today, what I really want to show to you is our philosophy

32
00:02:07,726 --> 00:02:11,450
and how we've been thinking about how we might be able to scale these systems up.

33
00:02:12,511 --> 00:02:15,214
Okay, so let's have a look at the background.

34
00:02:17,437 --> 00:02:21,202
The most high-level overview of an animation system looks something like this.

35
00:02:21,823 --> 00:02:23,205
We have some player input.

36
00:02:23,846 --> 00:02:29,253
This goes into this black box animation system, and as output we get a pose.

37
00:02:30,704 --> 00:02:32,686
We don't actually send the button presses directly.

38
00:02:33,486 --> 00:02:35,928
What we do is we convert them into some kind of high-level

39
00:02:35,988 --> 00:02:38,490
representation of what we want, like where we want the

40
00:02:38,510 --> 00:02:41,492
player to go, which direction we want them to be facing,

41
00:02:41,532 --> 00:02:42,313
these sorts of things.

42
00:02:42,613 --> 00:02:44,414
So this we're going to call gameplay.

43
00:02:45,335 --> 00:02:47,737
And one nice thing is that we can also use inputs from

44
00:02:47,897 --> 00:02:50,258
NPCs or other sorts of controllers.

45
00:02:52,422 --> 00:02:56,349
So what we actually do with this animation system,

46
00:02:56,389 --> 00:02:59,635
typically, or at least the way it's been done in many of the

47
00:02:59,675 --> 00:03:02,881
large Ubisoft games, is that basically here we

48
00:03:02,921 --> 00:03:03,722
have a state machine.

49
00:03:05,179 --> 00:03:08,481
And we have a bunch of different nodes in the state machine,

50
00:03:08,922 --> 00:03:11,684
and each of them represent a state the character can be in,

51
00:03:12,144 --> 00:03:15,527
and that roughly means what animation the player is playing.

52
00:03:15,607 --> 00:03:16,948
So here we might have a state,

53
00:03:17,248 --> 00:03:19,070
which means the character is in locomotion.

54
00:03:19,470 --> 00:03:20,651
And we can click on this state,

55
00:03:21,572 --> 00:03:23,033
and what happens is it pops open,

56
00:03:23,273 --> 00:03:25,255
and inside are many, many more states.

57
00:03:26,276 --> 00:03:28,318
And we can click on another one of these states,

58
00:03:28,378 --> 00:03:30,299
so maybe we click on a walking state.

59
00:03:31,235 --> 00:03:33,578
And inside, there's a whole bunch more states.

60
00:03:33,658 --> 00:03:36,421
So it's like hierarchical, this state machine.

61
00:03:36,521 --> 00:03:39,946
Inside are more states and more transitions and more logic

62
00:03:40,026 --> 00:03:41,828
for how to move between states.

63
00:03:42,889 --> 00:03:44,471
So maybe we click on another one, like a.

64
00:03:45,072 --> 00:03:49,156
Maybe this is like the turn right mode of the walking state of locomotion.

65
00:03:50,197 --> 00:03:51,878
And it pops up another state machine.

66
00:03:52,839 --> 00:03:54,461
And we can click again.

67
00:03:54,501 --> 00:03:59,265
So maybe here we have the turning right to idle state of the walking state

68
00:03:59,285 --> 00:04:00,766
machine of locomotion, et cetera.

69
00:04:01,427 --> 00:04:02,568
And we can click on this one.

70
00:04:03,008 --> 00:04:04,650
And what it pops open is something different.

71
00:04:04,890 --> 00:04:06,972
So inside here we have a blend tree.

72
00:04:07,814 --> 00:04:13,059
And a blend tree is basically saying how can we blend a bunch of different animations and

73
00:04:13,139 --> 00:04:17,323
what conditions are we using to blend them to produce the final pose we give as output.

74
00:04:17,343 --> 00:04:19,686
So we can click on one of these nodes in the blend tree as well.

75
00:04:20,587 --> 00:04:22,348
Oh, and it pops up another blend tree.

76
00:04:22,409 --> 00:04:24,511
So this thing is like hierarchical as well.

77
00:04:26,556 --> 00:04:27,998
And we can go down again.

78
00:04:28,459 --> 00:04:30,442
And now maybe finally we get to the data.

79
00:04:30,462 --> 00:04:33,366
Okay, so now we actually have inside this final node

80
00:04:33,386 --> 00:04:37,212
all the way down, we have a clip which we've recorded

81
00:04:37,272 --> 00:04:38,393
in the motion capture studio.

82
00:04:39,184 --> 00:04:43,628
maybe been touched up by animators, and the file name looks something crazy like this.

83
00:04:43,889 --> 00:04:49,314
So it's some sort of crazy string describing exactly what's happening in this clip.

84
00:04:50,275 --> 00:04:51,896
And so our data flow looks like this.

85
00:04:52,157 --> 00:04:56,641
We get our input from gameplay, we're going all the way through this crazy state machine,

86
00:04:56,741 --> 00:05:00,785
through the blend tree, all the way to get to the data here, and then we're outputting

87
00:05:00,825 --> 00:05:02,346
the pose of the character based on this.

88
00:05:03,544 --> 00:05:07,706
And you may think I'm exaggerating for comic effect,

89
00:05:08,066 --> 00:05:09,546
but actually it's much, much worse

90
00:05:09,606 --> 00:05:11,047
than you could ever imagine.

91
00:05:11,147 --> 00:05:14,408
So like I said, there's 15,000 animations.

92
00:05:15,029 --> 00:05:16,869
This is for the latest Assassin's Creed.

93
00:05:16,909 --> 00:05:18,470
I asked them to run some stats for me.

94
00:05:18,990 --> 00:05:20,491
About 15,000 animations,

95
00:05:20,851 --> 00:05:23,452
about 5,000 states in this state machine.

96
00:05:24,052 --> 00:05:25,953
And it's about 12 levels deep.

97
00:05:26,553 --> 00:05:29,174
So that's the kind of magnitude we're talking about.

98
00:05:30,621 --> 00:05:33,944
And OK, so you may think, OK, so that's how it is.

99
00:05:34,084 --> 00:05:37,687
But it shipped many, many successful games and made lots

100
00:05:37,727 --> 00:05:38,348
and lots of money.

101
00:05:38,408 --> 00:05:39,188
So what's the problem?

102
00:05:39,929 --> 00:05:41,110
So the problem is this, right?

103
00:05:41,841 --> 00:05:42,702
we have a director.

104
00:05:43,262 --> 00:05:44,803
And the director comes to you one day,

105
00:05:44,823 --> 00:05:46,584
and you're maintaining the state machine.

106
00:05:47,124 --> 00:05:50,306
And he says, we want the player character

107
00:05:50,346 --> 00:05:51,367
to be able to be injured.

108
00:05:51,407 --> 00:05:53,268
So we want him to be walking around injured.

109
00:05:54,108 --> 00:05:55,870
And so you're maintaining the state machine,

110
00:05:55,890 --> 00:05:58,271
and you think, well, what could I do to add this

111
00:05:58,311 --> 00:05:59,132
to the state machine?

112
00:05:59,292 --> 00:06:01,153
OK, so one thing you could do is you

113
00:06:01,173 --> 00:06:03,854
could add an injured node at every single leaf state

114
00:06:04,355 --> 00:06:07,036
and just have a kind of transition which switches

115
00:06:07,076 --> 00:06:08,237
between being injured and not.

116
00:06:09,594 --> 00:06:11,796
But some states don't make sense when injured.

117
00:06:11,856 --> 00:06:14,878
So maybe some whole parts of this hierarchical state

118
00:06:14,898 --> 00:06:16,979
machine don't really make sense when you're injured.

119
00:06:17,319 --> 00:06:20,041
So you need to do something like go through this whole

120
00:06:20,081 --> 00:06:22,963
state machine and work out when it makes sense to be

121
00:06:22,983 --> 00:06:23,884
injured and when it doesn't.

122
00:06:25,505 --> 00:06:27,666
Another option is we could kind of duplicate this whole

123
00:06:27,706 --> 00:06:31,449
graph and replace all the data in it with injured versions of

124
00:06:31,489 --> 00:06:32,250
the same data.

125
00:06:33,590 --> 00:06:35,432
But we sort of have the same issue, which is that

126
00:06:36,630 --> 00:06:39,972
Some states or some leafs we might not have recorded

127
00:06:40,032 --> 00:06:42,614
any injured data or injured data might not make sense.

128
00:06:42,974 --> 00:06:44,675
So we might have to kind of jump back

129
00:06:44,755 --> 00:06:46,577
to the original tree at random points.

130
00:06:48,438 --> 00:06:50,640
And finally, we could just try and hack something together

131
00:06:50,700 --> 00:06:51,340
for this case.

132
00:06:52,921 --> 00:06:55,463
But of course, if you do this and you have too many hacks,

133
00:06:55,523 --> 00:06:57,004
then the technical debt builds up

134
00:06:57,364 --> 00:06:59,106
and you start to get in a lot of trouble.

135
00:06:59,746 --> 00:07:01,827
But anyway, your boss wants you to do this,

136
00:07:01,947 --> 00:07:03,068
so you have to do something.

137
00:07:03,529 --> 00:07:04,369
So you do something.

138
00:07:05,250 --> 00:07:08,793
and then sometime later the director comes back to you again

139
00:07:09,213 --> 00:07:13,356
and he says great so the next scene the character is injured he's also tired

140
00:07:13,817 --> 00:07:18,080
and he gets stabbed in the eye halfway through so we need locomotion for all of

141
00:07:18,120 --> 00:07:21,102
these different things so then your action is something like

142
00:07:21,162 --> 00:07:21,343
this

143
00:07:24,245 --> 00:07:28,048
so the dream the dream would be a setup more like this okay

144
00:07:28,949 --> 00:07:33,232
so day one the director comes to you we want the player character to be injured

145
00:07:34,252 --> 00:07:36,693
Day two, we go to the motion capture studio.

146
00:07:37,113 --> 00:07:38,734
We capture a bunch of injured motion.

147
00:07:38,814 --> 00:07:39,994
We'd limp around or whatever.

148
00:07:41,635 --> 00:07:43,576
Day three, we grab all these files

149
00:07:43,616 --> 00:07:44,877
from the motion capture studio.

150
00:07:45,257 --> 00:07:46,938
We drag and drop them into our system.

151
00:07:48,038 --> 00:07:49,299
And day four, everything works.

152
00:07:51,600 --> 00:07:53,721
Day five, director says, okay,

153
00:07:54,001 --> 00:07:56,683
now we want the character to be injured, tired,

154
00:07:56,763 --> 00:07:58,223
and he gets stabbed in the eye.

155
00:07:58,824 --> 00:08:00,524
So we do this at the motion capture studio.

156
00:08:00,564 --> 00:08:02,105
We go and stab ourselves in the eye.

157
00:08:04,114 --> 00:08:07,597
drag and drop, everything works, great.

158
00:08:08,078 --> 00:08:10,480
Okay, so what this really is talking about

159
00:08:10,520 --> 00:08:11,661
is like scalability.

160
00:08:11,701 --> 00:08:15,344
So how can we have a process for building animation systems

161
00:08:15,384 --> 00:08:18,026
which is scalable and it doesn't give us a headache

162
00:08:18,066 --> 00:08:19,507
every time we wanna add something new?

163
00:08:20,548 --> 00:08:22,630
So that's what this talk is all about.

164
00:08:23,311 --> 00:08:25,953
And these are the three kind of ideas in this talk.

165
00:08:26,553 --> 00:08:29,356
And really these are ideas that come from

166
00:08:30,364 --> 00:08:31,564
machine learning in some way.

167
00:08:31,644 --> 00:08:34,765
So what we ideally want is a generalized solution, which

168
00:08:34,805 --> 00:08:36,245
can work for many different cases.

169
00:08:37,185 --> 00:08:38,506
And to get this, what we need to do

170
00:08:38,566 --> 00:08:41,626
is specify exactly what variables we want in the system.

171
00:08:42,666 --> 00:08:44,507
And to get this working well, we need

172
00:08:44,527 --> 00:08:46,567
to think more carefully about how we manage our data.

173
00:08:46,607 --> 00:08:48,067
So these are kind of the three stages

174
00:08:48,127 --> 00:08:48,888
I'm going to talk about.

175
00:08:49,968 --> 00:08:51,888
So first, we'll talk about data separation.

176
00:08:53,769 --> 00:08:55,749
So if we have a look again at our data.

177
00:08:56,858 --> 00:08:59,843
set up which we had before, there's kind of an awkward thing which is that

178
00:09:00,784 --> 00:09:04,209
conceptually all the data is living inside this state machine.

179
00:09:05,269 --> 00:09:10,693
So the first kind of conceptual step is to have a separate database and to pull your

180
00:09:10,733 --> 00:09:17,098
data out and have it live in the database and have your state machine, rather than outputting

181
00:09:17,138 --> 00:09:21,902
a pose, actually output a kind of a pointer of some kind or a file name with a time.

182
00:09:22,282 --> 00:09:25,825
So of course you're thinking, yeah, obviously this is how we actually do it in practice.

183
00:09:25,905 --> 00:09:29,187
Like we don't actually have the data living inside the state machine.

184
00:09:30,008 --> 00:09:32,330
But there's like an important conceptual difference here.

185
00:09:33,974 --> 00:09:36,456
And if we want to blend, well, we can have the blending kind

186
00:09:36,476 --> 00:09:38,778
of happen after this data retrieval stage.

187
00:09:39,239 --> 00:09:42,502
We ask for multiple different animations, and we get the

188
00:09:42,522 --> 00:09:43,363
different weights as well.

189
00:09:45,151 --> 00:09:48,934
And the kind of important conceptual reason why we want

190
00:09:48,954 --> 00:09:54,418
to separate out this database is that the first thing we can

191
00:09:54,458 --> 00:09:59,161
do, which is really nice, is get rid of this craziness with

192
00:09:59,181 --> 00:09:59,842
the file name.

193
00:10:00,422 --> 00:10:03,664
So we have our file names in our database, and we have our

194
00:10:03,745 --> 00:10:05,886
file name or some sort of pointer coming from the state

195
00:10:05,926 --> 00:10:09,289
machine, and the first kind of step is to basically replace.

196
00:10:10,129 --> 00:10:14,373
all of these things which were in this description, the file name, with tags.

197
00:10:14,453 --> 00:10:19,997
So now what the state machine outputs is basically a list of tags for what it desires the motion

198
00:10:20,038 --> 00:10:20,558
to be like.

199
00:10:21,439 --> 00:10:23,881
And we can have similar tags in our data.

200
00:10:23,961 --> 00:10:31,407
So for example, this is like a little prototype tool we have for tagging our data.

201
00:10:31,467 --> 00:10:34,069
So here we have a really long take.

202
00:10:34,109 --> 00:10:37,112
This is about 15 minutes of raw animation data.

203
00:10:38,045 --> 00:10:42,009
And what we've done is we've gone and tagged all the different sections for what they represent.

204
00:10:42,690 --> 00:10:47,055
And in a sense this is the same as editing or cutting up this clip into small sections.

205
00:10:47,655 --> 00:10:50,158
And your tags can be as detailed or as simple as you like.

206
00:10:50,198 --> 00:10:55,264
So, for example, if you have a cut scene, you could have a unique tag just for this cut scene.

207
00:10:55,724 --> 00:10:57,366
And your state machine could output this tag.

208
00:10:58,389 --> 00:11:01,190
Or you can have very general tags, like just walking or

209
00:11:01,231 --> 00:11:03,292
locomotion or turning, these sorts of things.

210
00:11:03,712 --> 00:11:06,214
So you have a lot of flexibility when you use these

211
00:11:06,254 --> 00:11:07,655
tags instead of the file names.

212
00:11:08,375 --> 00:11:10,857
And you can also have multiple ranges inside these files, so

213
00:11:10,877 --> 00:11:11,538
that's pretty nice.

214
00:11:13,573 --> 00:11:16,535
So already, there's a lot of nice things.

215
00:11:16,595 --> 00:11:19,137
So one thing is that your state machine development

216
00:11:19,177 --> 00:11:21,378
doesn't depend on your database.

217
00:11:21,398 --> 00:11:23,400
So you can update your state machine and update your

218
00:11:23,440 --> 00:11:24,481
database separately.

219
00:11:25,842 --> 00:11:28,383
You can also swap out your database for new characters.

220
00:11:28,483 --> 00:11:30,645
So you can keep your state machine logic exactly the

221
00:11:30,665 --> 00:11:32,887
same, and just swap out the database if you have a

222
00:11:32,927 --> 00:11:34,047
different style of character.

223
00:11:35,069 --> 00:11:36,990
Or you can have some sort of fallback database.

224
00:11:37,491 --> 00:11:40,494
So if the state machine requests a set of tags which

225
00:11:40,554 --> 00:11:42,776
are not in your database, you can fall back to some more

226
00:11:42,816 --> 00:11:43,677
general database.

227
00:11:45,259 --> 00:11:49,263
And the overall kind of idea is that let's move away from

228
00:11:49,323 --> 00:11:52,386
thinking about assets and start thinking about databases

229
00:11:52,446 --> 00:11:53,167
as a whole.

230
00:11:54,328 --> 00:11:56,870
And move away from kind of file names and start thinking

231
00:11:56,910 --> 00:11:57,571
about tags.

232
00:11:58,626 --> 00:12:00,967
and have a separate process for motion retrieval.

233
00:12:01,027 --> 00:12:05,508
So what we've basically done is kind of got this classic game

234
00:12:05,528 --> 00:12:08,449
development set up with file names and assets and tags

235
00:12:08,609 --> 00:12:10,490
and editing, these sorts of things.

236
00:12:10,530 --> 00:12:13,171
And we've moved and put it into a database, which

237
00:12:13,251 --> 00:12:16,051
is really how machine learning people think about data.

238
00:12:17,432 --> 00:12:18,612
So that's the first stage done.

239
00:12:19,152 --> 00:12:21,453
So now I'm going to talk about specifying

240
00:12:21,493 --> 00:12:22,453
the desired variables.

241
00:12:24,874 --> 00:12:24,954
OK.

242
00:12:27,040 --> 00:12:29,122
let's have a look at our setup again.

243
00:12:29,222 --> 00:12:31,344
So, okay, we've improved some things,

244
00:12:31,504 --> 00:12:35,788
but we still have this really huge state machine here.

245
00:12:36,228 --> 00:12:39,051
And this state machine is kind of a complex thing

246
00:12:39,091 --> 00:12:41,913
because it's a mix of gameplay and animation.

247
00:12:41,994 --> 00:12:44,155
So some states are purely aesthetic,

248
00:12:44,216 --> 00:12:48,519
so maybe like a turn 25 degrees or something like this.

249
00:12:48,820 --> 00:12:50,801
It doesn't really have any meaning in gameplay.

250
00:12:51,222 --> 00:12:52,623
It's just there so that we can play

251
00:12:52,663 --> 00:12:53,884
a slightly different animation.

252
00:12:54,745 --> 00:12:56,786
And we have some states which are important for game play.

253
00:12:56,806 --> 00:13:00,349
So maybe if the character is falling over, or if the

254
00:13:00,369 --> 00:13:03,531
character is doing a roll, that's actually a different

255
00:13:03,631 --> 00:13:04,512
thing in game play.

256
00:13:04,572 --> 00:13:07,614
And you can perform different actions

257
00:13:07,654 --> 00:13:08,714
depending on these states.

258
00:13:09,755 --> 00:13:11,877
So one thing which would be really nice is if we could

259
00:13:11,937 --> 00:13:16,480
separate out this big state machine into purely game play

260
00:13:16,740 --> 00:13:19,382
related states and purely aesthetic ones.

261
00:13:20,551 --> 00:13:21,612
So one thing we can do is this.

262
00:13:22,332 --> 00:13:25,995
We get our state machine, and we get just the kind of

263
00:13:26,035 --> 00:13:27,876
gameplay, simplified gameplay version.

264
00:13:28,157 --> 00:13:32,740
So only states which have some meaningful thing to do with

265
00:13:32,780 --> 00:13:34,921
gameplay that we keep, and all the aesthetic

266
00:13:34,961 --> 00:13:35,762
states we remove.

267
00:13:37,408 --> 00:13:41,490
And we pull in all the other data related to the aesthetics from outside.

268
00:13:42,030 --> 00:13:44,792
So for example, the fact that the character is male,

269
00:13:45,132 --> 00:13:49,094
this is kind of like a global variable or something we can get in from outside.

270
00:13:49,915 --> 00:13:53,217
Or things from gameplay like where we want the character to be going,

271
00:13:54,157 --> 00:13:56,698
what speed they want to be going at, where they want to be looking.

272
00:13:57,139 --> 00:13:59,400
Lots of these things have nothing to do with gameplay.

273
00:13:59,440 --> 00:14:02,001
They're not managed, they're not, they don't change the gameplay at all,

274
00:14:02,041 --> 00:14:02,902
they're purely aesthetic.

275
00:14:03,858 --> 00:14:06,680
So we can pull these right through and kind of bypass

276
00:14:06,760 --> 00:14:08,160
this state machine.

277
00:14:10,201 --> 00:14:15,024
And we can tag exactly the same variables and exactly the

278
00:14:15,084 --> 00:14:17,225
same properties in our data.

279
00:14:19,326 --> 00:14:21,407
So these are actually numerical values.

280
00:14:21,467 --> 00:14:22,987
So that's kind of the main difference now.

281
00:14:23,027 --> 00:14:23,908
They're not just tags.

282
00:14:24,248 --> 00:14:26,089
Some of these are like numerical values, which we

283
00:14:26,129 --> 00:14:27,550
also have labeled in our database.

284
00:14:28,777 --> 00:14:31,538
So we need to kind of change a little bit how we look up

285
00:14:33,340 --> 00:14:34,521
which clip to play next.

286
00:14:35,201 --> 00:14:38,583
And the basic idea you can use is just filter out the clips

287
00:14:38,623 --> 00:14:43,407
where the tags don't match and return the nearest numerical

288
00:14:43,447 --> 00:14:45,268
match for the rest of the numerical inputs.

289
00:14:46,629 --> 00:14:48,410
OK, so that's how we do this.

290
00:14:48,530 --> 00:14:51,752
And we're going to call this matching because it's not

291
00:14:51,812 --> 00:14:53,734
really like querying a database anymore.

292
00:14:54,054 --> 00:14:56,476
It's more like trying to find the best match for the desired

293
00:14:56,516 --> 00:14:56,856
inputs.

294
00:14:58,477 --> 00:15:03,880
And something kind of magical happens now, which is that this state machine sort of disappears

295
00:15:04,501 --> 00:15:05,281
into gameplay.

296
00:15:05,341 --> 00:15:10,084
So now maintaining the state machine is really like the role of the gameplay programmers.

297
00:15:10,845 --> 00:15:15,628
And as animation programmers, what we're really doing is the stuff on the left-hand side of

298
00:15:15,668 --> 00:15:15,848
this.

299
00:15:17,790 --> 00:15:18,110
So now...

300
00:15:19,212 --> 00:15:22,352
if we think that the game play is the state machines purely a game play

301
00:15:22,392 --> 00:15:22,912
construct

302
00:15:23,452 --> 00:15:27,193
now animation system looks more like this all we have is this matching

303
00:15:27,233 --> 00:15:28,093
component where

304
00:15:28,413 --> 00:15:31,334
we try and get the user input and we match it to something in the data

305
00:15:32,334 --> 00:15:34,835
there's one kind of extra step which is that

306
00:15:35,495 --> 00:15:39,295
we don't want gameplay to specify timing animation so we want them to say

307
00:15:39,636 --> 00:15:43,096
what kind of sort of animation they want but we don't want to say okay we want

308
00:15:43,496 --> 00:15:47,797
them we want an animation which is kind of halfway through playing or something

309
00:15:47,837 --> 00:15:48,197
like this

310
00:15:49,735 --> 00:15:54,858
And one thing we can do is we can use the previous pose which was output by our system

311
00:15:55,359 --> 00:15:57,660
to describe the timing animation.

312
00:15:57,720 --> 00:16:01,142
So maybe the previous pose was the character with the right foot down.

313
00:16:01,642 --> 00:16:06,245
So now we know when we match our next frame we want to output or our next clip we want

314
00:16:06,285 --> 00:16:09,206
to play that it should start with the right foot down.

315
00:16:11,177 --> 00:16:15,719
And this setup is basically extremely similar conceptually

316
00:16:16,079 --> 00:16:19,120
to what was shipped in For Honor and what was called

317
00:16:19,160 --> 00:16:20,321
motion matching at the time.

318
00:16:20,861 --> 00:16:23,942
So there was a gameplay state machine, and there was tags

319
00:16:24,102 --> 00:16:27,123
and numerical variables coming in, and there was this kind of

320
00:16:27,203 --> 00:16:29,024
matching happening with the database.

321
00:16:30,245 --> 00:16:32,766
So this is a little clip from For Honor where you can see a

322
00:16:32,806 --> 00:16:33,686
bit how it's working.

323
00:16:34,846 --> 00:16:37,607
So here we see all the different potential clips in

324
00:16:37,647 --> 00:16:39,368
the database which could be played next.

325
00:16:40,371 --> 00:16:42,894
And essentially, the system is picking the one which most

326
00:16:42,954 --> 00:16:44,956
closely matches the desired user input.

327
00:16:44,996 --> 00:16:47,779
So here, the desired user input is the red trajectory

328
00:16:47,819 --> 00:16:48,420
along the ground.

329
00:16:49,721 --> 00:16:51,683
And it's picking the clip which best matches that.

330
00:16:55,287 --> 00:16:57,970
And one of the really nice things about this setup is

331
00:16:58,030 --> 00:17:01,634
that, unlike this big state machine, new variables are

332
00:17:01,674 --> 00:17:02,554
really easy to add.

333
00:17:04,050 --> 00:17:07,411
So for example, if we had the injured,

334
00:17:08,032 --> 00:17:10,673
if we wanted to add an injured state to the character,

335
00:17:10,973 --> 00:17:12,494
we just have an additional input saying

336
00:17:12,514 --> 00:17:13,795
whether the character should be injured,

337
00:17:14,275 --> 00:17:17,376
and we add this as an additional tag in our database.

338
00:17:24,340 --> 00:17:27,001
So the idea here is that instead of states,

339
00:17:27,081 --> 00:17:29,863
we want to describe the animation we want by variables.

340
00:17:31,469 --> 00:17:33,631
Instead of querying this database, we want to have some

341
00:17:33,691 --> 00:17:36,494
sort of fuzzy matching where we just try and roughly get

342
00:17:36,534 --> 00:17:37,636
the best clip that matches.

343
00:17:38,417 --> 00:17:40,619
And we need to annotate these variables in the data.

344
00:17:40,679 --> 00:17:42,241
So we need to annotate these tags on

345
00:17:42,261 --> 00:17:43,443
these numerical variables.

346
00:17:44,989 --> 00:17:49,753
So that kind of gets us to the point of motion matching

347
00:17:50,233 --> 00:17:52,555
and something that's been shipped in For Honor

348
00:17:52,595 --> 00:17:54,517
and something which has been proven pretty effective.

349
00:17:55,097 --> 00:17:56,178
So what's next, right?

350
00:17:56,198 --> 00:17:57,179
What's the next step?

351
00:17:57,579 --> 00:17:58,760
How far can we push this?

352
00:17:59,361 --> 00:18:00,602
So that's the third point here,

353
00:18:00,642 --> 00:18:02,524
which is talking about generalizing the solution.

354
00:18:03,464 --> 00:18:05,206
So I have a minor warning, which is that

355
00:18:06,707 --> 00:18:10,009
When you talk about generalization, often the way

356
00:18:10,050 --> 00:18:11,631
we generalize is by using math.

357
00:18:11,751 --> 00:18:13,432
So there is a couple of equations.

358
00:18:14,333 --> 00:18:15,154
I hope that's OK.

359
00:18:17,416 --> 00:18:19,538
OK, so let's have a look at our setup again.

360
00:18:21,760 --> 00:18:23,982
The first thing we can do is we can move the pose over to

361
00:18:24,022 --> 00:18:24,742
the right-hand side.

362
00:18:24,802 --> 00:18:27,384
It doesn't really matter the fact that it's looping around.

363
00:18:27,424 --> 00:18:29,666
We can actually just consider it as part of the input.

364
00:18:31,422 --> 00:18:34,186
And we can think of this matching process

365
00:18:34,687 --> 00:18:36,389
as a mathematical function,

366
00:18:36,910 --> 00:18:38,992
which takes a big list of numbers,

367
00:18:39,593 --> 00:18:41,856
takes a vector and just produces another vector,

368
00:18:41,876 --> 00:18:43,298
so it produces a big list of numbers.

369
00:18:45,292 --> 00:18:47,074
And the way we can think about it is something like this.

370
00:18:47,715 --> 00:18:50,637
Let's say we have an input on the right here.

371
00:18:50,978 --> 00:18:53,340
The way we can represent it as a big list of numbers is

372
00:18:53,400 --> 00:18:54,101
something like this.

373
00:18:54,141 --> 00:18:57,244
So for example, we can have this sort of enumeration where

374
00:18:57,284 --> 00:19:00,547
we just give a one-hot vector to say whether the character

375
00:19:00,627 --> 00:19:02,008
is male or female.

376
00:19:03,169 --> 00:19:05,772
Similarly, for whichever kind of general state they're in,

377
00:19:05,832 --> 00:19:08,875
like locomotion, idle, falling, this can also be an

378
00:19:08,915 --> 00:19:09,515
enumeration.

379
00:19:10,750 --> 00:19:14,834
Similarly for the style and the numerical values we can just give directly.

380
00:19:14,894 --> 00:19:18,516
So this can be the position the character wants to be in in the future,

381
00:19:19,337 --> 00:19:21,839
the speed they want to be going at, where they're looking.

382
00:19:22,820 --> 00:19:23,961
We can also have the pose.

383
00:19:24,001 --> 00:19:27,924
So for example, if we had the pose as output as our Y here,

384
00:19:28,344 --> 00:19:32,027
we can represent it by using the position of each of the joints,

385
00:19:32,067 --> 00:19:33,728
position and rotation of each of the joints.

386
00:19:34,329 --> 00:19:36,671
So here we have the first joint position, first joint rotation,

387
00:19:37,892 --> 00:19:39,733
second joint position, second joint rotation.

388
00:19:40,782 --> 00:19:45,083
and then the final joint. So whatever we give as input and whatever we give as output

389
00:19:45,103 --> 00:19:47,865
we can represent them as two huge big lists of numbers

390
00:19:48,645 --> 00:19:52,546
and we can see our function, our matching as a mathematical function which maps

391
00:19:52,586 --> 00:19:53,247
from one to the other.

392
00:19:56,769 --> 00:20:01,311
When we think about our database in this regard, what we actually have are pairs of X's and

393
00:20:01,531 --> 00:20:04,133
Y's, pairs of X's and corresponding Y's.

394
00:20:04,193 --> 00:20:10,456
So for each pose in our database, each Y, we have all the associated variables and tags.

395
00:20:10,816 --> 00:20:12,697
So this is our kind of associated X.

396
00:20:14,558 --> 00:20:18,961
So our database now is really just pairs of X's and corresponding Y's.

397
00:20:20,683 --> 00:20:25,405
And our matching function is just a function that uses this database to map from X's to Y's.

398
00:20:26,405 --> 00:20:31,047
And this is exactly what we call supervised learning in the machine learning community.

399
00:20:31,127 --> 00:20:35,808
So it's using a database of X's and Y's to learn a mapping from X to Y.

400
00:20:39,150 --> 00:20:42,711
And we don't call this matching in the machine learning community, we call it a regression.

401
00:20:44,170 --> 00:20:48,135
So motion matching is a special case of this regression

402
00:20:48,656 --> 00:20:50,218
called nearest neighbor regression.

403
00:20:51,987 --> 00:20:56,251
So we can think about all these things in terms of existing machine learning concepts.

404
00:20:56,291 --> 00:20:57,873
So I'll explain how that works now.

405
00:20:58,433 --> 00:21:02,717
So basically in nearest neighbor regression what we're doing is we're taking our x,

406
00:21:03,598 --> 00:21:07,922
which we get as input, and we're calculating the distance to all the x's in our database

407
00:21:08,523 --> 00:21:12,767
and we're returning the y with the smallest distance to our x.

408
00:21:12,887 --> 00:21:15,850
So for example here we can calculate the distance to all these x's.

409
00:21:16,550 --> 00:21:19,411
And we see that this x1 has the smallest distance,

410
00:21:19,791 --> 00:21:20,992
so we return y1.

411
00:21:21,132 --> 00:21:22,432
We return the pose y1.

412
00:21:23,313 --> 00:21:25,393
So when we're doing this matching and finding

413
00:21:25,433 --> 00:21:26,934
the nearest numerical match, this

414
00:21:27,014 --> 00:21:29,495
is exactly the same as doing nearest neighbor regression.

415
00:21:29,515 --> 00:21:33,856
All right, so all of this has been a little bit conceptual,

416
00:21:33,876 --> 00:21:35,677
so let's actually see a real example.

417
00:21:37,037 --> 00:21:38,818
So here's some training data we use,

418
00:21:38,918 --> 00:21:40,078
or some of the training data.

419
00:21:40,498 --> 00:21:44,180
So it's really just a kind of really long, 15 minute

420
00:21:44,220 --> 00:21:46,220
take of unstructured locomotion.

421
00:21:48,756 --> 00:21:52,297
And we got about 200 megabytes of this training data

422
00:21:53,077 --> 00:21:54,717
with various different characters,

423
00:21:54,817 --> 00:21:55,498
all different things.

424
00:21:57,758 --> 00:21:59,759
And our input x, what we want to give

425
00:21:59,819 --> 00:22:03,799
is we're going to give the previous frame joint

426
00:22:03,819 --> 00:22:06,480
positions, so positions of the joints in the previous frame,

427
00:22:06,520 --> 00:22:08,120
their velocities, how fast they're moving.

428
00:22:09,401 --> 00:22:11,041
And also our target is going to be

429
00:22:11,321 --> 00:22:13,662
where we want the character to be in one second's time,

430
00:22:14,162 --> 00:22:15,842
what velocity we want them to be going at,

431
00:22:16,422 --> 00:22:18,363
and which direction we want them to be facing.

432
00:22:20,014 --> 00:22:23,436
And our output Y is actually going to be a block of animation.

433
00:22:23,496 --> 00:22:26,818
So we're going to output a one-second block of animation,

434
00:22:26,898 --> 00:22:30,540
all the joint positions and all the joint rotations for one second.

435
00:22:31,660 --> 00:22:35,562
And our function F, what we're going to do is we're going to call it every one second.

436
00:22:35,582 --> 00:22:39,564
So every time we need a new block, we're going to call it, get that new block and put it in.

437
00:22:40,285 --> 00:22:45,248
Or if the user input changes, they specify a new desired position or direction,

438
00:22:45,548 --> 00:22:47,889
we're going to call it straight away and get a new block straight away.

439
00:22:49,238 --> 00:22:51,420
So let's see what happens if we use nearest neighbor

440
00:22:51,460 --> 00:22:51,880
regression.

441
00:22:53,621 --> 00:22:55,462
So what we get is we get a system like this.

442
00:22:56,123 --> 00:22:59,105
So we have the character, and he's following the desired

443
00:22:59,365 --> 00:23:00,826
user input.

444
00:23:00,846 --> 00:23:04,228
And the most noticeable thing is that there's a click.

445
00:23:04,708 --> 00:23:07,650
So every time the nearest neighbor regression chooses a

446
00:23:07,690 --> 00:23:10,692
different clip to play from, you get this instantaneous

447
00:23:10,752 --> 00:23:12,193
jump where it's switching clips.

448
00:23:13,594 --> 00:23:15,275
But what we can do is we can add some blending.

449
00:23:16,384 --> 00:23:19,666
Okay, so now we have a little bit of blending between clips

450
00:23:20,047 --> 00:23:22,348
to make sure they kind of blend smoothly between each other.

451
00:23:22,909 --> 00:23:25,330
And we get quite a nice system, so it's pretty responsive.

452
00:23:26,111 --> 00:23:28,633
And most importantly, we get lots of kind of diverse,

453
00:23:28,913 --> 00:23:30,754
interesting locomotion.

454
00:23:30,794 --> 00:23:32,716
It doesn't look particularly formulaic.

455
00:23:34,657 --> 00:23:36,518
And we can get lots of different variety as well.

456
00:23:38,790 --> 00:23:43,032
So this result looks pretty much like most motion matching

457
00:23:43,052 --> 00:23:45,333
you've seen, because that's basically what it is.

458
00:23:45,493 --> 00:23:49,095
It's essentially doing motion matching, but under a more

459
00:23:49,135 --> 00:23:50,016
general framework.

460
00:23:51,577 --> 00:23:54,478
And the memory and runtime are both fairly reasonable.

461
00:23:54,598 --> 00:23:57,240
So we have about 200 megabytes of data, one millisecond.

462
00:23:57,280 --> 00:23:58,621
So it's obviously not fast.

463
00:23:59,381 --> 00:24:02,263
But for the amount of variety and the amount of different

464
00:24:02,683 --> 00:24:05,044
motions you see, and for the simplicity of it,

465
00:24:05,144 --> 00:24:05,705
it's pretty good.

466
00:24:08,932 --> 00:24:11,254
So there's kind of one interesting thing,

467
00:24:11,414 --> 00:24:13,416
if we frame this more generally, as we have,

468
00:24:13,876 --> 00:24:17,079
which is that there's not just kind of a few types

469
00:24:17,119 --> 00:24:19,341
of regression, there's actually an insane amount.

470
00:24:19,441 --> 00:24:21,303
So this is the contents page

471
00:24:21,383 --> 00:24:24,146
from a supervised learning library called scikit-learn.

472
00:24:24,946 --> 00:24:27,189
And we can see nearest neighbor regression is down here,

473
00:24:27,289 --> 00:24:29,070
so it's not some obscure edge case,

474
00:24:29,170 --> 00:24:31,773
it's actually quite a popular machine learning algorithm.

475
00:24:32,803 --> 00:24:34,744
And we can see there are some other ones, so for example,

476
00:24:35,284 --> 00:24:37,744
here's an algorithm called Gaussian processes.

477
00:24:37,804 --> 00:24:40,205
So these were pretty popular for a long time

478
00:24:40,945 --> 00:24:42,005
before neural networks.

479
00:24:42,946 --> 00:24:45,606
And basically, they do a smooth interpolation of the data.

480
00:24:46,306 --> 00:24:49,567
So let's see if we could just replace

481
00:24:49,727 --> 00:24:52,627
nearest neighbor regression with Gaussian processes

482
00:24:52,687 --> 00:24:53,468
and see what happens.

483
00:24:54,548 --> 00:24:58,209
So here we get our database, we train a Gaussian process,

484
00:24:58,289 --> 00:24:59,789
and we plug this in as our function f.

485
00:25:00,934 --> 00:25:02,375
So what happens is something like this.

486
00:25:03,096 --> 00:25:05,758
So it basically doesn't work, right?

487
00:25:06,298 --> 00:25:08,920
It looks pretty bizarre, and we're not really sure

488
00:25:08,940 --> 00:25:09,581
what's going on.

489
00:25:10,962 --> 00:25:15,405
And Gaussian processes, they're kind of limited in

490
00:25:15,445 --> 00:25:16,245
scalability.

491
00:25:16,346 --> 00:25:18,947
So they scale really poorly with the amount of data you

492
00:25:18,987 --> 00:25:19,488
train them on.

493
00:25:20,068 --> 00:25:23,070
And I could only use about 1,000 samples for training

494
00:25:23,111 --> 00:25:25,172
them, so maybe we just didn't use enough data.

495
00:25:25,192 --> 00:25:27,474
So let's look again at our contents page.

496
00:25:27,734 --> 00:25:29,575
Oh, so there's neural networks.

497
00:25:29,635 --> 00:25:29,856
Great.

498
00:25:31,089 --> 00:25:32,149
the hot thing at the moment.

499
00:25:32,589 --> 00:25:34,310
So we can try these and see what happens.

500
00:25:34,991 --> 00:25:39,253
So one nice thing about these is we've got virtually

501
00:25:39,313 --> 00:25:40,694
unlimited data capacity.

502
00:25:41,614 --> 00:25:43,735
And we can throw away the data once it's trained.

503
00:25:44,836 --> 00:25:47,477
And they're fast to evaluate with low memory usage.

504
00:25:47,517 --> 00:25:50,899
So as far as our goal of scalability is concerned, they

505
00:25:51,059 --> 00:25:54,101
seem like the ideal thing to use.

506
00:25:55,682 --> 00:25:59,764
OK, so how many of you know how a neural network works?

507
00:26:01,215 --> 00:26:02,476
Okay, a fair amount, that's good.

508
00:26:02,996 --> 00:26:05,778
So I'm gonna give my little five minute rundown

509
00:26:05,838 --> 00:26:06,719
of neural networks.

510
00:26:08,800 --> 00:26:11,062
So don't worry, it's just gonna be quick.

511
00:26:11,963 --> 00:26:15,865
So a neural network, like all machine learning algorithms,

512
00:26:15,945 --> 00:26:16,726
is just a function.

513
00:26:17,627 --> 00:26:19,988
So for example, this is a simple function

514
00:26:20,028 --> 00:26:21,650
you kind of recognize from high school.

515
00:26:23,391 --> 00:26:25,432
It takes some input and produces some output.

516
00:26:26,437 --> 00:26:31,021
And as we've seen, these are represented basically by vectors, big lists of numbers.

517
00:26:32,422 --> 00:26:37,446
And a single layer of a neural network is described by a function that looks like this.

518
00:26:38,547 --> 00:26:42,590
So these variables, w and b, are the weights of the neural network.

519
00:26:43,211 --> 00:26:49,596
So we have the input, x and y, the input and output, x and y, on either side.

520
00:26:51,330 --> 00:26:56,513
And the first operation we do is we multiply this big vector x by this weight matrix w.

521
00:26:57,834 --> 00:27:03,417
And we add another vector which is the bias b which is another set of weights for the

522
00:27:03,457 --> 00:27:03,858
network.

523
00:27:05,178 --> 00:27:08,080
And then we pass this through an activation function.

524
00:27:08,881 --> 00:27:13,443
And the activation function is basically a simple function which produces some sort of

525
00:27:13,523 --> 00:27:17,125
bend or non-linearity in the output.

526
00:27:19,518 --> 00:27:22,860
So it's actually super, super simple and super familiar.

527
00:27:22,980 --> 00:27:25,682
It looks exactly like this really basic function

528
00:27:25,722 --> 00:27:27,264
we can think of from high school.

529
00:27:28,464 --> 00:27:30,926
And when we stack, we have a deep neural network.

530
00:27:30,946 --> 00:27:33,788
We're basically just nesting this equation in upon itself.

531
00:27:35,583 --> 00:27:43,028
So this equation is going to be exactly the equation we use for our f in this machine

532
00:27:43,048 --> 00:27:43,688
learning problem.

533
00:27:44,049 --> 00:27:51,554
So it should be kind of immediately obvious how simple and how small and compact using

534
00:27:51,594 --> 00:27:52,634
this sort of function is.

535
00:27:52,975 --> 00:27:57,878
So it's really nothing complicated and the whole thing is encoded by these weights, these

536
00:27:58,178 --> 00:27:58,979
w's and these b's.

537
00:27:59,319 --> 00:28:02,401
So this is why we can throw away our database once it's trained.

538
00:28:03,903 --> 00:28:06,365
And the way we train it is fairly simple as well.

539
00:28:06,405 --> 00:28:09,647
So what we do is we put all of our x's in our database

540
00:28:09,707 --> 00:28:12,689
through this function, and we see what numbers they produce,

541
00:28:13,169 --> 00:28:15,431
and we compare them to the y's in our databases.

542
00:28:16,051 --> 00:28:18,493
And then we basically use this to update the network weights.

543
00:28:18,533 --> 00:28:20,654
So there's a bunch of different algorithms for doing this.

544
00:28:21,595 --> 00:28:23,676
And we repeat it thousands and thousands of times.

545
00:28:24,497 --> 00:28:26,518
And eventually we have the weights and biases

546
00:28:26,898 --> 00:28:28,719
which work for this particular function.

547
00:28:30,663 --> 00:28:34,985
I think it's kind of, you can see why it's exciting from a scalability standpoint to

548
00:28:35,065 --> 00:28:39,147
use neural networks because they're very, very small, very, very compact, and we can

549
00:28:39,247 --> 00:28:44,389
understand the size and the computational complexity of how they're working, and it's

550
00:28:44,429 --> 00:28:47,370
completely independent of the size of our training data.

551
00:28:48,431 --> 00:28:49,111
So we can do this.

552
00:28:49,171 --> 00:28:50,231
We train a neural network.

553
00:28:51,038 --> 00:28:54,921
and then we throw away our data and it looks something like this.

554
00:28:55,742 --> 00:28:58,804
So it's still basically not really working.

555
00:28:59,144 --> 00:29:03,267
It's doing kind of something, but it's not doing what we want.

556
00:29:03,668 --> 00:29:04,508
So why is this?

557
00:29:05,449 --> 00:29:09,972
The reason is that machine learning, it's not this magic black box.

558
00:29:10,012 --> 00:29:13,355
You can't just train this neural network with no regard for anything.

559
00:29:14,624 --> 00:29:20,926
And the results you're going to get are going to depend so much on how you represent your input,

560
00:29:20,986 --> 00:29:24,867
how you represent your output, and how and when you use this function f.

561
00:29:24,987 --> 00:29:30,209
Okay, so it's not as simple as, it's not as nice as just looking at this big contents

562
00:29:30,289 --> 00:29:33,870
page for supervised learning and just picking an algorithm and trying it out.

563
00:29:36,033 --> 00:29:40,416
And additionally there's something really bad about the problem we're trying to solve,

564
00:29:40,456 --> 00:29:42,377
which is this function f isn't well defined.

565
00:29:42,457 --> 00:29:46,319
So for example, if we have the character standing still and we say,

566
00:29:46,940 --> 00:29:49,941
go forward, so we just give him a target in the future to go forward,

567
00:29:50,422 --> 00:29:51,582
there's two different choices.

568
00:29:51,642 --> 00:29:55,204
So he could either lead with his left foot or he could lead with his right foot.

569
00:29:55,725 --> 00:29:58,526
And both of these choices will get him to the goal.

570
00:29:59,087 --> 00:30:02,428
But the neural network or whatever machine learning algorithm you're using,

571
00:30:02,729 --> 00:30:04,329
it doesn't know which one to choose.

572
00:30:04,569 --> 00:30:07,891
So what it tends to do is it just produces an average of them both.

573
00:30:08,612 --> 00:30:12,273
And this is what you see when you see the character kind of floating along the ground.

574
00:30:12,654 --> 00:30:15,915
It's an average of using your left foot and using your right foot.

575
00:30:17,696 --> 00:30:21,318
So the question is, can we resolve this ambiguity?

576
00:30:22,679 --> 00:30:27,603
Irregardless of tweaking our input and our output representation, it looks like we need

577
00:30:27,643 --> 00:30:31,326
to solve this problem first if we're going to have any chance of getting it working.

578
00:30:32,687 --> 00:30:35,569
So the answer is obviously yes, we can.

579
00:30:37,671 --> 00:30:39,913
And one way we can do it is by...

580
00:30:41,096 --> 00:30:45,138
specifying the timing directly. So we can use this concept of the phase

581
00:30:45,678 --> 00:30:50,980
to tell us exactly the timing of our left foot and our right foot and how we're

582
00:30:51,000 --> 00:30:52,340
cycling through our animation.

583
00:30:53,101 --> 00:30:53,921
So for example,

584
00:30:54,781 --> 00:30:59,063
the phase we can say it's a variable where when your left foot first goes down it's zero,

585
00:30:59,623 --> 00:31:01,284
when your right foot goes down it's pi,

586
00:31:01,664 --> 00:31:05,265
and when your left foot goes down again it's two pi. So it's this kind of cyclic

587
00:31:05,306 --> 00:31:07,746
variable which is cycling between zero and two pi.

588
00:31:09,730 --> 00:31:12,913
And then one idea is we could use a separate F

589
00:31:12,974 --> 00:31:15,316
depending on what the current phase of the character is.

590
00:31:15,797 --> 00:31:19,301
So assuming the character always has some value

591
00:31:19,421 --> 00:31:21,984
for his phase representing where he is in this cycle,

592
00:31:22,404 --> 00:31:23,806
we can try and use a different F.

593
00:31:24,867 --> 00:31:25,708
So let's try and do this

594
00:31:25,768 --> 00:31:27,270
in the kind of most simple way possible.

595
00:31:28,313 --> 00:31:30,934
What we can do is we can separate all our x's and y's

596
00:31:30,995 --> 00:31:34,156
in our database into different bins depending on the phase.

597
00:31:34,636 --> 00:31:36,997
So all of our data where the character's got his right foot

598
00:31:37,037 --> 00:31:40,398
down is going to go in a different bin to all the data

599
00:31:40,418 --> 00:31:42,439
where a character has his left foot down, for example.

600
00:31:43,980 --> 00:31:47,481
And then at runtime, given the phase, we basically select

601
00:31:47,541 --> 00:31:48,801
which bin we want to use.

602
00:31:50,161 --> 00:31:53,685
And we use the function we've trained for this particular bin

603
00:31:54,325 --> 00:31:56,407
to generate our y from our x.

604
00:31:56,447 --> 00:31:57,548
So let's see how this works.

605
00:31:57,969 --> 00:32:00,331
So let's say we've binned our data like this.

606
00:32:00,411 --> 00:32:03,294
So we have six different functions along this phase

607
00:32:03,314 --> 00:32:05,115
space, which we've binned our data into.

608
00:32:06,677 --> 00:32:08,879
And we get a new phase at runtime.

609
00:32:09,620 --> 00:32:11,081
So now we see which bin this is.

610
00:32:12,102 --> 00:32:14,705
We see which data is there, which function we've trained f.

611
00:32:15,742 --> 00:32:18,182
And we use this to produce our Y from our X.

612
00:32:18,283 --> 00:32:20,063
OK, so that's how we're going to do it.

613
00:32:20,103 --> 00:32:23,684
So let's try this again with a bunch of different machine

614
00:32:23,724 --> 00:32:25,185
learning algorithms and see what happens.

615
00:32:26,485 --> 00:32:28,566
So we're going to set it up a little bit differently.

616
00:32:29,307 --> 00:32:32,048
Our input will be exactly the same, the previous pose of the

617
00:32:32,088 --> 00:32:34,568
character and where we want to go in one second.

618
00:32:35,349 --> 00:32:37,950
But our output now is just going to be one frame.

619
00:32:37,990 --> 00:32:39,630
So we're not going to output a block of animation.

620
00:32:39,670 --> 00:32:41,351
We're just going to output one frame at a time.

621
00:32:42,921 --> 00:32:44,782
And what we're going to do is we're going to select our

622
00:32:44,822 --> 00:32:48,904
function f depending on our phase, call it to get our next

623
00:32:48,964 --> 00:32:52,026
pose of the character, and then update our phase value.

624
00:32:52,066 --> 00:32:54,427
So we also measure how much the phase

625
00:32:54,487 --> 00:32:56,228
changes at each frame.

626
00:32:56,688 --> 00:33:00,230
So for each frame in the database, we have the change

627
00:33:00,270 --> 00:33:01,751
in the phase, and we update this.

628
00:33:03,032 --> 00:33:05,893
So let's try it with nearest neighbor regression and see

629
00:33:05,933 --> 00:33:06,373
what happens.

630
00:33:08,132 --> 00:33:11,674
So here we kind of get something that looks pretty similar to our original

631
00:33:12,375 --> 00:33:15,357
nearest neighbor regression where we were outputting blocks of animation that was

632
00:33:15,437 --> 00:33:16,317
like motion matching.

633
00:33:16,738 --> 00:33:18,459
So that's kind of a good sign that

634
00:33:19,499 --> 00:33:21,400
things are roughly working as intended.

635
00:33:22,141 --> 00:33:24,222
And one thing you'll notice is that the

636
00:33:24,783 --> 00:33:26,504
cycle of the locomotion is kind of

637
00:33:27,024 --> 00:33:28,665
much more strongly preserved in this

638
00:33:29,385 --> 00:33:30,106
setup. So

639
00:33:30,966 --> 00:33:33,048
in the other one maybe there was more kind of diverse.

640
00:33:34,248 --> 00:33:38,290
movement where the phase could change fast or slow and different stepping

641
00:33:38,310 --> 00:33:42,791
patterns. Here we really kind of see the cycle going on and on. And we have the

642
00:33:42,831 --> 00:33:47,733
same kind of jumping issue where when it jumps to a new clip it kind of clicks. We

643
00:33:47,753 --> 00:33:52,535
could also add blending like before and we get a nice result. So let's see how

644
00:33:52,555 --> 00:33:54,235
the Gaussian process fares in this case.

645
00:33:56,654 --> 00:33:58,096
It basically doesn't work again.

646
00:33:58,876 --> 00:34:00,077
So I don't know what this means.

647
00:34:00,118 --> 00:34:03,221
So probably it means that having lots of data

648
00:34:03,301 --> 00:34:04,522
is very important, right?

649
00:34:05,343 --> 00:34:09,306
And we can sort of see that the phase is still cycling.

650
00:34:09,787 --> 00:34:11,108
It just looks a bit bizarre.

651
00:34:11,188 --> 00:34:13,230
So we kind of have some feeling

652
00:34:13,290 --> 00:34:14,972
that our phase thing is working.

653
00:34:15,252 --> 00:34:16,954
We're not just getting this gliding motion,

654
00:34:17,294 --> 00:34:18,896
but the quality of the output is not great.

655
00:34:20,573 --> 00:34:22,155
And there are some other kind of questions

656
00:34:22,195 --> 00:34:23,516
we can ask about this approach.

657
00:34:23,556 --> 00:34:26,759
Like, what if the phase is in between two different bins?

658
00:34:27,860 --> 00:34:30,822
And what about when it suddenly switches to a new bin?

659
00:34:31,283 --> 00:34:33,445
There maybe we'll get some sort of discontinuity

660
00:34:33,485 --> 00:34:34,325
or some sort of jump.

661
00:34:35,847 --> 00:34:38,349
And it seems like a waste to train multiple functions F.

662
00:34:38,449 --> 00:34:41,031
So in each of these bins we have a different F,

663
00:34:41,372 --> 00:34:43,814
but lots of them are gonna be extremely similar

664
00:34:43,874 --> 00:34:46,256
because they're only just kind of a little bit in time

665
00:34:46,296 --> 00:34:47,577
different from the previous one.

666
00:34:49,080 --> 00:34:53,484
and how can we use a neural network to attempt this same problem.

667
00:34:54,425 --> 00:35:01,191
This is basically where we were at for some previous research I did called face function

668
00:35:01,231 --> 00:35:04,874
neural networks and this is exactly what we did now.

669
00:35:04,894 --> 00:35:07,035
The basic idea is that

670
00:35:07,948 --> 00:35:12,069
we have a neural network where the weights are generated from the phase directly.

671
00:35:12,489 --> 00:35:15,309
So the weights are generated as a function of the phase

672
00:35:15,889 --> 00:35:19,110
and they change continuously and smoothly along with the phase.

673
00:35:20,550 --> 00:35:22,531
So as a diagram it looks something like this.

674
00:35:23,431 --> 00:35:26,332
What we have as input first is our phase variable

675
00:35:27,052 --> 00:35:29,672
and the phase variable it loops around in this circle.

676
00:35:31,393 --> 00:35:34,413
And we have kind of four sets of different neural network weights.

677
00:35:35,774 --> 00:35:38,755
and we have our phase function here. So what this does

678
00:35:39,195 --> 00:35:43,158
is it's basically interpolating these four sets of different neural network

679
00:35:43,218 --> 00:35:45,699
weights depending on what the phase value is.

680
00:35:46,259 --> 00:35:49,961
So for each different phase value we get a slightly different mix of these four

681
00:35:50,001 --> 00:35:51,222
different neural network weights.

682
00:35:52,970 --> 00:36:00,412
And these interpolated weights, they get given as the main weights to our normal neural network

683
00:36:00,452 --> 00:36:02,512
which maps from X's to Y's.

684
00:36:03,472 --> 00:36:08,833
So here we have our neural network, our equation which I showed before which takes an X and

685
00:36:08,873 --> 00:36:09,733
produces a Y.

686
00:36:11,074 --> 00:36:15,775
So if we do this in exactly our same setup as before, we get something that looks like

687
00:36:15,815 --> 00:36:16,015
this.

688
00:36:16,115 --> 00:36:17,875
So it's still not perfect.

689
00:36:17,975 --> 00:36:19,335
It's not completely...

690
00:36:20,801 --> 00:36:25,089
extremely responsive, but the quality of the generated motion is actually quite nice.

691
00:36:26,071 --> 00:36:29,338
And it's smooth and it varies continuously.

692
00:36:30,902 --> 00:36:33,284
So if we continue tweaking these x and these y's,

693
00:36:33,945 --> 00:36:38,068
and we continue tweaking this f, and we try and incorporate

694
00:36:38,188 --> 00:36:40,890
all our best practices for how we train a neural network,

695
00:36:41,471 --> 00:36:43,792
then we get something that looks more like this.

696
00:36:43,893 --> 00:36:46,535
So what we have in the end is we have a character

697
00:36:46,575 --> 00:36:48,997
which can follow a trajectory nicely,

698
00:36:49,377 --> 00:36:51,358
produce kind of nice, natural movement.

699
00:36:53,921 --> 00:36:55,502
And to show how scalable this is,

700
00:36:56,082 --> 00:36:57,904
we also trained it on a bunch of data

701
00:36:57,944 --> 00:36:59,845
where the character is going over rough terrain.

702
00:37:00,413 --> 00:37:06,055
So here we had about a gigabyte of data where the character was going over different rough

703
00:37:06,095 --> 00:37:06,495
terrain.

704
00:37:06,535 --> 00:37:11,256
So it was kind of roughly, I think an hour and a half of data we captured walking over

705
00:37:11,296 --> 00:37:12,156
different rough terrain.

706
00:37:12,576 --> 00:37:15,037
So we can see that the scalability is really there.

707
00:37:15,477 --> 00:37:20,878
We can train on absolutely huge amounts of data and it works and it adapts to all these

708
00:37:20,918 --> 00:37:22,019
different varied situations.

709
00:37:25,937 --> 00:37:29,082
And we can also give these kind of tags, which I described in the beginning.

710
00:37:29,142 --> 00:37:32,648
So here we tag whether the character should be crouching or not crouching.

711
00:37:33,148 --> 00:37:36,093
And we can give a continuous tag here based on the height of the ceiling.

712
00:37:36,574 --> 00:37:40,079
And the character will kind of naturally and somewhat smoothly adapt.

713
00:37:41,825 --> 00:37:44,107
Or we can give somewhat more discrete tags.

714
00:37:44,167 --> 00:37:46,969
So here we give a tag saying that the character should be

715
00:37:47,029 --> 00:37:48,651
jumping at this point in the future.

716
00:37:49,432 --> 00:37:50,873
And that's what the character does.

717
00:37:50,953 --> 00:37:54,056
So it sees that in the database, the only place where

718
00:37:54,076 --> 00:37:56,358
this tag was present was in jumping motions.

719
00:37:56,698 --> 00:37:57,719
So this is what it plays.

720
00:37:58,680 --> 00:38:01,322
And because we're also giving as input the height of the

721
00:38:01,662 --> 00:38:05,286
terrain under the character, he can somewhat adapt his

722
00:38:05,366 --> 00:38:07,488
style of jump based on what's below him.

723
00:38:10,393 --> 00:38:13,494
And the really nice thing is that once we have this neural network working,

724
00:38:14,074 --> 00:38:17,135
the performance is pretty good for the amount of data it was trained on.

725
00:38:17,215 --> 00:38:21,517
So in the kind of most compact way, what we can get

726
00:38:21,597 --> 00:38:25,058
is just 10 megabytes of neural network weights

727
00:38:25,518 --> 00:38:27,519
with a runtime of about 1 millisecond.

728
00:38:27,579 --> 00:38:30,300
So 10 megabytes is pretty incredible if you

729
00:38:30,340 --> 00:38:33,661
think that it was trained on literally gigabytes of motion data.

730
00:38:34,922 --> 00:38:38,403
And we've thrown all of this away, and we just keep this 10 megabytes.

731
00:38:40,025 --> 00:38:43,107
Or in this particular phase function neural network

732
00:38:43,147 --> 00:38:45,968
approach, we can do some sort of pre-computation,

733
00:38:46,348 --> 00:38:49,209
which uses more memory but can evaluate a bit faster.

734
00:38:49,329 --> 00:38:53,271
So it's definitely delivering what

735
00:38:53,311 --> 00:38:56,852
it promises in regards to scalability

736
00:38:57,272 --> 00:38:58,173
when it comes to data.

737
00:38:59,793 --> 00:39:03,495
So that's how you can kind of generalize this solution,

738
00:39:04,095 --> 00:39:06,396
frame it in a more classic machine learning way,

739
00:39:06,856 --> 00:39:09,557
and try out a bunch of different experiments to see what works.

740
00:39:11,359 --> 00:39:16,181
So in conclusion, there are a couple of sacrifices we have to make for this approach.

741
00:39:16,841 --> 00:39:19,082
So you have to give up precise control.

742
00:39:19,202 --> 00:39:26,304
So if we think about those 15,000 animations that were made for Assassin's Creed Origins,

743
00:39:27,945 --> 00:39:31,526
at some point we're just not going to be able to hand author all these animations.

744
00:39:32,206 --> 00:39:37,868
So we have to think about ways which can scale, and lots of these ways we can scale are going

745
00:39:37,888 --> 00:39:40,469
to require giving up precise control in many contexts.

746
00:39:42,299 --> 00:39:44,281
It also requires learning a whole new skill set.

747
00:39:44,321 --> 00:39:46,624
So doing machine learning programming

748
00:39:47,045 --> 00:39:49,188
is a completely different beast to normal programming.

749
00:39:50,269 --> 00:39:52,112
And it's very difficult and requires

750
00:39:52,793 --> 00:39:55,516
lots of different skills which you may not

751
00:39:55,556 --> 00:39:57,199
have learned in school or may not have learned

752
00:39:57,239 --> 00:39:57,860
over your career.

753
00:39:59,639 --> 00:40:03,884
And finally, it doesn't deal with a large number of special cases.

754
00:40:03,964 --> 00:40:07,769
So it's not like we can just throw out everything we've got already

755
00:40:07,869 --> 00:40:10,533
and just use this thing to replace everything.

756
00:40:10,953 --> 00:40:14,197
There's going to have to be some period of overlap

757
00:40:14,217 --> 00:40:15,800
between old systems and new systems.

758
00:40:17,081 --> 00:40:18,123
But for scalability.

759
00:40:19,160 --> 00:40:22,361
Animation quality is kind of losing the battle against complexity.

760
00:40:22,701 --> 00:40:26,983
So every time someone wants to push with the current approach of the state machine

761
00:40:27,023 --> 00:40:29,164
to add a new way of doing things,

762
00:40:29,685 --> 00:40:32,166
it's kind of suffering against the complexity of the system.

763
00:40:32,906 --> 00:40:35,287
And we can use machine learning to try and balance this fight,

764
00:40:35,387 --> 00:40:37,048
or at least some ideas from machine learning.

765
00:40:37,989 --> 00:40:41,310
And these ideas might be one of the best ways we can try and make progress

766
00:40:41,910 --> 00:40:42,631
in this direction.

767
00:40:43,882 --> 00:40:45,903
So some things we're looking at in the future.

768
00:40:45,963 --> 00:40:48,545
So how can we remove this phase variable, deal with

769
00:40:48,645 --> 00:40:49,705
non-cyclic motions?

770
00:40:50,786 --> 00:40:53,087
How can we scale to these hundreds of different styles?

771
00:40:53,127 --> 00:40:54,909
So we talked about all these different tags.

772
00:40:55,449 --> 00:40:56,530
What if we had tags?

773
00:40:56,690 --> 00:40:59,571
What if we had a huge database with hundreds of different

774
00:40:59,612 --> 00:41:01,673
styles, and all of these were tagged in detail?

775
00:41:01,953 --> 00:41:03,514
Would this work, and how would it work?

776
00:41:05,499 --> 00:41:07,940
And how can we continue to improve the quality?

777
00:41:08,000 --> 00:41:10,922
So in some sense, you can't get better quality than just

778
00:41:10,982 --> 00:41:13,324
playing back the motion capture data directly.

779
00:41:14,865 --> 00:41:18,467
But how can we throw out the motion capture data and still

780
00:41:18,507 --> 00:41:20,148
retain a really high quality solution?

781
00:41:21,850 --> 00:41:25,012
So I have a couple of bloopers so you can see what it's

782
00:41:25,072 --> 00:41:27,353
really like to do machine learning every day.

783
00:41:27,373 --> 00:41:30,776
So you have some nice artifacts like this.

784
00:41:34,998 --> 00:41:36,099
or one like this.

785
00:41:39,761 --> 00:41:42,203
And this is like a nice moonwalk he was doing.

786
00:41:42,223 --> 00:41:45,505
Okay, thank you very much.

787
00:41:45,525 --> 00:41:54,392
Applause

788
00:41:56,773 --> 00:41:59,135
So if you have any questions you can come up to the mics.

789
00:42:00,319 --> 00:42:01,220
Hi, hello.

790
00:42:01,320 --> 00:42:02,461
Great talk, great work.

791
00:42:02,681 --> 00:42:04,962
Thanks for sharing two short ones.

792
00:42:05,763 --> 00:42:09,205
I didn't get how do you generate the clips for the training

793
00:42:09,245 --> 00:42:09,545
data?

794
00:42:09,725 --> 00:42:11,186
How are you controlling the character?

795
00:42:11,266 --> 00:42:13,748
And the second one, how do you get any research paper

796
00:42:13,788 --> 00:42:14,568
that you can mention?

797
00:42:14,628 --> 00:42:15,789
We can get the details.

798
00:42:16,230 --> 00:42:16,430
Thank you.

799
00:42:19,287 --> 00:42:25,769
The clips for the training data are taken from this big, long database of long captures

800
00:42:25,809 --> 00:42:26,149
we did.

801
00:42:26,529 --> 00:42:31,910
So if we're talking about having one-second clips, we basically have overlapping one-second

802
00:42:31,950 --> 00:42:35,891
clips all the way through the database, unless it was kind of tagged as junk.

803
00:42:36,951 --> 00:42:42,733
And for the paper, if you Google face-functioned neural networks, it will pop up.

804
00:42:44,447 --> 00:42:45,008
Hey, great talk.

805
00:42:46,109 --> 00:42:48,271
In your diagram of the phase neural network,

806
00:42:48,732 --> 00:42:50,073
it looked like you had a separate network that

807
00:42:50,113 --> 00:42:52,976
was feeding weights into the existing neural network.

808
00:42:53,396 --> 00:42:55,018
Is there a reason why you couldn't just

809
00:42:55,058 --> 00:42:58,442
use the phase as an input feature into the set?

810
00:42:58,462 --> 00:43:00,163
Did that yield errors or something?

811
00:43:00,887 --> 00:43:03,729
Yeah, so you can use the phase as an additional input variable,

812
00:43:04,730 --> 00:43:09,254
and it works somewhat, but the quality is not as high.

813
00:43:09,434 --> 00:43:12,317
So the kind of details for why that is

814
00:43:12,817 --> 00:43:14,979
is a bit complicated to explain.

815
00:43:15,520 --> 00:43:17,221
But roughly, you can think about it

816
00:43:17,541 --> 00:43:20,104
as when you give the phase in at the side,

817
00:43:20,584 --> 00:43:23,547
it's really much more similar to binning the data

818
00:43:23,647 --> 00:43:24,968
into really separate bins.

819
00:43:25,589 --> 00:43:28,731
Whereas where you give it in at the top, it's more like

820
00:43:29,772 --> 00:43:32,534
giving another variable saying how to blend between the whole

821
00:43:32,574 --> 00:43:33,555
database at once.

822
00:43:34,156 --> 00:43:37,499
So giving it in the side is really like taking a slice of

823
00:43:37,539 --> 00:43:40,561
this database with just the phase values you want.

824
00:43:42,603 --> 00:43:43,504
Hey Daniel, great talk.

825
00:43:44,605 --> 00:43:46,146
I wanted to ask about the tagging process.

826
00:43:46,566 --> 00:43:50,590
And are you looking towards automating that kind of thing?

827
00:43:51,812 --> 00:43:52,973
It's a very manual process.

828
00:43:53,033 --> 00:43:54,894
And who would do that kind of work?

829
00:43:54,954 --> 00:43:57,496
Because as the data scales, obviously that's a lot of work

830
00:43:57,516 --> 00:43:58,116
to tag all that data.

831
00:43:58,696 --> 00:43:58,956
Yeah.

832
00:43:59,156 --> 00:44:02,558
So I guess one of the great things about doing this tagging

833
00:44:02,719 --> 00:44:07,761
is that you can also use it to train classifiers, animation

834
00:44:07,802 --> 00:44:08,482
classifiers.

835
00:44:09,102 --> 00:44:12,084
So you'd hope that you could get to a point where you have

836
00:44:12,104 --> 00:44:15,626
a large enough database where from then on you can do the

837
00:44:15,666 --> 00:44:17,427
tagging automatically with a classifier.

838
00:44:18,388 --> 00:44:18,608
Thank you.

839
00:44:21,749 --> 00:44:26,554
Hello, I was wondering how much iteration was done for representing the data going into

840
00:44:26,574 --> 00:44:31,980
the neural network because I could see the rotation as part of the pose being very hard

841
00:44:32,060 --> 00:44:40,048
to be consumed by a neural network and for a neural network to make sense of rotational

842
00:44:40,068 --> 00:44:40,288
data.

843
00:44:41,033 --> 00:44:42,874
Yeah, so there's a lot of iteration.

844
00:44:43,135 --> 00:44:47,197
That's kind of the black magic of machine learning.

845
00:44:47,937 --> 00:44:50,698
And that's where you need to have a good intuition

846
00:44:50,778 --> 00:44:52,439
and do lots of different experiments.

847
00:44:53,140 --> 00:44:57,022
So you can see exactly how we got the results we did

848
00:44:57,262 --> 00:44:59,183
in our paper if you look it up.

849
00:45:00,543 --> 00:45:03,245
But yeah, that's one thing you need to play around a lot

850
00:45:03,265 --> 00:45:04,205
with to get good results.

851
00:45:04,225 --> 00:45:05,646
All right, thank you.

852
00:45:09,696 --> 00:45:12,724
Um, excellent talk, very interesting.

853
00:45:13,044 --> 00:45:17,877
So a lot of the examples that I've, I've seen for both this talk and motion matching were

854
00:45:19,441 --> 00:45:21,823
were largely for character locomotion,

855
00:45:22,304 --> 00:45:23,544
navigation animation.

856
00:45:24,145 --> 00:45:27,207
If you're, say, working in a space where maybe you have

857
00:45:27,227 --> 00:45:30,550
a very sophisticated combat simulation in your game,

858
00:45:31,611 --> 00:45:34,192
have you found any success in this approach for

859
00:45:35,473 --> 00:45:38,716
driving additive animations or layered animations

860
00:45:38,756 --> 00:45:39,697
through something like this?

861
00:45:39,857 --> 00:45:42,018
Maybe something where your animation data

862
00:45:42,078 --> 00:45:44,500
doesn't necessarily have variables like speed

863
00:45:44,620 --> 00:45:47,262
or character intent, it's maybe just something like

864
00:45:47,322 --> 00:45:48,303
I got shot and...

865
00:45:48,843 --> 00:45:49,083
Yeah.

866
00:45:49,304 --> 00:45:49,944
Something like that.

867
00:45:50,464 --> 00:45:54,064
So you can use the motion matching stuff to drive events.

868
00:45:54,204 --> 00:45:57,385
So in For Honor, the attacks are actually matched too.

869
00:45:57,745 --> 00:45:59,225
So you'll have an input which says,

870
00:45:59,705 --> 00:46:03,126
please play this style of attack in this amount of time.

871
00:46:03,686 --> 00:46:06,826
And it will try and match the best one for the current pose

872
00:46:06,887 --> 00:46:08,247
and other factors.

873
00:46:09,727 --> 00:46:12,487
So probably you should go to the UFC talk tomorrow.

874
00:46:12,868 --> 00:46:14,868
I'm sure they're going to be talking a lot more

875
00:46:14,948 --> 00:46:15,988
about that sort of stuff.

876
00:46:16,168 --> 00:46:18,048
And I'm sure it will be super, super insightful.

877
00:46:18,569 --> 00:46:19,129
Thanks for the tip.

878
00:46:22,250 --> 00:46:22,871
Hi, great talk.

879
00:46:23,191 --> 00:46:25,692
Is it possible to incrementally train

880
00:46:25,852 --> 00:46:27,032
these sorts of neural networks?

881
00:46:27,153 --> 00:46:30,614
So if I add a couple of new clips of a new kind of turn

882
00:46:30,634 --> 00:46:32,995
or something, I don't have to necessarily retrain

883
00:46:33,155 --> 00:46:35,816
the whole thing for tens of hours of computation time?

884
00:46:36,683 --> 00:46:43,807
So, in theory, yeah, there is some research showing how you can incrementally train neural

885
00:46:43,827 --> 00:46:44,228
networks.

886
00:46:44,448 --> 00:46:50,532
So, I don't have any personal experience doing this, but the research is there at least.

887
00:46:50,552 --> 00:46:52,213
It just needs to be tried out, I think.

888
00:47:01,249 --> 00:47:03,712
Not to my knowledge. I guess we'll see.

889
00:47:04,253 --> 00:47:07,697
One nice thing about this approach for quadrupeds is that,

890
00:47:09,359 --> 00:47:12,062
well, it's very hard to get a quadruped to act,

891
00:47:12,262 --> 00:47:16,467
but you can get a whole bunch of raw animation data from a quadruped

892
00:47:16,527 --> 00:47:19,451
by just putting it in a motion capture studio.

893
00:47:20,612 --> 00:47:20,873
Thank you.

894
00:47:23,034 --> 00:47:23,775
Hi, great talk.

895
00:47:25,037 --> 00:47:27,402
Question, did you have to overlay a lot of IK

896
00:47:27,803 --> 00:47:29,065
to lock the hands and feet?

897
00:47:29,566 --> 00:47:33,354
And did you try to augment the rotation data

898
00:47:33,434 --> 00:47:35,578
with the absolute position of the joint?

899
00:47:37,042 --> 00:47:39,783
So there is a little so in the

900
00:47:40,443 --> 00:47:43,944
Examples I showed there was no IK or anything

901
00:47:44,924 --> 00:47:48,645
All the joints were represented in the character space rather than the local space

902
00:47:50,486 --> 00:47:55,967
In the previous paper with the guy walking over the terrain. There was a little bit of IK not that much

903
00:47:56,487 --> 00:48:01,349
It's not that strongly required. It again depends on your

904
00:48:01,929 --> 00:48:04,710
Your representation and these sorts of things. Thank you

905
00:48:06,911 --> 00:48:10,315
How many supervised training data do you use?

906
00:48:11,235 --> 00:48:11,516
Sorry?

907
00:48:12,176 --> 00:48:18,302
How many training data do you use?

908
00:48:18,322 --> 00:48:22,366
So in the stuff I was showing here, it was about 200

909
00:48:23,467 --> 00:48:27,551
megabytes, which I think is probably about kind of.

910
00:48:28,828 --> 00:48:31,190
half an hour to an hour of data.

911
00:48:31,451 --> 00:48:33,973
So you saw in the clip, there was kind of three of us

912
00:48:34,053 --> 00:48:36,776
in the motion capture studio doing a whole bunch

913
00:48:36,816 --> 00:48:38,497
of random locomotion moves.

914
00:48:39,058 --> 00:48:42,521
And I think we did about half an hour of that,

915
00:48:42,601 --> 00:48:43,382
something like that.

916
00:48:44,343 --> 00:48:44,663
Thank you.

917
00:48:46,700 --> 00:48:52,723
Yeah, great work. I was curious for fine, you know, you mentioned that you lose this fine detail.

918
00:48:53,724 --> 00:49:03,430
Do you switch to another system when you have to like animate like cut scene sort of thing where somebody has to pick up a bottle or, you know, pick something specific up? Can you use this for that? Or is it not?

919
00:49:03,850 --> 00:49:07,313
Yeah, you can just switch to the different system.

920
00:49:07,353 --> 00:49:08,554
There's no reason why not.

921
00:49:09,154 --> 00:49:12,037
But you could also potentially do it in the same system.

922
00:49:12,397 --> 00:49:14,499
Like I was saying, you could actually tag the cut scene

923
00:49:14,539 --> 00:49:18,242
with one specific tag and say, this is a super high priority

924
00:49:18,282 --> 00:49:22,626
tag, so please make sure you start this

925
00:49:22,886 --> 00:49:24,027
cut scene when it goes.

926
00:49:25,168 --> 00:49:28,951
But when you want that much control, it's probably easier

927
00:49:29,011 --> 00:49:31,273
to blend out to existing systems.

928
00:49:34,372 --> 00:49:35,153
Thanks, great talk.

929
00:49:36,073 --> 00:49:38,194
Thanks for making it very approachable.

930
00:49:39,814 --> 00:49:40,374
Two questions.

931
00:49:40,935 --> 00:49:41,235
How do you...

932
00:49:41,255 --> 00:49:42,615
I don't like this mic.

933
00:49:42,875 --> 00:49:45,516
How do you approach fine tuning?

934
00:49:46,817 --> 00:49:50,178
Where does that fall now between the animation lead...

935
00:49:50,738 --> 00:49:54,040
and the programmer on the neural network side of things

936
00:49:54,340 --> 00:49:55,041
with this approach?

937
00:49:55,801 --> 00:49:58,463
And then have you also taken, the second question is,

938
00:49:58,643 --> 00:50:00,984
have you taken this and applied it in a layered fashion,

939
00:50:01,004 --> 00:50:04,486
like run and gun, where you're kind of breaking apart

940
00:50:04,526 --> 00:50:05,927
into different subcomponents?

941
00:50:08,108 --> 00:50:15,833
So as far as fine-tuning is concerned, it's still quite early and we need probably dedicated

942
00:50:15,893 --> 00:50:18,094
tools to make the tuning a viable option.

943
00:50:18,474 --> 00:50:23,197
So if your data is relatively small and you're doing something like motion matching, you

944
00:50:23,237 --> 00:50:25,398
can actually edit the data and that works pretty well.

945
00:50:26,438 --> 00:50:29,760
If you're training a neural network, it's much harder to guarantee what the results

946
00:50:29,820 --> 00:50:33,082
are going to be like and probably will need dedicated tools.

947
00:50:34,771 --> 00:50:36,813
And your second question was?

948
00:50:37,033 --> 00:50:37,393
Layering.

949
00:50:37,413 --> 00:50:39,475
So running on, things like that.

950
00:50:40,175 --> 00:50:42,977
The kind of philosophy of machine learning

951
00:50:43,057 --> 00:50:46,200
is not to separate out things into layers.

952
00:50:46,980 --> 00:50:50,103
And rather, you hope that it learns, in some sense,

953
00:50:50,323 --> 00:50:52,364
what is a layer and what isn't a layer.

954
00:50:52,845 --> 00:50:54,866
For example, the crouching you saw.

955
00:50:56,468 --> 00:50:59,050
Although we never captured any crouching on rough terrain.

956
00:51:00,680 --> 00:51:03,601
If you crouch and you go over the rough terrain in our little

957
00:51:03,661 --> 00:51:08,044
demo, it does actually somewhat adapt the feet positions and

958
00:51:08,084 --> 00:51:08,864
these sorts of things.

959
00:51:09,305 --> 00:51:12,266
So it's somehow learned that crouching is kind of

960
00:51:12,366 --> 00:51:14,768
independent of what the other actions you do

961
00:51:15,308 --> 00:51:16,108
on rough terrain are.

962
00:51:16,229 --> 00:51:19,610
So the hope is that if you provide it with enough data,

963
00:51:19,670 --> 00:51:22,352
it will learn to do all these sorts of layering operations

964
00:51:22,432 --> 00:51:26,634
for you, and you can keep everything in one system.

965
00:51:27,835 --> 00:51:28,515
That's the idea.

966
00:51:28,535 --> 00:51:29,356
Thank you.

967
00:51:31,585 --> 00:51:33,747
Hello, thank you, very exciting talk.

968
00:51:34,007 --> 00:51:37,269
So I want to ask you about the neural network training.

969
00:51:37,349 --> 00:51:40,932
So how do you decide how many layers you use,

970
00:51:40,992 --> 00:51:42,713
how many nodes in each nodes,

971
00:51:43,153 --> 00:51:46,055
and then the activation function you use,

972
00:51:46,695 --> 00:51:49,837
and then either you normalize the input or not.

973
00:51:49,877 --> 00:51:52,439
So do you find everything optimized,

974
00:51:52,559 --> 00:51:54,761
or do you think there is still work to do

975
00:51:54,961 --> 00:51:56,542
to find the optimized solution?

976
00:51:57,823 --> 00:52:00,164
Uh, yeah, so there is some kind of.

977
00:52:01,129 --> 00:52:07,753
rules of thumb and intuitions you pick up if you do machine learning and you can also read

978
00:52:07,833 --> 00:52:13,816
the previous literature to see what researchers doing similar applications have done to get a

979
00:52:13,856 --> 00:52:17,398
kind of rough idea of how you might want to structure your neural network.

980
00:52:18,739 --> 00:52:21,640
Yeah, there's always going to be improvements, and actually that's one of the best things

981
00:52:21,720 --> 00:52:26,541
about using neural networks is that you can import all the future improvements people

982
00:52:26,581 --> 00:52:27,061
come up with.

983
00:52:27,141 --> 00:52:32,903
So maybe tomorrow someone will come up with a new activation function which improves everything.

984
00:52:33,323 --> 00:52:38,424
We can implement that pretty much then and there, tomorrow morning, try it and see what

985
00:52:38,464 --> 00:52:38,824
happens.

986
00:52:39,025 --> 00:52:41,685
And if it solves all our problems, that's amazing.

987
00:52:41,765 --> 00:52:45,866
So one huge benefit of generalizing and...

988
00:52:47,156 --> 00:52:49,939
making it more like a general machine learning framework is

989
00:52:50,320 --> 00:52:53,503
to borrow from these other people who are developing these things.

990
00:52:58,525 --> 00:53:01,466
Nicole Azarro from Zio and Follow the White Rabbit.

991
00:53:01,486 --> 00:53:04,307
I was curious, really, really great talk and definitely very approachable.

992
00:53:04,347 --> 00:53:10,289
I was curious about taking this approach to other, you know, aspects of animation and

993
00:53:10,369 --> 00:53:15,631
like if you had like an Errol Flynn, you know, kind of style of motion like maybe in the

994
00:53:15,691 --> 00:53:16,891
attacks or something like that.

995
00:53:17,852 --> 00:53:22,113
Would that approach, locomotion is, how the character moves is important and you want

996
00:53:22,133 --> 00:53:25,994
to have different kinds of motion for different types of characters, but if you have your

997
00:53:26,074 --> 00:53:31,236
star character with, you know, different special kind of moves, will that, will this approach

998
00:53:31,476 --> 00:53:32,456
work or what changes?

999
00:53:33,193 --> 00:53:37,856
Yeah, so it's a bit difficult if you really want handcrafted animation because you need

1000
00:53:37,936 --> 00:53:40,898
such vast quantities of data to get good results.

1001
00:53:41,378 --> 00:53:49,743
So probably my advice would be to start with a database of locomotion data which is motion

1002
00:53:49,763 --> 00:53:55,487
captured and large and to try and come up with procedural tweaks which automatically

1003
00:53:55,627 --> 00:53:56,867
stylize this motion.

1004
00:53:57,668 --> 00:53:59,609
in the way you want, or something like this,

1005
00:54:00,049 --> 00:54:02,670
so that you can easily produce large amounts

1006
00:54:02,730 --> 00:54:04,110
of stylized data.

1007
00:54:04,750 --> 00:54:06,151
And if you can do that, then yeah,

1008
00:54:06,731 --> 00:54:08,252
it works in exactly the same way.

1009
00:54:08,472 --> 00:54:10,072
It can work for stylized motion, too.

1010
00:54:10,772 --> 00:54:11,033
Thank you.

1011
00:54:14,334 --> 00:54:16,034
Okay, I think that's everyone.

1012
00:54:16,514 --> 00:54:17,075
Thank you very much.

