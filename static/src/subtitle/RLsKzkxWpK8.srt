1
00:00:06,154 --> 00:00:07,556
Hey guys, my name's Ben.

2
00:00:07,796 --> 00:00:10,220
I'm the lead AI developer at Havoc.

3
00:00:11,241 --> 00:00:14,345
Hi, and my name's Alin, and I'm a PhD student

4
00:00:14,465 --> 00:00:15,767
at the University of Pennsylvania.

5
00:00:16,208 --> 00:00:17,630
Alin is graduating in a few months,

6
00:00:17,710 --> 00:00:19,012
so if you like what you hear today,

7
00:00:19,072 --> 00:00:20,193
then you should totally hire her.

8
00:00:22,336 --> 00:00:25,538
So first, let's talk about why people don't use machine learning,

9
00:00:25,578 --> 00:00:28,219
because there's been a lot of resistance to this in game AI.

10
00:00:29,199 --> 00:00:30,880
People say it's too slow to execute.

11
00:00:31,360 --> 00:00:34,042
You can't really afford it in the context of a real-time game.

12
00:00:34,362 --> 00:00:35,422
They say it's too opaque.

13
00:00:35,702 --> 00:00:38,024
You can't figure out what it's doing when it does something wrong,

14
00:00:38,264 --> 00:00:39,524
so you can't correct the weaknesses.

15
00:00:40,265 --> 00:00:41,865
And of course, it's too unreliable.

16
00:00:42,026 --> 00:00:43,806
It doesn't get the right answer often enough.

17
00:00:45,507 --> 00:00:46,448
Is machine learning slow?

18
00:00:48,027 --> 00:00:50,028
Well, this is an autonomous helicopter.

19
00:00:50,108 --> 00:00:52,848
It is simultaneously flipping in place and pirouetting.

20
00:00:52,968 --> 00:00:55,689
It's a move known as the chaos.

21
00:00:56,649 --> 00:00:58,770
Accomplishing this crazy difficult feat

22
00:00:59,090 --> 00:01:01,950
requires adjusting four separate control settings

23
00:01:02,471 --> 00:01:03,251
50 times a second.

24
00:01:06,912 --> 00:01:07,612
What about opaque?

25
00:01:08,372 --> 00:01:11,873
These are the probability models being used by a simple handwriting

26
00:01:11,913 --> 00:01:12,473
recognizer.

27
00:01:12,893 --> 00:01:15,133
You may notice they basically just look like letters.

28
00:01:15,253 --> 00:01:17,334
There is nothing opaque about this model.

29
00:01:18,206 --> 00:01:22,168
If you want to know why C and E are being classified poorly with respect to each other,

30
00:01:22,228 --> 00:01:25,269
well, that's because their probability models look a lot alike.

31
00:01:27,470 --> 00:01:30,572
As for unreliable, this is the Lifeline Auto.

32
00:01:30,772 --> 00:01:34,454
It's an automatic defibrillator similar to those positioned throughout Moscone Hall.

33
00:01:34,894 --> 00:01:41,137
This device uses a ML model of cardiac rhythms to diagnose ventricular fibrillation.

34
00:01:41,677 --> 00:01:45,219
This box literally makes life and death decisions based on ML.

35
00:01:46,379 --> 00:01:47,140
It does a pretty good job.

36
00:01:50,012 --> 00:01:51,033
Here's a cup of cold coffee.

37
00:01:51,933 --> 00:01:54,655
The reason game AI people have had such a bad experience with ML

38
00:01:54,755 --> 00:01:56,355
is not that ML is inherently bad.

39
00:01:56,896 --> 00:01:59,297
It's that we as programmers don't automatically

40
00:01:59,357 --> 00:02:01,078
know how to do good ML.

41
00:02:02,038 --> 00:02:04,259
For most of us, our exposure to ML in school

42
00:02:04,539 --> 00:02:06,240
was basically limited to neural networks

43
00:02:06,280 --> 00:02:07,201
and genetic algorithms.

44
00:02:07,781 --> 00:02:09,842
These things are easy to understand, they're cool,

45
00:02:10,242 --> 00:02:11,963
and you can program them without knowing much math.

46
00:02:12,932 --> 00:02:17,273
Statisticians in school learn about clustering and component analysis and decision trees

47
00:02:17,313 --> 00:02:20,714
and boosting and bagging because they have the background for them.

48
00:02:22,915 --> 00:02:26,516
The lesson is, if you really want to try out ML, you'll need to step outside your comfort

49
00:02:26,536 --> 00:02:28,697
zone and learn some unfamiliar concepts.

50
00:02:31,558 --> 00:02:32,999
It's potentially very rewarding.

51
00:02:34,119 --> 00:02:38,460
ML is at its best when solving problems that you have no idea how to code up because ML

52
00:02:38,500 --> 00:02:41,481
is all about understanding the underlying process for you.

53
00:02:42,607 --> 00:02:45,949
ML can help you refine your AI automatically,

54
00:02:46,029 --> 00:02:48,390
rather than by endless knob tweaking and testing.

55
00:02:49,050 --> 00:02:52,052
It'll help you build NPCs with varied personalities,

56
00:02:52,452 --> 00:02:54,213
based on captured gameplay data.

57
00:02:55,013 --> 00:02:55,773
And a whole lot more.

58
00:02:55,813 --> 00:02:58,875
Like a lot of tools, until you really know how to use ML,

59
00:02:59,235 --> 00:03:01,816
you can't get a full appreciation of all the things

60
00:03:01,836 --> 00:03:02,516
you can use it for.

61
00:03:07,038 --> 00:03:07,359
All right.

62
00:03:07,599 --> 00:03:11,140
So before we continue on, let's go over some terminology.

63
00:03:13,513 --> 00:03:18,354
So first off, the primary goal of any machine learning algorithm is generalizability.

64
00:03:18,614 --> 00:03:23,455
As Ben mentioned previously, if we understood the process well enough to just code it up

65
00:03:23,495 --> 00:03:24,796
directly, that's what we would do.

66
00:03:24,816 --> 00:03:25,776
We would just code it up.

67
00:03:26,716 --> 00:03:30,997
So what we're trying to do instead is based on examples we have already, we're trying

68
00:03:31,037 --> 00:03:36,559
to learn a model which allows us to predict, classify, or cluster based on new examples

69
00:03:36,619 --> 00:03:38,019
that we've never seen before.

70
00:03:38,919 --> 00:03:40,060
And so how do we do this?

71
00:03:40,841 --> 00:03:42,563
Well, in general, there are two steps

72
00:03:42,743 --> 00:03:44,044
to any machine learning algorithm.

73
00:03:44,084 --> 00:03:45,966
The first step is the training phase,

74
00:03:46,507 --> 00:03:49,590
where given the examples that we have,

75
00:03:49,650 --> 00:03:53,294
we try to fit the model so that it explains those examples as

76
00:03:53,334 --> 00:03:54,054
well as possible.

77
00:03:54,335 --> 00:03:57,718
And then once we have that, we can then

78
00:03:58,079 --> 00:04:00,020
see how well our model works on new examples

79
00:04:00,040 --> 00:04:00,981
that it's never seen before.

80
00:04:03,946 --> 00:04:05,327
So what do we mean by models?

81
00:04:06,348 --> 00:04:09,130
So model is a very generic term, and it's just

82
00:04:09,170 --> 00:04:11,151
the representation of the underlying process.

83
00:04:11,291 --> 00:04:14,173
And its job is just to encode how the input should

84
00:04:14,193 --> 00:04:15,094
relate to the output.

85
00:04:17,814 --> 00:04:20,537
This, under the umbrella of machine learning,

86
00:04:20,577 --> 00:04:22,759
there are many, many, many different models

87
00:04:22,839 --> 00:04:24,261
that are tools that you might use.

88
00:04:24,321 --> 00:04:27,124
So for example, decision trees and k-nearest neighbors

89
00:04:27,204 --> 00:04:28,986
are two techniques that Ben's gonna talk about

90
00:04:29,046 --> 00:04:31,329
in a little bit, but it also includes things

91
00:04:31,409 --> 00:04:33,391
like linear regression, neural nets,

92
00:04:33,471 --> 00:04:35,113
Naive Bayes, support vector machines,

93
00:04:35,493 --> 00:04:36,835
and many, many, many, many more.

94
00:04:39,342 --> 00:04:41,083
So let's just talk briefly about features.

95
00:04:41,183 --> 00:04:43,663
So the machine learning inputs are often called features.

96
00:04:44,043 --> 00:04:45,324
And a lot of times, you're going to have

97
00:04:45,664 --> 00:04:47,184
many different features that you're then

98
00:04:47,204 --> 00:04:49,525
going to collect together in a big feature vector.

99
00:04:52,205 --> 00:04:55,166
And so again, features are very domain specific.

100
00:04:55,186 --> 00:04:56,586
They're going to look really different based

101
00:04:56,626 --> 00:04:58,007
on the applications that you're working on.

102
00:04:58,407 --> 00:05:01,228
So to give an example, if you're working with image data,

103
00:05:01,888 --> 00:05:03,908
you have here a 32 by 32 pixel image.

104
00:05:03,948 --> 00:05:05,909
You might flatten that into a 1 by 1024 feature vector.

105
00:05:08,500 --> 00:05:11,781
Alternatively, suppose you're working with animation data

106
00:05:11,821 --> 00:05:12,861
and you have motion features.

107
00:05:12,962 --> 00:05:16,142
So one thing you might do is just put all the key frames,

108
00:05:16,262 --> 00:05:18,503
one after each other, in a big feature vector.

109
00:05:18,643 --> 00:05:21,484
And then each key frame is itself just a list

110
00:05:21,764 --> 00:05:23,044
of joint rotation values.

111
00:05:23,084 --> 00:05:25,265
So that's just a big vector of real numbers.

112
00:05:27,926 --> 00:05:29,046
Suppose you're working with text.

113
00:05:29,927 --> 00:05:34,308
A feature vector you might use is a series of word counts.

114
00:05:34,569 --> 00:05:38,670
So for example, each element of your feature vector

115
00:05:38,710 --> 00:05:41,411
will be the number of times a certain word appeared

116
00:05:41,571 --> 00:05:43,011
in the document that you're analyzing.

117
00:05:45,832 --> 00:05:46,812
And then in addition.

118
00:05:47,474 --> 00:05:51,838
just a notational thing, once we have everything in these big feature vectors,

119
00:05:52,319 --> 00:05:55,522
it's very convenient to then put them into big matrices.

120
00:05:55,942 --> 00:06:01,147
And these matrices might represent a training set, or a test set, or new data we've never seen before.

121
00:06:01,647 --> 00:06:05,271
And just the idea here is each feature vector is just stacked one on top of each other,

122
00:06:05,411 --> 00:06:10,075
so that each row is an example, and then it just follows that each column is a feature.

123
00:06:11,808 --> 00:06:13,989
And then briefly, just to talk about the term labels,

124
00:06:15,170 --> 00:06:17,112
the output of your machine learning algorithm

125
00:06:17,132 --> 00:06:19,033
are often referred to as labels, particularly

126
00:06:19,073 --> 00:06:20,014
for classification.

127
00:06:20,414 --> 00:06:23,536
And so some examples, you might see your gesture type

128
00:06:23,776 --> 00:06:25,277
is fraudulent or is spam.

129
00:06:25,578 --> 00:06:27,919
So the types of things that your machine learning algorithm's

130
00:06:27,959 --> 00:06:29,360
trying to predict or classify.

131
00:06:31,590 --> 00:06:34,212
And similarly to your feature vectors,

132
00:06:34,692 --> 00:06:37,613
we might want to combine all these things together

133
00:06:37,653 --> 00:06:39,394
into a convenient matrix form.

134
00:06:39,534 --> 00:06:43,736
So here, each row is the outcome for a particular corresponding

135
00:06:43,796 --> 00:06:44,197
example.

136
00:06:45,397 --> 00:06:48,559
And that sums up some terminology.

137
00:06:49,779 --> 00:06:52,561
So now I'd like to cover a small number of techniques

138
00:06:52,581 --> 00:06:54,082
that I think are particularly worth knowing.

139
00:06:54,322 --> 00:06:56,143
This is nowhere near comprehensive.

140
00:06:56,443 --> 00:06:57,783
It's not all there is to ML, but it'll

141
00:06:57,803 --> 00:06:59,744
be a good start to understanding the various tools that

142
00:07:02,062 --> 00:07:04,644
Now there's three major areas that machine learning tackles.

143
00:07:04,804 --> 00:07:07,446
Supervised learning, where you train the system to answer questions.

144
00:07:07,947 --> 00:07:12,230
Unsupervised learning, where you uncover patterns and relationships in the data.

145
00:07:12,690 --> 00:07:16,853
And finally, reinforcement learning, where the system learns how to behave optimally in a particular task.

146
00:07:17,193 --> 00:07:21,336
I'm going to focus specifically on supervised learning, which I think is the most widely useful area.

147
00:07:21,657 --> 00:07:25,640
Most of the problems you have in ML will turn out to be supervised learning problems.

148
00:07:31,320 --> 00:07:32,321
First of all, decision trees.

149
00:07:33,183 --> 00:07:35,447
Decision trees are one of my favorite ML techniques,

150
00:07:35,507 --> 00:07:37,149
specifically for game AI stuff.

151
00:07:37,330 --> 00:07:39,273
They're beautifully simple in their operation.

152
00:07:39,293 --> 00:07:41,717
There's almost nothing non-obvious about them.

153
00:07:42,727 --> 00:07:45,509
Here's a decision tree that an NPC might use to determine

154
00:07:45,549 --> 00:07:46,970
whether to attack or retreat.

155
00:07:47,531 --> 00:07:49,992
It starts at the root node, and at each branch,

156
00:07:50,132 --> 00:07:52,294
it goes up or down depending on the answer

157
00:07:52,314 --> 00:07:53,115
to the test in the node.

158
00:07:53,515 --> 00:07:55,636
So first it checks whether its hit points are above 50.

159
00:07:56,257 --> 00:07:56,997
Above 50, it goes up.

160
00:07:57,238 --> 00:07:58,238
Below 50, it goes down.

161
00:07:58,278 --> 00:08:00,560
If it goes down, then it checks whether the player is stunned.

162
00:08:00,580 --> 00:08:01,861
If it's stunned, he attacks.

163
00:08:02,121 --> 00:08:03,002
Otherwise, he retreats.

164
00:08:03,382 --> 00:08:05,163
And if the hit points are above 50, he always attacks.

165
00:08:05,503 --> 00:08:06,524
So nothing weird here.

166
00:08:06,564 --> 00:08:07,965
This is just two if statements.

167
00:08:09,329 --> 00:08:13,712
The real power here is in having the computer build the decision tree for you.

168
00:08:14,513 --> 00:08:18,837
Say you've got this big table of example play situations that your designers or your testers

169
00:08:18,877 --> 00:08:19,438
came up with.

170
00:08:20,138 --> 00:08:24,062
In this situation, with these hit points and that hair color and this and that and the

171
00:08:24,082 --> 00:08:26,224
other thing, the correct decision was to attack.

172
00:08:27,432 --> 00:08:32,714
Looking at this table, it's not easy to see which factors are important and which never

173
00:08:32,754 --> 00:08:36,234
matter and which are only important in certain special situations.

174
00:08:36,955 --> 00:08:41,576
But a decision tree learning algorithm will take this data set and spit out a decision

175
00:08:41,596 --> 00:08:42,236
tree like this one.

176
00:08:45,857 --> 00:08:49,118
Decision trees are what's known as a white box algorithm.

177
00:08:49,980 --> 00:08:52,201
You can look at a neural network's weights all day

178
00:08:52,481 --> 00:08:55,962
and not have any idea why it's making some particular classification

179
00:08:55,982 --> 00:08:57,122
that you think it shouldn't be making.

180
00:08:58,103 --> 00:09:00,603
It just isn't questionable.

181
00:09:01,303 --> 00:09:04,084
But everything a decision tree does is clearly exposed.

182
00:09:04,704 --> 00:09:07,765
If you want to know what it was thinking when it gave you a particular result,

183
00:09:08,285 --> 00:09:11,746
you simply follow the branches it took from the root to the leaf,

184
00:09:12,766 --> 00:09:15,407
seeing what factors were important, seeing what it branched on.

185
00:09:16,108 --> 00:09:23,035
And at each node, you can even see what training examples were relevant to splitting the tree in such and such a way.

186
00:09:24,057 --> 00:09:30,464
And although machine learning people wouldn't normally do this, you can even manually tweak an automatically built decision tree.

187
00:09:31,084 --> 00:09:36,490
If you think some branch is stupid and it just came out from a weird quirk in your training data, you can just go ahead and snip it out.

188
00:09:39,421 --> 00:09:42,803
The algorithm most commonly used for decision trees is called ID3,

189
00:09:43,243 --> 00:09:44,844
and it's really easy to wrap your head around.

190
00:09:45,544 --> 00:09:47,185
It starts with the whole training set,

191
00:09:47,745 --> 00:09:49,886
and you decide which question you can ask

192
00:09:50,306 --> 00:09:53,668
to separate the different output labels as cleanly as possible.

193
00:09:54,548 --> 00:09:56,009
In the attack or retreat case,

194
00:09:56,369 --> 00:09:59,430
hit points were a much better predictor than, say, hair color.

195
00:10:00,091 --> 00:10:03,252
Knowing just the hit points didn't always tell you what you should do,

196
00:10:03,833 --> 00:10:05,553
but it did give you a pretty big hint.

197
00:10:06,917 --> 00:10:09,938
Then you separate the examples based on that feature.

198
00:10:10,818 --> 00:10:14,259
Then you make those subsets children of the decision node

199
00:10:14,279 --> 00:10:15,200
with that test in it.

200
00:10:16,000 --> 00:10:19,241
If all the labels in a subset match, as you can see on top,

201
00:10:19,301 --> 00:10:21,482
all the labels are attack, that becomes

202
00:10:21,502 --> 00:10:22,903
a leaf in your decision tree.

203
00:10:23,603 --> 00:10:26,304
Otherwise, with the remaining examples,

204
00:10:26,344 --> 00:10:28,925
you look for another feature, and you keep dividing.

205
00:10:31,005 --> 00:10:33,147
For continuous thresholds like hit points,

206
00:10:33,487 --> 00:10:36,070
you need a way to choose a split threshold,

207
00:10:36,491 --> 00:10:39,173
like the point at which things get divided.

208
00:10:39,194 --> 00:10:42,357
A really simple approach is to try

209
00:10:42,397 --> 00:10:44,239
splitting at a few random different thresholds

210
00:10:44,299 --> 00:10:45,520
and see which one works better.

211
00:10:45,981 --> 00:10:48,263
There are more clever approaches you can use,

212
00:10:48,363 --> 00:10:49,965
but it gets into some fairly deep stats.

213
00:10:52,642 --> 00:10:54,322
Decision tree learning isn't perfect.

214
00:10:54,703 --> 00:10:57,364
It can be difficult to tune the complexity of the trees.

215
00:10:57,884 --> 00:11:00,685
Overcomplicated ones will fixate on irrelevant features

216
00:11:00,785 --> 00:11:01,986
once they get down near the leaves,

217
00:11:02,386 --> 00:11:04,487
while oversimplified ones will ignore

218
00:11:04,547 --> 00:11:06,088
genuine special cases entirely.

219
00:11:07,429 --> 00:11:09,850
And they're not very good at discovering relationships

220
00:11:09,910 --> 00:11:13,411
between continuous features, like if you should retreat

221
00:11:13,451 --> 00:11:16,153
when your hit points get below the player's attack strength.

222
00:11:17,233 --> 00:11:20,555
Nevertheless, the simplicity and the transparency here

223
00:11:20,595 --> 00:11:21,675
can be a huge benefit.

224
00:11:22,250 --> 00:11:25,835
If you only remember one technique from today, let it be the ID3 algorithm

225
00:11:26,156 --> 00:11:27,198
for building decision trees.

226
00:11:29,938 --> 00:11:31,139
On to the nearest neighbor algorithm.

227
00:11:31,599 --> 00:11:33,821
With this one, there's no real training process.

228
00:11:34,161 --> 00:11:35,802
The model is the training data.

229
00:11:36,422 --> 00:11:38,083
When it needs to make a classification,

230
00:11:38,443 --> 00:11:41,485
it just determines, say, what's the most similar training

231
00:11:41,525 --> 00:11:45,768
example it's seen before based on the numbers in that example

232
00:11:45,808 --> 00:11:48,229
being close to the input numbers according

233
00:11:48,269 --> 00:11:49,750
to some measure of distance, which

234
00:11:49,790 --> 00:11:50,991
I'll talk a bit about later.

235
00:11:51,451 --> 00:11:53,392
And it just uses that example's label.

236
00:11:53,632 --> 00:11:54,273
It's dead simple.

237
00:11:57,580 --> 00:12:00,703
nearest neighbor by itself is really prone to overfitting.

238
00:12:01,744 --> 00:12:03,626
If there's any problems in your training set,

239
00:12:04,146 --> 00:12:07,209
any examples that maybe should have been labeled differently,

240
00:12:07,569 --> 00:12:10,112
that's going to show up in your model as classification errors.

241
00:12:10,752 --> 00:12:13,255
This test point should probably be yellow,

242
00:12:13,355 --> 00:12:15,377
but it happens to be closest to that red dot.

243
00:12:16,581 --> 00:12:19,324
So what's always done instead is called k-nearest neighbors,

244
00:12:19,805 --> 00:12:22,368
where you take the closest, say, five examples

245
00:12:22,669 --> 00:12:24,711
and let them vote on what the label should be,

246
00:12:25,472 --> 00:12:27,394
maybe giving more weight to closer examples.

247
00:12:29,517 --> 00:12:32,140
What just happened here?

248
00:12:32,160 --> 00:12:33,302
Oh, I think I went to the last slide.

249
00:12:42,245 --> 00:12:46,807
The higher the K is, the more it smooths over errors in your training set.

250
00:12:49,129 --> 00:12:52,590
But, it makes it harder to respect genuine special cases

251
00:12:52,630 --> 00:12:55,552
where the classification should differ from other nearby examples.

252
00:12:59,744 --> 00:13:01,505
KNN has some problems of its own.

253
00:13:01,785 --> 00:13:03,105
The big one is dimensionality.

254
00:13:03,706 --> 00:13:05,666
If your data set has only a few dimensions,

255
00:13:05,706 --> 00:13:07,147
you can do things really efficiently

256
00:13:07,847 --> 00:13:09,848
with techniques like KD trees.

257
00:13:10,328 --> 00:13:13,009
But in higher dimensions, these things don't work very well.

258
00:13:13,049 --> 00:13:14,729
You end up basically doing a linear scan

259
00:13:14,749 --> 00:13:15,450
through all your data.

260
00:13:15,470 --> 00:13:16,550
And that can get really slow.

261
00:13:17,050 --> 00:13:18,891
There's also a subtler problem where

262
00:13:18,931 --> 00:13:21,852
in really high dimensions, a few training examples

263
00:13:22,232 --> 00:13:24,173
more or less take over the entire data set.

264
00:13:24,873 --> 00:13:27,394
A little go for dimensionality later on.

265
00:13:28,668 --> 00:13:31,129
The other thing is, you need to decide what distance means.

266
00:13:31,269 --> 00:13:33,469
If your features aren't all in the same units,

267
00:13:33,589 --> 00:13:35,430
and generally they're not, then you

268
00:13:35,470 --> 00:13:37,650
need to scale them appropriately.

269
00:13:38,491 --> 00:13:39,811
Lynn, again, will talk about that later.

270
00:13:40,351 --> 00:13:42,512
And distance gets even more difficult

271
00:13:42,712 --> 00:13:46,373
to work with when your examples aren't continuous,

272
00:13:46,413 --> 00:13:47,873
but rather categorical data.

273
00:13:48,253 --> 00:13:51,094
What should the distance between orc and goblin be?

274
00:13:51,194 --> 00:13:52,634
Which one is closer to horse?

275
00:13:54,275 --> 00:13:56,495
It's a weird question to ask, but you need an answer for it

276
00:13:56,636 --> 00:13:58,036
if you're going to do k-nearest neighbors.

277
00:13:59,424 --> 00:14:02,946
So KNN can give you really high quality results,

278
00:14:03,347 --> 00:14:05,848
but it's limited to low dimensional feature spaces,

279
00:14:06,109 --> 00:14:08,130
and you'll want your training data to be really clean.

280
00:14:13,319 --> 00:14:17,940
Genetic algorithms, which is really a whole spectrum of techniques, but they all have a few things in common.

281
00:14:18,560 --> 00:14:23,702
The basic idea is to find a good solution over time by trying out a bunch of potential solutions.

282
00:14:24,342 --> 00:14:29,063
They get tested out against some black box fitness function, which decides how awesome each one is.

283
00:14:29,503 --> 00:14:33,324
The ones that do well get to pass on their genes to the next generation of solutions.

284
00:14:33,984 --> 00:14:35,965
Usually the genes are randomly changed a little bit,

285
00:14:36,465 --> 00:14:39,746
and you keep doing that until you breed a race of super solutions.

286
00:14:40,977 --> 00:14:43,480
This isn't exactly an ML technique specifically

287
00:14:43,580 --> 00:14:46,003
because it's not necessarily about learning from data.

288
00:14:46,364 --> 00:14:48,787
But if you're trying to build a really good model,

289
00:14:49,087 --> 00:14:51,990
you can use your test set as the black box

290
00:14:53,092 --> 00:14:54,594
and have the genetic algorithm mess around

291
00:14:54,634 --> 00:14:56,035
with the parameters of your model

292
00:14:56,316 --> 00:14:57,497
to make it as good as possible.

293
00:15:01,137 --> 00:15:02,938
There's a couple of decisions to be made

294
00:15:03,459 --> 00:15:05,321
when developing a genetic algorithm.

295
00:15:05,361 --> 00:15:07,843
The most important one is coming up with the rules

296
00:15:07,883 --> 00:15:10,285
of how genes get passed down to the next generation.

297
00:15:10,946 --> 00:15:14,089
A really common approach is called roulette wheel selection,

298
00:15:14,109 --> 00:15:17,432
where you randomly pick genes directly weighted by fitness score.

299
00:15:18,093 --> 00:15:21,216
So, if model A scored twice as well as model B,

300
00:15:21,676 --> 00:15:25,960
then each gene is twice as likely to come from A than from B.

301
00:15:28,180 --> 00:15:31,842
Roulette wheel selection relies on a well-behaved fitness score.

302
00:15:32,442 --> 00:15:37,244
If it's possible for one contestant to rack up a really high score compared

303
00:15:37,264 --> 00:15:40,806
to slightly inferior contestants, then those other contestants' genes

304
00:15:41,086 --> 00:15:42,967
are just going to disappear in the next generation,

305
00:15:43,527 --> 00:15:45,688
even if some of them could have been useful when combined.

306
00:15:46,809 --> 00:15:49,610
So an alternative is rank selection, where

307
00:15:49,690 --> 00:15:53,312
instead of using the score directly, you weight the selection based

308
00:15:53,372 --> 00:15:55,433
on what place each contestant came in.

309
00:15:56,532 --> 00:15:59,994
So, say, half the genes come from the first place solution,

310
00:16:00,475 --> 00:16:02,857
and a quarter come from the second place solution, and so on,

311
00:16:03,957 --> 00:16:05,719
regardless of what the raw scores were.

312
00:16:06,740 --> 00:16:08,461
This avoids that crowding out effect,

313
00:16:08,561 --> 00:16:10,322
particularly in the earlier generations,

314
00:16:10,342 --> 00:16:11,383
when things are pretty random,

315
00:16:11,423 --> 00:16:12,924
and some people just happen to be cooler.

316
00:16:14,766 --> 00:16:16,807
It does, however, tend to slow things down.

317
00:16:17,328 --> 00:16:19,590
It takes more generations to converge to an optimum.

318
00:16:23,240 --> 00:16:25,482
Genetic algorithms also aren't always the way to go.

319
00:16:26,042 --> 00:16:27,564
The big thing is they don't really

320
00:16:27,644 --> 00:16:30,106
know anything about the problem they're trying to solve.

321
00:16:30,127 --> 00:16:31,047
They're just banging at it.

322
00:16:31,608 --> 00:16:33,169
If there's an optimization method

323
00:16:33,370 --> 00:16:37,514
which is specific to the model, like ID3 for decision trees,

324
00:16:38,074 --> 00:16:40,016
it'll usually be the genetic algorithm

325
00:16:40,056 --> 00:16:41,498
in terms of speed and precision.

326
00:16:43,419 --> 00:16:48,140
The other problem with GAs is they're a whole extra panel of knobs and buttons to mess with,

327
00:16:48,461 --> 00:16:49,961
which isn't necessarily what you want.

328
00:16:50,021 --> 00:16:53,742
It can be really time-consuming to go through the results of a genetic algorithm

329
00:16:53,762 --> 00:16:55,383
to try to figure out why it did what it did.

330
00:16:55,843 --> 00:16:59,484
So tweaking it is usually a try-and-see sort of thing, which is obviously not great.

331
00:17:00,864 --> 00:17:02,965
Genetic algorithms, though, are really flexible,

332
00:17:03,245 --> 00:17:06,366
and it can be tempting to think of them as the solution to every problem.

333
00:17:06,766 --> 00:17:08,887
But I think they're best used as a backup plan

334
00:17:09,227 --> 00:17:12,528
once you've done everything you could with more model-specific techniques.

335
00:17:15,697 --> 00:17:18,780
OK, so now let's talk about all the different things that

336
00:17:18,820 --> 00:17:21,383
can go wrong, starting with the wrong features.

337
00:17:21,423 --> 00:17:23,485
Because having good features can really

338
00:17:23,565 --> 00:17:25,147
help your model perform better.

339
00:17:25,427 --> 00:17:27,550
And it's not a cut and dry process.

340
00:17:30,493 --> 00:17:32,014
So here's a situation you might see.

341
00:17:32,555 --> 00:17:34,197
You've tried a lot of different models,

342
00:17:34,237 --> 00:17:36,179
but you keep getting disappointing results.

343
00:17:36,459 --> 00:17:37,180
So what should you do?

344
00:17:38,995 --> 00:17:40,876
So the first thing you should do is look at your data

345
00:17:41,136 --> 00:17:44,218
and do something called exploratory data analysis or EDA.

346
00:17:44,538 --> 00:17:45,759
And what this is gonna do

347
00:17:45,959 --> 00:17:47,580
is let you check all your assumptions.

348
00:17:47,640 --> 00:17:51,022
So you can basically see which of your input variables

349
00:17:51,202 --> 00:17:52,603
actually seem to have a relationship

350
00:17:52,663 --> 00:17:54,384
with the outcomes that you're interested in.

351
00:17:54,564 --> 00:17:57,847
And also you'll be able to see what the shape

352
00:17:57,907 --> 00:18:00,108
and distributions of the various features

353
00:18:00,128 --> 00:18:01,409
you're interested in are.

354
00:18:03,129 --> 00:18:05,670
Then once you've done that, you can now boil down your data.

355
00:18:05,810 --> 00:18:11,231
So you can remove irrelevancies such as input features that

356
00:18:11,271 --> 00:18:12,451
have nothing to do with the outcome.

357
00:18:12,931 --> 00:18:15,152
Or you might even target some of your features.

358
00:18:16,072 --> 00:18:19,193
For example, cropping images so that the model can just

359
00:18:19,253 --> 00:18:22,054
focus on the aspects of the features that

360
00:18:22,114 --> 00:18:23,894
are probably most relevant.

361
00:18:26,427 --> 00:18:30,055
And then once you've done that, you can even look at your data some more and check whether

362
00:18:30,135 --> 00:18:31,718
transformations of your data help.

363
00:18:31,778 --> 00:18:36,628
So for example, you might see whether linearizing your data or normalizing your data can actually

364
00:18:36,768 --> 00:18:36,949
help.

365
00:18:38,741 --> 00:18:40,122
help your model work with your features.

366
00:18:40,182 --> 00:18:42,624
So here, you know, we can apply a log transform

367
00:18:42,664 --> 00:18:44,526
and now we have something that looks

368
00:18:44,606 --> 00:18:45,707
like a normal distribution.

369
00:18:46,127 --> 00:18:48,289
Or conversely, if we're working with images,

370
00:18:48,749 --> 00:18:50,771
we could desaturate the image so we work

371
00:18:50,871 --> 00:18:53,012
with single intensity values instead

372
00:18:53,032 --> 00:18:54,494
of a triplet of RGB numbers.

373
00:18:54,694 --> 00:18:56,415
And furthermore, we might even try blurring it

374
00:18:56,555 --> 00:18:58,877
so that the changes between pixels are gradual.

375
00:19:03,029 --> 00:19:05,792
And another thing we might do, just in the spirit of transformation,

376
00:19:05,812 --> 00:19:08,294
so this is something that's come up a couple times in this session,

377
00:19:08,534 --> 00:19:10,696
is make sure all your features have comparable scale.

378
00:19:10,976 --> 00:19:14,820
And it's really easy to have an intuition about why this needs to be true,

379
00:19:15,500 --> 00:19:18,863
particularly if your algorithm is based on distance metrics.

380
00:19:20,995 --> 00:19:26,017
Here's an example. So like suppose you have a feature, and it's got three, uh, feature vector, you've got three features in it.

381
00:19:26,117 --> 00:19:32,240
Weapon power, player level, and gold amount. And suppose gold amount has a range that's much, much greater than all the other features.

382
00:19:32,640 --> 00:19:37,401
Any distance metric that's based on this feature vector is going to be completely dominated by gold amount.

383
00:19:37,481 --> 00:19:42,683
So what should you do? You should scale down gold amount so that it has basically the same range as the other features.

384
00:19:42,783 --> 00:19:47,765
In fact, you might want to renormalize all these features so that they lie between 0 and 1.

385
00:19:50,900 --> 00:19:52,221
So here's another thing you might see.

386
00:19:52,961 --> 00:19:54,902
You're feeding in 50,000 features,

387
00:19:54,982 --> 00:19:56,102
and your classifier sucks.

388
00:19:56,563 --> 00:19:58,183
Furthermore, it worked better when

389
00:19:58,203 --> 00:19:59,484
it was only 100 features.

390
00:20:00,204 --> 00:20:01,805
So what might be going on?

391
00:20:02,065 --> 00:20:05,966
Well, it's very likely that you've come across the curse

392
00:20:05,986 --> 00:20:06,947
of dimensionality.

393
00:20:07,407 --> 00:20:08,768
And so the issue here is

394
00:20:10,190 --> 00:20:14,854
When your features become very, very, very highly dimensional,

395
00:20:15,334 --> 00:20:16,776
everything becomes far apart.

396
00:20:16,836 --> 00:20:18,617
Like, your intuition about distances

397
00:20:18,677 --> 00:20:21,439
don't apply the same way they do in three and four

398
00:20:21,959 --> 00:20:23,581
and small dimensions.

399
00:20:24,181 --> 00:20:25,883
And the other thing that's a problem

400
00:20:25,963 --> 00:20:28,064
is, as your feature space grows, you

401
00:20:28,104 --> 00:20:30,106
need more and more examples to understand it.

402
00:20:30,186 --> 00:20:31,927
So like, for the same size training set,

403
00:20:32,247 --> 00:20:35,790
you get more bang for your buck when you have fewer features

404
00:20:35,830 --> 00:20:36,711
than when you have a lot.

405
00:20:40,225 --> 00:20:41,425
So what can you do about this?

406
00:20:41,505 --> 00:20:44,366
Well, you can try reducing the dimensionality of your data.

407
00:20:44,426 --> 00:20:45,967
For example, you might use a technique

408
00:20:46,007 --> 00:20:47,547
like principal component analysis.

409
00:20:48,567 --> 00:20:50,908
And so what PCA will do for you is

410
00:20:50,948 --> 00:20:53,949
it can take a big vector of possibly correlated features

411
00:20:54,109 --> 00:20:58,370
and then reduce them down into a small number of features

412
00:20:58,390 --> 00:20:59,491
that are uncorrelated.

413
00:20:59,891 --> 00:21:03,372
And so here's an example based on walking data.

414
00:21:03,392 --> 00:21:03,432
So.

415
00:21:06,996 --> 00:21:08,977
Classes of motion work great with PCA,

416
00:21:09,017 --> 00:21:12,198
because a lot of the joint movements

417
00:21:12,238 --> 00:21:15,120
tend to be correlated, like the arms and legs move together.

418
00:21:15,560 --> 00:21:18,141
So here, we started with a feature vector

419
00:21:18,161 --> 00:21:22,483
that had 540 features in it, and PCA reduces it to 29 features.

420
00:21:25,565 --> 00:21:27,346
So here's another situation you might encounter.

421
00:21:27,646 --> 00:21:29,747
You have insanely good accuracy in the test set,

422
00:21:29,787 --> 00:21:31,748
but the model is still terrible in practice.

423
00:21:33,392 --> 00:21:36,915
So here, one possible problem is contamination.

424
00:21:36,975 --> 00:21:39,758
Just some of your test data snuck into your training set,

425
00:21:40,539 --> 00:21:42,361
and you should just fix, it's a bug,

426
00:21:42,381 --> 00:21:43,763
you should just check and fix your code.

427
00:21:45,842 --> 00:21:50,063
Another thing that might be happening, which is a little more subtle, is data leakage.

428
00:21:50,503 --> 00:21:56,224
And here what's going on is you're using a feature for prediction that's essentially

429
00:21:56,285 --> 00:21:56,705
cheating.

430
00:21:57,005 --> 00:22:00,625
Like a feature that you wouldn't normally have available for prediction.

431
00:22:00,846 --> 00:22:04,746
And so this is something that can particularly happen if you're working with log file data,

432
00:22:04,766 --> 00:22:05,346
for example.

433
00:22:05,747 --> 00:22:09,827
And here, you know, suppose you're trying to predict score from weapon power and player

434
00:22:09,867 --> 00:22:10,108
level.

435
00:22:12,028 --> 00:22:15,958
If score is a function of kills, so that the more kills you get, the higher your score,

436
00:22:16,319 --> 00:22:19,728
you don't want to use the number of kills for prediction because that's essentially cheating.

437
00:22:19,888 --> 00:22:20,830
You don't have that data.

438
00:22:23,830 --> 00:22:27,771
And the next possibility is sampling bias.

439
00:22:27,791 --> 00:22:29,371
And in this case, the training data

440
00:22:29,391 --> 00:22:31,572
is just not similar enough to the real world.

441
00:22:32,532 --> 00:22:35,693
And basically, decisions of how, what, and when you log data

442
00:22:35,733 --> 00:22:36,533
can really matter.

443
00:22:36,733 --> 00:22:39,794
And just to kind of give some intuition about why this can

444
00:22:39,814 --> 00:22:42,775
happen or why this is true, just think

445
00:22:42,815 --> 00:22:44,055
about the behaviors of players.

446
00:22:44,135 --> 00:22:46,156
Like, the players who log in every day

447
00:22:46,496 --> 00:22:48,816
likely behave really differently than the players

448
00:22:48,856 --> 00:22:49,717
who log in once a week.

449
00:22:52,037 --> 00:22:55,018
So now let's talk about when things go wrong with the wrong model.

450
00:22:56,559 --> 00:23:00,500
So here's the situation. You've tried a lot of different features, but you have disappointing results.

451
00:23:02,781 --> 00:23:02,921
So...

452
00:23:04,055 --> 00:23:06,296
What's a good solution? Well, trying a different model.

453
00:23:06,676 --> 00:23:08,816
Or actually, you can try lots and lots of models.

454
00:23:09,197 --> 00:23:12,558
And there are tools like Weka, which can come to the rescue.

455
00:23:12,778 --> 00:23:14,839
And so in particular, Weka will let you, like,

456
00:23:15,619 --> 00:23:17,940
input a big matrix of features

457
00:23:18,040 --> 00:23:20,461
and then run it on lots and lots of different models

458
00:23:20,741 --> 00:23:21,721
and give you the results.

459
00:23:22,101 --> 00:23:25,022
And it even has tools for clustering and preprocessing, too.

460
00:23:27,123 --> 00:23:30,063
The other thing you might try is using an ensemble of models.

461
00:23:30,504 --> 00:23:33,184
And some names for different ensemble techniques

462
00:23:33,264 --> 00:23:35,105
are boosting, stacking, bagging.

463
00:23:35,525 --> 00:23:37,005
And the basic idea of all of these

464
00:23:37,085 --> 00:23:40,126
is a bunch of weak learners working together

465
00:23:40,226 --> 00:23:42,806
can often outperform a more sophisticated learner.

466
00:23:43,447 --> 00:23:45,387
And to give an example, large ensemble models

467
00:23:45,427 --> 00:23:48,268
were the best performers in the Netflix prize

468
00:23:48,288 --> 00:23:48,948
a couple years ago.

469
00:23:51,021 --> 00:23:52,762
And so now let's talk about overfitting,

470
00:23:52,802 --> 00:23:54,642
which is another thing that's been mentioned a lot

471
00:23:54,743 --> 00:23:55,363
in this session.

472
00:23:55,643 --> 00:23:57,564
So here the situation is your classifier

473
00:23:57,584 --> 00:23:59,184
has amazing accuracy with the training set,

474
00:23:59,224 --> 00:24:01,865
but performs poorly on data it's never seen before.

475
00:24:03,086 --> 00:24:03,906
So what's happening?

476
00:24:04,427 --> 00:24:06,347
Basically what's happening is your model

477
00:24:06,587 --> 00:24:08,888
is memorizing the training set and has lost

478
00:24:08,968 --> 00:24:10,269
its ability to generalize.

479
00:24:10,429 --> 00:24:13,130
And this is just part of a broader challenge

480
00:24:13,250 --> 00:24:16,752
when you're developing and designing your models, where

481
00:24:16,772 --> 00:24:19,133
you need to balance having a model that's.

482
00:24:20,019 --> 00:24:24,123
too simple where the data can't capture the patterns that you want to capture.

483
00:24:24,183 --> 00:24:27,326
Like, so for example, having a decision tree that doesn't have enough depth,

484
00:24:27,767 --> 00:24:34,254
using a linear model when it's not appropriate, or using k-nearest neighbor with too many neighbors,

485
00:24:34,774 --> 00:24:38,278
versus the other extreme where you have a model that's way too complex,

486
00:24:38,338 --> 00:24:41,281
it has too many parameters you can fit, too many bells and whistles.

487
00:24:41,642 --> 00:24:44,502
and you get these schizo fits with no ability to generalize.

488
00:24:44,602 --> 00:24:46,562
And so this can happen when you have decision trees

489
00:24:46,582 --> 00:24:50,423
with arbitrary depth or using only one nearest neighbor

490
00:24:50,623 --> 00:24:54,024
or using a two high dimensional spline

491
00:24:54,044 --> 00:24:55,464
to fit your data points.

492
00:24:57,544 --> 00:25:00,345
So some overly fitty algorithms,

493
00:25:00,405 --> 00:25:01,825
again, things that have been mentioned today,

494
00:25:01,925 --> 00:25:03,726
k-nearest neighbors with low k,

495
00:25:04,546 --> 00:25:06,646
artificial neural networks with lots of neurons,

496
00:25:06,686 --> 00:25:09,267
decision trees with arbitrary depth and ensemble models.

497
00:25:11,414 --> 00:25:13,697
So how do you protect yourself against this?

498
00:25:15,059 --> 00:25:18,143
Well, using cross-validation is great.

499
00:25:18,464 --> 00:25:20,967
So what this lets you do is estimate how well your model

500
00:25:21,007 --> 00:25:21,989
performs on new data.

501
00:25:24,452 --> 00:25:25,893
And how does it do that?

502
00:25:25,993 --> 00:25:28,355
Well, by holding out, it lets you hold out

503
00:25:28,415 --> 00:25:29,936
subsets of your training data, which

504
00:25:29,956 --> 00:25:31,216
you can then use for testing.

505
00:25:31,777 --> 00:25:33,218
And while you're doing this, you can

506
00:25:33,258 --> 00:25:35,699
try different model parameters to determine

507
00:25:35,719 --> 00:25:38,160
the balance between, a good balance between simplicity

508
00:25:38,200 --> 00:25:38,621
and power.

509
00:25:39,301 --> 00:25:41,262
So just let me quickly show you how it works.

510
00:25:41,322 --> 00:25:41,903
It's very simple.

511
00:25:42,263 --> 00:25:44,604
You split your examples into training and test sets.

512
00:25:44,644 --> 00:25:46,505
So here we have 90, 10 split.

513
00:25:47,406 --> 00:25:48,306
Train your model.

514
00:25:49,467 --> 00:25:51,608
Then you evaluate your model just on your test data.

515
00:25:53,986 --> 00:25:59,647
And then you redo that with totally new training and test data.

516
00:26:01,208 --> 00:26:05,209
And then you do the, and then the average over multiple test sets is your actual estimate

517
00:26:05,229 --> 00:26:07,430
of how you're going to do on new data you've never seen before.

518
00:26:08,830 --> 00:26:09,810
So now to sum up.

519
00:26:11,291 --> 00:26:14,731
If you've fallen asleep during this talk, please wake up and internalize the following

520
00:26:14,751 --> 00:26:14,972
points.

521
00:26:17,023 --> 00:26:19,625
Contrary to what may have been your experience in the past,

522
00:26:19,785 --> 00:26:20,986
ML can be real time.

523
00:26:21,026 --> 00:26:21,986
It can be transparent.

524
00:26:22,026 --> 00:26:24,028
It can be totally reliable for your needs.

525
00:26:24,448 --> 00:26:26,669
It can be the best way for you to do your job.

526
00:26:27,350 --> 00:26:29,391
But in order to get good results out of it,

527
00:26:29,651 --> 00:26:31,412
you need to step outside your comfort zone.

528
00:26:31,432 --> 00:26:33,053
You need to take off your programmer hat.

529
00:26:33,814 --> 00:26:37,576
You need to move beyond the very small and limiting number

530
00:26:37,876 --> 00:26:41,278
of algorithms you've learned for machine learning,

531
00:26:41,598 --> 00:26:43,139
such as artificial neural networks

532
00:26:43,459 --> 00:26:44,440
and genetic algorithms.

533
00:26:45,269 --> 00:26:51,012
You need to understand at least some of the theoretical underpinnings of features and models.

534
00:26:51,312 --> 00:26:56,896
And you need to be aware of things like overfitting that really bite us all in the butt when we're doing this stuff.

535
00:27:00,658 --> 00:27:04,340
If you want more information about this, there is a free online course on machine learning

536
00:27:04,380 --> 00:27:08,742
offered by Stanford University and a couple of other good reads in this area.

537
00:27:09,322 --> 00:27:09,903
Thank you for your time.

