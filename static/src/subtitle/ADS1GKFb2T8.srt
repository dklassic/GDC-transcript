1
00:00:05,415 --> 00:00:13,063
I am Jin Hyun-Jung, the team leader for this project and the first speaker of this session.

2
00:00:13,063 --> 00:00:19,791
In this session, we will share our experience of making AI agents for the Arena Battle of

3
00:00:19,791 --> 00:00:20,472
Red and Soul.

4
00:00:22,954 --> 00:00:26,915
At first, I will introduce the problem we solved and the key

5
00:00:26,915 --> 00:00:28,516
ideas for it.

6
00:00:28,516 --> 00:00:32,597
And then, soon, the next speaker will explain how we

7
00:00:32,597 --> 00:00:35,658
solved the problems and challenges and show

8
00:00:35,658 --> 00:00:40,779
experimental results, including demo videos.

9
00:00:40,779 --> 00:00:45,381
Let's start with the problem explanation.

10
00:00:45,381 --> 00:00:50,922
Bread and Soul is an MMORPG launched in 2012 and is now

11
00:00:50,922 --> 00:00:52,103
being serviced globally.

12
00:00:53,208 --> 00:00:56,709
Bread and Soul has shown distinctive visual styles,

13
00:00:56,709 --> 00:01:00,151
movie-like scenes, and combat inspired

14
00:01:00,151 --> 00:01:04,433
by realistic martial arts.

15
00:01:04,433 --> 00:01:12,096
This is the arena battle of Bread and Soul.

16
00:01:12,096 --> 00:01:17,939
The arena battle is a PVP subcontent of Bread and Soul.

17
00:01:17,939 --> 00:01:20,380
Two players fight against each other

18
00:01:20,380 --> 00:01:22,001
in an instant battle arena.

19
00:01:23,012 --> 00:01:27,834
Two win, each should make the opponent HP to zero

20
00:01:27,834 --> 00:01:30,155
in three minutes.

21
00:01:30,155 --> 00:01:33,897
If both of their HP is still above zero after timeout,

22
00:01:33,897 --> 00:01:36,058
the one who deal more damage wins.

23
00:01:36,058 --> 00:01:40,420
For the fairness, their stats are standardized.

24
00:01:40,420 --> 00:01:45,782
Our research goal is to create pro-level AI agents

25
00:01:45,782 --> 00:01:49,504
in one-on-one arena battle using reinforcement learning.

26
00:01:51,390 --> 00:01:55,971
The reinforcement learning has been successfully applied to many games,

27
00:01:55,971 --> 00:02:02,634
including some Atari games, Go, Chess, Quake 3, Dota 2, and StarCraft 2.

28
00:02:02,634 --> 00:02:09,316
However, the AI modules of many commercial games are still being composed with handwritten rules.

29
00:02:09,316 --> 00:02:15,939
This tends to make agents simple with full of weakness, so as to give limited experience to users.

30
00:02:17,653 --> 00:02:21,036
To make various and flawless agents,

31
00:02:21,036 --> 00:02:23,757
we tried to apply reinforcement learning to it.

32
00:02:23,757 --> 00:02:29,001
To achieve it more effectively, we also

33
00:02:29,001 --> 00:02:34,064
tried to give styles to agents, aggressive, balanced,

34
00:02:34,064 --> 00:02:35,925
and defensive.

35
00:02:35,925 --> 00:02:39,707
We thought that this would give various experiences to users.

36
00:02:40,806 --> 00:02:45,727
In Bread and Soul, there are 11 classes Destroyer,

37
00:02:45,727 --> 00:02:49,549
Breadmaster, Assassin, Summoner, and so on.

38
00:02:49,549 --> 00:02:53,070
To reduce complexity, we made a Destroyer agent

39
00:02:53,070 --> 00:02:55,470
that could fight against Destroyers only.

40
00:02:55,470 --> 00:02:58,631
We would choose a Destroyer because it's a class

41
00:02:58,631 --> 00:03:03,253
that steadily appears in Bread and Soul World Championship.

42
00:03:03,253 --> 00:03:07,354
Their skill settings are fixed to be the same

43
00:03:07,354 --> 00:03:08,034
for the fairness.

44
00:03:10,533 --> 00:03:15,258
To describe an agent, we drew the Blood and Soul Arena battle in abstract form.

45
00:03:15,258 --> 00:03:23,547
An agent would observe the environment, including its own state, the opponent's state, and arena state.

46
00:03:23,547 --> 00:03:34,238
The observed states include hit point, skill point, the distance to the opponent, the skill cooldown, distance to the arena boundary, and so on.

47
00:03:35,562 --> 00:03:39,883
An agent would choose an appropriate action for the observation.

48
00:03:39,883 --> 00:03:47,224
The action affects the environment, and in turn, the agent observes the changed state

49
00:03:47,224 --> 00:03:48,605
to decide the next action.

50
00:03:48,605 --> 00:03:54,106
The agent has a chance for choosing actions every 100 milliseconds.

51
00:03:54,106 --> 00:04:01,267
In the Radiant Soul Arena Battle, action can be categorized into skill, move, and targeting.

52
00:04:01,918 --> 00:04:08,503
These actions should be decided strategically, since they have a trade-off between cost and

53
00:04:08,503 --> 00:04:08,963
effect.

54
00:04:12,117 --> 00:04:17,641
I'll explain the skills of Destroyer with examples of strategy decisions.

55
00:04:17,641 --> 00:04:24,626
It has 44 kinds of skills, which can be categorized as crowd control skill, damage dealing skills,

56
00:04:24,626 --> 00:04:28,289
resistance skills, and escape skills.

57
00:04:28,289 --> 00:04:34,314
To deal damage continuously, an agent should decide one of the crowd control skills before

58
00:04:34,314 --> 00:04:35,075
damage dealing skills.

59
00:04:35,075 --> 00:04:38,417
If the opponent is hit by a crowd control skill,

60
00:04:39,269 --> 00:04:48,117
The agent would decide damage dealing skills consecutively to reduce the opponent's HP as much as possible.

61
00:04:48,117 --> 00:04:58,828
To avoid such a situation, the opponent should use one of the resistance skills before being hit or use one of the escape skills after being hit.

62
00:05:00,315 --> 00:05:06,858
Since the resistance skills and escape skills have a large opportunity cost of cool time,

63
00:05:06,858 --> 00:05:09,599
you should decide to use them carefully.

64
00:05:09,599 --> 00:05:13,961
In some situations, you should save them to use later.

65
00:05:13,961 --> 00:05:18,143
This makes it difficult to learn the optimal timing for skill decision.

66
00:05:18,143 --> 00:05:24,146
In this project, we faced four challenges.

67
00:05:24,146 --> 00:05:29,328
High complexity, real-time response, generalization, and guiding fighting styles.

68
00:05:30,869 --> 00:05:34,390
The first challenge is high complexity.

69
00:05:34,390 --> 00:05:37,150
When compared with the game of Go,

70
00:05:37,150 --> 00:05:39,471
our agents have larger action space.

71
00:05:39,471 --> 00:05:44,192
In case of Go, the degree of freedom of each action space

72
00:05:44,192 --> 00:05:45,712
is about 10 to the power of 170.

73
00:05:45,712 --> 00:05:48,893
On the other hand, our dead of hours

74
00:05:48,893 --> 00:05:50,773
is about 10 to the power of 1,800.

75
00:05:50,773 --> 00:05:55,554
Longer game lengths and the combined action

76
00:05:55,554 --> 00:05:59,035
of skills, moves, and targeting make the action space larger.

77
00:06:00,374 --> 00:06:03,936
This makes our problem so challenging.

78
00:06:03,936 --> 00:06:07,357
To solve this challenge, we constrain the freedom

79
00:06:07,357 --> 00:06:10,098
of skill selection and reduce the decision

80
00:06:10,098 --> 00:06:11,259
interval of movements.

81
00:06:11,259 --> 00:06:17,562
The details about them will be explained later.

82
00:06:17,562 --> 00:06:19,342
Second challenge is the constraint

83
00:06:19,342 --> 00:06:20,463
of real-time response.

84
00:06:21,885 --> 00:06:24,848
Since the Arena Battle is a real-time fighting game,

85
00:06:24,848 --> 00:06:27,550
late response results in poor performance,

86
00:06:27,550 --> 00:06:30,853
even if our agent decides the optimal actions.

87
00:06:30,853 --> 00:06:35,136
This restricts the method for implementing the decision

88
00:06:35,136 --> 00:06:35,597
policy.

89
00:06:35,597 --> 00:06:41,141
For example, we cannot use any search-based method, which

90
00:06:41,141 --> 00:06:43,983
requires a huge amount of processing.

91
00:06:46,662 --> 00:06:53,024
The neural networks are very flexible and powerful computational methods to express a policy.

92
00:06:53,024 --> 00:07:01,447
Using neural networks, the computational complexity could be easily controlled to satisfy the real-time constraints.

93
00:07:01,447 --> 00:07:06,268
The third challenge is generalization.

94
00:07:06,268 --> 00:07:12,010
Actually, we could not expect which pro gamers our agents would encounter.

95
00:07:12,010 --> 00:07:15,371
To make our agent play well against any pro gamer,

96
00:07:16,746 --> 00:07:23,530
We needed to train our agents against the opponents of various styles.

97
00:07:23,530 --> 00:07:29,093
For this purpose, we provide our agents with sparring partners from the opponent pool of

98
00:07:29,093 --> 00:07:29,533
selves.

99
00:07:29,533 --> 00:07:37,917
Trained agents of various styles are added to the opponent pool continuously to have

100
00:07:37,917 --> 00:07:40,959
stronger and more diverse sparring partners.

101
00:07:43,209 --> 00:07:49,952
Finally, we needed to guide fighting styles without hand-written rules.

102
00:07:49,952 --> 00:07:53,934
Humans can easily adapt to the monotonous agents and feel bored.

103
00:07:53,934 --> 00:07:59,617
The agents of various styles could reduce this.

104
00:07:59,617 --> 00:08:03,739
We defined the trainable styles according to their aggressiveness.

105
00:08:03,739 --> 00:08:09,881
The aggressiveness could be guided with carefully designed rewards.

106
00:08:10,970 --> 00:08:14,577
The next speaker will explain how we tune the reverse

107
00:08:14,577 --> 00:08:20,290
to guide the different styles.

108
00:08:21,928 --> 00:08:25,089
Hello, I am Seunghoon Roh, AI research engineer from NCSoft.

109
00:08:25,089 --> 00:08:27,330
I will cover the methods applied

110
00:08:27,330 --> 00:08:29,951
for creating the Arena Battle AI.

111
00:08:29,951 --> 00:08:33,272
Since our agents are trained with reinforcement learning,

112
00:08:33,272 --> 00:08:36,833
RL, I will first introduce the basic concepts of RL,

113
00:08:36,833 --> 00:08:39,675
and then I will explain the overall learning process,

114
00:08:39,675 --> 00:08:42,596
including features and rewards.

115
00:08:42,596 --> 00:08:45,017
Finally, I will present some engineering techniques

116
00:08:45,017 --> 00:08:46,637
which facilitate learning process.

117
00:08:48,951 --> 00:08:52,973
Okay, now let's imagine a baby trying to learn how to work.

118
00:08:52,973 --> 00:08:55,735
He must learn by himself with numerous trial and errors.

119
00:08:55,735 --> 00:09:00,077
He falls down again and again until he figure out

120
00:09:00,077 --> 00:09:01,478
how to control his body.

121
00:09:01,478 --> 00:09:05,700
This type of learning is correspond to RL.

122
00:09:05,700 --> 00:09:06,600
Let's get into detail.

123
00:09:06,600 --> 00:09:12,103
These three boxes shows good and bad sequence of actions.

124
00:09:12,877 --> 00:09:15,137
If the baby somehow falls down,

125
00:09:15,137 --> 00:09:17,797
he realized that something was wrong,

126
00:09:17,797 --> 00:09:20,838
but he cannot figure out exactly which part of the action

127
00:09:20,838 --> 00:09:23,438
causes the bad result.

128
00:09:23,438 --> 00:09:26,199
Likewise, if he succeeded to keep walking,

129
00:09:26,199 --> 00:09:28,159
he knew he are doing well as a consequences.

130
00:09:28,159 --> 00:09:33,240
The signal that tells whether the baby doing good or bad

131
00:09:33,240 --> 00:09:34,540
is called as a reward.

132
00:09:34,540 --> 00:09:38,361
The objective is to find the best action sequence

133
00:09:38,361 --> 00:09:40,702
to maximize the cumulative reward.

134
00:09:42,445 --> 00:09:46,426
During a lot of trials, the baby modifies his acting policy

135
00:09:46,426 --> 00:09:49,026
to encourage actions in good sequences

136
00:09:49,026 --> 00:09:52,087
and suppressing actions in bad sequences.

137
00:09:52,087 --> 00:09:53,767
As experiences accumulate,

138
00:09:53,767 --> 00:09:55,868
he learns how to get more of rewards.

139
00:09:55,868 --> 00:10:00,469
This is the core of RL, trial and error,

140
00:10:00,469 --> 00:10:02,429
and improving acting policy.

141
00:10:02,429 --> 00:10:08,651
Then what is the reward in BNS Arena Battle?

142
00:10:08,651 --> 00:10:11,691
What is the signal that tells you are doing well?

143
00:10:11,691 --> 00:10:12,212
Of course.

144
00:10:13,154 --> 00:10:14,695
Winning is the main key.

145
00:10:14,695 --> 00:10:16,916
If you win, you are doing well.

146
00:10:16,916 --> 00:10:18,517
But it is very sparse signal

147
00:10:18,517 --> 00:10:21,819
because it only happens at the end of the match.

148
00:10:21,819 --> 00:10:25,682
We need another signal to follow, and that is HP.

149
00:10:25,682 --> 00:10:30,244
If you took away the opponent's HP, you are doing well,

150
00:10:30,244 --> 00:10:32,406
and if your HP is reduced, you are doing bad.

151
00:10:32,406 --> 00:10:34,627
Quite straightforward, isn't it?

152
00:10:34,627 --> 00:10:39,310
With these two components, the agents learn how to win

153
00:10:39,310 --> 00:10:41,411
and how to take away the opponent's HP

154
00:10:41,411 --> 00:10:42,411
while preserving its own.

155
00:10:44,971 --> 00:10:46,872
However, since our agents are required

156
00:10:46,872 --> 00:10:48,433
to have diverse styles,

157
00:10:48,433 --> 00:10:51,694
we introduced some additional rewards.

158
00:10:51,694 --> 00:10:55,096
We decided to guide different degree of aggressiveness

159
00:10:55,096 --> 00:10:56,797
into the agents.

160
00:10:56,797 --> 00:10:59,698
We have determined three dimensions of rewards

161
00:10:59,698 --> 00:11:02,620
to tune the aggressiveness.

162
00:11:02,620 --> 00:11:05,782
The aggressive player's priority is dealing damage,

163
00:11:05,782 --> 00:11:09,663
while defensive player's priority is preserving its own HP.

164
00:11:09,663 --> 00:11:11,905
So it is reflected in the first dimension.

165
00:11:13,921 --> 00:11:15,882
Since aggressive play tend to approach

166
00:11:15,882 --> 00:11:19,064
while the defensive play tend to secure distance,

167
00:11:19,064 --> 00:11:21,686
other two dimensions reflect these perspectives.

168
00:11:21,686 --> 00:11:27,250
You saw this plot before, right?

169
00:11:27,250 --> 00:11:30,352
Here, the reward signal added.

170
00:11:30,352 --> 00:11:31,632
The agent sends an action,

171
00:11:31,632 --> 00:11:33,013
then the environment gives you

172
00:11:33,013 --> 00:11:35,255
the next observation and reward.

173
00:11:35,255 --> 00:11:38,997
I hope you are familiar with each component of the plot,

174
00:11:38,997 --> 00:11:39,798
except feature.

175
00:11:39,798 --> 00:11:43,680
Features represent how the agent sees the world.

176
00:11:44,995 --> 00:11:48,817
The agent decides an action based on the feature.

177
00:11:48,817 --> 00:11:51,658
Features are basically the same

178
00:11:51,658 --> 00:11:53,518
that given to the human player.

179
00:11:53,518 --> 00:11:58,380
So any accessible information to human during a game

180
00:11:58,380 --> 00:12:00,881
are also provided to the AI,

181
00:12:00,881 --> 00:12:03,202
such as HP, SP, distance, position,

182
00:12:03,202 --> 00:12:07,644
skill cool time, and so on.

183
00:12:07,644 --> 00:12:10,645
Now let me show you the overview of the learning process.

184
00:12:11,667 --> 00:12:14,129
The agent, which is composed of neural network,

185
00:12:14,129 --> 00:12:16,391
proceeds combat simulations in parallel.

186
00:12:16,391 --> 00:12:22,837
In our system, 100 workers operate simultaneously.

187
00:12:22,837 --> 00:12:26,140
When games end, networks are updated from the game logs

188
00:12:26,140 --> 00:12:28,502
with an RL algorithm called Acer.

189
00:12:28,502 --> 00:12:32,566
What algorithm does is basically modifying

190
00:12:32,566 --> 00:12:35,088
the probability distribution of each actions.

191
00:12:35,088 --> 00:12:37,730
I mean, encouraging good actions

192
00:12:37,730 --> 00:12:39,091
and suppressing bad actions.

193
00:12:40,193 --> 00:12:43,656
This loop is repeated continuously.

194
00:12:43,656 --> 00:12:46,618
Here I guess you might wonder of who the opponents are.

195
00:12:46,618 --> 00:12:50,400
There are two conditions for good opponents.

196
00:12:50,400 --> 00:12:53,342
First, opponents should be diverse enough.

197
00:12:53,342 --> 00:12:56,584
If the opponents are limited, our agent just learns

198
00:12:56,584 --> 00:12:59,466
how to exploit the fixed strategy,

199
00:12:59,466 --> 00:13:01,907
but could not be generalized for different opponents.

200
00:13:01,907 --> 00:13:06,330
Second, the opponents also should be reinforced

201
00:13:06,330 --> 00:13:07,451
along with the agent.

202
00:13:09,005 --> 00:13:12,086
As both sides of the combat are becoming stronger,

203
00:13:12,086 --> 00:13:15,147
agents can experience higher quality of battle.

204
00:13:15,147 --> 00:13:18,328
The self-play method with opponent pull

205
00:13:18,328 --> 00:13:21,890
satisfy both of suggested conditions.

206
00:13:21,890 --> 00:13:25,751
We store each agent to the pull at fixed interval,

207
00:13:25,751 --> 00:13:28,092
so the pull gradually grows.

208
00:13:28,092 --> 00:13:32,153
Then the pull provides a variety of opponents

209
00:13:32,153 --> 00:13:35,574
and the opponents becoming stronger with the agent.

210
00:13:39,899 --> 00:13:41,760
From this to following two slides,

211
00:13:41,760 --> 00:13:45,301
I will explain more engineering-focused techniques.

212
00:13:45,301 --> 00:13:47,501
And first technique is about learning

213
00:13:47,501 --> 00:13:48,721
the move action policy.

214
00:13:48,721 --> 00:13:52,642
Learning the move policy is rather difficult

215
00:13:52,642 --> 00:13:58,244
because the effect of a single move is limited.

216
00:13:58,244 --> 00:14:01,265
A single move indicates a movement of 0.1 second.

217
00:14:01,265 --> 00:14:05,586
Considering the character's speed, how far could it go?

218
00:14:07,235 --> 00:14:10,556
Therefore, single move action barely affects to the state.

219
00:14:10,556 --> 00:14:15,897
To solve this, we enforced agent to maintain move decisions

220
00:14:15,897 --> 00:14:17,238
for 10 consecutive actions.

221
00:14:17,238 --> 00:14:21,119
This allows the agent to literally move

222
00:14:21,119 --> 00:14:24,440
and lead to the change of subsequent states and rewards.

223
00:14:24,440 --> 00:14:27,261
The method enables the agent to learn

224
00:14:27,261 --> 00:14:30,201
meaningful moving strategy,

225
00:14:30,201 --> 00:14:33,322
and finally, we're able to run away

226
00:14:33,322 --> 00:14:34,923
or approach to the opponent.

227
00:14:38,453 --> 00:14:41,755
We also facilitate learning the skill action policy

228
00:14:41,755 --> 00:14:44,537
with help of domain knowledge.

229
00:14:44,537 --> 00:14:46,618
Out of Destroyer's 44 skills,

230
00:14:46,618 --> 00:14:51,100
the skill that has longest skill reach is 16 meters.

231
00:14:51,100 --> 00:14:53,622
Therefore, when the distance between the agent

232
00:14:53,622 --> 00:14:57,104
and the opponent is farther than 16 meters,

233
00:14:57,104 --> 00:14:59,845
we just blocked most of skills

234
00:14:59,845 --> 00:15:01,706
because it is waste of resource,

235
00:15:01,706 --> 00:15:03,607
such as skill call time and SP.

236
00:15:05,159 --> 00:15:07,541
It helped to ignore meaningless exploration

237
00:15:07,541 --> 00:15:09,582
and boosted the learning process,

238
00:15:09,582 --> 00:15:15,385
especially in the early phase of learning.

239
00:15:15,385 --> 00:15:17,546
I also want to give you some useful findings

240
00:15:17,546 --> 00:15:19,067
during feature engineering,

241
00:15:19,067 --> 00:15:22,890
which is discretization of continuous feature

242
00:15:22,890 --> 00:15:24,811
actually helped.

243
00:15:24,811 --> 00:15:27,052
Each skill has its own targeting range

244
00:15:27,052 --> 00:15:29,313
and accurate distance recognition

245
00:15:29,313 --> 00:15:31,134
is essential to hit the enemy.

246
00:15:32,250 --> 00:15:34,991
For example, there is a skill named Drag,

247
00:15:34,991 --> 00:15:36,992
and its range is three meters to 16 meters.

248
00:15:36,992 --> 00:15:43,014
But the agent trained with continuous distance feature

249
00:15:43,014 --> 00:15:46,075
keeps miscalculating the skill range,

250
00:15:46,075 --> 00:15:48,817
and use the skill when the opponent is too far

251
00:15:48,817 --> 00:15:49,497
or too close.

252
00:15:49,497 --> 00:15:54,179
This is because continuous feature gives burden

253
00:15:54,179 --> 00:15:57,240
for model to learn the exact distance threshold

254
00:15:57,240 --> 00:15:58,000
for each skill.

255
00:16:00,141 --> 00:16:02,423
To solve this, we encoded the distance feature

256
00:16:02,423 --> 00:16:05,565
into one hot vector and continued training.

257
00:16:05,565 --> 00:16:09,348
And then, the agent was able to use the skill,

258
00:16:09,348 --> 00:16:10,348
drag, precisely.

259
00:16:10,348 --> 00:16:16,093
In this section, I will present some experimental results

260
00:16:16,093 --> 00:16:18,094
which includes diverse learning curves.

261
00:16:20,914 --> 00:16:24,496
Our first experiment was conducted with an opponent

262
00:16:24,496 --> 00:16:25,836
called Built-in AI.

263
00:16:25,836 --> 00:16:29,258
Learning how to defeat a single fixed opponent

264
00:16:29,258 --> 00:16:32,139
is relatively easier task compared to deal

265
00:16:32,139 --> 00:16:34,300
with multiple random opponents.

266
00:16:34,300 --> 00:16:37,881
Therefore, before training by self-play method,

267
00:16:37,881 --> 00:16:40,402
we first test the viability of the method

268
00:16:40,402 --> 00:16:43,303
by training against Built-in AI from Infinity Tower.

269
00:16:45,505 --> 00:16:48,527
Infinity Tower is a special dungeon in Blade and Soul

270
00:16:48,527 --> 00:16:50,869
where human players fight against NPC AI.

271
00:16:50,869 --> 00:16:55,372
We've used this sparring partner as a test bed

272
00:16:55,372 --> 00:16:58,455
of various algorithms and tunable hyperparameters.

273
00:16:58,455 --> 00:17:02,317
Let me show you the training results.

274
00:17:02,317 --> 00:17:07,221
The first graph is time-win-rate graph.

275
00:17:07,221 --> 00:17:09,843
It shows how win-rate increases as time passes.

276
00:17:09,843 --> 00:17:13,525
The second graph is time-HPDIF graph.

277
00:17:14,227 --> 00:17:17,187
HP diff indicates the difference of agent's HP

278
00:17:17,187 --> 00:17:19,168
and opponent's HP when the match ends.

279
00:17:19,168 --> 00:17:23,009
As can be seen in the graph,

280
00:17:23,009 --> 00:17:26,330
our agent achieves 90% of win rate

281
00:17:26,330 --> 00:17:27,770
in just 30 minutes of training.

282
00:17:27,770 --> 00:17:32,651
Although the learning curve was splendid,

283
00:17:32,651 --> 00:17:35,452
human player was able to easily defeat the agent.

284
00:17:36,362 --> 00:17:41,727
It had a lot of weaknesses and it was even easier to exploit

285
00:17:41,727 --> 00:17:42,768
than the built-in AI.

286
00:17:42,768 --> 00:17:46,452
So we moved on to the next step.

287
00:17:46,452 --> 00:17:50,416
We now enter to a self-play training phase.

288
00:17:50,416 --> 00:17:53,338
In this stage, the agent fight against the opponent,

289
00:17:53,338 --> 00:17:57,282
which is randomly sampled from the pool of past itself.

290
00:18:00,041 --> 00:18:03,324
Since there exists hundreds of opponents,

291
00:18:03,324 --> 00:18:06,748
so we displayed multiple learning curves here.

292
00:18:06,748 --> 00:18:09,791
Each block correspond to the training curve

293
00:18:09,791 --> 00:18:13,095
for each of the opponent in the poll.

294
00:18:13,095 --> 00:18:15,637
As you can see, the performance against

295
00:18:15,637 --> 00:18:18,661
each of the opponent all stably increases.

296
00:18:18,661 --> 00:18:22,304
Then we briefly tested this agent against human,

297
00:18:22,304 --> 00:18:23,105
and finally.

298
00:18:25,321 --> 00:18:27,441
The level of difficulty also increased,

299
00:18:27,441 --> 00:18:30,802
and human player felt hard to exploit the agent.

300
00:18:30,802 --> 00:18:34,383
Therefore, we evaluated the agent

301
00:18:34,383 --> 00:18:36,663
with more systematic approach.

302
00:18:36,663 --> 00:18:40,304
The goal of the evaluation is,

303
00:18:40,304 --> 00:18:42,285
we want to measure how the agent fight well

304
00:18:42,285 --> 00:18:46,245
against the opponent it never had met before.

305
00:18:46,245 --> 00:18:49,626
It can be called as a generalization performance

306
00:18:49,626 --> 00:18:51,167
or robustness of the agent.

307
00:18:52,361 --> 00:18:55,582
So we made a match with this agent against built-in AI

308
00:18:55,582 --> 00:18:58,203
and skillful human player.

309
00:18:58,203 --> 00:19:01,984
Note that the agent has never met built-in AI before

310
00:19:01,984 --> 00:19:05,965
because it is trained with self-play method.

311
00:19:05,965 --> 00:19:09,367
Nevertheless, it defeated both of built-in AI

312
00:19:09,367 --> 00:19:13,148
and even skillful human player.

313
00:19:13,148 --> 00:19:16,449
Maybe at this time, you might wanna see

314
00:19:16,449 --> 00:19:18,249
how the agents actually fight.

315
00:19:18,249 --> 00:19:20,830
So I guess I should move on to some demo videos.

316
00:19:25,867 --> 00:19:29,068
These videos tell how the agent is getting stronger

317
00:19:29,068 --> 00:19:31,689
as training goes on.

318
00:19:31,689 --> 00:19:32,870
Female characters are AI,

319
00:19:32,870 --> 00:19:38,231
and male characters are dummy opponent.

320
00:19:38,231 --> 00:19:43,433
The left most video shows the initial model

321
00:19:43,433 --> 00:19:45,414
which acts randomly.

322
00:19:45,414 --> 00:19:47,755
We measured time consumption for AI

323
00:19:47,755 --> 00:19:51,836
to reduce dummy opponent's HP from four to zero.

324
00:19:51,836 --> 00:19:55,117
It requires 120 seconds to defeat dummy model.

325
00:19:58,753 --> 00:20:01,595
After a day of training, the AI agent was capable

326
00:20:01,595 --> 00:20:04,537
of dealing damage with basic combo.

327
00:20:04,537 --> 00:20:08,420
Also, it used crowd control skills before dealing damage

328
00:20:08,420 --> 00:20:11,662
to make sure the opponents cannot react.

329
00:20:11,662 --> 00:20:17,106
It requires 42 seconds.

330
00:20:17,106 --> 00:20:21,388
Finally, within only 20 seconds,

331
00:20:21,388 --> 00:20:26,992
AI defeats the dummy opponent.

332
00:20:27,665 --> 00:20:34,051
with a single long combo sequence.

333
00:20:34,051 --> 00:20:41,599
As you can see, the initial model is still struggling

334
00:20:41,599 --> 00:20:44,161
with the dummy opponent.

335
00:20:44,161 --> 00:20:46,884
For your information, our final agents

336
00:20:46,884 --> 00:20:51,048
that compete against pro gamers has trained for two weeks.

337
00:20:52,045 --> 00:20:56,148
It means that each agent experienced up to four years

338
00:20:56,148 --> 00:21:02,234
of real-time gameplay.

339
00:21:02,234 --> 00:21:05,697
Now let me show you how fighting styles are different

340
00:21:05,697 --> 00:21:10,921
between the aggressive and the defensive agent.

341
00:21:10,921 --> 00:21:12,002
Black characters are AI,

342
00:21:12,002 --> 00:21:15,485
and white characters are human players.

343
00:21:19,112 --> 00:21:21,674
Aggressive agent keeps approaching

344
00:21:21,674 --> 00:21:24,135
and attacks relentlessly.

345
00:21:24,135 --> 00:21:27,237
Opponent have no break to think about counter-attack

346
00:21:27,237 --> 00:21:30,999
and assessing the current state.

347
00:21:30,999 --> 00:21:32,059
On the other hand,

348
00:21:32,059 --> 00:21:36,422
defensive agent tend to secure distance

349
00:21:36,422 --> 00:21:39,663
and it waits for an opportunity

350
00:21:39,663 --> 00:21:41,724
for proper timing to counter-attack.

351
00:21:41,724 --> 00:21:46,087
You can see that agents of each style

352
00:21:46,905 --> 00:21:51,068
manages the match in largely different manner.

353
00:21:51,068 --> 00:21:57,852
Just before the live event called Blind Match,

354
00:21:57,852 --> 00:22:00,874
we invited two prominent professional gamers

355
00:22:00,874 --> 00:22:04,136
who are the former winners of annual competition.

356
00:22:04,136 --> 00:22:06,557
For the fair evaluation,

357
00:22:06,557 --> 00:22:10,520
we restricted our agents' reaction time

358
00:22:10,520 --> 00:22:13,722
with a delay of 230 milliseconds on average.

359
00:22:14,416 --> 00:22:17,478
which is similar with that of professional gamers.

360
00:22:17,478 --> 00:22:20,280
As you can see in the table,

361
00:22:20,280 --> 00:22:22,622
the aggressive agents dominate the game,

362
00:22:22,622 --> 00:22:25,944
while other two types of agents have rather intense games.

363
00:22:25,944 --> 00:22:29,347
According to the gamers' interview after tests,

364
00:22:29,347 --> 00:22:32,429
this is because human player often lead

365
00:22:32,429 --> 00:22:34,791
to some break between fights,

366
00:22:34,791 --> 00:22:38,273
for just think about current state and high level strategy.

367
00:22:38,877 --> 00:22:41,800
However, the aggressive agent does not allow human

368
00:22:41,800 --> 00:22:44,422
to have a break between battles,

369
00:22:44,422 --> 00:22:46,223
but it only pours on attacks.

370
00:22:46,223 --> 00:22:51,787
Both of gamers agreed on that the aggressive agent

371
00:22:51,787 --> 00:22:54,810
is impeccable and flawless.

372
00:22:54,810 --> 00:22:57,812
They also added that aggressive agent plays

373
00:22:57,812 --> 00:23:00,314
in totally different way from human,

374
00:23:00,314 --> 00:23:03,596
while other two type of agents play similar to humans.

375
00:23:03,596 --> 00:23:06,999
And finally, with these three models,

376
00:23:06,999 --> 00:23:08,100
we went for the blind match.

377
00:23:09,225 --> 00:23:11,279
Let me show you the event highlight video.

378
00:23:34,562 --> 00:23:37,263
Players here, of course you know, you know ColorEyes.

379
00:23:37,263 --> 00:23:38,924
He's been fantastic there, over 9J.

380
00:23:38,924 --> 00:23:44,246
I don't know who that is, I don't think they know.

381
00:23:44,246 --> 00:23:46,847
A game right now, a bit of a...

382
00:23:46,847 --> 00:23:47,088
What?

383
00:23:47,088 --> 00:23:48,808
Okay, okay, alright.

384
00:23:48,808 --> 00:23:53,230
The moment ColorEyes showed his back, he's like, alright, I'm going in.

385
00:23:53,230 --> 00:23:57,052
There we go, but they both still, all persistence is actually out for 9J.

386
00:23:57,052 --> 00:23:59,413
It's looking great for ColorEyes for now.

387
00:23:59,413 --> 00:24:00,814
Ember Stop is down as well.

388
00:24:02,319 --> 00:24:13,028
Oh, drag messy there. I J already moving forward and seeing what he can do. We can get the win another one

389
00:24:13,028 --> 00:24:14,290
Oh, but not if I J

390
00:24:14,290 --> 00:24:15,731
Has best does gusto

391
00:24:15,731 --> 00:24:23,678
Even with gusto. We'll see how he's gonna time that one up should be in favor of colorized trip the damage

392
00:24:23,678 --> 00:24:24,439
The chase

393
00:24:30,449 --> 00:24:34,241
We're going to win for Shanharan, coming in from China.

394
00:24:34,241 --> 00:24:35,646
That's the retreat people.

395
00:24:38,828 --> 00:24:43,732
Actually the drag connecting there is gonna force out the gust already so pretty much

396
00:24:43,732 --> 00:24:48,896
similarities between the last set as well and what hopping it's you know, but

397
00:24:48,896 --> 00:24:52,699
It's to punish instead. Oh

398
00:24:52,699 --> 00:25:02,167
Gosh, he's going at it against how round one Ember stump is also down. Okay, not you. Oh, he's just against again, though

399
00:25:02,167 --> 00:25:04,689
This might just be it if I

400
00:25:05,509 --> 00:25:08,292
Only a couple of seconds away. That's been 14 more seconds.

401
00:25:08,292 --> 00:25:09,272
Disgusting Justin McLean.

402
00:25:09,272 --> 00:25:14,256
And that will be the damage made with the wedge coming up on that grab.

403
00:25:14,256 --> 00:25:25,185
And that will be enough for Shaman to take that game 1 victory.

404
00:25:25,185 --> 00:25:31,290
Yeah, who just lost in the third place match with his team.

405
00:25:31,290 --> 00:25:31,771
Comes back in this event.

406
00:25:31,771 --> 00:25:33,953
Yeah, we'll have to see how this happens.

407
00:25:33,953 --> 00:25:34,633
Oh my gosh.

408
00:25:34,633 --> 00:25:35,093
What?

409
00:25:36,089 --> 00:25:42,975
That reaction time is crazy, but I was saying no the first two destroyers were able to beat night J

410
00:25:42,975 --> 00:25:51,243
But this time around so the choice not looking great if he's the only destroyer that loses

411
00:25:51,243 --> 00:25:51,263
J

412
00:25:51,263 --> 00:25:52,083
This is devastating

413
00:25:52,083 --> 00:25:55,987
But there we go this is it three seconds not too long

414
00:25:55,987 --> 00:26:02,773
But it's gonna be looking a little bit better for a night J, but not quite good enough

415
00:26:02,773 --> 00:26:02,813
Oh

416
00:26:02,873 --> 00:26:08,576
No, no, we'll see once again the gusts got me coming back and that means like J

417
00:26:08,576 --> 00:26:15,059
If you hit the way now the heels coming back now Oh

418
00:26:15,059 --> 00:26:18,540
Wow playing with your food night J. Huh Wow

419
00:26:25,516 --> 00:26:28,718
The overall results including the blind match is as follows.

420
00:26:28,718 --> 00:26:32,261
The aggressive agents won all of the game except one,

421
00:26:32,261 --> 00:26:36,064
and balanced and defensive agent made competitive result

422
00:26:36,064 --> 00:26:37,305
against professional gamers.

423
00:26:37,305 --> 00:26:41,388
To the best of our knowledge, this level of performance

424
00:26:41,388 --> 00:26:43,070
is achieved for the first time

425
00:26:43,070 --> 00:26:45,031
in modern complex fighting games.

426
00:26:45,031 --> 00:26:50,115
Until now, we introduced the overall process

427
00:26:50,115 --> 00:26:53,258
with specific engineering techniques used for creating AI.

428
00:26:54,406 --> 00:26:58,628
I want to conclude with brief summaries of what we've made.

429
00:26:58,628 --> 00:27:01,870
First, we succeeded to make pro-level AI

430
00:27:01,870 --> 00:27:05,512
in complex fighting games based on RL methods.

431
00:27:05,512 --> 00:27:10,515
Second, we guided fighting style without any hard-coded rules.

432
00:27:10,515 --> 00:27:13,217
Aggressive style is just one example,

433
00:27:13,217 --> 00:27:16,959
and any type of styles can be guided with real shaping.

434
00:27:16,959 --> 00:27:21,242
Third, we trained robust AI with pool of opponents.

435
00:27:23,292 --> 00:27:25,434
Last, we reduced the problem space

436
00:27:25,434 --> 00:27:29,238
with various engineering techniques.

437
00:27:29,238 --> 00:27:32,461
It is the end of what we've prepared for today's session.

438
00:27:32,461 --> 00:27:35,624
I hope you've learned some useful lessons from our session.

439
00:27:35,624 --> 00:27:37,346
And please don't forget to evaluate our session.

440
00:27:37,346 --> 00:27:40,990
We are Jinhyun Chung and Seunghun Roh from NCSoft.

441
00:27:40,990 --> 00:27:41,390
Thank you.

442
00:27:48,165 --> 00:27:51,889
And our company prepared another session on Thursday, 3 p.m.

443
00:27:51,889 --> 00:27:55,992
It is about deep learning based inverse kinematics.

444
00:27:55,992 --> 00:27:57,213
If you have interest, please stop by.

445
00:27:57,213 --> 00:28:01,577
We have another two minutes if you want to take questions.

446
00:28:01,577 --> 00:28:02,918
You don't have to if you don't have to.

447
00:28:02,918 --> 00:28:05,140
OK, I'll just take one question.

448
00:28:05,140 --> 00:28:07,402
We have two minutes, so if you have a question,

449
00:28:07,402 --> 00:28:11,526
I will take only one.

450
00:28:11,526 --> 00:28:13,368
If not, it's OK.

451
00:28:13,368 --> 00:28:14,389
I'll jump in.

452
00:28:15,187 --> 00:28:16,268
Or did you want to go?

453
00:28:16,268 --> 00:28:16,588
No, no.

454
00:28:16,588 --> 00:28:19,310
You could probably answer this quickly.

455
00:28:19,310 --> 00:28:22,853
It seems like the aggressive style was something

456
00:28:22,853 --> 00:28:23,973
that the AI discovered.

457
00:28:23,973 --> 00:28:27,476
Did players start incorporating that into their gameplay

458
00:28:27,476 --> 00:28:30,618
in order, like, is it a discovery of a game style?

459
00:28:30,618 --> 00:28:35,502
Or is it just an unfair advantage the AI has?

460
00:28:35,502 --> 00:28:36,482
It is not unfair.

461
00:28:36,482 --> 00:28:43,047
We have very carefully designed for the fair match.

462
00:28:43,047 --> 00:28:43,407
So.

463
00:28:44,757 --> 00:28:55,805
Although Aggressive Agent acts perfectly, it has a delay between its decision and its reactive time in-game.

464
00:28:55,805 --> 00:28:58,387
230 milliseconds of delay, so yeah, it is a fair match.

465
00:28:58,387 --> 00:29:04,992
I wonder if you're allowed to tell us, how did you adjust the weightings of the neural

466
00:29:04,992 --> 00:29:07,855
network based on the reward signal that came back?

467
00:29:09,389 --> 00:29:10,109
How do we adjust the...

468
00:29:10,109 --> 00:29:11,449
Adjust the...

469
00:29:11,449 --> 00:29:13,810
The weightings in the neural network.

470
00:29:13,810 --> 00:29:15,811
Weights of reward?

471
00:29:15,811 --> 00:29:16,251
Yeah.

472
00:29:16,251 --> 00:29:16,611
We...

473
00:29:16,611 --> 00:29:21,633
Like RL method, we have a lot of trial and errors

474
00:29:21,633 --> 00:29:26,615
and to make the style...

475
00:29:26,615 --> 00:29:28,436
Obviously, we tune the weights

476
00:29:28,436 --> 00:29:31,297
and try many set of weights

477
00:29:31,297 --> 00:29:36,759
and if they look similar, we made it far apart

478
00:29:36,759 --> 00:29:38,880
the weight of each reward and...

479
00:29:39,571 --> 00:29:40,916
Yeah, ding.

480
00:29:40,916 --> 00:29:42,542
Yeah, like that.

481
00:29:42,542 --> 00:29:42,883
Thank you.

