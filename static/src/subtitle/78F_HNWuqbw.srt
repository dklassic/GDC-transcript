1
00:00:06,443 --> 00:00:12,847
Oh, okay. There we go. Integrating 2D UI with VR

2
00:00:12,887 --> 00:00:15,689
environments. You probably already read that on the

3
00:00:15,729 --> 00:00:21,353
program. Who am I? My name is Brad Davis. I've spent over 20

4
00:00:21,673 --> 00:00:23,314
years doing software development.

5
00:00:25,536 --> 00:00:29,978
I've been experimenting with VR since the kick starter days, the original dev kit one. I

6
00:00:31,058 --> 00:00:39,382
built the first Oculus, sorry, the first Java bindings for the Oculus SDK. I've co-authored a

7
00:00:39,422 --> 00:00:47,765
book on developing for the Oculus SDK called Oculus Rift in Action. For the past couple of

8
00:00:47,805 --> 00:00:50,126
years I've been at high fidelity.

9
00:00:51,068 --> 00:00:54,769
we're working on creating an open source framework for an

10
00:00:54,829 --> 00:00:58,129
actual metaverse. Distributed servers, anybody can build

11
00:00:58,209 --> 00:01:04,411
things, nobody's in control. And among other things there I've

12
00:01:04,451 --> 00:01:10,552
worked heavily on integrating the 2D UI systems into the 3D

13
00:01:10,612 --> 00:01:18,354
world. So that covers really both of these points. So why 2D

14
00:01:18,574 --> 00:01:19,354
user interfaces?

15
00:01:21,270 --> 00:01:24,814
VR is this massive shift, why do we still need 2D for anything,

16
00:01:24,895 --> 00:01:30,662
right? Well, 2D UI is everywhere and it's familiar to everyone.

17
00:01:31,283 --> 00:01:36,490
Everyone who's already using a computer. So that's a good

18
00:01:36,550 --> 00:01:36,851
argument.

19
00:01:38,909 --> 00:01:44,795
Also, VR, I think, as we evolve, we're going to see VR metaphor

20
00:01:44,935 --> 00:01:49,060
replace a desktop metaphor where a VR environment becomes your

21
00:01:49,120 --> 00:01:52,483
place to work in. We're already seeing moves towards that with

22
00:01:52,643 --> 00:01:55,546
things like big screen and virtual desktops. So we have to

23
00:01:55,666 --> 00:02:00,431
understand how we're going to integrate 2D UI into that 3D

24
00:02:00,491 --> 00:02:01,032
environment.

25
00:02:02,280 --> 00:02:08,582
Also, many things don't benefit from any sort of 3D UI. In fact, very few things that are

26
00:02:08,802 --> 00:02:19,544
inherently 3D, like a topographical map or a body scan. These things don't benefit from just

27
00:02:20,024 --> 00:02:26,826
being flashy and in 3D. While 3D representation in a VR headset can give you more

28
00:02:26,866 --> 00:02:31,607
information about an object, like depth, it can also obscure information.

29
00:02:33,209 --> 00:02:36,335
object that's in 3D, part of it is always going to be facing

30
00:02:36,375 --> 00:02:42,166
away from you. So with the 2D UI, that's not the case. Also

31
00:02:43,164 --> 00:02:46,266
the technology isn't necessarily there for translating some

32
00:02:46,326 --> 00:02:50,469
things in the real world that we have sort of either full 3D or

33
00:02:50,549 --> 00:02:54,952
semi 3D interfaces for. Take, for example, a light switch on a

34
00:02:54,972 --> 00:03:00,837
wall. A light switch on a wall and a check box on a 2D dialogue

35
00:03:01,317 --> 00:03:05,059
serve the same purpose. They have one of two states. They

36
00:03:05,140 --> 00:03:09,803
indicate to you which state they're in. To the extent that a

37
00:03:09,863 --> 00:03:11,364
wall switch has.

38
00:03:12,318 --> 00:03:17,622
a 3D interface. That's useful because I can reach over and

39
00:03:17,662 --> 00:03:21,345
touch a light switch and determine what state it's in or

40
00:03:21,405 --> 00:03:26,509
change its state without looking at it. And as nice as the Vive

41
00:03:26,549 --> 00:03:29,932
controllers and the Oculus touch controllers are, I don't think

42
00:03:29,952 --> 00:03:33,435
there's any amount of little haptic buzzing that's going to

43
00:03:33,495 --> 00:03:36,537
replicate that. So I don't think the technology is necessarily

44
00:03:36,577 --> 00:03:41,221
there for translating existing 3D stuff into 3D.

45
00:03:42,187 --> 00:03:47,830
the VR world yet. And finally, we need to walk before we run.

46
00:03:49,371 --> 00:03:52,814
I've actually heard this in a couple of other talks. But the

47
00:03:52,914 --> 00:03:56,396
idea that, you know, when film curse came about, we had to

48
00:03:56,556 --> 00:04:01,799
learn the new vernacular of film, the concepts you could

49
00:04:01,859 --> 00:04:05,542
translate in film that wouldn't work on a stage. How to

50
00:04:05,582 --> 00:04:07,903
interpret stage directions from Shakespeare.

51
00:04:08,813 --> 00:04:13,236
between a Baz Lerman production of Romeo and Juliet versus an

52
00:04:14,757 --> 00:04:18,099
original stage production. They're vastly different and it

53
00:04:18,139 --> 00:04:21,221
took time to learn the new techniques and it's going to

54
00:04:21,241 --> 00:04:25,944
take time to learn what new techniques we can apply from VR

55
00:04:26,304 --> 00:04:30,647
in 2D UI to enhance it, perhaps make it 3D, but that's going to

56
00:04:30,687 --> 00:04:31,128
take time.

57
00:04:36,413 --> 00:04:41,277
I like to break down the concept of input styles into three

58
00:04:41,297 --> 00:04:45,400
different categories that are essentially layers. The first

59
00:04:45,500 --> 00:04:48,803
kind of style that you have is the console style. And this

60
00:04:48,863 --> 00:04:51,285
dates all the way back to the original Nintendo entertainment

61
00:04:51,565 --> 00:04:56,169
system where you have up, down, left, right, enter and back. And

62
00:04:56,429 --> 00:05:00,932
maybe you have a meta button for accessing the OS. And we've

63
00:05:00,972 --> 00:05:03,815
seen this style.

64
00:05:04,638 --> 00:05:07,720
mirrored all the way up into current consoles. And for

65
00:05:07,840 --> 00:05:13,263
current console OSs, you still use these basic six functions,

66
00:05:13,663 --> 00:05:16,625
up, down, left, right, enter, back, to navigate the OS, to

67
00:05:16,685 --> 00:05:20,347
navigate menus and games, and it's very simple and easily

68
00:05:20,447 --> 00:05:25,389
conceptualized. It enables navigation. Above that, you

69
00:05:25,429 --> 00:05:28,451
have sort of the mobile style, and I call it mobile style

70
00:05:28,511 --> 00:05:30,172
because it's best exemplified by

71
00:05:31,099 --> 00:05:36,787
cell phones and tablets where in addition to simple navigation, you also have a point, a

72
00:05:36,827 --> 00:05:43,497
touch point on the UI surface, an XY coordinate that means the user is interacting at this

73
00:05:43,637 --> 00:05:44,739
point on the surface.

74
00:05:46,779 --> 00:05:50,364
whereas console style lets you enter text by left, left, left,

75
00:05:50,404 --> 00:05:54,909
left, left, left, now I'm on the Q key and I can enter Q. Touch

76
00:05:54,949 --> 00:05:58,774
surfaces allow you to much more rapidly enter text. I still

77
00:05:58,794 --> 00:06:01,317
wouldn't want to write a research paper in it, but it's

78
00:06:01,357 --> 00:06:01,577
there.

79
00:06:04,190 --> 00:06:09,954
And then finally, the top of the food chain, so to speak, is the full PC style input where you

80
00:06:09,974 --> 00:06:17,840
have a keyboard and a mouse. And this enables, in addition to, whereas the mobile enables

81
00:06:18,381 --> 00:06:24,185
dragging and drawing operations, the PC style enables massive amounts of text input and

82
00:06:24,225 --> 00:06:30,931
manipulation and also enables enhanced dragging and drawing operations. If you think about

83
00:06:31,351 --> 00:06:33,292
3D manipulation programs often.

84
00:06:34,313 --> 00:06:39,455
touching and dragging with one key press control will rotate or

85
00:06:39,535 --> 00:06:44,417
with shift will zoom. So it enables further kinds of input

86
00:06:44,457 --> 00:06:51,119
interactions. I also want to kind of divide up the kinds of

87
00:06:51,259 --> 00:06:55,740
UI that you can talk about in terms of internal and external.

88
00:06:57,061 --> 00:07:00,862
External UI is something that is UI that's part of the scene.

89
00:07:03,107 --> 00:07:11,071
screen on an ATM in an in world object is external UI. Whereas

90
00:07:11,691 --> 00:07:15,133
something like a browser window is something that's internal UI.

91
00:07:15,193 --> 00:07:20,276
It's private to you. In a multi user or social VR experience,

92
00:07:20,616 --> 00:07:23,238
external UI is something that's going to be visible to everyone.

93
00:07:23,258 --> 00:07:25,879
Internal UI is going to be visible only to you.

94
00:07:28,360 --> 00:07:30,961
Most of the stuff I'm going to be talking about, most of the

95
00:07:31,001 --> 00:07:34,963
interesting problems I think have to do with internal UI,

96
00:07:35,043 --> 00:07:45,529
presenting the customizability interface to a user. I'm trying

97
00:07:45,649 --> 00:07:49,291
not to zoom through this but I'm also trying to make sure I have

98
00:07:49,851 --> 00:07:54,353
time for questions. So external UI.

99
00:07:55,963 --> 00:08:01,948
external UI is conceptually easy in terms of rendering. You don't have to think about how

100
00:08:02,008 --> 00:08:06,971
am I going to position this relative to the user. It has a position in a scene. If you're

101
00:08:07,172 --> 00:08:12,275
rendering a cockpit and there's a UI element in the cockpit that has a particular place it

102
00:08:12,315 --> 00:08:16,579
goes and just as you're rendering the rest of the cockpit, you're going to render that UI

103
00:08:16,619 --> 00:08:23,584
element. Also, there's no conflict between the depth of the scene and the size of the scene.

104
00:08:24,257 --> 00:08:27,918
that the object is an occlusion. I'm going to cover this in a

105
00:08:27,938 --> 00:08:32,439
little bit more detail later, but essentially if there's a UI

106
00:08:32,459 --> 00:08:34,559
element there and there's something in the scene that's

107
00:08:34,599 --> 00:08:37,880
closer to you than the UI element, then you just have to

108
00:08:37,940 --> 00:08:40,921
deal with that. That object obscures the UI element and you

109
00:08:40,961 --> 00:08:43,742
have to figure out how to get it out of the way. Maybe that's

110
00:08:43,762 --> 00:08:46,143
part of the game play. Maybe that's just where the user stuck

111
00:08:46,163 --> 00:08:46,523
their head.

112
00:08:49,180 --> 00:08:53,322
As compared to internal UI, though, external UI is going to

113
00:08:53,342 --> 00:08:59,225
be harder for picking. If you have an arbitrary point in space

114
00:08:59,725 --> 00:09:04,348
and a ray projecting from that point, both for internal UI and

115
00:09:04,388 --> 00:09:09,430
external UI, you're going to want to be able to say where on

116
00:09:09,450 --> 00:09:15,374
the XY UI surface does that point intersect. Now, for

117
00:09:15,474 --> 00:09:16,194
internal UI,

118
00:09:17,112 --> 00:09:21,657
which I'll cover in a moment, you're directly in control of

119
00:09:21,697 --> 00:09:25,541
that. For external UI, the position of that UI surface is

120
00:09:25,701 --> 00:09:28,585
completely relative to where did the user stick their head and

121
00:09:28,645 --> 00:09:31,448
where is that in the scene. And you have to have some sort of

122
00:09:31,548 --> 00:09:32,629
picking logic for it.

123
00:09:34,263 --> 00:09:38,910
Hopefully whatever rendering system, whatever rendering engine you use lets you say,

124
00:09:38,930 --> 00:09:43,736
okay I need to do the triangle intersection, I need to know exactly what point on this rectangle

125
00:09:44,237 --> 00:09:47,541
I'm hitting and you can translate that into xy coordinates.

126
00:09:49,201 --> 00:09:53,043
Also for external UI, if you have multiple elements of

127
00:09:53,123 --> 00:09:58,145
external UI, you need to be able to find some way to manage

128
00:09:58,745 --> 00:09:59,706
where is the focus.

129
00:09:59,966 --> 00:10:03,707
If I provide a user input, does it go to this element or

130
00:10:03,767 --> 00:10:04,788
does it go to that element?

131
00:10:07,524 --> 00:10:12,547
Elite dangerous is a good example of gaze focus. It has

132
00:10:12,648 --> 00:10:16,670
multiple UI surfaces that you can activate by turning your

133
00:10:16,710 --> 00:10:20,753
head and focus on that and when you have that thing focused,

134
00:10:20,833 --> 00:10:26,237
that's where the input goes. You also have the same issue if you

135
00:10:26,277 --> 00:10:29,519
have both external UI and internal UI that are available

136
00:10:29,539 --> 00:10:30,299
at the same time.

137
00:10:32,957 --> 00:10:37,721
One thing I wanted to note is when you're building external UI in particular, you have to

138
00:10:37,801 --> 00:10:47,951
be aware of making your users uncomfortable. The Vive controllers in particular, when

139
00:10:47,971 --> 00:10:50,933
you're doing picking, you have to be careful about picking. You have to be careful about

140
00:10:52,641 --> 00:10:56,366
tend to shoot the picking rays out of the top of the Vive

141
00:10:56,406 --> 00:10:58,969
controller. So if you're holding like this, basically the rays

142
00:10:59,009 --> 00:11:03,053
are kind of going along the axis of your thumb. But if you tried

143
00:11:03,073 --> 00:11:05,536
to use a touch screen with your thumb that's below you,

144
00:11:06,412 --> 00:11:07,653
that's going to be really painful.

145
00:11:07,873 --> 00:11:12,718
You want to avoid anything that's going to bring the elbow up above the hand in order to do that.

146
00:11:13,739 --> 00:11:16,581
Also, if you're just using sort of gaze activation,

147
00:11:17,102 --> 00:11:21,386
again, anything that's kind of below a few degrees of the horizon,

148
00:11:21,946 --> 00:11:24,008
it's going to put strain on the neck.

149
00:11:24,646 --> 00:11:27,629
as you have to look down and especially if you are doing

150
00:11:28,190 --> 00:11:33,716
detailed gaze focus, you don't want to have to make your users

151
00:11:33,776 --> 00:11:36,679
have to make fine motions with their head as they're looking

152
00:11:36,740 --> 00:11:40,604
down. You want to kind of avoid having them crane their neck

153
00:11:40,644 --> 00:11:45,830
down if at all possible. So for internal UI, I'm going to

154
00:11:48,346 --> 00:11:51,707
you have to decide whether or not, this may be not your

155
00:11:51,727 --> 00:11:53,708
decision, but something that's just dictated by your

156
00:11:53,768 --> 00:11:57,469
application or game, is your UI going to be transient or

157
00:11:57,509 --> 00:12:02,811
persistent? A transient UI would be a menu you bring up to say,

158
00:12:02,831 --> 00:12:05,432
okay, I want to load this other level, I want to do this other

159
00:12:05,472 --> 00:12:10,033
thing. But a persistent UI might be you're in a social VR

160
00:12:10,093 --> 00:12:12,874
environment and you want to have a browser window that you can

161
00:12:12,914 --> 00:12:15,895
refer to while you're talking with this other person.

162
00:12:17,026 --> 00:12:21,629
So is it going to be something where interacting with the 3D

163
00:12:21,689 --> 00:12:25,092
scene and interacting with the UI need to be essentially

164
00:12:25,312 --> 00:12:29,715
almost concurrent or is it going to be something where it's

165
00:12:29,755 --> 00:12:35,580
sequential? You're doing one or the other. When you're dealing

166
00:12:35,660 --> 00:12:38,742
with internal UI, you're going to have to be careful about

167
00:12:40,312 --> 00:12:44,214
you're directly in control of how that UI is presented to the

168
00:12:44,314 --> 00:12:49,137
user. And you may have a desktop metaphor with lots of different

169
00:12:49,177 --> 00:12:51,738
elements that the user can move around. You may have lots of

170
00:12:51,818 --> 00:12:57,361
individual elements that are each rendered individually or

171
00:12:57,761 --> 00:13:01,643
something in between. But however you have it, you're

172
00:13:01,663 --> 00:13:04,645
going to have to figure out a way of how do I inject these

173
00:13:04,705 --> 00:13:05,305
into the scene.

174
00:13:08,073 --> 00:13:16,295
OpenVR and Oculus both have support for overlays or layers

175
00:13:16,755 --> 00:13:19,436
that they will inject into the compositing scene for you,

176
00:13:19,816 --> 00:13:21,616
into the compositing process for you.

177
00:13:22,756 --> 00:13:24,577
But they're fairly limited in scope

178
00:13:24,617 --> 00:13:26,257
in terms of how you can use them.

179
00:13:29,213 --> 00:13:35,037
they only, sorry, Oculus will only allow you to render to a

180
00:13:35,257 --> 00:13:40,601
flat planar surface. Open VR will mostly only let you render

181
00:13:40,641 --> 00:13:44,423
to a flat planar surface and then composite that in. It will

182
00:13:44,603 --> 00:13:47,906
also let you render one curved surface but you don't have

183
00:13:47,966 --> 00:13:52,789
direct control of the amount of curvature. And most of the

184
00:13:52,829 --> 00:13:54,090
feedback I've seen, most of the...

185
00:13:55,811 --> 00:13:59,574
most of the talks I've seen when they're talking about UI, really

186
00:13:59,614 --> 00:14:06,198
recommend that you want to have a curved surface. That's ‑‑

187
00:14:07,459 --> 00:14:14,263
sorry, getting off track. When you have these curved surfaces,

188
00:14:14,968 --> 00:14:18,070
internal UIs that are projected into your scene but are not part

189
00:14:18,110 --> 00:14:23,672
of the scene, you have the potential for occlusion or depth

190
00:14:23,712 --> 00:14:28,314
disparity, one or the other. Generally you do not want to

191
00:14:28,414 --> 00:14:35,177
render a UI closer to the user than at arm's length. It can

192
00:14:35,217 --> 00:14:38,198
become tiresome because the depth cues between accommodation

193
00:14:38,238 --> 00:14:42,820
and convergence will disagree. So you want to have the UI large

194
00:14:42,860 --> 00:14:43,480
and far away.

195
00:14:44,883 --> 00:14:48,106
but when you have a UI that's large and far away, that also

196
00:14:48,146 --> 00:14:51,470
means that elements in the scene might be poking through the UI.

197
00:14:52,831 --> 00:14:58,276
And you can either have that be a situation where the elements

198
00:14:58,336 --> 00:15:01,599
poking through the UI then occlude the UI, limiting its

199
00:15:01,699 --> 00:15:07,004
usefulness, or you can have the UI simply overlay the 3D scene.

200
00:15:08,115 --> 00:15:11,057
The problem with the latter is that if you have the UI overlay

201
00:15:11,097 --> 00:15:16,139
the 3D scene, then you get depth cues. If there's an object

202
00:15:16,159 --> 00:15:20,440
that's poking out below it and one of your depth cues is

203
00:15:20,480 --> 00:15:24,162
telling you that object should be closer and your other depth

204
00:15:24,202 --> 00:15:27,323
cues are telling you no, this overlay is closer because it's

205
00:15:27,463 --> 00:15:30,824
obscuring the other object and that can cause some discomfort.

206
00:15:39,779 --> 00:15:43,461
If you're creating a desktop metaphor where you're actually

207
00:15:43,501 --> 00:15:47,703
creating an entire windowing system that requires you to do

208
00:15:47,763 --> 00:15:51,184
a lot of work right now, there are UI libraries out there that

209
00:15:51,205 --> 00:15:57,207
will render UI for you. But generally they won't do a

210
00:15:57,347 --> 00:16:00,689
windowing system for you. At most you will be able to render

211
00:16:00,809 --> 00:16:04,551
a bunch of individual elements and you'll have to create the

212
00:16:04,591 --> 00:16:07,732
concept of window decoration or Z order, things like that.

213
00:16:10,357 --> 00:16:19,324
and then finally on internal UI, don't ever pin the origin of the

214
00:16:19,404 --> 00:16:26,810
UI directly to the viewpoint. If you want to make text more

215
00:16:26,850 --> 00:16:31,354
readable or you want to make various parts of the UI more

216
00:16:31,394 --> 00:16:34,797
accessible, it's important that a user be able to lean their

217
00:16:34,837 --> 00:16:35,177
head.

218
00:16:36,422 --> 00:16:40,224
left and right or lean in and get closer to the UI to gain

219
00:16:40,264 --> 00:16:44,446
greater clarity on it. You can also get about an order of

220
00:16:44,566 --> 00:16:48,227
magnitude more readability out of text even if you can just

221
00:16:48,307 --> 00:16:53,550
move your head in almost tiny amounts and see the text change

222
00:16:53,730 --> 00:17:04,034
you can make out the text shapes better. So compositing and

223
00:17:04,294 --> 00:17:06,255
projection. So that's my thought on that. Has anyone seen one of

224
00:17:06,658 --> 00:17:12,301
So when you have internal UI, you need to composite it into your scene and you need to

225
00:17:12,361 --> 00:17:20,525
choose a projection. Like I said, the existing SDKs have support for overlays but they're

226
00:17:20,545 --> 00:17:25,087
very limited. They will only allow you to do compositing over the scene. They will not

227
00:17:25,147 --> 00:17:30,670
allow you to do compositing into the scene where the UI can be included. Whatever

228
00:17:30,730 --> 00:17:32,211
projection that you choose,

229
00:17:33,502 --> 00:17:40,568
is going to lead to some kind of distortion of the objects. And when you're doing picking

230
00:17:42,169 --> 00:17:48,835
against a projection, the more complex the projection that you use, the more complex that

231
00:17:48,915 --> 00:17:57,162
mapping from a ray, an origin in a ray direction to an XY coordinate on the UI is going to be.

232
00:18:03,034 --> 00:18:08,597
Open VR does help you in that, in that they actually have an

233
00:18:08,617 --> 00:18:16,801
API that says given this ray and direction, what is the XY

234
00:18:16,821 --> 00:18:23,324
coordinate on this overlay service that I'm hitting? So for

235
00:18:23,344 --> 00:18:28,867
the issue of occlusion versus depth cues, there isn't really a

236
00:18:29,147 --> 00:18:30,507
general purpose solution.

237
00:18:33,001 --> 00:18:38,826
If you're using a transient UI, you can optionally fade the 3D

238
00:18:38,926 --> 00:18:41,749
environment to a mono environment while you have the

239
00:18:41,849 --> 00:18:46,212
UI open, which eliminates any potential for competing depth

240
00:18:46,272 --> 00:18:50,436
cues. But the fade effect itself, whether it's immediate

241
00:18:50,556 --> 00:18:53,819
or over a small amount of time, can be discomforting.

242
00:18:55,797 --> 00:19:00,358
The other option is you can call the problematic objects, but

243
00:19:00,858 --> 00:19:06,340
calling objects can result in like bisected avatars or

244
00:19:06,440 --> 00:19:11,281
important items missing from the scene. For our purposes at the

245
00:19:11,321 --> 00:19:14,682
moment, what we just do is live with the competing depth cues.

246
00:19:25,985 --> 00:19:33,294
integration details. What are the requirements for a UI

247
00:19:33,354 --> 00:19:36,898
library that you want to use to integrate into your 3D

248
00:19:36,958 --> 00:19:40,022
application? It has to work on your target platform.

249
00:19:43,954 --> 00:19:47,798
This may seem like a no-brainer, but you have to think about the platforms you're using now

250
00:19:48,058 --> 00:19:52,944
and the platforms you're going to be using in the future if you're eventually thinking, well,

251
00:19:53,144 --> 00:19:57,328
we're going to port this to Android. You don't want to have to use something, you don't want to

252
00:19:57,368 --> 00:20:01,252
be using something where you're going to have to rip out all of your UI and rewrite it.

253
00:20:03,262 --> 00:20:08,865
you have to use something that has capturable output. Ideally

254
00:20:08,905 --> 00:20:13,287
you want a piece of software that will render a UI directly

255
00:20:13,387 --> 00:20:18,850
to a GPU surface, a texture, either a direct 3D texture, a

256
00:20:19,190 --> 00:20:23,152
Vulkan image or an OpenGL texture. Less ideally, you

257
00:20:23,252 --> 00:20:25,854
could use something that will render to a...

258
00:20:28,761 --> 00:20:32,222
image in the CPU memory and do the transfer every time the UI

259
00:20:32,282 --> 00:20:36,203
changes, but that's going to be consuming a scarce resource,

260
00:20:37,364 --> 00:20:44,026
namely CPU, GPU transfer and GPU processing power.

261
00:20:45,177 --> 00:20:52,699
And finally, your library has to be able to accept programmatic input. Most UI systems just

262
00:20:52,899 --> 00:21:00,961
accept native events from the OS or from whatever framework you're using. You want to be

263
00:21:01,001 --> 00:21:05,762
able to tell the UI library that you're using, I don't care where the OS says the mouse is, I

264
00:21:06,522 --> 00:21:12,184
don't care what the OS said the keyboard input is, I'm handing it to you. Because you're

265
00:21:12,204 --> 00:21:14,084
going to need to be able to do translation.

266
00:21:19,721 --> 00:21:28,324
desirable for a U.I. library. You want it to map to a set of U.I. controls that are

267
00:21:28,384 --> 00:21:34,166
familiar to everyone. The check box, the button, the slider, the scroll bar, whatever. You

268
00:21:34,206 --> 00:21:39,368
don't want to have an in-house designer who comes up with a U.I. design and then discover

269
00:21:39,428 --> 00:21:43,249
that it's going to be a gigantic pain to implement because your U.I. library doesn't

270
00:21:43,269 --> 00:21:45,630
really support all of these common features.

271
00:21:46,828 --> 00:21:51,931
you want to have good tooling support. The ability to have

272
00:21:52,132 --> 00:21:56,974
lots of easy to use tools to build your UI. Things like

273
00:21:57,094 --> 00:22:03,638
Android provide that. Most UI systems have some sort of UI

274
00:22:04,138 --> 00:22:09,542
mockup mechanism. If it's something that provides good

275
00:22:09,802 --> 00:22:12,643
HTML rendering, then that's automatically going to hit the

276
00:22:12,683 --> 00:22:13,544
first two points.

277
00:22:14,792 --> 00:22:18,814
and you want it to provide a rich focus model. In particular,

278
00:22:19,574 --> 00:22:24,676
it's valuable to have knowledge of when the UI is focused on a

279
00:22:24,716 --> 00:22:30,778
text element in order to provide in world on screen keyboard

280
00:22:30,859 --> 00:22:39,822
elements. So the UI libraries that I've worked with, the

281
00:22:40,953 --> 00:22:48,177
I've mostly worked with QML in the past couple of years. That's part of the QT framework. It

282
00:22:48,257 --> 00:22:55,562
hits all of these notes. QML will only render directly to an OpenGL tech... I'm sorry. QML

283
00:22:55,682 --> 00:23:01,525
is a general purpose library. You can build a native application with it and it will render

284
00:23:01,625 --> 00:23:07,148
to the screen. In terms of rendering to an off screen surface, it only supports OpenGL.

285
00:23:08,936 --> 00:23:13,899
Crazy Eddie's GUI and lib rocket are both more special purpose

286
00:23:14,379 --> 00:23:18,882
libraries. They're targeted at game makers who want to build

287
00:23:18,982 --> 00:23:22,423
some sort of UI for their application. They're a little

288
00:23:22,463 --> 00:23:26,626
bit more limited in terms of their features, but they will

289
00:23:26,686 --> 00:23:37,412
both render directly to open GL and to direct 3D. So, I'm going

290
00:23:39,268 --> 00:23:45,772
When you're dealing with input, most UI libraries are typically

291
00:23:45,852 --> 00:23:49,274
built around the concept of keyboard events and mouse

292
00:23:49,314 --> 00:23:53,837
events. So if you have controllers, the RIF controller,

293
00:23:53,857 --> 00:23:58,120
the Vive controller, the Oculus remote, you're going to have to

294
00:23:58,140 --> 00:23:59,501
have a translation layer there.

295
00:24:11,713 --> 00:24:20,795
sorry. There we go. Keyboard events are fairly simple.

296
00:24:20,955 --> 00:24:23,636
Usually there's a straight one to one correlation between

297
00:24:24,316 --> 00:24:28,197
whatever keyboard event your framework supports and whatever

298
00:24:28,517 --> 00:24:33,258
keyboard event the UI library will support. Mouse events are

299
00:24:33,278 --> 00:24:36,839
a little bit more tricky. You usually need remapping of some

300
00:24:36,899 --> 00:24:40,779
kind. But naive remapping can also be tricky.

301
00:24:44,094 --> 00:24:48,755
One of the issues is in VR, you might have a very large

302
00:24:48,935 --> 00:24:51,976
off-screen UI surface that's acting as a kind of

303
00:24:51,996 --> 00:24:52,936
virtual desktop.

304
00:24:53,476 --> 00:24:55,217
So if you have a 4,000 by 2,000 UI surface

305
00:24:55,317 --> 00:24:58,618
and you have an on-screen window that's 100 by 100

306
00:24:58,698 --> 00:25:01,479
or something more reasonable, the mouse movements

307
00:25:01,539 --> 00:25:03,359
are not gonna map one-to-one.

308
00:25:10,722 --> 00:25:18,186
So it's better to keep a sort of virtual mouse position specifically for the off screen UI and

309
00:25:18,387 --> 00:25:24,330
use delta mouse events to move that around rather than try and do an absolute mapping.

310
00:25:27,292 --> 00:25:35,217
Hand controller picking into the UI surface is tricky. As I said, open VR has helpers for

311
00:25:35,257 --> 00:25:35,657
converting.

312
00:25:38,254 --> 00:25:42,416
If you use a simple projection on which to put your 3D,

313
00:25:42,897 --> 00:25:45,038
on which to put your 2D surfaces,

314
00:25:46,979 --> 00:25:50,541
then you should be able to create a simple mapping

315
00:25:50,621 --> 00:25:55,463
between 3D rays and the offscreen surface.

316
00:26:03,307 --> 00:26:04,268
So for output.

317
00:26:07,171 --> 00:26:17,440
You've got two levels of rendering. You have one process, you've got one, you have your UI

318
00:26:17,520 --> 00:26:22,204
scene which you need to render, your UI that you need to render and you have your scene that

319
00:26:22,224 --> 00:26:27,629
you need to render and you need a step to composite them together if you're not relying on

320
00:26:27,669 --> 00:26:29,371
the SDK to do that compositing.

321
00:26:31,062 --> 00:26:34,785
You want to make sure that you use a rotating buffer of output textures.

322
00:26:34,905 --> 00:26:39,348
You don't want to just use a single texture for rendering your UI to the texture and then

323
00:26:39,928 --> 00:26:41,589
compositing it into the scene and then

324
00:26:42,370 --> 00:26:43,491
waiting and doing that again.

325
00:26:44,351 --> 00:26:45,732
Because you need one

326
00:26:46,513 --> 00:26:52,737
version of the texture which is being used for reading, one is being used for writing, and one which is in flight.

327
00:26:53,298 --> 00:26:58,421
So typically you want a rotating buffer of at least three textures to hold your UI.

328
00:27:01,448 --> 00:27:06,134
Ideally you want a distinct rendering context for your UI.

329
00:27:06,214 --> 00:27:09,759
You don't want the UI rendering to be blocking the main

330
00:27:09,839 --> 00:27:12,362
rendering. You don't want the UI rendering to be changing the

331
00:27:12,423 --> 00:27:15,787
state of the open GL state machine or the direct 3D state

332
00:27:15,827 --> 00:27:18,110
machine while it's doing that.

333
00:27:20,078 --> 00:27:24,061
and then synchronization can be a pain. If you're familiar with

334
00:27:24,121 --> 00:27:27,744
working with fences and passing textures between multiple open

335
00:27:27,784 --> 00:27:33,649
GL contexts, you have to be very careful about fences and flushes

336
00:27:33,769 --> 00:27:36,831
and making sure that even though you've completed all the

337
00:27:36,871 --> 00:27:40,654
commands to render the UI, the GPU hasn't necessarily finished

338
00:27:40,674 --> 00:27:44,918
the work, so you need to use synchronization primitives to

339
00:27:45,258 --> 00:27:48,861
protect against rendering corruption.

340
00:27:55,506 --> 00:27:59,537
So tips for designing UI for VR.

341
00:28:01,578 --> 00:28:03,338
You want to go big with your elements.

342
00:28:04,939 --> 00:28:08,020
I recommend designing as though you had a fairly low

343
00:28:08,080 --> 00:28:11,061
resolution screen compared to current screens.

344
00:28:12,381 --> 00:28:16,042
You want to avoid controls that require fine motor skills.

345
00:28:17,543 --> 00:28:22,064
One of the recommendations I have is that if you have a

346
00:28:22,104 --> 00:28:25,905
touch screen available, design your UI and then practice

347
00:28:26,025 --> 00:28:27,786
using it at arm's length.

348
00:28:28,345 --> 00:28:32,049
with the touch screen. That's going to reasonably accurately

349
00:28:32,109 --> 00:28:35,072
simulate the kind of accuracy you're going to have when you're

350
00:28:35,152 --> 00:28:42,080
using hand controllers and a pointer. And be cautious with

351
00:28:42,140 --> 00:28:42,941
transparency.

352
00:28:44,492 --> 00:28:48,595
It might be very tempting to say, okay, I've got this 3D

353
00:28:48,695 --> 00:28:51,297
scene, I want to make sure that even though the user has a

354
00:28:51,337 --> 00:28:55,060
browser window up, they can see the 3D scene partially behind

355
00:28:55,120 --> 00:29:00,304
it. This ends up becoming a distorted mess. You can't

356
00:29:00,344 --> 00:29:02,846
really make out the text and you can't really make out the 3D

357
00:29:02,906 --> 00:29:06,789
scene. You don't really have any control over what's going to be

358
00:29:06,829 --> 00:29:07,310
behind it.

359
00:29:07,950 --> 00:29:12,135
necessarily. So avoid transparency particularly in

360
00:29:12,195 --> 00:29:14,377
elements that are going to be containing text or might

361
00:29:14,437 --> 00:29:18,020
contain text. It's fine if you want to use partially

362
00:29:18,060 --> 00:29:20,703
transparent elements for window decorations. But

363
00:29:23,201 --> 00:29:28,724
Also, in terms of avoiding fine movements, you know, compare a

364
00:29:29,465 --> 00:29:32,346
number field that has little spin box controls that are

365
00:29:32,887 --> 00:29:36,208
wedged in there. You want to avoid that if you want something

366
00:29:36,228 --> 00:29:41,611
that moves a number value up and down. Try and lean towards a

367
00:29:41,691 --> 00:29:44,193
giant dial like you would see on a tuner.

368
00:29:55,008 --> 00:30:01,252
Yeah. Okay. Also, more tips. You want to optimize for multiple

369
00:30:01,292 --> 00:30:05,094
kinds of inputs. And in particular, you want to look at

370
00:30:05,114 --> 00:30:09,756
the inputs that the user is actually using at the moment. I

371
00:30:09,797 --> 00:30:13,018
found a number of applications when we first started working

372
00:30:13,038 --> 00:30:13,939
with Oculus touch.

373
00:30:14,619 --> 00:30:17,800
It worked with Steam VR applications but a lot of the

374
00:30:17,860 --> 00:30:21,841
Steam VR applications were working from the assumption

375
00:30:21,881 --> 00:30:26,002
that you had touch pads, not joysticks for the analog

376
00:30:26,022 --> 00:30:31,423
controls, which meant some of the tools were just much harder

377
00:30:31,443 --> 00:30:34,624
to use than they needed to be because they expected you to be

378
00:30:34,684 --> 00:30:38,825
able to really finally control the XY coordinate of the

379
00:30:38,845 --> 00:30:42,406
joystick in a way that's easy with a touch pad, not easy with

380
00:30:42,426 --> 00:30:43,006
the joystick.

381
00:30:43,847 --> 00:30:48,588
And also, I can touch type. So, you know, if you've got a text

382
00:30:48,608 --> 00:30:55,490
field, don't force me to use the on-screen keyboard. Just try and

383
00:30:55,530 --> 00:30:57,690
be aware of, like, different people are going to use

384
00:30:57,730 --> 00:31:03,332
different inputs and try and put in the extra effort to customize

385
00:31:03,492 --> 00:31:07,233
the functionality of your application for whatever kind of

386
00:31:07,293 --> 00:31:08,293
input they're using.

387
00:31:10,253 --> 00:31:13,454
And then finally, you want to minimize friction that they

388
00:31:13,494 --> 00:31:17,896
have to encounter when they're switching focus between the UI

389
00:31:18,076 --> 00:31:26,839
and the world and vice versa. I don't want to have to hit a

390
00:31:26,879 --> 00:31:30,460
button or jump through a bunch of hoops in order to be able to

391
00:31:30,500 --> 00:31:34,042
look at a browser or look at the person I'm talking to. I want to

392
00:31:34,062 --> 00:31:35,302
be able to just turn my head. So...

393
00:31:39,032 --> 00:31:48,259
that's pretty much it. High Fidelity is hiring. You can

394
00:31:48,299 --> 00:31:53,323
contact me on Twitter or e-mail with any questions you might

395
00:31:53,363 --> 00:31:59,188
have. Also, we do have a little bit of time for questions. So

396
00:31:59,208 --> 00:32:02,911
if anybody has anything they wanted to ask, please step up to

397
00:32:02,931 --> 00:32:03,131
the mic.

398
00:32:28,315 --> 00:32:32,637
of things translating over to augmented reality. It seems like

399
00:32:32,677 --> 00:32:35,918
a lot of the lessons are sort of one for one. Do you see any big

400
00:32:35,938 --> 00:32:41,320
differences? For augmented reality, I kind of feel like

401
00:32:44,069 --> 00:32:45,790
Sorry, I've got something caught in my throat.

402
00:32:46,410 --> 00:32:48,032
The question is how do I see them

403
00:32:48,072 --> 00:32:49,473
translating into augmented reality?

404
00:32:50,694 --> 00:32:52,896
I kind of feel like in augmented reality,

405
00:32:53,496 --> 00:32:56,078
you expect to see even more UI elements,

406
00:32:56,239 --> 00:32:58,941
but they're going to be sort of somewhere

407
00:32:59,181 --> 00:33:02,424
on that spectrum of internal UI to external UI.

408
00:33:02,604 --> 00:33:04,485
They're kind of gonna be attached to things.

409
00:33:04,545 --> 00:33:07,188
That's what you would expect from augmented reality.

410
00:33:07,888 --> 00:33:09,509
I see a person walking down the street

411
00:33:09,870 --> 00:33:11,871
and they've got a tag floating off of them with.

412
00:33:12,252 --> 00:33:16,614
you know, hey, I'm this person, you know, here's my family's

413
00:33:16,694 --> 00:33:19,736
names and here's how I should greet you and here's how long

414
00:33:19,776 --> 00:33:24,159
I've known you. So I would expect a lot of the same

415
00:33:25,500 --> 00:33:31,884
information to apply. I don't, I'm not sure if I'm going to

416
00:33:33,175 --> 00:33:36,918
I kind of feel like augmented reality is providing rendered

417
00:33:36,958 --> 00:33:40,541
information to you but doesn't necessarily expect you to

418
00:33:40,941 --> 00:33:47,206
provide a lot of feedback into it. Certainly you'll want to

419
00:33:47,266 --> 00:33:49,708
have some sort of UI for overlays and there will probably

420
00:33:49,748 --> 00:33:50,569
be a lot of the same...

421
00:33:53,224 --> 00:33:57,925
same information you'd want, sorry, the same topics would

422
00:33:57,965 --> 00:34:01,926
apply to augmented reality in terms of like the integrated UI.

423
00:34:01,946 --> 00:34:04,867
It's like, oh, I'm going to activate this menu and I'm going

424
00:34:04,887 --> 00:34:11,608
to bring up this overlay on the real world. So I'd be mostly

425
00:34:11,668 --> 00:34:15,129
focused on the things I was talking about in terms of

426
00:34:15,789 --> 00:34:16,569
internal UI.

427
00:34:16,589 --> 00:34:17,730
Thank you.

428
00:34:24,567 --> 00:34:25,667
Okay, well, thank you.

