1
00:00:06,963 --> 00:00:11,486
So hello everyone, I'm Simon Clavet with Ubisoft La Forge.

2
00:00:11,486 --> 00:00:14,567
La Forge is a little research lab here in Ubisoft Montreal.

3
00:00:14,567 --> 00:00:17,409
So today this is the recording for the talk

4
00:00:17,409 --> 00:00:20,171
I was supposed to give at GDC.

5
00:00:20,171 --> 00:00:22,952
So I'm just recording it here with the camera.

6
00:00:22,952 --> 00:00:25,374
So the talk is called Ragdoll Motion Matching.

7
00:00:26,717 --> 00:00:31,301
And since people normally skip the first eight minutes of every video,

8
00:00:31,301 --> 00:00:33,623
I've put the result right here in the first slide.

9
00:00:33,623 --> 00:00:34,644
You see this guy there?

10
00:00:34,644 --> 00:00:40,429
This blue guy is a physically simulated ragdoll,

11
00:00:40,429 --> 00:00:42,951
like a virtual robot.

12
00:00:42,951 --> 00:00:46,494
It's self-powered with little motors,

13
00:00:46,494 --> 00:00:50,177
and he's trying to follow interactive motion capture

14
00:00:50,177 --> 00:00:52,879
while getting lots of cubes in the face,

15
00:00:52,879 --> 00:00:55,681
trying to survive difficult perturbations.

16
00:00:57,062 --> 00:01:00,265
So that's the purpose, the goal of this talk.

17
00:01:00,265 --> 00:01:03,108
There's two goals.

18
00:01:03,108 --> 00:01:08,214
I guess my first goal is to take a wild guess about where

19
00:01:08,214 --> 00:01:10,757
game animation might be going in the future.

20
00:01:11,204 --> 00:01:15,785
The second goal is to provide intuition about the specific solution to the reinforcement

21
00:01:15,785 --> 00:01:16,865
learning problem.

22
00:01:16,865 --> 00:01:22,406
The reason for that is because reinforcement learning is a big part of what makes this

23
00:01:22,406 --> 00:01:29,788
possible and I think it's a good idea to get a good intuition behind this specific equation.

24
00:01:29,788 --> 00:01:34,489
So we're going to actually go deep and find an equation that you can actually get a feeling

25
00:01:34,489 --> 00:01:39,250
for that's useful and that's very general and really fun and beautiful I think.

26
00:01:39,710 --> 00:01:44,053
So let's start with this question, just to give the why.

27
00:01:44,053 --> 00:01:45,714
Why would we do that?

28
00:01:45,714 --> 00:01:48,156
And I want to start with this weird question.

29
00:01:48,156 --> 00:01:51,098
What's the difference between a character playing an animation

30
00:01:51,098 --> 00:01:55,101
and a character actually doing something in his virtual world?

31
00:01:55,101 --> 00:01:59,425
That's a weird question, but look at this guy there.

32
00:02:01,521 --> 00:02:08,026
So I would claim that this guy is actually there, like he's there doing a thing in his virtual world.

33
00:02:08,026 --> 00:02:16,193
He's not just, he's playing animations, but he's actually like, he's very well integrated in his world.

34
00:02:16,193 --> 00:02:19,356
And this is like the emotion I want you to feel now.

35
00:02:19,356 --> 00:02:25,861
This is something different than what video games do today, because he's actually there, like.

36
00:02:26,770 --> 00:02:28,071
That's actually happening.

37
00:02:28,071 --> 00:02:31,654
So we need to get better at physically-based animation.

38
00:02:31,654 --> 00:02:34,877
So lots of people have been trying to figure this out.

39
00:02:34,877 --> 00:02:37,760
This is the same problem as robotics.

40
00:02:37,760 --> 00:02:40,382
So lots of different ways to approach the problem.

41
00:02:40,382 --> 00:02:43,084
Lots of people in very different domains.

42
00:02:43,084 --> 00:02:46,227
So robotics people, game people, academics people,

43
00:02:46,227 --> 00:02:50,110
there are different papers every year at many conferences.

44
00:02:50,110 --> 00:02:51,992
So if you meet one of these people,

45
00:02:51,992 --> 00:02:54,654
just tell them thank you, because they actually

46
00:02:54,654 --> 00:02:55,415
figured it out.

47
00:02:55,555 --> 00:03:01,183
But most of the recent research is pointing in a fun direction,

48
00:03:01,183 --> 00:03:02,605
which is reinforcement learning.

49
00:03:02,605 --> 00:03:06,010
So that's why we dive right now into this primer.

50
00:03:06,010 --> 00:03:08,814
Let's understand what reinforcement learning is about.

51
00:03:08,814 --> 00:03:11,838
So I even put like result videos.

52
00:03:12,399 --> 00:03:14,381
inside this primary so you can't skip it.

53
00:03:14,381 --> 00:03:16,463
Don't try to skip the video to the result at the end

54
00:03:16,463 --> 00:03:18,264
because results will be right in there.

55
00:03:18,264 --> 00:03:21,207
So hold on, we'll go, it's going to be fun.

56
00:03:21,207 --> 00:03:23,749
Let's dive in right now in reinforcement learning.

57
00:03:23,749 --> 00:03:26,152
So reinforcement learning is this.

58
00:03:26,152 --> 00:03:30,135
There's lots of little words and arrows there.

59
00:03:30,135 --> 00:03:33,899
So there's env is the environment and there's a policy.

60
00:03:34,709 --> 00:03:38,511
There's states, there's actions, and there's rewards.

61
00:03:38,511 --> 00:03:40,893
So what are these things?

62
00:03:40,893 --> 00:03:44,455
So the environment is the game world and its rules.

63
00:03:44,455 --> 00:03:47,277
So here's a little environment with a little character

64
00:03:47,277 --> 00:03:50,159
trying to move around, collect coins and stars,

65
00:03:50,159 --> 00:03:52,700
and avoid stepping in lava.

66
00:03:52,700 --> 00:03:55,382
And the policy is the decision maker.

67
00:03:55,382 --> 00:03:59,425
So that's you when you press buttons on a controller, maybe.

68
00:04:00,764 --> 00:04:05,507
And so the state in this case for this environment

69
00:04:05,507 --> 00:04:09,689
is just where you are on this grid.

70
00:04:09,689 --> 00:04:13,211
The actions that the policy can make is just little arrows.

71
00:04:13,211 --> 00:04:14,992
So you step in direction.

72
00:04:14,992 --> 00:04:18,273
So each possible actions are up, down, left, and right.

73
00:04:18,273 --> 00:04:22,316
And the rewards will be like plus 1 if you get a coin

74
00:04:22,316 --> 00:04:26,378
and plus 10 if you get the big star and minus 1.

75
00:04:26,378 --> 00:04:28,739
So it's like a cost when you step into the lava.

76
00:04:30,061 --> 00:04:32,583
And a policy now is really, you need

77
00:04:32,583 --> 00:04:34,044
to think about it as a function.

78
00:04:34,044 --> 00:04:35,045
Policy is a function.

79
00:04:35,045 --> 00:04:37,067
It's a mapping from state to action.

80
00:04:37,067 --> 00:04:39,950
So it's really a function that you give it a state,

81
00:04:39,950 --> 00:04:41,451
spits out an action.

82
00:04:41,451 --> 00:04:44,013
So you see all these little arrows there.

83
00:04:44,013 --> 00:04:46,295
The state gets in the policy, get an action.

84
00:04:46,295 --> 00:04:51,259
The environment takes the old state, and in the action,

85
00:04:51,259 --> 00:04:52,260
you get a new state.

86
00:04:52,260 --> 00:04:53,962
So every step in the environment,

87
00:04:53,962 --> 00:04:56,003
like over time, every frame in the game.

88
00:04:56,987 --> 00:04:59,510
And you get a little reward, which is just one number

89
00:04:59,510 --> 00:05:01,531
every frame.

90
00:05:01,531 --> 00:05:02,052
So a policy.

91
00:05:02,052 --> 00:05:02,773
What is a policy?

92
00:05:02,773 --> 00:05:03,753
It's these arrows.

93
00:05:03,753 --> 00:05:05,935
So this is one possible policy.

94
00:05:05,935 --> 00:05:09,238
So a policy would be like just arrows there on the floor

95
00:05:09,238 --> 00:05:12,001
tell you when you're in this state, go in this direction.

96
00:05:13,380 --> 00:05:16,985
So now the policy can be a little bit random,

97
00:05:16,985 --> 00:05:18,266
like stochastic.

98
00:05:18,266 --> 00:05:20,148
So sometimes we might have a probability

99
00:05:20,148 --> 00:05:21,950
of making different choices.

100
00:05:21,950 --> 00:05:24,593
So for example, for a baseball pitcher here,

101
00:05:24,593 --> 00:05:27,657
there's a couple of different throws

102
00:05:27,657 --> 00:05:30,400
he can do with different probabilities.

103
00:05:30,400 --> 00:05:32,663
So that's a stochastic policy.

104
00:05:33,904 --> 00:05:36,346
you can do different choices for us.

105
00:05:36,346 --> 00:05:41,170
So it means that one way to think about it is the action is not just equal to policy,

106
00:05:41,170 --> 00:05:45,734
but it's sampled from probability distribution.

107
00:05:45,734 --> 00:05:51,960
It just means that the policy doesn't always give the same result for the same input.

108
00:05:51,960 --> 00:05:53,541
So in our little world here,

109
00:05:53,541 --> 00:06:00,047
it means sometimes there's different probabilities to go in with different directions.

110
00:06:02,600 --> 00:06:04,062
And now, so we need to define more things.

111
00:06:04,062 --> 00:06:08,025
So when you evaluate a policy in an environment

112
00:06:08,025 --> 00:06:10,407
from some starting state, so the starting state

113
00:06:10,407 --> 00:06:12,228
is decided by the environment.

114
00:06:12,228 --> 00:06:14,670
So every time you start evaluating a policy,

115
00:06:14,670 --> 00:06:17,732
the environment can put you in a state.

116
00:06:17,732 --> 00:06:20,715
And then what happens is you get a trajectory.

117
00:06:20,715 --> 00:06:23,157
Can also call it an episode, it's the synonym.

118
00:06:23,157 --> 00:06:25,439
And so this is a trajectory that you get.

119
00:06:25,439 --> 00:06:29,022
It's a little path in the state space.

120
00:06:30,277 --> 00:06:32,319
If the policy has some randomness,

121
00:06:32,319 --> 00:06:34,082
you could get different trajectories,

122
00:06:34,082 --> 00:06:37,707
even if you're with the same policy.

123
00:06:37,707 --> 00:06:39,529
So let's say you get this and this,

124
00:06:39,529 --> 00:06:42,673
different trajectories with the same policy.

125
00:06:44,286 --> 00:06:45,986
And now we need to define another thing.

126
00:06:45,986 --> 00:06:48,887
So what's the return of a trajectory?

127
00:06:48,887 --> 00:06:51,848
So it's the sum of rewards along the trajectory.

128
00:06:51,848 --> 00:06:53,868
So you just sum all the rewards that you

129
00:06:53,868 --> 00:06:55,349
got along the trajectory.

130
00:06:55,349 --> 00:06:58,190
It gives you a return.

131
00:06:58,190 --> 00:07:01,830
So like this trajectory there, you sum the rewards you get,

132
00:07:01,830 --> 00:07:03,271
plus 1, plus 1, plus 10.

133
00:07:03,271 --> 00:07:06,732
So the return of this specific trajectory is 12.

134
00:07:06,732 --> 00:07:11,153
But then with the same policy, you

135
00:07:11,153 --> 00:07:13,353
get different trajectories with different returns.

136
00:07:14,002 --> 00:07:19,943
And now we can define, uh, like the important thing is the average return that you get with

137
00:07:19,943 --> 00:07:20,503
this policy.

138
00:07:20,503 --> 00:07:25,784
And now the goal of reinforcement learning is to find the policy with the highest average

139
00:07:25,784 --> 00:07:28,405
return that you get with all the trajectories you get.

140
00:07:28,405 --> 00:07:31,245
Maybe trajectories are start with different start states also.

141
00:07:31,245 --> 00:07:35,886
And since the policy is a little bit random, you get different trajectories.

142
00:07:35,886 --> 00:07:37,666
Also the environment itself could be random.

143
00:07:37,666 --> 00:07:40,987
So yeah, so that's the reinforcement learning problem.

144
00:07:41,761 --> 00:07:43,543
And yeah, for this example here, you

145
00:07:43,543 --> 00:07:46,306
see the average return of this policy would be the average

146
00:07:46,306 --> 00:07:47,647
of the returns you got.

147
00:07:47,647 --> 00:07:50,290
So that was the beginning of the tutorial.

148
00:07:50,290 --> 00:07:55,234
Let's take a little break and look at the result a little bit.

149
00:07:55,234 --> 00:07:56,135
You see this guy.

150
00:07:56,135 --> 00:07:59,999
So it's hard to see how what we just discussed

151
00:07:59,999 --> 00:08:03,363
will help us with this little problem of a guy trying

152
00:08:03,363 --> 00:08:04,724
to climb these stairs there.

153
00:08:08,674 --> 00:08:14,739
So the trick is to now we're going to see the actions in the state as continuous and

154
00:08:14,739 --> 00:08:15,480
high dimensional.

155
00:08:15,480 --> 00:08:21,105
So it won't be like little discrete actions and discrete states.

156
00:08:21,105 --> 00:08:26,029
It's going to be continuous actions in states in high dimensions.

157
00:08:26,029 --> 00:08:27,731
So RL in big continuous spaces.

158
00:08:29,480 --> 00:08:30,501
So how to think about this?

159
00:08:30,501 --> 00:08:33,643
So we're gonna, the idea for our environment

160
00:08:33,643 --> 00:08:35,924
is gonna be a guy that tries not to fall.

161
00:08:35,924 --> 00:08:38,305
Sometimes he falls, sometimes he doesn't fall.

162
00:08:38,305 --> 00:08:40,927
And the rewards will be, you get rewards

163
00:08:40,927 --> 00:08:43,729
when you're not falling, and you get penalties

164
00:08:43,729 --> 00:08:44,229
when you fall.

165
00:08:45,410 --> 00:08:49,812
So but I will explain the details of our specific environment a little bit later.

166
00:08:49,812 --> 00:08:53,613
Let's continue with our little tutorial.

167
00:08:53,613 --> 00:08:59,195
So we'll go back to deterministic case, like let's forget about randomness for a little bit.

168
00:08:59,195 --> 00:09:00,756
And now the states is a point.

169
00:09:00,756 --> 00:09:05,597
Let's say there's four dimensions, continuous little point there with four dimensions.

170
00:09:05,597 --> 00:09:11,339
And the actions also is a continuous point in some other space.

171
00:09:11,339 --> 00:09:12,620
Let's say there's three dimensions there.

172
00:09:14,065 --> 00:09:17,738
And let's try to see what the policy is now with this.

173
00:09:18,924 --> 00:09:21,905
So we're going to plot actions with respect to state.

174
00:09:21,905 --> 00:09:24,046
It's just a little function.

175
00:09:24,046 --> 00:09:27,207
Just to see it, let's think about just one dimension

176
00:09:27,207 --> 00:09:29,548
for state, one dimension for action for now.

177
00:09:29,548 --> 00:09:33,209
And so yeah, so when you're in this state, 3.8,

178
00:09:33,209 --> 00:09:35,510
you get the action 9.9.

179
00:09:35,510 --> 00:09:39,031
And you can think of a policy being this function.

180
00:09:39,031 --> 00:09:42,172
So the goal is now not to find an action necessarily,

181
00:09:42,172 --> 00:09:43,872
but really finding a function.

182
00:09:43,872 --> 00:09:46,173
So we're going to discover the best function.

183
00:09:47,419 --> 00:09:49,901
that gives us the best reward.

184
00:09:49,901 --> 00:09:53,305
So you see reward is still just one number.

185
00:09:53,305 --> 00:09:55,447
It's not a vector.

186
00:09:55,447 --> 00:10:00,452
So here's a simple policy you can come up with.

187
00:10:00,452 --> 00:10:02,714
This, that's a policy, that's a valid policy.

188
00:10:02,714 --> 00:10:07,439
That's a little equation that takes as input the state

189
00:10:07,439 --> 00:10:09,140
and returns an action.

190
00:10:09,140 --> 00:10:09,521
Looks like this.

191
00:10:12,829 --> 00:10:15,210
So you see there's numbers there, the .3 and .2,

192
00:10:15,210 --> 00:10:19,251
that's just the parameters of this line.

193
00:10:19,251 --> 00:10:22,273
And in general, you see you can have parameters.

194
00:10:22,273 --> 00:10:24,034
Let's call them theta.

195
00:10:24,034 --> 00:10:25,394
Theta zero and theta one would be

196
00:10:25,394 --> 00:10:27,155
the parameters of the function.

197
00:10:27,155 --> 00:10:29,716
So you see there's inputs and parameters.

198
00:10:29,716 --> 00:10:30,616
It's two different things.

199
00:10:30,616 --> 00:10:32,777
The parameters are like hard-coded numbers

200
00:10:32,777 --> 00:10:35,759
inside the function.

201
00:10:35,759 --> 00:10:37,720
And we can place them also in a little vector.

202
00:10:37,720 --> 00:10:40,021
So we have a little vector of parameters, we call it theta,

203
00:10:40,021 --> 00:10:42,242
and that's the parameters of this function.

204
00:10:43,704 --> 00:10:48,486
So now the fun things today in this day and age of deep learning,

205
00:10:48,486 --> 00:10:50,587
the functions can get pretty big.

206
00:10:50,587 --> 00:10:54,810
And one thing we can say is that if the equation is big enough,

207
00:10:54,810 --> 00:10:56,510
it becomes naturally a neural net.

208
00:10:56,510 --> 00:10:59,252
A neural net is just a big equation with lots of parameters.

209
00:10:59,252 --> 00:11:03,154
That's a little bit hierarchical, but we don't actually care about the details

210
00:11:03,154 --> 00:11:05,035
of what a neural net is for now.

211
00:11:05,035 --> 00:11:09,057
Let's just think about it as an equation with parameters.

212
00:11:09,057 --> 00:11:11,678
It could be the weights of the neural net.

213
00:11:13,660 --> 00:11:17,723
And yeah, so when you change the parameters of this function,

214
00:11:17,723 --> 00:11:19,845
you get different functions.

215
00:11:19,845 --> 00:11:25,369
So now finding a policy means finding a vector theta

216
00:11:25,369 --> 00:11:28,491
of parameters that gives the best rewards.

217
00:11:28,491 --> 00:11:31,654
So what do we do with that?

218
00:11:31,654 --> 00:11:36,117
Like with our environment, we're going to start at some state.

219
00:11:37,498 --> 00:11:41,981
We S0, and then we give it to our policy.

220
00:11:41,981 --> 00:11:43,282
We get an action, A0.

221
00:11:43,282 --> 00:11:46,844
Give that to our environment, state an action.

222
00:11:46,844 --> 00:11:49,905
And we get to a new state, and the environment

223
00:11:49,905 --> 00:11:51,086
gives us a little reward.

224
00:11:51,086 --> 00:11:56,909
And repeat, and repeat, and you get our trajectory like this.

225
00:11:56,909 --> 00:11:59,551
So now the goal is to tweak data.

226
00:11:59,551 --> 00:12:02,753
Like, we're going to change data a little bit

227
00:12:02,753 --> 00:12:05,294
to get more rewards later in the future.

228
00:12:06,643 --> 00:12:08,485
So one thing we can do is plot it like this.

229
00:12:08,485 --> 00:12:10,608
So we're going to put on this axis there,

230
00:12:10,608 --> 00:12:11,449
theta, the parameter.

231
00:12:11,449 --> 00:12:13,191
Let's say there's just one parameter.

232
00:12:13,191 --> 00:12:16,436
Let's install a neural network with just one neuron

233
00:12:16,436 --> 00:12:17,557
or something.

234
00:12:17,557 --> 00:12:20,481
And so all policies are somewhere on this axis.

235
00:12:21,812 --> 00:12:23,353
And we're going to try one.

236
00:12:23,353 --> 00:12:28,175
Let's try one neural network with this parameter.

237
00:12:28,175 --> 00:12:32,638
And we're going to run many trajectories with this policy

238
00:12:32,638 --> 00:12:34,539
and compute the average return.

239
00:12:34,539 --> 00:12:39,501
Average return would be like average summed over all steps,

240
00:12:39,501 --> 00:12:42,202
over all trajectories, all the episodes that you do.

241
00:12:42,202 --> 00:12:44,283
Maybe divide it by number of trajectories,

242
00:12:44,283 --> 00:12:45,944
but we don't actually care.

243
00:12:45,944 --> 00:12:47,625
And you get this number that says how

244
00:12:47,625 --> 00:12:50,667
happy you are with this policy.

245
00:12:52,947 --> 00:12:58,982
And you can try it again with different vectors of parameters.

246
00:12:58,982 --> 00:13:02,250
Look around, see what's happening there.

247
00:13:03,193 --> 00:13:09,534
And you see, like, virtually, like, magically, there is this world of policies, and you try

248
00:13:09,534 --> 00:13:15,675
to climb some, hopefully there's some mountain there that you try to get to the top of.

249
00:13:15,675 --> 00:13:21,897
So that's finding this data best with the best average return is the goal of reinforcement

250
00:13:21,897 --> 00:13:22,957
learning.

251
00:13:22,957 --> 00:13:28,938
And one way to do it is to start with a guess, a guess policy, and try to climb, to find

252
00:13:28,938 --> 00:13:30,458
a slope where in the...

253
00:13:30,818 --> 00:13:35,441
parameter world you should go in this direction to improve your policy.

254
00:13:35,441 --> 00:13:40,663
So for this you can just as I said just try a bunch of different policies and

255
00:13:40,663 --> 00:13:44,765
the problem is policies it's really neural networks like it's parameters of

256
00:13:44,765 --> 00:13:48,627
neural networks you can really try them randomly that's gonna that might be hard

257
00:13:48,627 --> 00:13:52,168
but you can still try it it leads to evolution algorithms that's what

258
00:13:52,168 --> 00:13:55,290
evolution is is trying different

259
00:13:56,323 --> 00:14:03,527
Policies you could do it with like particle swarms or evolution like genetic evolution you could have

260
00:14:03,527 --> 00:14:11,151
Genetic evolution is part of that so each of these little dots there is one policy one vector of parameters

261
00:14:11,151 --> 00:14:13,252
And you can evaluate

262
00:14:13,252 --> 00:14:14,153
the

263
00:14:14,153 --> 00:14:15,173
average return you get

264
00:14:16,174 --> 00:14:19,516
lots of average, lots of return in the white part there.

265
00:14:19,516 --> 00:14:23,678
So, generations after generation, you're going to hopefully

266
00:14:23,678 --> 00:14:29,241
explore in the policy world and focus and focus on the solution.

267
00:14:30,909 --> 00:14:34,913
So what's hard with this is that when theta has lots of dimensions,

268
00:14:34,913 --> 00:14:38,736
it's hard to sample when there's too many dimensions because of the curse of

269
00:14:38,736 --> 00:14:42,459
dimensionality. You can't really explore efficiently. It's very difficult.

270
00:14:42,459 --> 00:14:45,662
But the actual problem with evolution is not that the actual problem with

271
00:14:45,662 --> 00:14:49,184
evolution is like need to die a lot in order to learn something like it's

272
00:14:49,184 --> 00:14:51,827
learning by dying. It's not super efficient.

273
00:14:51,827 --> 00:14:54,829
That's why it took like billions of years.

274
00:14:55,610 --> 00:14:57,231
Or millions, I don't know.

275
00:14:57,231 --> 00:15:02,234
So and also the other problem is we're not using all the available information on good

276
00:15:02,234 --> 00:15:03,835
and bad actions.

277
00:15:03,835 --> 00:15:08,518
You just know that like at the end of the episodes it was bad but you don't know which

278
00:15:08,518 --> 00:15:10,740
actions were actually responsible for the problem.

279
00:15:12,290 --> 00:15:15,331
And it would be better to use all the information.

280
00:15:15,331 --> 00:15:17,932
For example, these little cats there,

281
00:15:17,932 --> 00:15:19,812
they learned that if they press the button,

282
00:15:19,812 --> 00:15:21,453
they get some food.

283
00:15:21,453 --> 00:15:23,553
But the reason why they managed to learn it

284
00:15:23,553 --> 00:15:26,214
is because they get the reward just not too long

285
00:15:26,214 --> 00:15:27,575
after they did the action.

286
00:15:29,516 --> 00:15:36,042
So we're going to try to use this knowledge to improve our algorithms.

287
00:15:36,042 --> 00:15:40,827
So if you pick up a book on reinforcement learning or Google online, you'll see there's

288
00:15:40,827 --> 00:15:44,110
a whole zoo of methods you can use.

289
00:15:44,530 --> 00:15:47,172
And evolution is just one of them.

290
00:15:47,172 --> 00:15:48,914
There's a bunch of different words there,

291
00:15:48,914 --> 00:15:54,038
like roboticists spend more time on one side of this.

292
00:15:54,038 --> 00:15:56,039
And you see Q-learning there.

293
00:15:56,039 --> 00:15:58,101
That's one that we won't talk about today,

294
00:15:58,101 --> 00:16:00,483
but it's really good for learning video games.

295
00:16:00,483 --> 00:16:04,266
So we're going to focus on this region there, policy, gradient,

296
00:16:04,266 --> 00:16:07,088
and actor-critic, because there's some good intuition

297
00:16:07,088 --> 00:16:08,269
to be gained there.

298
00:16:08,269 --> 00:16:11,091
And it actually works in a surprisingly general way.

299
00:16:12,612 --> 00:16:16,716
So let's take a little break and look at our guy again,

300
00:16:16,716 --> 00:16:19,319
try to guess what's happening.

301
00:16:19,319 --> 00:16:23,943
And he said his goal is to follow motion capture,

302
00:16:23,943 --> 00:16:28,288
but also to not fall by, he decides where to put his foot.

303
00:16:36,125 --> 00:16:41,447
So what's the plan for inventing this fun algorithm of policy

304
00:16:41,447 --> 00:16:41,747
gradient?

305
00:16:41,747 --> 00:16:44,628
So we're going to forget about most things

306
00:16:44,628 --> 00:16:48,149
and really focus on this simple idea of trying

307
00:16:48,149 --> 00:16:52,351
to make good actions more probable.

308
00:16:52,351 --> 00:16:53,171
So OK.

309
00:16:53,171 --> 00:16:54,431
So we'll have a policy.

310
00:16:55,259 --> 00:16:56,880
It's going to be a little bit random.

311
00:16:56,880 --> 00:16:59,061
It's going to take different actions.

312
00:16:59,061 --> 00:17:01,762
Find good actions, make them more probable.

313
00:17:01,762 --> 00:17:02,643
So that's the idea.

314
00:17:02,643 --> 00:17:04,964
So sometimes you make an action.

315
00:17:04,964 --> 00:17:07,985
You're in some state that look a little bit the same.

316
00:17:07,985 --> 00:17:10,867
And your policy will give you different actions.

317
00:17:10,867 --> 00:17:13,628
At every step, there's going to be lots of different actions,

318
00:17:13,628 --> 00:17:14,589
lots of dimensions.

319
00:17:14,589 --> 00:17:17,230
But let's say at some point, you make this action.

320
00:17:17,230 --> 00:17:20,392
And another point, another try, another episode,

321
00:17:20,392 --> 00:17:21,832
you make another action.

322
00:17:21,832 --> 00:17:24,934
Like you see 3.7 got a better result.

323
00:17:26,210 --> 00:17:33,537
So like you see like the returns after the 3.9 got to zero because he fell.

324
00:17:33,537 --> 00:17:38,543
So we like better 3.7 in this state should do this action.

325
00:17:38,543 --> 00:17:42,847
We don't care too much about what this action means. Maybe it's a torque,

326
00:17:42,847 --> 00:17:47,412
maybe it's a motor activation in the knee or something deciding to put the foot

327
00:17:47,412 --> 00:17:48,873
somewhere else to avoid falling.

328
00:17:50,341 --> 00:17:52,984
So yeah, so we're going to find the high return actions

329
00:17:52,984 --> 00:17:55,366
and change our policy to make them more probable.

330
00:17:55,366 --> 00:17:58,009
So there's two things we need to think about now.

331
00:17:58,009 --> 00:17:59,511
So let's start with the second one.

332
00:17:59,511 --> 00:18:02,053
So how to make good actions more probable.

333
00:18:02,053 --> 00:18:04,356
Let's say we already know which ones are good.

334
00:18:06,038 --> 00:18:09,901
So we go back to this little drawing there, a random policy.

335
00:18:09,901 --> 00:18:11,783
There is the action there.

336
00:18:11,783 --> 00:18:12,804
We're in a fixed state.

337
00:18:12,804 --> 00:18:14,986
Let's say the state is fixed.

338
00:18:14,986 --> 00:18:17,247
And now this graph is the probability

339
00:18:17,247 --> 00:18:18,549
of picking the action.

340
00:18:18,549 --> 00:18:21,171
But now we're in continuous space.

341
00:18:21,171 --> 00:18:22,172
So it's more like this.

342
00:18:22,172 --> 00:18:25,054
So we're sampling the action from a probability density.

343
00:18:25,054 --> 00:18:27,636
This curve there is a probability density.

344
00:18:27,636 --> 00:18:30,198
The sum under the curve should be 1

345
00:18:30,198 --> 00:18:30,879
because it's a probability.

346
00:18:30,879 --> 00:18:33,081
And you see there's more probability

347
00:18:33,081 --> 00:18:34,522
of being in the middle there.

348
00:18:36,147 --> 00:18:40,212
And now we have a sample, this action 3.7, and we like it,

349
00:18:40,212 --> 00:18:42,775
because we think it's a good action.

350
00:18:42,775 --> 00:18:47,541
We want to increase the probability of doing that more.

351
00:18:47,541 --> 00:18:49,223
We want to do it more in the future.

352
00:18:49,223 --> 00:18:50,785
So what it means, good returns.

353
00:18:50,785 --> 00:18:53,448
We want to increase this probability.

354
00:18:53,448 --> 00:18:56,471
So we want to make a little bump there in the curve.

355
00:18:57,270 --> 00:19:00,393
But that's weird, like how are we going to actually do this?

356
00:19:00,393 --> 00:19:04,756
So we need to be more specific about the form of our policy.

357
00:19:04,756 --> 00:19:09,559
We won't do like a big table, like the tabular thing.

358
00:19:09,559 --> 00:19:11,040
We want to be more specific.

359
00:19:11,040 --> 00:19:14,083
So the simplest choice is to just pick a Gaussian,

360
00:19:14,083 --> 00:19:17,325
like a simple curve with just two parameters, the mean

361
00:19:17,325 --> 00:19:18,486
and the standard deviation.

362
00:19:19,388 --> 00:19:22,590
And now the idea is that our little neural network that

363
00:19:22,590 --> 00:19:24,411
was supposed to give us our action now

364
00:19:24,411 --> 00:19:27,153
will give us instead the parameters

365
00:19:27,153 --> 00:19:29,514
of this distribution of action.

366
00:19:30,383 --> 00:19:31,744
And this is the weird part.

367
00:19:31,744 --> 00:19:35,366
You need to stop and just think about it for five seconds.

368
00:19:35,366 --> 00:19:37,608
Now our neural network, that's our policy,

369
00:19:37,608 --> 00:19:41,930
gives us the parameters of a distribution of actions

370
00:19:41,930 --> 00:19:42,710
for each state.

371
00:19:42,710 --> 00:19:45,392
You see the state is still an input, the network.

372
00:19:45,392 --> 00:19:49,014
It outputs, the neural network outputs two things now,

373
00:19:49,014 --> 00:19:52,316
the mean and the standard deviation,

374
00:19:52,316 --> 00:19:53,957
the variance of this Gaussian.

375
00:19:53,957 --> 00:19:56,598
So now we have this good action.

376
00:19:56,598 --> 00:19:59,840
We're going to increase the probability there.

377
00:20:01,298 --> 00:20:03,861
So we can think about it for a couple of seconds.

378
00:20:03,861 --> 00:20:06,244
What's going to happen with the mean and standard deviation

379
00:20:06,244 --> 00:20:09,587
if we want to increase the probability there?

380
00:20:09,587 --> 00:20:11,529
You see, the first thing that comes to mind

381
00:20:11,529 --> 00:20:14,712
is to move the mean in this direction.

382
00:20:14,712 --> 00:20:16,654
That's going to increase the probability there.

383
00:20:16,654 --> 00:20:19,377
But the other idea with the standard deviation, what

384
00:20:19,377 --> 00:20:22,620
should happen to it if the good action was near the tail?

385
00:20:23,075 --> 00:20:27,317
It might be a good idea to make the Gaussian a little bit more fat,

386
00:20:27,317 --> 00:20:28,338
like to explore more.

387
00:20:28,338 --> 00:20:32,100
So when you get good actions in the tail of your distribution,

388
00:20:32,100 --> 00:20:36,302
it makes you think, OK, I should explore more because I got good actions really

389
00:20:36,302 --> 00:20:37,143
far.

390
00:20:37,143 --> 00:20:39,104
So that's like, feel smart.

391
00:20:40,068 --> 00:20:41,088
And you're going to do both.

392
00:20:41,088 --> 00:20:42,429
You're going to change the mean and STD.

393
00:20:42,429 --> 00:20:44,490
But you see there's an indirection.

394
00:20:44,490 --> 00:20:46,811
We won't change the mean and STD directly.

395
00:20:46,811 --> 00:20:49,973
What we want to do is to nudge theta, the parameters,

396
00:20:49,973 --> 00:20:52,475
so that the mean and STD changes,

397
00:20:52,475 --> 00:20:53,995
so that you increase the probability

398
00:20:53,995 --> 00:20:54,756
of your good actions.

399
00:20:54,756 --> 00:20:56,597
You see there's three levels of indirection.

400
00:20:56,597 --> 00:20:59,198
That's what makes it weird a little bit.

401
00:20:59,198 --> 00:21:00,199
But it works.

402
00:21:00,199 --> 00:21:03,541
And when the good action is close to the middle there,

403
00:21:03,541 --> 00:21:04,421
what's going to happen?

404
00:21:04,421 --> 00:21:05,622
It's going to get spikier.

405
00:21:05,622 --> 00:21:09,884
And this is where you're going to focus on the good solution.

406
00:21:11,758 --> 00:21:16,022
So what happens when you have two-dimensional state

407
00:21:16,022 --> 00:21:17,103
and one-dimensional action?

408
00:21:17,103 --> 00:21:20,206
You can have this fun little visualization here.

409
00:21:20,206 --> 00:21:24,771
You see each of these little curves is for one state.

410
00:21:24,771 --> 00:21:27,214
And you see the action is one-dimensional.

411
00:21:27,214 --> 00:21:29,316
So as the thing is learning.

412
00:21:30,196 --> 00:21:33,538
it becomes more focused on the solution.

413
00:21:33,538 --> 00:21:38,042
It's like a little breathing of exploration and exploitation.

414
00:21:38,042 --> 00:21:41,364
Exploitation, the exploration dilemma is a really big thing

415
00:21:41,364 --> 00:21:43,706
so we need to be smart about it.

416
00:21:43,706 --> 00:21:47,269
So it's like breathing your exploration like this.

417
00:21:47,269 --> 00:21:49,851
So let's get a little bit more precise.

418
00:21:49,851 --> 00:21:52,113
What we actually want to do.

419
00:21:52,113 --> 00:21:56,036
This new policy, what's gonna happen is that we're gonna

420
00:21:57,464 --> 00:22:01,705
do lots of actions, lots of states, a big batch of actions.

421
00:22:01,705 --> 00:22:04,546
And now, with this batch of actions,

422
00:22:04,546 --> 00:22:08,167
for each of them, we're going to nudge our probability,

423
00:22:08,167 --> 00:22:13,309
like change our probability so that the good actions will

424
00:22:13,309 --> 00:22:15,550
be more probable.

425
00:22:15,550 --> 00:22:17,371
So look at these curves there.

426
00:22:17,371 --> 00:22:20,892
New probability, old probability at this action.

427
00:22:20,892 --> 00:22:24,233
So one idea is to pick the ratio of these two.

428
00:22:24,233 --> 00:22:25,994
What's the ratio of these things?

429
00:22:26,635 --> 00:22:31,062
In this case it's like 3, it's like 3 times bigger approximately.

430
00:22:31,062 --> 00:22:35,529
We want this to be big if we were happy with the action.

431
00:22:35,529 --> 00:22:40,216
So the trick is to just multiply it with some number that tells us how happy we are with the action.

432
00:22:40,905 --> 00:22:42,946
So we have an equation here.

433
00:22:42,946 --> 00:22:44,247
Yeah, so how happy with the action?

434
00:22:44,247 --> 00:22:46,588
Let's put this number.

435
00:22:46,588 --> 00:22:50,870
Let's make sure that it's negative when we're unhappy.

436
00:22:50,870 --> 00:22:53,070
So it's going to bring it down the ratio.

437
00:22:53,070 --> 00:22:55,251
It's going to go below 1 in this case.

438
00:22:55,251 --> 00:22:57,772
So that's like the force that we want.

439
00:22:57,772 --> 00:23:00,814
We want to nudge data to pull this number up.

440
00:23:00,814 --> 00:23:01,454
That should work.

441
00:23:03,372 --> 00:23:05,114
So how do we actually do this?

442
00:23:05,114 --> 00:23:10,719
Like you see the new probability is a prob- it's a function of data and the current action.

443
00:23:10,719 --> 00:23:12,040
So- and we want to nudge data.

444
00:23:12,040 --> 00:23:15,903
What does it mean to nudge data with the trick to nudge things

445
00:23:15,903 --> 00:23:18,966
in machine learning and every like optimization world.

446
00:23:18,966 --> 00:23:22,589
If you know a little bit about these kind of things,

447
00:23:22,589 --> 00:23:27,093
it- it means taking the derivative of the thing with respect.

448
00:23:27,774 --> 00:23:29,174
to the thing that you want to nudge.

449
00:23:29,174 --> 00:23:32,457
So that's the slope of the thing with respect

450
00:23:32,457 --> 00:23:33,697
to the parameters theta.

451
00:23:33,697 --> 00:23:35,398
So that's the big equation.

452
00:23:35,398 --> 00:23:39,061
So what it means there, we're going to update theta

453
00:23:39,061 --> 00:23:40,802
to go in the direction of the slope.

454
00:23:40,802 --> 00:23:42,623
So that's a big thing, but that's it.

455
00:23:42,623 --> 00:23:45,025
That's the big equation I wanted to get to.

456
00:23:45,025 --> 00:23:49,488
So you see where the 0.01, that's like a learning rate.

457
00:23:49,488 --> 00:23:53,850
We're going to nudge the parameters in this direction.

458
00:23:53,850 --> 00:23:56,672
That should make the good actions more probable.

459
00:23:59,006 --> 00:24:00,846
So yeah, so that's where we are now.

460
00:24:00,846 --> 00:24:02,887
We need to decide what,

461
00:24:02,887 --> 00:24:05,867
how to decide if we're happy with an action or not.

462
00:24:05,867 --> 00:24:07,928
And if you look here,

463
00:24:07,928 --> 00:24:12,189
the only data that appears in this equation are there.

464
00:24:12,189 --> 00:24:15,750
Like the other things are just sampled, there's no derivatives.

465
00:24:15,750 --> 00:24:18,910
But this thing, derivative with respect to data of a neural network,

466
00:24:18,910 --> 00:24:22,531
that's what TensorFlow and PyTorch and all these things are good at.

467
00:24:22,531 --> 00:24:24,832
That's what they do. That's what deep learning is.

468
00:24:24,832 --> 00:24:26,832
It's to find derivatives of.

469
00:24:28,032 --> 00:24:30,774
things with respect to parameters of neural networks.

470
00:24:30,774 --> 00:24:32,455
So that's what we like.

471
00:24:32,455 --> 00:24:33,036
It's autodiff.

472
00:24:33,036 --> 00:24:37,959
That means finding derivatives automatically,

473
00:24:37,959 --> 00:24:40,341
and big batches of that on GPU to go super fast.

474
00:24:40,341 --> 00:24:45,124
And that's given for free with TensorFlow and PyTorch

475
00:24:45,124 --> 00:24:45,805
and all these things.

476
00:24:45,805 --> 00:24:50,348
So the last little piece of the puzzle,

477
00:24:50,348 --> 00:24:53,110
how to find good actions.

478
00:24:53,110 --> 00:24:56,212
So yeah, so there's lots of things to think about.

479
00:24:56,949 --> 00:25:01,394
So let's look back at our little cats and our little beaver there.

480
00:25:01,394 --> 00:25:03,756
But let's look at one trajectory there.

481
00:25:03,756 --> 00:25:08,321
Different actions, let's say all the little a's are different actions,

482
00:25:08,321 --> 00:25:10,042
and you get different rewards.

483
00:25:10,042 --> 00:25:13,005
Look at the little rewards.

484
00:25:13,005 --> 00:25:13,826
You got a minus 30 there,

485
00:25:13,826 --> 00:25:15,928
that means that's bad,

486
00:25:15,928 --> 00:25:17,270
and a 40 there, that's fun.

487
00:25:18,214 --> 00:25:22,338
So it means probably that these actions that led to the good reward were good.

488
00:25:22,338 --> 00:25:27,662
And the bad actions before the big bad reward, those were bad actions.

489
00:25:27,662 --> 00:25:33,447
So so maybe one of these actions is the thing that caused the problem.

490
00:25:33,447 --> 00:25:35,869
So trying to decide which actions

491
00:25:35,869 --> 00:25:39,212
are responsible for good and bad things that happen later in the future.

492
00:25:39,212 --> 00:25:40,753
That's the credit assignment problem.

493
00:25:40,753 --> 00:25:44,096
It feels like an easy problem when just looking at it like this.

494
00:25:44,096 --> 00:25:45,477
But there's like.

495
00:25:46,865 --> 00:25:50,027
how far in the future should you look for the rewards

496
00:25:50,027 --> 00:25:52,968
and how to decide which actions cause the problems,

497
00:25:52,968 --> 00:25:53,869
that's a hard problem.

498
00:25:53,869 --> 00:25:56,250
And one easy way to think about it

499
00:25:56,250 --> 00:25:58,271
is to think about this question,

500
00:25:58,271 --> 00:26:00,913
like do you prefer some money now

501
00:26:00,913 --> 00:26:03,915
or a little bit more money but far in the future?

502
00:26:03,915 --> 00:26:07,117
Maybe you prefer $1,000 now, that's possible.

503
00:26:07,117 --> 00:26:09,018
So that means that in your head,

504
00:26:09,018 --> 00:26:12,960
you discounted the rewards that are too far in the future

505
00:26:12,960 --> 00:26:15,101
with some discount factor that you have in your brain.

506
00:26:16,186 --> 00:26:21,190
So let's say the discount factor, gamma, this little Greek letter there, 0.9.

507
00:26:21,190 --> 00:26:26,814
We're going to scale the future rewards with this number and more and more scale

508
00:26:26,814 --> 00:26:28,396
down as you go far in the future.

509
00:26:28,396 --> 00:26:33,240
So for example, what's the discounted return of this action that you made there?

510
00:26:35,583 --> 00:26:37,124
So how would you compute this?

511
00:26:37,124 --> 00:26:39,205
Like the idea is to compute it like this.

512
00:26:39,205 --> 00:26:40,886
So you get the first reward.

513
00:26:40,886 --> 00:26:43,027
That doesn't get discounted.

514
00:26:43,027 --> 00:26:44,408
The second one in the future,

515
00:26:44,408 --> 00:26:48,270
this minus 30, we're going to multiply it with 0.9.

516
00:26:48,270 --> 00:26:50,951
The next one, multiply it by 0.9 times 0.9.

517
00:26:50,951 --> 00:26:54,993
See, if you go further and further in the future,

518
00:26:54,993 --> 00:26:59,335
you're going to multiply it by a smaller and smaller number.

519
00:26:59,335 --> 00:27:03,177
See, to the three here and continue to the four.

520
00:27:03,873 --> 00:27:06,574
And it gives us this little equation for a discounted return.

521
00:27:06,574 --> 00:27:09,696
So that's why you're going to see always this little thing there,

522
00:27:09,696 --> 00:27:11,918
sum with gamma to the t.

523
00:27:11,918 --> 00:27:13,719
So that's discounted return.

524
00:27:13,719 --> 00:27:17,881
So from now on, when I say return, it means actually discounted return.

525
00:27:17,881 --> 00:27:22,624
So that solves this part for deciding which actions

526
00:27:22,624 --> 00:27:24,805
are responsible for which rewards.

527
00:27:28,606 --> 00:27:32,187
Now, let's say we're in some state that

528
00:27:32,187 --> 00:27:33,507
looks a little bit the same.

529
00:27:33,507 --> 00:27:35,428
Let's say S3 is some state.

530
00:27:35,428 --> 00:27:36,648
Don't really care too much what it is.

531
00:27:36,648 --> 00:27:38,889
Let's say it's a state that looks a little bit the same.

532
00:27:38,889 --> 00:27:42,550
And with our policy, we did different actions randomly,

533
00:27:42,550 --> 00:27:44,511
with our stochastic policy, with the different actions.

534
00:27:44,511 --> 00:27:47,472
You see actions are a little bit different.

535
00:27:47,472 --> 00:27:49,252
And we're trying to decide if we're

536
00:27:49,252 --> 00:27:53,554
happy with this specific action that we

537
00:27:53,554 --> 00:27:57,175
made at this point in our big trajectories.

538
00:27:58,658 --> 00:28:01,041
So to know if we're happy, we're going to look at what happened after.

539
00:28:01,041 --> 00:28:02,682
Compute the discounted return.

540
00:28:02,682 --> 00:28:06,065
You see, maybe was that good?

541
00:28:06,065 --> 00:28:11,670
Like there's a minus 35 there, so maybe we're not happy, but we also need to look

542
00:28:11,670 --> 00:28:15,333
at the other ones and that's the main idea of this slide.

543
00:28:15,333 --> 00:28:22,059
If you look at the other one there and the other one here, that's even worse.

544
00:28:24,030 --> 00:28:25,251
big negative numbers there.

545
00:28:25,251 --> 00:28:27,053
So it means we are actually happy with this.

546
00:28:27,053 --> 00:28:29,196
Minus 35 was not too bad.

547
00:28:29,196 --> 00:28:32,761
So it brings this intuition that what we actually

548
00:28:32,761 --> 00:28:35,524
want to compute is the return that we got

549
00:28:35,524 --> 00:28:38,448
minus the average return from this state.

550
00:28:39,800 --> 00:28:42,041
So average return from the state is an important concept,

551
00:28:42,041 --> 00:28:45,004
like roboticists and everybody really focused on that a lot.

552
00:28:45,004 --> 00:28:46,525
That's called the value.

553
00:28:46,525 --> 00:28:50,508
That's where every time you hear value or reward to go,

554
00:28:50,508 --> 00:28:52,990
or cost to go if you're a little bit pessimistic,

555
00:28:52,990 --> 00:28:54,551
like roboticists.

556
00:28:54,551 --> 00:28:56,212
So that's the value, it's important.

557
00:28:56,212 --> 00:29:01,456
And the trick is to actually keep track of that.

558
00:29:02,058 --> 00:29:05,900
another little neural network that we're going to try and train on the side.

559
00:29:05,900 --> 00:29:10,282
And that's almost supervised learning with a moving target,

560
00:29:10,282 --> 00:29:12,002
but still a little bit supervised.

561
00:29:12,002 --> 00:29:18,245
So it's kind of simple in a way because you can always compute the discounted

562
00:29:18,245 --> 00:29:19,686
returns that you get after.

563
00:29:20,286 --> 00:29:25,569
some state and then train this neural network in a supervised way.

564
00:29:25,569 --> 00:29:31,453
It's just, you know, the answer like, and you just train this on the side with your

565
00:29:31,453 --> 00:29:32,593
main policy network.

566
00:29:33,367 --> 00:29:35,408
So some people call this the critic.

567
00:29:35,408 --> 00:29:38,428
And we have our real policy network

568
00:29:38,428 --> 00:29:40,669
that we're going to actually ship at runtime

569
00:29:40,669 --> 00:29:41,969
when you want to test the thing.

570
00:29:41,969 --> 00:29:43,289
You just need the actor.

571
00:29:43,289 --> 00:29:45,030
You don't need a critic anymore.

572
00:29:45,030 --> 00:29:47,650
So that's where the actor-critic ID comes from.

573
00:29:47,650 --> 00:29:51,831
And now, as you train this, you get a little dance

574
00:29:51,831 --> 00:29:52,912
between the actor and the critic.

575
00:29:52,912 --> 00:29:55,052
The actor decides what actions to do,

576
00:29:55,052 --> 00:29:57,113
and the critic is judging and trying

577
00:29:57,113 --> 00:29:59,553
to guess how much discounted return you're

578
00:29:59,553 --> 00:30:02,434
going to get from this state with this policy.

579
00:30:04,755 --> 00:30:06,576
So yes, so that's about it.

580
00:30:06,576 --> 00:30:12,039
Like you can plug this in our equation that we found a couple of slides ago.

581
00:30:12,039 --> 00:30:14,541
We plug this and there you have it.

582
00:30:14,541 --> 00:30:15,822
Like this is the thing.

583
00:30:15,822 --> 00:30:17,803
This is the equation I wanted to get to.

584
00:30:17,803 --> 00:30:18,944
That's the policy gradient.

585
00:30:18,944 --> 00:30:21,365
It's a beautiful equation.

586
00:30:21,365 --> 00:30:26,728
This is the thing that gives more probability toward a good actions.

587
00:30:27,719 --> 00:30:29,841
And this is surprisingly general.

588
00:30:29,841 --> 00:30:31,643
It's really surprising that it actually works

589
00:30:31,643 --> 00:30:33,945
because it feels really awkward.

590
00:30:33,945 --> 00:30:36,749
But you can actually solve video games with that.

591
00:30:36,749 --> 00:30:38,551
You can solve robotics problems with that.

592
00:30:38,551 --> 00:30:40,953
The Berkeley people and OpenAI people

593
00:30:40,953 --> 00:30:42,555
really like this algorithm a lot.

594
00:30:42,555 --> 00:30:44,657
And there's lots of variants of that.

595
00:30:45,752 --> 00:30:47,052
Here's the actual math.

596
00:30:47,052 --> 00:30:49,994
If you want to read this, and you'll see in the books,

597
00:30:49,994 --> 00:30:52,455
that's exactly what we talked about,

598
00:30:52,455 --> 00:30:57,318
but in more mathematical form.

599
00:30:57,318 --> 00:31:00,980
Here's some code, lots of implementations

600
00:31:00,980 --> 00:31:01,501
of that on the web.

601
00:31:01,501 --> 00:31:05,123
You see everything we talked about is there,

602
00:31:05,123 --> 00:31:07,944
how happy with actions, the losses, the episodes.

603
00:31:08,925 --> 00:31:10,265
That's my suggestion.

604
00:31:10,265 --> 00:31:13,127
If you want to learn this, because I think it's cool.

605
00:31:13,127 --> 00:31:15,528
I think everybody should try to learn that if they can,

606
00:31:15,528 --> 00:31:15,888
if they want.

607
00:31:15,888 --> 00:31:19,450
I think the steps would be to read this book.

608
00:31:19,450 --> 00:31:22,671
This book is really good, Reinforcement Learning

609
00:31:22,671 --> 00:31:24,012
by Sutton and Bartow.

610
00:31:24,012 --> 00:31:27,393
And then watch this boot camp, bunch of little videos

611
00:31:27,393 --> 00:31:28,334
on YouTube.

612
00:31:28,334 --> 00:31:29,134
Watch this.

613
00:31:29,134 --> 00:31:31,675
And then if you want to run some code,

614
00:31:31,675 --> 00:31:35,417
start with this spinning up in Deep RL by the OpenAI guys.

615
00:31:36,202 --> 00:31:40,743
That's a good place to start if you want to just try different algorithms,

616
00:31:40,743 --> 00:31:45,505
see which one works on your little environment that you want to try.

617
00:31:45,505 --> 00:31:48,187
So, yeah, so that's what that's the our primary.

618
00:31:48,187 --> 00:31:52,749
And now we can look at our specific environment.

619
00:31:52,749 --> 00:31:53,929
So our initial problem.

620
00:31:55,387 --> 00:31:57,729
So the environment is everything that is not the policy.

621
00:31:57,729 --> 00:32:00,451
There's going to be lots of different pieces in our environment.

622
00:32:00,451 --> 00:32:02,792
But the one-liner is this.

623
00:32:02,792 --> 00:32:05,054
Our environment is a self-powered physical character

624
00:32:05,054 --> 00:32:09,497
that is rewarded when it follows motion matching.

625
00:32:09,497 --> 00:32:11,478
Motion matching is a thing

626
00:32:11,478 --> 00:32:16,581
that was made for a foreigner a couple of years ago.

627
00:32:17,765 --> 00:32:21,207
The main idea is to just, you have a big pile of animation,

628
00:32:21,207 --> 00:32:23,869
big database of animation, and every frame,

629
00:32:23,869 --> 00:32:26,491
we're gonna find the little piece of mocap

630
00:32:26,491 --> 00:32:28,933
that fits your current pose and where you wanna go.

631
00:32:28,933 --> 00:32:31,155
So it's actually not too complicated.

632
00:32:31,155 --> 00:32:35,118
So the input would be just raw mocap.

633
00:32:35,118 --> 00:32:37,480
Some guy like here is a style, very tired.

634
00:32:37,480 --> 00:32:40,042
You see like the kind of detail you wanna get.

635
00:32:40,042 --> 00:32:44,045
So you just have a guy randomly going on the mocap floor

636
00:32:44,045 --> 00:32:45,166
for a couple minutes.

637
00:32:45,595 --> 00:32:47,777
Then you plug that there, clean the game.

638
00:32:47,777 --> 00:32:52,020
And the idea is, yeah, so every couple of frames, maybe,

639
00:32:52,020 --> 00:32:54,162
you choose which little piece of animation to play.

640
00:32:54,162 --> 00:32:56,244
You see all the choices you have there.

641
00:32:56,244 --> 00:32:58,826
And that's what it looks like at runtime.

642
00:32:58,826 --> 00:33:03,350
The red arrow is me controlling the stick in Forerunner.

643
00:33:05,098 --> 00:33:06,438
There are more details on that.

644
00:33:06,438 --> 00:33:08,799
There is another GDC talk for 2016.

645
00:33:08,799 --> 00:33:12,121
You see the blue path here

646
00:33:12,121 --> 00:33:14,522
is the currently playing piece of mocap.

647
00:33:14,522 --> 00:33:17,783
The red path is the desired trajectory from the gameplay.

648
00:33:17,783 --> 00:33:21,364
You see that the blue paths try to match the red path.

649
00:33:21,364 --> 00:33:23,965
You see we jump everywhere in the mocap all the time.

650
00:33:23,965 --> 00:33:26,886
To see the animation switching,

651
00:33:26,886 --> 00:33:30,348
the trick is to disable the animation blending.

652
00:33:30,348 --> 00:33:31,928
If I disable blending, you will see.

653
00:33:33,871 --> 00:33:34,812
It switches all the time.

654
00:33:34,812 --> 00:33:35,732
You see it pops.

655
00:33:35,732 --> 00:33:36,473
That's normal.

656
00:33:36,473 --> 00:33:38,394
It's because we switched animation without blending.

657
00:33:38,394 --> 00:33:41,436
See, we don't even wait until the step is finished.

658
00:33:41,436 --> 00:33:44,098
We just switch when the input switches.

659
00:33:44,098 --> 00:33:46,840
So that's the idea of motion matching.

660
00:33:46,840 --> 00:33:50,422
And now the goal is to follow this with a ragdoll

661
00:33:50,422 --> 00:33:52,483
that's self-powered with motors.

662
00:33:53,817 --> 00:33:55,699
without cheating, without God forces.

663
00:33:55,699 --> 00:33:58,461
The God forces are the forces that people use sometimes

664
00:33:58,461 --> 00:34:03,184
with ragdolls that attaches your bones to like world points.

665
00:34:03,184 --> 00:34:05,086
That's not allowed.

666
00:34:05,086 --> 00:34:09,089
And so yeah, so you have the white guy

667
00:34:09,089 --> 00:34:10,389
is the motion matching guy.

668
00:34:10,389 --> 00:34:13,952
He's randomly playing motion matching

669
00:34:13,952 --> 00:34:16,754
and the blue guy is trying to follow.

670
00:34:16,754 --> 00:34:18,976
You see the reward signal at the top.

671
00:34:20,063 --> 00:34:23,443
So at the beginning of training, he falls all the time.

672
00:34:23,443 --> 00:34:26,344
He falls on his face all the time.

673
00:34:26,344 --> 00:34:30,325
Then after a couple hours of training,

674
00:34:30,325 --> 00:34:31,265
he starts to get better,

675
00:34:31,265 --> 00:34:34,285
tries to follow the white guy a little bit better.

676
00:34:34,285 --> 00:34:36,486
And at the end, you see the white guy interactive animation

677
00:34:36,486 --> 00:34:38,706
that I control with the sticks there

678
00:34:38,706 --> 00:34:42,667
and the physically simulated guy there.

679
00:34:42,667 --> 00:34:45,987
So you see the kind of quality we're trying to get.

680
00:34:45,987 --> 00:34:49,288
Different moves, you can crouch, you can walk backward.

681
00:34:58,455 --> 00:35:01,578
Okay, so that's the problem and the solution.

682
00:35:01,578 --> 00:35:02,799
But now let's look at this.

683
00:35:02,799 --> 00:35:04,080
So this is the big diagram.

684
00:35:04,080 --> 00:35:07,002
Everything that's not the policy is the environment.

685
00:35:07,002 --> 00:35:11,887
There's lots of arrows there.

686
00:35:11,887 --> 00:35:13,609
Let's focus on this part here first.

687
00:35:13,609 --> 00:35:16,431
You see that that's a state that we give to the policy.

688
00:35:16,431 --> 00:35:17,412
See the two arrows there?

689
00:35:18,424 --> 00:35:20,285
On the left, there's the motion matching part

690
00:35:20,285 --> 00:35:23,166
that gives an animation state, like a kinematic state.

691
00:35:23,166 --> 00:35:25,767
And on the right, there's a physics engine

692
00:35:25,767 --> 00:35:28,648
that has a physical state for the physical character.

693
00:35:28,648 --> 00:35:33,590
We give both as a state to our policy.

694
00:35:33,590 --> 00:35:35,190
This is what it looks like.

695
00:35:35,190 --> 00:35:37,291
It's like lots of red lines there.

696
00:35:37,291 --> 00:35:39,992
That's what the policy sees.

697
00:35:39,992 --> 00:35:44,033
It sees the animation, sees the robot.

698
00:35:45,262 --> 00:35:47,344
Sees also the future ground to help it a little bit,

699
00:35:47,344 --> 00:35:49,526
predict what's gonna happen.

700
00:35:49,526 --> 00:35:51,708
You see the white guy is actually doing

701
00:35:51,708 --> 00:35:53,249
the decision of going up

702
00:35:53,249 --> 00:35:55,771
because there's animations of going up and down.

703
00:35:55,771 --> 00:35:58,233
There's inverse kinematics also on the white guy

704
00:35:58,233 --> 00:35:59,495
to help a little bit.

705
00:35:59,495 --> 00:36:02,257
So the blue guy really just has to avoid falling.

706
00:36:06,433 --> 00:36:07,794
And then the action.

707
00:36:07,794 --> 00:36:11,397
The action, in our case, we decided to do the following.

708
00:36:11,397 --> 00:36:16,261
The action will be an offset from the reference animation,

709
00:36:16,261 --> 00:36:17,782
from the white guy.

710
00:36:17,782 --> 00:36:19,883
So you see every frame, we're going

711
00:36:19,883 --> 00:36:24,687
to have little offsets and angles for a subset of joints.

712
00:36:24,687 --> 00:36:27,489
We're going to have this offset to get the green guy.

713
00:36:29,532 --> 00:36:32,173
You see we're going to add it to the animation state.

714
00:36:32,173 --> 00:36:34,694
So the green guy is what we're going

715
00:36:34,694 --> 00:36:37,994
to give the physics engine as a target pose for the ragdoll.

716
00:36:37,994 --> 00:36:40,255
So it's a PD control target for people

717
00:36:40,255 --> 00:36:43,136
that know what proportional derivative control is.

718
00:36:43,136 --> 00:36:46,037
So we're going to give that to the physics engine.

719
00:36:46,037 --> 00:36:49,097
And the physics engine will find the torques that are necessary

720
00:36:49,097 --> 00:36:51,098
to go to this pose.

721
00:36:51,098 --> 00:36:52,558
So this is what it looks like.

722
00:36:53,042 --> 00:36:56,144
See the way the green guy shakes?

723
00:36:56,144 --> 00:36:58,065
That's the balance feedback.

724
00:36:58,065 --> 00:37:00,827
That's deciding, OK, I need to tweak a little bit

725
00:37:00,827 --> 00:37:03,909
where I'm going to put my foot to avoid falling.

726
00:37:03,909 --> 00:37:07,932
So that's what the network learned to do.

727
00:37:15,398 --> 00:37:18,920
And then the last little bit is the reward.

728
00:37:18,920 --> 00:37:21,922
When you compute the reward, just a single number,

729
00:37:21,922 --> 00:37:24,363
we can subtract the physics and animation

730
00:37:24,363 --> 00:37:28,585
and take the inverse of this, like these little distances.

731
00:37:28,585 --> 00:37:30,986
So the reward will be a combination of many things.

732
00:37:30,986 --> 00:37:33,547
Could be like trying to get the correct velocity,

733
00:37:33,547 --> 00:37:34,588
the correct pose.

734
00:37:34,968 --> 00:37:39,693
and avoid falling, so there's going to be multiple little things, not just...

735
00:37:39,693 --> 00:37:45,539
And in the end, it goes to zero if the tracking is bad.

736
00:37:45,539 --> 00:37:49,123
So you can devise different ways to compute the reward,

737
00:37:49,123 --> 00:37:51,765
but basically it boils down to trying to follow the enemy.

738
00:37:54,693 --> 00:37:57,854
So if you're not careful and you forget to follow the style,

739
00:37:57,854 --> 00:38:00,416
you could get this.

740
00:38:00,416 --> 00:38:02,657
This is what happens when you forget

741
00:38:02,657 --> 00:38:05,338
some terms in the rewards, like forget the style.

742
00:38:05,338 --> 00:38:08,320
You can get some weird things.

743
00:38:17,468 --> 00:38:20,550
So yeah, so we train this, multiple guys during the night.

744
00:38:20,550 --> 00:38:24,672
Takes a couple of hours, maybe 10 hours, maybe 40 hours

745
00:38:24,672 --> 00:38:27,674
if you're not lucky and the admissions are too difficult.

746
00:38:27,674 --> 00:38:30,295
See many guys at the same time

747
00:38:30,295 --> 00:38:32,076
training this during the night.

748
00:38:34,093 --> 00:38:40,056
And yeah, so for more details you can check out this paper that we got accepted at SIGGRAPH Asia

749
00:38:40,056 --> 00:38:46,080
last year with Kevin Bergamin, a student at McGill. So read this paper if you want.

750
00:38:46,080 --> 00:38:53,324
And yeah, so let's look at this guy again because I want to go back to like the actual feeling that

751
00:38:53,324 --> 00:39:00,528
you get when you see this guy. So you just met this little character but you already care for him I think.

752
00:39:02,478 --> 00:39:06,884
Because he shares things with you, like you have to deal with gravity in your own life,

753
00:39:06,884 --> 00:39:09,026
he has to deal with gravity.

754
00:39:09,026 --> 00:39:13,492
So it makes you feel sad when you fail.

755
00:39:13,492 --> 00:39:15,415
So maybe we should do something about it.

756
00:39:17,168 --> 00:39:20,111
See, the problem with this is that the animation system

757
00:39:20,111 --> 00:39:22,754
is completely isolated from physics.

758
00:39:22,754 --> 00:39:25,457
So to get the get up, like the fall and get up,

759
00:39:25,457 --> 00:39:27,559
if we have that in our database,

760
00:39:27,559 --> 00:39:28,800
to actually pick up the get up,

761
00:39:28,800 --> 00:39:33,765
we need to feed back the physics inside the kinematic part.

762
00:39:33,765 --> 00:39:37,008
And that's the next part.

763
00:39:37,008 --> 00:39:38,330
But that's just...

764
00:39:39,800 --> 00:39:47,963
subject for another talk maybe but the main idea is sometimes we would like to bootstrap the physics state inside the kinematic state to

765
00:39:47,963 --> 00:39:48,083
find a

766
00:39:48,083 --> 00:39:54,045
pose at the beginning of a getup that matches where the ragdoll is

767
00:39:54,045 --> 00:39:57,906
Presently like the current ragdoll pose your current robot pose

768
00:39:57,906 --> 00:40:05,809
So this is what you saw there. The white guy is going to grab the the physics guy

769
00:40:05,809 --> 00:40:06,629
in the correct pose

770
00:40:07,550 --> 00:40:13,535
And so we get some sort of fall get up behavior.

771
00:40:13,535 --> 00:40:20,260
So yeah, virtual torture of our little guy.

772
00:40:20,260 --> 00:40:21,801
It's beautiful.

773
00:40:21,801 --> 00:40:24,804
Well, that's not the end of it.

774
00:40:24,804 --> 00:40:25,985
There's lots of things to do.

775
00:40:25,985 --> 00:40:27,286
It's just the beginning.

776
00:40:27,286 --> 00:40:28,687
I think it's very exciting.

777
00:40:28,687 --> 00:40:32,850
I want to finish by saying, as I always say, this to me is.

778
00:40:35,152 --> 00:40:40,137
the natural next step for video game animation,

779
00:40:40,137 --> 00:40:44,301
going from animation playback to actual virtual robotics

780
00:40:44,301 --> 00:40:48,205
that we could actually play our video games on real robots.

781
00:40:48,205 --> 00:40:51,908
But that would be dangerous, but yeah, anyway.

782
00:40:51,908 --> 00:40:54,471
So that's what I prepared for you today, thank you.

