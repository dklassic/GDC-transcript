1
00:00:06,382 --> 00:00:09,145
Awesome, well, thank you everyone for making it out here.

2
00:00:09,165 --> 00:00:12,688
It's good to see people, doors are closing.

3
00:00:14,050 --> 00:00:17,693
House cleaning tips, please silence your cell phones.

4
00:00:19,996 --> 00:00:22,198
And then, because I may forget later,

5
00:00:22,418 --> 00:00:24,420
once we get to Q&A, make sure you step up

6
00:00:24,460 --> 00:00:25,201
to the microphones.

7
00:00:28,003 --> 00:00:31,184
So a quick show of hands, if I could get some audience

8
00:00:31,224 --> 00:00:32,024
participation.

9
00:00:32,624 --> 00:00:36,645
How many people here just plain up gather telemetry?

10
00:00:37,085 --> 00:00:38,745
I'm hoping that all the hands go up.

11
00:00:40,326 --> 00:00:41,006
I'm seeing about 70%.

12
00:00:42,406 --> 00:00:43,427
And then lower your hands.

13
00:00:43,587 --> 00:00:43,927
Hold on.

14
00:00:44,847 --> 00:00:47,267
How many have access to reports by end of week?

15
00:00:50,368 --> 00:00:53,289
So how many people are more than a week before you can see

16
00:00:53,309 --> 00:00:54,469
the results of your telemetry?

17
00:00:55,469 --> 00:00:55,729
OK.

18
00:00:56,530 --> 00:00:56,630
So.

19
00:00:57,423 --> 00:01:01,847
It went from like 70% to like 30% and then 0%.

20
00:01:01,887 --> 00:01:04,128
So I'm not sure what that 40% delta there is.

21
00:01:05,249 --> 00:01:05,509
OK.

22
00:01:06,810 --> 00:01:10,273
How many people get access to their information sub-minute

23
00:01:11,233 --> 00:01:12,514
from the telemetry that they're gathering?

24
00:01:12,634 --> 00:01:13,375
That's awesome.

25
00:01:14,015 --> 00:01:15,376
We should definitely talk afterwards.

26
00:01:16,117 --> 00:01:16,957
I want to hear what you're doing.

27
00:01:17,177 --> 00:01:17,558
That's cool.

28
00:01:18,178 --> 00:01:18,919
It's a hard problem.

29
00:01:21,533 --> 00:01:23,274
My name is Tom Matthews, as you can see.

30
00:01:24,234 --> 00:01:28,816
I started in the mid-90s working on line of business applications

31
00:01:29,996 --> 00:01:33,517
for Best Western, Starwood Lodging, Universal Studios.

32
00:01:34,198 --> 00:01:38,659
After about five years there, I moved on to Microsoft,

33
00:01:39,339 --> 00:01:41,900
working on SQL Server Analysis Services,

34
00:01:41,960 --> 00:01:45,181
which is a multi-dimensional database architecture.

35
00:01:46,442 --> 00:01:49,943
Five years into that, I moved on to the Advanced Technology Group.

36
00:01:51,538 --> 00:01:58,162
developers improve their audio implementations. Five years into that, I moved on to 343 Studios

37
00:01:58,182 --> 00:01:58,462
to help them.

38
00:02:00,518 --> 00:02:06,783
Maintain the existing Halo architecture for gathering telemetry,

39
00:02:07,804 --> 00:02:11,427
and architect, what you guys are here to see,

40
00:02:12,007 --> 00:02:16,951
the subsequent iteration of telemetry gathering for Halo 5.

41
00:02:17,812 --> 00:02:21,855
So, if you've noticed a pattern, every five years I seem to do something different,

42
00:02:22,476 --> 00:02:28,841
and that was about four years ago, so I'm no longer a software senior, software engineer engineer.

43
00:02:30,934 --> 00:02:34,736
I am now going to be a senior data and applied scientist.

44
00:02:34,876 --> 00:02:37,138
So make sure you fill out the surveys.

45
00:02:37,218 --> 00:02:39,659
I want to come back next year to talk about all the cool

46
00:02:40,099 --> 00:02:41,540
investigations we've been doing.

47
00:02:43,322 --> 00:02:44,682
But enough about me.

48
00:02:44,702 --> 00:02:51,267
We'll start out with a bit of interesting stuff, some of the

49
00:02:51,567 --> 00:02:52,467
numbers of our launch.

50
00:02:52,507 --> 00:02:55,249
And then we'll go into how we actually achieved it.

51
00:02:56,357 --> 00:03:00,399
So, events per second, seems like not a lot of people talk about it.

52
00:03:01,179 --> 00:03:03,440
It was hard for me to find publication numbers.

53
00:03:04,020 --> 00:03:12,743
I found one large casual app that was saying that they hit 2 million events on their launch

54
00:03:12,803 --> 00:03:14,564
day when they launched their big popular app.

55
00:03:16,464 --> 00:03:18,485
And when we started working on...

56
00:03:19,419 --> 00:03:21,620
what kind of architecture we would use and land on,

57
00:03:21,661 --> 00:03:23,161
what kind of technologies we would use.

58
00:03:23,181 --> 00:03:25,683
That was about four years ago, three and a half years ago.

59
00:03:26,544 --> 00:03:27,704
And I was putting this talk together,

60
00:03:27,724 --> 00:03:29,746
I was like, oh, did we really hit big data scale?

61
00:03:29,766 --> 00:03:33,048
I mean, it seems like we did from three years ago,

62
00:03:33,128 --> 00:03:35,429
but times change rather rapidly.

63
00:03:35,569 --> 00:03:38,751
So for our launch day, what we were able to do

64
00:03:38,771 --> 00:03:40,152
is 700,000 events per second,

65
00:03:40,172 --> 00:03:40,752
which is 2.5 million.

66
00:03:45,994 --> 00:03:50,880
hour. So that's a pretty significant amount of data

67
00:03:51,581 --> 00:03:56,946
churning through the systems. And our peak volumes were

68
00:03:56,987 --> 00:04:04,675
831,000 events per second. The funny story there is that

69
00:04:05,516 --> 00:04:07,158
half of that volume was actually

70
00:04:08,244 --> 00:04:14,446
One particular event a developer had left turned on for release, which was a composer

71
00:04:14,506 --> 00:04:20,748
event that was firing for every object in every frame and resulted in 400,000 events

72
00:04:20,768 --> 00:04:25,729
per second for the first few hours until we finally got around to turning that off.

73
00:04:25,810 --> 00:04:31,531
So first finding is make sure you're aware of the data that your game is transmitting

74
00:04:31,551 --> 00:04:33,412
in the release configuration.

75
00:04:35,360 --> 00:04:38,681
And then the next highest event is down at 90,000.

76
00:04:39,721 --> 00:04:43,723
So it's a pretty exponential tail that we have going.

77
00:04:44,103 --> 00:04:51,246
So overview of the general slide deck presentation that

78
00:04:51,306 --> 00:04:55,048
we are going to be presenting today is where we started,

79
00:04:55,088 --> 00:04:58,829
like what the Halo 4 and earlier code base looked like,

80
00:05:00,150 --> 00:05:03,351
what goals we wanted as we looked into overhauling that

81
00:05:03,691 --> 00:05:04,671
and creating something new.

82
00:05:05,809 --> 00:05:09,231
The implementation, naturally, and the results.

83
00:05:09,431 --> 00:05:11,953
There'll be some findings at the end.

84
00:05:18,277 --> 00:05:20,418
So, the existing...

85
00:05:22,130 --> 00:05:25,311
At 343 we inherited the Halo codebase from Bungie.

86
00:05:26,111 --> 00:05:29,692
So this was the existing implementation of logging.

87
00:05:29,952 --> 00:05:31,812
They had two different logging systems.

88
00:05:32,493 --> 00:05:35,113
One was this log underscore event macro.

89
00:05:36,094 --> 00:05:40,135
And this macro was very easy for developers to use.

90
00:05:40,175 --> 00:05:41,315
It was like a printf statement.

91
00:05:41,895 --> 00:05:46,877
You would key in the string that you wanted printed out

92
00:05:47,197 --> 00:05:51,298
over Telnet or to the screen or to log files.

93
00:05:51,965 --> 00:05:57,010
and the parameters that would go into that system.

94
00:05:57,050 --> 00:06:01,834
And then it would feed into a database architecture

95
00:06:01,894 --> 00:06:03,035
called data mine.

96
00:06:05,197 --> 00:06:07,780
The good thing is that it was easy to use, right?

97
00:06:07,820 --> 00:06:08,961
So developers could just write

98
00:06:09,501 --> 00:06:10,863
using this macro printf statement.

99
00:06:11,884 --> 00:06:14,626
The bad thing is that there was not really strong typing.

100
00:06:14,746 --> 00:06:16,107
They could pass anything in there

101
00:06:16,148 --> 00:06:17,569
and printf would do the best.

102
00:06:18,800 --> 00:06:23,043
that it could to create a string that was fairly legible.

103
00:06:24,424 --> 00:06:27,786
And there wasn't enforced parameter naming.

104
00:06:27,886 --> 00:06:29,647
So there were several times where

105
00:06:31,208 --> 00:06:33,569
developers would swap the events,

106
00:06:34,810 --> 00:06:38,072
seek the order of the parameters in the event,

107
00:06:39,273 --> 00:06:43,155
and so the meanings of the reports that were coming out

108
00:06:43,396 --> 00:06:44,616
would change, right, and there was no...

109
00:06:45,114 --> 00:06:47,236
warnings that something was going on different.

110
00:06:48,177 --> 00:06:51,900
Other times they would change the data type itself

111
00:06:52,280 --> 00:06:54,442
to be more or less precise.

112
00:06:54,803 --> 00:06:58,106
So they would go from like a map coordinate system

113
00:06:58,126 --> 00:07:01,249
that's in world units to a normalized map coordinate

114
00:07:01,349 --> 00:07:03,671
x and y that was minus one to positive one.

115
00:07:04,272 --> 00:07:07,515
And naturally that would mess up reports that were based on that.

116
00:07:09,357 --> 00:07:13,638
And the worst part about it is that

117
00:07:14,638 --> 00:07:16,459
what it did was it actually zipped up

118
00:07:16,579 --> 00:07:18,740
the results of all the telemetry generated

119
00:07:18,780 --> 00:07:20,821
during that particular match

120
00:07:21,561 --> 00:07:25,002
and upload that zip file at the end of the match.

121
00:07:25,642 --> 00:07:26,903
So it was incredibly slow.

122
00:07:26,983 --> 00:07:30,084
During internal Microsoft-wide betas,

123
00:07:31,024 --> 00:07:33,505
you're looking at up to several minutes

124
00:07:33,585 --> 00:07:35,466
to upload the telemetry that was generated.

125
00:07:35,486 --> 00:07:36,826
And players are just sitting there

126
00:07:37,046 --> 00:07:38,366
waiting for the next match to start.

127
00:07:39,980 --> 00:07:42,040
So naturally, this was compiled out of release.

128
00:07:42,440 --> 00:07:48,522
So there were on the order of 9,000 events that were using

129
00:07:48,542 --> 00:07:51,603
this log underscore message or log underscore critical

130
00:07:51,723 --> 00:07:56,844
format, but you didn't get any of those event information

131
00:07:56,904 --> 00:07:57,724
from the release build.

132
00:08:00,665 --> 00:08:02,645
What they did have in the release build and in the

133
00:08:02,665 --> 00:08:05,106
development builds was the binary logging.

134
00:08:06,107 --> 00:08:10,710
And so this is the binary log format, BLF files.

135
00:08:11,711 --> 00:08:14,593
And what they pretty much were was,

136
00:08:16,574 --> 00:08:17,695
they were good from the perspective

137
00:08:17,735 --> 00:08:19,556
that they were more compressed.

138
00:08:19,997 --> 00:08:21,498
The devs found it very easy to use.

139
00:08:22,758 --> 00:08:25,320
What it was was basically just a mem copy of the struct.

140
00:08:26,081 --> 00:08:28,062
So they would make sure that it wouldn't be doing

141
00:08:28,102 --> 00:08:29,303
any reordering or padding,

142
00:08:29,923 --> 00:08:31,584
and then they would mem copy that struct off

143
00:08:32,065 --> 00:08:33,626
and send it on the wire.

144
00:08:35,324 --> 00:08:41,007
The downside is that this is really, really frustrating to use from a services perspective.

145
00:08:41,547 --> 00:08:44,669
Because as soon as a developer adds one field in the middle of their struct,

146
00:08:45,409 --> 00:08:49,871
now the C-sharp code that's trying to read these two bytes and interpret it this way,

147
00:08:49,911 --> 00:08:52,072
and read the next three bytes and interpret it this way,

148
00:08:53,712 --> 00:08:55,233
That starts giving you garbage results.

149
00:08:55,453 --> 00:08:58,854
And hopefully you have the testing that identifies

150
00:08:58,894 --> 00:09:00,274
that bogus values are coming through.

151
00:09:00,314 --> 00:09:03,295
And hopefully the change that the developer did

152
00:09:03,796 --> 00:09:06,376
and the client results in bogus values

153
00:09:06,416 --> 00:09:07,837
that would trigger that alerting.

154
00:09:08,117 --> 00:09:08,317
So.

155
00:09:11,578 --> 00:09:13,879
The big pain from the services side,

156
00:09:14,179 --> 00:09:16,840
besides the fact that stuff could change on the fly,

157
00:09:17,420 --> 00:09:19,640
is that it requires the source code to understand.

158
00:09:22,313 --> 00:09:27,274
Depending on the level of commenting going on in your source code, that could be easy

159
00:09:27,494 --> 00:09:33,136
or very difficult for a services engineer to understand the intent of a particular field.

160
00:09:35,137 --> 00:09:41,559
And similar to the previous logging system, this is transmitting at the end of the match.

161
00:09:42,119 --> 00:09:47,660
So when the match is over, the game takes all of the statistics that it's gathered and

162
00:09:48,241 --> 00:09:51,402
aggregated and uploads it at the end of the match.

163
00:10:02,813 --> 00:10:06,576
So when looking at where we wanted to move to,

164
00:10:07,977 --> 00:10:11,379
one of the big key things was thinking about

165
00:10:12,480 --> 00:10:15,462
those easy to use printf style statements

166
00:10:16,383 --> 00:10:17,544
in the first logging system.

167
00:10:19,285 --> 00:10:24,449
And trying to come up with a way that we could use that

168
00:10:25,810 --> 00:10:28,252
in release if those events were turned on,

169
00:10:28,272 --> 00:10:29,613
so having a configurable system.

170
00:10:30,944 --> 00:10:34,386
That would generate telemetry in a performant way.

171
00:10:36,148 --> 00:10:38,249
But the real goal of this is to

172
00:10:40,391 --> 00:10:42,693
what I was calling in the slide

173
00:10:43,073 --> 00:10:46,656
summary, gaming intelligence. So we've all heard of

174
00:10:46,696 --> 00:10:50,800
business intelligence, which is the information that the

175
00:10:50,880 --> 00:10:54,563
business guys want, right, you know, return on investment and ARPU and

176
00:10:56,344 --> 00:10:58,486
churn numbers and that kind of thing. So they're looking at, you know,

177
00:10:59,303 --> 00:11:01,384
new unique users coming in, that kind of thing.

178
00:11:01,964 --> 00:11:04,165
But what we really wanted to do is make sure

179
00:11:04,205 --> 00:11:06,365
that developers had available to them

180
00:11:07,206 --> 00:11:11,767
the data that was most valuable for ensuring

181
00:11:11,828 --> 00:11:13,408
that the systems that they were writing

182
00:11:14,068 --> 00:11:15,849
continue to operate as they expected.

183
00:11:19,330 --> 00:11:20,571
So, towards that end,

184
00:11:22,531 --> 00:11:25,993
we were going to focus on reliably transmitting telemetry

185
00:11:26,773 --> 00:11:27,493
in real time.

186
00:11:28,842 --> 00:11:31,126
enabling sub-second service response.

187
00:11:31,326 --> 00:11:33,409
So one of the earlier requirements for this

188
00:11:33,529 --> 00:11:36,032
was having the player do something

189
00:11:36,933 --> 00:11:39,156
and within one second having,

190
00:11:40,218 --> 00:11:41,640
from the user's perspective,

191
00:11:41,680 --> 00:11:43,763
from the instant that they started an action.

192
00:11:44,677 --> 00:11:52,481
having services perform a calculation and then having that reflected in the title and

193
00:11:52,501 --> 00:11:53,381
the user's perception.

194
00:11:54,021 --> 00:11:57,263
So you can think of it as medals, essentially.

195
00:11:57,463 --> 00:12:00,925
You get three kills in a row within a certain time span.

196
00:12:01,345 --> 00:12:06,167
We want the services to be able to be tracking that and then show the medal within one second

197
00:12:06,187 --> 00:12:07,008
in a timely fashion.

198
00:12:10,948 --> 00:12:17,915
So, what we have is the rough budgets that we sketched out on the back of a napkin, essentially.

199
00:12:18,295 --> 00:12:27,365
When we first started out, it was that for the highest priority events, we would want a message pump that was firing every frame.

200
00:12:29,107 --> 00:12:30,008
Primarily because...

201
00:12:31,175 --> 00:12:33,576
We're supporting up to 400 milliseconds latency.

202
00:12:34,216 --> 00:12:37,458
So if our goal is to be responsive in a one second time

203
00:12:37,518 --> 00:12:41,640
frame, if that's our SLA, 400 milliseconds up and 400

204
00:12:41,680 --> 00:12:43,801
milliseconds down, that's gone right away.

205
00:12:44,021 --> 00:12:47,162
So it leaves us 200 milliseconds in client side

206
00:12:47,322 --> 00:12:49,704
and in services side to do our operations.

207
00:12:50,884 --> 00:12:53,145
So we want to transmit as fast as possible.

208
00:12:53,325 --> 00:12:56,127
If we're waiting even two frames or three frames, that's

209
00:12:56,267 --> 00:12:59,608
eating up a good 20%, 30% of the budget that we have.

210
00:13:02,434 --> 00:13:05,816
Once it's in the cloud, there's a queuing and dequeuing stage

211
00:13:06,076 --> 00:13:11,379
that has to happen in order to support horizontal scalability.

212
00:13:12,100 --> 00:13:14,481
So that, we're budgeting at 50 milliseconds.

213
00:13:14,541 --> 00:13:17,803
Fortunately, the queuing system we're using

214
00:13:18,364 --> 00:13:19,384
comes in much under that.

215
00:13:21,546 --> 00:13:25,048
And then service code, it basically

216
00:13:25,068 --> 00:13:27,370
leaves at 70 milliseconds with a tiny bit of headroom.

217
00:13:28,151 --> 00:13:30,152
And then you've got 50 milliseconds to throw back

218
00:13:30,212 --> 00:13:32,814
on the queue, and 400 milliseconds

219
00:13:33,235 --> 00:13:34,475
transmission latency again.

220
00:13:44,743 --> 00:13:47,565
So, for the purposes of this talk,

221
00:13:48,845 --> 00:13:53,269
the component that I'll be referring to as cell

222
00:13:54,049 --> 00:13:56,651
is the client-side component.

223
00:13:57,667 --> 00:14:00,988
And the component that I'll be referring to as Maelstrom

224
00:14:01,948 --> 00:14:05,089
is that, those bottom three boxes.

225
00:14:07,510 --> 00:14:08,430
When we were going through this,

226
00:14:08,470 --> 00:14:11,811
there was Storm and Spark.

227
00:14:12,871 --> 00:14:14,392
You know, they were like research projects

228
00:14:14,412 --> 00:14:15,792
because this was three and a half years ago,

229
00:14:15,832 --> 00:14:17,953
and you're not sure which one is actually going to

230
00:14:18,353 --> 00:14:19,853
become adopted by the community,

231
00:14:20,273 --> 00:14:22,874
which ones are gonna stay university projects.

232
00:14:23,574 --> 00:14:24,555
But they all were having...

233
00:14:25,816 --> 00:14:27,917
meteorological themed names.

234
00:14:28,157 --> 00:14:30,758
So that's one reason why we settled on these.

235
00:14:34,980 --> 00:14:45,465
So in the logging architecture that we're creating, there's three primary requirements

236
00:14:45,505 --> 00:14:45,985
that we have.

237
00:14:47,285 --> 00:14:49,586
The first one is sequentiality.

238
00:14:51,267 --> 00:14:55,509
The reason why this is important is because in a lot of the real-time

239
00:14:56,340 --> 00:15:01,781
modeling, processing systems that are out there, you'll actually find that they require

240
00:15:01,821 --> 00:15:07,723
you to have a window of operations. So, a lot of these systems, what they do is you'll

241
00:15:07,763 --> 00:15:15,165
collect for five minutes, and then what you'll do is say, okay, as of this point in time,

242
00:15:15,665 --> 00:15:20,487
I'm going to sort all of the events that I've received. I'll take the oldest two and a half

243
00:15:20,567 --> 00:15:24,308
minutes and say that anything that comes in, I'm going to sort it. And then I'm going to

244
00:15:25,152 --> 00:15:26,555
more than two and a half minutes ago

245
00:15:27,638 --> 00:15:28,700
is now going to be thrown out

246
00:15:28,820 --> 00:15:30,344
because I've waited two and a half minutes.

247
00:15:30,484 --> 00:15:33,511
That's the latencies that I'm willing to work with.

248
00:15:34,266 --> 00:15:38,187
And I know that anything older than two and a half minutes is bogus.

249
00:15:38,507 --> 00:15:41,308
So I'll work on the worldview that I have

250
00:15:41,848 --> 00:15:45,729
with the events two and a half minutes out to five minutes in the past.

251
00:15:46,529 --> 00:15:49,070
And then it'll do its aggregation and calculations,

252
00:15:49,330 --> 00:15:52,411
and then two and a half minutes from now it'll do the same thing.

253
00:15:52,451 --> 00:15:55,531
It'll sort the last five minutes, take the oldest two and a half minutes,

254
00:15:56,072 --> 00:15:57,032
and process that again.

255
00:15:58,212 --> 00:16:00,633
What Sell and Maelstrom

256
00:16:02,064 --> 00:16:07,970
implemented, what we implemented with Sel and Mellstrom, is the concept of for high

257
00:16:08,010 --> 00:16:14,095
priority events, we guarantee at least once sequentiality.

258
00:16:15,056 --> 00:16:17,478
So it's kind of like the TCP style.

259
00:16:17,999 --> 00:16:25,005
We will have acknowledgments if the host is not, if the cloud service is not responding

260
00:16:25,045 --> 00:16:26,566
with an ACK in a timely fashion.

261
00:16:27,310 --> 00:16:33,980
We will disconnect from that service and then try connecting again, which hopefully with

262
00:16:34,020 --> 00:16:40,670
the load balancer will give us a host that, or I should say a cloud service entry point

263
00:16:41,271 --> 00:16:42,834
that is in a healthier state.

264
00:16:44,409 --> 00:16:48,992
and retransmit the events that we had been expecting to get axed.

265
00:16:49,172 --> 00:16:54,256
So you'll get repeats, but from a consumer perspective that's reading this event stream,

266
00:16:55,517 --> 00:17:00,000
if they see an event they haven't seen before, they can act on it immediately.

267
00:17:00,400 --> 00:17:05,744
There's no ordering that they have to do, and that really improves on the latencies

268
00:17:06,644 --> 00:17:08,866
that we have in the service itself.

269
00:17:11,145 --> 00:17:14,886
We also have a low priority stream, and these two concepts

270
00:17:14,946 --> 00:17:17,547
are very decoupled, high priority streams and low

271
00:17:17,567 --> 00:17:21,088
priority streams, because we don't want something like a

272
00:17:22,749 --> 00:17:26,910
super spammy event that should be off to affect the high

273
00:17:26,930 --> 00:17:27,650
priority stream.

274
00:17:27,750 --> 00:17:31,552
So anything that's user facing is going to go on that high

275
00:17:31,692 --> 00:17:34,933
priority stream, deaths, kills, which are the same

276
00:17:34,973 --> 00:17:38,394
thing, but spawn locations, that kind of thing, those are

277
00:17:38,434 --> 00:17:38,954
high priority.

278
00:17:42,993 --> 00:17:45,734
Oh, one thing to note is that the low priority stream,

279
00:17:46,294 --> 00:17:47,635
that's more of a UDP style.

280
00:17:47,915 --> 00:17:50,115
So it's an at most once transmission

281
00:17:50,295 --> 00:17:52,736
because there's no acts on the low priority stream

282
00:17:54,197 --> 00:17:56,918
because having to maintain that much state

283
00:17:57,298 --> 00:18:00,259
on such a high volume would be a bit catastrophic.

284
00:18:03,680 --> 00:18:06,261
Contextual events is also important.

285
00:18:10,814 --> 00:18:19,502
Events that we transmit are very small, and because we're working with the concept of

286
00:18:19,802 --> 00:18:25,527
a streaming architecture, where we know the events are sequential and we are subscribing

287
00:18:25,567 --> 00:18:27,729
to the events from the beginning of the stream.

288
00:18:29,977 --> 00:18:32,038
The consumers, and they're guaranteed,

289
00:18:32,458 --> 00:18:36,080
the consumers can primarily be paying attention

290
00:18:36,200 --> 00:18:38,100
to the client session.

291
00:18:38,781 --> 00:18:43,362
So when the client spins up and connects to the cloud service

292
00:18:43,402 --> 00:18:47,864
for the very first time, it will be creating

293
00:18:47,904 --> 00:18:50,705
its own unique GUID that's associating itself

294
00:18:51,465 --> 00:18:53,146
for the lifetime of that executable.

295
00:18:56,798 --> 00:19:03,105
With that, one piece of contextualization, that's associated with the stream.

296
00:19:03,485 --> 00:19:07,249
So each individual event doesn't get that GUID attached to it.

297
00:19:10,052 --> 00:19:12,355
And I've got numbers later in.

298
00:19:13,798 --> 00:19:18,861
The reason why that's important is because adding even one GUID to an event would increase

299
00:19:18,921 --> 00:19:21,962
our event sizes by something like 20 to 30 percent.

300
00:19:22,703 --> 00:19:28,025
So our events are so small and compact that adding a lot of contextualization information

301
00:19:28,485 --> 00:19:32,307
would be detrimental to the point of having this very fast system.

302
00:19:33,868 --> 00:19:39,151
So the consumers, it does shift some of the

303
00:19:42,406 --> 00:19:45,308
processing work onto the consumers of the streams,

304
00:19:45,408 --> 00:19:48,671
because they have to monitor for a match start event

305
00:19:48,991 --> 00:19:51,493
and say, okay, from now until I get a match end event,

306
00:19:52,274 --> 00:19:54,496
the events that I'm getting are actually associated

307
00:19:54,536 --> 00:19:55,277
with this match.

308
00:19:56,598 --> 00:19:59,000
And if they wanted to do contextualization

309
00:19:59,260 --> 00:20:02,282
on the duration in which a person was alive,

310
00:20:02,803 --> 00:20:05,645
they would also have to track spawn to death events,

311
00:20:05,685 --> 00:20:06,386
that kind of thing.

312
00:20:07,066 --> 00:20:11,049
But if you work with contextualization from

313
00:20:11,963 --> 00:20:18,528
the beginning, that means that you're thinking in terms of contextualization.

314
00:20:18,588 --> 00:20:27,154
So it is a tax on the services side, but once they start adopting that mindset, it becomes

315
00:20:27,214 --> 00:20:31,437
easier to be creating these kind of contextualization checkpoints.

316
00:20:32,278 --> 00:20:33,699
And some windowed processing...

317
00:20:35,581 --> 00:20:43,289
Architectures like Trill make it easier to identify new boundaries by creating new events

318
00:20:43,329 --> 00:20:44,511
that are injected into the stream.

319
00:20:59,952 --> 00:21:01,492
There's compatibility.

320
00:21:01,692 --> 00:21:05,454
So I alluded to this earlier with the other two systems,

321
00:21:05,534 --> 00:21:08,655
where developers making changes to the events

322
00:21:09,236 --> 00:21:11,316
can have very deleterious effects

323
00:21:11,577 --> 00:21:12,877
to the services downstream.

324
00:21:14,118 --> 00:21:15,858
So the architecture we wanted to implement

325
00:21:16,659 --> 00:21:19,700
from the beginning, the concept of having

326
00:21:21,761 --> 00:21:25,182
events that were both forwards and backwards compatible,

327
00:21:26,363 --> 00:21:26,883
meaning that.

328
00:21:29,166 --> 00:21:33,197
As you create your events, the events will be...

329
00:21:34,701 --> 00:21:36,466
You can continue to add new fields to them.

330
00:21:37,992 --> 00:21:42,895
But you can't change the types of the fields that you've

331
00:21:42,975 --> 00:21:44,236
already transmitted.

332
00:21:44,676 --> 00:21:48,238
And this allows us to make our reporting systems such that

333
00:21:48,818 --> 00:21:51,059
the reports that we generate will continue to work.

334
00:21:51,439 --> 00:21:53,661
I mean, there may be nulls there because people stopped

335
00:21:53,781 --> 00:21:55,502
transmitting old fields kind of thing.

336
00:21:55,922 --> 00:21:56,942
We still have to handle that.

337
00:21:57,443 --> 00:22:01,165
But you don't have bogus data creeping into your code paths.

338
00:22:05,187 --> 00:22:06,047
So, Cell.

339
00:22:06,292 --> 00:22:09,416
CEL stands for Common Event Logging Library.

340
00:22:10,878 --> 00:22:12,800
Within the CEL architecture on the client,

341
00:22:14,042 --> 00:22:17,627
we have a component called the telemetry manager,

342
00:22:18,688 --> 00:22:21,853
and you can have a number of these telemetry managers.

343
00:22:24,327 --> 00:22:27,650
In Halo 5 we have a high priority telemetry manager

344
00:22:27,690 --> 00:22:29,471
and a low priority telemetry manager,

345
00:22:29,491 --> 00:22:33,054
but you could create one for different services.

346
00:22:33,434 --> 00:22:36,437
For example, some studios that are adopting the technology,

347
00:22:37,538 --> 00:22:40,861
they have one manager that's for their own personal services

348
00:22:40,901 --> 00:22:42,642
and they have another manager that's for the...

349
00:22:45,908 --> 00:22:50,356
I forget what the latest name is, but the event tracing for Xbox or

350
00:22:50,376 --> 00:22:55,144
ETW style eventing that's on the Microsoft platform.

351
00:22:57,449 --> 00:22:59,690
So, there's a telemetry manager.

352
00:23:00,310 --> 00:23:02,990
Developers interact with this using a macro

353
00:23:03,670 --> 00:23:07,331
that's been optimized down so that the client-side impact

354
00:23:07,651 --> 00:23:10,172
is 15 to 30 microseconds.

355
00:23:10,852 --> 00:23:15,173
And I would get calls if that ever crept up

356
00:23:15,193 --> 00:23:16,193
to 50 microseconds.

357
00:23:16,673 --> 00:23:20,654
So, we spent a lot of work optimizing that macro.

358
00:23:21,414 --> 00:23:26,119
And what it's doing is it's pretty much just,

359
00:23:26,639 --> 00:23:29,842
it has two very small branches to determine

360
00:23:30,042 --> 00:23:33,065
if the overall category of event has been turned on

361
00:23:33,685 --> 00:23:35,427
and the overall priority threshold

362
00:23:35,467 --> 00:23:36,467
of the event has turned on.

363
00:23:37,128 --> 00:23:40,651
So in release, we can set a priority threshold

364
00:23:40,951 --> 00:23:45,555
and we can turn on or off the categories

365
00:23:46,816 --> 00:23:47,557
that will be emitted.

366
00:23:47,777 --> 00:23:48,858
And that's the kind of like,

367
00:23:50,160 --> 00:23:55,045
Nuclear option, that's the most effective

368
00:23:55,185 --> 00:23:58,127
and broadest scoped configuration option

369
00:23:58,167 --> 00:23:59,549
in our, on the client side.

370
00:24:01,510 --> 00:24:02,631
And it's also the fastest.

371
00:24:02,711 --> 00:24:04,533
There are other filters that you can apply,

372
00:24:04,593 --> 00:24:07,195
but those have to be processed on every event

373
00:24:07,496 --> 00:24:08,196
as it comes through.

374
00:24:08,236 --> 00:24:11,079
You have to crack open the event on the client

375
00:24:11,159 --> 00:24:12,640
and that consumes more time.

376
00:24:14,663 --> 00:24:17,904
So that macro is essentially mem copying

377
00:24:18,104 --> 00:24:21,486
onto a circular lockless buffer.

378
00:24:22,826 --> 00:24:25,608
And I put quotes there because it was originally

379
00:24:25,668 --> 00:24:27,268
implemented in a lockless fashion,

380
00:24:27,649 --> 00:24:30,810
and it turns out that lockless programming

381
00:24:30,830 --> 00:24:33,391
is as hard as all of the books say it is.

382
00:24:34,892 --> 00:24:36,092
You'd think interlocked increment,

383
00:24:36,112 --> 00:24:37,613
you know, what can go wrong with that?

384
00:24:37,653 --> 00:24:42,335
But you have to do a lot of fencing and thread marshaling,

385
00:24:42,915 --> 00:24:43,036
and...

386
00:24:46,041 --> 00:24:49,122
So, we addressed all those issues, we're pretty sure.

387
00:24:50,082 --> 00:24:53,304
But for the preview, they didn't want to take the risk, so we threw some locks around it.

388
00:24:54,244 --> 00:25:00,946
And that 15 to 30 microsecond number up there is actually using those locks.

389
00:25:00,986 --> 00:25:02,567
So we didn't take the locks out, because...

390
00:25:03,516 --> 00:25:06,818
We didn't have a performance requirement to remove them,

391
00:25:07,038 --> 00:25:10,761
and I was keeping it safe, so we actually left that in.

392
00:25:10,901 --> 00:25:13,382
I think we could get a bit faster performance,

393
00:25:14,923 --> 00:25:16,784
but at these speeds, you're pretty much waiting

394
00:25:16,844 --> 00:25:20,447
for cross-core synchronization issues, right,

395
00:25:20,487 --> 00:25:22,028
which you have whether you're dealing

396
00:25:22,068 --> 00:25:23,569
with interlocked increments,

397
00:25:23,889 --> 00:25:28,192
or if you're working with fast lock semantics.

398
00:25:29,713 --> 00:25:31,954
So, might shave a few microseconds off.

399
00:25:35,386 --> 00:25:36,627
Within the Telemetry Manager,

400
00:25:37,167 --> 00:25:38,667
there's a concept of an endpoint,

401
00:25:38,987 --> 00:25:41,468
and there's a collection of endpoints that you can create.

402
00:25:42,268 --> 00:25:45,569
We've got network endpoints and telnet endpoints

403
00:25:46,549 --> 00:25:49,510
and screen endpoints that display on the screen

404
00:25:49,550 --> 00:25:50,670
if it's critical or higher.

405
00:25:51,070 --> 00:25:53,491
And that's where I was saying each endpoint can,

406
00:25:53,691 --> 00:25:56,931
as it pulls an event from the circular buffer,

407
00:25:57,992 --> 00:25:59,812
and it's working directly with the copy

408
00:25:59,872 --> 00:26:01,092
that's on the circular buffer.

409
00:26:01,372 --> 00:26:03,253
So it's not getting a copy.

410
00:26:04,323 --> 00:26:05,644
and incurring that overhead.

411
00:26:06,424 --> 00:26:08,686
So theoretically, if you have a slow endpoint,

412
00:26:09,367 --> 00:26:10,628
that could become a bottleneck

413
00:26:11,188 --> 00:26:14,310
and halt the telemetry gathering system.

414
00:26:15,311 --> 00:26:19,735
But we have an awful lot of testing in the studio

415
00:26:19,775 --> 00:26:21,356
to make sure that that's not going to happen.

416
00:26:22,216 --> 00:26:25,799
In development time, we run with probably,

417
00:26:25,819 --> 00:26:27,400
I wanna say, 10 times the volume,

418
00:26:27,621 --> 00:26:28,681
10 to 100 times the volume.

419
00:26:31,189 --> 00:26:33,870
as the retail configuration.

420
00:26:34,351 --> 00:26:36,312
So we're putting a lot of stress

421
00:26:36,352 --> 00:26:37,913
on that circular buffer already.

422
00:26:52,482 --> 00:26:57,526
So the, one thing to note, if I go back one slide,

423
00:26:58,286 --> 00:27:00,528
the message prompt.

424
00:27:02,191 --> 00:27:05,274
So we don't have a Telnet endpoint turned on and released.

425
00:27:05,535 --> 00:27:07,877
The main endpoint that we have is the network endpoint

426
00:27:08,538 --> 00:27:14,585
and an endpoint that is integrating with the Microsoft Achievement System and Present System.

427
00:27:16,267 --> 00:27:21,193
And our budget that we have is 1 millisecond.

428
00:27:22,464 --> 00:27:28,366
per frame on a single core. So we've decoupled the system in that the

429
00:27:29,006 --> 00:27:33,448
C++ macro is very very fast in all of the different client subsystems that

430
00:27:34,048 --> 00:27:39,810
we're interacting with and then we have a worker thread on a low priority core that's waking up

431
00:27:41,331 --> 00:27:46,152
every frame to pull off those events that have accumulated in the circular buffer.

432
00:27:47,760 --> 00:27:51,345
And with that, our targets were 2 to 4,000 events per second

433
00:27:52,127 --> 00:27:53,929
before the circular buffer would fill up,

434
00:27:54,330 --> 00:27:58,876
and keeping to the one millisecond per frame CPU guideline.

435
00:28:00,218 --> 00:28:03,262
And we were able to hit 3,000 events per second.

436
00:28:03,402 --> 00:28:04,624
So it worked out pretty well.

437
00:28:08,334 --> 00:28:13,157
So to enable this, we have a build time preprocessor.

438
00:28:13,617 --> 00:28:17,179
So the macro covers that ease of use bullet point

439
00:28:17,659 --> 00:28:19,661
that was important to us at the beginning.

440
00:28:20,501 --> 00:28:23,163
And it looks a lot like the log underscore event macro.

441
00:28:24,123 --> 00:28:26,344
The preprocessor, what it does,

442
00:28:27,725 --> 00:28:31,687
is create a global schema store.

443
00:28:32,188 --> 00:28:35,370
So at compile time on a developer's box,

444
00:28:36,948 --> 00:28:39,851
The preprocessor will run as part of the build step,

445
00:28:39,891 --> 00:28:40,772
it's a pre-build step.

446
00:28:41,652 --> 00:28:44,515
It'll parse through writable code as an optimization,

447
00:28:46,237 --> 00:28:48,419
assuming that those are the checked out files

448
00:28:48,459 --> 00:28:49,300
they may have edited,

449
00:28:50,121 --> 00:28:54,405
and identify if there have been new event parameters created

450
00:28:54,425 --> 00:28:55,486
or if there have been new events

451
00:28:56,107 --> 00:28:58,289
that haven't been registered in the global database.

452
00:28:59,990 --> 00:29:06,474
This is configurable. Right now we're using TFS because it allows us to do ADF security

453
00:29:06,514 --> 00:29:12,477
credential stuff and we can share with studios outside of the Microsoft internal corpnet

454
00:29:13,078 --> 00:29:19,101
and they can still access the externally public TFS address and all that. So the important

455
00:29:19,141 --> 00:29:21,682
thing here is that events are...

456
00:29:22,557 --> 00:29:25,738
The Schema Store stores events at the category level.

457
00:29:26,138 --> 00:29:27,599
So if you have two networking engineers

458
00:29:27,659 --> 00:29:28,520
that are adding events,

459
00:29:29,500 --> 00:29:32,401
you need to create a unique identifier

460
00:29:32,461 --> 00:29:34,923
for that event type that they're about to check in,

461
00:29:36,323 --> 00:29:38,745
and you wanna make sure that they're not stomping

462
00:29:38,785 --> 00:29:41,126
on each other and both saying plus one

463
00:29:41,166 --> 00:29:42,766
to whatever view that they have.

464
00:29:43,427 --> 00:29:45,468
So the Global Schema Store supports that.

465
00:29:46,548 --> 00:29:49,150
It also supports the enforcing

466
00:29:49,170 --> 00:29:51,871
the ever-growing schema requirement.

467
00:29:53,725 --> 00:30:02,671
So, which makes sure that the parameter types aren't changing in an unenforceable way.

468
00:30:02,691 --> 00:30:04,052
You know, they're not going from a UN64 down to a byte.

469
00:30:06,452 --> 00:30:08,814
And then all of our reporting infrastructure,

470
00:30:08,834 --> 00:30:12,076
I guess that would be supported going from,

471
00:30:12,517 --> 00:30:15,219
so we just don't let them change the types really,

472
00:30:15,399 --> 00:30:17,020
because figuring out that logic

473
00:30:17,060 --> 00:30:19,522
and trying to message that is a bit challenging.

474
00:30:20,023 --> 00:30:23,986
But that allows us to have our reporting stay consistent.

475
00:30:24,446 --> 00:30:27,469
Now this was a big pain point, unfortunately,

476
00:30:28,210 --> 00:30:30,812
because developers

477
00:30:31,988 --> 00:30:36,793
Sorry, my slide timer is like 20 minutes ahead of the real world time, so I had a little

478
00:30:36,813 --> 00:30:37,353
freak out there.

479
00:30:39,376 --> 00:30:41,778
Developers were...

480
00:30:43,075 --> 00:30:47,897
It's iterating on the events that they wanted to generate, as we all want to do.

481
00:30:48,757 --> 00:30:54,299
And every time they were compiling, they were compiling with each iteration, which was registering

482
00:30:54,339 --> 00:30:59,180
their event in the global schema store, which means that now they can't change the event

483
00:30:59,220 --> 00:31:03,621
types because they realized they needed a UN64 instead of a UN32.

484
00:31:04,482 --> 00:31:06,742
So it's on the books.

485
00:31:06,782 --> 00:31:07,903
We haven't implemented it yet.

486
00:31:08,603 --> 00:31:11,184
One of the things we need to do is support an offline...

487
00:31:12,956 --> 00:31:16,538
do not transmit this data to the cloud because we're iterating on

488
00:31:16,898 --> 00:31:19,519
what kinds of events we will want to transmit.

489
00:31:20,659 --> 00:31:23,561
The reason why this defaults to the behavior that we implemented

490
00:31:24,501 --> 00:31:28,243
is because we would have engineers that would create

491
00:31:29,303 --> 00:31:32,765
local one-off builds with a bunch of instrumentation.

492
00:31:35,027 --> 00:31:37,928
transmit that, run the game with that instrumentation

493
00:31:37,948 --> 00:31:39,529
because they're trying to track down some issue

494
00:31:39,569 --> 00:31:43,510
that's hard to debug, and then want to report from us

495
00:31:44,090 --> 00:31:46,952
saying, hey, in the cloud, I'm pulling up the session

496
00:31:46,972 --> 00:31:49,613
that I just ran through and I don't see my data.

497
00:31:50,113 --> 00:31:53,775
So there's, the power of this is that developers

498
00:31:53,855 --> 00:31:58,157
have access to their data in less than a minute.

499
00:31:58,257 --> 00:31:59,997
They have access to the full log of events

500
00:32:00,377 --> 00:32:01,938
that's being generated in real time.

501
00:32:02,578 --> 00:32:03,879
And our developers are,

502
00:32:05,275 --> 00:32:09,463
Most slash some developers are making use of that particular feature.

503
00:32:10,545 --> 00:32:12,028
So and it's a really powerful feature.

504
00:32:12,950 --> 00:32:15,234
The ones that use it are strong advocates for it.

505
00:32:17,625 --> 00:32:22,828
And finally, the preprocessor handles the bond serialization.

506
00:32:23,129 --> 00:32:26,491
And we're using Bond, which is a Microsoft protocol.

507
00:32:26,551 --> 00:32:27,271
It's open source.

508
00:32:28,252 --> 00:32:30,473
And it's similar to Google's protocol buffer.

509
00:32:31,253 --> 00:32:32,854
And so it does bit packing.

510
00:32:33,154 --> 00:32:36,636
Like in the first four fields, the first four fields

511
00:32:36,696 --> 00:32:39,158
on an event, they're ordinals.

512
00:32:39,718 --> 00:32:41,659
They're positioned in that event.

513
00:32:44,534 --> 00:32:46,815
1, 2, 3, 4 is actually stored in like the first couple bits

514
00:32:47,616 --> 00:32:52,339
of a particular entry within the bond serialization format.

515
00:32:52,879 --> 00:32:57,142
And then the next few bits store the type of the entity.

516
00:32:57,202 --> 00:33:00,004
And this lets us get away with an average event

517
00:33:00,044 --> 00:33:01,865
size of only 120 bytes.

518
00:33:02,925 --> 00:33:04,406
You can't do bit packing on GUIDs,

519
00:33:04,566 --> 00:33:07,809
which is why if people are adding in match ID and

520
00:33:09,169 --> 00:33:15,513
If you have a vehicle ID and life instance ID and all these other things, your event

521
00:33:15,533 --> 00:33:17,374
sizes start getting rather large.

522
00:33:28,340 --> 00:33:29,681
So that was Cell.

523
00:33:29,721 --> 00:33:32,282
That's the client-side architecture.

524
00:33:34,304 --> 00:33:37,365
On the service side, we've got Maelstrom.

525
00:33:39,511 --> 00:33:46,232
And this is, the Maelstrom component is fairly small

526
00:33:46,292 --> 00:33:49,613
because it is the, it's small in concept I suppose,

527
00:33:49,673 --> 00:33:51,833
but not necessarily in implementation.

528
00:33:52,873 --> 00:33:56,994
Because it's focus is to be the event pipeline

529
00:33:57,014 --> 00:34:00,615
that all of our other services can tie into and work with.

530
00:34:02,355 --> 00:34:03,695
So we have the ingestion service,

531
00:34:04,075 --> 00:34:05,956
and this is the primary service

532
00:34:05,976 --> 00:34:08,156
the clients are connecting to.

533
00:34:10,111 --> 00:34:15,972
The clients, what we're using is WebSocket protocol

534
00:34:16,312 --> 00:34:20,033
to connect up to the cloud, to the ingestion service.

535
00:34:20,713 --> 00:34:23,033
We're using a single WebSocket connection

536
00:34:23,614 --> 00:34:25,554
for both the high and the low priority channels,

537
00:34:26,734 --> 00:34:28,474
which adds a little bit of complexity.

538
00:34:29,575 --> 00:34:33,155
And in order to multiplex over that path,

539
00:34:33,235 --> 00:34:38,336
we're using AMQP, which is a queuing protocol.

540
00:34:43,404 --> 00:34:47,127
So, the clients are connecting to the ingestion service,

541
00:34:47,167 --> 00:34:48,248
and then the ingestion service,

542
00:34:48,308 --> 00:34:51,490
as it receives the events from the client,

543
00:34:51,510 --> 00:34:53,311
now remember I said we weren't transmitting

544
00:34:53,371 --> 00:34:58,455
the session information or the authentication claims

545
00:34:59,696 --> 00:35:01,697
with every event payload, every frame.

546
00:35:02,896 --> 00:35:04,958
What we are instead doing is transmitting those

547
00:35:05,358 --> 00:35:08,601
during the connection negotiation for the web socket,

548
00:35:09,162 --> 00:35:11,904
and then the ingestion service keeps track of that

549
00:35:12,205 --> 00:35:14,187
as part of that web socket information.

550
00:35:15,147 --> 00:35:18,551
It will then stick that onto the payload

551
00:35:18,571 --> 00:35:19,812
that it receives from the client

552
00:35:20,653 --> 00:35:25,157
and injects that into the event queue

553
00:35:25,637 --> 00:35:26,638
that we're using in the cloud.

554
00:35:28,070 --> 00:35:30,151
And that way, whoever's reading off the data,

555
00:35:30,211 --> 00:35:32,892
then they can have access to that session GUID

556
00:35:33,052 --> 00:35:35,293
and say, oh, this is something I've subscribed to.

557
00:35:36,233 --> 00:35:40,154
I want this event and this event and this event off that queue.

558
00:35:42,615 --> 00:35:47,916
And then we're transmitting our primary queuing system

559
00:35:47,936 --> 00:35:50,197
that we're using is the Azure Event Hub, which

560
00:35:50,257 --> 00:35:53,538
has performed quite well for our needs.

561
00:35:54,358 --> 00:35:56,739
It's stood up to some pretty high volume stuff.

562
00:35:59,394 --> 00:36:05,826
We also, in the ingestion service, support non-AMQP paths.

563
00:36:06,107 --> 00:36:08,231
So we're getting telemetry from other.

564
00:36:10,146 --> 00:36:15,649
components that might only transmit JSON payloads, for example.

565
00:36:16,189 --> 00:36:19,771
And so the ingestion service will take those JSON payloads

566
00:36:20,131 --> 00:36:24,613
and convert them into bond. Those are much lower volume streams,

567
00:36:24,873 --> 00:36:26,994
and so it's something we can do in the ingestion service.

568
00:36:27,674 --> 00:36:30,896
But for the most part, the ingestion service never cracks open

569
00:36:30,996 --> 00:36:33,497
any already encoded in bond payload.

570
00:36:33,757 --> 00:36:35,198
Its sole purpose is to...

571
00:36:36,059 --> 00:36:39,321
Take the claims and the session ID,

572
00:36:39,821 --> 00:36:42,163
associate it with whatever it just received from the client,

573
00:36:42,523 --> 00:36:46,145
and let someone else deal with authorization.

574
00:36:46,466 --> 00:36:49,248
So ingestion service will do authentication

575
00:36:49,448 --> 00:36:51,209
to ensure that you're allowed to talk to it,

576
00:36:52,030 --> 00:36:54,071
but then the authorization is offloaded

577
00:36:54,131 --> 00:36:56,713
to whoever's going to actually use that information.

578
00:37:01,573 --> 00:37:07,715
So, I mean, that's Maelstrom, it's the event hub and the ingestion service, mostly.

579
00:37:07,895 --> 00:37:13,857
So there's also the common API we have for consuming those event streams.

580
00:37:14,117 --> 00:37:21,119
So the API layer that allows you to connect to that event stream on event hub, and then

581
00:37:21,559 --> 00:37:24,140
allows you to read the stream of events.

582
00:37:26,489 --> 00:37:30,696
Coupled with the global schema story I was talking about.

583
00:37:30,917 --> 00:37:32,460
Right, so we have a global registry

584
00:37:33,241 --> 00:37:37,930
of what every event should be interpreted as.

585
00:37:38,777 --> 00:37:40,138
Field number two, it's an int.

586
00:37:40,398 --> 00:37:42,179
Okay, that's part of the bond protocol,

587
00:37:42,840 --> 00:37:46,343
but understanding that that is a weapon ID, for example,

588
00:37:46,643 --> 00:37:49,305
that's metadata that's stored in the schema store.

589
00:37:49,646 --> 00:37:52,888
So we have a common set of APIs

590
00:37:53,188 --> 00:37:55,911
that allow for understanding the event streams

591
00:37:55,971 --> 00:37:56,711
as they're coming off.

592
00:37:58,313 --> 00:37:59,534
Then we have several services

593
00:37:59,994 --> 00:38:02,936
that feed off of the Maelstrom pipeline.

594
00:38:04,458 --> 00:38:07,400
The stats service.

595
00:38:09,185 --> 00:38:12,432
is the primary one that is in charge of the leaderboards

596
00:38:12,933 --> 00:38:15,339
and the end of game stats that you see,

597
00:38:15,619 --> 00:38:16,682
hopefully, at the end of the game.

598
00:38:18,565 --> 00:38:22,987
And the storage service is the one that takes off Event Hub

599
00:38:24,248 --> 00:38:27,770
because that retains for so many days into the past.

600
00:38:28,210 --> 00:38:30,112
You can recover your services

601
00:38:30,172 --> 00:38:31,973
anytime within like a seven-day window.

602
00:38:32,653 --> 00:38:34,394
And as long as you've persisted

603
00:38:34,634 --> 00:38:36,295
where you were in the Event Hub queue,

604
00:38:36,735 --> 00:38:39,477
you can spin everything up and read everything out again.

605
00:38:43,523 --> 00:38:44,864
that you're using the right offset.

606
00:38:45,725 --> 00:38:46,907
Funny anecdote time,

607
00:38:47,327 --> 00:38:49,529
there was a partner team we were working with that

608
00:38:50,651 --> 00:38:52,753
always spun up their service using offset zero.

609
00:38:53,313 --> 00:38:56,497
So they were reading from the beginning of time

610
00:38:57,238 --> 00:39:00,201
and at that time there weren't very many

611
00:39:00,630 --> 00:39:03,231
safeguards to throttle the reading.

612
00:39:03,291 --> 00:39:07,994
So it kind of took things down during the Halo preview.

613
00:39:08,794 --> 00:39:09,895
So, lesson learned.

614
00:39:10,575 --> 00:39:12,877
Make sure that everybody's using and persisting

615
00:39:13,817 --> 00:39:16,499
their offsets as part of their checkpointing process.

616
00:39:17,099 --> 00:39:18,079
And also make sure that you've got

617
00:39:18,199 --> 00:39:20,761
throttling naturally in there.

618
00:39:22,760 --> 00:39:29,384
So storage sends stuff off to blobs, and then later on, as I'll be getting into, we can

619
00:39:29,424 --> 00:39:33,887
crack open those blobs for doing some larger scale reporting.

620
00:39:34,548 --> 00:39:38,350
The stats service is essentially a state machine.

621
00:39:38,830 --> 00:39:42,352
So since we don't have to do any of the windowing processing,

622
00:39:43,841 --> 00:39:46,982
I think at one point they did have Trill implemented

623
00:39:47,002 --> 00:39:51,403
so they could do more complex windowed processing style queries

624
00:39:51,643 --> 00:39:55,864
as part of the event stream, but at its most basic level

625
00:39:55,904 --> 00:39:57,844
it's not necessarily necessary.

626
00:39:57,884 --> 00:40:02,745
They just read the events and increment their internal counters

627
00:40:03,325 --> 00:40:04,466
and persist their state.

628
00:40:06,457 --> 00:40:07,997
I didn't go into it.

629
00:40:08,678 --> 00:40:09,578
We have several talks.

630
00:40:10,238 --> 00:40:15,200
If you go into, I think the GDC Vault,

631
00:40:16,140 --> 00:40:19,422
as well as the, I think we've given talks at Build

632
00:40:20,762 --> 00:40:21,943
that cover how.

633
00:40:24,326 --> 00:40:29,407
343 uses the Orleans technology, which is another open source technology bed that allows

634
00:40:29,527 --> 00:40:39,990
for these things called virtual actors, which are essentially your, they call them stateless

635
00:40:40,010 --> 00:40:46,131
state machines, because they're state machines, but then they can migrate as the servers go

636
00:40:46,211 --> 00:40:49,172
up and down and everything's handled for you, and it's really cool tech.

637
00:40:49,948 --> 00:40:51,450
that we leverage a lot in 343.

638
00:40:51,670 --> 00:40:54,353
So I recommend looking into that.

639
00:40:57,035 --> 00:40:59,998
Finally, we have the librarian.

640
00:41:00,159 --> 00:41:03,722
And what the librarian does is it's cracking open,

641
00:41:03,782 --> 00:41:06,485
finally, we're cracking open these events

642
00:41:07,426 --> 00:41:11,731
from a BI perspective, right?

643
00:41:11,751 --> 00:41:12,471
So the stats.

644
00:41:15,007 --> 00:41:19,089
Components are cracking open the events that they want to read,

645
00:41:19,789 --> 00:41:22,230
and they're subscribing to the events that they want to subscribe to

646
00:41:22,731 --> 00:41:27,293
in order to drive the end-of-game leaderboards in a real-time fashion.

647
00:41:28,114 --> 00:41:30,575
But they're only really subscribing to the high-priority feed,

648
00:41:31,095 --> 00:41:36,158
which allows them to shave off, like, 80% of the network volumes

649
00:41:36,358 --> 00:41:38,879
and processing requirements that they would otherwise have.

650
00:41:39,896 --> 00:41:49,002
So, the librarian, what it does is it's actually parsing the events and we've stored just enough

651
00:41:49,042 --> 00:41:50,923
information in the headers of the events.

652
00:41:51,463 --> 00:41:57,487
And Bond is really great in that you can deserialize in the event stream just as much as you need.

653
00:41:58,388 --> 00:42:05,313
and then skip the rest of the payload and not incur any of the overhead in deserializing the rest of that event.

654
00:42:06,133 --> 00:42:15,340
So our events are architected such that the information you need, the event type ID, the title ID, the session information, timestamp,

655
00:42:15,740 --> 00:42:18,222
those are all in the very first few dozen bytes.

656
00:42:18,822 --> 00:42:24,326
And so the librarian is parsing out that header and making a note of...

657
00:42:24,787 --> 00:42:26,827
What time stamp, different events happened.

658
00:42:27,068 --> 00:42:29,849
And so now we can do reporting on the session,

659
00:42:30,189 --> 00:42:32,050
observed this many of this kind of event.

660
00:42:33,570 --> 00:42:36,972
And then if that's, if we're looking for a very rare event,

661
00:42:37,032 --> 00:42:40,013
then we can say, well, just give us the 20 sessions where

662
00:42:40,053 --> 00:42:41,694
this event happened, for example.

663
00:42:41,734 --> 00:42:42,994
And then we get the full context.

664
00:42:46,176 --> 00:42:49,557
Finally, we've got the telemetry event viewer.

665
00:42:51,038 --> 00:42:52,718
And the telemetry event viewer.

666
00:42:54,267 --> 00:42:56,589
is a work in progress.

667
00:42:57,049 --> 00:43:00,892
So right now, it actually just outputs a CSV file

668
00:43:01,092 --> 00:43:02,673
that developers can open in Excel.

669
00:43:03,434 --> 00:43:05,475
I wish I had a screenshot of it, actually,

670
00:43:06,796 --> 00:43:08,457
because it gives you a lot of data.

671
00:43:09,117 --> 00:43:11,139
But it's essentially the event stream

672
00:43:11,959 --> 00:43:13,380
in human-readable format,

673
00:43:13,801 --> 00:43:16,963
but also with every column.

674
00:43:18,784 --> 00:43:21,506
Each field has been separated into its own column.

675
00:43:22,207 --> 00:43:24,609
which allows you to do filtering and sorting

676
00:43:24,629 --> 00:43:27,031
and what have you on the fields within those events.

677
00:43:29,133 --> 00:43:33,336
And this has been an amazingly useful tool

678
00:43:33,916 --> 00:43:38,079
in debugging what exactly is going on in the game,

679
00:43:38,200 --> 00:43:39,901
especially for the networking guys,

680
00:43:40,541 --> 00:43:47,207
because networking is inherently an integrated problem.

681
00:43:47,859 --> 00:43:52,642
So you've got 24 clients and a server and several services

682
00:43:53,183 --> 00:43:54,263
that are all interacting.

683
00:43:54,644 --> 00:43:57,345
And as soon as you hit a bug, then you've

684
00:43:57,385 --> 00:43:59,787
got to figure out what were the states of all

685
00:43:59,807 --> 00:44:04,070
the different actors at the time that that negative event

686
00:44:04,110 --> 00:44:04,450
happened.

687
00:44:05,431 --> 00:44:07,092
So with the Telemetry Event Viewer,

688
00:44:07,112 --> 00:44:09,773
you can actually pull down all of the events

689
00:44:10,114 --> 00:44:11,795
from all of the clients that were connected.

690
00:44:12,481 --> 00:44:14,302
within like a five minute window

691
00:44:14,662 --> 00:44:15,642
from when the event happened.

692
00:44:15,662 --> 00:44:17,603
And you can see what everybody's doing

693
00:44:18,664 --> 00:44:19,804
up to when the event happened.

694
00:44:20,804 --> 00:44:24,226
So that's been a really useful tool.

695
00:44:24,846 --> 00:44:28,868
And because we're flushing every 30 to 60 seconds

696
00:44:29,168 --> 00:44:32,269
for the larger blobs in storage,

697
00:44:32,949 --> 00:44:36,591
we're able to pull up sessions as they're live.

698
00:44:36,631 --> 00:44:38,011
So if somebody's saying, hey, you know,

699
00:44:38,052 --> 00:44:41,093
I'm getting some warping going on in this game.

700
00:44:41,985 --> 00:44:47,328
We can actually load it up and see what's going on in less than a minute.

701
00:44:48,409 --> 00:44:59,775
The corollary to that, one reason why we have to support so much volume, is that the networking

702
00:44:59,815 --> 00:45:02,156
guys generate the most events.

703
00:45:02,416 --> 00:45:07,279
So there were something like 9,000 events in the legacy system, the old system, that

704
00:45:07,619 --> 00:45:09,480
was log underscore event.

705
00:45:10,102 --> 00:45:14,823
We migrated probably a thousand events into Cell

706
00:45:14,843 --> 00:45:16,043
before we launched.

707
00:45:17,663 --> 00:45:20,764
And of those, I would say probably three-fourths of them

708
00:45:21,124 --> 00:45:23,945
are networking related, just because they have

709
00:45:23,985 --> 00:45:25,405
so many things that they need to track.

710
00:45:26,386 --> 00:45:33,987
And then for doing our big data processing,

711
00:45:34,748 --> 00:45:38,008
we're using Hadoop and Hive, and we have a custom

712
00:45:39,587 --> 00:45:44,551
Java bond Sirday that is reading the roblobs

713
00:45:45,552 --> 00:45:52,659
so we have a job every hour that runs that traverses through the roblobs that are stored in bond and

714
00:45:53,480 --> 00:45:56,623
Converts the events that we're interested in into

715
00:45:57,904 --> 00:45:59,526
the orc format which is

716
00:46:01,260 --> 00:46:03,221
It's a columnar format, so it's very,

717
00:46:04,321 --> 00:46:05,661
if your data is slowly changing,

718
00:46:06,081 --> 00:46:08,642
it's a very highly compressed and efficient way

719
00:46:08,742 --> 00:46:10,522
to store your data.

720
00:46:10,542 --> 00:46:12,362
So if you have data that's like,

721
00:46:13,263 --> 00:46:15,443
if you always have a map name in your event,

722
00:46:16,023 --> 00:46:18,303
then it only stores the map name once

723
00:46:18,664 --> 00:46:20,284
and then has like a vector that says,

724
00:46:20,384 --> 00:46:23,745
okay, well, this value, it's like run length encoding.

725
00:46:23,785 --> 00:46:25,745
This value applies for the next 20,000 entries

726
00:46:25,765 --> 00:46:26,485
for this hour.

727
00:46:28,385 --> 00:46:33,227
So it creates very small files and it's fairly standard in the big data world.

728
00:46:36,347 --> 00:46:39,468
And I just spoke to all of that.

729
00:46:39,508 --> 00:46:45,589
And then we've got ad hoc and regular reports that are running in the Hive query language.

730
00:46:46,590 --> 00:46:54,332
And we're also using Tableau and R for visualization tools and to create our dashboards.

731
00:47:06,354 --> 00:47:08,535
So some of the gotchas in implementing this.

732
00:47:10,556 --> 00:47:12,097
The client implementation took time.

733
00:47:12,798 --> 00:47:15,159
It took a lot of time, I have to say.

734
00:47:16,600 --> 00:47:21,803
We originally anticipated it to be something that was on the

735
00:47:21,843 --> 00:47:22,644
six-month scale.

736
00:47:23,124 --> 00:47:26,666
And I think we ended up spending most of the three

737
00:47:26,686 --> 00:47:29,788
years continually working on the client-side

738
00:47:29,808 --> 00:47:32,690
implementation and ensuring the performance was right.

739
00:47:33,771 --> 00:47:33,911
And

740
00:47:36,135 --> 00:47:44,979
A fair amount of that was working with the build integration systems that are part of

741
00:47:45,019 --> 00:47:45,880
game development.

742
00:47:45,940 --> 00:47:49,161
You know, somebody breaks the build and you've got to integrate and you've got to figure

743
00:47:49,181 --> 00:47:50,922
out was it you that broke the build, etc., etc.

744
00:47:54,289 --> 00:47:59,212
A lot of people that I've been working with are more on the services side, so helping

745
00:47:59,252 --> 00:48:04,035
those teams understand the implications of doing client-side development, making sure

746
00:48:04,055 --> 00:48:08,558
that they understand how absolutely critical it is for a high-performance, low-latency,

747
00:48:08,978 --> 00:48:14,041
client-side impact was pivotal for the success of making this happen.

748
00:48:14,701 --> 00:48:19,484
We wouldn't have had client team buy-off if we didn't convey how much we understood.

749
00:48:19,504 --> 00:48:22,126
It was important to keep this at the microsecond level.

750
00:48:24,914 --> 00:48:29,259
There's this thing called a statistics event that I haven't spoken to until now.

751
00:48:30,620 --> 00:48:37,006
Every minute, the client side writes a little histogram event into the stream that says

752
00:48:37,527 --> 00:48:42,311
here's all the events that I've seen, so that we can do matchups to say, oh, well, we didn't

753
00:48:43,665 --> 00:48:48,307
It was a high priority stream, we shouldn't have any dropped events, but apparently we did drop these three events.

754
00:48:48,367 --> 00:48:49,668
We need to go investigate that.

755
00:48:51,668 --> 00:48:56,110
We actually had to turn that off for release because...

756
00:48:57,030 --> 00:49:01,732
Actually, no, it is in release, but it's got an issue in that

757
00:49:03,513 --> 00:49:07,394
every event is sequentially incrementing its sequence ID, right?

758
00:49:07,734 --> 00:49:08,575
Everything is sequential.

759
00:49:12,217 --> 00:49:18,602
The statistics event is attached as part of the header of the event, but its sequence

760
00:49:18,662 --> 00:49:24,006
ID is allocated at the end of the event because the batch it's about to send, it wants to

761
00:49:24,066 --> 00:49:24,586
report on it.

762
00:49:25,167 --> 00:49:29,911
So we had some issues with people that were doing the right thing and making sure they

763
00:49:29,951 --> 00:49:34,554
only interpreted sequential events, but they were having all sorts of dropouts because

764
00:49:35,255 --> 00:49:38,597
once a minute this particular event would show up out of sequence.

765
00:49:40,272 --> 00:49:45,160
Whenever you're doing these kinds of custom scenarios,

766
00:49:45,560 --> 00:49:48,365
you really have to keep track of all the different systems

767
00:49:48,385 --> 00:49:49,787
that could be impacted by it.

768
00:49:54,828 --> 00:50:01,255
from the beginning that a client session would be the instance of the executable from when

769
00:50:01,275 --> 00:50:06,681
the executable started to when the executable was terminated and released from memory.

770
00:50:07,682 --> 00:50:14,490
The thing we didn't really account for is that on the Xbox One, titles go to sleep.

771
00:50:15,527 --> 00:50:18,750
And it could be a week before the person comes back

772
00:50:19,330 --> 00:50:20,751
and fires up their console again.

773
00:50:21,552 --> 00:50:22,973
Or they could go off and play Netflix

774
00:50:23,573 --> 00:50:25,635
for several hours or days,

775
00:50:26,235 --> 00:50:28,737
and then come back and start the title again.

776
00:50:29,658 --> 00:50:31,860
At which point, the title instantly comes on.

777
00:50:32,080 --> 00:50:34,782
You don't have to load the waiting screen or anything,

778
00:50:35,623 --> 00:50:37,864
but you're using the same session ID.

779
00:50:38,485 --> 00:50:41,287
And that's caused a fair amount of headaches

780
00:50:41,707 --> 00:50:43,108
for our reporting systems,

781
00:50:43,308 --> 00:50:44,189
because now instead of...

782
00:50:44,879 --> 00:50:47,462
carrying forward the information from the previous hour

783
00:50:48,604 --> 00:50:51,207
and aggregating that information.

784
00:50:52,829 --> 00:50:55,473
Now, like if they're playing a campaign game

785
00:50:56,394 --> 00:50:58,156
and they're mid-game and then they go to sleep,

786
00:50:58,877 --> 00:51:01,901
now you wake up a week later and you need to figure out what map they were on.

787
00:51:02,999 --> 00:51:05,660
But because of the contextualization decisions we made,

788
00:51:06,300 --> 00:51:09,101
that information is stored a week ago.

789
00:51:11,761 --> 00:51:14,762
That was a headache and it's still not solved.

790
00:51:15,642 --> 00:51:18,663
We basically carry forward summary information

791
00:51:18,703 --> 00:51:20,884
for players as a stopgap.

792
00:51:24,059 --> 00:51:30,707
And then things came in a bit hot, and we needed to get our actual business intelligence

793
00:51:31,327 --> 00:51:32,108
reports going.

794
00:51:33,049 --> 00:51:39,116
So the stream querying that we really wanted to make a lot of use with, so we're using

795
00:51:40,517 --> 00:51:43,681
the non-windowed streaming analytics in our stat screens.

796
00:51:44,520 --> 00:51:48,183
But we haven't had an opportunity yet to implement Trill

797
00:51:48,223 --> 00:51:49,824
and some of these other technologies,

798
00:51:49,864 --> 00:51:51,966
like Esper is one that we've been looking at,

799
00:51:52,887 --> 00:51:57,211
to have kind of business intelligence-driven

800
00:51:57,511 --> 00:51:59,433
streaming queries off this stuff.

801
00:52:02,615 --> 00:52:05,117
And as I said earlier, the schema store

802
00:52:05,918 --> 00:52:09,361
caused some problems during developer iteration.

803
00:52:11,703 --> 00:52:11,923
So.

804
00:52:14,785 --> 00:52:19,186
Running low on time, but I do want to show you some cool graphs, right?

805
00:52:19,906 --> 00:52:23,247
The map balance.

806
00:52:23,287 --> 00:52:30,468
So I don't know, how many people here saw the community news article about red team

807
00:52:30,488 --> 00:52:31,308
versus blue team?

808
00:52:33,349 --> 00:52:35,069
Awesome, great, this is going to be new to all of you.

809
00:52:36,069 --> 00:52:40,910
So there were concerns in our community that our maps were not balanced, red team versus

810
00:52:40,930 --> 00:52:41,310
blue team.

811
00:52:43,555 --> 00:52:48,697
Indeed, if you look at the raw numbers, you can see that blue team seems to be doing,

812
00:52:51,258 --> 00:52:57,740
these are wins versus red team wins, and blue team seems to be winning more frequently except

813
00:52:57,760 --> 00:52:58,860
for the line at the very top.

814
00:52:59,720 --> 00:53:03,302
This is all on one map, these are just different game modes on that map.

815
00:53:05,222 --> 00:53:11,184
And then the previous one was all up across for that entire day.

816
00:53:14,235 --> 00:53:18,997
This one is broken out by, let's see,

817
00:53:19,037 --> 00:53:21,278
I think the previous one was all up for the entire week.

818
00:53:21,878 --> 00:53:23,499
The next one is broken out by day,

819
00:53:23,899 --> 00:53:26,640
but significantly, it's looking at even team

820
00:53:26,680 --> 00:53:27,561
versus even team.

821
00:53:27,801 --> 00:53:29,561
So it's taking out the noise that can happen

822
00:53:29,621 --> 00:53:35,064
if a really strong team is playing one color consistently

823
00:53:35,124 --> 00:53:37,705
for some reason, which did happen.

824
00:53:38,865 --> 00:53:41,346
And so these are even teams that we're comparing,

825
00:53:41,386 --> 00:53:42,707
and we see there's a lot of blue there.

826
00:53:44,445 --> 00:53:51,851
And then you can also do something cool like looking at what is the matchmaking rating

827
00:53:53,352 --> 00:54:00,138
delta at which you now get an even win-loss rate.

828
00:54:01,198 --> 00:54:10,346
So with that, you can see that in some game modes, red team has to be several, it's like 0.3.

829
00:54:10,826 --> 00:54:13,968
Let's see, I don't really have a pointer.

830
00:54:14,412 --> 00:54:23,157
But some of those lines indicate that you have to be 0.3 mu better on the red team versus the blue team before you start having an even match.

831
00:54:24,038 --> 00:54:32,723
And so we wanted to look into this, and our hypothesis was that some of the maps allowed for blue to blend in easier.

832
00:54:32,823 --> 00:54:39,286
In fact, we think that blue is just generally a harder color to get target acquisition on,

833
00:54:39,546 --> 00:54:43,789
because red is a much more high-contrast color versus blue.

834
00:54:44,867 --> 00:54:50,056
So we were looking at heat maps, and I'll speed up through this.

835
00:54:50,076 --> 00:54:53,021
So deaths.

836
00:54:55,141 --> 00:54:58,983
Red versus blue, there's more red deaths there,

837
00:55:00,003 --> 00:55:01,284
but it's kind of hard to make out.

838
00:55:01,784 --> 00:55:04,466
A pattern doesn't really show up well.

839
00:55:05,186 --> 00:55:08,248
So what you can do is you can look at traversals,

840
00:55:09,008 --> 00:55:10,869
and you can see that red base is at the bottom,

841
00:55:11,229 --> 00:55:13,010
and blue base is at the top, right?

842
00:55:13,070 --> 00:55:16,992
So on the previous slide, there's more red deaths

843
00:55:17,072 --> 00:55:20,094
on the bottom, but that's red's base, right?

844
00:55:20,114 --> 00:55:22,015
So I mean, it makes sense that red would die more

845
00:55:22,415 --> 00:55:23,315
if they're spawning down there.

846
00:55:24,821 --> 00:55:28,523
And so you can get this lethality matrix,

847
00:55:28,904 --> 00:55:32,186
although looking at it at the individual dots,

848
00:55:32,946 --> 00:55:33,747
it's hard to make out,

849
00:55:33,787 --> 00:55:35,708
but you can see that there's some patterns

850
00:55:35,728 --> 00:55:36,569
starting to emerge.

851
00:55:37,309 --> 00:55:40,491
So what I did was we've got these things

852
00:55:40,531 --> 00:55:42,433
called call-outs in the maps.

853
00:55:43,474 --> 00:55:45,675
that tell you where you are in the map.

854
00:55:45,695 --> 00:55:48,456
So you can say, oh, there's somebody in upper catwalk

855
00:55:48,496 --> 00:55:50,317
or lower catwalk or what have you.

856
00:55:50,837 --> 00:55:55,399
So I actually walked the borders of these callouts

857
00:55:55,439 --> 00:55:56,460
just as a prototype.

858
00:55:57,280 --> 00:56:02,283
And outputted my XYZ coordinates at a regular interval,

859
00:56:02,383 --> 00:56:04,884
like every second, threw them into Excel

860
00:56:05,044 --> 00:56:06,825
so that I could make sure that I was collecting

861
00:56:06,845 --> 00:56:07,985
the right kind of information.

862
00:56:08,885 --> 00:56:10,866
Imported that into SQL Server using a,

863
00:56:12,521 --> 00:56:20,949
a two-dimensional, they've got a 2D query extension that's part of SQL Server, so you

864
00:56:20,969 --> 00:56:22,570
can do bounded queries.

865
00:56:23,231 --> 00:56:29,817
And then correlated the traversal information with the death information to say, okay, inside

866
00:56:29,857 --> 00:56:33,140
this polygon, how many deaths happen, that kind of thing.

867
00:56:34,776 --> 00:56:38,858
in this heat map, which is much more useful in my opinion.

868
00:56:39,759 --> 00:56:41,199
And because I have the Z order,

869
00:56:41,720 --> 00:56:45,061
I can layer the polygons one on top of each other.

870
00:56:45,842 --> 00:56:48,183
So you can see the catwalks are above the courtyards.

871
00:56:48,963 --> 00:56:50,064
And now you can see that red

872
00:56:50,644 --> 00:56:52,906
is not having the greatest time.

873
00:56:54,166 --> 00:56:56,988
Red seems to be dying regardless.

874
00:56:58,409 --> 00:57:00,572
Now that we take into account traversals,

875
00:57:00,973 --> 00:57:03,518
especially when red is entering blue base,

876
00:57:03,838 --> 00:57:07,024
and blue is finally dying when blue is entering red base,

877
00:57:07,424 --> 00:57:08,867
and the walls have a slight hue that...

878
00:57:12,773 --> 00:57:15,955
make those opposing colors much more visible.

879
00:57:16,435 --> 00:57:20,158
Except for this little blue circle that's really, really strong in the bottom right.

880
00:57:21,599 --> 00:57:22,699
There you go. I circled it.

881
00:57:23,320 --> 00:57:25,141
And so I loaded up the map and I'm like,

882
00:57:25,161 --> 00:57:27,523
what's going on with this little circle? That's curious.

883
00:57:28,283 --> 00:57:32,886
And there's this big red wall right behind.

884
00:57:33,547 --> 00:57:35,969
This little platform that people can stand on and shoot.

885
00:57:36,550 --> 00:57:39,733
So it was kind of a nice confirmation

886
00:57:39,773 --> 00:57:43,617
that the hypothesis of hue versus background color

887
00:57:43,637 --> 00:57:46,500
and what have you was being borne out.

888
00:57:48,016 --> 00:57:53,661
So we took that to the map designers, and what they did was they said, well, so this

889
00:57:53,701 --> 00:57:59,387
particular map, we're at like 54% blue win versus 46% red.

890
00:58:00,548 --> 00:58:07,795
And they said, well, okay, there's this bug where the best player always, in particular

891
00:58:07,855 --> 00:58:11,598
scenarios, the best player is always put on the blue team.

892
00:58:12,517 --> 00:58:15,158
So maybe that's what we're seeing, right?

893
00:58:15,358 --> 00:58:18,279
So it's like, fine, we waited for that fix to go out.

894
00:58:18,999 --> 00:58:22,060
And did my, yeah, there we go.

895
00:58:22,560 --> 00:58:25,141
And so we went down a percent point.

896
00:58:25,401 --> 00:58:26,502
So the effect was there,

897
00:58:26,662 --> 00:58:28,102
and we're able to measure it and show that.

898
00:58:29,063 --> 00:58:31,903
And then they tweaked the hue of the color

899
00:58:31,943 --> 00:58:32,844
to be a brighter hue.

900
00:58:33,704 --> 00:58:37,465
And the day that, or the week that that patch went out,

901
00:58:37,625 --> 00:58:38,446
there was an immediate,

902
00:58:40,102 --> 00:58:46,205
Advantage seen. Now this is across all maps. So the thing to remember when you're looking at the data is

903
00:58:46,245 --> 00:58:51,688
There's always another story, right? So you go digging into it and you're like, well, you know, let me look at a map

904
00:58:51,708 --> 00:58:52,648
that's that's kind of

905
00:58:54,990 --> 00:58:57,931
Reddish so in this case the

906
00:58:59,492 --> 00:59:01,973
Advantage that red has is like a few percentage points

907
00:59:04,054 --> 00:59:06,436
Versus the column on the left which is the overall advantage

908
00:59:07,691 --> 00:59:12,816
And the fix went out, and sure enough, that dropped by a percentage point, just like it

909
00:59:12,856 --> 00:59:14,378
did for the average across everything.

910
00:59:15,319 --> 00:59:23,107
And the blue color went out, and now red team is actually getting an advantage that they

911
00:59:23,147 --> 00:59:23,847
didn't have before.

912
00:59:24,168 --> 00:59:31,355
So blue is now at a consistent two or three percentage point disadvantage, but that is

913
00:59:31,415 --> 00:59:31,575
now

914
00:59:32,508 --> 00:59:34,710
being bad on the redder maps, right?

915
00:59:35,050 --> 00:59:37,492
And there's one other map that's even worse than this,

916
00:59:37,512 --> 00:59:40,435
and they went really not good.

917
00:59:40,515 --> 00:59:43,958
So we're going to have to do something like a per map color

918
00:59:43,978 --> 00:59:45,039
variation or something.

919
00:59:45,319 --> 00:59:52,406
But I'm out of time, so I'm going to play these two things.

920
00:59:55,748 --> 00:59:56,329
There you go.

921
00:59:56,389 --> 00:59:57,509
This is a war zone map.

922
00:59:58,670 --> 01:00:00,552
Blue team crushing red team.

923
01:00:02,293 --> 01:00:03,134
Play it one more time.

924
01:00:06,016 --> 01:00:11,281
And then conversely, this is blue team versus red team

925
01:00:11,321 --> 01:00:12,081
are evenly matched.

926
01:00:12,702 --> 01:00:15,364
And so they just sit there duking it out.

927
01:00:16,125 --> 01:00:18,447
Oh, except the wrong animation got in.

928
01:00:18,687 --> 01:00:19,587
So it was the same thing.

929
01:00:19,768 --> 01:00:20,288
That's a bummer.

930
01:00:22,625 --> 01:00:23,647
Get my card afterwards.

931
01:00:23,687 --> 01:00:25,591
We'll go outside, and we'll have lots of questions, I'm

932
01:00:25,651 --> 01:00:28,697
sure, and I'll email you the real animation.

933
01:00:28,937 --> 01:00:31,081
So thank you all for your time.

934
01:00:31,101 --> 01:00:31,963
I appreciate it.

