1
00:00:06,117 --> 00:00:07,098
Hello, everybody.

2
00:00:07,098 --> 00:00:10,139
Thanks for coming to this presentation.

3
00:00:10,139 --> 00:00:12,720
My name is Cesar Romero, and I'm a machine learning engineer.

4
00:00:12,720 --> 00:00:17,481
But today, I'm mostly talking to artists.

5
00:00:17,481 --> 00:00:19,402
And my goal with this presentation

6
00:00:19,402 --> 00:00:21,603
is hopefully to maybe inspire you a little bit,

7
00:00:21,603 --> 00:00:25,524
maybe expose you to some tools that are becoming available,

8
00:00:25,524 --> 00:00:27,345
some techniques that have made a lot of progress

9
00:00:27,345 --> 00:00:28,625
in the recent years.

10
00:00:30,680 --> 00:00:33,400
and hopefully you can come up with even better ideas

11
00:00:33,400 --> 00:00:37,141
of things that you might create using some of these tools.

12
00:00:37,141 --> 00:00:40,182
I think it's just a matter of time before

13
00:00:40,182 --> 00:00:43,142
some of these tools become ready for production.

14
00:00:43,142 --> 00:00:46,963
In some of the examples that I'm gonna go over,

15
00:00:46,963 --> 00:00:50,084
I'll point you to tools, in some cases open source,

16
00:00:50,084 --> 00:00:54,304
in some cases our websites you can just run directly.

17
00:00:54,304 --> 00:00:55,245
Some other cases don't have tools yet.

18
00:00:55,245 --> 00:00:57,165
I still believe it's just a matter of time.

19
00:00:57,165 --> 00:00:57,825
So let's get started.

20
00:00:59,003 --> 00:01:03,447
The first example I'm gonna go over is style transfer.

21
00:01:03,447 --> 00:01:06,970
If my slides...

22
00:01:06,970 --> 00:01:11,074
Okay, so just quick, who's familiar with style transfer?

23
00:01:11,074 --> 00:01:14,757
Okay, that's more than half, so that's good.

24
00:01:14,757 --> 00:01:19,862
You probably have seen many, many of these examples.

25
00:01:19,862 --> 00:01:21,043
First paper was published around 2016.

26
00:01:21,043 --> 00:01:24,005
Google even has some examples

27
00:01:24,005 --> 00:01:26,287
in the keynote earlier this week.

28
00:01:27,016 --> 00:01:30,580
the case is basically that you have some content

29
00:01:30,580 --> 00:01:33,203
and you want to see what that content would look like

30
00:01:33,203 --> 00:01:34,124
if you paint over it.

31
00:01:34,124 --> 00:01:36,226
In the case of a game, you might be exploring

32
00:01:36,226 --> 00:01:40,391
whether a specific scenario or setting environment,

33
00:01:40,391 --> 00:01:43,294
how might it feel, like how do you get the right mood

34
00:01:43,294 --> 00:01:45,516
or the right style, does it work or doesn't it work?

35
00:01:46,057 --> 00:01:50,261
and painting over it, of course, takes some time,

36
00:01:50,261 --> 00:01:51,523
but you could do just style transfer

37
00:01:51,523 --> 00:01:54,305
and get something like this in a matter of clicks.

38
00:01:54,305 --> 00:01:56,328
So remember I said I'm a machine learning engineer,

39
00:01:56,328 --> 00:01:58,129
which means I'm not an artist.

40
00:01:58,129 --> 00:02:00,212
Even though this is not particularly mind-blowing,

41
00:02:00,212 --> 00:02:01,913
I would not be able to create this myself.

42
00:02:01,913 --> 00:02:05,757
And I would be able to create it running a machinery model,

43
00:02:05,757 --> 00:02:08,520
but in this case, I didn't even have to run

44
00:02:08,520 --> 00:02:09,581
that model myself.

45
00:02:10,089 --> 00:02:11,890
So I intentionally show this.

46
00:02:11,890 --> 00:02:13,991
There's a couple things I want to highlight.

47
00:02:13,991 --> 00:02:15,171
It's not production quality, right?

48
00:02:15,171 --> 00:02:17,552
There's some artifacts you can tell.

49
00:02:17,552 --> 00:02:20,033
But here we're just applying Starry Night from Van Gogh,

50
00:02:20,033 --> 00:02:23,635
which you may have seen similar examples on San Francisco,

51
00:02:23,635 --> 00:02:24,956
which is probably familiar to you.

52
00:02:24,956 --> 00:02:27,157
At the bottom right, you see watermark.

53
00:02:27,157 --> 00:02:28,217
It says deepart.io.

54
00:02:28,217 --> 00:02:31,859
So the author's original paper created a site

55
00:02:31,859 --> 00:02:33,840
where you can upload your own image,

56
00:02:33,840 --> 00:02:36,461
your own style, select from existing styles, apply it.

57
00:02:37,199 --> 00:02:41,842
All you need is a browser and a handful of minutes.

58
00:02:41,842 --> 00:02:45,064
It took me, I don't know, a few clicks and maybe three minutes

59
00:02:45,064 --> 00:02:47,245
to create this example.

60
00:02:47,245 --> 00:02:50,587
And if you want higher resolution,

61
00:02:50,587 --> 00:02:52,088
you also have that option.

62
00:02:52,088 --> 00:02:54,189
You can pay to make your images private, et cetera.

63
00:02:54,189 --> 00:02:57,091
So it's just one of the examples of the cheap tools

64
00:02:57,091 --> 00:02:58,732
that are available to check it out.

65
00:02:58,732 --> 00:03:01,654
And of course, you might be interested in applying this to

66
00:03:02,798 --> 00:03:05,119
to video, so let's take a quick look.

67
00:03:05,119 --> 00:03:06,660
We have a simple video of a beach,

68
00:03:06,660 --> 00:03:09,000
you have waves going from left to right,

69
00:03:09,000 --> 00:03:11,561
someone swimming on the left side,

70
00:03:11,561 --> 00:03:14,702
and you might also be interested in seeing what

71
00:03:14,702 --> 00:03:15,782
it might look to apply a style,

72
00:03:15,782 --> 00:03:17,183
so let's get the same style,

73
00:03:17,183 --> 00:03:19,783
Stereo Nights from Van Gogh, and apply it.

74
00:03:19,783 --> 00:03:21,724
And then you get something that looks like this.

75
00:03:21,724 --> 00:03:25,945
Again, it doesn't have to necessarily be production ready.

76
00:03:25,945 --> 00:03:28,526
You're the artist, you decide when it's ready,

77
00:03:28,526 --> 00:03:30,967
but it might help you explore a lot more quickly.

78
00:03:33,097 --> 00:03:35,739
In particular, this example was created using

79
00:03:35,739 --> 00:03:36,479
10 lines of Python code.

80
00:03:36,479 --> 00:03:38,900
You don't even need to run the model yourself.

81
00:03:38,900 --> 00:03:41,682
This one was intentionally created using Algorithmia,

82
00:03:41,682 --> 00:03:46,104
which is a sort of a platform for hosting

83
00:03:46,104 --> 00:03:48,346
machine learning algorithms.

84
00:03:48,346 --> 00:03:50,887
It's really a couple lines of Python code,

85
00:03:50,887 --> 00:03:54,609
and then off you go, and just wait for a couple minutes.

86
00:03:55,700 --> 00:03:58,362
So, then, both of these examples,

87
00:03:58,362 --> 00:04:01,384
the style was applied to the entire image.

88
00:04:01,384 --> 00:04:04,346
So, how about we want to control a little bit more

89
00:04:04,346 --> 00:04:05,827
what the style is applied to,

90
00:04:05,827 --> 00:04:07,007
using mask or something like this.

91
00:04:07,007 --> 00:04:10,309
Someone had this idea, Alex J.C.,

92
00:04:10,309 --> 00:04:11,470
some of you might know him,

93
00:04:11,470 --> 00:04:12,851
he's not in GDC this year, I believe,

94
00:04:12,851 --> 00:04:16,013
but he proposed something called Neural Doodle.

95
00:04:17,041 --> 00:04:19,302
And I won't go into the details,

96
00:04:19,302 --> 00:04:22,122
because the intention here is not to do a deep dive

97
00:04:22,122 --> 00:04:24,103
into the architectures and the techniques

98
00:04:24,103 --> 00:04:25,203
and loss functions and things like this

99
00:04:25,203 --> 00:04:28,364
that are familiar for machinery engineers like myself.

100
00:04:28,364 --> 00:04:30,164
The intent is to show you examples

101
00:04:30,164 --> 00:04:32,745
so you get the idea of what things are possible.

102
00:04:32,745 --> 00:04:34,725
And this is just a sample.

103
00:04:34,725 --> 00:04:37,166
There's a lot more where this came from.

104
00:04:37,783 --> 00:04:40,164
So in particular, the idea of neural rules

105
00:04:40,164 --> 00:04:44,526
is to control a little bit more what the style is applied to,

106
00:04:44,526 --> 00:04:48,327
as opposed to applying it to the entire content.

107
00:04:48,327 --> 00:04:51,868
So this is some quick thing that we put together at a Hack Week

108
00:04:51,868 --> 00:04:55,370
like about a year and a half ago.

109
00:04:55,370 --> 00:04:56,790
The paper was published in 2016.

110
00:04:56,790 --> 00:05:00,231
And the idea is that then I use a shape

111
00:05:00,231 --> 00:05:02,452
to decide what the style is applied to.

112
00:05:02,452 --> 00:05:04,073
So the style comes from the left side.

113
00:05:04,940 --> 00:05:09,703
the shape just controls what in the output image

114
00:05:09,703 --> 00:05:13,506
is the style gonna look like.

115
00:05:14,369 --> 00:05:18,412
So you could rearrange, essentially, even a piece of art.

116
00:05:18,412 --> 00:05:21,214
We're using a single style here for simplicity of the demo,

117
00:05:21,214 --> 00:05:23,216
but here you see how it actually works.

118
00:05:23,216 --> 00:05:24,617
So if you have the style on the top left,

119
00:05:24,617 --> 00:05:28,060
the bottom left, you just have a guide or a mask

120
00:05:28,060 --> 00:05:30,602
that decides where to extract the style from.

121
00:05:30,602 --> 00:05:33,524
Then you draw what you want the shape to look like,

122
00:05:33,524 --> 00:05:35,766
and then the output has the shape

123
00:05:35,766 --> 00:05:36,767
and the style that you wanted.

124
00:05:37,607 --> 00:05:40,088
and then you can redraw it if you don't like that shape.

125
00:05:40,088 --> 00:05:42,149
You get different looking trees,

126
00:05:42,149 --> 00:05:44,971
and then you can decide whatever you wanna do

127
00:05:44,971 --> 00:05:46,592
with that content.

128
00:05:46,592 --> 00:05:49,373
In this case, it's just generating images.

129
00:05:49,373 --> 00:05:53,895
But the point is, keep the artist in control.

130
00:05:53,895 --> 00:05:56,436
These are just tools that are learning

131
00:05:56,436 --> 00:05:59,178
how to interpret images.

132
00:05:59,178 --> 00:06:03,039
But you're the driver.

133
00:06:03,039 --> 00:06:05,901
So this was just three examples of style transfer.

134
00:06:07,201 --> 00:06:11,884
And I'm gonna give you here a couple of references

135
00:06:11,884 --> 00:06:14,526
so I can move off to the next set of examples, yeah?

136
00:06:14,526 --> 00:06:17,768
So at the top here, we have the first paper.

137
00:06:17,768 --> 00:06:19,429
The first two are the papers

138
00:06:19,429 --> 00:06:22,591
for the machine learning folks in the audience.

139
00:06:22,591 --> 00:06:24,733
That's what you wanna read if you haven't already.

140
00:06:24,733 --> 00:06:28,755
Then deepheart.io is a site created

141
00:06:28,755 --> 00:06:29,996
by the authors of the first paper

142
00:06:29,996 --> 00:06:32,037
where you can upload your own style and your own image.

143
00:06:32,770 --> 00:06:34,691
Right below it is Neural Doodle.

144
00:06:34,691 --> 00:06:36,451
It's a GitHub repo, open source.

145
00:06:36,451 --> 00:06:36,972
You can grab it.

146
00:06:36,972 --> 00:06:39,053
That's the one we used to create that little demo

147
00:06:39,053 --> 00:06:41,594
with the potato and so on.

148
00:06:41,594 --> 00:06:42,954
I put the license here next to it.

149
00:06:42,954 --> 00:06:46,296
In my experience, the gaming industry

150
00:06:46,296 --> 00:06:48,857
is not super familiar with open source licenses.

151
00:06:48,857 --> 00:06:52,158
In this particular case, what you need to know is that AGP-LB3

152
00:06:52,158 --> 00:06:54,319
means the code is totally there.

153
00:06:54,319 --> 00:06:55,820
You can use it.

154
00:06:55,820 --> 00:06:57,281
You might not want to release something

155
00:06:57,281 --> 00:07:00,542
that uses that code because this license would require that you

156
00:07:00,542 --> 00:07:01,763
also open source your code.

157
00:07:02,305 --> 00:07:04,907
But at least you can just grab it and explore it,

158
00:07:04,907 --> 00:07:07,348
just like we did for that little demo that I just showed you.

159
00:07:07,348 --> 00:07:11,611
And at the bottom is the site that I mentioned earlier,

160
00:07:11,611 --> 00:07:12,852
the Algorithmia.

161
00:07:12,852 --> 00:07:14,273
It's kind of like a marketplace

162
00:07:14,273 --> 00:07:15,653
for machine learning algorithms.

163
00:07:15,653 --> 00:07:18,735
And they have plenty of demos and simple,

164
00:07:18,735 --> 00:07:21,197
like sample Python code that you can run yourself.

165
00:07:21,197 --> 00:07:23,258
And that's the one that was used

166
00:07:23,258 --> 00:07:27,881
to create the video style transfer.

167
00:07:27,881 --> 00:07:30,682
So let's move on to another set of examples.

168
00:07:31,805 --> 00:07:37,767
I want to see quickly if you can think

169
00:07:37,767 --> 00:07:40,869
of what these people have in common.

170
00:07:40,869 --> 00:07:44,571
And if you look, well it's not gender,

171
00:07:44,571 --> 00:07:48,352
or age, or the hair, or facial expression,

172
00:07:48,352 --> 00:07:51,254
or clothing, or colors of eyes or skin,

173
00:07:51,254 --> 00:07:53,295
or background, so what is it that they have in common?

174
00:07:53,295 --> 00:07:54,875
Someone I think knows.

175
00:07:55,309 --> 00:07:57,130
They're not real.

176
00:07:57,130 --> 00:07:58,672
They never existed.

177
00:07:58,672 --> 00:08:02,817
These are completely generated by a model.

178
00:08:02,817 --> 00:08:06,420
Which I think is pretty interesting.

179
00:08:06,420 --> 00:08:10,245
What you could do if the models are now getting to the point

180
00:08:10,245 --> 00:08:12,287
that are generating images that look very real,

181
00:08:12,287 --> 00:08:14,529
like it's very, very hard to tell

182
00:08:14,529 --> 00:08:16,531
just from looking at the pixels that this is not real.

183
00:08:19,046 --> 00:08:22,730
In particular, this is generated by a class of models

184
00:08:22,730 --> 00:08:25,593
called Generative Adversarial Networks.

185
00:08:25,593 --> 00:08:28,916
The machine learning people in the group

186
00:08:28,916 --> 00:08:31,398
are probably familiar with it.

187
00:08:31,398 --> 00:08:33,841
Javier here from EA had a fantastic talk

188
00:08:33,841 --> 00:08:36,583
on Tuesday afternoon, check it out, on the vault,

189
00:08:36,583 --> 00:08:38,946
explaining how generative models in general work

190
00:08:38,946 --> 00:08:41,888
if you're interested in the math.

191
00:08:43,910 --> 00:08:46,510
want you to think at high level is that many people

192
00:08:46,510 --> 00:08:48,091
are familiar with how machine learning models

193
00:08:48,091 --> 00:08:50,792
can classify images, for example.

194
00:08:50,792 --> 00:08:53,593
So think of a model that, given a face,

195
00:08:53,593 --> 00:08:56,413
can tell whether or not this is actually a face.

196
00:08:56,413 --> 00:09:00,875
And then the insight here, 2013 or so,

197
00:09:00,875 --> 00:09:03,936
was what if I use that information,

198
00:09:03,936 --> 00:09:08,517
whether the model believes this is real,

199
00:09:08,517 --> 00:09:10,738
to automate the process of showing another model

200
00:09:11,763 --> 00:09:14,443
whether the image it's generating is real or not real.

201
00:09:14,443 --> 00:09:16,364
So you have now two models,

202
00:09:16,364 --> 00:09:18,444
that's where the adversarial word comes from.

203
00:09:18,444 --> 00:09:22,325
And it's kind of like these models are playing a Minimax game

204
00:09:22,325 --> 00:09:23,626
or competing against each other.

205
00:09:23,626 --> 00:09:27,127
What matters to us here is that the generator

206
00:09:27,127 --> 00:09:29,567
can get very good at generating the content

207
00:09:29,567 --> 00:09:32,688
that a traditional machine learning model

208
00:09:32,688 --> 00:09:35,309
knows how to tell apart whether it's real or fake.

209
00:09:35,309 --> 00:09:37,409
Now, in that,

210
00:09:37,922 --> 00:09:39,843
example that I had earlier, just faces,

211
00:09:39,843 --> 00:09:44,184
the generator just learns the space of human faces.

212
00:09:44,184 --> 00:09:47,905
But for an artist, it might not be enough,

213
00:09:47,905 --> 00:09:51,306
because you want to control what the face looks like,

214
00:09:51,306 --> 00:09:52,286
how you're gonna use it, depending on

215
00:09:52,286 --> 00:09:52,986
where you're gonna use it.

216
00:09:52,986 --> 00:09:57,488
So, one technique, it's a clever,

217
00:09:57,488 --> 00:10:00,468
simple from a machine learning point of view,

218
00:10:00,468 --> 00:10:03,049
but it's a clever technique that I came across

219
00:10:03,049 --> 00:10:03,809
some six months ago or so.

220
00:10:05,743 --> 00:10:08,825
It brings that idea of trying to provide more control

221
00:10:08,825 --> 00:10:11,528
to the human in what other space is going to be generated.

222
00:10:11,528 --> 00:10:14,170
For the machine learning people in the audience,

223
00:10:14,170 --> 00:10:16,492
it's a very simple linear model that

224
00:10:16,492 --> 00:10:18,634
learns the mapping from the input to the generator

225
00:10:18,634 --> 00:10:22,937
into the output of a multi-level classifier that takes

226
00:10:22,937 --> 00:10:24,078
the output of the generator.

227
00:10:24,078 --> 00:10:26,721
If that didn't make sense, it doesn't matter.

228
00:10:26,721 --> 00:10:29,863
I have a short video to show you what the user experience might

229
00:10:29,863 --> 00:10:32,125
look like for something like this.

230
00:10:33,783 --> 00:10:35,644
The model knows how to generate faces.

231
00:10:35,644 --> 00:10:38,325
And what you might get is sliders or a tool like this,

232
00:10:38,325 --> 00:10:40,266
where you can individually control

233
00:10:40,266 --> 00:10:43,067
what each of the attributes of the face.

234
00:10:43,067 --> 00:10:44,848
Do you want more beard or less beard?

235
00:10:44,848 --> 00:10:46,489
Or maybe blonde hair?

236
00:10:46,489 --> 00:10:48,830
Generate faces at random, which the model obviously

237
00:10:48,830 --> 00:10:49,570
knows how to do.

238
00:10:49,570 --> 00:10:51,311
You might want more hair.

239
00:10:51,311 --> 00:10:53,392
You might want younger or older looking face.

240
00:10:53,392 --> 00:10:54,673
You might want bangs.

241
00:10:54,673 --> 00:10:54,993
Who knows?

242
00:10:54,993 --> 00:10:57,854
It's up to you to create it.

243
00:10:58,315 --> 00:11:00,316
And again, for the machine learning audience,

244
00:11:00,316 --> 00:11:03,078
one very cool thing about this technique

245
00:11:03,078 --> 00:11:06,881
is it does not require that the generator is retrained.

246
00:11:06,881 --> 00:11:08,643
Anyone that has worked with GANs,

247
00:11:08,643 --> 00:11:11,145
the hardest part is training the generator, of course.

248
00:11:11,145 --> 00:11:15,268
This doesn't even require that the generator is retrained.

249
00:11:15,268 --> 00:11:17,469
So you can grab an off-the-shelf, pre-trained generator,

250
00:11:17,469 --> 00:11:19,111
in particular, in this case,

251
00:11:19,111 --> 00:11:21,533
it's a progressive GAN by NVIDIA,

252
00:11:21,533 --> 00:11:23,294
and just learn that mapping.

253
00:11:23,294 --> 00:11:25,235
And then you can build a tool like this,

254
00:11:25,235 --> 00:11:27,377
which I think is pretty interesting.

255
00:11:28,285 --> 00:11:31,087
I'll give you references all together in a couple of slides.

256
00:11:31,087 --> 00:11:36,152
Let's look at another example of content generation.

257
00:11:36,152 --> 00:11:39,015
This particular one is called image translation.

258
00:11:39,015 --> 00:11:42,298
The original authors called it image-to-image translation.

259
00:11:42,298 --> 00:11:48,343
In particular, I'm talking about the Pix2Pix model for those mission learning in the audience.

260
00:11:48,343 --> 00:11:49,264
The idea is that...

261
00:11:50,017 --> 00:11:53,518
The generative models that I showed you before

262
00:11:53,518 --> 00:11:56,880
learn a space, and then with the trick that I mentioned,

263
00:11:56,880 --> 00:11:59,221
you might get some control on attributes,

264
00:11:59,221 --> 00:12:04,244
but what about instead of turning something

265
00:12:04,244 --> 00:12:07,266
somewhat random into a face,

266
00:12:07,266 --> 00:12:09,627
get an image and turn it into a different image?

267
00:12:09,627 --> 00:12:11,308
That's what they call image translation.

268
00:12:11,308 --> 00:12:13,669
I won't go into details of how that works,

269
00:12:13,669 --> 00:12:16,671
but what I want you to take away from this is that

270
00:12:17,515 --> 00:12:22,219
As long as you can think and produce what this input-output pair should look like,

271
00:12:22,219 --> 00:12:25,322
you can train a model that does this and it's the same architecture.

272
00:12:25,322 --> 00:12:29,065
And on the site that I'll give you the reference, there are multiple examples.

273
00:12:29,065 --> 00:12:30,066
So I'm going to show you one of them.

274
00:12:30,066 --> 00:12:32,768
This is something I created in my browser.

275
00:12:32,768 --> 00:12:33,829
This is running in real time.

276
00:12:34,670 --> 00:12:36,970
And again, it's not a production tool, right?

277
00:12:36,970 --> 00:12:38,851
It's a tool that might allow you to explore.

278
00:12:38,851 --> 00:12:40,251
So here I'm just creating facades.

279
00:12:40,251 --> 00:12:42,812
And doing very, it's a very, very simple UI

280
00:12:42,812 --> 00:12:45,653
where I'm just selecting different colors for blocks.

281
00:12:45,653 --> 00:12:48,874
I might select doors and I might want to create an entrance

282
00:12:48,874 --> 00:12:50,654
or I might want to create windows.

283
00:12:50,654 --> 00:12:53,415
So running 100% in my browser in real time,

284
00:12:53,415 --> 00:12:56,056
I'm just selecting where I might want a window

285
00:12:56,056 --> 00:12:58,857
and whether I want to add some window sill

286
00:12:58,857 --> 00:13:02,798
or a trim or some shutters like a curtain.

287
00:13:03,679 --> 00:13:06,721
and every time I want, I can generate a new facade,

288
00:13:06,721 --> 00:13:08,762
and it starts looking like a facade.

289
00:13:08,762 --> 00:13:10,442
You're not gonna put this directly in the game,

290
00:13:10,442 --> 00:13:12,903
but it might allow you to create things,

291
00:13:12,903 --> 00:13:16,565
to prototype quickly, and it takes literally seconds

292
00:13:16,565 --> 00:13:18,646
to do something like this.

293
00:13:19,491 --> 00:13:21,511
And then you can iterate as much as you want.

294
00:13:21,511 --> 00:13:22,952
You can generate random examples.

295
00:13:22,952 --> 00:13:25,732
You can add more windows, add a third floor,

296
00:13:25,732 --> 00:13:27,893
add a balcony, and then reprocess.

297
00:13:27,893 --> 00:13:31,514
And at the end, you might end up with something

298
00:13:31,514 --> 00:13:33,434
that actually kind of looks like a facade

299
00:13:33,434 --> 00:13:35,475
from, I don't know, maybe 100 years ago,

300
00:13:35,475 --> 00:13:36,915
which might be what you're after.

301
00:13:36,915 --> 00:13:40,056
And this model was trained with facades.

302
00:13:40,056 --> 00:13:44,357
But if you can generate this input-output pairs,

303
00:13:44,357 --> 00:13:47,698
then you can just, you know, get a machine learning friend

304
00:13:47,698 --> 00:13:48,818
and it will train it for you.

305
00:13:49,441 --> 00:13:53,564
In this site, in particular, there's also examples

306
00:13:53,564 --> 00:13:57,666
about going from sketches to cats, which you may have seen.

307
00:13:57,666 --> 00:14:01,249
What matters is that you're translating an image

308
00:14:01,249 --> 00:14:01,989
into another image.

309
00:14:01,989 --> 00:14:06,813
I have one more example in terms of content generation,

310
00:14:06,813 --> 00:14:08,894
which I think is relevant for gaming.

311
00:14:09,668 --> 00:14:12,290
In machine learning, we call it super resolution.

312
00:14:12,290 --> 00:14:15,492
In gaming, the closest, I mean, it's kind of like remastering.

313
00:14:15,492 --> 00:14:18,474
So many people probably have enjoyed remastered games,

314
00:14:18,474 --> 00:14:22,717
like The Last of Us comes to mind, Spyro more recently.

315
00:14:22,717 --> 00:14:26,540
Resident Evil is kind of more than a remaster,

316
00:14:26,540 --> 00:14:27,861
but you get the idea, right?

317
00:14:27,861 --> 00:14:30,803
Some old game that you want to kind of adapt it

318
00:14:30,803 --> 00:14:32,605
to a newer platform, better resolution, et cetera.

319
00:14:32,605 --> 00:14:35,607
So one of the things that needs to happen

320
00:14:35,607 --> 00:14:37,428
is that the materials need to kind of.

321
00:14:38,243 --> 00:14:40,544
now grow and need to become bigger,

322
00:14:40,544 --> 00:14:42,625
but a simple bicubic interpolation is very naive

323
00:14:42,625 --> 00:14:43,805
and it doesn't work well.

324
00:14:43,805 --> 00:14:45,886
So if I grab a random texture from the Ass Store,

325
00:14:45,886 --> 00:14:51,427
and I stretch it four times, it looks something like this.

326
00:14:51,427 --> 00:14:55,468
This might be a little bit hard to see from the back.

327
00:14:55,468 --> 00:14:56,649
So I'm gonna try to do back and forth

328
00:14:56,649 --> 00:14:58,289
and hopefully you can see it.

329
00:14:58,289 --> 00:15:01,850
If I stretch this image four times,

330
00:15:01,850 --> 00:15:03,831
you see the pixels, you see the artifacts

331
00:15:03,831 --> 00:15:06,071
generated by the bicubic interpolation.

332
00:15:07,255 --> 00:15:12,397
If instead we use, in particular this model is called ESRGAN,

333
00:15:12,397 --> 00:15:15,318
I came across it last year at some point.

334
00:15:15,318 --> 00:15:20,380
You can then teach the model what an image looks like.

335
00:15:20,380 --> 00:15:24,081
This model wasn't particularly trained on textures in the Asset Store.

336
00:15:24,081 --> 00:15:27,483
This model was just trained on a large collection of random images

337
00:15:27,483 --> 00:15:28,903
from the Cocoa Data Center.

338
00:15:30,174 --> 00:15:33,396
Then you can ask it to output not the same image,

339
00:15:33,396 --> 00:15:36,197
but a larger version of the same image that you show as input.

340
00:15:36,197 --> 00:15:42,021
If you do that, let's see if you can tell the difference.

341
00:15:42,021 --> 00:15:44,322
So the second image is the output of the model.

342
00:15:44,322 --> 00:15:46,923
So I'm going to go back and forth.

343
00:15:46,923 --> 00:15:50,565
Bicubic interpolation, ESRGAN.

344
00:15:50,565 --> 00:15:53,747
Bicubic interpolation, ESRGAN.

345
00:15:53,747 --> 00:15:55,568
And it's not specific to textures,

346
00:15:55,568 --> 00:15:59,470
so you can apply it to some random character.

347
00:16:01,180 --> 00:16:03,261
Jeremy also had a talk earlier this week,

348
00:16:03,261 --> 00:16:06,123
Jeremy from Unity, on a few other examples.

349
00:16:06,123 --> 00:16:12,647
And we have even started to see examples in the wild

350
00:16:12,647 --> 00:16:16,269
of somewhat recently, someone grabbed all the textures

351
00:16:16,269 --> 00:16:19,211
from Max Payne and remastered it using just this model

352
00:16:19,211 --> 00:16:22,633
without any retraining, and just put them out there.

353
00:16:22,633 --> 00:16:25,094
I came across a couple of examples from Diablo 2.

354
00:16:25,815 --> 00:16:31,978
And again, this could be a tool where it can help you

355
00:16:31,978 --> 00:16:34,359
if you're in the process of remastering a game.

356
00:16:34,359 --> 00:16:36,660
My understanding from talking to a couple artists

357
00:16:36,660 --> 00:16:38,361
is that it's also not necessarily

358
00:16:38,361 --> 00:16:40,402
the most exciting thing to do.

359
00:16:40,402 --> 00:16:43,743
It might be tedious, and if this can help you

360
00:16:43,743 --> 00:16:47,705
get that job faster, then you can get more creative.

361
00:16:47,705 --> 00:16:50,466
So, a couple of references.

362
00:16:50,466 --> 00:16:53,748
The first one.

363
00:16:54,060 --> 00:16:55,962
is the image to image translation.

364
00:16:55,962 --> 00:16:58,085
That's the one that I use for the facade example.

365
00:16:58,085 --> 00:17:00,447
The first three are just the papers

366
00:17:00,447 --> 00:17:02,430
for the machine learning people in the audience,

367
00:17:02,430 --> 00:17:05,193
or whoever really wants to get into the math.

368
00:17:05,193 --> 00:17:08,637
The second one is the one used to generate the faces.

369
00:17:08,637 --> 00:17:10,079
The third one is for super resolution.

370
00:17:10,985 --> 00:17:13,628
So things that you can look at today,

371
00:17:13,628 --> 00:17:16,671
even if you don't have a machine learning background at all,

372
00:17:16,671 --> 00:17:17,591
this person does not exist.com,

373
00:17:17,591 --> 00:17:19,914
is where all the spaces were generated.

374
00:17:19,914 --> 00:17:23,617
It took me just, I don't know, a couple minutes.

375
00:17:23,617 --> 00:17:27,481
The first GitHub repo that's called TransparentLatinGAN

376
00:17:27,481 --> 00:17:30,023
is the one that gives you this fine-grained control

377
00:17:30,023 --> 00:17:32,785
of the attributes of the content that you're generating.

378
00:17:32,785 --> 00:17:34,087
The example was used with faces,

379
00:17:34,087 --> 00:17:37,250
but conceptually it doesn't have to be limited to faces.

380
00:17:39,247 --> 00:17:42,489
It was basically built on top of Progressive GAN,

381
00:17:42,489 --> 00:17:46,972
which is the third repo, which was created by NVIDIA.

382
00:17:46,972 --> 00:17:51,435
And then ESRGAN, so it's the second GitHub repo,

383
00:17:51,435 --> 00:17:54,017
it's for super resolution, and the pics to pics

384
00:17:54,017 --> 00:17:57,239
at the bottom with the BSD license is the image translation.

385
00:17:57,239 --> 00:17:59,741
Sorry about the order, it doesn't exactly match.

386
00:18:00,750 --> 00:18:03,452
But I put the license here intentionally.

387
00:18:03,452 --> 00:18:05,794
Remember how before I said GPLv3 and so on?

388
00:18:05,794 --> 00:18:08,156
This is MIT, Apache 2, and BST.

389
00:18:08,156 --> 00:18:11,599
All three of these licenses are open source

390
00:18:11,599 --> 00:18:13,360
and very business friendly, actually.

391
00:18:13,360 --> 00:18:16,002
You are completely free to grab this code,

392
00:18:16,002 --> 00:18:18,584
build with it, modify it, link it.

393
00:18:18,584 --> 00:18:19,985
You don't have to pay anyone anything.

394
00:18:19,985 --> 00:18:23,928
All you have to do is give credit to the creator,

395
00:18:23,928 --> 00:18:25,009
like mention somewhere.

396
00:18:25,793 --> 00:18:29,095
The leafy licenses have a different way

397
00:18:29,095 --> 00:18:31,797
suggesting how to do this, but you can use this.

398
00:18:31,797 --> 00:18:35,459
This is not commercial software, you can just grab it,

399
00:18:35,459 --> 00:18:40,062
go look at the code, if you wanna run it, you can do it.

400
00:18:40,062 --> 00:18:43,744
So I wanna look at a third class of examples,

401
00:18:43,744 --> 00:18:45,305
which is animation.

402
00:18:47,430 --> 00:18:51,332
Animation is harder than the other two classes

403
00:18:51,332 --> 00:18:52,473
that I've shown you.

404
00:18:52,473 --> 00:18:57,356
I've seen a couple of good talks related to animation

405
00:18:57,356 --> 00:19:01,498
this year at UDC, which makes me happy.

406
00:19:01,498 --> 00:19:04,439
So let's just look at the first example.

407
00:19:04,439 --> 00:19:05,940
Let's look at smoke, in general,

408
00:19:05,940 --> 00:19:08,241
fluids or particle would have something similar.

409
00:19:08,241 --> 00:19:10,382
Some of you may even be familiar

410
00:19:10,382 --> 00:19:12,223
with this paper in particular.

411
00:19:12,223 --> 00:19:15,885
This is from Seagraph a couple of years ago.

412
00:19:17,062 --> 00:19:21,127
Smoke here is being simulated by a neural network, but what does that really mean?

413
00:19:21,127 --> 00:19:27,634
Today, what you typically have is some simulation that has...

414
00:19:28,564 --> 00:19:33,545
an implementation of some complex physics formula

415
00:19:33,545 --> 00:19:35,945
without getting into the details of the math.

416
00:19:35,945 --> 00:19:38,486
It somewhat limits the amount of smoke

417
00:19:38,486 --> 00:19:40,086
that you might have, the amount of particles

418
00:19:40,086 --> 00:19:41,867
that you can simulate, the amount of iterations

419
00:19:41,867 --> 00:19:43,307
that you can run the simulation for.

420
00:19:43,307 --> 00:19:47,828
So someone had the idea that because machine learning models

421
00:19:47,828 --> 00:19:49,168
can be general purpose approximators,

422
00:19:49,168 --> 00:19:52,389
and someone understood the simulator enough

423
00:19:52,389 --> 00:19:54,909
to identify the piece that was expensive,

424
00:19:54,909 --> 00:19:57,790
why don't we replace that with a machine learning model?

425
00:19:58,775 --> 00:20:00,916
This is trained completely on synthetic data,

426
00:20:00,916 --> 00:20:02,918
which means you have infinite data to train.

427
00:20:02,918 --> 00:20:06,201
So then, can we train a machine learning model

428
00:20:06,201 --> 00:20:07,802
to approximate this function?

429
00:20:07,802 --> 00:20:10,684
And the end result is that you get smoke

430
00:20:10,684 --> 00:20:14,187
that looks like this, but it's 10 times cheaper

431
00:20:14,187 --> 00:20:16,589
than what you might encounter in your engine of choice.

432
00:20:17,590 --> 00:20:20,372
These kinds of things, I think, it's just a matter of time

433
00:20:20,372 --> 00:20:26,415
where until these kinds of tools are just in the engines,

434
00:20:26,415 --> 00:20:30,457
in the tools that you can use to create your content.

435
00:20:30,457 --> 00:20:32,978
Let's look at character animation.

436
00:20:32,978 --> 00:20:36,320
So the first example we're gonna look like

437
00:20:36,320 --> 00:20:38,481
for the machine learning people in the audience

438
00:20:38,481 --> 00:20:40,622
is called PFNN.

439
00:20:40,622 --> 00:20:40,942
Fabio.

440
00:20:41,291 --> 00:20:44,032
from EA earlier this week had an excellent talk

441
00:20:44,032 --> 00:20:47,293
comparing the next two examples, coincidentally.

442
00:20:47,293 --> 00:20:51,895
Daniel Holden, who I don't think is here in the audience,

443
00:20:51,895 --> 00:20:53,896
is the author of the first example.

444
00:20:53,896 --> 00:20:56,877
The key observation here is that

445
00:20:56,877 --> 00:20:59,898
animation is very complicated,

446
00:20:59,898 --> 00:21:03,139
and can you simplify the pipeline would be our goal.

447
00:21:03,139 --> 00:21:08,041
And in particular, you can use mockup data

448
00:21:08,667 --> 00:21:11,448
And a machinery model that takes as input,

449
00:21:11,448 --> 00:21:14,429
let's say the context of the character,

450
00:21:14,429 --> 00:21:16,330
the direction of where it's going

451
00:21:16,330 --> 00:21:17,490
and where it was recently.

452
00:21:17,490 --> 00:21:20,112
And that's enough for the model to decide

453
00:21:20,112 --> 00:21:21,872
how to animate the character

454
00:21:21,872 --> 00:21:24,173
in a way that adapts to the terrain.

455
00:21:24,173 --> 00:21:26,214
There's no kind of handcrafted animation here

456
00:21:26,214 --> 00:21:27,715
for exactly where to place the hand

457
00:21:27,715 --> 00:21:31,156
or where to like slow down and scale as if they're steps.

458
00:21:31,156 --> 00:21:33,998
The model is reacting.

459
00:21:35,346 --> 00:21:37,787
to an extent, to the terrain.

460
00:21:37,787 --> 00:21:41,308
And it basically simplifies the pipeline.

461
00:21:41,308 --> 00:21:43,829
You know, I'm speaking in simple terms,

462
00:21:43,829 --> 00:21:48,511
but Daniel Holden had a talk also on exactly,

463
00:21:48,511 --> 00:21:51,311
not this paper, but newer work, more recent work

464
00:21:51,311 --> 00:21:54,233
that they have on EA that even shipped

465
00:21:54,233 --> 00:21:56,313
in Assassin's Creed Odyssey.

466
00:21:56,313 --> 00:21:56,873
Sorry, Ubisoft.

467
00:21:57,573 --> 00:22:01,316
And Fabio from EA had a talk explaining this,

468
00:22:01,316 --> 00:22:04,037
and he showed some examples in FIFA and so on.

469
00:22:04,037 --> 00:22:07,319
This is not something that is a tool

470
00:22:07,319 --> 00:22:08,500
that you can grab today.

471
00:22:08,500 --> 00:22:12,222
I won't give you an open source repo where this exists,

472
00:22:12,222 --> 00:22:15,404
but these things are coming.

473
00:22:15,404 --> 00:22:16,785
And so we would take that example

474
00:22:16,785 --> 00:22:20,287
and we extend it to quadruplets.

475
00:22:20,287 --> 00:22:21,347
With some generalization.

476
00:22:22,387 --> 00:22:25,394
there's some extra tricks, but the machine learning details

477
00:22:25,394 --> 00:22:26,937
we don't need to get into.

478
00:22:26,937 --> 00:22:30,044
Then you get something that looks like this.

479
00:22:30,044 --> 00:22:33,110
So this is from SIGGRAPH last year.

480
00:22:34,557 --> 00:22:37,598
inspired by the PFNN, which was that yellow soldier

481
00:22:37,598 --> 00:22:38,138
you saw earlier.

482
00:22:38,138 --> 00:22:40,979
And so then you get a wolf in particular,

483
00:22:40,979 --> 00:22:43,360
who's just like the soldier,

484
00:22:43,360 --> 00:22:47,021
it's just reacting to the user input.

485
00:22:47,021 --> 00:22:51,102
Based on the terrain and the speed,

486
00:22:51,102 --> 00:22:53,783
it just decides how the character should be animated.

487
00:22:53,783 --> 00:22:55,643
And it learns like, you know, jumps and

488
00:22:55,643 --> 00:22:57,724
how to go up or down and turn and stop

489
00:22:57,724 --> 00:23:00,404
in a way that looks realistic.

490
00:23:00,404 --> 00:23:02,465
Just from mockup data, pretty much.

491
00:23:02,465 --> 00:23:03,085
And,

492
00:23:05,874 --> 00:23:12,080
I want to give you one more example, which some of us know as Tidmimic.

493
00:23:12,080 --> 00:23:13,722
This came from Berkeley last year.

494
00:23:13,722 --> 00:23:21,509
And it's also related to animation, but in particular it's related to physics-based animation.

495
00:23:21,509 --> 00:23:25,653
And the key observation here is that...

496
00:23:26,597 --> 00:23:30,138
If the model can learn a behavior,

497
00:23:30,138 --> 00:23:33,820
but not by stitching mockup data together,

498
00:23:33,820 --> 00:23:38,383
but by learning what force to apply to the rig,

499
00:23:38,383 --> 00:23:41,204
how to move the muscles, if you will,

500
00:23:41,204 --> 00:23:45,086
then that allows the model to react to changes

501
00:23:45,086 --> 00:23:48,048
in the environment, including obstacles or projectiles

502
00:23:48,048 --> 00:23:50,649
or things that could be thrown at the model.

503
00:23:50,649 --> 00:23:51,970
So using either

504
00:23:53,445 --> 00:23:56,766
kind of manually animated character or mockup data,

505
00:23:56,766 --> 00:24:01,907
you can have a model that learns how to imitate that,

506
00:24:01,907 --> 00:24:05,729
hence the mimic in the name.

507
00:24:05,729 --> 00:24:08,369
And then you put this character out there

508
00:24:08,369 --> 00:24:10,790
and affect its environment.

509
00:24:10,790 --> 00:24:12,231
So let's take a look at what that looks like.

510
00:24:12,231 --> 00:24:13,091
Here.

511
00:24:14,380 --> 00:24:17,162
here with dragon, animated by hand,

512
00:24:17,162 --> 00:24:20,864
and then this is the one learned by the model.

513
00:24:20,864 --> 00:24:23,205
So obviously you can't do mockup data for a dragon,

514
00:24:23,205 --> 00:24:25,146
but then you can start throwing things

515
00:24:25,146 --> 00:24:27,888
in front of the dragon, and the dragon adapts by itself

516
00:24:27,888 --> 00:24:29,789
in a way that looks realistic.

517
00:24:29,789 --> 00:24:32,490
Like it has to, like maybe it looks like it's turning

518
00:24:32,490 --> 00:24:33,351
to avoid the boxes.

519
00:24:33,351 --> 00:24:36,412
You can have a quadruped, in this case it's a lion,

520
00:24:36,412 --> 00:24:39,014
you could, you know, it could be a dog or whatever,

521
00:24:39,014 --> 00:24:40,374
for which you could actually have mockup data,

522
00:24:40,374 --> 00:24:41,695
and then learn.

523
00:24:43,188 --> 00:24:45,770
how to run, and then react.

524
00:24:45,770 --> 00:24:48,551
You see how it has to work to get out of the cubes.

525
00:24:48,551 --> 00:24:51,473
It can even get completely bombarded and fall

526
00:24:51,473 --> 00:24:54,135
in a way that actually looks kind of natural.

527
00:24:54,135 --> 00:24:56,477
And none of that was animated by hand.

528
00:24:56,477 --> 00:24:59,599
It learned how to apply force to the body

529
00:24:59,599 --> 00:25:02,121
so that it would actually do what it was expected to do.

530
00:25:02,121 --> 00:25:08,285
So I'm gonna give you here the references.

531
00:25:08,285 --> 00:25:10,406
Unfortunately, these are...

532
00:25:11,397 --> 00:25:17,280
Unfortunately for some of you in the audience, these are just papers. In this case, I don't have code that I can share with you.

533
00:25:17,280 --> 00:25:20,783
But I do believe this is a matter of time.

534
00:25:20,783 --> 00:25:27,547
On Tuesday, there was a machine learning talk where I learned several students.

535
00:25:28,000 --> 00:25:30,843
using machine learning for animation,

536
00:25:30,843 --> 00:25:32,165
different parts of the pipeline,

537
00:25:32,165 --> 00:25:35,448
and some of which has shipped in games,

538
00:25:35,448 --> 00:25:38,210
some of which is shipping in games.

539
00:25:38,210 --> 00:25:40,933
It's very exciting work,

540
00:25:40,933 --> 00:25:43,596
and I think it's just a matter of time.

541
00:25:43,596 --> 00:25:44,977
So with this,

542
00:25:44,977 --> 00:25:49,522
one of the messages that I wanna leave with is,

543
00:25:49,522 --> 00:25:52,424
help us, machine learning people, help you artists.

544
00:25:52,957 --> 00:25:55,798
just hopefully from this you get some idea of the things

545
00:25:55,798 --> 00:25:56,819
that are going to become possible

546
00:25:56,819 --> 00:25:58,520
or that are possible today.

547
00:25:58,520 --> 00:26:01,081
And get creative on what other tools you might want,

548
00:26:01,081 --> 00:26:03,522
what are the things you want to create

549
00:26:03,522 --> 00:26:06,884
that would be very difficult to do without tools.

550
00:26:06,884 --> 00:26:09,686
And there's a lot more where this came from.

551
00:26:09,686 --> 00:26:14,889
So I think with that, we have a few minutes for questions.

552
00:26:14,889 --> 00:26:19,091
So I'd like to, I don't know, hear from you and learn from you.

553
00:26:33,835 --> 00:26:37,897
How the last part with the animation and oh great presentation by the way

554
00:26:37,897 --> 00:26:44,960
The last part with the animation where you're saying it was learning does that is that something that still takes a lot of?

555
00:26:44,960 --> 00:26:51,262
Performance to calculate or anything or is that something that's getting really close to keyframe animation great question because I should have mentioned

556
00:26:51,262 --> 00:26:53,683
And I did not so um

557
00:26:53,683 --> 00:26:57,845
This is just reinforcement learning so what it's doing is um

558
00:26:59,620 --> 00:27:03,143
Applying starts, imagine that you start with random policy

559
00:27:03,143 --> 00:27:05,445
that just applies random force to muscles.

560
00:27:05,445 --> 00:27:09,009
And it's just gonna fail to imitate the reference animation.

561
00:27:09,009 --> 00:27:12,191
And then it gets a penalty or reward.

562
00:27:12,191 --> 00:27:15,514
Over time, it learns to mimic that behavior

563
00:27:15,514 --> 00:27:19,077
by applying force to the right places at the right times.

564
00:27:19,077 --> 00:27:24,402
The process of training takes a long time.

565
00:27:24,402 --> 00:27:25,063
In particular.

566
00:27:26,022 --> 00:27:28,223
In Unity, that's one of the reasons we released

567
00:27:28,223 --> 00:27:30,304
the ML Legends framework that you may have heard of.

568
00:27:30,304 --> 00:27:32,524
It is for the purpose of reinforcement learning,

569
00:27:32,524 --> 00:27:34,145
not for the purpose of this animation,

570
00:27:34,145 --> 00:27:38,346
but for the purpose of reinforcement learning in general.

571
00:27:38,346 --> 00:27:40,547
And it takes time, it takes some compute power,

572
00:27:40,547 --> 00:27:43,168
just like many other machine learning models take.

573
00:27:43,168 --> 00:27:45,648
So we're talking hours, perhaps, yeah?

574
00:27:45,648 --> 00:27:46,929
I don't recall off the top of my head

575
00:27:46,929 --> 00:27:49,250
how long it took to train each of those.

576
00:27:49,250 --> 00:27:53,011
But once it's trained, then you can embed that

577
00:27:53,011 --> 00:27:55,572
in like a mobile device, and it can run in near real time.

578
00:27:59,522 --> 00:28:05,410
Hello, are there versions of GANs or I should say any sort of machine learning approach

579
00:28:05,410 --> 00:28:11,858
that does image translation from say something that may be a game that's trying to look

580
00:28:11,858 --> 00:28:14,421
photorealistic into actual photorealism?

581
00:28:15,790 --> 00:28:18,451
So like a celebrity GAN is an example

582
00:28:18,451 --> 00:28:20,933
where you can make a face and then make it look

583
00:28:20,933 --> 00:28:21,693
like a human face.

584
00:28:21,693 --> 00:28:25,095
Are there also GANs like that for say,

585
00:28:25,095 --> 00:28:27,957
structures or real world or like?

586
00:28:27,957 --> 00:28:30,719
I'm not aware of something,

587
00:28:30,719 --> 00:28:32,520
so just to make sure that the question's,

588
00:28:32,520 --> 00:28:35,402
it's like are there GANs that can be applied

589
00:28:35,402 --> 00:28:38,543
to examples being terrain or content in games

590
00:28:38,543 --> 00:28:41,445
to make it look like hyper-realistic almost?

591
00:28:41,445 --> 00:28:41,505
Yes.

592
00:28:43,532 --> 00:28:48,198
I can think of proxies of things that might be in that domain.

593
00:28:48,198 --> 00:28:51,101
Not something that you would apply directly just to, like,

594
00:28:51,101 --> 00:28:53,004
especially meshes or something like this.

595
00:28:53,004 --> 00:28:56,928
GANs have been very successful with 2D data, so with images,

596
00:28:56,928 --> 00:28:59,091
not directly with an entire scene.

597
00:28:59,091 --> 00:29:01,514
But there are.

598
00:29:02,719 --> 00:29:06,361
So obviously the super resolution is also a GAN, by the way,

599
00:29:06,361 --> 00:29:08,541
and so it can be applied to textures individually.

600
00:29:08,541 --> 00:29:10,082
There are also examples from Z-Graph

601
00:29:10,082 --> 00:29:12,683
where this has been applied to terrain, for example.

602
00:29:12,683 --> 00:29:15,504
A version of image-to-image translation

603
00:29:15,504 --> 00:29:18,205
would be from a sketch done by hand

604
00:29:18,205 --> 00:29:20,266
into something that's not a sketch,

605
00:29:20,266 --> 00:29:22,787
whatever that is, as long as it's an image.

606
00:29:22,787 --> 00:29:23,868
In that Z-Graph example,

607
00:29:24,638 --> 00:29:26,639
you can sketch some mountains,

608
00:29:26,639 --> 00:29:29,420
and then you get a full terrain,

609
00:29:29,420 --> 00:29:30,681
high map type of a situation.

610
00:29:30,681 --> 00:29:34,462
So it's more creation, it's not as much refinement,

611
00:29:34,462 --> 00:29:35,862
which is I think maybe what you're after.

612
00:29:35,862 --> 00:29:38,943
But if we talk in detail about the concrete examples,

613
00:29:38,943 --> 00:29:42,104
we might be able to, even just in a conversation,

614
00:29:42,104 --> 00:29:45,646
to determine whether or not these things apply.

615
00:29:45,646 --> 00:29:49,127
And this is the kind of conversation that I want to

616
00:29:49,127 --> 00:29:51,668
start between machine learning and artists,

617
00:29:51,668 --> 00:29:54,348
to see what problems we can help to solve, for example.

618
00:29:54,975 --> 00:29:56,036
Cool, thank you.

619
00:29:56,036 --> 00:30:01,160
Hi, great, so thank you.

620
00:30:01,160 --> 00:30:05,244
When you spoke about enlarging textures

621
00:30:05,244 --> 00:30:06,945
and training it beyond bicubic,

622
00:30:06,945 --> 00:30:10,288
do you think that that could ever become a runtime solution

623
00:30:10,288 --> 00:30:11,489
as opposed to an offline thing

624
00:30:11,489 --> 00:30:14,492
to actually be optimized well enough without?

625
00:30:14,492 --> 00:30:19,597
So I can say that based on my experiments,

626
00:30:19,597 --> 00:30:21,238
it's not real-time today.

627
00:30:21,947 --> 00:30:27,050
But it takes, let's say, in the order of a second

628
00:30:27,050 --> 00:30:29,152
to get a 4x or an 8x outpress.

629
00:30:29,152 --> 00:30:33,814
I do know of people doing research,

630
00:30:33,814 --> 00:30:37,937
and particularly I've talked to people at places like LG,

631
00:30:37,937 --> 00:30:41,379
for example, that are interested in ultimately doing this

632
00:30:41,379 --> 00:30:42,820
in near real time on device.

633
00:30:42,927 --> 00:30:44,448
when you don't have 4K content,

634
00:30:44,448 --> 00:30:48,291
you want to do something better than just stretching that.

635
00:30:48,291 --> 00:30:50,773
I was just thinking you could take shadow map density

636
00:30:50,773 --> 00:30:53,135
and every element of the game beyond it,

637
00:30:53,135 --> 00:30:55,496
just rather than just underlying textures.

638
00:30:55,496 --> 00:30:57,778
And we're not there today.

639
00:30:57,778 --> 00:31:01,200
It might be a matter of time.

640
00:31:01,200 --> 00:31:03,422
Many of these things are possible

641
00:31:03,422 --> 00:31:04,783
because hardware has gotten better.

642
00:31:04,783 --> 00:31:06,324
There's obviously improvements in the algorithms,

643
00:31:06,324 --> 00:31:08,706
but hardware has gotten significantly better.

644
00:31:09,745 --> 00:31:11,146
To run something like that in a second,

645
00:31:11,146 --> 00:31:12,067
you still need a decent GPU.

646
00:31:12,067 --> 00:31:15,189
So it's not something that you can do today on every frame.

647
00:31:15,189 --> 00:31:19,593
Some of these things might become even more viable

648
00:31:19,593 --> 00:31:23,996
as more game content is streamed from the cloud, for example.

649
00:31:23,996 --> 00:31:26,258
Because that provides us, machine learning people,

650
00:31:26,258 --> 00:31:27,499
a very controlled environment where

651
00:31:27,499 --> 00:31:30,041
we don't have to worry as much about the cross-platform

652
00:31:30,041 --> 00:31:33,504
and the different levels of hardware availability.

653
00:31:33,504 --> 00:31:35,025
And you reduce the latency between where

654
00:31:35,025 --> 00:31:37,387
the game is being rendered and where the machine learning is

655
00:31:37,387 --> 00:31:38,608
happening.

656
00:31:39,306 --> 00:31:42,552
but the short answer is not today.

657
00:31:42,552 --> 00:31:44,477
Once per second kind of thing.

658
00:31:44,477 --> 00:31:47,663
Awesome, thank you.

659
00:31:47,663 --> 00:31:49,588
Alright, I think we're done with time anyways,

660
00:31:49,588 --> 00:31:50,249
so thank you.

