1
00:00:06,673 --> 00:00:08,915
So I'm Magnus Nordin.

2
00:00:08,935 --> 00:00:10,436
I'm technical director at Seed.

3
00:00:11,097 --> 00:00:16,522
And Seed is an R&D department of electronic arts.

4
00:00:16,762 --> 00:00:19,344
And we try to look a bit further than the typical game

5
00:00:19,384 --> 00:00:19,945
team can.

6
00:00:20,265 --> 00:00:22,407
They usually have a horizon of one, two, or three years.

7
00:00:22,687 --> 00:00:24,429
We can look three to five years, even.

8
00:00:26,174 --> 00:00:29,656
And we do a lot of game AI, deep learning.

9
00:00:29,716 --> 00:00:34,278
We also do rendering, graphics, virtual humans, avatars.

10
00:00:35,478 --> 00:00:38,280
And we also have a group that builds prototypes

11
00:00:38,320 --> 00:00:40,000
and new game experiences.

12
00:00:42,381 --> 00:00:46,023
And I will talk about deep learning beyond the hype today.

13
00:00:46,303 --> 00:00:49,765
And first, I'll show you a few use cases of deep learning.

14
00:00:50,785 --> 00:00:52,346
And then some possibilities.

15
00:00:52,406 --> 00:00:53,927
This will be the sci-fi part.

16
00:00:54,827 --> 00:00:54,927
But.

17
00:00:55,624 --> 00:00:58,385
what we probably can do in the future that we can't do today.

18
00:00:58,906 --> 00:01:01,507
Then I'll deep dive into deep reinforcement learning and

19
00:01:01,547 --> 00:01:02,427
look at some results.

20
00:01:03,007 --> 00:01:05,468
Because without results, it would still only be hype.

21
00:01:07,029 --> 00:01:09,670
And then we'll talk about some of the difficulties of doing

22
00:01:09,710 --> 00:01:12,552
this deep reinforcement learning and why it's worth

23
00:01:12,592 --> 00:01:13,272
doing it anyway.

24
00:01:16,853 --> 00:01:19,835
And yeah, one sign of the hype, I guess, is that I'm

25
00:01:19,875 --> 00:01:22,216
required to have this disclaimer now.

26
00:01:22,476 --> 00:01:24,057
It's for the first time in the presentation.

27
00:01:25,185 --> 00:01:29,389
but it says that we are doing, it's a safe AI.

28
00:01:32,432 --> 00:01:33,653
Okay, so neural networks.

29
00:01:34,314 --> 00:01:36,556
Deep learning is neural networks, deep neural networks.

30
00:01:37,777 --> 00:01:39,859
It's just a function estimator, really.

31
00:01:40,239 --> 00:01:43,202
It can take any input and produce almost any other output.

32
00:01:43,222 --> 00:01:47,266
So in this case, it's 10 million pixels, and the output is the label cat.

33
00:01:48,500 --> 00:01:51,441
And the nice thing, this of course is a very complex function.

34
00:01:51,862 --> 00:01:57,184
The nice thing with this is that we don't have to define this function ourselves.

35
00:01:57,404 --> 00:01:58,805
It's trainable.

36
00:01:59,405 --> 00:02:00,926
So we train the function instead.

37
00:02:02,427 --> 00:02:04,728
And that's why it's called learning.

38
00:02:07,710 --> 00:02:09,611
We can get more elaborate computer vision.

39
00:02:09,731 --> 00:02:11,372
We get the description of a picture instead.

40
00:02:11,452 --> 00:02:16,955
And then we use one computer vision network and another language generation network.

41
00:02:18,055 --> 00:02:19,176
We can recognize voice.

42
00:02:20,458 --> 00:02:21,499
We can generate voice.

43
00:02:22,500 --> 00:02:24,141
We can create music.

44
00:02:26,163 --> 00:02:29,727
We can generate images from descriptions, in this case.

45
00:02:30,627 --> 00:02:31,648
So the text is the input.

46
00:02:31,668 --> 00:02:32,549
The image is the output.

47
00:02:34,411 --> 00:02:36,493
And we can play games, which is, of course, the most

48
00:02:36,533 --> 00:02:37,714
important part for us.

49
00:02:39,732 --> 00:02:41,853
So let's look at some use cases.

50
00:02:42,394 --> 00:02:44,135
The first one is, like I said, playing games.

51
00:02:44,195 --> 00:02:45,976
We can play board games like AlphaGo did.

52
00:02:46,176 --> 00:02:47,177
And I'll come back to this.

53
00:02:47,617 --> 00:02:49,358
I guess most people have heard about this one.

54
00:02:50,939 --> 00:02:52,580
I'll talk a bit more later about this.

55
00:02:53,460 --> 00:03:00,284
But then we have this when OpenAI's Dota agent challenged

56
00:03:00,404 --> 00:03:01,545
one of the best players in the world.

57
00:03:02,345 --> 00:03:03,906
It's just one versus one Dota.

58
00:03:06,827 --> 00:03:12,292
So it's a limited version of Dota, but it was still very impressive that they can do this.

59
00:03:12,332 --> 00:03:17,356
Because no human has been able to program a bot without cheating, just using the same information as humans.

60
00:03:17,437 --> 00:03:20,019
And beating a human professional.

61
00:03:25,707 --> 00:03:27,188
We can also do pose estimation.

62
00:03:27,809 --> 00:03:29,171
And why is this important?

63
00:03:29,211 --> 00:03:32,714
We do this today in big capture studios, motion capture

64
00:03:32,734 --> 00:03:33,075
studios.

65
00:03:34,016 --> 00:03:36,658
But this technology will soon provide anyone

66
00:03:36,738 --> 00:03:40,282
with a mobile phone at home to do motion capture.

67
00:03:40,302 --> 00:03:41,563
Of course, the precision is not as

68
00:03:41,603 --> 00:03:43,565
high as a professional motion capture studio yet.

69
00:03:44,767 --> 00:03:45,828
But I'm sure we'll get that.

70
00:03:51,867 --> 00:03:56,151
And voice, most people know that the voice generation

71
00:03:56,171 --> 00:03:58,433
standard, especially the latest thing coming out of

72
00:03:58,473 --> 00:04:01,436
Google and Baidu and others, is amazingly good.

73
00:04:01,636 --> 00:04:05,359
So I won't actually show you any voice generation samples

74
00:04:05,400 --> 00:04:07,021
because you already know that.

75
00:04:07,482 --> 00:04:08,823
But I'll show you another interesting thing.

76
00:04:09,463 --> 00:04:10,885
We are encouraged by the news.

77
00:04:14,448 --> 00:04:15,829
We are encouraged by the news.

78
00:04:16,932 --> 00:04:19,613
So what this was, the first was the input to a network.

79
00:04:19,973 --> 00:04:22,513
The second sentence was the output of the network.

80
00:04:22,913 --> 00:04:25,774
So it converted from one voice into another voice.

81
00:04:26,474 --> 00:04:28,274
And this is, of course, very interesting for us.

82
00:04:28,394 --> 00:04:30,555
We record a lot of voice in some of our games.

83
00:04:30,595 --> 00:04:33,676
And if you want to change one line of that voice recording,

84
00:04:33,716 --> 00:04:36,276
which can be hours or hundreds of hours in some cases, of

85
00:04:36,316 --> 00:04:38,376
voice, you have to bring the actor back.

86
00:04:38,416 --> 00:04:39,497
That might not be possible.

87
00:04:39,997 --> 00:04:42,317
So what if we could have someone else speak and

88
00:04:42,437 --> 00:04:44,418
generate that actor's voice instead?

89
00:04:45,238 --> 00:04:49,320
And of course, if you have your Lord of the Rings MMO,

90
00:04:50,140 --> 00:04:53,362
you don't want Galadriel to sound like 12-year-old Bert.

91
00:04:53,642 --> 00:04:56,163
It can actually sound like Galadriel with this technique.

92
00:04:56,183 --> 00:04:57,663
So let's hear two more examples.

93
00:04:58,684 --> 00:04:59,904
Who was the mystery MP?

94
00:05:03,686 --> 00:05:04,907
Who was the mystery MP?

95
00:05:08,308 --> 00:05:09,469
It was a breathtaking moment.

96
00:05:13,290 --> 00:05:14,371
It was a breathtaking moment.

97
00:05:15,752 --> 00:05:17,433
This, by the way, is work by Google DeepMind.

98
00:05:17,473 --> 00:05:19,395
And I will mention Google DeepMind a lot,

99
00:05:19,475 --> 00:05:22,317
because they, OpenAI and NVIDIA and a few others,

100
00:05:22,357 --> 00:05:23,958
have a very good research team that

101
00:05:24,058 --> 00:05:26,200
create a lot of new, cool, deep learning stuff.

102
00:05:28,201 --> 00:05:30,102
We can also make neural networks sing.

103
00:05:30,563 --> 00:05:31,964
So the input to this neural network

104
00:05:32,044 --> 00:05:33,885
is the lyrics and the musical notes.

105
00:05:34,165 --> 00:05:35,006
And this is the output.

106
00:05:36,147 --> 00:05:37,348
Let's see.

107
00:05:37,448 --> 00:05:43,112
Like almost always, when something dies.

108
00:05:45,842 --> 00:05:53,268
Nace la nostalgia, buscando un corazÃ³n.

109
00:05:55,329 --> 00:05:59,632
More things we can do with voice is animating faces from voice.

110
00:05:59,672 --> 00:06:06,897
This work is from NVIDIA Research and to the left here we see the fully motion captured face made in a capture studio.

111
00:06:07,237 --> 00:06:13,241
To the right we have a neural network animating the face using the voice as the only input to animate the face.

112
00:06:15,267 --> 00:06:18,190
Are those Eurasian footwear cowboy chaps or jolly,

113
00:06:18,270 --> 00:06:19,351
earth-moving headgear?

114
00:06:20,912 --> 00:06:22,153
This is my reality.

115
00:06:22,814 --> 00:06:24,636
And this is the reality of my people.

116
00:06:25,817 --> 00:06:26,778
NBC GLAAD.

117
00:06:27,018 --> 00:06:27,278
Why?

118
00:06:27,819 --> 00:06:29,600
Fox TV jerks, quiz PM.

119
00:06:30,901 --> 00:06:33,283
So it's not perfect, but it's very good.

120
00:06:33,323 --> 00:06:36,206
It's better than most other voice-to-face

121
00:06:36,566 --> 00:06:37,347
solutions I've seen.

122
00:06:39,229 --> 00:06:41,351
So we also have content generation.

123
00:06:43,360 --> 00:06:44,741
procedural content generation.

124
00:06:44,901 --> 00:06:49,167
This is an example from Anastasia in the CT.

125
00:06:50,368 --> 00:06:53,592
And everything in this picture is procedurally generated

126
00:06:53,652 --> 00:06:58,257
so you can, you have parameters to change everything

127
00:06:58,318 --> 00:07:01,081
to whatever variations you want of this scene.

128
00:07:04,731 --> 00:07:10,353
And of course, we can also use the same for, and maybe that's more common, for natural things like trees.

129
00:07:11,053 --> 00:07:17,176
So in this case, Anastasia used biological-based rules to generate trees.

130
00:07:18,656 --> 00:07:20,098
like they would actually look in nature.

131
00:07:20,698 --> 00:07:22,639
And as you see, we can get a lot of variance

132
00:07:22,700 --> 00:07:25,722
and it actually looks like natural trees.

133
00:07:26,442 --> 00:07:28,044
But of course, this requires a lot of work.

134
00:07:28,644 --> 00:07:30,185
It's hard to come up with these rules

135
00:07:30,245 --> 00:07:31,686
and you also have to tweak the parameters

136
00:07:31,726 --> 00:07:33,408
to actually get the result you want.

137
00:07:34,188 --> 00:07:37,471
So what if we could train something

138
00:07:37,551 --> 00:07:40,653
to create these rules and parameters?

139
00:07:41,574 --> 00:07:44,216
So let's look at one of the best examples I've seen,

140
00:07:44,256 --> 00:07:45,557
and this is also from the same group

141
00:07:45,597 --> 00:07:47,018
that did the voice-to-face animation.

142
00:07:48,154 --> 00:07:49,575
And none of these people are real.

143
00:07:50,756 --> 00:07:54,599
They are created by a neural network that has learned the

144
00:07:54,739 --> 00:07:57,901
rules for faces by looking at 30,000 people, 30,000

145
00:07:58,422 --> 00:07:59,883
celebrities, which you can probably tell.

146
00:08:00,944 --> 00:08:04,126
But none of these images is in that set, the learning set.

147
00:08:04,166 --> 00:08:05,167
They are all new people.

148
00:08:05,968 --> 00:08:09,010
And the thing is, I said, procedural content, it's

149
00:08:09,090 --> 00:08:11,552
parameterized rules, so you can change the parameters.

150
00:08:11,652 --> 00:08:15,075
And in this case, they glide through, they interpolate

151
00:08:15,135 --> 00:08:16,976
through parameter sets of the faces.

152
00:08:19,030 --> 00:08:20,930
So this is pretty cool.

153
00:08:21,370 --> 00:08:23,911
And of course, this is 2D generation.

154
00:08:24,331 --> 00:08:26,952
And it's super impressive, but we would like 3D generation.

155
00:08:28,072 --> 00:08:32,053
And 3D generation is not really there yet, but there's

156
00:08:32,093 --> 00:08:33,193
lots of research going on.

157
00:08:34,014 --> 00:08:36,434
And I think within a couple of years, we will have useful

158
00:08:37,275 --> 00:08:40,155
3D generation using generative networks.

159
00:08:42,328 --> 00:08:44,690
But when you use procedural content generation

160
00:08:44,770 --> 00:08:48,152
to generate worlds, they are often quite lifeless,

161
00:08:48,212 --> 00:08:50,393
so you need to generate life in them.

162
00:08:51,414 --> 00:08:55,196
So emergent behavior and life

163
00:08:56,477 --> 00:08:58,698
is also something we can generate with machine learning.

164
00:08:58,738 --> 00:08:59,819
This is also from DeepMind.

165
00:08:59,979 --> 00:09:03,181
The only goal this agent has is to move forward.

166
00:09:03,721 --> 00:09:07,543
It can control the muscles in the little body.

167
00:09:08,410 --> 00:09:11,551
And it has to solve the problem to just move forward,

168
00:09:11,591 --> 00:09:12,371
keep moving forward.

169
00:09:13,712 --> 00:09:15,592
And it gets some hard.

170
00:09:15,692 --> 00:09:18,213
Here's a harder body to control, and it's also harder.

171
00:09:19,674 --> 00:09:21,014
And yeah, sometimes it fails.

172
00:09:22,155 --> 00:09:28,337
And we actually had a guy at DICE do a more spiced up

173
00:09:28,377 --> 00:09:30,198
version of this, so we can take a look at that instead.

174
00:09:54,913 --> 00:09:58,955
So, if anyone out there is working with sound, you know how important you are.

175
00:09:59,035 --> 00:10:00,356
Sound is super important.

176
00:10:03,790 --> 00:10:08,814
Possibilities, so let's look, we have seen now a lot of individual features that can be used,

177
00:10:08,855 --> 00:10:13,639
but what if we combine a few of these features to look at something that's pure sci-fi today,

178
00:10:13,699 --> 00:10:16,701
but I'm sure is possible within a couple of years.

179
00:10:17,502 --> 00:10:20,765
So most games today are very violent, and why is that?

180
00:10:21,345 --> 00:10:25,549
That's because violence is the simplest interaction to create in a game, actually.

181
00:10:25,749 --> 00:10:27,551
It's much harder to do social interaction.

182
00:10:29,525 --> 00:10:32,427
And that's a pity, because when we ask,

183
00:10:32,447 --> 00:10:35,029
or I asked a few people what are their best gaming

184
00:10:35,069 --> 00:10:37,932
experiences ever, a surprising amount of people

185
00:10:38,092 --> 00:10:39,613
answered something like this.

186
00:10:39,633 --> 00:10:41,595
When a woman walks up to you and says,

187
00:10:42,736 --> 00:10:46,299
that power you hold, that's strange and ancient.

188
00:10:46,979 --> 00:10:47,500
What are you?

189
00:10:48,320 --> 00:10:50,162
I'm a witch hunter.

190
00:10:50,502 --> 00:10:50,682
Oh!

191
00:10:50,702 --> 00:10:50,922
Oh!

192
00:10:51,303 --> 00:10:51,383
Oh!

193
00:10:51,563 --> 00:10:51,903
Oh!

194
00:10:54,726 --> 00:10:54,806
Oh!

195
00:10:54,846 --> 00:10:55,586
You hear a voice.

196
00:10:55,606 --> 00:10:57,068
It's the voice of the young woman following you.

197
00:10:57,885 --> 00:10:59,986
An offering of outside of love is great.

198
00:11:00,006 --> 00:11:00,447
Oh, I knew it!

199
00:11:00,467 --> 00:11:01,387
I knew it!

200
00:11:01,487 --> 00:11:02,528
It cleans the palate.

201
00:11:02,928 --> 00:11:05,249
You see her flesh extend as her arms grow.

202
00:11:07,090 --> 00:11:09,231
Yeah, so have you ever seen Vin Diesel this happy?

203
00:11:09,791 --> 00:11:13,573
So old-fashioned pen and paper role-playing.

204
00:11:14,233 --> 00:11:16,514
It's of course the social interaction you get around the table

205
00:11:17,114 --> 00:11:18,915
that we really can't do in games today.

206
00:11:20,604 --> 00:11:22,165
But it requires a lot of imagination.

207
00:11:22,205 --> 00:11:24,546
They have to visualize everything in front of them.

208
00:11:24,666 --> 00:11:27,928
So that's something we should be able to help.

209
00:11:27,968 --> 00:11:31,150
Some people have tried to help with live role playing,

210
00:11:31,190 --> 00:11:32,831
but it's definitely not for everyone.

211
00:11:34,832 --> 00:11:37,633
So we in the game industry came up

212
00:11:37,673 --> 00:11:40,194
with this, massive multiplayer online role playing games.

213
00:11:42,155 --> 00:11:44,236
However, they ended up like this, most of them.

214
00:11:45,702 --> 00:11:49,305
So this is a more than social interaction is probably you can

215
00:11:49,325 --> 00:11:51,747
of course do social interaction in World of Warcraft, but it's

216
00:11:52,067 --> 00:11:55,249
mostly a highly complex exercise in coordination and intricate

217
00:11:55,309 --> 00:11:56,089
game mechanics.

218
00:11:57,010 --> 00:11:58,511
So not much role playing there.

219
00:11:59,612 --> 00:12:02,534
So we have this concept called true role playing

220
00:12:04,035 --> 00:12:07,337
because we think it's important board gaming is more in is more

221
00:12:07,377 --> 00:12:08,118
popular than ever.

222
00:12:08,598 --> 00:12:10,499
More than a board games have never been more popular.

223
00:12:11,240 --> 00:12:13,401
And I think one of the reasons is because the social

224
00:12:13,421 --> 00:12:14,962
interaction you can have around the table.

225
00:12:16,480 --> 00:12:20,161
So this is an experiment we did in our lab a while back,

226
00:12:20,261 --> 00:12:22,062
and it's a skinning experiment in VR.

227
00:12:22,402 --> 00:12:23,723
So you can control a character.

228
00:12:25,424 --> 00:12:26,564
It's a Battlefield 4 character.

229
00:12:27,845 --> 00:12:30,806
And we can change characters.

230
00:12:30,946 --> 00:12:34,148
And this is just a prototype, but I

231
00:12:34,188 --> 00:12:37,649
think there's lots of animation technology

232
00:12:38,790 --> 00:12:40,731
going on with deep learning as well that

233
00:12:40,811 --> 00:12:42,932
will enable this to become really, really good soon.

234
00:12:44,681 --> 00:12:48,204
And if we take this skinning and we add the voice conversion

235
00:12:48,244 --> 00:12:49,965
I showed you, then you can look and sound

236
00:12:50,225 --> 00:12:51,386
like a character of your choice.

237
00:12:52,627 --> 00:12:54,428
And of course, we need the face animation

238
00:12:54,469 --> 00:12:57,511
as well to actually animate the skin to say what you say.

239
00:13:00,113 --> 00:13:03,956
So I'll show you two scenes that we absolutely can't do today.

240
00:13:04,476 --> 00:13:06,298
Imagine that this is you and your friends

241
00:13:06,538 --> 00:13:09,040
in virtual reality doing some role playing.

242
00:13:11,341 --> 00:13:11,702
Here we go.

243
00:13:12,422 --> 00:13:13,683
If you're going to play with the big dogs.

244
00:13:14,192 --> 00:13:14,693
No fair.

245
00:13:15,633 --> 00:13:15,913
I'm in.

246
00:13:16,914 --> 00:13:17,674
That's you, Axel.

247
00:13:18,295 --> 00:13:19,155
I'm in.

248
00:13:20,156 --> 00:13:20,976
Oh, Hilo.

249
00:13:21,577 --> 00:13:25,439
When are you going to learn?

250
00:13:25,499 --> 00:13:31,463
I just want to say, it's been a while since we opened the books, and in regards to you

251
00:13:31,503 --> 00:13:35,685
guys, Bert, Jerry, as a man of few words, I...

252
00:13:35,746 --> 00:13:36,886
Not few enough, though, huh?

253
00:13:37,447 --> 00:13:37,987
No blood.

254
00:13:38,087 --> 00:13:38,667
No blood.

255
00:13:38,847 --> 00:13:39,208
Sallow.

256
00:13:39,228 --> 00:13:39,568
No blood.

257
00:13:39,608 --> 00:13:40,488
No blood.

258
00:13:40,508 --> 00:13:40,729
Sallow.

259
00:13:43,710 --> 00:13:46,551
So for being TV, scenes in a TV show,

260
00:13:46,631 --> 00:13:48,432
there's nothing special with these scenes at all.

261
00:13:48,852 --> 00:13:51,654
But if we even try to imagine doing this in a game,

262
00:13:51,694 --> 00:13:53,935
a multiplayer setting with many players,

263
00:13:54,355 --> 00:13:55,736
it's impossible to do today.

264
00:13:56,736 --> 00:13:58,737
But I think combining those techniques I mentioned,

265
00:13:59,278 --> 00:14:01,579
it will be hard, and there's a few years left

266
00:14:01,639 --> 00:14:04,340
before this is realizable, but I think it can be done.

267
00:14:04,800 --> 00:14:07,962
And then it will open a whole new genre of games,

268
00:14:08,722 --> 00:14:11,063
like more amateur theater and true role-playing.

269
00:14:13,339 --> 00:14:15,722
Okay, let's start looking at game AI

270
00:14:15,762 --> 00:14:16,643
and reinforcement learning.

271
00:14:19,245 --> 00:14:21,127
So a short introduction to reinforcement learning.

272
00:14:21,167 --> 00:14:24,131
We have an agent, that's the intelligent thing,

273
00:14:24,211 --> 00:14:26,033
and it's acting in an environment.

274
00:14:27,414 --> 00:14:29,116
From the environment, it gets observations

275
00:14:29,176 --> 00:14:29,977
and rewards back.

276
00:14:31,979 --> 00:14:34,321
So when it does something good, it gets rewards.

277
00:14:35,449 --> 00:14:37,591
And it can also observe what happens when it acts.

278
00:14:38,251 --> 00:14:39,492
Usually it has a goal as well.

279
00:14:39,973 --> 00:14:42,335
It's typically to just optimize the rewards to get

280
00:14:42,375 --> 00:14:43,456
the high rewards possible.

281
00:14:44,437 --> 00:14:48,801
So of course, reinforcement learning is an old technique.

282
00:14:48,861 --> 00:14:51,024
But the new thing is that we combine this with the neural

283
00:14:51,064 --> 00:14:53,646
network and get the deep reinforcement learning.

284
00:14:55,628 --> 00:14:56,829
And this is learning by doing.

285
00:14:56,949 --> 00:14:59,812
It's the same way that both animals and humans learn.

286
00:15:02,223 --> 00:15:04,145
So let's look at the simplest possible example

287
00:15:04,185 --> 00:15:05,086
of reinforcement learning.

288
00:15:05,187 --> 00:15:06,728
This is a very simple game.

289
00:15:07,569 --> 00:15:10,613
The blue dot here is our hero, and he or it

290
00:15:11,194 --> 00:15:14,658
is supposed to eat the green dots and avoid the red dots.

291
00:15:15,797 --> 00:15:17,999
So first, we just drop him into the world.

292
00:15:18,359 --> 00:15:20,542
And it's not going well.

293
00:15:21,162 --> 00:15:23,565
The score is negative, and it's going down.

294
00:15:24,106 --> 00:15:27,489
Even though he hits, I have a hard time saying he or she

295
00:15:27,529 --> 00:15:28,210
about this, it.

296
00:15:28,851 --> 00:15:31,313
It hits the red dots, green dots sometimes.

297
00:15:31,414 --> 00:15:32,955
It's also hitting red dots.

298
00:15:33,716 --> 00:15:36,339
But just after a few minutes of reinforcement learning, this

299
00:15:36,379 --> 00:15:37,060
is what it looks like.

300
00:15:37,815 --> 00:15:40,977
So it's not trying to hide in the corner and dash out to to

301
00:15:41,077 --> 00:15:43,538
eat some greens now and then it's it's far from optimal,

302
00:15:43,759 --> 00:15:48,761
but at least the score is now positive going up and after a

303
00:15:48,801 --> 00:15:50,402
couple hours of training it looks like this.

304
00:15:53,544 --> 00:15:57,166
So now it's definitely playing this game super human will try

305
00:15:57,206 --> 00:16:00,267
to play it there's no way to play it with this efficiency.

306
00:16:03,137 --> 00:16:06,498
So that's the simplest possible reinforcement learning game I could come up with.

307
00:16:06,718 --> 00:16:09,919
And of course, my interest in this started when I saw this,

308
00:16:10,159 --> 00:16:12,500
that this is now more than 5 years ago,

309
00:16:13,340 --> 00:16:16,081
when DeepMind started playing Atari games just from the Pixel.

310
00:16:16,241 --> 00:16:18,662
They just looked at the Pixel and they got the score

311
00:16:19,362 --> 00:16:22,163
and then they learned to play 57 different Atari games.

312
00:16:23,383 --> 00:16:25,164
They didn't play all of them equally well,

313
00:16:25,364 --> 00:16:28,145
but most of them they played, at least nowadays,

314
00:16:28,205 --> 00:16:30,225
most of them are definitely played better than a human.

315
00:16:32,147 --> 00:16:35,728
And this is amazing, but our games are a lot more

316
00:16:35,768 --> 00:16:36,368
complicated.

317
00:16:36,388 --> 00:16:38,009
A lot has happened in 40 years.

318
00:16:39,269 --> 00:16:43,491
So how do we go about to actually play in AAA games?

319
00:16:43,991 --> 00:16:47,873
Or any game, I mean any modern 3D game at least, is more

320
00:16:47,913 --> 00:16:48,433
complicated.

321
00:16:49,673 --> 00:16:52,754
So here's an early example of things that can go wrong when

322
00:16:52,794 --> 00:16:55,575
we tried to do this over a year back in.

323
00:16:58,283 --> 00:17:00,124
So this is very simple.

324
00:17:00,344 --> 00:17:03,866
The little guys here running on the road, they're supposed

325
00:17:03,886 --> 00:17:05,447
to capture the road between the walls.

326
00:17:07,668 --> 00:17:10,329
And there's one single guy coming up there by the house.

327
00:17:10,349 --> 00:17:11,830
He's on the opposite team, even though it's

328
00:17:11,850 --> 00:17:13,411
hard to see here.

329
00:17:14,071 --> 00:17:15,031
And let's see what happens.

330
00:17:24,202 --> 00:17:25,403
So what happened?

331
00:17:26,884 --> 00:17:28,385
Of course, they're pretty stupid.

332
00:17:28,865 --> 00:17:31,327
We didn't have very strong networks when we did this.

333
00:17:31,427 --> 00:17:32,869
But they also don't have hearing.

334
00:17:33,169 --> 00:17:34,950
Every one of them was looking in one direction.

335
00:17:35,611 --> 00:17:37,792
They see this low resolution view of the world.

336
00:17:38,013 --> 00:17:40,975
So what we did was add a small hearing radar, the thing you

337
00:17:40,995 --> 00:17:42,016
see in the lower left corner.

338
00:17:42,596 --> 00:17:43,537
That's very crude.

339
00:17:43,617 --> 00:17:44,598
It's very short range.

340
00:17:44,658 --> 00:17:46,379
But at least it gives them an indication that

341
00:17:46,419 --> 00:17:47,240
someone is behind you.

342
00:17:47,940 --> 00:17:49,361
And that helped a lot.

343
00:17:49,421 --> 00:17:53,383
Another cool thing that speaks about power of neural

344
00:17:53,403 --> 00:17:57,145
networks is that they had learned using the 3D view, the

345
00:17:57,165 --> 00:17:57,685
vision view here.

346
00:17:57,985 --> 00:18:00,526
So we just slapped this 2D radar on top of the 3D view.

347
00:18:00,566 --> 00:18:01,947
We didn't do any other changes.

348
00:18:02,027 --> 00:18:02,607
We just slapped it.

349
00:18:02,647 --> 00:18:04,648
And it picked it up immediately, what it was

350
00:18:04,668 --> 00:18:05,788
supposed to do with that radar.

351
00:18:06,329 --> 00:18:08,630
We had to retrain it, of course, but nothing else.

352
00:18:10,909 --> 00:18:13,830
Another problem playing a real game is multi-action.

353
00:18:14,291 --> 00:18:16,472
When we play real games, especially if you play a

354
00:18:16,512 --> 00:18:19,554
console or with a keyboard and mouse, you use a lot of

355
00:18:19,594 --> 00:18:20,555
simultaneous actions.

356
00:18:21,235 --> 00:18:23,577
All of the game playing we have seen, this is from the

357
00:18:23,617 --> 00:18:27,819
Atari paper, they can only perform one action at a time.

358
00:18:29,341 --> 00:18:32,962
So in this case, you see that the primitive actions,

359
00:18:32,982 --> 00:18:34,443
so to say, are per half,

360
00:18:34,543 --> 00:18:36,604
and then they have created all possible actions

361
00:18:36,664 --> 00:18:38,905
like as new actions, combination actions,

362
00:18:39,046 --> 00:18:41,527
go forward and press the button, for example.

363
00:18:43,048 --> 00:18:45,689
And that's fine when you have an old Atari controller.

364
00:18:46,549 --> 00:18:47,670
It doesn't work today.

365
00:18:48,991 --> 00:18:52,452
PS4 controller has around 20 inputs.

366
00:18:53,453 --> 00:18:56,434
And if you combine all the possible combinations

367
00:18:56,474 --> 00:18:58,015
of 20 inputs, it's around 2 million.

368
00:18:59,467 --> 00:19:03,253
So we can't create all these new actions for all the possible combinations.

369
00:19:03,353 --> 00:19:04,715
The action space is too large.

370
00:19:05,376 --> 00:19:08,701
Unfortunately, you can't play, once again, this is from Battlefield.

371
00:19:09,281 --> 00:19:11,465
We're in the same building as DICE in Stockholm, by the way,

372
00:19:11,485 --> 00:19:13,528
so that's why we do a lot of Battlefield.

373
00:19:14,927 --> 00:19:18,089
battlefield you can't play without using a lot of simultaneous actions.

374
00:19:18,730 --> 00:19:20,431
So we had to solve that problem because if you

375
00:19:21,291 --> 00:19:24,933
just start allowing simultaneous actions for the agent it will just button mash,

376
00:19:25,013 --> 00:19:27,155
it will press half of them on average and

377
00:19:28,255 --> 00:19:32,478
it takes an enormous long time to learn even the most basic things when you do that.

378
00:19:33,338 --> 00:19:36,760
So what we did, and don't worry I won't go through the details here, we...

379
00:19:38,430 --> 00:19:39,911
We did some imitation learning.

380
00:19:39,951 --> 00:19:43,154
So we had the agent watch 30 minutes of human gameplay

381
00:19:43,514 --> 00:19:45,756
before starting, or actually simultaneously

382
00:19:46,337 --> 00:19:47,418
with the reinforcement learning.

383
00:19:48,499 --> 00:19:49,860
And just for the beginning, so we

384
00:19:49,900 --> 00:19:51,642
decayed the amount of imitation learning

385
00:19:51,682 --> 00:19:54,484
it did over time, the first hour maybe of training.

386
00:19:55,385 --> 00:19:58,348
And what this did, it helped the agent

387
00:19:58,388 --> 00:20:00,570
understand which combinations are valuable,

388
00:20:01,510 --> 00:20:03,312
which combinations make sense of the controller.

389
00:20:05,402 --> 00:20:07,103
So when we did that, that's the green line.

390
00:20:07,264 --> 00:20:12,207
So I won't go into details once again, but the higher

391
00:20:12,227 --> 00:20:14,849
you get there, the more score you get, the better.

392
00:20:15,389 --> 00:20:17,491
So as you can see, when we added imitation learning to

393
00:20:17,511 --> 00:20:22,034
this multi-action agent, it behaved much, much better.

394
00:20:23,435 --> 00:20:25,857
So let's look at some results, how the agent behaved.

395
00:20:27,143 --> 00:20:33,125
So when we went from a target games we we started learning on the targets we

396
00:20:33,165 --> 00:20:39,827
couldn't jump directly into a real game so we built a very small simple. FPS

397
00:20:39,867 --> 00:20:44,288
game essentially so let's start the movie and look at it.

398
00:20:46,008 --> 00:20:50,730
So the green guy here is the agent and as you can see is 12 different actions

399
00:20:50,890 --> 00:20:52,490
can perform all of them simultaneously.

400
00:20:54,591 --> 00:20:54,691
And.

401
00:20:56,995 --> 00:21:00,718
The goal here is this blue circle,

402
00:21:02,279 --> 00:21:03,359
to protect the blue circle.

403
00:21:03,760 --> 00:21:05,240
We'll see here soon.

404
00:21:05,280 --> 00:21:07,062
Yeah, that's the objective area.

405
00:21:07,462 --> 00:21:09,283
So that's what the trade agent is trying to do,

406
00:21:09,363 --> 00:21:10,544
is trying to find the circles.

407
00:21:10,584 --> 00:21:12,865
They move around every 30 seconds or every minute.

408
00:21:13,786 --> 00:21:17,308
And it has also got health pickups and ammunition pickups.

409
00:21:17,548 --> 00:21:18,949
And of course, there are opposing bots.

410
00:21:19,429 --> 00:21:21,110
He's alone against 10 opposing bots.

411
00:21:21,150 --> 00:21:23,311
They use classical AI techniques.

412
00:21:24,597 --> 00:21:28,560
He has a much higher rate of fire though, so it's still not impossible.

413
00:21:29,040 --> 00:21:30,241
So this is what the agent sees.

414
00:21:30,481 --> 00:21:32,062
This is the only input it has.

415
00:21:32,162 --> 00:21:36,124
It also has access actually to its health and ammo, but otherwise it's just a visual

416
00:21:36,164 --> 00:21:36,484
input.

417
00:21:37,125 --> 00:21:41,047
And this is a low resolution input and you can also see the hearing radar.

418
00:21:42,388 --> 00:21:47,471
The blue dot in the hearing radar indicates the direction to the objective area.

419
00:21:49,372 --> 00:21:50,593
in case the agent can't see it.

420
00:21:50,753 --> 00:21:52,514
And another thing that surprised us was the

421
00:21:52,554 --> 00:21:54,375
navigation capabilities.

422
00:21:54,855 --> 00:21:56,536
There's no navigation systems here.

423
00:21:56,996 --> 00:21:58,457
There's no nav meshes or anything.

424
00:21:58,997 --> 00:22:02,619
But still, it navigates this maze of houses to find

425
00:22:02,679 --> 00:22:04,600
objective without problem.

426
00:22:07,002 --> 00:22:10,323
And once it reaches the objective area, it has a few

427
00:22:10,363 --> 00:22:11,184
behaviors as well.

428
00:22:11,704 --> 00:22:13,905
It, of course, starts defending the area, and it's

429
00:22:13,965 --> 00:22:15,066
also patrolling the area.

430
00:22:20,710 --> 00:22:22,691
The hardest thing for it to learn was supplies.

431
00:22:23,411 --> 00:22:27,313
So when it runs out of ammo now, it immediately, which it

432
00:22:27,373 --> 00:22:30,535
did now, it immediately prioritizes finding ammo

433
00:22:30,555 --> 00:22:31,275
before anything else.

434
00:22:31,295 --> 00:22:33,336
You can see that it ignores the enemies, it ignores the

435
00:22:33,356 --> 00:22:35,537
objective area, runs to the green box, and

436
00:22:35,577 --> 00:22:36,298
then turns around.

437
00:22:39,097 --> 00:22:43,982
It also, this scanning behavior was also something it

438
00:22:44,302 --> 00:22:46,645
discovered after a while to search more efficiently.

439
00:22:47,526 --> 00:22:49,227
So all these behaviors were emergent.

440
00:22:49,307 --> 00:22:52,170
We didn't say that it should do like this or that.

441
00:22:52,231 --> 00:22:55,514
We only gave it the objective, protect the area.

442
00:22:59,327 --> 00:23:02,048
Another cool thing about this agents, they generalize very

443
00:23:02,308 --> 00:23:05,610
good well so this is exactly the same agent of course it's a

444
00:23:05,670 --> 00:23:09,111
new action space new buttons to push, but otherwise it's the

445
00:23:09,151 --> 00:23:10,431
fact that the same agent.

446
00:23:11,211 --> 00:23:14,072
So it's a very simple racing game, but it only took a few

447
00:23:14,112 --> 00:23:16,873
minutes for agent to learn how to lap this circuit.

448
00:23:18,114 --> 00:23:20,975
So they are they can solve a lot of different problems of

449
00:23:21,015 --> 00:23:23,175
course you have to train them for each problem right now.

450
00:23:25,576 --> 00:23:28,277
So what about reinforcement learning in AAA games?

451
00:23:30,211 --> 00:23:34,275
So we have collaborated with dice to try to do this in

452
00:23:34,575 --> 00:23:35,176
Battlefield 1.

453
00:23:37,218 --> 00:23:41,482
And the case study we set out to do was automated testing.

454
00:23:42,203 --> 00:23:44,325
Battlefield 1 is a very complicated game.

455
00:23:44,586 --> 00:23:45,747
It's 64 players.

456
00:23:46,207 --> 00:23:48,209
It's four character classes.

457
00:23:48,249 --> 00:23:49,350
There's infantry play.

458
00:23:49,370 --> 00:23:50,211
There's vehicle play.

459
00:23:50,231 --> 00:23:51,032
There's airplanes.

460
00:23:51,072 --> 00:23:51,773
There's horses.

461
00:23:51,853 --> 00:23:52,894
There's zeppelins.

462
00:23:53,937 --> 00:23:56,138
There's lots of game modes, there's lots of maps,

463
00:23:56,558 --> 00:23:59,578
and this needs to be tested for every new build, essentially.

464
00:24:00,058 --> 00:24:01,819
So this is a nightmare for QA.

465
00:24:03,819 --> 00:24:06,239
So we thought, what if we can help by actually

466
00:24:06,560 --> 00:24:13,061
have self-learning agents to play, not all 64 players,

467
00:24:13,721 --> 00:24:14,201
but a few.

468
00:24:14,821 --> 00:24:16,221
Fill them up with 50 agents, maybe,

469
00:24:16,241 --> 00:24:17,602
and have 10 humans or something.

470
00:24:18,482 --> 00:24:21,102
That would make us be able to scale up testing a lot.

471
00:24:23,849 --> 00:24:28,311
So before going further and showing you how this turned

472
00:24:28,411 --> 00:24:31,993
out, we tried actually to use rendered observations first.

473
00:24:32,153 --> 00:24:35,714
But the visuals of Battlefield are far too complex for the

474
00:24:36,014 --> 00:24:37,975
small visual networks we use.

475
00:24:38,876 --> 00:24:41,016
We actually use almost the same network that

476
00:24:41,036 --> 00:24:41,877
played Atari games.

477
00:24:42,017 --> 00:24:46,199
So actually just seeing the difference of uniforms in

478
00:24:46,239 --> 00:24:47,079
Battlefield was too hard.

479
00:24:48,173 --> 00:24:51,074
So we have a simplified observation instead.

480
00:24:51,474 --> 00:24:53,455
So the blue in this observation is obstacles,

481
00:24:53,555 --> 00:24:55,596
and we use ray casting to find that.

482
00:24:56,176 --> 00:24:57,376
So we don't render that, really.

483
00:24:58,117 --> 00:25:01,738
And the red guys are the enemy.

484
00:25:01,838 --> 00:25:03,518
And that's in a higher resolution.

485
00:25:03,558 --> 00:25:05,139
So it's 12 by 12 for the obstacles.

486
00:25:05,219 --> 00:25:07,079
It's 128 by 128 for the enemies.

487
00:25:07,379 --> 00:25:09,660
And we also have this hearing radar, a 20-minute range.

488
00:25:12,081 --> 00:25:14,622
The main reward for the agents was the score from the game.

489
00:25:14,642 --> 00:25:15,942
So we're trying to maximize the score.

490
00:25:17,522 --> 00:25:22,143
But to get them started, we also introduced a waypoint.

491
00:25:22,463 --> 00:25:25,024
So both teams are trying to go to the same waypoint.

492
00:25:25,064 --> 00:25:27,244
And then that also helps them meet each other

493
00:25:27,304 --> 00:25:28,044
on a large map.

494
00:25:29,185 --> 00:25:30,685
And supplies, we also added,

495
00:25:30,705 --> 00:25:33,286
they don't have the actions heal and resupply.

496
00:25:33,886 --> 00:25:36,326
So we added the boxes from the previous games

497
00:25:36,406 --> 00:25:37,387
to Battlefield.

498
00:25:38,863 --> 00:25:41,163
The agents learn, the last agent we saw,

499
00:25:41,703 --> 00:25:43,964
that agent was alone against classical AI bots.

500
00:25:44,064 --> 00:25:47,005
In this case, we have the agents learn by self-play.

501
00:25:47,405 --> 00:25:48,805
So both teams are agents.

502
00:25:50,565 --> 00:25:52,786
So the second team's brain is an older version

503
00:25:52,806 --> 00:25:54,386
of the first team's, so why don't we use

504
00:25:54,446 --> 00:25:57,067
the best brain for both teams?

505
00:25:57,707 --> 00:25:58,627
Well, because of this.

506
00:25:58,647 --> 00:26:00,348
Let's go.

507
00:26:00,408 --> 00:26:01,728
Let's go.

508
00:26:01,768 --> 00:26:02,708
Let's go.

509
00:26:02,748 --> 00:26:03,108
Let's go.

510
00:26:03,388 --> 00:26:03,808
Let's go.

511
00:26:03,868 --> 00:26:04,468
Let's go.

512
00:26:04,588 --> 00:26:05,169
Let's go.

513
00:26:05,209 --> 00:26:05,909
Let's go.

514
00:26:06,109 --> 00:26:06,569
Let's go.

515
00:26:06,629 --> 00:26:07,169
Let's go.

516
00:26:29,113 --> 00:26:31,015
Strange game.

517
00:26:31,075 --> 00:26:33,796
The only winning move is not to play.

518
00:26:35,498 --> 00:26:37,399
So this literally happened when we had

519
00:26:37,419 --> 00:26:39,040
the same brain for both teams.

520
00:26:41,742 --> 00:26:44,443
They discovered that I don't shoot, I don't get shot.

521
00:26:45,204 --> 00:26:48,206
So they stopped shooting and just went around

522
00:26:48,286 --> 00:26:49,006
picking up boxes.

523
00:26:56,106 --> 00:26:59,469
So we actually had to, well, we froze one of the brains

524
00:26:59,529 --> 00:27:01,230
to be an older version, so they are not the same.

525
00:27:01,671 --> 00:27:04,293
And another thing we did was actually introduce a few

526
00:27:04,433 --> 00:27:06,875
of the really stupid testing bots we have

527
00:27:07,475 --> 00:27:09,797
into each side as well, but they actually shoot.

528
00:27:09,817 --> 00:27:11,439
So now the agents have to defend themselves

529
00:27:11,659 --> 00:27:12,219
from the beginning.

530
00:27:12,920 --> 00:27:13,661
So does it work?

531
00:27:17,384 --> 00:27:19,305
Yeah, well, we can look at this clip first.

532
00:27:36,762 --> 00:27:37,825
The enemy is in the lead.

533
00:27:38,246 --> 00:27:40,593
This is not exactly fine Battlefield gameplay.

534
00:27:42,619 --> 00:27:44,905
And they also have a problem with being indoors.

535
00:27:53,766 --> 00:27:58,669
Yeah, so they end up doing this when they don't have an obvious way of getting a reward.

536
00:27:59,010 --> 00:28:03,472
They don't see an enemy, they are already close to the waypoint and they have no need

537
00:28:03,532 --> 00:28:04,313
to pick up supplies.

538
00:28:04,933 --> 00:28:08,135
They have nothing else to do, so they circle until they find something to do.

539
00:28:09,056 --> 00:28:12,638
So let's look at some more successful moments.

540
00:28:16,468 --> 00:28:20,350
This is programmer art or programmer video recording,

541
00:28:20,410 --> 00:28:23,231
rather, no professional video editor has been involved

542
00:28:23,271 --> 00:28:26,633
in this, so apologize for the shaky camera movements.

543
00:28:27,593 --> 00:28:28,854
So it's about a two minute clip,

544
00:28:29,054 --> 00:28:32,136
and every player you see here is controlled

545
00:28:32,156 --> 00:28:32,856
by neural network.

546
00:30:11,684 --> 00:30:12,204
So it works.

547
00:30:13,085 --> 00:30:16,285
So to my knowledge, this is the first time anyone has

548
00:30:16,906 --> 00:30:19,366
been able to play a first-person immersive modern

549
00:30:19,406 --> 00:30:21,967
game with deep reinforcement learning.

550
00:30:23,368 --> 00:30:27,429
So of course, there are lots of challenges with doing this

551
00:30:27,489 --> 00:30:27,749
as well.

552
00:30:28,669 --> 00:30:29,430
This isn't easy.

553
00:30:30,250 --> 00:30:32,110
So one of the biggest problems is slow training.

554
00:30:32,611 --> 00:30:34,371
So what you just saw, those agents had

555
00:30:34,411 --> 00:30:35,391
trained for six days.

556
00:30:37,010 --> 00:30:42,038
On and we use 8 machines in parallel to to to play the game so it amounts to

557
00:30:42,118 --> 00:30:47,226
about 15,000 game runs for 300 days of gameplay if you count all the agents

558
00:30:47,367 --> 00:30:50,772
experience so it's definitely slow slow going.

559
00:30:52,407 --> 00:30:54,148
It's also behavioral design.

560
00:30:54,748 --> 00:30:57,450
As I talked, you do reward shaping.

561
00:30:57,610 --> 00:30:59,652
You have to design these rewards to get the agents to

562
00:30:59,692 --> 00:31:00,853
do what you want them to do.

563
00:31:01,353 --> 00:31:02,814
Of course, in this case, it was mostly the

564
00:31:02,854 --> 00:31:03,514
Battlefield score.

565
00:31:03,835 --> 00:31:07,757
But it's hard for a game designer to actually go through

566
00:31:07,817 --> 00:31:09,779
reward shaping to get the behavior they

567
00:31:09,959 --> 00:31:10,779
need from an agent.

568
00:31:11,360 --> 00:31:12,120
So that's a hard part.

569
00:31:12,521 --> 00:31:13,521
You don't have a full control.

570
00:31:13,541 --> 00:31:15,123
You will be surprised by the behavior.

571
00:31:15,163 --> 00:31:16,343
That can be both good and bad.

572
00:31:16,944 --> 00:31:20,446
It's very hard to debug a neural network, we have

573
00:31:20,626 --> 00:31:20,987
discovered.

574
00:31:22,220 --> 00:31:26,605
And there's also the question how we integrate this with classical AI systems like behavior trees,

575
00:31:26,705 --> 00:31:32,151
because right now all the behavior of the agent is controlled by the neural network.

576
00:31:32,531 --> 00:31:37,216
I don't think that will be the first thing that happens in real games. I think some parts of the

577
00:31:37,256 --> 00:31:41,801
behavior will be controlled by neural networks, and that means we need to integrate this into

578
00:31:42,142 --> 00:31:43,203
current AI systems.

579
00:31:45,819 --> 00:31:48,280
And of course, the execution, it's actually not a huge

580
00:31:48,320 --> 00:31:51,021
problem, but the GPU is busy doing graphics, typically.

581
00:31:51,281 --> 00:31:53,921
And so all the agents run on CPU now.

582
00:31:54,622 --> 00:31:57,282
But inference, that is actually running a trained

583
00:31:57,662 --> 00:31:58,783
agent, is called inference.

584
00:31:59,803 --> 00:32:01,684
That's much, much cheaper than training the agent.

585
00:32:02,384 --> 00:32:03,744
It's a magnitude cheaper.

586
00:32:04,204 --> 00:32:05,685
So right now, it's not a huge problem.

587
00:32:11,303 --> 00:32:14,864
And if some of you have been at a few of our rendering talks,

588
00:32:14,904 --> 00:32:16,685
you might have seen this image already.

589
00:32:16,805 --> 00:32:20,766
But this is a small project called Pika Pika.

590
00:32:20,986 --> 00:32:24,146
It's built up on top of an R&D game engine.

591
00:32:24,346 --> 00:32:27,247
So it's mostly called Halcyon, the game engine.

592
00:32:27,947 --> 00:32:31,328
So it's mostly for rendering research.

593
00:32:33,322 --> 00:32:35,844
While we were building a new game engine,

594
00:32:35,864 --> 00:32:38,446
why not make sure that it's good at training agents

595
00:32:38,626 --> 00:32:41,889
with that is fast and that it has all the mechanism needed

596
00:32:41,909 --> 00:32:42,549
to train agents.

597
00:32:43,610 --> 00:32:45,892
So these little robots, the yellow robots here,

598
00:32:45,912 --> 00:32:49,135
I'll show you a short trailer for this project.

599
00:32:49,335 --> 00:32:51,637
And the robots are trying to repair machines.

600
00:32:51,917 --> 00:32:55,219
And this is a much simpler task than the battlefield task

601
00:32:55,239 --> 00:32:55,720
we just saw.

602
00:32:55,780 --> 00:32:57,701
But this was just right now to get some life

603
00:32:57,741 --> 00:32:59,263
into this rendered environment.

604
00:32:59,983 --> 00:33:01,945
So let's have a look.

605
00:34:17,826 --> 00:34:20,908
So in this environment, we get it's fast.

606
00:34:21,028 --> 00:34:22,969
We get a lot of different rendering modes

607
00:34:23,010 --> 00:34:23,910
that the agent can use.

608
00:34:24,110 --> 00:34:26,232
It has very fast communication with the brains.

609
00:34:26,872 --> 00:34:28,353
Those are typically written in Python.

610
00:34:28,974 --> 00:34:30,314
The game itself is in C++.

611
00:34:31,655 --> 00:34:33,977
So we'll continue making sure that this engine has

612
00:34:34,017 --> 00:34:35,558
good support for self-learning.

613
00:34:36,939 --> 00:34:38,480
Because speed is very important.

614
00:34:38,780 --> 00:34:40,982
The more data you can get when you're training,

615
00:34:41,662 --> 00:34:43,323
the faster you can learn.

616
00:34:44,539 --> 00:34:46,601
And in this case, we see 36 agents training

617
00:34:46,621 --> 00:34:47,381
in the same process.

618
00:34:48,222 --> 00:34:50,964
And we can actually run a few of these processes

619
00:34:51,004 --> 00:34:51,605
on one machine.

620
00:34:51,825 --> 00:34:54,247
So we can get a lot of machines training in parallel.

621
00:34:57,089 --> 00:34:57,809
So the hype.

622
00:34:58,390 --> 00:34:59,831
I promised to talk a bit about the hype.

623
00:35:01,332 --> 00:35:05,936
And one sign of the hype is that I counted to more than 20 talks

624
00:35:05,996 --> 00:35:08,818
at GDC this year that have to do with deep learning or machine

625
00:35:08,838 --> 00:35:09,138
learning.

626
00:35:10,219 --> 00:35:11,100
And I think that's a record.

627
00:35:12,705 --> 00:35:16,467
And of course, much of the hype is about artificial general intelligence.

628
00:35:17,568 --> 00:35:21,050
And that's not really what we are doing, but let's talk for a short while about it.

629
00:35:21,310 --> 00:35:26,993
So that is when a computer is as good as a human is on most tasks.

630
00:35:29,114 --> 00:35:32,135
So, will there be artificial general intelligence?

631
00:35:32,576 --> 00:35:34,116
I think that's quite easy to answer.

632
00:35:34,537 --> 00:35:37,658
If you believe two things, technological progress will continue.

633
00:35:38,670 --> 00:35:44,273
and intelligence is not magic, it is biologically based, it's physics, it's not supernatural

634
00:35:44,413 --> 00:35:46,074
in some way, then yes.

635
00:35:46,935 --> 00:35:50,757
Of course, technological progress might not continue, there might be the third world war

636
00:35:50,817 --> 00:35:55,280
or something, but if these two things happen, then we will eventually have artificial general

637
00:35:55,300 --> 00:35:58,501
intelligence, according to my belief at least.

638
00:35:59,647 --> 00:36:00,747
So then the question is when?

639
00:36:01,688 --> 00:36:04,350
So a couple of years ago, there was a questionnaire at

640
00:36:04,370 --> 00:36:07,532
the conference, and a lot of experts guessed when we would

641
00:36:07,552 --> 00:36:08,293
have AGI.

642
00:36:08,994 --> 00:36:12,216
And the median, I say average here, but it's a median.

643
00:36:12,256 --> 00:36:16,179
The median guess was somewhere in the 2040s, so a little bit

644
00:36:16,199 --> 00:36:17,180
more than 20 years away.

645
00:36:18,060 --> 00:36:19,621
Of course, this is a wild guess.

646
00:36:20,262 --> 00:36:22,343
It's very hard to make these kinds of predictions.

647
00:36:25,053 --> 00:36:32,058
new technology. So for example in 1902 the Wright brothers said that it would take 50 years before humans would fly.

648
00:36:32,599 --> 00:36:33,760
In 1903 they flew.

649
00:36:36,122 --> 00:36:43,067
In 2015 people believed it would take another 10 or 15 years at least before a computer program would beat a Go master.

650
00:36:43,247 --> 00:36:45,009
In 2016 AlphaGo did it.

651
00:36:46,230 --> 00:36:50,954
But of course we still don't have those flying cars that we were promised in the 50s. So

652
00:36:51,674 --> 00:36:53,496
it's very hard to make this kind of prediction.

653
00:36:54,203 --> 00:36:57,444
But let's go with 20 years away for now.

654
00:36:59,205 --> 00:37:04,346
And the hype, this doesn't necessarily have to be

655
00:37:04,446 --> 00:37:05,566
artificial intelligence.

656
00:37:05,606 --> 00:37:07,407
But there's some negative concept.

657
00:37:07,427 --> 00:37:10,207
There's lots of exaggerations going around, that AI will

658
00:37:10,247 --> 00:37:11,308
solve every problem.

659
00:37:11,928 --> 00:37:14,108
It probably won't, not for a long time, because it's still

660
00:37:14,168 --> 00:37:14,888
very hard to do.

661
00:37:15,889 --> 00:37:18,509
And every startup is now an AI startup, or a blockchain

662
00:37:18,529 --> 00:37:22,850
startup, or even the best is an AI blockchain startup.

663
00:37:25,088 --> 00:37:28,128
So the label AI is very overused.

664
00:37:28,909 --> 00:37:30,449
Every feature is suddenly an AI feature.

665
00:37:30,469 --> 00:37:32,729
I saw a thermostat that could lower the temperature

666
00:37:32,769 --> 00:37:33,850
in your house during night.

667
00:37:34,070 --> 00:37:35,410
That was an AI thermostat.

668
00:37:38,171 --> 00:37:40,271
Another problem, not noticeable maybe

669
00:37:40,291 --> 00:37:42,512
if you're not in academia, but it works in practice,

670
00:37:42,552 --> 00:37:43,352
but not in theory.

671
00:37:43,492 --> 00:37:45,392
We really don't know why deep learning

672
00:37:45,432 --> 00:37:46,472
works as well as it does.

673
00:37:46,773 --> 00:37:47,773
It shouldn't work this well.

674
00:37:49,659 --> 00:37:54,283
And we don't have the theory for it yet, which means that building a neural network architecture

675
00:37:54,323 --> 00:37:58,146
and tuning the hyperparameters and everything is more of an art than a science right now.

676
00:37:59,787 --> 00:38:02,610
It's not a huge problem in practice because if it works, it works.

677
00:38:02,750 --> 00:38:06,393
But if we have the theory, it would of course be much easier to do things.

678
00:38:06,453 --> 00:38:10,536
Then we could actually calculate what the network architecture should be instead of

679
00:38:10,576 --> 00:38:11,297
guessing and trying.

680
00:38:14,678 --> 00:38:17,119
Yes, the AI winters, there's also a lot of naysayers

681
00:38:17,159 --> 00:38:19,120
that we will soon have a new winter.

682
00:38:19,160 --> 00:38:21,462
We had winters during the 80s and 90s

683
00:38:21,562 --> 00:38:23,163
where we reached a plateau, nothing,

684
00:38:23,223 --> 00:38:26,184
we could solve toy problems, but nothing really interesting.

685
00:38:27,185 --> 00:38:29,226
And of course we might end up there again,

686
00:38:29,386 --> 00:38:31,748
but I'm more hopeful this time because,

687
00:38:31,768 --> 00:38:34,850
yeah, I'll come back to that at the end of the talk.

688
00:38:37,551 --> 00:38:39,052
We have taken a big step up now

689
00:38:39,112 --> 00:38:41,153
and can solve a lot more than toy problems.

690
00:38:43,135 --> 00:38:46,177
And of course, there's people that think that AI should be banned.

691
00:38:46,437 --> 00:38:51,340
I saw that one union here in the US wants to ban all deliveries with automated trucks

692
00:38:51,540 --> 00:38:52,660
and drones, for example.

693
00:38:53,201 --> 00:38:56,583
This is already a debate that has started.

694
00:38:58,524 --> 00:39:01,926
So let's get back to narrow AI, not the general AI.

695
00:39:02,506 --> 00:39:07,369
So deep reinforcement learning is hard, and there's been some great blog posts recently

696
00:39:07,889 --> 00:39:10,811
about something like this one, deep reinforcement learning doesn't work yet.

697
00:39:11,932 --> 00:39:12,032
And...

698
00:39:13,027 --> 00:39:18,229
They bring up a lot of great points that are not unsolved really.

699
00:39:18,369 --> 00:39:23,491
And it only works well for games and simulations, they say, and that's very lucky for us because

700
00:39:23,532 --> 00:39:24,912
we are in, of course, in gaming.

701
00:39:26,500 --> 00:39:28,662
So one problem is the reward shaping I mentioned before.

702
00:39:28,882 --> 00:39:30,284
So this is work from OpenAI.

703
00:39:31,345 --> 00:39:34,147
And the boat here is supposed to go around the track.

704
00:39:34,447 --> 00:39:35,568
It's not doing that.

705
00:39:35,648 --> 00:39:36,889
It's going in the wrong direction.

706
00:39:38,211 --> 00:39:40,212
But it's still winning the game because it

707
00:39:40,252 --> 00:39:43,055
has found these green things that it gets a lot of score for.

708
00:39:43,135 --> 00:39:45,337
You can see on the small radar to the upper left

709
00:39:45,497 --> 00:39:48,419
the other players going around like they're supposed to do.

710
00:39:49,180 --> 00:39:51,042
But this boat actually wins the game.

711
00:39:51,882 --> 00:39:55,405
So it's because it has much higher score than the other ones

712
00:39:55,506 --> 00:39:56,346
when the game ends.

713
00:39:56,506 --> 00:39:58,848
So it's found an exploit.

714
00:39:59,009 --> 00:40:01,231
And that's actually something these agents are very useful

715
00:40:01,251 --> 00:40:03,012
for, finding exploits in games.

716
00:40:03,372 --> 00:40:07,216
They found bugs and old exploits that were unknown for 40 years

717
00:40:07,276 --> 00:40:07,997
in Atari games.

718
00:40:10,101 --> 00:40:12,183
But this is also a problem because reward shaping,

719
00:40:12,223 --> 00:40:13,024
as I said, is hard.

720
00:40:14,045 --> 00:40:17,750
We don't want to have lots of specific rewards

721
00:40:17,950 --> 00:40:20,112
to get a certain behavior out of the,

722
00:40:20,733 --> 00:40:23,056
it's too hard to do that, it takes a long time testing.

723
00:40:24,598 --> 00:40:27,321
So how do we try to solve that?

724
00:40:28,255 --> 00:40:30,777
So in this case, the robot, this is from a recent paper

725
00:40:30,837 --> 00:40:35,379
by DeepMind, and the robot here is supposed to first staple

726
00:40:35,679 --> 00:40:38,600
the blocks and then clean up and put them in a box,

727
00:40:38,801 --> 00:40:39,721
opening the lid of the box.

728
00:40:40,201 --> 00:40:42,342
And this is a hard problem because usually you have to get

729
00:40:42,442 --> 00:40:45,784
lots of small rewards here, like first try to move the arm

730
00:40:45,844 --> 00:40:49,726
towards the block, then grip the block, then open the lid,

731
00:40:50,046 --> 00:40:53,068
then grip the block again, open and drop the block,

732
00:40:53,268 --> 00:40:54,808
and then you're finished.

733
00:40:55,293 --> 00:41:01,547
And every one of these subtasks consists of maybe 100 actions,

734
00:41:01,567 --> 00:41:03,151
small actions, I mean motor actions,

735
00:41:03,372 --> 00:41:04,695
to actually achieve this.

736
00:41:06,777 --> 00:41:09,858
So what they have done here is hierarchical reinforcement

737
00:41:09,878 --> 00:41:10,138
learning.

738
00:41:10,239 --> 00:41:15,020
It actually learns to use these higher level actions

739
00:41:15,080 --> 00:41:15,380
instead.

740
00:41:15,480 --> 00:41:19,102
So for example, move to block or grip, those consists of a

741
00:41:19,142 --> 00:41:20,322
lot of small motor actions.

742
00:41:20,642 --> 00:41:23,163
But it has learned to use a sequence of those higher level

743
00:41:23,203 --> 00:41:23,843
actions instead.

744
00:41:23,863 --> 00:41:25,924
And then the problem becomes much simpler.

745
00:41:25,944 --> 00:41:27,504
Then we can simply give it a reward.

746
00:41:28,005 --> 00:41:30,305
Once it has cleaned up and closed the box, you get one

747
00:41:30,345 --> 00:41:30,986
point in reward.

748
00:41:31,046 --> 00:41:32,106
You get no other rewards.

749
00:41:32,166 --> 00:41:33,306
And that's a very hard problem.

750
00:41:34,409 --> 00:41:41,612
But I think we have to solve that problem to make this feasible, or at least much simpler than it is today.

751
00:41:43,273 --> 00:41:49,535
Another problem is, of course, if you saw this curve, if you look at the horizontal axis, that's tens of millions of steps.

752
00:41:50,876 --> 00:41:56,458
It takes a long time, like I said. So sample efficiency, this is called, we don't use the data efficiently enough.

753
00:41:58,689 --> 00:42:01,191
So yeah, so they are very slow to learn.

754
00:42:01,612 --> 00:42:02,272
And why is that?

755
00:42:03,013 --> 00:42:06,096
So if you take a newborn kitten as an example, they are also

756
00:42:07,029 --> 00:42:08,770
They're not great at playing games.

757
00:42:09,811 --> 00:42:12,232
Because they are born blind, they don't know how to move,

758
00:42:12,292 --> 00:42:12,612
really.

759
00:42:12,952 --> 00:42:15,834
They know nothing about gravity until they fall a

760
00:42:15,874 --> 00:42:17,555
couple of times, and so on.

761
00:42:18,015 --> 00:42:19,616
So they have to learn everything from scratch.

762
00:42:20,036 --> 00:42:21,777
And that's also what our agent does.

763
00:42:22,257 --> 00:42:24,058
Every time we train it, it starts from scratch.

764
00:42:24,198 --> 00:42:26,640
It has to relearn its visual system, and so on.

765
00:42:26,700 --> 00:42:29,861
So it takes a long time, and that's not strange.

766
00:42:30,302 --> 00:42:31,923
So what we need to do is transfer learning.

767
00:42:31,943 --> 00:42:34,864
We need to be able to take an agent that already has learned

768
00:42:34,904 --> 00:42:36,185
a few things and continue learning.

769
00:42:37,366 --> 00:42:38,606
But that's also a hard problem.

770
00:42:39,387 --> 00:42:40,808
Yeah, so this would be slow.

771
00:42:43,770 --> 00:42:46,052
So there was actually recently an experiment

772
00:42:46,232 --> 00:42:47,092
on human priors.

773
00:42:47,252 --> 00:42:49,834
So to the right here you have a simple platformer.

774
00:42:50,715 --> 00:42:52,996
And there's so much we take for granted

775
00:42:53,036 --> 00:42:54,858
that you can climb a mirror ladder

776
00:42:54,938 --> 00:42:56,659
just that you can stand on the gray stuff.

777
00:42:56,999 --> 00:43:00,381
And that you should avoid the pointy stuff

778
00:43:00,461 --> 00:43:03,423
and the monsters and so on.

779
00:43:03,941 --> 00:43:06,183
And if you see here up to the right, there's a key.

780
00:43:06,863 --> 00:43:08,864
And we know that a key is usually used in a door.

781
00:43:09,105 --> 00:43:11,907
So the player doesn't have a problem opening the door.

782
00:43:12,247 --> 00:43:14,628
To the left is what the computer sees, or an

783
00:43:14,669 --> 00:43:16,089
approximation of what the computer sees.

784
00:43:16,570 --> 00:43:19,312
And they let humans play like this.

785
00:43:20,032 --> 00:43:21,994
And then suddenly, the agent wasn't slower

786
00:43:22,034 --> 00:43:22,774
than the human learning.

787
00:43:22,794 --> 00:43:25,496
You can imagine playing the game to the left.

788
00:43:25,996 --> 00:43:27,317
It's a lot harder.

789
00:43:28,278 --> 00:43:29,719
And that's because we have priors.

790
00:43:29,759 --> 00:43:31,780
We have a lot of knowledge about the world.

791
00:43:32,662 --> 00:43:37,825
So one trend almost, or at least a few recent papers have

792
00:43:38,785 --> 00:43:41,667
been starting to talking about this, we have to have our

793
00:43:41,827 --> 00:43:43,468
agents play.

794
00:43:45,088 --> 00:43:46,729
So playing to build a model of the world.

795
00:43:46,969 --> 00:43:51,051
So not try to solve the direct task that eventually we want

796
00:43:51,091 --> 00:43:51,552
them to solve.

797
00:43:51,992 --> 00:43:54,473
Have them play around in the world instead, and let them

798
00:43:54,513 --> 00:43:57,014
learn a model of the world, what happens, so they can

799
00:43:57,034 --> 00:43:58,535
predict what happens when they do an action.

800
00:44:02,153 --> 00:44:10,622
Another criticism against deep reinforcement learning is, like I said, we have to retrain it when it wants to learn something new.

801
00:44:10,682 --> 00:44:11,663
So it's a one-trick pony.

802
00:44:14,065 --> 00:44:23,775
And that's mostly true, until a couple of weeks ago, when once again DeepMind released a paper where they actually have one agent playing 30 games without retraining.

803
00:44:23,815 --> 00:44:25,736
It's the same agent playing all 30 games.

804
00:44:27,897 --> 00:44:32,738
So it doesn't necessarily have to be a one-trick pony anymore.

805
00:44:34,839 --> 00:44:37,360
And finally, I want to talk more about AlphaGo.

806
00:44:38,400 --> 00:44:42,041
So Go is a much harder game for a computer to play than chess.

807
00:44:42,181 --> 00:44:45,822
Chess was beaten in 97, when Casparo lost to Deep Blue.

808
00:44:46,743 --> 00:44:49,104
But that was completely hand-coded, Deep Blue.

809
00:44:49,704 --> 00:44:51,384
AlphaGo, of course, used machine learning.

810
00:44:51,824 --> 00:44:52,345
And it used

811
00:44:54,073 --> 00:44:56,934
machine learning first to learn the game.

812
00:44:56,994 --> 00:45:00,355
It looked at hundreds of thousands of human games

813
00:45:00,555 --> 00:45:04,877
from archives to be able to pick up something to start with.

814
00:45:05,117 --> 00:45:06,918
And then it started using reinforcement learning

815
00:45:06,958 --> 00:45:07,918
to become better.

816
00:45:08,898 --> 00:45:12,820
And in 2016, March 2016, it beat Lee Sedol,

817
00:45:13,060 --> 00:45:14,140
one of the best players in the world.

818
00:45:16,322 --> 00:45:20,588
And of course, that's a huge success, but AlphaGo was pretty complicated.

819
00:45:20,728 --> 00:45:23,733
It had to use imitation learning to learn from humans first.

820
00:45:24,514 --> 00:45:26,437
It used an enormous amount of hardware.

821
00:45:28,495 --> 00:45:30,916
And if we look at what has happened since then, two years

822
00:45:30,956 --> 00:45:34,037
ago, we've had a few versions of AlphaGo.

823
00:45:34,117 --> 00:45:37,078
So AlphaGo Master is just a continuation of AlphaLEE.

824
00:45:37,698 --> 00:45:41,299
And that managed to beat 60 masters undefeated.

825
00:45:42,540 --> 00:45:44,760
So that's a much better version.

826
00:45:45,260 --> 00:45:47,521
But then the really amazing thing is AlphaGo Zero.

827
00:45:48,261 --> 00:45:51,002
AlphaGo Zero is a much simpler version of AlphaGo.

828
00:45:51,522 --> 00:45:53,163
It uses no imitation learning.

829
00:45:53,583 --> 00:45:55,024
It learns completely from scratch.

830
00:45:55,364 --> 00:45:56,705
It uses much less hardware.

831
00:45:57,105 --> 00:45:58,887
The network architecture is much simpler.

832
00:45:59,527 --> 00:46:01,108
And still, let's see.

833
00:46:02,569 --> 00:46:04,950
Well, before we look at a diagram,

834
00:46:05,010 --> 00:46:06,591
let's talk a bit about the ELO rating.

835
00:46:06,691 --> 00:46:09,753
So the best human is rated a bit below 3,700.

836
00:46:09,793 --> 00:46:10,853
If you have 400 more than your opponent,

837
00:46:10,873 --> 00:46:13,415
then you have a 91% chance to win, and so on.

838
00:46:13,515 --> 00:46:13,575
800, 99, and 1,500.

839
00:46:20,138 --> 00:46:23,281
1 in 10,000 almost that you will lose.

840
00:46:25,342 --> 00:46:27,484
So here is AlphaGo Zero's performance.

841
00:46:27,684 --> 00:46:29,606
This is training in days that you

842
00:46:29,626 --> 00:46:30,667
see on the horizontal axis.

843
00:46:32,728 --> 00:46:34,590
And the green line is the AlphaGo Li version.

844
00:46:36,391 --> 00:46:38,153
And remember, this is using much less hardware.

845
00:46:38,213 --> 00:46:41,195
It's a much simpler algorithm, a much simpler network model.

846
00:46:51,531 --> 00:46:55,032
So when the line stops here, it's around 5,200 or

847
00:46:55,072 --> 00:46:56,072
something in Elo.

848
00:46:56,112 --> 00:46:58,013
And that's 1,500 above the best human.

849
00:46:58,093 --> 00:47:00,513
And that's one in 10,000 to get beaten.

850
00:47:00,533 --> 00:47:02,354
So this is truly superhuman performance.

851
00:47:04,575 --> 00:47:07,075
And the other thing after that, there came a version

852
00:47:07,095 --> 00:47:09,276
called AlphaZero only, because that can play many

853
00:47:09,396 --> 00:47:10,056
different games.

854
00:47:10,156 --> 00:47:13,137
Because in this simple system, all you have to do is change

855
00:47:13,157 --> 00:47:14,157
the rules, nothing else.

856
00:47:14,177 --> 00:47:16,258
So they inserted chess rules instead of Go rules.

857
00:47:17,038 --> 00:47:19,659
And of course, it became the best chess-playing program in

858
00:47:19,679 --> 00:47:20,259
the world as well.

859
00:47:21,619 --> 00:47:25,783
And a cool thing from this paper was that it discovers

860
00:47:25,803 --> 00:47:27,424
these standard openings of chess.

861
00:47:27,464 --> 00:47:29,726
And you can see the hours of training on the horizontal

862
00:47:29,786 --> 00:47:32,509
axis, how it discovers an opening.

863
00:47:32,709 --> 00:47:34,731
And then when it becomes more and more experienced, it

864
00:47:34,791 --> 00:47:35,992
abandons some of the openings.

865
00:47:36,592 --> 00:47:39,275
So it's too bad of those of us that are playing car or can

866
00:47:39,295 --> 00:47:40,176
defense still.

867
00:47:40,356 --> 00:47:42,137
You shouldn't, according to AlphaZero.

868
00:47:45,801 --> 00:47:45,981
So.

869
00:47:49,176 --> 00:47:51,537
But one of the most amazing things that's not visible on

870
00:47:51,557 --> 00:47:53,577
this graph I just showed you is this.

871
00:47:55,278 --> 00:47:57,178
This is the best handcrafted Go program.

872
00:47:58,558 --> 00:48:00,179
It's around 2,000 Elo rating.

873
00:48:00,479 --> 00:48:05,180
So within two days, AlphaGo beat decades of software

874
00:48:05,220 --> 00:48:08,881
engineering and 1,000 years of Go experience with a simple

875
00:48:08,921 --> 00:48:09,721
learning algorithm.

876
00:48:10,281 --> 00:48:12,022
And this is one of the important points here.

877
00:48:12,482 --> 00:48:15,043
With learning, we can do things that we cannot

878
00:48:15,103 --> 00:48:16,963
possibly, we are not good enough to

879
00:48:17,023 --> 00:48:18,103
program this ourselves.

880
00:48:20,550 --> 00:48:23,051
So deep learning is still hard.

881
00:48:24,051 --> 00:48:26,273
It's still simpler to solve many problems with the

882
00:48:26,793 --> 00:48:28,894
conventional methods rather than DL.

883
00:48:29,274 --> 00:48:29,894
So why do it?

884
00:48:30,715 --> 00:48:33,196
Well, I've been in software engineering for a long time,

885
00:48:33,256 --> 00:48:35,937
and this is definitely the largest boost to capabilities

886
00:48:35,997 --> 00:48:37,598
of computers that I've ever seen.

887
00:48:40,060 --> 00:48:43,101
We can now do things that were previously impossible in

888
00:48:43,241 --> 00:48:45,562
computer vision or post-estimation or some of the

889
00:48:45,602 --> 00:48:46,543
other examples I showed you.

890
00:48:48,463 --> 00:48:53,424
or just learning to play one of these complex games was also impossible just a couple of years ago.

891
00:48:54,444 --> 00:48:58,485
And that learning methods can quickly outperform decades of software engineering effort.

892
00:48:59,185 --> 00:49:02,706
We can try very, very hard to solve a complex problem,

893
00:49:02,886 --> 00:49:08,087
but it's becoming more and more probable that soon some machine learning method

894
00:49:08,107 --> 00:49:10,648
will be able to solve it much better than you can as a programmer.

895
00:49:12,842 --> 00:49:14,923
So many difficult challenges remain,

896
00:49:15,803 --> 00:49:18,525
but the future potential of this is enormous.

897
00:49:19,205 --> 00:49:22,526
And as I said in the beginning about the AI winter,

898
00:49:23,347 --> 00:49:25,167
we are just starting to learn

899
00:49:25,307 --> 00:49:26,488
what we can do with deep learning.

900
00:49:26,568 --> 00:49:29,209
There's so much left.

901
00:49:29,529 --> 00:49:31,550
As I said, it's more an art than a science still,

902
00:49:32,010 --> 00:49:34,111
and we're still in the early days of deep learning,

903
00:49:34,131 --> 00:49:35,471
and that's why I'm very hopeful.

904
00:49:38,393 --> 00:49:38,653
Thank you.

905
00:49:51,430 --> 00:49:51,855
Questions?

906
00:50:04,986 --> 00:50:08,089
Hey, so you mentioned somewhere in the middle of the talk

907
00:50:08,149 --> 00:50:10,531
that we don't actually have a formal understanding of why

908
00:50:10,551 --> 00:50:12,013
deep learning works as well as it does.

909
00:50:12,653 --> 00:50:14,956
So actually tuning the parameters as much of an art

910
00:50:15,016 --> 00:50:16,958
form as actually having something that's

911
00:50:17,578 --> 00:50:18,579
formalized around that.

912
00:50:19,020 --> 00:50:24,104
I'm wondering if you have any anecdotal evidence of how did

913
00:50:24,144 --> 00:50:25,586
you guys actually triage?

914
00:50:26,488 --> 00:50:30,510
how you should change parameters between different iterations

915
00:50:30,570 --> 00:50:33,592
of the game, or of any sort of problem that you're actually

916
00:50:33,632 --> 00:50:35,493
in, because theoretically, people

917
00:50:35,533 --> 00:50:38,095
can make different arguments as to whether you should keep

918
00:50:38,155 --> 00:50:40,656
going, whether you should try to change parameters, et cetera.

919
00:50:40,877 --> 00:50:43,258
Yeah, so the answer to that is we didn't triage that much,

920
00:50:43,318 --> 00:50:44,859
because we didn't have much hardware

921
00:50:44,899 --> 00:50:47,100
to do many parallel trials.

922
00:50:47,661 --> 00:50:50,783
So we went with intuition and had some luck.

923
00:50:50,923 --> 00:50:54,005
But I'm certain that these agents I've shown you today,

924
00:50:54,165 --> 00:50:55,786
they are in no way optimal attuned.

925
00:50:56,266 --> 00:50:59,147
They could become much better if we put more resources

926
00:50:59,187 --> 00:51:03,229
towards it, or get better theory behind why

927
00:51:03,269 --> 00:51:04,069
they work as they do.

928
00:51:04,629 --> 00:51:08,331
So yeah, we didn't do much tuning.

929
00:51:09,111 --> 00:51:09,291
Yes?

930
00:51:10,952 --> 00:51:11,172
Hi.

931
00:51:12,653 --> 00:51:13,793
Very impressive presentation.

932
00:51:13,813 --> 00:51:14,433
Thank you very much.

933
00:51:15,194 --> 00:51:18,015
I was just curious about, you were talking about the network

934
00:51:18,055 --> 00:51:19,896
architecture, and it's very hard to tune.

935
00:51:20,696 --> 00:51:23,037
And it seems like it's one of those intractable problems

936
00:51:23,077 --> 00:51:25,458
that machine learning is actually very good at solving.

937
00:51:26,506 --> 00:51:28,687
Because we don't really understand how machine learning

938
00:51:28,727 --> 00:51:29,688
works, but it does work.

939
00:51:29,728 --> 00:51:31,388
We don't really know how the parameters work,

940
00:51:31,448 --> 00:51:33,990
but it's great at finding optimal parameters

941
00:51:34,010 --> 00:51:34,590
to do something.

942
00:51:34,670 --> 00:51:37,151
So do you think there's some sort of future

943
00:51:37,271 --> 00:51:40,413
for having machine learning to tune machine learning?

944
00:51:41,654 --> 00:51:41,854
Yes.

945
00:51:42,574 --> 00:51:46,156
Well, I don't know, but there is this topic of meta-learning,

946
00:51:46,596 --> 00:51:48,137
where you actually try to learn to learn.

947
00:51:49,165 --> 00:51:52,490
and you have another neural network controlling the learning algorithm itself.

948
00:51:53,432 --> 00:51:57,839
So I guess that's one route towards what you just described, but otherwise I don't know.

949
00:51:57,859 --> 00:51:58,620
Thank you.

950
00:52:00,684 --> 00:52:11,048
Hi. I'm curious if there's a reason why you chose vision as the primary input for your

951
00:52:11,108 --> 00:52:15,970
algorithms as opposed to more traditional knowledge representation of AI in the world.

952
00:52:15,990 --> 00:52:19,611
I mean, obviously you need some element of vision, but things like ray tracing, things

953
00:52:19,691 --> 00:52:24,393
like, you know, things that you would give a traditional AI as knowledge in order to

954
00:52:24,453 --> 00:52:24,733
reason.

955
00:52:26,014 --> 00:52:27,896
Why vision was the input instead of that?

956
00:52:27,996 --> 00:52:31,839
One reason was because that's how the algorithms were put

957
00:52:31,859 --> 00:52:32,760
together when we started.

958
00:52:32,820 --> 00:52:35,383
I mean, the Atari examples, they were with vision.

959
00:52:35,463 --> 00:52:37,144
But as I said, in the Battlefield example, we

960
00:52:37,184 --> 00:52:39,486
couldn't use vision anymore because the vision was, it

961
00:52:39,506 --> 00:52:41,708
took too long to train and it was too complex.

962
00:52:42,249 --> 00:52:44,691
So we actually used, what you said, ray tracing in

963
00:52:44,711 --> 00:52:45,432
the world instead.

964
00:52:45,792 --> 00:52:48,094
And of course, we could use some kind of internal game

965
00:52:48,114 --> 00:52:49,255
state representation as well.

966
00:52:49,295 --> 00:52:51,057
We don't have to use anything visual at all.

967
00:52:51,938 --> 00:52:57,945
But the problem with using internal game state, you have to be very careful to not give the agent more information than it should have.

968
00:52:58,466 --> 00:53:01,569
Because the game state, of course, is the perfect information of everything in the game.

969
00:53:01,810 --> 00:53:07,176
You only want the partial information that the player would have to get it to play like a player.

970
00:53:07,777 --> 00:53:11,541
So that's also one reason to use these simple vision-like observations.

971
00:53:14,639 --> 00:53:17,560
I was wondering if your team does any work in using deep

972
00:53:17,600 --> 00:53:21,221
learning to analyze analytics data from online games, or is

973
00:53:21,262 --> 00:53:22,262
that outside your scope?

974
00:53:22,922 --> 00:53:24,623
It is outside my scope, but we definitely

975
00:53:24,663 --> 00:53:26,004
do it within EA, yes.

976
00:53:27,084 --> 00:53:27,344
Thanks.

977
00:53:35,188 --> 00:53:36,128
If there's no one else.

978
00:53:36,929 --> 00:53:37,549
OK, one more.

979
00:53:39,129 --> 00:53:46,941
Did you do most of your experiments with very simple single hidden layers, or did you do a lot of experiments with different graph formations?

980
00:53:48,824 --> 00:53:50,907
You mean the network model itself?

981
00:53:51,187 --> 00:53:51,468
Correct.

982
00:53:52,559 --> 00:53:54,261
We didn't do much experimenting.

983
00:53:54,281 --> 00:53:58,427
We used essentially a slightly modified Atari network with

984
00:53:58,487 --> 00:54:00,449
some convolutional networks for the vision system.

985
00:54:00,770 --> 00:54:04,474
But we did add an LSTM, the LSTM version, to get some

986
00:54:04,635 --> 00:54:06,197
understanding of time sequences.

987
00:54:07,639 --> 00:54:08,880
But that's about it.

988
00:54:09,761 --> 00:54:09,982
Thank you.

989
00:54:24,892 --> 00:54:27,412
So the question was have we considered using internal game

990
00:54:27,452 --> 00:54:29,133
state to speed up the training?

991
00:54:29,633 --> 00:54:30,673
And yes, we have considered it.

992
00:54:30,733 --> 00:54:31,654
We haven't done it yet.

993
00:54:31,774 --> 00:54:35,335
But yes, trying to filter the game state to actually be

994
00:54:35,615 --> 00:54:37,535
approximately the same information that the player

995
00:54:37,555 --> 00:54:40,476
has, because the problem is you have to start to do a lot

996
00:54:40,496 --> 00:54:42,197
of occlusion and other things to hide, and

997
00:54:42,217 --> 00:54:43,677
that becomes expensive.

998
00:54:44,498 --> 00:54:46,718
It's actually much cheaper to give the agent all the

999
00:54:46,738 --> 00:54:49,159
information, but then, of course, it becomes omniscient

1000
00:54:49,219 --> 00:54:50,139
and superhuman.

1001
00:54:50,379 --> 00:54:51,020
And that's no fun.

1002
00:54:56,027 --> 00:54:56,796
Okay, thank you.

