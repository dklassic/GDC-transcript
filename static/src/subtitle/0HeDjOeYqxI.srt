1
00:00:05,847 --> 00:00:11,990
So many of you are here, biggest crowd I've, I've ever drawn. Uh, first things first, so it says

2
00:00:11,990 --> 00:00:17,474
Tatu Aalto lead graphics programmer, so that's not me, I'm Tomas Puha from Remedy.

3
00:00:18,368 --> 00:00:25,392
Um, Tatu is still in the airplane. This was him last night with the rest of the Remedy

4
00:00:25,392 --> 00:00:31,996
crew, they were stuck in Frankfurt so 2am San Francisco time we were going through this

5
00:00:31,996 --> 00:00:36,858
presentation so bear with me, I haven't had a lot of time. Time to rehearse but excited

6
00:00:36,858 --> 00:00:40,180
to represent the team. We got some really cool stuff.

7
00:00:41,581 --> 00:00:49,212
to show and discuss. And Tatu should be here if security and immigration and British Airways

8
00:00:49,212 --> 00:00:55,321
permits him to arrive at the end of the day so he can then answer questions a bit better than I can.

9
00:00:56,046 --> 00:00:57,187
All right, so let's go.

10
00:00:57,187 --> 00:01:02,113
So this is the roundabout topic, but as we all know,

11
00:01:02,113 --> 00:01:03,475
not really true.

12
00:01:03,475 --> 00:01:05,878
So what we're going to talk about here today is

13
00:01:05,878 --> 00:01:08,902
experiments with direct X-ray tracing in Remedies

14
00:01:08,902 --> 00:01:10,464
Northlight engine.

15
00:01:10,464 --> 00:01:11,765
A lot more exciting, right?

16
00:01:11,765 --> 00:01:12,186
OK, yeah, there you go.

17
00:01:16,185 --> 00:01:22,691
So, a little bit quickly about Remedy. So we're based back in Finland in Espoo, started in 1995.

18
00:01:22,691 --> 00:01:27,334
You might know us from Max Payne, Alan Wake and Quantum Break.

19
00:01:27,334 --> 00:01:33,079
And we used to be a single project studio, but now we're working on multiple projects.

20
00:01:33,079 --> 00:01:36,101
P7 and Crossfire, which is our first person shooter.

21
00:01:36,101 --> 00:01:41,726
And of course, since we're Scandinavian, we do do our own tech, kind of, because we're...

22
00:01:41,886 --> 00:01:43,807
started from the demo scene and all that.

23
00:01:43,807 --> 00:01:46,909
So we have a sizable team working on the engine and tools.

24
00:01:46,909 --> 00:01:50,612
OK, enough advertising about the awesome company I work for.

25
00:01:50,612 --> 00:01:55,335
So the agenda, what are we going to talk about here today?

26
00:01:55,335 --> 00:01:57,036
Well, I'm going to talk, you're going to listen, hopefully.

27
00:01:57,036 --> 00:01:59,618
So I'm going to give you a quick introduction to DirectX,

28
00:01:59,618 --> 00:02:02,960
ray tracing, then talk about area shadows,

29
00:02:02,960 --> 00:02:08,624
ambient occlusion, reflections, and indirect diffuse.

30
00:02:08,624 --> 00:02:08,784
OK.

31
00:02:08,784 --> 00:02:09,364
So.

32
00:02:11,390 --> 00:02:14,431
In order to explore, we created in the past couple of months

33
00:02:14,431 --> 00:02:17,312
this moody scene that's not from any of our games.

34
00:02:17,312 --> 00:02:18,933
It's a separate scene.

35
00:02:18,933 --> 00:02:21,013
And I'll start off with this demo

36
00:02:21,013 --> 00:02:23,374
and discuss the new technical aspects after it.

37
00:02:23,374 --> 00:02:27,256
So I'd recommend to keep an eye on how the pieces fit together,

38
00:02:27,256 --> 00:02:31,117
and especially how the different aspects of the lighting work.

39
00:02:31,117 --> 00:02:32,738
So we have audio, I hope.

40
00:03:10,365 --> 00:03:10,425
You

41
00:04:03,988 --> 00:04:14,778
Thanks for watching!

42
00:05:13,907 --> 00:05:15,747
All right, pretty cool, huh?

43
00:05:20,235 --> 00:05:25,279
OK, so video you just saw was captured from the engine

44
00:05:25,279 --> 00:05:27,840
with all the ray tracing effects enabled

45
00:05:27,840 --> 00:05:30,683
and with increased sample counts.

46
00:05:30,683 --> 00:05:33,665
So kind of high quality settings, if you will.

47
00:05:33,665 --> 00:05:36,307
I know that's a bit vague.

48
00:05:36,307 --> 00:05:38,669
So let's start with a very high level introduction

49
00:05:38,669 --> 00:05:39,609
on DirectX ray tracing.

50
00:05:39,609 --> 00:05:43,352
I will not go through the API in this presentation,

51
00:05:43,352 --> 00:05:45,133
but I'll try to give you some background on what

52
00:05:45,133 --> 00:05:47,816
you need to know before you can start shooting rays.

53
00:05:50,217 --> 00:05:54,158
So ray tracing API provides bottom and top level

54
00:05:54,158 --> 00:05:56,279
acceleration structures.

55
00:05:56,279 --> 00:05:58,680
The idea is that bottom level structure

56
00:05:58,680 --> 00:06:01,501
is constructed to hold geometry while the top level contains

57
00:06:01,501 --> 00:06:02,461
bottom level structures.

58
00:06:02,461 --> 00:06:04,782
Simple way to think about this is

59
00:06:04,782 --> 00:06:08,163
that each mesh should be a single bottom level structure.

60
00:06:08,163 --> 00:06:10,023
And top level tree is your scene that

61
00:06:10,023 --> 00:06:12,564
contains a bunch of bottom level structure instances

62
00:06:12,564 --> 00:06:13,865
with some transformation.

63
00:06:17,734 --> 00:06:20,616
First thing you're going to need is the bottom level structure

64
00:06:20,616 --> 00:06:23,119
for the static parts of your scene.

65
00:06:23,119 --> 00:06:25,861
In this picture, each red square represents bounds

66
00:06:25,861 --> 00:06:27,522
of a single bottom level tree.

67
00:06:27,522 --> 00:06:30,264
Building bottom level tree is very simple.

68
00:06:30,264 --> 00:06:31,766
In API, you'll have a function that

69
00:06:31,766 --> 00:06:35,389
takes normal directX GPU geometry and index buffers

70
00:06:35,389 --> 00:06:39,032
and returns bottom level tree built around the geometry.

71
00:06:42,189 --> 00:06:45,811
So in here, I have highlighted four instances of a small chair

72
00:06:45,811 --> 00:06:47,232
that you saw in the demo.

73
00:06:47,232 --> 00:06:49,273
These four chairs share the same geometry

74
00:06:49,273 --> 00:06:50,153
but have separate transformations.

75
00:06:50,153 --> 00:06:50,893
The mid-sized trees are small sofas.

76
00:06:50,893 --> 00:06:52,274
And the biggest ones are large, round sofas.

77
00:07:06,517 --> 00:07:08,797
In order to build a scene for ray tracing,

78
00:07:08,797 --> 00:07:12,318
you'll need to insert these bottom level structures

79
00:07:12,318 --> 00:07:13,258
into a top level structure.

80
00:07:13,258 --> 00:07:16,279
This is, again, a very simple thing to do.

81
00:07:16,279 --> 00:07:18,239
The API provides you a function that

82
00:07:18,239 --> 00:07:20,840
takes in a number of bottom level tree instances

83
00:07:20,840 --> 00:07:22,481
with transformations.

84
00:07:22,481 --> 00:07:24,261
So building a top level tree is fast.

85
00:07:24,261 --> 00:07:26,161
So you move static meshes every frame

86
00:07:26,161 --> 00:07:29,182
and then just build top level from scratch each frame.

87
00:07:32,831 --> 00:07:35,353
Supporting deformed geometry is a little bit more trickier,

88
00:07:35,353 --> 00:07:38,695
as the bottom level builder eats only static buffers.

89
00:07:38,695 --> 00:07:40,977
For us, the easiest way to support deformation

90
00:07:40,977 --> 00:07:43,258
was to create a simple compute shader that

91
00:07:43,258 --> 00:07:45,180
takes in geometry and skinning matrices

92
00:07:45,180 --> 00:07:47,581
and writes out deformed geometry.

93
00:07:47,581 --> 00:07:51,664
This is still very fast, as you can keep everything on GPU

94
00:07:51,664 --> 00:07:54,766
and just feed the output from the compute shader

95
00:07:54,766 --> 00:07:56,147
to the bottom level builder.

96
00:07:59,973 --> 00:08:01,035
And that's it.

97
00:08:01,035 --> 00:08:03,759
Now you can start shooting rays.

98
00:08:03,759 --> 00:08:07,764
I'll start off by showing a couple of examples

99
00:08:07,764 --> 00:08:08,966
you can try out with data only.

100
00:08:12,413 --> 00:08:18,817
Ambient occlusion. So ambient occlusion is purely visibility-based algorithm that can be easily tested out with ray tracing.

101
00:08:18,817 --> 00:08:25,801
Image you see here is taken by shooting four samples per pixel using cosine distribution.

102
00:08:25,801 --> 00:08:32,124
Maximum length of the rays is set to four meters and hits to the geometry are considered black and others white.

103
00:08:32,124 --> 00:08:37,567
This is added as direct replacement to what our screen space ambient occlusion outputs,

104
00:08:37,567 --> 00:08:41,650
so it is easy to compare results of different algorithms.

105
00:08:46,075 --> 00:08:49,358
So split screen, we got screen space ambient occlusion

106
00:08:49,358 --> 00:08:54,663
on the left side and ray traced ambient occlusion on the right.

107
00:08:54,663 --> 00:08:58,967
While the screen space technique does a pretty good job

108
00:08:58,967 --> 00:09:01,910
on grounding things, it's clearly lacking information.

109
00:09:01,910 --> 00:09:03,572
Screen space algorithms can't know

110
00:09:03,572 --> 00:09:06,435
what's outside the screen or behind the surfaces that

111
00:09:06,435 --> 00:09:08,437
are directly visible to the camera.

112
00:09:12,019 --> 00:09:16,181
Shooting rays is, of course, more expensive than looking up screen space depth.

113
00:09:16,181 --> 00:09:21,202
In our quick prototype, shooting a single ray with maximum distance of 4 meters cost

114
00:09:21,202 --> 00:09:23,263
roughly 5 milliseconds.

115
00:09:23,263 --> 00:09:31,806
The cost scales pretty much linearly, so shooting 16 rays for a full HD picture takes roughly

116
00:09:31,806 --> 00:09:32,827
80 milliseconds.

117
00:09:32,827 --> 00:09:35,948
The picture you see here is shot with 16 rays.

118
00:09:40,197 --> 00:09:43,819
Finally, here you can see a picture with comparison of different ray counts.

119
00:09:43,819 --> 00:09:49,141
You should note that these are all taken with just our basic temporal anti-aliasing.

120
00:09:49,141 --> 00:09:53,203
I'm sure it would be possible to do a better job with filtering noise and you can't make

121
00:09:53,203 --> 00:09:54,803
it out very well in the image.

122
00:09:54,803 --> 00:09:58,625
There's a lot of noise on the left and far less on the right side of the image.

123
00:09:58,625 --> 00:09:59,445
All right, so moving on.

124
00:10:05,192 --> 00:10:06,714
Shadows.

125
00:10:06,714 --> 00:10:10,197
We implemented a path to replace sunlight cascaded shadow maps

126
00:10:10,197 --> 00:10:11,257
by shooting rays.

127
00:10:11,257 --> 00:10:14,280
If you happen to feel the shadows

128
00:10:14,280 --> 00:10:16,402
of the directional light into full screen buffer

129
00:10:16,402 --> 00:10:18,964
before actual lighting, it's very easy

130
00:10:18,964 --> 00:10:21,226
to just replace cascade map lookup shader

131
00:10:21,226 --> 00:10:24,188
with ray tracing kernel that writes into the same texture.

132
00:10:28,619 --> 00:10:31,623
Here you can see a comparison using shadow maps

133
00:10:31,623 --> 00:10:35,227
on the left side and ray tracing on the right side.

134
00:10:35,227 --> 00:10:36,829
In order to be fair, I didn't try to tune up

135
00:10:36,829 --> 00:10:38,091
the resolution on the shadow map,

136
00:10:38,091 --> 00:10:40,434
so this is just something we get out of the engine

137
00:10:40,434 --> 00:10:42,836
with the default settings.

138
00:10:42,836 --> 00:10:46,141
Anyway, the amount of detail you can gain with accurate shadows

139
00:10:46,141 --> 00:10:47,462
is pretty remarkable.

140
00:10:51,657 --> 00:10:56,939
In addition to accuracy, it's easy to do real area shadows with ray tracing.

141
00:10:56,939 --> 00:11:00,921
Clip here is using eight samples.

142
00:11:00,921 --> 00:11:04,663
Sampling pattern is basic blue noise that's changing between frames.

143
00:11:04,663 --> 00:11:09,086
For each sample on the pixel, we offset the same blue noise with Sobol sequence and wrap

144
00:11:09,086 --> 00:11:10,787
the values on disk.

145
00:11:15,064 --> 00:11:19,065
So here you can see a close-up of the same effect.

146
00:11:19,065 --> 00:11:21,326
As you can expect with ray tracing,

147
00:11:21,326 --> 00:11:24,087
the contact points are accurate, and you

148
00:11:24,087 --> 00:11:26,107
get a nice mix of sharp and soft shadows.

149
00:11:26,107 --> 00:11:29,988
While this is nothing new or radical on algorithm level,

150
00:11:29,988 --> 00:11:32,329
it's very cool to be able to go and look at your existing game

151
00:11:32,329 --> 00:11:35,290
scenes and content to see how much detail is lost

152
00:11:35,290 --> 00:11:36,970
without accurate shadows.

153
00:11:49,473 --> 00:11:51,275
And here's one more example.

154
00:11:51,275 --> 00:11:54,337
So looking at the amount of detail added to a simple scene

155
00:11:54,337 --> 00:11:55,918
like this makes you wonder if there's any sense to keep

156
00:11:55,918 --> 00:11:57,519
increasing the output resolution before we can sample lower

157
00:11:57,519 --> 00:11:58,440
resolution with good quality.

158
00:11:58,440 --> 00:12:00,181
So in the demo, the performance on shooting shadow rays

159
00:12:00,181 --> 00:12:01,382
is a bit better than with ambient occlusion.

160
00:12:17,624 --> 00:12:22,110
Even through the rays are longer, resolving shadow visibility is more coherent, low than

161
00:12:22,110 --> 00:12:22,211
AO.

162
00:12:22,211 --> 00:12:26,717
Getting something like this up and running with existing DX12 engine should take you

163
00:12:26,717 --> 00:12:28,499
something like a couple of days.

164
00:12:28,499 --> 00:12:34,408
I can say that it's definitely worth the effort, even if you are just going to use it as reference.

165
00:12:39,149 --> 00:12:42,692
So I've been talking about algorithms that can be implemented by knowing only visibility

166
00:12:42,692 --> 00:12:44,093
between two points in the scene.

167
00:12:44,093 --> 00:12:48,216
Going beyond visibility requires access to additional data.

168
00:12:48,216 --> 00:12:54,321
In this selection I'll quickly introduce how we expose geometry, materials, and lighting

169
00:12:54,321 --> 00:12:56,502
to be used in ray tracing kernels.

170
00:13:00,945 --> 00:13:04,046
On a geometry hit, the API provides you

171
00:13:04,046 --> 00:13:07,407
the intersection point in scene and triangle base.

172
00:13:07,407 --> 00:13:09,248
Other geometric properties than position

173
00:13:09,248 --> 00:13:10,888
you will need to read in manually

174
00:13:10,888 --> 00:13:13,669
and interpolate based on barycentric coordinate.

175
00:13:13,669 --> 00:13:16,009
This means that you will need to be

176
00:13:16,009 --> 00:13:19,110
able to access all your geometry from shader.

177
00:13:19,110 --> 00:13:21,291
New DirectX comes with a new binding API

178
00:13:21,291 --> 00:13:24,131
where you can set different resource bindings

179
00:13:24,131 --> 00:13:27,032
for each instance of a bottom level piece in the scene.

180
00:13:32,623 --> 00:13:36,067
So let's say I have two instances of a chair.

181
00:13:36,067 --> 00:13:38,269
Each chair has five pieces for different materials,

182
00:13:38,269 --> 00:13:40,131
like in the diagram on this slide.

183
00:13:40,131 --> 00:13:42,634
I can now define separate resource bindings

184
00:13:42,634 --> 00:13:44,055
for all the pieces, so in total,

185
00:13:44,055 --> 00:13:46,538
I have 10 resource binding records.

186
00:13:46,538 --> 00:13:49,802
Each of the records can then have a custom binding

187
00:13:49,802 --> 00:13:52,004
for vertex and index buffer, for example.

188
00:13:56,367 --> 00:14:01,229
It would be possible to bind material data using the same API, but just before starting

189
00:14:01,229 --> 00:14:06,832
DXR, we had completed a new material binding system that works also with basic DX12.

190
00:14:06,832 --> 00:14:11,474
By packing all the material parameters into a single buffer and reserving dedicated space

191
00:14:11,474 --> 00:14:16,096
on front of the descriptor heap, we could already do draw calls without additional bindings.

192
00:14:16,096 --> 00:14:21,379
This also means that we can access everything related to materials in a single shader call.

193
00:14:21,379 --> 00:14:25,200
Each index on this slide refers to a single material.

194
00:14:31,294 --> 00:14:33,875
Lighting happened to be pretty easy for us.

195
00:14:33,875 --> 00:14:35,416
Since quantum break, we have already

196
00:14:35,416 --> 00:14:37,716
had all the lighting data accessible to a single shader

197
00:14:37,716 --> 00:14:38,277
call.

198
00:14:38,277 --> 00:14:41,238
This means that we pack all the shadow and projection

199
00:14:41,238 --> 00:14:43,779
maps in a few atlases.

200
00:14:43,779 --> 00:14:47,220
We also keep the light parameters packed in buffers.

201
00:14:47,220 --> 00:14:48,000
That's it.

202
00:14:48,000 --> 00:14:51,481
Now we have geometry, materials, and lights exposed in a way

203
00:14:51,481 --> 00:14:55,043
that we can access everything in a single ray trace kernel.

204
00:14:55,043 --> 00:14:57,503
OK, so let's get into trying out reflections next.

205
00:14:59,253 --> 00:15:02,715
Am I, is the pace the right, not going too fast?

206
00:15:02,715 --> 00:15:03,516
Okay.

207
00:15:03,516 --> 00:15:04,096
Wrapped attention.

208
00:15:04,096 --> 00:15:08,680
Okay, so following section is dedicated

209
00:15:08,680 --> 00:15:10,721
to implementing reflections with ray tracing.

210
00:15:10,721 --> 00:15:13,423
I'll start up by quick comparison

211
00:15:13,423 --> 00:15:14,804
to our screen space technique

212
00:15:14,804 --> 00:15:16,585
and continue to visualizing how

213
00:15:16,585 --> 00:15:18,827
the previously introduced data bindings are used.

214
00:15:18,827 --> 00:15:21,289
As with the previous technique shown,

215
00:15:21,289 --> 00:15:23,290
we implemented ray traced reflections

216
00:15:23,290 --> 00:15:26,432
as a direct replacement to screen space technique.

217
00:15:26,432 --> 00:15:28,934
This way, it's easy to compare the results again.

218
00:15:31,957 --> 00:15:34,318
So here we can see the split-screen comparison

219
00:15:34,318 --> 00:15:36,519
with screen space reflections on the left side

220
00:15:36,519 --> 00:15:38,120
and ray tracing on the right side.

221
00:15:38,120 --> 00:15:41,541
This is screen space only.

222
00:15:41,541 --> 00:15:47,764
Reflection ray generation works the exact same way

223
00:15:47,764 --> 00:15:51,166
in both the screen space and ray trace technique.

224
00:15:51,166 --> 00:15:52,487
For each sample on the screen.

225
00:15:59,983 --> 00:16:02,986
You read properties of the surface from the geometry buffer.

226
00:16:02,986 --> 00:16:07,531
Then, based on smoothness, you pick a random direction from GGX distribution

227
00:16:07,531 --> 00:16:10,514
and transform it according to geometry normal.

228
00:16:14,330 --> 00:16:21,494
On screen space, we treat reflection ray as a 2D line from a point on screen to a point on edge of the screen.

229
00:16:21,494 --> 00:16:26,277
In order to figure out if the ray hit geometry, we take number of depth samples along the line.

230
00:16:26,277 --> 00:16:33,802
It's possible to miss features along the way due to holes in sampling and due to having access to only directly visible surfaces.

231
00:16:37,304 --> 00:16:39,985
Also the hit position can be outside of the screen.

232
00:16:39,985 --> 00:16:42,486
I don't have any backup solution in use on this picture.

233
00:16:42,486 --> 00:16:45,008
The most obvious fallback would be to sample some cube map

234
00:16:45,008 --> 00:16:47,929
when the ray hit is out of the screen.

235
00:16:47,929 --> 00:16:50,630
Okay, so getting back to ray tracing.

236
00:16:50,630 --> 00:16:52,191
So ray tracing doesn't miss any geometry

237
00:16:52,191 --> 00:16:54,772
that is out of the screen behind visible surfaces

238
00:16:54,772 --> 00:16:56,493
or due to lack of samples.

239
00:16:56,493 --> 00:16:59,394
On a geometry hit, we need to figure out

240
00:16:59,394 --> 00:17:02,216
radiance transmitted towards reflection ray origin.

241
00:17:03,149 --> 00:17:08,492
For this, we need to know the material and the lighting at the heat location.

242
00:17:08,492 --> 00:17:14,436
The XR provides the heat distance, position, and barycentric position within a triangle

243
00:17:14,436 --> 00:17:17,477
as in shader intrinsic.

244
00:17:17,477 --> 00:17:25,962
With this information, we can interpolate UVs from the geometry buffers exposed earlier.

245
00:17:28,192 --> 00:17:32,835
So the UVs at the reflection heat location will end up looking a mess like this.

246
00:17:32,835 --> 00:17:37,079
With UV and other data we can sample the rest of the material properties.

247
00:17:37,079 --> 00:17:42,663
Here you can see the diffuse albedo on the reflected surface.

248
00:17:42,663 --> 00:17:48,307
Next it's time to evaluate the lighting and combine the results.

249
00:17:48,307 --> 00:17:52,330
This is lighting at the reflected surface.

250
00:17:52,330 --> 00:17:56,834
We can now combine this and material with Fresnel term.

251
00:17:59,012 --> 00:18:01,436
And here's what the result looks like.

252
00:18:01,436 --> 00:18:04,222
This is reflection-only visualization.

253
00:18:04,222 --> 00:18:05,825
In order to get the final image, we

254
00:18:05,825 --> 00:18:08,671
combine this with diffuse and specular component

255
00:18:08,671 --> 00:18:09,773
of direct lighting.

256
00:18:14,015 --> 00:18:19,782
The final image. As you can expect with the techniques like this, the end result will have some amount of noise.

257
00:18:19,782 --> 00:18:26,650
We didn't have a lot of time to experiment with noise filtering, but I have a couple of image loops to visualize the problems.

258
00:18:26,650 --> 00:18:34,039
The following loops are taken from a bit harder lighting gaze and are using our basic temporal anti-aliasing.

259
00:18:41,385 --> 00:18:43,625
So this is the raw image you get when

260
00:18:43,625 --> 00:18:45,586
shooting a single reflection ray per pixel

261
00:18:45,586 --> 00:18:47,187
without any filtering.

262
00:18:47,187 --> 00:18:50,208
The lighting here is from sunlight.

263
00:18:50,208 --> 00:18:52,068
It's a pretty complicated, hard case

264
00:18:52,068 --> 00:18:55,309
as the lighting has high contrast and a lot of detail.

265
00:18:55,309 --> 00:18:56,810
As we all know, the variance in lighting

266
00:18:56,810 --> 00:18:58,790
decreases when shooting more rays per pixel.

267
00:18:58,790 --> 00:19:02,712
While shooting more rays for all the pixels is expensive,

268
00:19:02,712 --> 00:19:05,452
with ray tracing it's easy to pick a few locations

269
00:19:05,452 --> 00:19:06,453
to shoot more.

270
00:19:09,728 --> 00:19:16,674
It's almost free to shoot a couple more rays for the pixels that are over two times more bright than the white point in the image.

271
00:19:16,674 --> 00:19:21,897
In this loop you can see that we can remove most of the fireflies with just this.

272
00:19:21,897 --> 00:19:29,383
It would be interesting to see if we're using something more advanced like gradient domain sampling would get us.

273
00:19:29,383 --> 00:19:31,104
Yeah, you can, well, you can kind of make it up.

274
00:19:32,817 --> 00:19:38,422
So in the demo, we are also clamping the reflections that are over 10 times brighter than the white

275
00:19:38,422 --> 00:19:39,783
point on the image.

276
00:19:39,783 --> 00:19:44,947
This is of course a hack and it's not recommended you do this unless you're shipping.

277
00:19:44,947 --> 00:19:48,310
Great advice.

278
00:19:52,244 --> 00:19:54,465
While it would be perfectly possible to use ray tracing

279
00:19:54,465 --> 00:19:56,527
to resolve shadows at reflected surfaces,

280
00:19:56,527 --> 00:20:00,069
we use shadow maps for most of the lights in the demo.

281
00:20:00,069 --> 00:20:02,391
The only exception is sunlight.

282
00:20:02,391 --> 00:20:05,132
Since we use cascaded shadow maps with the sun,

283
00:20:05,132 --> 00:20:08,034
there is valid depth for only the pixels on the screen.

284
00:20:08,034 --> 00:20:11,556
Having to shoot a single shadow ray for the sun

285
00:20:11,556 --> 00:20:13,918
on every reflection ray almost doubles

286
00:20:13,918 --> 00:20:15,519
the cost of the reflections.

287
00:20:15,519 --> 00:20:17,720
OK, that's all about reflection, so we're

288
00:20:17,720 --> 00:20:20,402
going to look at the diffuse lighting next.

289
00:20:23,552 --> 00:20:27,416
So in this image, I have diffused global illumination

290
00:20:27,416 --> 00:20:30,418
interpolated from the volume texture.

291
00:20:30,418 --> 00:20:33,601
The approach you see here was implemented for quantum break

292
00:20:33,601 --> 00:20:37,945
and is in use by default in our Northlight engine.

293
00:20:37,945 --> 00:20:40,387
Voxel resolution of the GI data here is 25 centimeters.

294
00:20:40,387 --> 00:20:45,351
We combine the results with screen space ambient occlusion.

295
00:20:46,324 --> 00:20:50,007
As I showed you earlier, we have a path

296
00:20:50,007 --> 00:20:52,930
to replace screen space AO with a ray traced one.

297
00:20:52,930 --> 00:20:54,911
So let's have a look how that looks.

298
00:20:54,911 --> 00:20:57,753
So this is the same data combined

299
00:20:57,753 --> 00:21:00,836
with ray traced ambient occlusion.

300
00:21:00,836 --> 00:21:03,798
Results are better, but this still

301
00:21:03,798 --> 00:21:06,481
looks like ambient occlusion applied on top of volume data

302
00:21:06,481 --> 00:21:10,544
that doesn't have enough resolution for the case seen

303
00:21:10,544 --> 00:21:10,704
here.

304
00:21:10,704 --> 00:21:13,246
Instead of just shooting a lot of rays

305
00:21:13,246 --> 00:21:15,868
around to resolve indirect diffuse lighting.

306
00:21:16,450 --> 00:21:22,554
I'll be talking a bit about improving sampling of volumetric global illumination and resolving

307
00:21:22,554 --> 00:21:24,215
near-field global illumination.

308
00:21:24,215 --> 00:21:27,497
So let's quickly break this down with a few diagrams.

309
00:21:27,497 --> 00:21:34,701
Our pre-computed global illumination is stored in sparse volume texture.

310
00:21:34,701 --> 00:21:37,503
The grid on the screen represents the data.

311
00:21:37,503 --> 00:21:43,306
Each crossing contains indirect lighting data that has been calculated with path tracer.

312
00:21:45,965 --> 00:21:48,766
The blue shape on top of the global illumination volume

313
00:21:48,766 --> 00:21:50,606
is static geometry that we use for calculating the data.

314
00:21:50,606 --> 00:21:52,407
You can see that the dense parts of the GI volume

315
00:21:52,407 --> 00:21:54,388
are on the areas that overlap with geometry.

316
00:21:54,388 --> 00:21:56,248
In addition to static geometry, we have dynamic geometry.

317
00:21:56,248 --> 00:21:57,769
The smaller green piece in the middle of the picture

318
00:21:57,769 --> 00:21:58,289
is dynamic geometry.

319
00:22:12,052 --> 00:22:15,075
Dynamic geometry is not used in pre-calculation,

320
00:22:15,075 --> 00:22:19,138
so part of it resides on low resolution area of the volume.

321
00:22:19,138 --> 00:22:20,960
There are a couple of problems in using the lighting directly

322
00:22:20,960 --> 00:22:23,222
sampled from the volume.

323
00:22:23,222 --> 00:22:28,887
First, we'll miss the dynamic geometry completely.

324
00:22:28,887 --> 00:22:33,250
We've been relying on the screen space ambient occlusion

325
00:22:33,250 --> 00:22:36,773
to ground dynamic geometry to the rest of the scene.

326
00:22:39,511 --> 00:22:42,474
Second problem is linear filtering of the volume data.

327
00:22:42,474 --> 00:22:45,357
On the surfaces that are not aligned to volume,

328
00:22:45,357 --> 00:22:48,180
we get staircasing that can be quite visible

329
00:22:48,180 --> 00:22:50,382
due to low volume resolution.

330
00:22:50,382 --> 00:22:54,486
Finally, low resolution will cause the lighting

331
00:22:54,486 --> 00:22:56,908
to leak through the thin geometry.

332
00:22:56,908 --> 00:22:59,471
It's possible to try weighting the sampling so

333
00:22:59,471 --> 00:23:01,813
that it will leak a bit less, but ultimately, you

334
00:23:01,813 --> 00:23:03,415
are just moving the artifacts.

335
00:23:08,217 --> 00:23:13,802
Instead of sampling the volume directly on the surface, we can shoot rays and sample

336
00:23:13,802 --> 00:23:17,105
global illumination on the locations that didn't hit the geometry.

337
00:23:17,105 --> 00:23:21,228
Positions that are not on the surface are less likely to contain leaked light in our

338
00:23:21,228 --> 00:23:21,829
data.

339
00:23:21,829 --> 00:23:26,193
This will reduce leaking and results in smoother interpolation.

340
00:23:28,668 --> 00:23:33,672
Simplest thing to do when hitting geometry is to consider the sample black.

341
00:23:33,672 --> 00:23:38,116
This is pretty close to calculating ambient occlusion with a benefit that it helps with

342
00:23:38,116 --> 00:23:41,739
light leaking and results in better interpolation of the GI data.

343
00:23:41,739 --> 00:23:48,124
So let's take a look at the comparison between this and direct sampling of volume data with

344
00:23:48,124 --> 00:23:49,365
ambient occlusion.

345
00:23:57,640 --> 00:23:59,601
So this is the GI volume sample directly

346
00:23:59,601 --> 00:24:03,183
and combined with ray traced ambient occlusion.

347
00:24:03,183 --> 00:24:06,084
If you look at the table or a magazine

348
00:24:06,084 --> 00:24:08,005
on the bottom left corner of the image,

349
00:24:08,005 --> 00:24:12,127
you can clearly see that there is something wrong going on.

350
00:24:12,127 --> 00:24:15,908
By modifying the way we use the volumetric GI data,

351
00:24:15,908 --> 00:24:18,990
we can fix many of the issues that can be

352
00:24:18,990 --> 00:24:20,891
seen when using direct sampling.

353
00:24:20,891 --> 00:24:23,352
I'll toggle a couple of times back and forth

354
00:24:23,352 --> 00:24:25,232
and highlight the locations to look at.

355
00:24:25,232 --> 00:24:27,273
So direct sampling and ray traced AO.

356
00:24:32,348 --> 00:24:34,489
and ray traced gather.

357
00:24:34,489 --> 00:24:37,770
So this is other difficult location or example for volume data.

358
00:24:37,770 --> 00:24:43,353
Look at the dome on top of the scene

359
00:24:43,353 --> 00:24:46,815
that is made of thin geometry.

360
00:24:46,815 --> 00:24:49,436
So ray traced gather again.

361
00:25:00,745 --> 00:25:04,707
So on the previous example, we considered rays

362
00:25:04,707 --> 00:25:07,288
that hit any geometry to be black.

363
00:25:07,288 --> 00:25:09,488
Instead of darkening the image, we

364
00:25:09,488 --> 00:25:12,229
can do better and evaluate lighting and material

365
00:25:12,229 --> 00:25:13,410
like we did with reflections.

366
00:25:13,410 --> 00:25:15,030
So the same thing I showed you earlier.

367
00:25:26,756 --> 00:25:30,400
So here, I've added a single bounce near field

368
00:25:30,400 --> 00:25:32,341
global illumination.

369
00:25:32,341 --> 00:25:34,924
The accuracy on the first bounce is something

370
00:25:34,924 --> 00:25:38,307
that you couldn't easily store in limited size volume.

371
00:25:38,307 --> 00:25:40,509
Also, the indirect lighting read from the volume

372
00:25:40,509 --> 00:25:41,790
is better filtered that you could do with a few rays.

373
00:25:41,790 --> 00:25:44,393
This is the direct lighting that was used for the near field.

374
00:25:57,692 --> 00:26:02,855
This is direct lighting combined with indirect diffuse.

375
00:26:02,855 --> 00:26:11,739
And this is final lighting with specular.

376
00:26:11,739 --> 00:26:13,140
And this is the final image.

377
00:26:13,140 --> 00:26:14,320
All right, so summary.

378
00:26:22,488 --> 00:26:26,131
on this stuff. So easy access to state-of-the-art GPU ray tracing via DXR.

379
00:26:26,131 --> 00:26:30,474
Performance is getting there. Yes, that's vague.

380
00:26:30,474 --> 00:26:34,658
Easy to prototype algorithms that don't fit the rasterization

381
00:26:34,658 --> 00:26:38,781
and possible to combine with existing low frequency structures.

382
00:26:40,848 --> 00:26:43,670
And of course, shout out to the team back home in Espoo,

383
00:26:43,670 --> 00:26:45,692
or people on the plane right now that

384
00:26:45,692 --> 00:26:49,756
worked really hard on this demo in the last couple of months.

385
00:26:49,756 --> 00:26:52,499
Of course, thanks to the folks at Nvidia as well.

386
00:26:52,499 --> 00:26:56,162
This was a fun thing to do.

387
00:26:56,162 --> 00:26:57,583
Then the absolute necessary thing

388
00:26:57,583 --> 00:26:59,085
that Finland is an awesome country,

389
00:26:59,085 --> 00:27:00,266
and we are hiring at Remedy.

390
00:27:00,266 --> 00:27:02,248
It's not cold.

391
00:27:02,248 --> 00:27:02,928
It's all a lie.

392
00:27:02,928 --> 00:27:04,049
There's never a winter.

393
00:27:08,023 --> 00:27:14,726
And we can answer some questions, I can, but Tatu will be here by the end of the day, but

394
00:27:14,726 --> 00:27:19,107
write down that email and probably should have said in the beginning, so we have the

395
00:27:19,107 --> 00:27:28,131
entire slide deck with the notes and including the video on our website as I speak.

396
00:27:28,131 --> 00:27:31,412
So you can go check that out.

397
00:27:31,412 --> 00:27:35,174
Will we try to answer some questions or no?

398
00:27:36,617 --> 00:27:47,243
Yes? Okay. Well, thank you very much.

399
00:27:47,243 --> 00:27:47,383
And... ...

400
00:27:47,383 --> 00:27:51,245
... ...questions, please. ...

401
00:27:51,245 --> 00:27:51,546
... Hi.

402
00:27:51,546 --> 00:28:00,971
It looks really great, but I think the obvious question is that all the images are very noisy.

403
00:28:00,971 --> 00:28:04,353
So, did you think about applying some denoise filter?

404
00:28:06,366 --> 00:28:15,749
Yes, we didn't have too much time to explore denoising but that's a very good question

405
00:28:15,749 --> 00:28:27,354
and I think that for many of these effects a working and fast enough denoising solution

406
00:28:27,354 --> 00:28:31,875
is needed to ship something real.

407
00:28:32,594 --> 00:28:38,619
But there might be some use cases where you can go pretty

408
00:28:38,619 --> 00:28:51,448
far with only one sample per pixel as well.

409
00:28:51,448 --> 00:28:54,591
Could you use the microphone please?

410
00:29:00,837 --> 00:29:04,898
Is this hybrid for thin geo versus for volumetric ray

411
00:29:04,898 --> 00:29:05,298
traced?

412
00:29:05,298 --> 00:29:06,699
I see some god rays over there.

413
00:29:06,699 --> 00:29:08,300
I'm assuming those are ray traced.

414
00:29:08,300 --> 00:29:11,361
And probably the dome is geo direct rendered.

415
00:29:11,361 --> 00:29:12,521
Is it a combination of everything?

416
00:29:12,521 --> 00:29:15,582
Or is the full scene either volumetric,

417
00:29:15,582 --> 00:29:16,443
full scene is direct rendered?

418
00:29:16,443 --> 00:29:17,623
How does that work?

419
00:29:17,623 --> 00:29:19,704
Sorry if I understood completely.

420
00:29:19,704 --> 00:29:23,586
But the volumetric effects, the god rays you saw here,

421
00:29:23,586 --> 00:29:25,626
they are not ray traced.

422
00:29:25,626 --> 00:29:29,128
They are done with more traditional techniques.

423
00:29:31,815 --> 00:29:36,400
And same for the lighting effects that I see on the dome?

424
00:29:36,400 --> 00:29:39,603
What do you mean by the lighting effects?

425
00:29:39,603 --> 00:29:45,730
The lines and the actual blur that's being caused by those.

426
00:29:45,730 --> 00:29:49,033
Yeah, the bright lines drawn on the dome, yeah, they are just...

427
00:29:49,033 --> 00:29:51,376
They're not atmospheric effects, that's what I'm asking.

428
00:29:51,376 --> 00:29:52,377
Yeah, they are not ray traced.

429
00:29:56,497 --> 00:30:03,228
Hi, great talk. Thanks for all of this. I have a question based on the video you showed first.

430
00:30:04,245 --> 00:30:09,207
And we can see there is a lot of flickering and fireflies.

431
00:30:09,207 --> 00:30:13,909
And I was wondering if you tried the technique that you

432
00:30:13,909 --> 00:30:18,591
convert the normal buffer to a curvature buffer,

433
00:30:18,591 --> 00:30:23,473
and then modifying the roughness based on this curvature

434
00:30:23,473 --> 00:30:26,774
to having less fireflies like this one.

435
00:30:31,805 --> 00:30:38,630
because it happens on really smooth materials and when you have a really high curvature

436
00:30:38,630 --> 00:30:45,035
on smooth materials you have this kind of artifacts which is basically a specular aliasing

437
00:30:45,035 --> 00:30:50,119
and when you want to avoid this kind of stuff you can use the normal to curvature and curvature

438
00:30:50,119 --> 00:30:52,601
to roughness modification.

439
00:30:54,913 --> 00:30:58,075
Sorry for my question, it seemed to be not understood.

440
00:30:58,075 --> 00:31:01,437
Yeah, I'm sorry if I completely understand the question.

441
00:31:01,437 --> 00:31:06,501
Yeah, basically what did you try for avoiding specular aliasing?

442
00:31:06,501 --> 00:31:09,883
Basically there is a reasonable amount of flickering due to fireflies.

443
00:31:20,363 --> 00:31:27,927
We didn't do much more than what was described in the presentation, shooting additional rays

444
00:31:27,927 --> 00:31:38,953
on overprimed pixels and we did also experiment with some denoising solutions but they were

445
00:31:38,953 --> 00:31:43,635
not included in this video.

446
00:31:45,989 --> 00:31:50,375
When you try to do the reflections with a very limited

447
00:31:50,375 --> 00:31:53,359
rays per pixel, you get noise and fireflies.

448
00:31:53,359 --> 00:31:56,444
You have to do something about them.

449
00:31:56,444 --> 00:31:57,445
Okay, thanks.

450
00:32:02,823 --> 00:32:08,485
Okay, and we have this at the Nvidia booth, so you can go check it out.

451
00:32:08,485 --> 00:32:13,328
And really, if there's additional questions, please don't hesitate to email Tato and we'll

452
00:32:13,328 --> 00:32:15,368
make sure he'll get back to you.

453
00:32:15,368 --> 00:32:17,349
Thanks very much for your attention.

454
00:32:17,349 --> 00:32:18,110
Thank you very much.

