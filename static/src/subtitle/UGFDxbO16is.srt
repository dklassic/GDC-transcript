1
00:00:05,392 --> 00:00:09,717
Hello, thank you for coming to our talk today on evolving mixed reality.

2
00:00:10,698 --> 00:00:11,159
I'm Jono.

3
00:00:12,581 --> 00:00:13,502
I'm Samantha Gorman.

4
00:00:14,383 --> 00:00:15,024
I'm Ron Gull.

5
00:00:15,645 --> 00:00:16,205
I'm Brian Schwab.

6
00:00:17,960 --> 00:00:20,001
So before we really launch into the talk,

7
00:00:20,201 --> 00:00:21,942
I just want to kind of frame the space

8
00:00:21,962 --> 00:00:23,002
that we're talking about a little bit.

9
00:00:23,983 --> 00:00:26,684
So right now, a lot of AR that we see looks like this.

10
00:00:27,304 --> 00:00:29,845
A lot of it is sort of single-plane, user-directed,

11
00:00:31,326 --> 00:00:32,707
tabletop, six-doff AR.

12
00:00:32,987 --> 00:00:34,687
And people have done really, really cool,

13
00:00:34,807 --> 00:00:36,548
incredible, inspiring, exciting work there.

14
00:00:37,189 --> 00:00:38,489
But I think that also everybody in this room

15
00:00:38,509 --> 00:00:39,890
probably has this sense of like,

16
00:00:39,990 --> 00:00:43,672
this is the first step towards this true mixed reality thing.

17
00:00:45,417 --> 00:00:48,880
So to draw a couple examples what we mean when we say that,

18
00:00:49,581 --> 00:00:52,964
you know, that can mean everything from immersive games.

19
00:00:53,725 --> 00:00:54,025
Sorry?

20
00:00:54,626 --> 00:00:55,066
XR.

21
00:00:55,327 --> 00:00:55,787
XR.

22
00:00:57,929 --> 00:01:03,935
Everything from immersive games to industrial training to IoT controls.

23
00:01:05,277 --> 00:01:07,419
I'll throw more examples out there, the kind of stuff you're thinking about.

24
00:01:10,927 --> 00:01:14,308
I mean, the thing is, where we're going is just compute

25
00:01:14,348 --> 00:01:17,268
anywhere, pixels anywhere, with whatever device you use.

26
00:01:17,809 --> 00:01:18,649
That's where we want to hit.

27
00:01:19,809 --> 00:01:20,749
We're a long way from there.

28
00:01:21,410 --> 00:01:21,770
Absolutely.

29
00:01:21,790 --> 00:01:27,811
So let's meet each of our speakers.

30
00:01:29,843 --> 00:01:30,424
Hi, everybody.

31
00:01:30,444 --> 00:01:35,267
So my name is Ron Gow, and I work at the Interactive Media

32
00:01:35,287 --> 00:01:36,708
Group at Microsoft Research.

33
00:01:37,028 --> 00:01:38,569
Just to make sure everybody understand,

34
00:01:38,709 --> 00:01:39,930
I'm not part of HoloLens.

35
00:01:39,970 --> 00:01:42,591
So don't take anything I say as something

36
00:01:42,611 --> 00:01:43,992
that is going to happen with HoloLens.

37
00:01:44,032 --> 00:01:46,774
We are working with them in Microsoft Research,

38
00:01:46,874 --> 00:01:50,496
but I know as much about them as I know about Magic Leap.

39
00:01:50,636 --> 00:01:52,697
So we contribute ideas.

40
00:01:52,737 --> 00:01:54,718
We have no idea what they are doing with them.

41
00:01:57,534 --> 00:02:03,722
So in our group, we are working on several things that are relevant to this presentation

42
00:02:04,102 --> 00:02:05,464
and also on other things.

43
00:02:06,205 --> 00:02:14,895
We started to focus on AR and VR and XR, sorry, about five to six years ago, and we did a

44
00:02:14,935 --> 00:02:15,816
couple of projects.

45
00:02:18,506 --> 00:02:24,911
In our group we develop devices, we research ideas of how to design to these environments,

46
00:02:25,091 --> 00:02:28,213
and we have a lot of fun doing it.

47
00:02:28,433 --> 00:02:29,834
It's pretty cool.

48
00:02:30,355 --> 00:02:30,735
Well, that's it.

49
00:02:36,688 --> 00:02:43,291
Hello, so I'm Samantha Gorman and I'm the co-founder of Tender Claws and we're about a 10 person studio and

50
00:02:43,331 --> 00:02:48,834
We're often we're artists directors designers that are often commissioned to think creatively around emerging tech

51
00:02:49,754 --> 00:02:52,776
Often at the same time as that tech is actually being developed.

52
00:02:53,716 --> 00:02:57,058
So this is, I'm going to talk a little bit about Tendar, which was our

53
00:02:57,578 --> 00:03:00,839
collaboration with ARCore right when ARCore was being developed.

54
00:03:01,880 --> 00:03:05,141
And as a case study, I was interested in going

55
00:03:06,162 --> 00:03:10,024
kind of away from tabletop, not because I don't think it's a really great area,

56
00:03:10,064 --> 00:03:12,245
but I want to do something more room-scale with AR.

57
00:03:13,085 --> 00:03:17,267
And Tendar is essentially a long-form AR content app.

58
00:03:17,848 --> 00:03:23,174
It's about three weeks of content, and it reacts meaningfully to the user's environment.

59
00:03:23,554 --> 00:03:29,221
So it has modes that can address a user at home, a user in the outside world, multi-person

60
00:03:29,261 --> 00:03:30,062
and social use.

61
00:03:30,803 --> 00:03:36,910
And at the core of Tendar's design is a strategic use of our device, of on-device rather,

62
00:03:37,991 --> 00:03:43,800
mobile vision, such as object recognition, sentiment analysis, to create a virtual friend

63
00:03:43,860 --> 00:03:49,868
and pet that can evolve over the three weeks by feasting on the player's emotions, the tears and

64
00:03:49,908 --> 00:03:54,114
joy of you and your loved ones, and responding to objects in the player's world.

65
00:03:59,085 --> 00:04:02,908
So Tendar is actually, we're going to talk a little bit about machine learning later,

66
00:04:03,028 --> 00:04:09,434
but it's a fictitious company that ostensibly created this virtual pet guppy to improve

67
00:04:09,494 --> 00:04:11,015
its model for emotion recognition.

68
00:04:11,616 --> 00:04:17,000
And by inviting users to build a relationship with this guppy, it teaches, as you play the

69
00:04:17,020 --> 00:04:21,584
game, a little bit about what machine learning is and how some of the object recognition

70
00:04:21,704 --> 00:04:22,865
in the game works itself.

71
00:04:23,743 --> 00:04:27,286
Guppy in itself is a computer vision model

72
00:04:27,326 --> 00:04:31,048
that's inspired by various neurobiology research

73
00:04:31,568 --> 00:04:33,250
that attempts to emulate and automate

74
00:04:33,310 --> 00:04:37,052
how real vision systems operate and identify patterns.

75
00:04:38,213 --> 00:04:41,175
It's kind of both a literal and figurative representation

76
00:04:41,795 --> 00:04:44,477
of machine learning since as you are playing the game,

77
00:04:44,957 --> 00:04:47,699
Guppy must eat and absorb the image,

78
00:04:47,719 --> 00:04:50,461
the data sets, the images of your emotions to survive.

79
00:04:51,201 --> 00:04:56,444
And by eating those emotions, he's being trained on how to process and understand human emotions

80
00:04:56,524 --> 00:04:57,504
more accurately.

81
00:04:58,025 --> 00:05:01,787
And you know, hilarity ensues as things kind of devolve over time.

82
00:05:03,047 --> 00:05:04,808
Here's a brief teaser.

83
00:05:15,454 --> 00:05:19,436
And pretend there's like, you know, fun instrumental music in the background.

84
00:05:56,806 --> 00:06:00,867
So some of the things you're seeing in the video, there's various modes using both the

85
00:06:00,967 --> 00:06:07,510
front and back facing camera of obtaining emotion from a multiple, not just you as yourself,

86
00:06:07,570 --> 00:06:08,910
but multiple people in the space.

87
00:06:09,290 --> 00:06:11,311
You can pull off the emotions from their face.

88
00:06:11,851 --> 00:06:16,893
They turn into kind of these wonderful digestible blobs that Guppy can eat and sample and then

89
00:06:16,933 --> 00:06:19,675
give you a fortune of what you're really feeling.

90
00:06:21,256 --> 00:06:23,477
So there's the sentiment analysis aspect.

91
00:06:23,497 --> 00:06:25,178
You also see some of the object recognition,

92
00:06:25,218 --> 00:06:29,401
which is a context dependent on the various objects

93
00:06:29,421 --> 00:06:31,023
that can be in any of these spaces

94
00:06:31,063 --> 00:06:32,123
that the user will inhabit.

95
00:06:32,183 --> 00:06:33,965
And there's hints of the game, like how to find them.

96
00:06:35,566 --> 00:06:38,728
Some of the things that it also does is

97
00:06:41,307 --> 00:06:44,148
It thinks about, well, I guess to step back for a second,

98
00:06:44,688 --> 00:06:46,108
and we can talk more about this,

99
00:06:46,268 --> 00:06:49,109
but the challenge of why we did this project

100
00:06:49,169 --> 00:06:51,069
kind of early on into AR space was

101
00:06:51,509 --> 00:06:54,310
that our main interest in XR in general

102
00:06:54,370 --> 00:06:56,010
is that we see it as a rich space

103
00:06:56,070 --> 00:06:58,491
to prototype interfaces and models

104
00:06:58,531 --> 00:07:01,071
that kind of hint at or lead to a future

105
00:07:01,091 --> 00:07:02,152
of spatial computing.

106
00:07:03,332 --> 00:07:04,772
That it's sort of the forerunner

107
00:07:04,912 --> 00:07:07,193
of what it will actually be in that environment.

108
00:07:07,613 --> 00:07:10,073
So we value interaction design as a key.

109
00:07:11,194 --> 00:07:16,261
A key element for making these spatial digital content that feels like truly present.

110
00:07:22,593 --> 00:07:26,298
And there's actually a module that's not just world-facing,

111
00:07:26,358 --> 00:07:32,065
but that's self-facing, where you can decide to make the food

112
00:07:32,085 --> 00:07:35,449
you're feeding your fish by actually engaging with the fish

113
00:07:35,509 --> 00:07:37,151
and teaching it emotions.

114
00:07:37,191 --> 00:07:38,933
And it's kind of this emotional dialogue

115
00:07:38,953 --> 00:07:41,136
you're having with the pet that then, like,

116
00:07:41,697 --> 00:07:49,805
results into this very, I guess, kind of like a little bit upsetting, you know, distillation

117
00:07:49,845 --> 00:07:52,567
of your emotions into food pellets that you feed.

118
00:07:52,647 --> 00:07:54,910
And that's how, you know, the fish gets his image set data.

119
00:07:55,310 --> 00:08:00,135
And then you get this wonderful little like fortune from Tendar that analyzes like, yes,

120
00:08:00,155 --> 00:08:00,835
this is accurate.

121
00:08:00,855 --> 00:08:01,656
This is how you feel.

122
00:08:02,637 --> 00:08:05,140
You know, you can choose what to do about that.

123
00:08:17,604 --> 00:08:20,506
So one of the things that I think this panel in particular

124
00:08:20,546 --> 00:08:23,389
is relevant is that Tendr has to work with all kinds of player

125
00:08:23,429 --> 00:08:25,971
context and aspects of change to their world.

126
00:08:26,612 --> 00:08:30,095
How do we design for a space where the content should

127
00:08:30,135 --> 00:08:34,919
be anchored to the user's world and have meaningful semantics

128
00:08:34,959 --> 00:08:37,522
with the user's world without feeling overly

129
00:08:37,562 --> 00:08:38,743
tethered to a surface?

130
00:08:39,323 --> 00:08:41,966
It can happen on a bus, on a grocery store, on a toilet,

131
00:08:42,006 --> 00:08:42,726
you name it.

132
00:08:42,746 --> 00:08:43,407
Act.

133
00:08:46,302 --> 00:08:48,763
The other sort of unique thing about Tendar

134
00:08:48,863 --> 00:08:51,064
is it's thinking about AR for social play.

135
00:08:51,465 --> 00:08:53,406
So there is a module that we often show

136
00:08:53,486 --> 00:08:56,588
at conferences and galleries that is for two players.

137
00:08:57,248 --> 00:08:59,289
And one of the things that is happening there

138
00:08:59,309 --> 00:09:01,971
with the front-facing camera is the emotions

139
00:09:02,011 --> 00:09:04,172
of the two players are actually playing off each other

140
00:09:04,552 --> 00:09:06,454
and the audio is splitting to different ears.

141
00:09:06,834 --> 00:09:08,555
So the different players are getting kind of

142
00:09:09,395 --> 00:09:11,396
live procedural updates on what the other player

143
00:09:11,436 --> 00:09:12,217
might be feeling.

144
00:09:12,717 --> 00:09:15,519
which then changes how they're engaging with each other on screen,

145
00:09:15,559 --> 00:09:20,422
but it also changes how two users can engage physically in the space with the device

146
00:09:20,822 --> 00:09:24,504
and what that user intent means for moving an AR device around that space

147
00:09:24,564 --> 00:09:28,626
and how it changes the design of the experience or the application.

148
00:09:28,646 --> 00:09:35,670
Hey guys, my name is Brian.

149
00:09:36,491 --> 00:09:39,032
I work as a director on a thing called the Interaction Lab.

150
00:09:40,049 --> 00:09:45,453
And so if you go to the next thing, it's basically a rapid prototyping group.

151
00:09:46,113 --> 00:09:49,836
Over the last four and a half years, we've probably made 600 or 700 prototypes.

152
00:09:51,858 --> 00:09:53,399
You can just pop through them, I don't care.

153
00:09:54,040 --> 00:09:55,040
We did a bunch of things.

154
00:09:55,060 --> 00:09:58,863
We did first turn on of all of the hardware and software and perception features as they

155
00:09:58,903 --> 00:09:59,444
came online.

156
00:09:59,944 --> 00:10:03,947
We tried to give really fast feedback and guidance to those features as they got built.

157
00:10:04,747 --> 00:10:07,729
We tried to think about how you would consume those features

158
00:10:07,809 --> 00:10:10,190
as a dev so we could actually iterate a little bit on the

159
00:10:10,310 --> 00:10:11,471
SDK before it got there.

160
00:10:12,311 --> 00:10:15,353
We did a bunch of documentation and best known practices.

161
00:10:15,393 --> 00:10:17,374
And then we finally did a lot of knowledge sharing.

162
00:10:17,414 --> 00:10:21,156
And then slowly, as we walked up to launch, we kind of

163
00:10:21,476 --> 00:10:23,097
pushed a little bit further in that direction.

164
00:10:23,177 --> 00:10:26,999
And the team, for about the last year or so, made a bunch

165
00:10:27,039 --> 00:10:27,159
of MR.

166
00:10:29,503 --> 00:10:34,546
little moments called Magic Kit that was a bunch of stuff that we put out there specifically

167
00:10:34,586 --> 00:10:40,070
just to give some tiny little taste of spatialized computing and then all the source code in

168
00:10:40,170 --> 00:10:45,174
Unreal and Unity so that people could actually pull the dials apart and see what made it

169
00:10:45,234 --> 00:10:49,757
happen and when it fell apart and which perceptual cliffs were important and which one weren't

170
00:10:49,777 --> 00:10:50,938
and why everything was doing it.

171
00:10:50,998 --> 00:10:54,520
So the source is all up there on our website.

172
00:10:56,871 --> 00:10:59,713
Hey, I'm Jono. I'm on the labs team at Unity.

173
00:11:00,333 --> 00:11:04,356
I'm leading UX dev for Mars, Mix and Augmented Reality Studio.

174
00:11:05,197 --> 00:11:09,520
That's an upcoming set of tools for building XR experiences,

175
00:11:09,980 --> 00:11:10,780
M-R-A-R experiences,

176
00:11:11,821 --> 00:11:13,522
with a more visual, approachable workflow.

177
00:11:14,803 --> 00:11:16,384
So, for example, we do simulation.

178
00:11:16,484 --> 00:11:18,626
We'll talk kind of at length about simulation in a sec.

179
00:11:19,987 --> 00:11:22,789
But this is an example of how this example game content

180
00:11:22,929 --> 00:11:24,850
would fit into this simulated living room.

181
00:11:26,895 --> 00:11:29,878
We also have these visual tools for specifying

182
00:11:30,158 --> 00:11:32,160
the kinds of parameters of objects that you're seeking.

183
00:11:32,180 --> 00:11:34,542
So here I'm saying I want my hero character to be on,

184
00:11:35,163 --> 00:11:36,965
it's gonna be on a surface, the surface is gonna be

185
00:11:37,465 --> 00:11:39,427
in some range of size, in some range of elevation.

186
00:11:39,447 --> 00:11:41,569
You can see that here abstractly,

187
00:11:41,589 --> 00:11:43,771
and then also simulated, see how it actually matches.

188
00:11:45,234 --> 00:11:50,058
Of course, face content is a big part of commercial AR right now.

189
00:11:50,979 --> 00:11:52,180
So we have workflows for that.

190
00:11:52,220 --> 00:11:55,383
And you can see here that I'm simulating live against the webcam.

191
00:11:55,543 --> 00:11:57,385
Notice that we're not in play mode. This is at edit time.

192
00:11:58,085 --> 00:12:01,148
I think this also kind of shows you the shape of things to come

193
00:12:01,328 --> 00:12:03,850
in terms of augmenting individual objects.

194
00:12:04,010 --> 00:12:06,292
So I can say, like, I have this canonical version of an object

195
00:12:06,332 --> 00:12:09,194
that I can mark up and then, you know,

196
00:12:09,314 --> 00:12:12,537
see how that fits against, you know, real fed examples.

197
00:12:14,338 --> 00:12:16,661
Here's that first scene that you saw running on device.

198
00:12:17,602 --> 00:12:18,723
You see a little sample of

199
00:12:19,384 --> 00:12:21,085
some of our procedural tools and utilities here.

200
00:12:22,026 --> 00:12:23,448
This has a rule in it that says,

201
00:12:24,650 --> 00:12:28,093
So any horizontal surface that's not the floor

202
00:12:28,693 --> 00:12:30,594
should fill with this terrain texture,

203
00:12:30,955 --> 00:12:32,055
this terrain material thing.

204
00:12:32,756 --> 00:12:35,298
And then also those and the floor get nav-matched

205
00:12:35,318 --> 00:12:36,398
so our character can walk around.

206
00:12:36,519 --> 00:12:38,620
And then also all the raised surfaces

207
00:12:39,140 --> 00:12:40,701
should build a ladder down to the floor.

208
00:12:40,721 --> 00:12:42,202
So you can see all those elements.

209
00:12:42,943 --> 00:12:44,544
And one of our goals with this project is

210
00:12:46,247 --> 00:12:48,912
to be able to open up XR experiences and XR development

211
00:12:49,033 --> 00:12:51,357
to people who are not programmers necessarily.

212
00:12:52,439 --> 00:12:54,082
So to that end, everything you see here

213
00:12:54,142 --> 00:12:55,304
has no additional scripting.

214
00:12:55,324 --> 00:12:57,448
This is all just kind of out of the box tooling.

215
00:12:58,850 --> 00:13:01,490
And then also we're looking at tools on device.

216
00:13:01,530 --> 00:13:05,251
This is a design mock-up of using a phone to capture a space

217
00:13:05,411 --> 00:13:09,352
and then to start to identify layout and rules

218
00:13:09,492 --> 00:13:11,313
and content descriptions that you want

219
00:13:11,613 --> 00:13:13,853
right there in the real space.

220
00:13:14,574 --> 00:13:16,274
So ultimately you would pull this into the editor

221
00:13:16,334 --> 00:13:18,495
and use this room as a simulation

222
00:13:18,535 --> 00:13:20,195
that you can test against and also use.

223
00:13:21,235 --> 00:13:22,416
The rules I'm starting to set up here

224
00:13:22,656 --> 00:13:24,017
as the beginning of your content scene.

225
00:13:24,577 --> 00:13:26,898
So I'm saying like if I find a surface

226
00:13:26,918 --> 00:13:27,979
that looks kind of like this one,

227
00:13:28,019 --> 00:13:29,460
then put my hero on it.

228
00:13:29,720 --> 00:13:31,561
And that would generate that rule that you saw earlier.

229
00:13:36,363 --> 00:13:38,625
Okay, so that's who we are

230
00:13:38,685 --> 00:13:40,546
and kind of where we're all coming from.

231
00:13:40,866 --> 00:13:43,307
So hopefully now you spent the setup

232
00:13:43,367 --> 00:13:45,908
to give you an idea of the kinds of things

233
00:13:45,928 --> 00:13:47,729
that we're interested in in the XR space

234
00:13:47,769 --> 00:13:49,330
and where we're going with all this.

235
00:13:51,503 --> 00:13:53,810
So let's talk about why AR is complicated.

236
00:13:54,091 --> 00:13:55,335
What are some of the many challenges of it?

237
00:13:55,355 --> 00:13:57,422
Want to start us off?

238
00:13:59,879 --> 00:14:05,820
In terms of us, we are trying to think about how to compose an AR experience that could be transferable to different

239
00:14:05,980 --> 00:14:11,221
environments of the user and still have semantic significance for those environments.

240
00:14:12,682 --> 00:14:16,322
And part of one of the things that we were thinking about is,

241
00:14:16,803 --> 00:14:19,923
for instance, all that the objects that Guppy can recognize that

242
00:14:20,663 --> 00:14:26,785
he can respond to and that the user is kind of, there's Easter eggs and it's seeded throughout the game what these objects are, but users

243
00:14:26,825 --> 00:14:28,825
can go out and find these objects in their world.

244
00:14:29,586 --> 00:14:31,329
And they started with like a thousand,

245
00:14:31,729 --> 00:14:32,651
we can talk about this later,

246
00:14:32,691 --> 00:14:35,615
and then worked our model down to about 200 of these.

247
00:14:37,358 --> 00:14:39,541
And trying to figure out how.

248
00:14:42,633 --> 00:14:45,875
Like a fire hydrant in one context may not look like

249
00:14:45,895 --> 00:14:47,457
the fire hydrant in another context.

250
00:14:47,797 --> 00:14:52,480
So how do you design the AR experience to be able to

251
00:14:52,540 --> 00:14:55,563
recognize those and the interface that comes off of it

252
00:14:55,623 --> 00:14:58,585
and the visual design will be different depending on

253
00:14:58,605 --> 00:15:00,867
the different shape for what's on the screen

254
00:15:00,887 --> 00:15:02,148
and what the user encounters.

255
00:15:03,048 --> 00:15:05,430
So thinking really, trying to think broader than

256
00:15:06,891 --> 00:15:10,334
just mapping a space to almost mapping a world.

257
00:15:11,737 --> 00:15:13,918
So I actually had a question I wanted to ask you about that one.

258
00:15:13,938 --> 00:15:20,041
Like, since you recognize all these objects that Guppy can respond to, how do you keep

259
00:15:20,121 --> 00:15:23,963
people from missing so much of the content that is there or that is possible?

260
00:15:24,234 --> 00:15:45,891
Yeah, so the actual content is actually, it's like writing worked with eight writers over five months, and it's like a whole giant corpus of text messages that is partially generative and partially written that can come up in response to things the user does with Guppy in the AR world. So there's like weeks of content.

261
00:15:46,431 --> 00:15:52,875
But parts of those peppered in within those messages is kind of hints about he's learning about the human world.

262
00:15:52,935 --> 00:15:59,079
You're training him to like see and find objects of like things he may want to see.

263
00:16:00,539 --> 00:16:06,023
So he's like, oh, you know, I heard about this type of device that, you know, does X, Y, and Z.

264
00:16:06,083 --> 00:16:07,784
So that's a more explicit example.

265
00:16:08,504 --> 00:16:14,788
But that can get players to like go out and bring their AR, you know, application into other spaces in the world.

266
00:16:15,188 --> 00:16:18,169
If it's something that you can find at a grocery store, for instance, or

267
00:16:18,189 --> 00:16:21,451
you know, like a car wash.

268
00:16:21,491 --> 00:16:24,773
There's also a module where Guppy sends you on excursions

269
00:16:25,193 --> 00:16:29,376
explicitly to like, for instance, a grocery store to talk about what he wants to see in the fish aisle.

270
00:16:30,376 --> 00:16:31,096
So, you know, there's...

271
00:16:31,837 --> 00:16:33,218
Element second, work with that.

272
00:16:35,901 --> 00:16:40,383
I actually think that one of the biggest challenges of AR is not just that the experiences themselves

273
00:16:40,423 --> 00:16:44,385
are tough to make, it's that the player expectation is essentially nil.

274
00:16:44,866 --> 00:16:49,208
Like they don't know what to think or do or if they can even move or if physics works

275
00:16:49,268 --> 00:16:49,808
or whatever.

276
00:16:49,928 --> 00:16:57,692
And so like one of the challenges of making experiences for XR is to quickly and sort

277
00:16:57,732 --> 00:17:02,255
of transparently educate your users on what the experience is even going to do and what

278
00:17:02,275 --> 00:17:03,055
they can expect.

279
00:17:03,735 --> 00:17:07,699
If I'm sitting in front of a screen, I know where all the pixels are going to come from.

280
00:17:08,160 --> 00:17:12,184
I know where the controls are. And now that's all gone.

281
00:17:12,865 --> 00:17:17,549
And so people don't even know where to look, unless you explicitly kind of make that contract

282
00:17:17,569 --> 00:17:21,854
with them very quickly. And they don't know what to do if you don't make that explicit

283
00:17:21,894 --> 00:17:25,517
contract with them fairly quickly. And that's one of the biggest challenges for now. That's

284
00:17:25,537 --> 00:17:27,019
sort of a moment in time challenge, right?

285
00:17:29,289 --> 00:17:34,591
And in my case, we started to work on this area about six

286
00:17:34,791 --> 00:17:37,853
years ago when devices with an array of sensors

287
00:17:37,913 --> 00:17:38,813
started to appear.

288
00:17:39,613 --> 00:17:44,395
And we faced more, you know, we were in uncharted territory.

289
00:17:44,595 --> 00:17:48,217
And I'm coming from the background of geometry

290
00:17:48,257 --> 00:17:48,717
processing.

291
00:17:48,757 --> 00:17:52,679
And then we found ourselves with a lot of data streaming

292
00:17:52,879 --> 00:17:56,180
fused to some extent into our application.

293
00:17:56,240 --> 00:17:57,461
And OK, what?

294
00:17:58,725 --> 00:18:00,845
Do I need to do in order to create a simple demo?

295
00:18:01,345 --> 00:18:04,126
And then we start to find out that design tools

296
00:18:04,266 --> 00:18:05,046
are really missing.

297
00:18:05,546 --> 00:18:06,846
So you get a lot of data,

298
00:18:07,046 --> 00:18:09,787
you get a lot of intelligent analysis

299
00:18:09,867 --> 00:18:12,408
of what's going on in the scene around you,

300
00:18:13,108 --> 00:18:17,069
but as a programmer, not that I'm an XR developer,

301
00:18:17,109 --> 00:18:21,470
but as a simple developer with tools like Unity or Unreal,

302
00:18:22,650 --> 00:18:24,050
I'm able to create stuff,

303
00:18:24,390 --> 00:18:26,130
but what stuff should I create

304
00:18:26,190 --> 00:18:27,411
and where should I put stuff?

305
00:18:28,498 --> 00:18:30,339
in order to adapt to the environment around me.

306
00:18:30,379 --> 00:18:34,861
And this is what led us to start thinking about design tools

307
00:18:35,241 --> 00:18:39,703
that will help developers, or will allow developers,

308
00:18:39,903 --> 00:18:43,124
to work in this environment without going

309
00:18:43,244 --> 00:18:47,066
through all the low-level geometry analysis and whatever.

310
00:18:47,606 --> 00:18:49,907
So to that end, of course, one of the

311
00:18:51,145 --> 00:18:52,045
It may be the biggest challenge.

312
00:18:52,065 --> 00:18:55,246
One of the biggest challenges is just the unknown nature

313
00:18:55,346 --> 00:18:56,206
of the real world, right?

314
00:18:56,226 --> 00:18:57,547
We don't know where the user's gonna be.

315
00:18:57,607 --> 00:18:58,787
Samantha's been talking about that,

316
00:18:58,847 --> 00:19:00,548
of are they on a bus, are they in a field,

317
00:19:00,588 --> 00:19:01,368
are they in a grocery store?

318
00:19:01,988 --> 00:19:03,769
And how do you deal with that?

319
00:19:04,989 --> 00:19:08,250
So, Ron, you wanna just kinda keep running with rules?

320
00:19:08,310 --> 00:19:08,570
Yeah.

321
00:19:11,042 --> 00:19:32,103
There was something I was going to say about the actually from the dev perspective, the most challenging thing and why it's important for the tools that these guys are building is getting everything to play nice together, especially we are trying to enhance the future of like XR by adding all these other like world sensing modules, just the combination of getting those things to play nice is difficult. And that's why what it's some of what these guys do is so important.

322
00:19:33,416 --> 00:19:39,532
Absolutely. Yeah, we're going to talk about the challenging matrix of devices and capabilities and all that craziness.

323
00:19:42,544 --> 00:19:46,044
So this, the nature of the thing that we're trying to get at,

324
00:19:46,285 --> 00:19:48,805
in a lot of ways you can think of it like CSS on the web,

325
00:19:49,205 --> 00:19:53,206
where we talk about, I want to specify some parameters and some rules

326
00:19:53,627 --> 00:19:57,708
that will let, in the CSS case, if my browser window is really small,

327
00:19:57,728 --> 00:20:00,849
or if it's really large, my content will intelligently adapt.

328
00:20:01,549 --> 00:20:04,630
In this case we say, if the user's in that room, or that room,

329
00:20:04,790 --> 00:20:07,891
or the bus, or whatever, that their content adapts.

330
00:20:08,691 --> 00:20:11,532
So to that end, I'm going to take this one.

331
00:20:12,526 --> 00:20:23,132
So, continuing on what I said before, we started in a bunch of engineers trying to find a solution to a common problem.

332
00:20:23,152 --> 00:20:30,416
We wanted to create amazing, magical VR and AR demos, and we had the mesh.

333
00:20:31,901 --> 00:20:34,884
detected by some kind of depth camera.

334
00:20:35,665 --> 00:20:37,226
And then we asked ourself, what do we need more?

335
00:20:37,867 --> 00:20:40,409
So we started to throw balls in order to create

336
00:20:40,510 --> 00:20:42,932
magical physical simulations.

337
00:20:43,352 --> 00:20:45,394
And then we found out about holes in the measures.

338
00:20:46,155 --> 00:20:49,298
And then we thought about how to fix this problem

339
00:20:49,358 --> 00:20:53,182
and how to allow a more plausible kind of experiences.

340
00:20:54,243 --> 00:20:54,843
And then...

341
00:20:56,351 --> 00:20:57,752
We started to think about, okay,

342
00:20:58,212 --> 00:21:00,054
suppose I want to create an application,

343
00:21:00,374 --> 00:21:01,755
where should I put stuff?

344
00:21:02,456 --> 00:21:05,158
Where should I hang my synthetic objects in the scene?

345
00:21:06,599 --> 00:21:09,461
We started to think about consistency,

346
00:21:10,101 --> 00:21:11,842
adapting to different environments,

347
00:21:12,063 --> 00:21:14,384
adapting to different environments from the first frame,

348
00:21:14,765 --> 00:21:18,247
or asking the user to scan more data

349
00:21:18,287 --> 00:21:20,049
in order to adapt even better.

350
00:21:21,309 --> 00:21:25,773
And the virtual objects needs to sit in the scene.

351
00:21:28,333 --> 00:21:33,057
Nice locations, nice regarding color, regarding contrast.

352
00:21:33,097 --> 00:21:35,639
I don't want to see, especially with additive devices,

353
00:21:35,679 --> 00:21:38,421
I don't want to put stuff on a window

354
00:21:38,561 --> 00:21:40,002
facing the sun outside.

355
00:21:40,523 --> 00:21:43,665
All kind of rules that, when you think about them,

356
00:21:43,865 --> 00:21:44,846
seem simple, but.

357
00:21:45,993 --> 00:21:51,377
We didn't want to go through this pipeline every new demo that we wanted to do, because

358
00:21:51,397 --> 00:21:54,819
we said to ourselves, this is a common set, this is the basic point.

359
00:21:54,919 --> 00:21:58,681
I need to start, I need to have tools to start developing immediately.

360
00:21:59,862 --> 00:22:04,165
And of course we took into account multiple users in the same environment that I want

361
00:22:04,185 --> 00:22:04,465
to put.

362
00:22:05,538 --> 00:22:11,220
Even the simplest thing, I want to put something in the room that is visible or is viewed by

363
00:22:11,520 --> 00:22:17,642
two or more users, then it becomes a simple point that you can fix or you can solve theoretically

364
00:22:18,143 --> 00:22:20,963
very easily if you have all the data.

365
00:22:20,983 --> 00:22:23,024
And this is what we wanted to say to the developer.

366
00:22:23,524 --> 00:22:24,505
Don't take all the data.

367
00:22:24,565 --> 00:22:26,645
We will try to help in this regard.

368
00:22:27,685 --> 00:22:31,926
So and of course multiple application in the same area.

369
00:22:32,147 --> 00:22:33,607
I want to open a new application,

370
00:22:33,647 --> 00:22:36,849
I want the previous open application to move gracefully

371
00:22:36,909 --> 00:22:40,610
and allow me or allow application not to fight

372
00:22:40,790 --> 00:22:44,412
over real estate of my screen and over my pixels.

373
00:22:47,993 --> 00:22:50,774
So we developed a simple service called Flare.

374
00:22:50,834 --> 00:22:53,175
This is before XR, so it's fast layout

375
00:22:53,235 --> 00:22:54,536
for augmented reality.

376
00:22:55,747 --> 00:23:01,189
And we used declarative rules, which are very simple.

377
00:23:01,249 --> 00:23:05,331
You just state the rules that you want your experience

378
00:23:05,371 --> 00:23:11,053
to follow, such as I want these TV screens

379
00:23:11,133 --> 00:23:12,453
to be hanged on a wall.

380
00:23:14,414 --> 00:23:18,516
And this brings us to another point, what is a wall

381
00:23:18,576 --> 00:23:21,357
and how much semantic information do I have about the scene?

382
00:23:21,857 --> 00:23:22,918
So we started with...

383
00:23:23,619 --> 00:23:27,024
put it on any vertical, flat surface.

384
00:23:27,164 --> 00:23:29,047
We don't care if it's a wall or something else,

385
00:23:29,448 --> 00:23:32,132
but the nice thing about rule-based systems

386
00:23:32,212 --> 00:23:34,195
is that the more information you have,

387
00:23:34,315 --> 00:23:36,278
the more intelligent your rule can become.

388
00:23:37,580 --> 00:23:38,902
And then we started to...

389
00:23:41,757 --> 00:23:44,518
figure out ways to solve such a system.

390
00:23:44,798 --> 00:23:47,960
Because this is a kind of optimization problem,

391
00:23:48,000 --> 00:23:50,381
but a very non-convex optimization problem.

392
00:23:51,181 --> 00:23:53,802
And we started to look for

393
00:23:55,803 --> 00:23:58,625
iterative methods, and iterative on purpose

394
00:23:58,805 --> 00:24:02,426
in order to be able to invest as much time as we have.

395
00:24:02,486 --> 00:24:05,508
We wanted a solution, we wanted a good solution,

396
00:24:05,548 --> 00:24:06,368
but if we don't have

397
00:24:08,483 --> 00:24:10,784
30 seconds to invest in it, we wanted a plausible

398
00:24:10,864 --> 00:24:13,825
or as good as possible solution in two seconds.

399
00:24:14,946 --> 00:24:20,248
And we started to, we used a couple of methods

400
00:24:20,288 --> 00:24:24,550
of solving it and we achieved a very surprisingly

401
00:24:24,610 --> 00:24:27,711
good results on very low end devices.

402
00:24:28,152 --> 00:24:29,992
And this is what motivate us to continue

403
00:24:30,012 --> 00:24:30,953
to look into the matter.

404
00:24:32,874 --> 00:24:35,055
So this is a simple example.

405
00:24:36,613 --> 00:24:38,734
And I wish I could play it.

406
00:24:38,754 --> 00:24:41,696
Yeah, it's playing, yep.

407
00:24:42,236 --> 00:24:44,917
Okay, so this is an example of building a racetrack

408
00:24:45,418 --> 00:24:48,879
in the room and the rules that were used

409
00:24:48,899 --> 00:24:52,861
to construct the racetrack is put a number of points

410
00:24:53,062 --> 00:24:55,743
in the room at a certain distance,

411
00:24:55,823 --> 00:24:58,664
range of distance between them and that are visible.

412
00:24:58,905 --> 00:25:01,666
Each point can view the next point,

413
00:25:02,747 --> 00:25:03,507
view in the sense of.

414
00:25:05,463 --> 00:25:09,026
There is nothing blocking the path.

415
00:25:09,046 --> 00:25:10,847
And we construct the racetrack

416
00:25:10,927 --> 00:25:12,428
using a spline between these points.

417
00:25:13,069 --> 00:25:15,951
And when we increase the number of points,

418
00:25:16,011 --> 00:25:18,113
we see that we adapt to the environment.

419
00:25:18,393 --> 00:25:22,696
The racetrack is created larger and larger,

420
00:25:22,736 --> 00:25:25,319
the different, and of course, at some point,

421
00:25:25,379 --> 00:25:28,841
you might not be able to adhere to all the rules,

422
00:25:29,382 --> 00:25:31,143
but if your rules are

423
00:25:32,267 --> 00:25:34,728
valid in your environment, you'll get a solution

424
00:25:34,808 --> 00:25:37,890
which is surprisingly nice.

425
00:25:38,491 --> 00:25:40,953
We were surprised, at least I personally was surprised.

426
00:25:45,776 --> 00:25:49,198
And this is the same example, and the nice thing is

427
00:25:49,258 --> 00:25:52,560
that it's the same set of rules to design all the,

428
00:25:52,660 --> 00:25:54,842
to construct all these examples,

429
00:25:55,662 --> 00:26:00,946
and we found out that rules are very easy to use.

430
00:26:01,232 --> 00:26:04,698
So, you can teach people, but it's not as clear as...

431
00:26:05,519 --> 00:26:07,723
The problem is that you always, as a human,

432
00:26:07,823 --> 00:26:10,528
needs to decide, okay, I know the rules,

433
00:26:11,008 --> 00:26:13,452
but which one has more priorities?

434
00:26:14,552 --> 00:26:22,820
And which one I want to follow no matter what, and which one I can allow to degrade from

435
00:26:22,940 --> 00:26:24,561
or not fulfill fully.

436
00:26:25,142 --> 00:26:32,948
So it is a kind of art, but at least the basic idea is very simple to explain, and we actually

437
00:26:33,069 --> 00:26:34,490
had developers work with it.

438
00:26:36,774 --> 00:26:41,678
It needs getting used to because you find out the difference between in the user's

439
00:26:41,738 --> 00:26:47,183
first room and visible to the user because you have stuff that can block your view.

440
00:26:48,644 --> 00:26:54,249
But it's very easy to integrate multiple users, multiple applications, additional semantic

441
00:26:54,269 --> 00:26:58,152
data that will become available over time, object recognition, and what else.

442
00:26:58,918 --> 00:27:04,339
So, and this is where the connection with Unity happened

443
00:27:04,379 --> 00:27:07,360
because Unity apparently is going more or less

444
00:27:07,500 --> 00:27:08,800
in the same direction and then.

445
00:27:10,021 --> 00:27:11,881
We, yeah, we came across the Flare paper

446
00:27:12,801 --> 00:27:14,182
as we're working on our project

447
00:27:14,542 --> 00:27:16,382
and it was very exciting to us

448
00:27:16,402 --> 00:27:18,683
because we're like, yeah, like this is exactly,

449
00:27:19,183 --> 00:27:21,064
exactly the kind of approach that we're taking.

450
00:27:22,752 --> 00:27:24,572
Just seeing the results that you had there

451
00:27:24,892 --> 00:27:27,613
was very encouraging on that route.

452
00:27:29,053 --> 00:27:32,134
So yeah, let's see, did you want to talk about this one?

453
00:27:34,214 --> 00:27:37,995
This is another example of graceful adaptation.

454
00:27:38,335 --> 00:27:41,275
So we constructed a set of rules

455
00:27:41,335 --> 00:27:44,216
to put the creature in circle facing a middle creature.

456
00:27:44,476 --> 00:27:46,837
We didn't define the radius of the circle.

457
00:27:47,597 --> 00:27:49,077
And when the scene change,

458
00:27:50,914 --> 00:27:55,457
If obstacles appear, such as inanimate or a human,

459
00:27:55,497 --> 00:28:01,901
goes and disturb the previous arrangement,

460
00:28:02,781 --> 00:28:05,783
we simply added another rule to resolve,

461
00:28:06,063 --> 00:28:10,066
but be as close as possible to the previous solution.

462
00:28:10,766 --> 00:28:11,947
So everything is very.

463
00:28:13,583 --> 00:28:15,665
It's like Lego, everything fit together.

464
00:28:16,426 --> 00:28:20,070
The only question is how much time do you have to invest

465
00:28:20,130 --> 00:28:21,611
in order to find a new solution.

466
00:28:22,172 --> 00:28:26,035
And it's highly, the minute you have a strong GPU around,

467
00:28:26,136 --> 00:28:27,697
it's a matter of milliseconds.

468
00:28:28,798 --> 00:28:31,160
On low end device, it becomes an issue, so.

469
00:28:31,401 --> 00:28:31,541
Yeah.

470
00:28:33,264 --> 00:28:36,448
So, yeah, like I was saying, we were really excited to find Flare.

471
00:28:36,468 --> 00:28:37,629
I'd recommend everybody read it, by the way.

472
00:28:37,649 --> 00:28:40,212
Just Google Flare, Microsoft Research paper, you'll get it.

473
00:28:40,973 --> 00:28:41,413
Really good read.

474
00:28:41,433 --> 00:28:46,719
And when we read it, we said, yes, this is exactly the kind of thing that we're talking

475
00:28:46,759 --> 00:28:46,959
about.

476
00:28:48,587 --> 00:28:52,910
So we have very analogous systems to everything that Ranjit just described.

477
00:28:53,530 --> 00:28:58,253
We talk about real-world objects, which is a scene object in your hierarchy that represents

478
00:28:58,313 --> 00:28:58,954
a real thing.

479
00:28:58,974 --> 00:29:02,996
So I could be like, this game object in my scene represents a table in the real world.

480
00:29:04,357 --> 00:29:07,539
The complexity of course is that table may or may not actually end up existing, so that's

481
00:29:07,939 --> 00:29:08,279
a whole thing.

482
00:29:10,089 --> 00:29:11,710
Real-world objects are made out of conditions.

483
00:29:11,910 --> 00:29:14,112
So you can see in this object, I'm defining.

484
00:29:14,772 --> 00:29:16,634
So I have an object in the inspector there called floor.

485
00:29:16,714 --> 00:29:18,195
I'm defining it with a tag condition,

486
00:29:18,475 --> 00:29:19,816
looking for the tag floor.

487
00:29:20,516 --> 00:29:23,659
That's provided to us by Magic Leap and HoloLens.

488
00:29:23,799 --> 00:29:25,640
And then on the other platforms that we support

489
00:29:25,680 --> 00:29:27,001
that don't give us that for free,

490
00:29:27,741 --> 00:29:29,943
we do some extra logic to fill that in

491
00:29:30,123 --> 00:29:32,265
so that we can just say every platform knows what a floor is.

492
00:29:34,120 --> 00:29:35,461
That's actually enough for the floor.

493
00:29:35,621 --> 00:29:37,242
In this case, I've added some extra conditions

494
00:29:37,302 --> 00:29:39,023
just to illustrate the case a little bit more.

495
00:29:39,123 --> 00:29:40,344
I could also throw something on there saying,

496
00:29:40,384 --> 00:29:42,005
I'm looking for something that's horizontal.

497
00:29:42,025 --> 00:29:43,206
I'm not looking for something that's vertical

498
00:29:43,246 --> 00:29:44,227
or something that's off-axis.

499
00:29:45,047 --> 00:29:47,569
I also have a condition there for I want a surface

500
00:29:47,589 --> 00:29:51,311
that's of a particular range of size that's acceptable.

501
00:29:52,052 --> 00:29:53,893
So we use those conditions, kind of stack them up

502
00:29:54,033 --> 00:29:56,935
to describe that real object in this sort of atomic way.

503
00:29:58,159 --> 00:30:00,179
And then as Ron described, right,

504
00:30:00,219 --> 00:30:03,021
we then have this rules concept that sort of takes all that.

505
00:30:03,061 --> 00:30:04,741
At a higher level, we say like,

506
00:30:04,821 --> 00:30:07,843
okay, when there is a floor, stick water on it.

507
00:30:07,903 --> 00:30:10,684
When there is a raised surface, stick grass on it, et cetera.

508
00:30:11,004 --> 00:30:12,765
You can see those rules kind of in the lower left

509
00:30:12,785 --> 00:30:15,486
of the screenshot there and see that they're all matching

510
00:30:15,526 --> 00:30:18,087
in this simulated kitchen environment here.

511
00:30:18,943 --> 00:30:22,505
And then, yeah, Ron was also just describing degradation.

512
00:30:22,865 --> 00:30:25,446
So we call those fallbacks, and we say,

513
00:30:25,466 --> 00:30:29,107
basically, you can describe the sort of ideal case

514
00:30:29,127 --> 00:30:30,668
where you're like, oh, I have this app

515
00:30:30,708 --> 00:30:33,549
that you need a two-story building and six different walls

516
00:30:33,589 --> 00:30:34,469
and three humans in it

517
00:30:34,489 --> 00:30:36,610
and have some crazy set of expectations.

518
00:30:37,030 --> 00:30:39,190
But then say, if that fails, then here are some more,

519
00:30:39,771 --> 00:30:41,471
some simpler, simpler, simpler, simpler cases

520
00:30:41,531 --> 00:30:42,292
down to the point of like,

521
00:30:42,352 --> 00:30:43,292
oh, I don't even have tracking.

522
00:30:43,792 --> 00:30:44,172
Now what?

523
00:30:44,352 --> 00:30:45,573
Right now you're in a 3DOF experience

524
00:30:45,613 --> 00:30:47,673
or you're in a just 2D experience at that point.

525
00:30:49,615 --> 00:30:51,959
Brian, you mentioned at one point the basketball thing.

526
00:30:51,979 --> 00:30:52,900
Do you want to talk about that?

527
00:30:52,920 --> 00:30:57,947
Yeah, so I was going to just mention, this isn't super new.

528
00:30:58,268 --> 00:31:01,633
We have had very complicated game AIs in the past that

529
00:31:01,733 --> 00:31:02,854
use very similar systems.

530
00:31:03,555 --> 00:31:03,976
I worked on

531
00:31:05,741 --> 00:31:09,482
A basketball game for Sony computer in the early 2000s.

532
00:31:09,742 --> 00:31:15,384
And we had almost five megabytes of AI data.

533
00:31:16,664 --> 00:31:20,705
It was just pure HTML of situations.

534
00:31:20,805 --> 00:31:24,246
Everything from, oh, if you've got the ball in front of you.

535
00:31:25,467 --> 00:31:29,547
And nobody in front of you can take a shot all the way till in the last three seconds of the game,

536
00:31:29,687 --> 00:31:33,848
if you're this particular player and the guy in front of you has a broken leg, do this thing.

537
00:31:34,288 --> 00:31:38,269
Like all the way down to that level of specificity. And so these systems have been

538
00:31:38,309 --> 00:31:43,950
around for a while. I think that they're not typically needed for most games. It's more

539
00:31:44,570 --> 00:31:49,851
games that had a high degree of sort of knowledge base, which the real world represents the largest

540
00:31:49,931 --> 00:31:53,772
knowledge base that we need understanding that there is. And so that's why they're starting to

541
00:31:53,832 --> 00:31:53,972
make.

542
00:31:54,412 --> 00:31:55,413
Kind of this resurgence.

543
00:31:56,493 --> 00:31:59,795
The other thing that's nice about those rule-based systems is that they're human readable.

544
00:32:00,155 --> 00:32:02,176
You can look at them as a human and you can say,

545
00:32:02,196 --> 00:32:07,638
I understand that in the last three seconds, if you're this guy and there's this blah blah blah,

546
00:32:07,838 --> 00:32:09,499
you can actually understand that very easily.

547
00:32:09,559 --> 00:32:13,561
And so the reason why we had these huge AI systems on that particular game was that

548
00:32:15,297 --> 00:32:17,599
By being human readable, we could have a small army

549
00:32:17,679 --> 00:32:20,062
of hardcore basketball game designers

550
00:32:20,122 --> 00:32:23,445
who could crank stuff out year after year

551
00:32:23,485 --> 00:32:27,769
and add to that massive database of AI scenarios

552
00:32:28,070 --> 00:32:29,111
that it could respond to.

553
00:32:29,731 --> 00:32:34,096
And so it was sort of a beautiful system

554
00:32:34,196 --> 00:32:36,218
so that non-technical staff,

555
00:32:36,258 --> 00:32:37,740
it was much more accessible to them.

556
00:32:38,814 --> 00:32:42,137
So on the point of AI making a big comeback here,

557
00:32:42,157 --> 00:32:45,279
we promised we'd get into machine learning a little bit.

558
00:32:45,299 --> 00:32:46,941
Do you wanna take us away there?

559
00:32:49,763 --> 00:32:52,886
Yeah, I think when we were talking about this panel before,

560
00:32:52,926 --> 00:32:55,388
we had some interesting discussions and interplay

561
00:32:55,428 --> 00:32:56,989
about the use of machine learning.

562
00:32:58,450 --> 00:33:01,253
In our case, I think it's actually,

563
00:33:01,273 --> 00:33:03,835
there's, it's such a wide topic, but

564
00:33:05,436 --> 00:33:13,540
Ways to sense the player's world and ways to use existing machine learning models and

565
00:33:13,760 --> 00:33:19,802
modules to incorporate that into an AR experience as a dev, as ways to sense the player's world

566
00:33:19,863 --> 00:33:26,385
and give you more data, I think is a really strong push towards the future of how we could

567
00:33:26,425 --> 00:33:27,086
engage with AR.

568
00:33:28,207 --> 00:33:34,893
And for us in particular, no tools existed to really do that well at the time.

569
00:33:36,174 --> 00:33:43,662
So it was learning a lot about how to operate within different systems and bring them into Unity.

570
00:33:44,443 --> 00:33:52,915
And for, we actually used parts of TensorFlow as part of the object recognition and TensorFlow

571
00:33:52,935 --> 00:33:58,663
was not at that moment compatible with Unity so that we had to kind of build a wrapper

572
00:33:58,683 --> 00:33:59,424
around that.

573
00:34:00,125 --> 00:34:03,406
And then essentially take the model,

574
00:34:03,646 --> 00:34:05,646
one of the models we were using in TensorFlow

575
00:34:05,686 --> 00:34:07,607
was one that was created at Stanford.

576
00:34:08,087 --> 00:34:12,048
And it can recognize up to 1,000 plus objects.

577
00:34:12,448 --> 00:34:15,249
But the image set it was trained on was mostly dog breeds.

578
00:34:16,229 --> 00:34:18,650
So this is where kind of like the design thinking

579
00:34:18,750 --> 00:34:21,711
of being a dev working with like machine, you know, well.

580
00:34:22,191 --> 00:34:25,933
So, this is a very interesting example of how machine learning models comes into place.

581
00:34:25,953 --> 00:34:32,817
We knew that we wanted the fish to have an Easter egg where he could maybe identify all,

582
00:34:32,997 --> 00:34:34,938
take advantage of all those specific dog breeds.

583
00:34:35,319 --> 00:34:39,281
So, we can very sensitively know the difference between a German Shepherd and a Schnauzer.

584
00:34:39,781 --> 00:34:46,985
But very intensely, because we took that model and we binned it into objects that were recognizable

585
00:34:47,045 --> 00:34:47,986
and objects that could be identified.

586
00:34:48,166 --> 00:34:50,947
make creative design decisions for what would be in the player's world.

587
00:34:51,628 --> 00:34:53,569
And there were some very obvious things it couldn't recognize.

588
00:34:53,929 --> 00:34:56,490
So very early on we had to make the decision that, okay,

589
00:34:56,810 --> 00:35:00,112
we're going to just bin X number of these objects into chickens.

590
00:35:00,512 --> 00:35:04,214
So all types of, you know, like animals that walk in this particular way are been chickens,

591
00:35:04,314 --> 00:35:07,276
and narratively the fish is going to have a fixation with chickens.

592
00:35:07,636 --> 00:35:09,997
And therefore that's why in this machine learning model,

593
00:35:10,037 --> 00:35:12,318
we're going to compensate that with narrative by like just,

594
00:35:12,398 --> 00:35:13,859
you know, talking about his chicken obsession.

595
00:35:15,521 --> 00:35:22,328
Yeah, designing around modules is like one way to, you know, I think incorporate that into like the development of the game.

596
00:35:24,944 --> 00:35:28,886
Anytime you have a massive soup of data and you're looking for temporal patterns,

597
00:35:29,306 --> 00:35:35,529
machine learning is an obvious choice. And because this is a new field, a lot of what we're talking

598
00:35:35,569 --> 00:35:40,131
about is machine learning, but don't be put off. A lot of what we're also talking about is stuff

599
00:35:40,151 --> 00:35:44,933
that essentially needs to be built at a very low level so that most people can just use it as a

600
00:35:45,013 --> 00:35:50,115
function, like where's the nearest table? It should be a function you can call, as opposed to let's

601
00:35:50,595 --> 00:35:52,596
ram through all of the geometry and run a

602
00:35:53,236 --> 00:35:59,600
learning blah blah blah on it so that we can find the flat surfaces and we'll declare certain types of flat surfaces to be tables.

603
00:36:00,160 --> 00:36:01,761
That's the thing that the low level should do.

604
00:36:02,241 --> 00:36:07,524
And a lot of what has evolved over the past four years is we've gone from

605
00:36:08,204 --> 00:36:14,848
you know, almost no semantic understanding of a geometric soup, to we're getting more and more and more

606
00:36:15,489 --> 00:36:21,732
through machine learning and other analytic methods, we're getting more and more semantic data that we can start to provide to developers.

607
00:36:23,184 --> 00:36:26,045
Yeah, we were debating, when we were talking about this,

608
00:36:26,085 --> 00:36:28,726
we were debating should we tell everybody in the room,

609
00:36:28,826 --> 00:36:29,847
go learn machine learning?

610
00:36:31,507 --> 00:36:33,428
And I think Brian just touched on that a lot,

611
00:36:33,448 --> 00:36:36,489
where in a lot of cases that's really on us,

612
00:36:36,669 --> 00:36:39,530
it's on the platform holders here, to provide that.

613
00:36:39,990 --> 00:36:42,591
And I mentioned earlier that one of our goals on my team

614
00:36:43,052 --> 00:36:45,012
is to make this approachable to people

615
00:36:45,052 --> 00:36:46,113
who aren't programmers.

616
00:36:47,073 --> 00:36:48,894
So we, like Brian was saying,

617
00:36:49,994 --> 00:36:51,758
We want to be able to just provide a model that's like,

618
00:36:51,798 --> 00:36:54,986
this one recognizes tables and chairs and household objects,

619
00:36:55,006 --> 00:36:56,629
and this one recognizes types of dogs.

620
00:36:57,591 --> 00:36:59,055
You can just plug that in and use it.

621
00:36:59,536 --> 00:36:59,977
That said.

622
00:37:01,160 --> 00:37:04,942
I think we would all encourage everybody to at least get the shape of machine learning,

623
00:37:04,962 --> 00:37:10,745
just understand what is it, what are people talking about, and what are the general techniques

624
00:37:10,825 --> 00:37:11,086
used there.

625
00:37:11,126 --> 00:37:15,068
So even if you're not a hardcore programmer, even if you're not going to make your own

626
00:37:15,088 --> 00:37:19,891
model, it is still just a very interesting thing to know about, and it will help a lot

627
00:37:19,931 --> 00:37:21,371
in this XR journey.

628
00:37:21,692 --> 00:37:23,872
Yeah, from a dev perspective, I think that's true.

629
00:37:23,892 --> 00:37:26,093
I think understanding a little bit about it

630
00:37:26,233 --> 00:37:28,714
helps underpin certain design decisions that you can make

631
00:37:29,134 --> 00:37:31,674
in terms of knowing what's available to you

632
00:37:31,774 --> 00:37:33,175
and what types of things you can do. 100%.

633
00:37:37,716 --> 00:37:38,716
Who wants to talk about the user?

634
00:37:39,438 --> 00:37:41,519
So the user is just as varied as the environments.

635
00:37:42,820 --> 00:37:45,441
You know, even if you talk about just the quote-unquote human hand

636
00:37:45,922 --> 00:37:49,163
being this natural input, it's like, maybe.

637
00:37:49,784 --> 00:37:51,385
Like, what if you have rheumatoid arthritis?

638
00:37:51,685 --> 00:37:53,846
What if you really, really favor one hand over the other,

639
00:37:53,926 --> 00:37:55,047
even if you are left-handed?

640
00:37:55,547 --> 00:37:57,148
You like using your right hand.

641
00:37:57,528 --> 00:38:00,950
What if you just don't culturally think that this means yes?

642
00:38:01,230 --> 00:38:01,711
You know what I mean?

643
00:38:01,751 --> 00:38:05,793
Like, there's a number of things that have absolutely no bearing.

644
00:38:05,833 --> 00:38:06,053
And so...

645
00:38:06,934 --> 00:38:12,617
User input is in many ways almost the same problem as world understanding is user understanding.

646
00:38:12,997 --> 00:38:16,959
And so a lot of what we end up doing on this side is the exact same problem with a different

647
00:38:17,019 --> 00:38:24,603
user, with a different data set. Like in exactly the same way that head pose is a fused input with

648
00:38:24,643 --> 00:38:28,965
a number of different things, hardware, software, and human understanding, right, that gives you

649
00:38:29,025 --> 00:38:34,488
head pose. Knowing where the hand is going is very much a hardware, software, and a human

650
00:38:34,528 --> 00:38:35,309
understanding problem.

651
00:38:36,050 --> 00:38:41,513
You no longer just have a joystick that you're reading the value of an analog float coming out of it.

652
00:38:41,953 --> 00:38:55,118
And so this is also a very tough problem, and again, is requiring the platforms to up their game as far as delivering solutions to this kind of understanding for the average dev.

653
00:38:57,415 --> 00:39:02,896
For the sake of time, we're just going to plow through procedural content, learn up on it.

654
00:39:03,216 --> 00:39:09,558
Same idea as machine learning, it'll do you well to check in on generating meshes.

655
00:39:10,958 --> 00:39:13,738
But like you saw earlier, we're going to be providing some utilities for that,

656
00:39:13,918 --> 00:39:16,279
and there'll be more utilities, but it's important.

657
00:39:16,299 --> 00:39:19,159
Does anybody want to touch on that before we roll on?

658
00:39:19,179 --> 00:39:19,760
All right, cool.

659
00:39:20,960 --> 00:39:26,321
So let's talk a bit about some specific tools and workflows that are there or that are coming online.

660
00:39:27,323 --> 00:39:29,145
For one thing, simulation, I mentioned earlier.

661
00:39:29,425 --> 00:39:31,528
This is the simulation view inside of Mars.

662
00:39:32,129 --> 00:39:34,271
And here, I'm just going to kind of click through a couple different spaces,

663
00:39:34,351 --> 00:39:38,316
like how would this content work in this room versus that room versus that room versus anywhere else.

664
00:39:38,896 --> 00:39:44,002
And here I'm swapping into a secondary simulation view where I can actually simulate as a device.

665
00:39:44,743 --> 00:39:45,604
So here you can see that.

666
00:39:46,005 --> 00:39:49,907
If you look in the left panel, you kind of see all the rays and the planes,

667
00:39:49,947 --> 00:39:51,787
the surfaces being generated by that device.

668
00:39:51,807 --> 00:39:55,909
So this is a very coarse version of like, generally,

669
00:39:55,949 --> 00:39:59,611
this is how a phone or a headset would see the world.

670
00:40:00,832 --> 00:40:04,553
So in this way, you can see like, as I move through the world,

671
00:40:04,613 --> 00:40:06,514
how would I expect my content to adapt?

672
00:40:06,714 --> 00:40:08,215
And you can pick up a lot of issues there.

673
00:40:09,716 --> 00:40:12,177
And another tool that I want to point out is

674
00:40:14,020 --> 00:40:15,061
debugging in the editor.

675
00:40:15,081 --> 00:40:17,623
So here I see in my simulation

676
00:40:17,663 --> 00:40:19,165
that one of these trees didn't show up.

677
00:40:19,565 --> 00:40:21,206
So I'm going to this compare mode where I'm like,

678
00:40:21,647 --> 00:40:23,869
why did it not show up on this surface that I'm expecting?

679
00:40:24,621 --> 00:40:26,642
And sorry, the text is a little tiny there,

680
00:40:26,682 --> 00:40:28,583
but what's going on is in the inspector

681
00:40:28,623 --> 00:40:30,505
in those conditions that I was describing earlier,

682
00:40:30,945 --> 00:40:32,686
we're showing like this one is failing,

683
00:40:32,726 --> 00:40:34,267
this one, like the surface is too small.

684
00:40:34,687 --> 00:40:36,848
So I say, okay, you know, adjust for that condition,

685
00:40:36,888 --> 00:40:39,650
include that also, and now you see it pop in there.

686
00:40:40,691 --> 00:40:43,352
So I think that the idea of being able to ask your content,

687
00:40:43,412 --> 00:40:46,014
like, hey, why aren't you matching against this data

688
00:40:46,034 --> 00:40:47,155
that I expect you to work on,

689
00:40:47,895 --> 00:40:50,677
is very complicated in this context

690
00:40:50,917 --> 00:40:53,358
and very useful to have some tooling around.

691
00:40:57,098 --> 00:41:01,963
Oh, so here there's one of the things was you can just fly around a little RC car

692
00:41:02,503 --> 00:41:05,706
and so I'm just flying around real quick then I push pause

693
00:41:06,327 --> 00:41:10,611
BAM and now what I can do is I can scrub backwards through all of that data

694
00:41:11,131 --> 00:41:15,235
right there in place standing in the same room. I can scrub backwards, I can scrub forwards

695
00:41:15,616 --> 00:41:18,258
I can realize there's way too much crap so I can like

696
00:41:18,599 --> 00:41:22,022
go over to the panel I can turn off a particular signal I can then

697
00:41:23,197 --> 00:41:28,641
Go back and forth and watch what the events are happening, when they're happening, and then right there in the place I can go,

698
00:41:28,721 --> 00:41:35,967
oh, this thing isn't firing because this particular collider is turned off because there's a ray of light that's coming into the room right here that's causing something.

699
00:41:37,247 --> 00:41:41,330
This kind of like in-situ debugging is almost...

700
00:41:42,300 --> 00:41:47,006
Crazy important for bugs that only happen in a particular room with particular lighting

701
00:41:48,167 --> 00:41:52,573
and a particular moment basically. And being able to just sort of pause right there. So we've made

702
00:41:52,593 --> 00:41:55,958
this module, this recording module, that you can just sort of drop into anything

703
00:41:56,418 --> 00:41:59,943
and like you've got a little recording window that you can kind of go backwards and forwards.

704
00:42:00,482 --> 00:42:02,383
The awesome thing is, is hit pause there.

705
00:42:02,763 --> 00:42:07,507
You can also like record what would, what would be seen through the device off to

706
00:42:07,527 --> 00:42:11,570
the side so that you can kind of like look at where you are right now and also

707
00:42:11,770 --> 00:42:15,754
put your finger over into the, into the stream and see the video from the, the

708
00:42:16,254 --> 00:42:18,015
device side view from that part of the stream.

709
00:42:18,035 --> 00:42:20,077
So you can compare two streams right then and there.

710
00:42:20,838 --> 00:42:24,701
And then lastly, this is all in world data, but you couldn't bake it out and

711
00:42:24,741 --> 00:42:27,363
just call it relative and then just take the dataset back to your desk.

712
00:42:27,933 --> 00:42:30,575
and try and debug some things from your desk as well.

713
00:42:30,715 --> 00:42:33,697
So it makes a nice little data set.

714
00:42:33,757 --> 00:42:35,719
Instead of a Fraps video in your bug report,

715
00:42:35,739 --> 00:42:38,320
you can just include an entire huge data set

716
00:42:38,360 --> 00:42:39,962
and say the bug happens at this time.

717
00:42:40,722 --> 00:42:44,025
And people back at their desk can try and debug it

718
00:42:44,625 --> 00:42:46,326
by importing that data set directly.

719
00:42:49,589 --> 00:42:52,651
So let's talk about the ecosystem a little bit.

720
00:42:55,184 --> 00:43:01,907
And one aspect of this, so Brian and I and our organizations obviously have been talking

721
00:43:01,927 --> 00:43:08,230
for a long time about, you know, I think we have very similar and inverted goals with

722
00:43:08,250 --> 00:43:13,653
each other, where we're trying to build tools that will let developers make experiences

723
00:43:13,693 --> 00:43:15,654
that will work on any platform, right?

724
00:43:15,694 --> 00:43:17,295
That's kind of our whole thing, right?

725
00:43:19,174 --> 00:43:22,654
And I don't want to speak too much for Magically, but generally the inverse.

726
00:43:22,694 --> 00:43:28,656
Yeah, we're trying to allow creators to use any engine they want to make that content.

727
00:43:28,976 --> 00:43:34,158
And so in the end, we have a particular set of hardware functionality.

728
00:43:36,238 --> 00:43:45,441
And what Unity is trying to do is trying to basically make a set of authoring tools that are

729
00:43:46,831 --> 00:43:48,071
functionality agnostic.

730
00:43:48,692 --> 00:43:52,453
So you not only have to abstract away the hardware,

731
00:43:52,473 --> 00:43:54,454
you have to abstract away the features themselves.

732
00:43:55,354 --> 00:43:57,815
So that another human being might be an additional feature

733
00:43:57,895 --> 00:44:00,136
to an experience, which would lead you to a multi-user.

734
00:44:00,817 --> 00:44:02,197
It's not just about hardware.

735
00:44:02,557 --> 00:44:04,658
And so this sort of abstraction layer.

736
00:44:05,796 --> 00:44:09,958
is not only going to allow different devices to run, but different devices with different

737
00:44:09,998 --> 00:44:14,840
functionality in different environments that afford different experiences all have to be

738
00:44:14,880 --> 00:44:20,302
abstracted out so that not only can the editor tell us what it needs to tell us, but that

739
00:44:20,322 --> 00:44:22,743
the devices can respond to the things that make sense.

740
00:44:26,001 --> 00:44:35,116
Yeah, from a dev perspective, we worked with this project on AirCore and well, you know, it's out on Android, but

741
00:44:36,618 --> 00:44:43,122
You obviously want your game to be across as many platforms as possible, which is one of the reasons that you can use tools with Unity and create with Unity.

742
00:44:44,062 --> 00:44:44,442
And from a...

743
00:44:44,462 --> 00:44:45,943
You can say the other guys too, it's okay.

744
00:44:45,963 --> 00:44:48,724
You know, well, we do, you know, we do work for...

745
00:44:48,864 --> 00:44:56,748
But it's really actually the creating the AR across multiple platforms is not so hard, like there's no barrier really to us bringing this to iOS.

746
00:44:57,288 --> 00:45:01,870
It's more that if you were doing, you're adding on these other modules of sensing,

747
00:45:02,251 --> 00:45:04,752
each platform can have its own kind of...

748
00:45:06,173 --> 00:45:11,138
I guess like ways of doing world sensing that don't necessarily come across.

749
00:45:12,039 --> 00:45:16,745
So if you're trying to advance AR in that way, then that is more the sticking point

750
00:45:16,885 --> 00:45:20,869
rather than bringing the AR content across platforms, at least in our experience.

751
00:45:20,989 --> 00:45:25,254
Well, it's so interesting seeing your app that does all these things that we haven't

752
00:45:25,314 --> 00:45:25,675
seen before.

753
00:45:27,399 --> 00:45:28,539
Many things we haven't seen in an app before,

754
00:45:28,579 --> 00:45:30,359
and many things we haven't seen simultaneously

755
00:45:30,399 --> 00:45:31,380
in the same app before.

756
00:45:31,920 --> 00:45:34,460
And I think that seems like it speaks to the fact

757
00:45:34,480 --> 00:45:37,701
that you really dove deep into a platform

758
00:45:37,741 --> 00:45:39,602
versus trying to make something broad.

759
00:45:39,722 --> 00:45:41,582
Yeah, and that's one of the advantages

760
00:45:41,642 --> 00:45:42,923
of diving deep into the platform

761
00:45:42,963 --> 00:45:45,043
is that you can push innovation in certain ways,

762
00:45:46,524 --> 00:45:49,504
but then you are tied to that vocabulary

763
00:45:49,604 --> 00:45:50,925
of working within that platform.

764
00:45:54,617 --> 00:45:57,598
I just wanted to show this little table that we have on my team.

765
00:45:58,058 --> 00:46:03,920
The y-axis there is platforms, whether that's hardware devices or software platforms, and

766
00:46:04,000 --> 00:46:06,261
the x-axis is functionality.

767
00:46:07,062 --> 00:46:11,683
So it kind of gives you a sense of what it's like trying to support all these things, and

768
00:46:11,703 --> 00:46:13,124
we want all this to work nicely together.

769
00:46:13,144 --> 00:46:15,965
So I just thought it was fun to see.

770
00:46:16,105 --> 00:46:20,667
We were jokingly saying that in the 90s we saw this with 3D cards, and in the 80s we

771
00:46:20,707 --> 00:46:22,468
saw this with Sound Blaster equivalent cards.

772
00:46:22,768 --> 00:46:23,948
So this is not new either.

773
00:46:25,128 --> 00:46:27,829
Not a new problem, but an interesting space for sure.

774
00:46:27,849 --> 00:46:32,711
Anything anybody else wanted to touch on on ecosystem?

775
00:46:33,651 --> 00:46:35,492
I mean, I think that what you should be,

776
00:46:35,832 --> 00:46:37,732
the biggest part of what you should be taking away

777
00:46:37,772 --> 00:46:41,094
from this talk is that the evolution proved

778
00:46:41,294 --> 00:46:44,355
just how much data there was and how much we need tools

779
00:46:44,435 --> 00:46:46,195
to support the use of that data.

780
00:46:46,635 --> 00:46:49,176
I think in exactly the same way that like in the early 90s,

781
00:46:49,196 --> 00:46:51,117
let's say when shaders became a thing,

782
00:46:51,217 --> 00:46:53,318
suddenly we realized, well not just shaders, but like,

783
00:46:54,162 --> 00:47:00,170
Bump maps and light maps and normal maps and like suddenly the tools to make good art became

784
00:47:01,111 --> 00:47:06,097
tenfold more important than it was when you could just make your textures and apply them

785
00:47:06,137 --> 00:47:06,337
to your.

786
00:47:07,140 --> 00:47:08,502
You know, 200 poly model.

787
00:47:08,902 --> 00:47:15,148
And so, likewise, now in order to sort of get the reality flavoring behind all of the things,

788
00:47:15,368 --> 00:47:21,474
like not just the art, but the interactions and the UI and the placement and all of the things,

789
00:47:21,915 --> 00:47:23,296
you have all of this extra data.

790
00:47:23,677 --> 00:47:25,118
And so we're incorporating this.

791
00:47:25,719 --> 00:47:32,542
explosion of tools in the ecosystem to try and get our heads wrapped around that and to get people to

792
00:47:33,082 --> 00:47:37,145
back to the productivity level that they've had for a long time now. Because we actually have

793
00:47:37,245 --> 00:47:42,868
pretty shockingly good tools nowadays to make screen-based entertainment or just regular

794
00:47:42,908 --> 00:47:49,371
applications. But those tools are largely, you know, the Microsoft paint of going forward is

795
00:47:49,391 --> 00:47:54,474
in that they are sufficient but they're somewhat basic for the level of data that we now need to

796
00:47:54,514 --> 00:47:55,094
push forward with.

797
00:48:00,310 --> 00:48:04,332
So let's also touch on, we're talking all XR here, right?

798
00:48:04,352 --> 00:48:07,893
Baran, I know you had some interesting other spaces.

799
00:48:08,414 --> 00:48:13,496
Yeah, so in our case, we took the same rule-based system

800
00:48:13,556 --> 00:48:16,757
that we developed and knew how to solve fairly quickly

801
00:48:16,997 --> 00:48:18,938
and tried to see, and this is something

802
00:48:18,978 --> 00:48:21,579
that I really, really hopeful will happen

803
00:48:21,659 --> 00:48:24,440
when Unity will release their rule system,

804
00:48:25,160 --> 00:48:26,601
because rules are amazing.

805
00:48:27,181 --> 00:48:29,442
It's stuff that you can't even imagine yet.

806
00:48:30,310 --> 00:48:32,833
So we applied it, we said to ourself,

807
00:48:33,414 --> 00:48:35,716
okay, so layout of synthetic objects

808
00:48:35,796 --> 00:48:37,098
on the real world is one thing,

809
00:48:37,298 --> 00:48:41,904
but what if we want to create a layout of text over an image?

810
00:48:42,184 --> 00:48:44,627
Then we have a set of rules, they are different.

811
00:48:45,027 --> 00:48:48,551
Here we have color and contrast and font size and whatever.

812
00:48:49,492 --> 00:48:50,793
But the idea is the same.

813
00:48:51,113 --> 00:48:53,433
So here we took the metadata of the image

814
00:48:54,334 --> 00:48:55,854
and created a couple of examples.

815
00:48:56,114 --> 00:48:58,155
Again, they are not perfect and they are not

816
00:48:58,275 --> 00:49:00,656
ready to be productized, but they show you

817
00:49:00,696 --> 00:49:05,697
the power of rules as a way of thinking about problems.

818
00:49:06,217 --> 00:49:09,418
The way you have a solver, even if it's not a good solver,

819
00:49:09,878 --> 00:49:12,419
it gives you a starting point that

820
00:49:12,499 --> 00:49:14,720
will allow you to see things differently.

821
00:49:14,860 --> 00:49:16,520
That's what we are hoping for.

822
00:49:18,884 --> 00:49:21,347
And we, of course, we can't do stuff

823
00:49:21,367 --> 00:49:22,308
without machine learning.

824
00:49:22,428 --> 00:49:24,811
So we are using the same set of rules

825
00:49:24,911 --> 00:49:29,056
to arrange for furniture arrangement

826
00:49:29,116 --> 00:49:32,981
and creation of environments

827
00:49:33,061 --> 00:49:35,163
because right now we are testing.

828
00:49:37,064 --> 00:49:41,745
Machine learning models on completely synthetic rendering

829
00:49:41,785 --> 00:49:44,946
in order to achieve high accuracy object detection

830
00:49:45,086 --> 00:49:46,487
and these are a couple of examples

831
00:49:46,547 --> 00:49:48,627
that we are working on right now.

832
00:49:50,508 --> 00:49:52,868
Some of the rules are created by a set of rules,

833
00:49:52,948 --> 00:49:58,009
not Feng Shui but similar walkable areas,

834
00:49:58,250 --> 00:50:03,131
facing TV in such basic rules.

835
00:50:03,831 --> 00:50:09,215
And the object themselves in the scenes are placed using a set of rules to place them

836
00:50:09,436 --> 00:50:13,599
in a natural, as much as natural conditions as possible.

837
00:50:14,079 --> 00:50:19,884
And we are checking to see how can photorealistic rendering right now, or physically based rendering,

838
00:50:20,404 --> 00:50:27,450
can help in easily create models such, applicable models, such as recognize a set of kitchen

839
00:50:27,490 --> 00:50:31,033
tools, recognize that in the future to the ecosystem.

840
00:50:31,814 --> 00:50:32,054
All right.

841
00:50:32,975 --> 00:50:36,657
I also found it really interesting, the upper right example there,

842
00:50:36,697 --> 00:50:40,919
where you're using the rules to then generate a layout of furniture, right?

843
00:50:40,939 --> 00:50:41,799
Yes.

844
00:50:42,380 --> 00:50:44,341
I found that example really compelling in particular,

845
00:50:44,401 --> 00:50:49,063
because we really want that functionality where we could just generate a billion rooms

846
00:50:49,103 --> 00:50:50,644
to go test your content against and say,

847
00:50:50,724 --> 00:50:54,465
oh, your content works in like 80% of rooms, according to our generator.

848
00:50:56,166 --> 00:50:58,507
But then, Brian, I know you had some opinions about that.

849
00:50:58,667 --> 00:51:00,988
I just, yeah, the thing I would watch out for is that...

850
00:51:02,393 --> 00:51:08,015
Watch that your room generator isn't too simple, because it can make rooms that are nice and clean,

851
00:51:08,575 --> 00:51:12,636
and then your algorithms may only work in nice clean rooms, or it might make things that are,

852
00:51:12,656 --> 00:51:17,178
you know, who would ever put a television in the middle of the room? And it turns out that in some

853
00:51:17,218 --> 00:51:20,078
cultures that's where they always put the television. You just always have to watch out

854
00:51:20,118 --> 00:51:25,180
for implied biases in your generators at that point. And if you have algorithms that are

855
00:51:25,220 --> 00:51:29,921
learning off of those generators, you have to watch that the cart isn't pulling the horse and

856
00:51:29,961 --> 00:51:31,361
that sort of thing. And so...

857
00:51:33,176 --> 00:51:41,402
We've used this a lot though, like he said, synthetic datasets end up being the thing that gets you a large corpus of your training,

858
00:51:41,443 --> 00:51:45,806
and then you try and layer it with real datasets as you can collect them.

859
00:51:46,307 --> 00:51:50,750
But this is very well known in the magic, in the, what's it called?

860
00:51:51,847 --> 00:51:53,908
machine learning community.

861
00:51:53,948 --> 00:51:57,329
There's formulas for how much synthetic data is too much

862
00:51:57,369 --> 00:51:58,370
and that kind of thing already.

863
00:51:58,430 --> 00:52:02,211
So again, this sort of stuff is,

864
00:52:02,831 --> 00:52:06,072
it's just hard problems and there's just so many of them.

865
00:52:06,172 --> 00:52:07,153
So it's good stuff though.

866
00:52:11,116 --> 00:52:15,578
One application of this stuff that is near and dear for me is

867
00:52:16,558 --> 00:52:20,800
applying this sort of rule-based thinking to strictly VR applications.

868
00:52:22,180 --> 00:52:24,201
You know, right now in room-scale VR,

869
00:52:24,962 --> 00:52:29,664
I imagine probably many people in this room have probably done room-scale VR stuff, and kind of the

870
00:52:30,464 --> 00:52:35,428
Arguably the best you can do right now is sort of like fit a pre-built

871
00:52:35,488 --> 00:52:40,532
virtual room into the size that your guardian will allow. Like, oh, it turns out, you know, you're in a big empty room.

872
00:52:40,552 --> 00:52:45,156
We can stick a really big virtual space in there. Or, you know, you don't have that much space

873
00:52:45,176 --> 00:52:46,757
so we're gonna stick a smaller version of that in.

874
00:52:48,289 --> 00:52:50,250
Even that is pretty relatively rare.

875
00:52:51,111 --> 00:52:53,333
So I'm really looking forward to being able to apply

876
00:52:53,373 --> 00:52:55,174
this sort of rule-based stuff to,

877
00:52:55,775 --> 00:52:59,758
you know, the system can take your guardian boundary shape,

878
00:53:00,278 --> 00:53:01,880
and on more advanced systems than that,

879
00:53:01,920 --> 00:53:03,861
it can take the actual mesh of the world

880
00:53:03,921 --> 00:53:06,323
for headsets that have cameras on them, right?

881
00:53:06,844 --> 00:53:10,066
And actually give you a full-blown VR experience

882
00:53:10,106 --> 00:53:12,728
that also actually matches to your real space,

883
00:53:12,929 --> 00:53:14,970
which is just another flavor of the same problem, of course.

884
00:53:19,096 --> 00:53:22,478
And there is so much that we could not jam into this talk.

885
00:53:24,619 --> 00:53:25,560
Yeah, when you think about,

886
00:53:26,040 --> 00:53:28,121
think about like what I said earlier was,

887
00:53:28,542 --> 00:53:30,503
you have to quickly determine

888
00:53:30,863 --> 00:53:32,324
what the rules of engagement are

889
00:53:32,344 --> 00:53:33,344
for this particular experience.

890
00:53:33,364 --> 00:53:35,126
Well, imagine if there's four experiences

891
00:53:35,186 --> 00:53:36,967
all running in the same room at once.

892
00:53:37,327 --> 00:53:39,408
Like how do you tell people that?

893
00:53:39,788 --> 00:53:40,609
Or do you limit it?

894
00:53:40,709 --> 00:53:41,309
Or do you not?

895
00:53:41,429 --> 00:53:41,830
Or whatever.

896
00:53:43,956 --> 00:53:47,197
The thing I would say is that this is a very wide open space.

897
00:53:47,717 --> 00:53:50,418
XR is huge and it's very infantile.

898
00:53:50,658 --> 00:53:53,619
And I would say that if this sort of stuff interests you,

899
00:53:53,639 --> 00:53:55,840
there's a huge amount of companies that would love,

900
00:53:56,380 --> 00:53:59,921
you know, if you love building tools and you love solving problems that nobody has solved yet,

901
00:54:00,001 --> 00:54:02,022
I would say please come and talk to people because

902
00:54:04,163 --> 00:54:07,904
there's a lot more tools to make and there's a lot more rules to figure out.

903
00:54:08,885 --> 00:54:08,945
And...

904
00:54:10,652 --> 00:54:15,236
We need to get to the point where we have those high-powered tools that allow everybody

905
00:54:15,276 --> 00:54:19,320
to start making this space because this is where human-computer interaction is going.

906
00:54:20,541 --> 00:54:24,044
Soon, there will not be screens up on this wall.

907
00:54:24,365 --> 00:54:27,207
We'll all be sitting and we'll all be talking and we'll all be interacting with something

908
00:54:27,227 --> 00:54:30,430
that's floating around and we'll all have a little copy here and we'll all be talking

909
00:54:30,470 --> 00:54:30,530
and...

910
00:54:30,810 --> 00:54:36,772
I'll be collecting a bunch of stuff over to the side, and I'll get flagged eventually that one of your guys' questions is pretty big,

911
00:54:36,812 --> 00:54:39,773
and so I'll start to formulate an answer, and then I'll push it out to the crowd.

912
00:54:39,954 --> 00:54:43,995
We'll be doing stuff like this very, very frequently, very, very quickly.

913
00:54:44,295 --> 00:54:46,476
I don't think it's even 10 years out, personally.

914
00:54:46,916 --> 00:54:48,777
And so, like, this is where we're heading.

915
00:54:48,837 --> 00:54:52,178
This is why we want to do the things that we're doing, because...

916
00:54:53,225 --> 00:54:56,389
We're sick and tired of watching people walk around with their little screen in their hand,

917
00:54:56,650 --> 00:54:57,992
and they're not connecting with each other.

918
00:54:58,392 --> 00:55:02,778
And what we really want to do is share compute with each other and share pixels with each other,

919
00:55:02,818 --> 00:55:04,341
and this is really the only way we can do it.

920
00:55:04,401 --> 00:55:04,921
But it's hard.

921
00:55:05,683 --> 00:55:05,923
Hell yeah.

922
00:55:12,542 --> 00:55:17,549
There's definitely things I could, sorry, touch on about privacy and ethics.

923
00:55:18,290 --> 00:55:25,861
I did want to just back up a little bit to the procedural and the fact that something that may be useful to discuss about how

924
00:55:28,038 --> 00:55:31,321
The Tendar was created in terms of like,

925
00:55:31,361 --> 00:55:33,363
if you are creating a character

926
00:55:33,603 --> 00:55:36,326
that's not necessarily tethered to a surface,

927
00:55:36,647 --> 00:55:38,548
how do you get it to move and act in a room

928
00:55:38,589 --> 00:55:39,930
that it feels more natural?

929
00:55:40,751 --> 00:55:43,153
And that there's a very complex algorithm

930
00:55:43,193 --> 00:55:45,556
behind how the fish swims and moves around that room.

931
00:55:46,096 --> 00:55:48,137
And at the point when we were making the app,

932
00:55:48,498 --> 00:55:51,240
this semantic definition of what a wall is

933
00:55:51,640 --> 00:55:52,901
was not available to us.

934
00:55:53,561 --> 00:55:56,183
So one of the ways to get around that

935
00:55:56,263 --> 00:55:58,484
is to use the point cloud data.

936
00:55:58,885 --> 00:56:01,186
And then we were able to ray cast

937
00:56:01,246 --> 00:56:03,868
to figure out the boundaries of the room

938
00:56:04,008 --> 00:56:07,851
and generate this model so that the character can

939
00:56:07,911 --> 00:56:09,011
move within that space.

940
00:56:09,232 --> 00:56:12,434
So it's a mix of generating from the procedural.

941
00:56:13,054 --> 00:56:13,695
That was helpful.

942
00:56:15,096 --> 00:56:18,299
I'm happy to also talk about privacy and ethics, too.

943
00:56:18,359 --> 00:56:20,181
There's Tendar starts with a disclaimer

944
00:56:20,221 --> 00:56:24,406
that all the data is on your phone, and that is true.

945
00:56:24,606 --> 00:56:27,529
It's all models that are just saved to your phone,

946
00:56:27,950 --> 00:56:30,452
even though the company is a kind of fictional,

947
00:56:30,532 --> 00:56:33,215
speculative fiction that talks about gathering emotion.

948
00:56:35,213 --> 00:56:39,934
I always like having permissions, privacy, and ethics on any talk that we do in this

949
00:56:39,974 --> 00:56:45,196
space when we're not directly talking about that, just because it's obviously such an

950
00:56:45,336 --> 00:56:49,117
aspect of all of this, and it's becoming more and more a part of the cultural conversation,

951
00:56:49,137 --> 00:56:49,897
which is wonderful.

952
00:56:49,917 --> 00:56:54,278
But I just kind of want to bookmark that in there, of like, make sure when we're having

953
00:56:54,298 --> 00:56:56,359
these conversations that we're also having those conversations too.

954
00:56:58,208 --> 00:56:59,349
But yeah, like Brian was saying,

955
00:56:59,369 --> 00:57:02,491
there's so, so much in this space.

956
00:57:02,511 --> 00:57:04,193
There are so many unsolved problems

957
00:57:04,333 --> 00:57:07,575
and any of us who are cracking into a particular part of it,

958
00:57:07,956 --> 00:57:10,738
we're doing that knowing that there's all,

959
00:57:10,818 --> 00:57:13,120
this is like infinite amount of stuff on the sides

960
00:57:13,861 --> 00:57:15,922
that we're gonna have to address in time as well.

961
00:57:15,982 --> 00:57:18,544
So like, I'm sure everybody in this room is doing,

962
00:57:19,185 --> 00:57:21,367
is working on a particular piece of the puzzle

963
00:57:21,427 --> 00:57:22,628
and I'm really excited to see this all

964
00:57:23,689 --> 00:57:25,450
coalesce in the next coming years.

965
00:57:27,540 --> 00:57:28,161
That's what we got.

966
00:57:28,722 --> 00:57:29,563
Thank you very much.

967
00:57:29,943 --> 00:57:31,665
And if you have any questions,

968
00:57:31,746 --> 00:57:34,429
please remember to use the microphones in the aisles there.

969
00:57:34,449 --> 00:57:37,112
Thank you.

970
00:57:37,132 --> 00:57:37,372
Thank you.

971
00:57:37,433 --> 00:57:38,314
Thank you.

972
00:57:38,354 --> 00:57:38,854
Thank you.

973
00:57:38,874 --> 00:57:39,415
Thanks again.

974
00:57:39,475 --> 00:57:43,740
Thanks again.

975
00:57:43,760 --> 00:57:43,901
Cool.

976
00:57:44,481 --> 00:57:45,242
Total understanding.

977
00:57:45,262 --> 00:57:46,704
I have a question.

978
00:57:48,073 --> 00:57:48,493
Is this on?

979
00:57:48,813 --> 00:57:49,054
OK.

980
00:57:50,495 --> 00:57:54,237
In regards to understanding the diverse worlds that are out

981
00:57:54,297 --> 00:57:57,980
there and your diverse users, have your companies found good

982
00:57:58,080 --> 00:58:02,143
ways to understand how to make your experiences inclusive?

983
00:58:02,623 --> 00:58:05,025
Like, how do you understand the variety of living room

984
00:58:05,065 --> 00:58:10,409
setups that are around the world, the variety of gestures

985
00:58:10,449 --> 00:58:12,310
that people understand and how those relate to their

986
00:58:12,330 --> 00:58:13,631
cultures at scale?

987
00:58:14,952 --> 00:58:16,533
I mean, quite honestly, it's just a

988
00:58:16,573 --> 00:58:17,634
massive data collection.

989
00:58:19,322 --> 00:58:21,023
Program that we're undergoing right now.

990
00:58:21,823 --> 00:58:26,225
Like, you know, every region that we're going to go into as a hardware developer,

991
00:58:26,245 --> 00:58:28,946
we have to go in and actually find that information out.

992
00:58:29,926 --> 00:58:33,508
And in exactly the same way that, like, when I worked at Blizzard

993
00:58:33,668 --> 00:58:36,609
and we put Hearthstone out on 19 different languages,

994
00:58:36,649 --> 00:58:41,331
we had to, like, get all of the text translated to those 19 languages using native speakers.

995
00:58:41,731 --> 00:58:43,692
It's sort of the same thing. You have to go out.

996
00:58:43,952 --> 00:58:46,393
You have to do the hard work in order to do that.

997
00:58:47,573 --> 00:58:48,213
Cool. Thank you.

998
00:58:49,083 --> 00:58:49,764
Yeah, totally agree with that.

999
00:58:49,784 --> 00:58:51,365
I mean, I also want to put it out there.

1000
00:58:51,385 --> 00:58:53,846
I mean, I know this is, you know,

1001
00:58:53,886 --> 00:58:54,887
always said and always understood,

1002
00:58:54,927 --> 00:58:56,107
but I really want to underscore it here

1003
00:58:56,147 --> 00:58:57,628
that like, I think it's super important

1004
00:58:57,648 --> 00:59:00,550
that you work with people from different backgrounds

1005
00:59:00,710 --> 00:59:02,250
to be asking those kinds of questions

1006
00:59:02,290 --> 00:59:04,271
and to be calling things out as they see them

1007
00:59:04,311 --> 00:59:06,593
of like, oh, hey, that is an assumption right there.

1008
00:59:06,633 --> 00:59:07,193
That's a bias.

1009
00:59:07,213 --> 00:59:11,335
So, you know, plus one, always gather your group.

1010
00:59:11,355 --> 00:59:12,436
Thanks guys.

1011
00:59:13,042 --> 00:59:13,462
Thank you.

1012
00:59:13,942 --> 00:59:15,682
Hey guys, got a quick question.

1013
00:59:15,722 --> 00:59:19,843
I noticed if the device that you're using is stable, then

1014
00:59:19,863 --> 00:59:21,283
the animation plays smoothly.

1015
00:59:21,303 --> 00:59:26,024
But if there's any type of motion, it seems very glitchy.

1016
00:59:26,424 --> 00:59:28,965
Or is there better stabilization or

1017
00:59:29,545 --> 00:59:30,945
frame-to-frame blending?

1018
00:59:30,965 --> 00:59:34,606
Because if you're moving around, any augmented reality

1019
00:59:34,666 --> 00:59:37,447
that I've seen so far just is kind of jerky.

1020
00:59:37,507 --> 00:59:37,847
Is that a?

1021
00:59:39,843 --> 00:59:46,088
Software, hardware, is that something moving into the future that you guys see being fixed, or do you guys

1022
00:59:47,469 --> 00:59:48,550
don't think it'll be fixed?

1023
00:59:49,791 --> 00:59:53,674
No, I mean you probably have just seen fairly early examples.

1024
00:59:54,115 --> 01:00:00,740
I mean I see stuff every day that doesn't feel very jerky and so like the thing is is that especially with

1025
01:00:02,253 --> 01:00:07,635
You know, head mounted systems, the frame rate, frame rate is super important

1026
01:00:07,675 --> 01:00:10,695
because you're, you're dealing with a physics prediction problem and the

1027
01:00:10,755 --> 01:00:14,416
smaller, the Delta T on that physics prediction problem, the better the

1028
01:00:14,596 --> 01:00:15,016
prediction.

1029
01:00:15,536 --> 01:00:19,757
And so like the faster the system runs, the better, like, like exponentially

1030
01:00:19,837 --> 01:00:21,518
better the, the, the head post gets.

1031
01:00:21,938 --> 01:00:25,819
And so if you've seen an early system, that's running at a rock solid 30 frames

1032
01:00:25,859 --> 01:00:29,840
per second, it's going to be jerky just because when you move the predicting

1033
01:00:29,900 --> 01:00:31,960
module can't really predict.

1034
01:00:33,241 --> 01:00:35,363
With a third of a millisecond, you know,

1035
01:00:35,463 --> 01:00:37,244
with 300 milliseconds where it's going to be.

1036
01:00:37,284 --> 01:00:39,586
Where as if it's running at 90 or 120 Hertz,

1037
01:00:39,906 --> 01:00:41,648
suddenly that becomes silky smooth.

1038
01:00:42,188 --> 01:00:42,468
Okay.

1039
01:00:42,688 --> 01:00:44,570
And for devs the things we most care about

1040
01:00:44,610 --> 01:00:48,092
in shipping is frame rate and smoothness, so.

1041
01:00:49,033 --> 01:00:50,694
Awesome, thank you guys.

1042
01:00:50,714 --> 01:00:51,655
Can we switch this side?

1043
01:00:52,218 --> 01:00:56,641
Hi, I'm doing a graduate project on AR and the environment.

1044
01:00:56,922 --> 01:01:00,785
And I was wondering, can you recommend some solid resources

1045
01:01:00,865 --> 01:01:04,268
for us non-engineers who are trying to reverse engineer

1046
01:01:04,288 --> 01:01:06,229
a lot of the stuff we're finding in tutorials?

1047
01:01:06,729 --> 01:01:07,790
Then they're too old.

1048
01:01:07,870 --> 01:01:12,074
And so we're basically trying to do planar target image

1049
01:01:12,374 --> 01:01:15,957
and GPS tagged 3D objects.

1050
01:01:17,862 --> 01:01:22,728
Come talk afterwards and we can toss you some stuff.

1051
01:01:23,268 --> 01:01:26,332
It's difficult on camera to give you recommendations.

1052
01:01:26,372 --> 01:01:29,836
Yeah, I was just working through, like, I can name a few people, but we're not supposed to, like, pick sides.

1053
01:01:31,709 --> 01:01:36,472
I wanted to touch on something you guys had mentioned early on about how to train people

1054
01:01:36,692 --> 01:01:38,133
to interact with these spaces.

1055
01:01:38,513 --> 01:01:42,675
Everybody knows how to swipe to unlock your phone, but we had to be trained to understand

1056
01:01:43,336 --> 01:01:46,378
how to interact with that screen space.

1057
01:01:46,698 --> 01:01:51,761
So what do you guys see as the future of a design language that we can all adopt that

1058
01:01:52,201 --> 01:01:55,203
is easy for people to understand and we don't have to build that into our experience?

1059
01:01:55,524 --> 01:01:59,305
Yeah, I mean, that is very, very much what is being built right now.

1060
01:01:59,665 --> 01:02:05,047
That is the bastion level that is forming under our feet as we run down the hallway, right?

1061
01:02:05,107 --> 01:02:09,809
And so, I would say that if you have interest in that work, please get involved in it,

1062
01:02:09,869 --> 01:02:14,010
because we're building it right now, and that is a moment-in-time problem.

1063
01:02:14,390 --> 01:02:19,092
I see little kids now walk up to stores and touch the window because they think it's going to do something.

1064
01:02:19,452 --> 01:02:24,034
And so they have an expectation right off the bat that a shiny, glossy surface means

1065
01:02:24,054 --> 01:02:29,296
I can do stuff with it. And we're getting to that point now where I can put a device on people's

1066
01:02:29,336 --> 01:02:33,797
head and they stick their hand up into the field of sensing right off the bat, which you didn't

1067
01:02:33,857 --> 01:02:39,659
have even two or three years ago. And so we're getting in that direction, but there's so much

1068
01:02:39,719 --> 01:02:45,901
work to do. I mean, it took 50 years to get Swype. And so it's going to take us a chunk of time to

1069
01:02:45,921 --> 01:02:46,541
get a good set.

1070
01:02:46,980 --> 01:02:50,043
So it's a chicken and egg problem as we develop the tools.

1071
01:02:50,383 --> 01:02:53,506
Whatever sticks best is going to be what we adopt.

1072
01:02:53,526 --> 01:02:55,248
And there's also a stepping stone problem.

1073
01:02:55,528 --> 01:03:00,853
People currently have their expectations here based on desktop and mobile.

1074
01:03:01,054 --> 01:03:04,337
Here's what we could even potentially offer them, but they wouldn't even think to try it.

1075
01:03:04,357 --> 01:03:08,080
They have to do this and then you have to give them this and you kind of have to step them up there.

1076
01:03:08,741 --> 01:03:12,222
So a lot of times, we end up having tutorial levels that have

1077
01:03:12,242 --> 01:03:15,404
input schemes that are more based off mobile and then

1078
01:03:15,484 --> 01:03:17,365
slowly you introduce them to the fact that they don't

1079
01:03:17,405 --> 01:03:19,645
have to use that input scheme.

1080
01:03:19,966 --> 01:03:22,246
Just over the course of one tutorial,

1081
01:03:22,567 --> 01:03:24,387
you translate them from nothing,

1082
01:03:24,687 --> 01:03:25,688
to a touch-based thing,

1083
01:03:25,868 --> 01:03:26,908
to a gesture-based thing,

1084
01:03:26,968 --> 01:03:28,609
and then they spend the rest of their time in the app in

1085
01:03:28,649 --> 01:03:30,590
gestures and they never even use the touch stuff.

1086
01:03:31,430 --> 01:03:32,731
Like it's important.

1087
01:03:33,786 --> 01:03:41,678
And I think it's a function of both how these platforms be adopted by the public, because

1088
01:03:42,299 --> 01:03:48,629
Magic Leap just came out, but you can see in the HoloLens history, the...

1089
01:03:49,620 --> 01:03:56,223
initialization, the realization that it's not a good practice, and then the changing

1090
01:03:56,263 --> 01:03:56,783
of the mind.

1091
01:03:57,384 --> 01:03:58,904
So it's an iterative process.

1092
01:03:59,124 --> 01:04:05,828
And because the adaptation is not, you know, it's not millions of users sending data.

1093
01:04:07,385 --> 01:04:08,545
We don't know what will work.

1094
01:04:09,486 --> 01:04:10,846
And we are learning as we go.

1095
01:04:10,926 --> 01:04:13,108
So as Brian said, get involved.

1096
01:04:13,208 --> 01:04:13,988
Yeah, certainly.

1097
01:04:14,388 --> 01:04:15,289
Well, thank you for the answer.

1098
01:04:15,789 --> 01:04:19,891
And I think it's like in Tender actually, it is a question also of user interface and

1099
01:04:19,931 --> 01:04:22,512
design on this on particular applications.

1100
01:04:23,012 --> 01:04:27,254
Like one of the things we were doing was using like thumbprints in certain ways and indicating

1101
01:04:27,294 --> 01:04:28,975
graphically that the phone could swivel.

1102
01:04:29,555 --> 01:04:32,918
But I think that with more head-mounted displays now,

1103
01:04:32,978 --> 01:04:35,140
it will be more apparent, like the way that people will be

1104
01:04:35,200 --> 01:04:36,361
trained to look around the world.

1105
01:04:36,681 --> 01:04:38,543
But when you're making a mobile application, they're

1106
01:04:38,563 --> 01:04:40,845
going to still look at it as if it's a 2D monitor.

1107
01:04:41,225 --> 01:04:43,367
So in order to collect the emotions, we have to kind of

1108
01:04:43,487 --> 01:04:45,909
train them a little bit to actually look around.

1109
01:04:46,329 --> 01:04:50,733
And it is part of the actual graphics on screen.

1110
01:04:51,793 --> 01:04:52,814
That's what I put out there.

1111
01:04:54,187 --> 01:04:56,828
Every day on Unity's internal Slack,

1112
01:04:57,349 --> 01:04:59,871
and I'm sure this is the same at Epic and Microsoft

1113
01:04:59,891 --> 01:05:01,111
and Magic Leap and everywhere else,

1114
01:05:01,332 --> 01:05:03,053
people are just posting all day,

1115
01:05:03,153 --> 01:05:04,834
like look at the cool things this user's doing,

1116
01:05:04,854 --> 01:05:06,555
look at what this person made, look at what this person made.

1117
01:05:07,476 --> 01:05:09,397
So yeah, please keep making that stuff

1118
01:05:09,457 --> 01:05:10,878
and share it and tweet it and do all that

1119
01:05:11,299 --> 01:05:14,361
because we see it and it does work directly

1120
01:05:14,421 --> 01:05:15,722
back into the tools that we're making

1121
01:05:15,742 --> 01:05:17,923
and the conversations we're having and all of that.

1122
01:05:18,964 --> 01:05:19,484
Very exciting.

1123
01:05:19,805 --> 01:05:20,045
Take a nap?

1124
01:05:20,065 --> 01:05:21,506
One more?

1125
01:05:21,526 --> 01:05:22,346
We should probably get going.

1126
01:05:23,451 --> 01:05:26,414
I'll be quick. I had five questions, but I'll just ask one.

1127
01:05:26,654 --> 01:05:27,134
Okay, please.

1128
01:05:28,776 --> 01:05:32,759
Device or server-based ML solution for spatial recognition.

1129
01:05:33,660 --> 01:05:37,543
5G, is that a solution or we should still rely on the device itself?

1130
01:05:40,846 --> 01:05:42,728
Sorry, repeat the question one more time.

1131
01:05:43,428 --> 01:05:46,731
Server-based or device-based spatial recognition.

1132
01:05:46,751 --> 01:05:47,652
No.

1133
01:05:48,049 --> 01:05:51,691
So you can walk and determine the space without staying in one place.

1134
01:05:51,811 --> 01:05:52,732
So what is the question?

1135
01:05:53,492 --> 01:05:55,734
Server-based or client-based?

1136
01:05:55,994 --> 01:05:58,255
Yeah, what do you think is going to take off better?

1137
01:05:58,295 --> 01:05:59,616
Server-based or client-based?

1138
01:05:59,636 --> 01:06:01,077
Both, yeah, yeah, yeah.

1139
01:06:01,157 --> 01:06:03,719
So the heavy-duty stuff will always be in the cloud,

1140
01:06:04,600 --> 01:06:07,902
but you need the responsiveness of on-device.

1141
01:06:08,322 --> 01:06:12,145
Yeah, I mean, even, like, let's, you know,

1142
01:06:13,025 --> 01:06:14,226
let's say we did jump to 6G.

1143
01:06:16,397 --> 01:06:20,764
I would say that we still have a latency issue, especially when I was talking earlier about

1144
01:06:20,784 --> 01:06:25,392
the fact that you need, in some cases, to have really good head pose, 120 hertz, you

1145
01:06:25,412 --> 01:06:30,239
know, sort of frame rate, and nothing on the cloud is going to be at that rate.

1146
01:06:32,167 --> 01:06:38,332
So let's say Google does give you the data of 3D space around you, they do the Google 3D map,

1147
01:06:38,773 --> 01:06:41,375
and you can pull the data from Google. Will that help?

1148
01:06:41,775 --> 01:06:46,539
Oh sure, I mean, don't get me wrong, there will be a ton of applications where stuff on the cloud

1149
01:06:46,599 --> 01:06:51,923
makes it down to the device in time or in a way that makes it super usable. It's just that...

1150
01:06:52,824 --> 01:06:58,391
Because of the latency, if I'm going to do completely recognizing stuff on my hand,

1151
01:06:58,451 --> 01:07:03,017
I don't want to be sending that to some cloud and then bringing it back down when I'm specifically

1152
01:07:03,057 --> 01:07:07,382
trying to do very, very hard, fast, precise work off of my hand, let's say.

1153
01:07:09,004 --> 01:07:14,188
So the question of, for instance, analyzing a room can be done on the cloud because it's

1154
01:07:14,549 --> 01:07:21,935
nothing to, it's not very urgent to, I can stand like three seconds or four seconds until

1155
01:07:21,955 --> 01:07:28,680
you get a result, but identifying your dog running into the room needs to be immediate.

1156
01:07:28,740 --> 01:07:33,024
You can't tolerate latency there, otherwise things will go very wrong.

1157
01:07:33,884 --> 01:07:35,346
So, yeah, both.

1158
01:07:36,713 --> 01:07:39,469
Let's cut it there and take it to the hallway.

1159
01:07:39,710 --> 01:07:40,555
Thank you very much for coming.

