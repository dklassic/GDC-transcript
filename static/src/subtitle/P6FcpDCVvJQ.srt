1
00:00:05,965 --> 00:00:07,346
Thank you.

2
00:00:07,346 --> 00:00:11,310
In 1986, Fred Brooks proclaimed

3
00:00:11,310 --> 00:00:14,212
that there would be no single development

4
00:00:14,212 --> 00:00:17,355
that would deliver one order of magnitude improvement

5
00:00:17,355 --> 00:00:20,117
in a decade in software engineering.

6
00:00:20,117 --> 00:00:24,621
Fast forward 30 years, this slide was presented

7
00:00:24,621 --> 00:00:27,183
by Chelsea Finn at the Deep RL Bootcamp

8
00:00:27,183 --> 00:00:28,605
at UC Berkeley last summer.

9
00:00:28,605 --> 00:00:31,587
And what this slide is showing is that

10
00:00:31,587 --> 00:00:34,590
every generation of machine learning algorithm

11
00:00:35,020 --> 00:00:37,322
is a 10x improvement on the last.

12
00:00:37,322 --> 00:00:42,926
That means that a problem that was taking 100 million steps

13
00:00:42,926 --> 00:00:44,688
just a few years ago,

14
00:00:44,688 --> 00:00:48,251
is now been able to be learned in real time

15
00:00:48,251 --> 00:00:50,072
on a robot in about 20 minutes.

16
00:00:50,072 --> 00:00:54,356
And we're seeing these improvements not every 10 years,

17
00:00:54,356 --> 00:00:57,138
but every six to 12 months.

18
00:00:57,138 --> 00:00:58,619
So my hypothesis is,

19
00:00:59,407 --> 00:01:02,248
If reinforcement learning along with reward functions,

20
00:01:02,248 --> 00:01:04,429
if we can apply that to software development,

21
00:01:04,429 --> 00:01:07,671
that will result in a multi-magnitude improvement

22
00:01:07,671 --> 00:01:10,872
in productivity, reliability, and in simplicity.

23
00:01:10,872 --> 00:01:14,893
And to use video game AI as proof.

24
00:01:17,207 --> 00:01:20,168
So today I want to take you through a couple of stages.

25
00:01:20,168 --> 00:01:22,569
I want to talk about, kind of a brief,

26
00:01:22,569 --> 00:01:25,130
give you a brief taste of reinforcement learning.

27
00:01:25,130 --> 00:01:27,771
And then I want to share the results of the research

28
00:01:27,771 --> 00:01:30,152
that I've been working on into this space.

29
00:01:30,152 --> 00:01:32,933
First, a little bit about my background.

30
00:01:33,919 --> 00:01:36,622
I started in the 1980s on the Commodore 64.

31
00:01:36,622 --> 00:01:44,769
I'm dyslexic, dysgraphic, I also have ADHD, and I really struggled at school in traditional

32
00:01:44,769 --> 00:01:50,475
learning, but I had this aptitude to be able to understand a computer, and I had this passion

33
00:01:50,475 --> 00:01:55,800
for video games, and so while I was failing at school, I was winning contracts to make

34
00:01:55,800 --> 00:01:59,823
video games, and I left school at 15 to join the industry.

35
00:02:00,880 --> 00:02:07,367
Over the next 25 years, I worked my way up and grew with it as the industry grew,

36
00:02:07,367 --> 00:02:13,114
working in multi-disciplines, so in engineering, production, and creative direction.

37
00:02:13,114 --> 00:02:17,099
And at the end, I was an executive producer with Xbox up in Microsoft.

38
00:02:19,458 --> 00:02:24,059
Over the last couple of years, I've been working in a couple of different areas.

39
00:02:24,059 --> 00:02:30,581
So on the one hand, I'm working with neuroscientists and people at the bleeding edge of academic

40
00:02:30,581 --> 00:02:38,123
research on projects that take three to five years to bring to market from neuroscience.

41
00:02:38,123 --> 00:02:41,905
And I also work with a guy called Nils Leyer who is...

42
00:02:42,245 --> 00:02:46,647
He is one of the fathers of video on the internet.

43
00:02:46,647 --> 00:02:50,829
And he's also a VC, so I work with him building his companies.

44
00:02:50,829 --> 00:02:53,431
And both of those kind of pull into this research

45
00:02:53,431 --> 00:02:57,273
that I do into reinforcement learning, which

46
00:02:57,273 --> 00:02:58,454
I started about three years ago.

47
00:02:58,454 --> 00:03:07,078
So there's a problem for me with learning reinforcement learning.

48
00:03:07,817 --> 00:03:12,821
If you look at this equation at the bottom here, I literally cannot read that.

49
00:03:12,821 --> 00:03:19,406
And to make matters worse, my big hack for reading complex materials is to use voice-to-text

50
00:03:19,406 --> 00:03:26,833
software, but math notation breaks that software.

51
00:03:26,833 --> 00:03:31,256
So I have a couple of tricks that I've used.

52
00:03:31,256 --> 00:03:35,019
So the first thing and most important thing for me is building a mental model.

53
00:03:36,312 --> 00:03:40,057
The second thing I would say is get hands on and get hands on as quickly as you can.

54
00:03:40,057 --> 00:03:46,426
I would say don't waste your time with all these online courses that you see everywhere

55
00:03:46,426 --> 00:03:51,612
because you're going to be writing gradient descent algorithms in Python and that really

56
00:03:51,612 --> 00:03:53,895
doesn't offer you a lot of advantage.

57
00:03:55,057 --> 00:03:58,598
Other things, if you are dyslexic, I recommend doing podcasts, videos.

58
00:03:58,598 --> 00:04:02,100
I also have learned to skim read papers.

59
00:04:02,100 --> 00:04:06,582
The one online course place that I did find is machinelearningmastery.com.

60
00:04:06,582 --> 00:04:11,385
That is built for engineers who want to learn machine learning.

61
00:04:11,385 --> 00:04:16,247
So it's high on the engineering and low on the math background.

62
00:04:18,158 --> 00:04:19,879
So if we think about this model here,

63
00:04:19,879 --> 00:04:23,821
this is the classic reinforcement model

64
00:04:23,821 --> 00:04:27,002
that was created by Sutton and Bartow in the 70s and 80s.

65
00:04:27,002 --> 00:04:30,764
And the picture on the left there

66
00:04:30,764 --> 00:04:32,865
is from the Atari DeepMind paper.

67
00:04:35,688 --> 00:04:38,789
So to give you a little kind of instinct,

68
00:04:38,789 --> 00:04:41,591
or try and build your mental model of how this is working,

69
00:04:41,591 --> 00:04:44,692
so the environment in this case are raw pixels coming in,

70
00:04:44,692 --> 00:04:47,554
and they go into a convolutional neural network.

71
00:04:47,554 --> 00:04:50,675
And if you think of that as compressing the pictures,

72
00:04:50,675 --> 00:04:53,237
so the problem space becomes much less

73
00:04:53,237 --> 00:04:55,618
than having to work on the raw pixels.

74
00:04:55,618 --> 00:04:59,560
Then there's an action space, so in the case of breakout,

75
00:04:59,560 --> 00:05:01,321
you can go left, you can go right,

76
00:05:01,321 --> 00:05:03,982
or you can just do nothing.

77
00:05:05,555 --> 00:05:07,936
The third step is delayed discounting.

78
00:05:07,936 --> 00:05:11,338
So when it gets rewarded at the end of the game

79
00:05:11,338 --> 00:05:13,818
or a reward of a point, it steps back in time

80
00:05:13,818 --> 00:05:16,099
and gives a discount of that to those previous steps.

81
00:05:16,099 --> 00:05:20,781
And then each of those, each individual frame

82
00:05:20,781 --> 00:05:23,302
and that reward and the step that it took,

83
00:05:23,302 --> 00:05:25,483
the action that it took, they go into something

84
00:05:25,483 --> 00:05:28,044
called an experience replay buffer.

85
00:05:28,044 --> 00:05:32,205
And this is a million big, right?

86
00:05:34,140 --> 00:05:36,502
it randomly samples that.

87
00:05:36,502 --> 00:05:42,026
And that is really the core of the DeepQN algorithm.

88
00:05:42,026 --> 00:05:43,668
It does no planning,

89
00:05:43,668 --> 00:05:46,029
it's not thinking about anything in the future,

90
00:05:46,029 --> 00:05:51,114
but it is able to find this advanced strategy

91
00:05:51,114 --> 00:05:55,457
of working its way around the side of the breakout.

92
00:05:57,318 --> 00:06:02,419
One thing just to talk about is the exploration versus exploitation trade-off.

93
00:06:02,419 --> 00:06:06,680
So exploration is just randomly searching inside the environment.

94
00:06:06,680 --> 00:06:13,501
Exploitation is taking the signal that has the highest future reward.

95
00:06:13,501 --> 00:06:23,483
And there's a trade-off there and algorithms have to work in different ways to explore the space in an efficient way.

96
00:06:24,911 --> 00:06:28,796
For the DQN paper, they had a very naive approach.

97
00:06:28,796 --> 00:06:30,839
They just, for the first million steps,

98
00:06:30,839 --> 00:06:32,521
were totally random.

99
00:06:32,521 --> 00:06:35,585
And then they reduced that so that over time,

100
00:06:35,585 --> 00:06:38,169
2% of the choices would be random.

101
00:06:38,740 --> 00:06:44,408
But I wanted to step back to dyslexia and neuroscience and automaticity because it turns

102
00:06:44,408 --> 00:06:53,300
out that us as humans have this trade-off as well and different sets of people will

103
00:06:53,300 --> 00:06:55,323
explore problems in a different way.

104
00:06:55,943 --> 00:07:00,585
So there's a researcher in the UK called Rod Nicholson, and he researched dyslexics, and

105
00:07:00,585 --> 00:07:06,387
he found that for them to learn things through rote memorization and automaticity, it took

106
00:07:06,387 --> 00:07:08,288
n times the square root of n.

107
00:07:08,288 --> 00:07:16,711
So if it takes 100 repetitions for a normal student to learn the math times table, it

108
00:07:16,711 --> 00:07:18,472
would take 1,000 repetitions.

109
00:07:19,452 --> 00:07:23,735
But the trade-off there is that a dyslexic will find,

110
00:07:23,735 --> 00:07:26,677
will be looking at more different solutions in life.

111
00:07:26,677 --> 00:07:28,979
So I recommend you do keep them on your team.

112
00:07:28,979 --> 00:07:31,821
In terms of the landscape,

113
00:07:31,821 --> 00:07:33,662
so if you wanna get your hands dirty

114
00:07:33,662 --> 00:07:35,803
and start playing with reinforcement learning today.

115
00:07:36,683 --> 00:07:40,386
These are three of the many players, but I think these are the top ones to look at.

116
00:07:40,386 --> 00:07:42,929
So first, DeepMind.

117
00:07:42,929 --> 00:07:48,955
They write some really great papers and do a lot of research that you're probably familiar

118
00:07:48,955 --> 00:07:50,336
with.

119
00:07:50,336 --> 00:07:55,581
The frameworks that they have, StarCraft 2, if you're a StarCraft player, that's maybe

120
00:07:55,581 --> 00:07:57,203
a good place to start.

121
00:07:57,203 --> 00:07:58,023
And DeepMind Lab.

122
00:07:58,769 --> 00:08:01,371
But they're not so good at publishing algorithms.

123
00:08:01,371 --> 00:08:04,954
They did publish one version of the DQN.

124
00:08:04,954 --> 00:08:06,936
OpenAI, that's where they excel.

125
00:08:06,936 --> 00:08:10,359
So their baselines has nine world-class algorithms

126
00:08:10,359 --> 00:08:11,780
that you can use.

127
00:08:11,780 --> 00:08:12,981
It's all open source.

128
00:08:12,981 --> 00:08:16,984
They have OpenAI GYM framework

129
00:08:16,984 --> 00:08:19,867
that has Classic Control, Atari, Majoko.

130
00:08:20,607 --> 00:08:23,308
And they also built Universe on top of that,

131
00:08:23,308 --> 00:08:27,090
that allows you to play games like GTA or Flash games,

132
00:08:27,090 --> 00:08:29,391
or to learn against those games.

133
00:08:29,391 --> 00:08:32,492
And the new kid on the block is Unity,

134
00:08:32,492 --> 00:08:34,113
and Danny's gonna talk about this.

135
00:08:34,113 --> 00:08:37,695
So they launched ML Agents last year.

136
00:08:37,695 --> 00:08:41,276
So they already have a couple of algorithms there,

137
00:08:41,276 --> 00:08:44,557
and they have at least 10 different sample environments

138
00:08:44,557 --> 00:08:45,258
that you can use.

139
00:08:46,627 --> 00:08:48,608
The first thing that I did with machine learning,

140
00:08:48,608 --> 00:08:50,549
this is going back two or three years,

141
00:08:50,549 --> 00:08:53,550
was I was playing around with a soccer, some soccer AI,

142
00:08:53,550 --> 00:08:56,472
and I took a high cost function,

143
00:08:56,472 --> 00:09:01,975
and I sampled this over 24 hours, this function,

144
00:09:01,975 --> 00:09:05,037
I put it into a linear regression,

145
00:09:05,037 --> 00:09:08,659
so I learned this and I put the model back into the game,

146
00:09:08,659 --> 00:09:12,881
and I was seeing that this was choosing

147
00:09:12,881 --> 00:09:15,442
the same outcome as the function 90 to 95% of the time.

148
00:09:16,143 --> 00:09:18,504
And so that gave me enough kind of confidence

149
00:09:18,504 --> 00:09:21,024
to carry on looking at this space.

150
00:09:21,024 --> 00:09:22,965
Today I've kind of built up my own system.

151
00:09:22,965 --> 00:09:23,865
I call it Analog.

152
00:09:23,865 --> 00:09:29,047
I started before Unity had created their system.

153
00:09:29,047 --> 00:09:30,487
What my system does is it bridges

154
00:09:30,487 --> 00:09:33,748
between OpenAI's baselines and Unity,

155
00:09:33,748 --> 00:09:35,689
and it gives me this nice UI

156
00:09:35,689 --> 00:09:38,069
so that it kind of like being ADHD,

157
00:09:38,069 --> 00:09:39,490
I remember what I was working on

158
00:09:39,490 --> 00:09:41,330
when I go back and it's finished a training run.

159
00:09:43,424 --> 00:09:46,146
So next, I want to talk about the research

160
00:09:46,146 --> 00:09:48,408
that I've been doing and share some of the results.

161
00:09:48,408 --> 00:09:51,231
The first area I'm going to share here

162
00:09:51,231 --> 00:09:53,393
is using reinforcement learning for locomotion.

163
00:09:53,393 --> 00:09:58,077
So if you think why we might want to do this,

164
00:09:58,077 --> 00:10:01,660
well, locomotion is very hard to get right

165
00:10:01,660 --> 00:10:04,063
and is very expensive, is a big part of our budgets.

166
00:10:04,063 --> 00:10:06,505
And it would be good to have an extra tool set

167
00:10:06,505 --> 00:10:07,686
to be able to do that.

168
00:10:10,117 --> 00:10:11,942
So the video here is from DeepMind.

169
00:10:11,942 --> 00:10:16,514
This is a character that's learned completely from scratch

170
00:10:16,514 --> 00:10:17,958
with a reinforcement algorithm.

171
00:10:18,655 --> 00:10:22,376
and the one in the bottom left is from the same paper.

172
00:10:22,376 --> 00:10:27,737
And this one is from OpenAI where they took that algorithm

173
00:10:27,737 --> 00:10:29,877
and they improved on it using something

174
00:10:29,877 --> 00:10:31,017
called parameter noise.

175
00:10:31,017 --> 00:10:34,618
The photo in the bottom left there is,

176
00:10:34,618 --> 00:10:37,218
this is from a talk from a couple of GDCs ago

177
00:10:37,218 --> 00:10:39,939
from Ben Fody, he was talking about physics

178
00:10:39,939 --> 00:10:41,819
and making gains with physics,

179
00:10:41,819 --> 00:10:45,920
and he came up with this idea to use a MIDI controller

180
00:10:45,920 --> 00:10:47,400
to tune all the parameters that.

181
00:10:48,598 --> 00:10:51,942
you need to do just to do a 2D game that he was doing.

182
00:10:51,942 --> 00:10:56,166
And so it would be good if we could put that to rest

183
00:10:56,166 --> 00:11:00,271
and give people the ability to learn those things.

184
00:11:00,271 --> 00:11:03,534
So in terms of the methodology, I'm

185
00:11:03,534 --> 00:11:06,037
building the architecture on top of my analog.

186
00:11:06,037 --> 00:11:09,140
I'm using a couple of algorithms, Actor and DDPG.

187
00:11:10,444 --> 00:11:11,945
I went through some stages.

188
00:11:11,945 --> 00:11:13,745
So initially I wanted to use my naivety

189
00:11:13,745 --> 00:11:16,666
of not really knowing Unity,

190
00:11:16,666 --> 00:11:21,927
not really knowing the physics system inside Unity.

191
00:11:21,927 --> 00:11:25,809
And I'm taking the baselines of the Majoko,

192
00:11:25,809 --> 00:11:30,710
which is the world-class gold standard physics engine.

193
00:11:30,710 --> 00:11:32,951
With Majoko, the simulations are so strong

194
00:11:32,951 --> 00:11:34,591
that if you train a model on Majoko,

195
00:11:34,591 --> 00:11:37,232
you can put it onto a real-life robot

196
00:11:37,232 --> 00:11:38,432
and it will reproduce that.

197
00:11:39,861 --> 00:11:42,702
And then I went through stages of progressively looking more

198
00:11:42,702 --> 00:11:45,403
into the research to improve my outcomes.

199
00:11:45,403 --> 00:11:50,345
So initially, things worked very well.

200
00:11:50,345 --> 00:11:53,527
I was able to get this little worm moving.

201
00:11:53,527 --> 00:11:58,049
And I was able to recreate the ant from Majoko.

202
00:11:58,049 --> 00:12:01,990
But to give you a sense of the time that took in terms

203
00:12:01,990 --> 00:12:04,872
of the experimentation, so it was 44 different experiments

204
00:12:04,872 --> 00:12:05,632
just for the ant.

205
00:12:06,372 --> 00:12:09,695
And that's over 86 million training steps.

206
00:12:09,695 --> 00:12:15,380
So to get things, you know, when I plugged it in some other.

207
00:12:16,187 --> 00:12:18,788
characters, the Hopper or the Humanoid.

208
00:12:18,788 --> 00:12:22,210
I started to see that I'm getting some results,

209
00:12:22,210 --> 00:12:23,991
but they're kind of strange, right?

210
00:12:23,991 --> 00:12:27,113
They don't look like you're seeing in the papers.

211
00:12:27,113 --> 00:12:31,676
And so to address that, I dug into the research some more.

212
00:12:31,676 --> 00:12:34,918
One of the most important things I found was that

213
00:12:34,918 --> 00:12:38,681
a lot of the finesse comes from tuning the reward functions

214
00:12:38,681 --> 00:12:40,001
and also the termination function.

215
00:12:40,001 --> 00:12:42,743
So if you see at the top, we've got the Hopper.

216
00:12:43,384 --> 00:12:47,671
and it will terminate the hopper, so say it's failed if the angle goes over

217
00:12:47,671 --> 00:12:54,581
you know more than 20 degrees either way. It also has things like a cost function so it's kind of...

218
00:12:55,110 --> 00:13:00,193
penalizing the AI for overexertion.

219
00:13:00,193 --> 00:13:04,136
And the one on the bottom is the open,

220
00:13:04,136 --> 00:13:09,339
sorry, is the deep mind reward function for the walker.

221
00:13:09,339 --> 00:13:11,641
And that has things that rewards it

222
00:13:11,641 --> 00:13:15,383
for being upright and the height.

223
00:13:16,102 --> 00:13:20,767
So there I was able to reproduce the benchmarks.

224
00:13:20,767 --> 00:13:23,310
The ones we're seeing on the left or on the top,

225
00:13:23,310 --> 00:13:25,172
that's about a million steps.

226
00:13:25,172 --> 00:13:27,554
And the one on the right is about 3 million.

227
00:13:29,782 --> 00:13:37,185
And then the final stage I went through is comparing the results with the Mojoco.

228
00:13:37,185 --> 00:13:44,148
And I was able to see that the training time was within about 5% and the steps were within about 10%.

229
00:13:44,148 --> 00:13:54,093
It does train differently, which is kind of nice because the algorithm gets around problems that you may have in the physics implementation.

230
00:13:55,165 --> 00:13:57,987
So in conclusion, I've been able to reproduce the hopper

231
00:13:57,987 --> 00:14:00,769
and walker inside a commercial game engine.

232
00:14:00,769 --> 00:14:03,330
I expect that those deep mind results,

233
00:14:03,330 --> 00:14:07,373
so having more elaborate obstacles,

234
00:14:07,373 --> 00:14:09,154
will be reproducible,

235
00:14:09,154 --> 00:14:10,875
and this is definitely worthy of more study.

236
00:14:10,875 --> 00:14:13,157
In terms of future work,

237
00:14:13,157 --> 00:14:15,739
there's a lot of new algorithms that I'd like to...

238
00:14:16,819 --> 00:14:22,502
look at as well as mixing motion capture data with reward functions.

239
00:14:22,502 --> 00:14:29,546
In terms of the total scope, I added this up last night, so everything I did in that

240
00:14:29,546 --> 00:14:33,709
space was 380 experiments and that's about 750 million steps.

241
00:14:33,709 --> 00:14:42,073
So the second area that I'm going to talk about is learning and advanced control in

242
00:14:42,073 --> 00:14:43,054
9-9 training steps.

243
00:14:43,858 --> 00:14:48,440
So the reason to do this is that often the problems

244
00:14:48,440 --> 00:14:52,801
that we want to solve are pretty simple, right?

245
00:14:52,801 --> 00:14:55,301
They may not have the same problem space

246
00:14:55,301 --> 00:14:56,362
as learning locomotion.

247
00:14:56,362 --> 00:14:59,963
And we may want to retrain this very quickly.

248
00:14:59,963 --> 00:15:05,524
So this was an exercise to see how much I could optimize it

249
00:15:05,524 --> 00:15:06,944
and what were the steps I needed to do

250
00:15:06,944 --> 00:15:08,205
to be able to optimize it.

251
00:15:08,205 --> 00:15:12,546
The problem space I created was teaching the AI

252
00:15:12,546 --> 00:15:13,246
to hold fire.

253
00:15:14,002 --> 00:15:19,823
So it's got two actions, it can have fire or not press fire,

254
00:15:19,823 --> 00:15:22,644
but it's got to hold fire over an appropriate number

255
00:15:22,644 --> 00:15:26,506
of frames to hit the target an appropriate distance.

256
00:15:26,506 --> 00:15:29,146
So we know from usability that people like my wife

257
00:15:29,146 --> 00:15:31,707
have struggled learning this control,

258
00:15:31,707 --> 00:15:34,968
so I thought it'd be fun to train an AI.

259
00:15:35,980 --> 00:15:39,604
The way to get this down is I use something called a grid search.

260
00:15:39,604 --> 00:15:44,528
So this is taking the number of layers in a neural network and the layer size,

261
00:15:44,528 --> 00:15:48,212
and just running iteration over iteration until I'm seeing a trend or

262
00:15:48,212 --> 00:15:50,434
a convergence on one combination.

263
00:15:50,434 --> 00:15:54,878
And then taking in another variable, so in this case, the number of stacks to

264
00:15:54,878 --> 00:16:00,122
the number of observations, previous observations that it takes in a row.

265
00:16:01,283 --> 00:16:04,444
and then doing a search to break that down into 99 steps.

266
00:16:04,444 --> 00:16:08,125
So I was very surprised I was able to get it

267
00:16:08,125 --> 00:16:10,505
to train this quickly.

268
00:16:10,505 --> 00:16:15,686
One caveat is that the solution to the problem

269
00:16:15,686 --> 00:16:18,847
is probably correlated to the design of the network

270
00:16:18,847 --> 00:16:22,188
and there isn't a lot of research over that.

271
00:16:22,188 --> 00:16:25,129
So we know that there's a correlation

272
00:16:25,129 --> 00:16:26,729
but we don't necessarily know,

273
00:16:26,729 --> 00:16:28,810
okay for this problem what network should we do?

274
00:16:30,672 --> 00:16:37,554
The third area I want to present today is the case for real-time server,

275
00:16:37,554 --> 00:16:39,015
real reinforcement learning.

276
00:16:39,015 --> 00:16:42,275
So let's say you want to publish a game today,

277
00:16:42,275 --> 00:16:45,116
you have your Python back end that's doing your reinforcement learning.

278
00:16:45,116 --> 00:16:47,957
Could you publish that using?

279
00:16:48,543 --> 00:16:51,646
some of the technology that we use in multiplayer games here.

280
00:16:51,646 --> 00:16:58,833
So my inspiration was for the visual reaction delay.

281
00:16:58,833 --> 00:17:00,575
So on average, we have a 250 millisecond delay

282
00:17:00,575 --> 00:17:05,580
between the visual stimulus and being able to react.

283
00:17:05,580 --> 00:17:07,582
My method was to train 20 frames a second

284
00:17:07,582 --> 00:17:09,404
with a 10 step delay.

285
00:17:09,404 --> 00:17:11,106
So it's 200 milliseconds delay.

286
00:17:11,606 --> 00:17:14,989
And what I found is that this slowed down learning by about 30%.

287
00:17:14,989 --> 00:17:23,175
So it's not a silver bullet, but it is worth looking at if you want to launch today and

288
00:17:23,175 --> 00:17:24,356
bring a product to market.

289
00:17:24,356 --> 00:17:26,958
Okay, so that concludes my talk.

290
00:17:26,958 --> 00:17:28,760
And I'm going to hand over to Wolf.

291
00:17:28,760 --> 00:17:31,602
And please come find me.

292
00:17:31,602 --> 00:17:32,703
I love talking about this stuff.

293
00:17:32,703 --> 00:17:34,584
Thank you very much.

294
00:17:34,584 --> 00:17:36,205
All right, that's been my talk.

295
00:17:37,048 --> 00:17:37,868
That's been my talk.

296
00:17:37,868 --> 00:17:39,289
It's been great talking to you all.

297
00:17:39,289 --> 00:17:40,729
We'll move on.

298
00:17:40,729 --> 00:17:43,230
Hi, I'm Wolf, and I'm from Google.

299
00:17:43,230 --> 00:17:45,991
And the reason why you're listening to me

300
00:17:45,991 --> 00:17:49,632
is because I am the technical lead for developer relations

301
00:17:49,632 --> 00:17:50,432
for TensorFlow.

302
00:17:50,432 --> 00:17:53,893
TensorFlow is a giant framework that

303
00:17:53,893 --> 00:17:56,094
does machine learning and a bunch of other stuff.

304
00:17:56,094 --> 00:17:58,535
It's built by the brain team at Google.

305
00:17:59,907 --> 00:18:01,828
And I'm not going to talk to you about TensorFlow much at all,

306
00:18:01,828 --> 00:18:04,248
because honestly, there's no way I'm going to teach it to you

307
00:18:04,248 --> 00:18:05,209
all in 20 minutes.

308
00:18:05,209 --> 00:18:10,370
But we are at this sort of, but oh,

309
00:18:10,370 --> 00:18:13,991
and I should go back and say that I'm also a game developer.

310
00:18:13,991 --> 00:18:15,031
I've been a game developer.

311
00:18:15,031 --> 00:18:17,092
I actually worked on applying machine learning

312
00:18:17,092 --> 00:18:19,632
to AAA games like a decade ago.

313
00:18:19,632 --> 00:18:20,772
It's tricky.

314
00:18:20,772 --> 00:18:21,613
It's interesting.

315
00:18:21,613 --> 00:18:24,273
And what I wanted to share with you today here, oh, well,

316
00:18:24,273 --> 00:18:24,573
sorry.

317
00:18:25,134 --> 00:18:27,915
And as we alluded to before,

318
00:18:27,915 --> 00:18:29,376
you know, we're at this sort of magical time

319
00:18:29,376 --> 00:18:31,737
in machine learning where things that we,

320
00:18:31,737 --> 00:18:33,898
that a decade ago that we thought

321
00:18:33,898 --> 00:18:35,659
we were going to be able to do,

322
00:18:35,659 --> 00:18:38,780
we just can't, or sorry, we finally can do

323
00:18:38,780 --> 00:18:40,821
things like self-driving cars,

324
00:18:40,821 --> 00:18:43,743
and obviously, you know, our friends in London

325
00:18:43,743 --> 00:18:46,384
having success playing games.

326
00:18:46,384 --> 00:18:48,805
We have, we're beginning to apply machine learning

327
00:18:48,805 --> 00:18:51,106
and deep learning to medical data,

328
00:18:51,106 --> 00:18:52,147
which is pretty amazing.

329
00:18:53,036 --> 00:18:55,838
and actually getting quite interesting results.

330
00:18:55,838 --> 00:18:57,539
That's looking at retina scans.

331
00:18:57,539 --> 00:19:00,080
And of course, making little stompy robots,

332
00:19:00,080 --> 00:19:02,261
which, you know, everybody likes stompy robots.

333
00:19:02,261 --> 00:19:03,162
He's in our lab.

334
00:19:03,162 --> 00:19:06,104
But in this talk, as a game developer,

335
00:19:06,104 --> 00:19:08,005
I want to take you on a tour of a bunch of things

336
00:19:08,005 --> 00:19:10,506
that I find interesting in machine learning,

337
00:19:10,506 --> 00:19:13,888
ways to make the process of making games more interesting,

338
00:19:13,888 --> 00:19:16,150
more exciting, more efficient.

339
00:19:16,150 --> 00:19:17,511
But I'm not really going to talk about NPCs,

340
00:19:17,511 --> 00:19:20,092
because NPCs are cool, but it's a different problem.

341
00:19:21,112 --> 00:19:22,253
So let's start with part one.

342
00:19:22,253 --> 00:19:24,255
Let's talk about statistics.

343
00:19:24,255 --> 00:19:28,760
So your players are all out there creating data.

344
00:19:28,760 --> 00:19:32,204
And in that data, you are finding out about the kinds of

345
00:19:32,204 --> 00:19:34,146
things they like and they don't like to do.

346
00:19:34,146 --> 00:19:36,628
And you should be taking advantage of that.

347
00:19:38,723 --> 00:19:39,623
you should be, ah, there we go,

348
00:19:39,623 --> 00:19:41,785
you should be taking advantage of that.

349
00:19:41,785 --> 00:19:43,966
So, you know, is your player gonna quit today?

350
00:19:43,966 --> 00:19:45,227
Is your player gonna spend today?

351
00:19:45,227 --> 00:19:48,209
These are all sort of time series models you can use.

352
00:19:48,209 --> 00:19:51,311
You can learn to do anomaly detection with cheating.

353
00:19:51,311 --> 00:19:53,372
You can learn about, check if they're grieving.

354
00:19:53,372 --> 00:19:55,313
You can find out if they bought X, are they gonna buy Y?

355
00:19:55,313 --> 00:19:58,075
These are all sorts of things you should be doing.

356
00:19:58,075 --> 00:20:00,556
And, you know, talking about sentiment analysis,

357
00:20:00,556 --> 00:20:02,757
you know, this is a well-studied problem.

358
00:20:02,757 --> 00:20:04,739
Learning, you know, learning about your community

359
00:20:04,739 --> 00:20:05,299
and learning the.

360
00:20:05,439 --> 00:20:08,462
kinds of things that they're saying,

361
00:20:08,462 --> 00:20:10,563
and trying to figure out if they're saying the right things

362
00:20:10,563 --> 00:20:12,285
or things that maybe you don't want so much,

363
00:20:12,285 --> 00:20:15,147
this can make your game a more interesting experience

364
00:20:15,147 --> 00:20:16,968
and more pleasant experience for your users.

365
00:20:16,968 --> 00:20:19,951
You should be totally doing this.

366
00:20:19,951 --> 00:20:22,693
There's open source models where you can try this stuff out.

367
00:20:22,693 --> 00:20:26,296
It's a lot of fun and you can set up a service

368
00:20:26,296 --> 00:20:28,378
to do things like sentiment analysis

369
00:20:28,378 --> 00:20:29,318
or time series prediction

370
00:20:29,318 --> 00:20:30,980
and make your game more efficient.

371
00:20:32,450 --> 00:20:35,431
But I want to talk about things that would affect

372
00:20:35,431 --> 00:20:39,432
kind of the look and the feel of the game directly

373
00:20:39,432 --> 00:20:41,393
that the player interacts with,

374
00:20:41,393 --> 00:20:43,514
rather than the sort of subtle things.

375
00:20:43,514 --> 00:20:45,034
How many of you know about style transfer?

376
00:20:45,034 --> 00:20:46,755
Eh, some of you.

377
00:20:46,755 --> 00:20:48,836
So style transfer is the idea that you take

378
00:20:48,836 --> 00:20:50,916
the semantic structure of an image,

379
00:20:50,916 --> 00:20:53,437
like this picture of a food dog I took in Japan,

380
00:20:53,437 --> 00:20:54,858
and, uh,

381
00:20:55,504 --> 00:20:59,788
a model trained on like a famous piece of art,

382
00:20:59,788 --> 00:21:02,070
like Hokusai's wave, and you put them together

383
00:21:02,070 --> 00:21:03,991
and you get sort of Hokusai's food dog.

384
00:21:05,347 --> 00:21:08,629
This is kind of a neat trick,

385
00:21:08,629 --> 00:21:10,490
and it comes out of the research we did

386
00:21:10,490 --> 00:21:12,571
into finding out how convolutional networks

387
00:21:12,571 --> 00:21:13,652
actually do their work.

388
00:21:13,652 --> 00:21:18,295
So you find out that the beginning level layers

389
00:21:18,295 --> 00:21:19,956
on a network can find small features

390
00:21:19,956 --> 00:21:22,878
like lines and circles and things,

391
00:21:22,878 --> 00:21:24,559
and then as you go down, you see textures,

392
00:21:24,559 --> 00:21:25,739
and eventually it's detecting.

393
00:21:26,300 --> 00:21:30,221
are activating around groups of features like

394
00:21:30,221 --> 00:21:31,761
dog faces and things like that.

395
00:21:31,761 --> 00:21:34,522
And with that, you can do things like style transfer.

396
00:21:34,522 --> 00:21:35,382
You can do Deep Dream.

397
00:21:35,382 --> 00:21:36,843
And I'm not gonna show you pictures of Deep Dream

398
00:21:36,843 --> 00:21:37,763
because they're really trippy,

399
00:21:37,763 --> 00:21:39,023
but you can look it up on the internet.

400
00:21:39,023 --> 00:21:42,184
That's where you imagine dogs on everything.

401
00:21:42,184 --> 00:21:44,525
There's actually a really great paper

402
00:21:44,525 --> 00:21:46,726
we published recently on distil.pub

403
00:21:46,726 --> 00:21:48,626
about how to understand what's going on

404
00:21:48,626 --> 00:21:50,487
inside convolutional networks.

405
00:21:50,487 --> 00:21:51,667
It's totally fun.

406
00:21:51,667 --> 00:21:52,527
I recommend checking it out.

407
00:21:53,833 --> 00:21:56,114
Style transfer is the kind of thing that you can actually do

408
00:21:56,114 --> 00:21:56,775
with a good GPU.

409
00:21:56,775 --> 00:21:57,695
Frame rate.

410
00:21:57,695 --> 00:22:00,496
In this example, we're actually blending between

411
00:22:00,496 --> 00:22:03,617
multiple kinds of styles at the same time.

412
00:22:03,617 --> 00:22:06,798
And you can imagine, as a game developer, using style

413
00:22:06,798 --> 00:22:10,419
transfer to bring up that emotional punch that you need

414
00:22:10,419 --> 00:22:14,201
in your game by beginning to add one of these effects based

415
00:22:14,201 --> 00:22:16,882
on art that maybe your artist drew to get this.

416
00:22:16,882 --> 00:22:18,062
So we're blending between like.

417
00:22:18,542 --> 00:22:19,763
pastelly and wooden.

418
00:22:19,763 --> 00:22:22,583
I mean, think about the ways you can do this.

419
00:22:22,583 --> 00:22:23,684
It's pretty neat.

420
00:22:23,684 --> 00:22:26,604
And of course, there's other image transformations like

421
00:22:26,604 --> 00:22:30,305
CycleGAN, where, and this is from the Berkeley AI Research.

422
00:22:30,305 --> 00:22:32,446
You can go try it out.

423
00:22:32,446 --> 00:22:36,487
And here you can do transformations between things

424
00:22:36,487 --> 00:22:39,308
like horses to zebras and apply other kinds of style.

425
00:22:39,308 --> 00:22:41,648
The neat thing about this is it doesn't need paired images

426
00:22:41,648 --> 00:22:42,008
to do it.

427
00:22:42,609 --> 00:22:48,532
So, we had a student at the Jeju ML camp in Korea that we did last year.

428
00:22:48,532 --> 00:22:50,873
She was doing image simplification.

429
00:22:50,873 --> 00:22:56,276
So what she was really doing is converting pencils to inks without actually having an

430
00:22:56,276 --> 00:22:58,418
entire corpus of inks to train on.

431
00:22:58,418 --> 00:23:03,741
She just had some inked things and some pencils and then she was able, using a variant of

432
00:23:03,741 --> 00:23:04,861
CycleGAN, to train on.

433
00:23:05,482 --> 00:23:08,563
Do an okay job of kind of inking in this picture.

434
00:23:08,563 --> 00:23:10,484
It's not a thing that your artist would probably

435
00:23:10,484 --> 00:23:13,385
be completely happy with, but it's something you can do

436
00:23:13,385 --> 00:23:14,966
in a hurry if you want to try something out.

437
00:23:14,966 --> 00:23:17,747
So let's talk about generators,

438
00:23:17,747 --> 00:23:18,967
speaking of generated images.

439
00:23:21,106 --> 00:23:22,246
Text generators are fun.

440
00:23:22,246 --> 00:23:24,688
They're just an example of the kinds of things

441
00:23:24,688 --> 00:23:25,788
you can generate.

442
00:23:25,788 --> 00:23:29,150
RNN generated text, it's been around for a while,

443
00:23:29,150 --> 00:23:30,170
it's kind of fun.

444
00:23:30,170 --> 00:23:32,071
You predict sequences of text.

445
00:23:32,071 --> 00:23:34,492
Andrej Karpathy has a just wonderful blog post

446
00:23:34,492 --> 00:23:35,793
about the unreasonable effectiveness

447
00:23:35,793 --> 00:23:37,073
of recurrent neural networks.

448
00:23:37,073 --> 00:23:41,495
There's a guy, some folks got together,

449
00:23:41,495 --> 00:23:42,696
it's called RoboRosewater,

450
00:23:42,696 --> 00:23:45,437
and they began, they took all of the cards

451
00:23:45,437 --> 00:23:47,658
in Magic the Gathering and poured them into an RNN

452
00:23:47,658 --> 00:23:49,279
and then began predicting.

453
00:23:50,495 --> 00:23:56,619
more cards, and on a per-character basis, not on a per-word basis.

454
00:23:56,619 --> 00:24:00,102
So sometimes it's sort of nonsense, but as you train the network, it becomes more and

455
00:24:00,102 --> 00:24:05,146
more plausible, and with the thousands upon thousands of Magic the Gathering cards, you

456
00:24:05,146 --> 00:24:06,487
eventually get into kind of fun stuff.

457
00:24:07,089 --> 00:24:11,672
They actually made an entire draft deck out of RNN generated things.

458
00:24:11,672 --> 00:24:15,014
Only about 40% of the cards are playable, according to this blog post,

459
00:24:15,014 --> 00:24:18,916
but nonetheless, everybody's really excited to try out these things.

460
00:24:18,916 --> 00:24:20,237
And I actually, just for fun, I did this...

461
00:24:20,237 --> 00:24:22,759
How many of you played Dominion?

462
00:24:22,759 --> 00:24:23,799
I did this with Dominion.

463
00:24:23,799 --> 00:24:25,440
Dominion doesn't have nearly as many cards,

464
00:24:25,440 --> 00:24:27,942
so you run the risk of overfitting pretty badly.

465
00:24:27,942 --> 00:24:30,363
But I ended up generating this totally ludicrous knight card.

466
00:24:31,024 --> 00:24:36,006
or Dame card where you reveal the top three cards of your deck, you look through your discard pile,

467
00:24:36,006 --> 00:24:39,087
then you trash a card that's not a treasure, and then you look through your discard pile again

468
00:24:39,087 --> 00:24:43,509
because the network is just in this little like loop, and then eventually you trash a card in the trash.

469
00:24:44,647 --> 00:24:47,587
costing three to six, and then you gain one of them.

470
00:24:47,587 --> 00:24:49,187
So this is a very silly card.

471
00:24:49,187 --> 00:24:52,108
But the thing to think about is, if you have a lot of

472
00:24:52,108 --> 00:24:54,228
content, pouring it into a giant hopper and sort of

473
00:24:54,228 --> 00:24:56,409
shaking it around and having it sort of generate things

474
00:24:56,409 --> 00:24:59,029
back to you can give you perspective on ideas that you

475
00:24:59,029 --> 00:25:00,250
haven't had before.

476
00:25:00,250 --> 00:25:02,070
The fact that you can make an entire draft deck out of

477
00:25:02,070 --> 00:25:05,471
completely novel Magic the Gathering cards is pretty

478
00:25:05,471 --> 00:25:06,611
playable.

479
00:25:06,611 --> 00:25:09,711
With hard work and editing means that this is a tool.

480
00:25:09,711 --> 00:25:12,232
This is a little thing generating away.

481
00:25:12,232 --> 00:25:13,332
You as an artist are still doing.

482
00:25:13,928 --> 00:25:22,200
all kinds of work, but you have this helper that's making you efficient and creative.

483
00:25:22,200 --> 00:25:25,204
So let's talk about image transformation.

484
00:25:26,918 --> 00:25:30,421
So this is a task called super resolution.

485
00:25:30,421 --> 00:25:32,123
And it's an area of study.

486
00:25:32,123 --> 00:25:36,106
And basically, you start with the high resolution photos on

487
00:25:36,106 --> 00:25:39,428
the left, you down sample them, and then you learn the

488
00:25:39,428 --> 00:25:42,631
mapping between the down sampled, pixelated images to

489
00:25:42,631 --> 00:25:45,013
the high resolution images.

490
00:25:45,013 --> 00:25:48,135
And then at any point, you can hand a low resolution photo,

491
00:25:48,135 --> 00:25:50,577
and it will generate a high resolution photo for you.

492
00:25:50,577 --> 00:25:54,400
Which is crazy, but it totally works.

493
00:25:55,621 --> 00:25:57,162
Not all of them are perfectly convincing,

494
00:25:57,162 --> 00:26:00,483
but you can imagine handing this off to your pixel artists

495
00:26:00,483 --> 00:26:01,223
and them having fun.

496
00:26:01,223 --> 00:26:03,363
You can imagine handing it to your players

497
00:26:03,363 --> 00:26:05,664
and having them sort of poke around with some pixels

498
00:26:05,664 --> 00:26:08,265
and generating art that's in style into your game,

499
00:26:08,265 --> 00:26:09,625
things like this.

500
00:26:09,625 --> 00:26:11,706
Neural editing is a really interesting task

501
00:26:11,706 --> 00:26:14,187
and there's a lot of different ways of doing it.

502
00:26:15,121 --> 00:26:16,942
And once you have one of these models,

503
00:26:16,942 --> 00:26:20,285
you're starting to do this adversarial training stuff,

504
00:26:20,285 --> 00:26:21,486
you can start generating things.

505
00:26:21,486 --> 00:26:25,389
So this is, there's a Japanese engineer,

506
00:26:25,389 --> 00:26:27,151
started, took a giant database

507
00:26:27,151 --> 00:26:28,772
full of Japanese pop star faces,

508
00:26:28,772 --> 00:26:33,776
and then just began generating more pop star faces.

509
00:26:33,776 --> 00:26:36,038
Which, again, can be another way

510
00:26:36,038 --> 00:26:37,299
of sort of shuffling your data around

511
00:26:37,299 --> 00:26:39,121
and trying to find something new and interesting

512
00:26:39,121 --> 00:26:41,082
and give you ideas.

513
00:26:42,333 --> 00:26:44,494
And now we're getting into tasks that are a little bit

514
00:26:44,494 --> 00:26:47,297
more like what your artist might do.

515
00:26:47,297 --> 00:26:48,838
This is infill painting.

516
00:26:48,838 --> 00:26:52,501
So in infill painting, you basically give part of an image

517
00:26:52,501 --> 00:26:55,343
to a trained network, and it tries to fill in what's

518
00:26:55,343 --> 00:26:57,304
missing without any context.

519
00:26:57,304 --> 00:27:01,187
In this thing we have in the upper right-hand corner,

520
00:27:01,187 --> 00:27:03,649
that's a human trying to fill in the painting.

521
00:27:03,649 --> 00:27:06,611
Lower left-hand corner, we're using kind of an average-y loss

522
00:27:06,611 --> 00:27:10,014
on a network, and you get kind of an average-y result.

523
00:27:11,154 --> 00:27:17,658
And, you know, if there were a car in there, sometimes cars come out kind of brown because

524
00:27:17,658 --> 00:27:19,980
that's the average color of a car.

525
00:27:19,980 --> 00:27:22,501
Very few cars are brown, but if you average them all together.

526
00:27:22,501 --> 00:27:26,243
But if you use adversarial loss, where you have another piece of your network that's

527
00:27:26,243 --> 00:27:29,625
saying, this picture is not true.

528
00:27:30,746 --> 00:27:33,288
It makes that, you know, this is not what cars look like.

529
00:27:33,288 --> 00:27:36,871
You eventually end up with these kind of much more crisp things.

530
00:27:36,871 --> 00:27:40,034
And again, this is, this is, you know, an artist can totally do this work,

531
00:27:40,034 --> 00:27:43,457
but this might be a way for an artist to be faster or more efficient or get ideas.

532
00:27:43,457 --> 00:27:43,557
OK.

533
00:27:43,557 --> 00:27:47,701
Now let's talk about latent spaces.

534
00:27:49,968 --> 00:27:53,710
And here I'm going to be talking about Project Magenta,

535
00:27:53,710 --> 00:27:54,431
most of their work.

536
00:27:54,431 --> 00:27:57,473
Project Magenta is a team at Google

537
00:27:57,473 --> 00:28:00,254
that works on the intersection

538
00:28:00,254 --> 00:28:02,156
between art and machine learning,

539
00:28:02,156 --> 00:28:03,697
and I just really love their stuff.

540
00:28:03,697 --> 00:28:08,399
Variational autoencoders.

541
00:28:08,399 --> 00:28:10,401
What are these?

542
00:28:10,401 --> 00:28:12,442
Well, you take an image and you encode it

543
00:28:12,442 --> 00:28:14,423
into some latent vector, z.

544
00:28:15,216 --> 00:28:20,320
And then you train another decoder that takes this sort of compressed format,

545
00:28:20,320 --> 00:28:22,062
this just a series of floating point numbers,

546
00:28:22,062 --> 00:28:24,684
and you expand it out to make the image back again.

547
00:28:24,684 --> 00:28:29,268
And what you're trying to do is reduce the loss between the picture on the left

548
00:28:29,268 --> 00:28:30,689
and the picture on the right.

549
00:28:30,689 --> 00:28:34,652
And you also have another kind of loss in the middle, in the latent vector,

550
00:28:34,652 --> 00:28:39,236
where you're trying to make the latent vector of these variables

551
00:28:39,236 --> 00:28:41,217
contain the most information they can.

552
00:28:41,217 --> 00:28:44,580
It's complicated to explain, but that's basically what you're up to.

553
00:28:45,040 --> 00:28:53,127
And when you're done, you end up with a vector space that can describe a lot of things, and

554
00:28:53,127 --> 00:28:56,550
a whole bunch of things that the network has never seen before.

555
00:28:56,550 --> 00:29:04,356
And with the special loss, you end up with a smooth space of describable things that

556
00:29:04,356 --> 00:29:08,479
doesn't have a lot of holes in it, like a lot of weird jumps or nonsense in it.

557
00:29:09,972 --> 00:29:11,914
So you want to train on a big data set.

558
00:29:11,914 --> 00:29:14,036
And we wanted to work on sketch data.

559
00:29:14,036 --> 00:29:17,238
So we asked our friends at the Google Creative Lab to make a

560
00:29:17,238 --> 00:29:20,081
game, and in that game, we asked people randomly on the

561
00:29:20,081 --> 00:29:22,002
internet to draw objects that they.

562
00:29:22,805 --> 00:29:26,386
know about, like draw a bear, and then the AI would try to guess whether or not they'd

563
00:29:26,386 --> 00:29:27,567
drawn a bear.

564
00:29:27,567 --> 00:29:34,450
And it was a fun game, but it also generated a huge data set of sketches of common objects.

565
00:29:34,450 --> 00:29:38,172
So going back to this picture, imagine these aren't pixels.

566
00:29:38,172 --> 00:29:40,834
Imagine these are now vector drawings.

567
00:29:40,834 --> 00:29:45,216
So now we have a vector drawing of a cat, and then we pour it into this fancy encoder,

568
00:29:45,216 --> 00:29:48,577
which is now no longer convolutional, but instead this sort of LSTM thing.

569
00:29:48,577 --> 00:29:50,858
Goes into the vector and back out again.

570
00:29:52,479 --> 00:29:55,940
And so we can actually reconstruct the drawing of the cat.

571
00:29:55,940 --> 00:29:59,101
And you can see it's kind of generalizing,

572
00:29:59,101 --> 00:30:01,061
because if you draw a sideways cat,

573
00:30:01,061 --> 00:30:02,722
and you draw human input,

574
00:30:02,722 --> 00:30:05,623
and then the output is another sideways cat,

575
00:30:05,623 --> 00:30:08,023
and if I draw a cat face with five whiskers,

576
00:30:08,023 --> 00:30:10,284
I get a cat face with six whiskers,

577
00:30:10,284 --> 00:30:12,865
because most cats have balanced whiskers in the data set.

578
00:30:12,865 --> 00:30:15,145
If I draw a three-eyed cat, I get a two-eyed cat,

579
00:30:15,145 --> 00:30:16,166
for obvious reasons.

580
00:30:16,166 --> 00:30:17,966
And if I pass in a toothbrush,

581
00:30:17,966 --> 00:30:20,247
well, the model hasn't been trained on toothbrushes,

582
00:30:20,247 --> 00:30:20,667
so I get...

583
00:30:22,346 --> 00:30:22,926
toothbrush cat.

584
00:30:22,926 --> 00:30:26,950
And we can do it with pigs as well.

585
00:30:26,950 --> 00:30:30,873
If the model trained on pigs, pig face, pig face, sideways

586
00:30:30,873 --> 00:30:35,136
pig, sideways pig, eight-legged pig, four-legged pig, and

587
00:30:35,136 --> 00:30:37,558
truck, you get truck pig.

588
00:30:39,477 --> 00:30:42,839
And because this is a vector description of this,

589
00:30:42,839 --> 00:30:44,040
and this is a giant vector space,

590
00:30:44,040 --> 00:30:46,021
you can begin to do vector math inside this space.

591
00:30:46,021 --> 00:30:49,143
So if we pick any two points in this space,

592
00:30:49,143 --> 00:30:52,324
we can interpolate linearly between these things,

593
00:30:52,324 --> 00:30:56,807
and then we can ask the decoder to generate a picture

594
00:30:56,807 --> 00:30:57,847
in each one of those things,

595
00:30:57,847 --> 00:31:00,109
even though it hasn't actually seen, necessarily,

596
00:31:00,109 --> 00:31:01,549
this particular point in space before.

597
00:31:02,170 --> 00:31:04,070
And so in this case, we're interpolating

598
00:31:04,070 --> 00:31:06,051
between a cat face and a pig.

599
00:31:06,051 --> 00:31:08,471
And you can see as we get closer and closer to pig,

600
00:31:08,471 --> 00:31:12,092
it begins taking on both the sidewaysness of the pig

601
00:31:12,092 --> 00:31:14,413
and also the little curly tail eventually pops up

602
00:31:14,413 --> 00:31:17,173
in the snout, because that's the thing that distinguishes

603
00:31:17,173 --> 00:31:19,494
the concept of a pig and a cat, really,

604
00:31:19,494 --> 00:31:21,414
when you think about it, on drawings of this scale.

605
00:31:21,414 --> 00:31:24,095
And we can do it as well, interpolating

606
00:31:24,095 --> 00:31:25,095
between cats and trucks.

607
00:31:25,095 --> 00:31:29,256
I just, the little, the cat that's just to the right

608
00:31:29,256 --> 00:31:31,236
of the middle one is like my favorite cat.

609
00:31:31,236 --> 00:31:31,957
I love that guy.

610
00:31:32,485 --> 00:31:35,407
And with a model like this as well,

611
00:31:35,407 --> 00:31:37,469
you can begin to do image completion.

612
00:31:37,469 --> 00:31:40,071
You can start drawing something and then it will try,

613
00:31:40,071 --> 00:31:42,733
like, oh, I've seen this before, let me see if I can help.

614
00:31:42,733 --> 00:31:44,474
And with a little bit of variance,

615
00:31:44,474 --> 00:31:46,355
you can draw all kinds of different kinds of,

616
00:31:46,355 --> 00:31:50,839
say, mosquitoes, if you have a model trained on mosquitoes.

617
00:31:50,839 --> 00:31:53,081
And again, thinking about the kind of work

618
00:31:53,081 --> 00:31:55,242
that you do from day to day, you may not actually want,

619
00:31:55,242 --> 00:31:57,824
you would obviously not necessarily take any of this art,

620
00:31:57,824 --> 00:32:00,726
but you might try it, just to see, try it on for size,

621
00:32:00,726 --> 00:32:01,447
see what it looks like.

622
00:32:02,117 --> 00:32:04,998
And you can do it with fonts, too.

623
00:32:04,998 --> 00:32:07,659
This is, here we're interpolating between

624
00:32:07,659 --> 00:32:11,860
the sans-serif fonts and serif fonts,

625
00:32:11,860 --> 00:32:13,921
in the upper left and the upper right.

626
00:32:13,921 --> 00:32:16,582
And we can sort of, and the results are in the lower right there,

627
00:32:16,582 --> 00:32:20,303
kind of going back and forth between serifs and boldy

628
00:32:20,303 --> 00:32:22,364
and less bold and more serify.

629
00:32:22,364 --> 00:32:24,444
And again, you can imagine trying this out.

630
00:32:24,444 --> 00:32:27,645
In fact, this is a little toy you can play with if you follow that link.

631
00:32:27,645 --> 00:32:29,166
It's totally neat.

632
00:32:31,509 --> 00:32:34,409
And lastly, with these latent spaces,

633
00:32:34,409 --> 00:32:35,950
we can also describe music.

634
00:32:35,950 --> 00:32:40,931
So here, this was just published last Thursday,

635
00:32:40,931 --> 00:32:46,273
music VAE basically has a giant space of melodies

636
00:32:46,273 --> 00:32:49,313
and then you can interpolate between any two places

637
00:32:49,313 --> 00:32:50,854
in this melody space.

638
00:32:50,854 --> 00:32:52,174
So let's take a listen.

639
00:32:52,174 --> 00:32:53,054
Whoop, that didn't work.

640
00:32:53,054 --> 00:32:56,475
Let's try actually pressing the button and take a listen.

641
00:32:59,759 --> 00:33:01,180
So this is the first melody.

642
00:33:01,180 --> 00:33:17,185
And this is where we want to go.

643
00:33:17,185 --> 00:33:28,569
Okay, let's start in triple A.

644
00:33:39,834 --> 00:33:56,210
Thanks for watching!

645
00:34:26,334 --> 00:34:32,343
So it's really cool, and it's open source.

646
00:34:32,343 --> 00:34:36,850
So we have blog posts and more ways to play with this, and

647
00:34:36,850 --> 00:34:38,913
other ways to think about it.

648
00:34:38,913 --> 00:34:41,036
So I'm running out of time, so let me wrap up.

649
00:34:42,247 --> 00:34:44,008
I know this from being a game developer myself,

650
00:34:44,008 --> 00:34:45,528
making games is hard.

651
00:34:45,528 --> 00:34:48,789
And when I think about all the things I spent time,

652
00:34:48,789 --> 00:34:51,189
I spent making games, a huge amount of it was

653
00:34:51,189 --> 00:34:52,729
trying things and throwing them away

654
00:34:52,729 --> 00:34:53,830
over and over and over again.

655
00:34:53,830 --> 00:34:56,890
Trying to find in the latent space of games

656
00:34:56,890 --> 00:34:59,131
what the fun thing actually is

657
00:34:59,131 --> 00:35:01,051
and what the beautiful thing actually is.

658
00:35:01,051 --> 00:35:02,792
And I think tools like this

659
00:35:02,792 --> 00:35:05,132
and thinking about these problems like this

660
00:35:05,132 --> 00:35:07,753
might be a really interesting way

661
00:35:07,753 --> 00:35:10,313
to become more creative and more efficient.

662
00:35:10,313 --> 00:35:11,773
Although, I will warn you,

663
00:35:12,450 --> 00:35:16,534
Machine learning can be a lot of work up front

664
00:35:16,534 --> 00:35:18,935
for good results later.

665
00:35:18,935 --> 00:35:21,017
So by no means am I promising you

666
00:35:21,017 --> 00:35:23,199
you will get things done faster,

667
00:35:23,199 --> 00:35:25,100
but you might try a lot more things.

668
00:35:25,100 --> 00:35:28,263
So going back to the very beginning,

669
00:35:28,263 --> 00:35:30,465
right now there's tools right away

670
00:35:30,465 --> 00:35:33,767
where machine learning can make your game

671
00:35:33,767 --> 00:35:37,510
more efficient or more easy to manage.

672
00:35:39,031 --> 00:35:41,673
But later on, I think, you know, and even now,

673
00:35:41,673 --> 00:35:44,075
with some of these tools that we're bringing out

674
00:35:44,075 --> 00:35:47,197
and people are building in the community,

675
00:35:47,197 --> 00:35:48,558
deep learning can help you generate, you know,

676
00:35:48,558 --> 00:35:51,681
sort of unique, interesting, compelling content.

677
00:35:51,681 --> 00:35:57,065
But we always need artists and programmers and designers.

678
00:35:57,065 --> 00:36:00,648
Because machines know what they've been taught.

679
00:36:00,648 --> 00:36:03,590
And you're the people out there being creative.

680
00:36:03,590 --> 00:36:07,073
And, you know, these things are here to help you.

681
00:36:07,463 --> 00:36:10,284
If you want to know more about any of this stuff,

682
00:36:10,284 --> 00:36:11,784
TensorFlow.org is a great place to go.

683
00:36:11,784 --> 00:36:14,805
Obviously, a lot of the research I talked about here,

684
00:36:14,805 --> 00:36:16,285
although by no means all of it,

685
00:36:16,285 --> 00:36:20,526
because we talked about other stuff, not by Google,

686
00:36:20,526 --> 00:36:22,347
research.googleblog.com,

687
00:36:22,347 --> 00:36:23,327
and Magenta has its own link there.

688
00:36:23,327 --> 00:36:25,048
And that's it. Thank you very much.

689
00:36:25,048 --> 00:36:25,208
Thanks.

690
00:36:25,208 --> 00:36:28,749
Okay.

691
00:36:28,749 --> 00:36:32,770
This is going to be a tough act to follow

692
00:36:32,770 --> 00:36:35,470
because I don't even have audio in my presentation.

693
00:36:36,290 --> 00:36:39,931
So anyway, I'm Danny Lang.

694
00:36:39,931 --> 00:36:45,013
I'm running the AI and machine learning team at Unity.

695
00:36:45,013 --> 00:36:50,454
And for those of you who don't know me,

696
00:36:50,454 --> 00:36:52,615
because this is my first time at...

697
00:36:53,883 --> 00:36:54,904
at the AI Summit here.

698
00:36:54,904 --> 00:37:02,308
I have been leading the machine learning efforts at Microsoft and Amazon and Uber.

699
00:37:02,308 --> 00:37:11,354
Both Amazon and Uber, I rolled out company-wide machine learning platforms that basically

700
00:37:11,354 --> 00:37:15,737
have been used in every corner of both companies.

701
00:37:17,474 --> 00:37:20,656
Coming to Unity, I decided to deal

702
00:37:20,656 --> 00:37:25,999
with one of the harder questions in this world,

703
00:37:25,999 --> 00:37:28,240
which is what is AI?

704
00:37:28,240 --> 00:37:34,023
And I was a bit involved in some of the Alexa work.

705
00:37:34,023 --> 00:37:37,805
People ask me all the time, is Alexa and Siri, is that AI?

706
00:37:37,805 --> 00:37:39,386
I'm like, nah, I don't think so.

707
00:37:39,386 --> 00:37:40,967
It's sort of really hardwired.

708
00:37:40,967 --> 00:37:44,449
Though there's a lot of machine learning in there.

709
00:37:45,520 --> 00:37:48,162
on the speech recognition part and the text to speech part.

710
00:37:48,162 --> 00:37:50,564
So there's a lot of small elements here and there,

711
00:37:50,564 --> 00:37:52,666
but they're not really AI, yeah?

712
00:37:52,666 --> 00:37:56,269
And then of course, there's all the recommendations work.

713
00:37:56,269 --> 00:37:58,691
I did some of that at Amazon, you know, Netflix,

714
00:37:58,691 --> 00:37:59,771
all these recommendations.

715
00:37:59,771 --> 00:38:00,572
Is that AI?

716
00:38:00,572 --> 00:38:03,594
No, it's not really AI, it's machine learning.

717
00:38:03,594 --> 00:38:06,116
But again, there's some very, very good elements in there.

718
00:38:06,116 --> 00:38:09,679
There's actually some really incredible stuff going on,

719
00:38:09,679 --> 00:38:11,660
say in the work on recommendations.

720
00:38:12,381 --> 00:38:16,087
the latent vectors that the previous speaker mentioned.

721
00:38:16,087 --> 00:38:18,972
That's really advanced stuff that goes on in there.

722
00:38:18,972 --> 00:38:20,995
Fraud detection for your credit card,

723
00:38:20,995 --> 00:38:24,019
that's another fantastic area of detecting

724
00:38:24,019 --> 00:38:25,942
strange patterns in there.

725
00:38:26,715 --> 00:38:31,017
and catch the bad guys when they tinker with your credit card.

726
00:38:31,017 --> 00:38:32,978
Yeah. Equity trading.

727
00:38:32,978 --> 00:38:35,919
A lot of machine learning there where you get in, you know,

728
00:38:35,919 --> 00:38:38,860
tens of thousands of new sources and you try to trade stock.

729
00:38:38,860 --> 00:38:42,681
Facebook feed, you know, get all the best fake news for you.

730
00:38:42,681 --> 00:38:47,083
And and then, of course, one thing that AI is really

731
00:38:47,083 --> 00:38:50,965
that is in every other job title on LinkedIn.

732
00:38:50,965 --> 00:38:54,626
I think in my title, I had sort of doubts that, you know, people are going to

733
00:38:56,092 --> 00:38:59,353
talk about me behind my back, but now everybody has it.

734
00:38:59,353 --> 00:39:01,755
So I feel like like an everyday Joe now.

735
00:39:01,755 --> 00:39:06,277
So what about real intelligence?

736
00:39:06,277 --> 00:39:06,817
Yeah.

737
00:39:06,817 --> 00:39:12,440
And if you I hate people doing this, showing definitions from the dictionary,

738
00:39:12,440 --> 00:39:18,063
but it's something about, you know, your ability to acquire and apply knowledge

739
00:39:18,063 --> 00:39:19,584
and skills. So.

740
00:39:20,500 --> 00:39:23,223
That's sort of too abstract for me too.

741
00:39:23,223 --> 00:39:25,965
So I thought a lot about it.

742
00:39:25,965 --> 00:39:31,169
And when you really think about intelligence, it's in action around us.

743
00:39:31,169 --> 00:39:38,395
It's basically the senses and processes that nature gave all living things so that they

744
00:39:38,395 --> 00:39:40,036
can run around or grow.

745
00:39:41,519 --> 00:39:43,080
by consuming energy, yeah?

746
00:39:43,080 --> 00:39:45,703
So they have to eat to survive, yeah?

747
00:39:45,703 --> 00:39:49,306
They have to grow, they have to consume energy.

748
00:39:49,306 --> 00:39:51,307
They also have to avoid getting eaten,

749
00:39:51,307 --> 00:39:54,410
or at least delay the process

750
00:39:54,410 --> 00:39:56,091
of becoming energy themselves, yeah?

751
00:39:56,091 --> 00:39:58,513
They have to multiply, we're not gonna go.

752
00:39:58,867 --> 00:40:03,828
deeply into that today. And they have to be very careful with physics, in particular,

753
00:40:03,828 --> 00:40:08,249
you know, with inertia and gravity, you know, when you climb after that apple and you fall

754
00:40:08,249 --> 00:40:14,110
down and break something and kill yourself, etc. So this kind of intelligence in action

755
00:40:14,110 --> 00:40:19,911
is really everywhere around us, yeah? It's from bacteria, which are not particularly

756
00:40:19,911 --> 00:40:25,753
intelligent, but they have some stuff built into them that make them a little smart, yeah?

757
00:40:25,753 --> 00:40:28,613
Bees are a little smarter than that, and humans, we are...

758
00:40:29,494 --> 00:40:32,296
sort of, sometimes very smart, sometimes not so smart.

759
00:40:32,296 --> 00:40:37,320
So it's really all that intelligence is made from infrastructure.

760
00:40:37,320 --> 00:40:43,005
So it's really the evolution created complex apparatus for that.

761
00:40:43,005 --> 00:40:48,649
Chemical mechanisms, cellular structure, all this stuff was created so that nature

762
00:40:49,162 --> 00:40:52,824
the version of nature we live in survives and grows.

763
00:40:52,824 --> 00:40:53,284
Yeah.

764
00:40:53,284 --> 00:40:57,906
There may be another version of nature that failed because it forgot some of these things

765
00:40:57,906 --> 00:40:58,046
here.

766
00:40:58,046 --> 00:40:58,146
Yeah.

767
00:40:58,146 --> 00:41:02,489
And, and we see that organisms, organisms have messaging systems.

768
00:41:02,489 --> 00:41:04,670
That's what neuroscience is about.

769
00:41:04,670 --> 00:41:10,773
And we had developed sensors, you know, like vision came around 450 million years ago that

770
00:41:10,773 --> 00:41:11,594
suddenly led to.

771
00:41:13,242 --> 00:41:18,024
the creatures at that time suddenly having a very different life and evolution

772
00:41:18,024 --> 00:41:19,464
took them in a very different direction.

773
00:41:19,464 --> 00:41:22,926
So if you think about the game engine,

774
00:41:22,926 --> 00:41:26,587
it's pretty close to what I talked about.

775
00:41:26,587 --> 00:41:30,468
It's a 3D environment where there's a physics engine,

776
00:41:30,468 --> 00:41:34,149
so things have inertia, there's gravity,

777
00:41:34,149 --> 00:41:40,131
and it's a controlled self-sufficient ecosystem

778
00:41:40,131 --> 00:41:41,392
that sort of mimics the real world.

779
00:41:43,443 --> 00:41:49,447
Looking at that, that's actually an awesome environment

780
00:41:49,447 --> 00:41:53,669
for really working with your private AI biodome.

781
00:41:53,669 --> 00:41:57,852
So at Unity, we launched ML Agents

782
00:41:57,852 --> 00:42:02,976
because it adds one last thing to that equation

783
00:42:02,976 --> 00:42:07,078
of the three environment, this extension,

784
00:42:07,078 --> 00:42:08,799
and that's the...

785
00:42:10,322 --> 00:42:13,044
mechanisms, the neuroscience mechanisms, yeah?

786
00:42:13,044 --> 00:42:16,626
That all those things in that game engine

787
00:42:16,626 --> 00:42:19,869
can now start exhibiting some kind of intelligence

788
00:42:19,869 --> 00:42:22,851
surviving in that 3D world with a physics engine.

789
00:42:22,851 --> 00:42:25,893
So what is Unity ML Agents?

790
00:42:25,893 --> 00:42:31,898
It's essentially, it's open source.

791
00:42:31,898 --> 00:42:32,718
It's an AI toolkit.

792
00:42:35,014 --> 00:42:40,218
It uses environments which are worlds which are unity based.

793
00:42:40,218 --> 00:42:43,099
So you can create your own biodome.

794
00:42:43,099 --> 00:42:43,460
Yeah?

795
00:42:43,460 --> 00:42:46,201
You remember the biodomes?

796
00:42:46,201 --> 00:42:48,163
You can create that in Unity

797
00:42:48,163 --> 00:42:50,664
and then you can add NPCs in there.

798
00:42:50,664 --> 00:42:51,405
You can add agents.

799
00:42:51,405 --> 00:42:55,568
And they're essentially game objects

800
00:42:55,568 --> 00:42:58,169
that are attached to an agent.

801
00:42:58,169 --> 00:43:01,311
The agent can observe.

802
00:43:01,311 --> 00:43:03,433
And that's actually the sensors, yeah?

803
00:43:03,837 --> 00:43:09,978
You can have a camera in the agent so it can see visually, or you can have some ray tracing

804
00:43:09,978 --> 00:43:19,179
going on, you can have collision, you can basically mimic everything that nature created.

805
00:43:19,179 --> 00:43:21,660
Let's see, how many sensors is it that we have?

806
00:43:21,660 --> 00:43:24,320
Some people say five.

807
00:43:24,320 --> 00:43:30,582
Let's see, that's, you know, we can taste and smell and see, feel, and hear.

808
00:43:31,010 --> 00:43:34,712
There's something about, we also feel heat, cold stuff.

809
00:43:34,712 --> 00:43:38,013
We can also feel the balance and we can feel movement.

810
00:43:38,013 --> 00:43:40,335
We actually have many, many sensors.

811
00:43:40,335 --> 00:43:43,476
All those sensors you can actually build

812
00:43:43,476 --> 00:43:44,997
into your game objects.

813
00:43:44,997 --> 00:43:45,518
Think about it.

814
00:43:45,518 --> 00:43:48,579
You can actually invent new sensors.

815
00:43:48,579 --> 00:43:50,620
We actually had a guy, he did a LIDAR.

816
00:43:50,620 --> 00:43:52,701
You know, the laser thing that robots

817
00:43:52,701 --> 00:43:56,323
and self-driving cars have.

818
00:43:56,323 --> 00:43:58,345
Boom, add that as an asset into Unity

819
00:43:58,345 --> 00:44:00,526
and now you have another sensor.

820
00:44:00,526 --> 00:44:00,566
So.

821
00:44:01,620 --> 00:44:05,282
So observations of these game objects or the agent

822
00:44:05,282 --> 00:44:07,303
is really about senses.

823
00:44:07,303 --> 00:44:11,486
They can take actions, they can move around,

824
00:44:11,486 --> 00:44:13,848
and when they accomplish something,

825
00:44:13,848 --> 00:44:18,611
they can sort of send a signal back and say success,

826
00:44:18,611 --> 00:44:19,512
or they maybe fall over and die,

827
00:44:19,512 --> 00:44:20,652
and they failed to have success.

828
00:44:20,652 --> 00:44:21,453
That's the reward signals.

829
00:44:21,453 --> 00:44:23,474
And then we use reinforcement learning.

830
00:44:24,773 --> 00:44:29,855
We talked a bit about, first speaker spoke a bit about reinforcement learning today.

831
00:44:29,855 --> 00:44:34,257
So it's essentially the idea of running a simulation,

832
00:44:34,257 --> 00:44:38,139
learning from that simulation which is basically coming up,

833
00:44:38,139 --> 00:44:42,861
let the system come up, the policy that is essential and

834
00:44:42,861 --> 00:44:48,383
always think about a policy as an optimal mapping between what you see and what you do.

835
00:44:49,896 --> 00:44:51,698
So there's always the random policy which is,

836
00:44:51,698 --> 00:44:54,741
you know, you throw a coin to make decisions, yeah?

837
00:44:54,741 --> 00:44:56,623
But that's not an optimal policy.

838
00:44:56,623 --> 00:45:01,188
So we try to let the system learn the optimal policy.

839
00:45:01,188 --> 00:45:02,850
And then when you have that policy,

840
00:45:02,850 --> 00:45:05,272
you can now move into the inference phase

841
00:45:05,272 --> 00:45:07,475
which is you can let the system play out.

842
00:45:07,475 --> 00:45:11,839
In the diagram.

843
00:45:12,749 --> 00:45:14,590
Very briefly, this is what it looks like.

844
00:45:14,590 --> 00:45:17,811
You have the learning environment, that's Unity,

845
00:45:17,811 --> 00:45:19,251
the big gray thing.

846
00:45:19,251 --> 00:45:21,532
There's a Python side to it.

847
00:45:21,532 --> 00:45:25,854
The Python part of it provides you with the mechanisms

848
00:45:25,854 --> 00:45:30,055
and the ability to hook algorithms up outside Unity.

849
00:45:30,055 --> 00:45:33,336
So we're doing all of our work on TensorFlow,

850
00:45:33,336 --> 00:45:35,397
but you can hook it up with your favorite

851
00:45:35,397 --> 00:45:36,537
machine learning system.

852
00:45:36,537 --> 00:45:40,879
But if you go to the ML Agent GitHub,

853
00:45:41,597 --> 00:45:45,737
there will be this of how to download and install

854
00:45:45,737 --> 00:45:47,658
and get TensorFlow hooked up with this.

855
00:45:47,658 --> 00:45:50,858
So you have that training taking site outside Unity.

856
00:45:50,858 --> 00:45:53,479
Then inside Unity, there's the Academy

857
00:45:53,479 --> 00:45:56,200
that embeds a communicator

858
00:45:56,200 --> 00:45:58,400
that you don't have to care much about,

859
00:45:58,400 --> 00:46:00,780
but that's the thing that sort of takes things

860
00:46:00,780 --> 00:46:03,721
from inside Unity in your Unity environment

861
00:46:03,721 --> 00:46:07,862
out to the, for training outside.

862
00:46:07,862 --> 00:46:09,042
So in the Academy.

863
00:46:09,876 --> 00:46:16,200
You have basically agents, that's game objects, you have game objects attached to agents and

864
00:46:16,200 --> 00:46:20,723
they can do stuff, as I said, observe and have actions and send rewards.

865
00:46:20,723 --> 00:46:23,664
And there are one or more brains.

866
00:46:23,664 --> 00:46:29,608
The brains is sort of what you try to build here and that's what I call, that's the new

867
00:46:29,608 --> 00:46:31,549
science part of our biodome.

868
00:46:35,977 --> 00:46:36,878
training scenarios.

869
00:46:36,878 --> 00:46:39,442
So we already heard about some of them.

870
00:46:39,442 --> 00:46:44,230
But this is actually why I'm so excited

871
00:46:44,230 --> 00:46:46,474
and why we are doing ML Agents because

872
00:46:47,048 --> 00:46:51,789
by making, basically democratizing AI so that everybody,

873
00:46:51,789 --> 00:46:54,310
not just the big corporations can play with this.

874
00:46:54,310 --> 00:46:56,610
Yeah, it's really bringing it out.

875
00:46:56,610 --> 00:46:58,611
And what I hope you take home from this

876
00:46:58,611 --> 00:47:00,911
is that there's so much to be done.

877
00:47:00,911 --> 00:47:02,631
We're just scratching the surface.

878
00:47:02,631 --> 00:47:05,972
And what we're trying to do with the ML agents at Unity

879
00:47:05,972 --> 00:47:09,933
is to open the door for you to take this and run with it.

880
00:47:09,933 --> 00:47:11,773
So I'm gonna give you some examples.

881
00:47:11,773 --> 00:47:14,293
And this is, you can just imagine how.

882
00:47:16,223 --> 00:47:18,085
how many opportunities there are here

883
00:47:18,085 --> 00:47:19,226
when I go through this.

884
00:47:19,226 --> 00:47:21,428
It's like as an example, you have a single agent

885
00:47:21,428 --> 00:47:23,890
with a single brain doing something, yeah?

886
00:47:23,890 --> 00:47:26,652
It's a very, very simple combination

887
00:47:26,652 --> 00:47:30,435
that can be a game object trying to get from A to B

888
00:47:30,435 --> 00:47:31,556
or learn to get from A to B

889
00:47:31,556 --> 00:47:33,878
with some obstacles in between, yeah?

890
00:47:33,878 --> 00:47:37,641
Single agent, single brain, fairly straightforward.

891
00:47:37,641 --> 00:47:42,305
But then you can also have examples of multiple agents.

892
00:47:43,661 --> 00:47:45,461
with a single brain.

893
00:47:45,461 --> 00:47:48,002
And what that does is that it sort of

894
00:47:48,002 --> 00:47:51,363
gives you an opportunity to have a lot of action going on,

895
00:47:51,363 --> 00:47:53,464
a lot of reward signals coming back in,

896
00:47:53,464 --> 00:47:56,825
but you accumulate all that knowledge in a single brain.

897
00:47:56,825 --> 00:48:00,145
There's also adversarial self-play.

898
00:48:00,145 --> 00:48:01,766
You have two agents.

899
00:48:02,798 --> 00:48:05,080
have a single brain, single set of skills,

900
00:48:05,080 --> 00:48:10,062
but for each agent, the reward signals are inverse.

901
00:48:10,062 --> 00:48:13,144
I try to beat this guy, and this guy tries to beat me,

902
00:48:13,144 --> 00:48:15,846
and we feed all into a single brain.

903
00:48:15,846 --> 00:48:19,428
So we get two guys who are actually really, really good

904
00:48:19,428 --> 00:48:20,688
at playing against each other,

905
00:48:20,688 --> 00:48:22,930
and they make each other better and better

906
00:48:22,930 --> 00:48:25,171
because all the experience goes into the same brain.

907
00:48:25,700 --> 00:48:27,601
So you can see a lot of this computer science stuff,

908
00:48:27,601 --> 00:48:31,784
it really sounds more like what we do every day together

909
00:48:31,784 --> 00:48:34,746
by learning and exchanging information and playing games.

910
00:48:34,746 --> 00:48:36,627
And I'm not even computer games,

911
00:48:36,627 --> 00:48:39,409
but maybe even soccer games or things like that

912
00:48:39,409 --> 00:48:42,211
where we do things together.

913
00:48:42,211 --> 00:48:44,212
So brings me on to cooperative

914
00:48:44,212 --> 00:48:47,695
and cooperative multi-agent scenarios

915
00:48:47,695 --> 00:48:50,617
where we have multiple agents, multiple brains.

916
00:48:50,617 --> 00:48:53,518
They may have shared or inverse rewards.

917
00:48:55,799 --> 00:49:03,669
So hopefully you can understand that the opportunities here of working and

918
00:49:03,669 --> 00:49:13,382
combining these brains and agents and the rewards are basically endless.

919
00:49:13,382 --> 00:49:13,802
I'm gonna tell you.

920
00:49:14,051 --> 00:49:20,696
So the training environments, that's basically the setting

921
00:49:20,696 --> 00:49:23,798
in which your game objects are doing things.

922
00:49:23,798 --> 00:49:28,581
And what we've been doing is that we have provided you

923
00:49:28,581 --> 00:49:32,023
with a bunch of pre-created training environments

924
00:49:32,023 --> 00:49:34,925
that you can just grab and start playing with,

925
00:49:34,925 --> 00:49:36,286
which means you can actually go home

926
00:49:36,286 --> 00:49:38,548
and do all these things I show you right away.

927
00:49:38,548 --> 00:49:42,330
But you can also take these training environments

928
00:49:42,330 --> 00:49:43,131
and change them.

929
00:49:44,581 --> 00:49:47,563
or you can clone them and then change them like crazy.

930
00:49:47,563 --> 00:49:48,864
There's no limits here.

931
00:49:48,864 --> 00:49:51,946
So a lot of the work going on in reinforcement learning

932
00:49:51,946 --> 00:49:53,587
and other machine learning and AI areas

933
00:49:53,587 --> 00:49:56,708
ends up being a lot of papers that you read

934
00:49:56,708 --> 00:50:00,011
and then you're kind of, oh, I wish I could try this.

935
00:50:00,011 --> 00:50:02,572
This is what we're giving you here,

936
00:50:02,572 --> 00:50:05,474
is this ability to actually stop

937
00:50:05,474 --> 00:50:06,895
maybe even writing all those papers

938
00:50:06,895 --> 00:50:10,837
and actually do some of these things and share them.

939
00:50:10,837 --> 00:50:13,759
I'm gonna show you a few examples.

940
00:50:15,537 --> 00:50:17,458
Here's the 3D balancing ball.

941
00:50:17,458 --> 00:50:21,039
It's one brain, but it's 12 agents.

942
00:50:21,039 --> 00:50:25,881
And I'm gonna show you the video here.

943
00:50:25,881 --> 00:50:30,022
So it's about balancing a ball on a pad.

944
00:50:30,022 --> 00:50:31,823
And it's 12 agents.

945
00:50:31,823 --> 00:50:35,704
And the observation for these 12 agents

946
00:50:35,704 --> 00:50:41,806
is actually the location and velocity of the ball.

947
00:50:41,806 --> 00:50:43,287
So where's the ball rolling?

948
00:50:43,287 --> 00:50:44,507
Based on that input,

949
00:50:45,560 --> 00:50:51,263
When we run this with 12 agents, they learn from scratch,

950
00:50:51,263 --> 00:50:54,545
what we call tabula rasa, from a clean slate.

951
00:50:54,545 --> 00:50:59,227
They learn to balance that ball in 30 seconds.

952
00:50:59,227 --> 00:51:04,090
In this case, by adding 12 agents in there,

953
00:51:04,090 --> 00:51:07,412
it's a way of just generating more data.

954
00:51:08,753 --> 00:51:11,776
And I just said something very important there, because what we use the Unity

955
00:51:11,776 --> 00:51:13,978
engine to here is to generate the data.

956
00:51:13,978 --> 00:51:17,722
We need tons of data to train these systems.

957
00:51:17,722 --> 00:51:19,623
I'll show you another one here.

958
00:51:19,623 --> 00:51:23,006
It's basically two brains.

959
00:51:23,006 --> 00:51:28,612
It's two person talk, or four people, two on each team playing soccer.

960
00:51:28,612 --> 00:51:29,893
So it's really two brains.

961
00:51:30,688 --> 00:51:32,288
and four agents, so before I run it,

962
00:51:32,288 --> 00:51:36,770
I just have to explain that there's a goalie brain

963
00:51:36,770 --> 00:51:38,331
and a striker brain, yeah?

964
00:51:38,331 --> 00:51:41,692
The two goalies, they both share brain

965
00:51:41,692 --> 00:51:42,793
because their job is the same,

966
00:51:42,793 --> 00:51:44,934
and the two strikers share brain

967
00:51:44,934 --> 00:51:46,554
because their job is the same, yeah?

968
00:51:46,554 --> 00:51:48,215
But they're on two different teams,

969
00:51:48,215 --> 00:51:50,096
so the rewards are inversed

970
00:51:50,096 --> 00:51:52,077
because they're trying to beat the other guys, yeah?

971
00:51:52,077 --> 00:51:53,237
So you see,

972
00:51:59,952 --> 00:52:02,713
In this case, the observation is actor Ray Karst.

973
00:52:02,713 --> 00:52:05,754
So you will see the little square agents.

974
00:52:05,754 --> 00:52:07,635
I apologize for the graphics here,

975
00:52:07,635 --> 00:52:09,535
but it's sort of to keep things simple.

976
00:52:09,535 --> 00:52:11,216
Has a little eye, yeah?

977
00:52:11,216 --> 00:52:15,537
They are one-eyed.

978
00:52:15,537 --> 00:52:18,918
And they look at the strikers, by the way.

979
00:52:18,918 --> 00:52:19,759
Have you noticed the strikers

980
00:52:19,759 --> 00:52:20,539
are looking sideways all the time?

981
00:52:20,539 --> 00:52:23,440
Yeah, but the goalies are looking forward.

982
00:52:23,440 --> 00:52:25,660
Because the goalies need to look

983
00:52:25,660 --> 00:52:27,041
at the ball coming against them.

984
00:52:27,500 --> 00:52:32,042
But actually when you're a striker with a 180 degrees view,

985
00:52:32,042 --> 00:52:35,224
point of view, then if you're on the side looking

986
00:52:35,224 --> 00:52:38,726
at the field from the side, you see the most.

987
00:52:38,726 --> 00:52:42,608
If we would narrow in that view of field,

988
00:52:42,608 --> 00:52:45,290
they would probably start turning a lot more around

989
00:52:45,290 --> 00:52:46,310
to scan the space.

990
00:52:46,310 --> 00:52:50,333
So that's one thing that's important to remember here

991
00:52:50,333 --> 00:52:53,294
is that you have opportunity to

992
00:52:55,023 --> 00:52:59,424
to basically put a camera on the agent

993
00:52:59,424 --> 00:53:01,645
or put a camera over the entire world.

994
00:53:01,645 --> 00:53:04,065
So in this case in soccer,

995
00:53:04,065 --> 00:53:08,106
here the agents actually don't see everything.

996
00:53:08,106 --> 00:53:10,067
They only see their point of view.

997
00:53:10,067 --> 00:53:11,687
They don't see the entire world.

998
00:53:11,687 --> 00:53:14,128
You could in other cases have cameras

999
00:53:14,128 --> 00:53:15,869
that show the entire world.

1000
00:53:15,869 --> 00:53:18,669
Let's do some, oh that was a joke

1001
00:53:18,669 --> 00:53:20,110
that was not supposed to get in here.

1002
00:53:21,204 --> 00:53:32,031
But it's wall jumping where we basically give an agent two brains because it needs to do

1003
00:53:32,031 --> 00:53:33,312
two different things, yeah?

1004
00:53:33,312 --> 00:53:39,876
It needs to learn to push another block next to the wall so it can step up on the block

1005
00:53:39,876 --> 00:53:41,757
and get over the wall, yeah?

1006
00:53:43,167 --> 00:53:44,908
So look there, it's cheating.

1007
00:53:44,908 --> 00:53:47,931
It could jump on the low wall, yeah.

1008
00:53:47,931 --> 00:53:52,594
But in this case, it has to basically learn two skills.

1009
00:53:52,594 --> 00:53:53,735
So one is pushing the block,

1010
00:53:53,735 --> 00:53:57,298
and I wanna tell you how it learns that,

1011
00:53:57,298 --> 00:54:01,361
because this is, again, so fascinating, yeah?

1012
00:54:01,361 --> 00:54:04,343
Because it learns that through curriculum learning.

1013
00:54:04,343 --> 00:54:07,825
So what we do, and we support this directly in ML Agents,

1014
00:54:07,825 --> 00:54:11,648
we basically give you a chance to teach the agent

1015
00:54:11,648 --> 00:54:12,689
the simplest case.

1016
00:54:13,015 --> 00:54:16,838
where the wall is actually just tiny, tiny, tiny.

1017
00:54:16,838 --> 00:54:20,361
And then we make the wall taller and taller

1018
00:54:20,361 --> 00:54:25,185
as the agent reach some objective,

1019
00:54:25,185 --> 00:54:27,927
some threshold with the low wall,

1020
00:54:27,927 --> 00:54:29,889
then make the wall higher, yeah?

1021
00:54:29,889 --> 00:54:32,711
And what's so fascinating is that

1022
00:54:32,711 --> 00:54:37,135
you get a better model out of curriculum learning

1023
00:54:37,135 --> 00:54:39,777
rather than just let the agent try out

1024
00:54:39,777 --> 00:54:41,838
to scale the tall.

1025
00:54:42,382 --> 00:54:43,282
wall right away.

1026
00:54:43,282 --> 00:54:51,303
Actually, well, you can see the orange one is its ability to scale the tall wall, just

1027
00:54:51,303 --> 00:54:53,303
never seen the low wall, yeah?

1028
00:54:53,303 --> 00:54:58,104
So it's a bit like when you go to school and go to first grade and second grade, third

1029
00:54:58,104 --> 00:54:59,925
grade, you sort of learn things stepwise, yeah?

1030
00:54:59,925 --> 00:55:02,205
Instead of just jumping right into it.

1031
00:55:02,205 --> 00:55:07,186
Isn't it weird that these systems, with all their primitive technology from TensorFlow

1032
00:55:07,186 --> 00:55:11,387
or wherever it comes from, exhibit that same behavior, yeah?

1033
00:55:11,938 --> 00:55:17,782
It's the same behavior that building upon a previous learned skill.

1034
00:55:17,782 --> 00:55:22,726
We have seen other examples of that where if you want to build a robot hand that really

1035
00:55:22,726 --> 00:55:28,951
grabs stuff, you teach it a lot of primitive things it can do and then it gets much better

1036
00:55:28,951 --> 00:55:30,792
at learning the hard task.

1037
00:55:30,792 --> 00:55:30,952
So

1038
00:55:32,905 --> 00:55:37,748
That's an important message because it really tells everybody

1039
00:55:37,748 --> 00:55:40,529
that you need to sort of take this out of pure research

1040
00:55:40,529 --> 00:55:42,930
and really put it in the hands of a lot of people

1041
00:55:42,930 --> 00:55:46,652
because they would actually be able to explore

1042
00:55:46,652 --> 00:55:47,973
many more aspects of this.

1043
00:55:47,973 --> 00:55:51,875
I wanna mention another one which is essentially

1044
00:55:51,875 --> 00:55:53,275
a sweet little one using LSTM,

1045
00:55:53,275 --> 00:55:56,537
which is basically called long short-term memory.

1046
00:55:56,537 --> 00:55:59,999
It's the ability to remember something

1047
00:56:00,843 --> 00:56:01,843
and use it later on.

1048
00:56:01,843 --> 00:56:06,106
So what this little blue cube does is

1049
00:56:06,106 --> 00:56:08,147
that it goes and look at the big cube,

1050
00:56:08,147 --> 00:56:10,888
and then if the big cube is red, it goes to that side,

1051
00:56:10,888 --> 00:56:12,729
and if it's orange, goes to the other side.

1052
00:56:12,729 --> 00:56:14,730
But it has to do two things again, yeah?

1053
00:56:14,730 --> 00:56:20,253
It has to use raycast, identify the color of the first cube,

1054
00:56:20,772 --> 00:56:24,553
and then decide where to go and find its way over there

1055
00:56:24,553 --> 00:56:25,753
and end up in the right corner.

1056
00:56:25,753 --> 00:56:27,654
So again, two different things, yeah?

1057
00:56:27,654 --> 00:56:30,675
So it walks, goes to the orange side,

1058
00:56:30,675 --> 00:56:32,295
this means goes to the red side,

1059
00:56:32,295 --> 00:56:34,696
go to the orange side, looks at it,

1060
00:56:34,696 --> 00:56:37,236
and then go to the orange side, yeah?

1061
00:56:37,236 --> 00:56:43,218
So it's an example of the use of specific, of memory, yeah?

1062
00:56:43,218 --> 00:56:45,978
I think that's fascinating too.

1063
00:56:48,235 --> 00:56:52,298
With the Unity engine, we have high fidelity graphics,

1064
00:56:52,298 --> 00:56:58,323
we have physics, we have rich assets.

1065
00:56:58,323 --> 00:57:03,487
I mentioned the radar, light, stuff like that,

1066
00:57:03,487 --> 00:57:06,969
our sensors, and you can create scalable simulations

1067
00:57:06,969 --> 00:57:10,592
where in theory you can run 100,000 instances of Unity

1068
00:57:10,592 --> 00:57:11,433
and gather data.

1069
00:57:11,433 --> 00:57:12,834
And, oh, sorry, I went too fast here.

1070
00:57:12,834 --> 00:57:14,275
I just wanna show this if it runs.

1071
00:57:14,275 --> 00:57:16,657
Is it running?

1072
00:57:16,657 --> 00:57:18,138
Yeah, here we go.

1073
00:57:19,008 --> 00:57:25,678
We are working with Microsoft and the city of Bellevue.

1074
00:57:25,678 --> 00:57:30,865
And the idea here is to actually train a machine learning system

1075
00:57:30,865 --> 00:57:32,908
to detect dangerous.

1076
00:57:33,907 --> 00:57:36,369
traffic situations through cameras.

1077
00:57:36,369 --> 00:57:40,111
But the traditional way of doing this would be to record hours and

1078
00:57:40,111 --> 00:57:43,553
hours of video and have a thousand people hand labeling the video, etc.

1079
00:57:43,553 --> 00:57:46,034
To train the machine learning system.

1080
00:57:46,034 --> 00:57:54,319
And what we're doing is we're showing that you can actually create these

1081
00:57:54,319 --> 00:57:58,621
scenarios that in unity that will basically kind of fool the machine

1082
00:57:58,621 --> 00:58:00,922
learning system and think this is the real world and train it.

1083
00:58:00,922 --> 00:58:02,563
And on a very sad day like today.

1084
00:58:02,890 --> 00:58:06,857
when there was a pedestrian killed by an Uber self-driving vehicle.

1085
00:58:06,857 --> 00:58:14,368
This is kind of a way to train vehicles, train machine learning systems.

1086
00:58:17,570 --> 00:58:22,915
in an artificial environment before you actually bring them out in the real world.

1087
00:58:22,915 --> 00:58:27,939
So both for cars, for robots and also for systems that actually

1088
00:58:27,939 --> 00:58:30,661
deal with what we call real world situations.

1089
00:58:30,661 --> 00:58:33,844
It's a fantastic environment for doing that.

1090
00:58:33,844 --> 00:58:35,745
And this is sort of when you look at ML

1091
00:58:35,745 --> 00:58:37,146
agents and then you kind of put them on steroids.

1092
00:58:37,146 --> 00:58:41,310
OK, that was all I had for today and I appreciate your time.

