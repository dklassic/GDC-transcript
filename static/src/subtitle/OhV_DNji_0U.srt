1
00:00:06,059 --> 00:00:09,181
Hi everybody, I'm Tianyang Shi from Fuxi AI Lab.

2
00:00:09,181 --> 00:00:12,563
In this talk, I would like to share our new technology,

3
00:00:12,563 --> 00:00:15,125
named Face2ParameterTranslation,

4
00:00:15,125 --> 00:00:18,407
which can help players create their character

5
00:00:18,407 --> 00:00:20,448
with single photo.

6
00:00:20,448 --> 00:00:23,290
And this technology can be used in the games

7
00:00:23,290 --> 00:00:27,733
with the character customization system.

8
00:00:27,733 --> 00:00:31,916
Fuxi AI Lab was established in September 2017.

9
00:00:32,901 --> 00:00:35,122
We are the first game AI lab in China,

10
00:00:35,122 --> 00:00:40,425
and our vision is to unite games with AI.

11
00:00:40,425 --> 00:00:45,247
Now we are serving our games based on various AI abilities,

12
00:00:45,247 --> 00:00:50,690
and this talk focuses on the topic of computer vision.

13
00:00:50,690 --> 00:00:54,952
We also have a series of talks in this year,

14
00:00:54,952 --> 00:00:59,414
and we welcome you all very much.

15
00:00:59,414 --> 00:01:01,595
Back to this talk, it has three parts.

16
00:01:02,573 --> 00:01:10,438
An introduction of character customization system, the details of our algorithm, and some discussions.

17
00:01:10,438 --> 00:01:19,243
Character customization system is a common system in recent RPGs.

18
00:01:19,243 --> 00:01:25,366
It allows players to edit the profiles of their in-game characters.

19
00:01:25,366 --> 00:01:32,350
For example, in the game Justice, players not only can adjust the facial shape by controllers,

20
00:01:34,315 --> 00:01:40,957
but also can choose various hairstyles and makeups for their characters.

21
00:01:40,957 --> 00:01:44,838
However, it is very consuming to create a perfect character

22
00:01:44,838 --> 00:01:46,879
with hundreds of parameters,

23
00:01:46,879 --> 00:01:54,261
especially when players hope to put themselves into the game.

24
00:01:54,261 --> 00:01:57,342
In this talk, we focus on the facial part

25
00:01:57,342 --> 00:02:02,663
and explain how to automatically create characters for our players.

26
00:02:04,100 --> 00:02:08,101
Obviously, we need to achieve the following goals.

27
00:02:08,101 --> 00:02:11,462
Firstly, we need to generate a set of parameters

28
00:02:11,462 --> 00:02:13,843
based on the input photo.

29
00:02:13,843 --> 00:02:17,405
And then these parameters can be recognized by the game

30
00:02:17,405 --> 00:02:19,125
to create a corresponding character.

31
00:02:19,125 --> 00:02:23,267
Most importantly, this character should

32
00:02:23,267 --> 00:02:24,747
look like the input photo.

33
00:02:24,747 --> 00:02:29,869
And we also need to meet the requirement of further editing.

34
00:02:32,622 --> 00:02:37,263
Now let me explain how we design and implement the character auto-creation.

35
00:02:37,263 --> 00:02:43,646
There are two challenges in character creation.

36
00:02:43,646 --> 00:02:50,789
The first challenge is how to solve the problem that the game engine is non-differentiable.

37
00:02:50,789 --> 00:02:54,850
If the game engine is differentiable, we can use a powerful tool,

38
00:02:54,850 --> 00:03:00,092
the gradient descent method, to solve this appearance matching problem.

39
00:03:01,188 --> 00:03:07,453
On the other hand, it's a second challenge to find a suitable metric that can measure

40
00:03:07,453 --> 00:03:11,035
the similarity between the input photo and the created character.

41
00:03:11,035 --> 00:03:19,261
Generally speaking, we adopt three convolutional neural networks in our method, and you can

42
00:03:19,261 --> 00:03:22,344
see them in the pink and green trapezoids.

43
00:03:22,344 --> 00:03:30,130
In phase one, we use a generating neural network to imitate the behavior of the game engine.

44
00:03:31,219 --> 00:03:41,749
which means it can generate the same facial images based on the input parameters as what the game agent does.

45
00:03:41,749 --> 00:03:49,235
In phase 2, we use two discriminative neural networks to build our feature extractor,

46
00:03:49,235 --> 00:03:56,202
which defines the facial similarity between input photo and game character on identity level and shape level.

47
00:03:58,296 --> 00:04:04,540
Because neural networks are naturally differentiable, we can combine these three networks together

48
00:04:04,540 --> 00:04:13,004
and use the gradient descent method to iteratively optimize the facial parameters of a character

49
00:04:13,004 --> 00:04:20,829
according to the input photo. Now let me introduce how to make the game engine differentiable.

50
00:04:20,829 --> 00:04:27,293
Facial parameters are the bridge between players and characters.

51
00:04:28,195 --> 00:04:32,338
In our method, we define two kinds of parameters,

52
00:04:32,338 --> 00:04:36,501
which are continuous parameters and discrete parameters.

53
00:04:36,501 --> 00:04:39,943
The former ones are the combination

54
00:04:39,943 --> 00:04:45,587
of bone parameters, which correspond to the player

55
00:04:45,587 --> 00:04:48,149
available controllers.

56
00:04:48,149 --> 00:04:52,512
For example, the game Justice provides 208 controllers

57
00:04:52,512 --> 00:04:56,355
for players to adjust the facial shape.

58
00:04:57,315 --> 00:05:01,056
The later ones represent hairstyles and makeups.

59
00:05:01,056 --> 00:05:03,877
We only focus on some important ones

60
00:05:03,877 --> 00:05:08,198
and encode them with one-hot coding for easy processing.

61
00:05:08,198 --> 00:05:13,520
Based on the definition of facial parameters,

62
00:05:13,520 --> 00:05:17,321
we build our generative neural network named Imitator

63
00:05:17,321 --> 00:05:19,942
in order to imitate the behavior of the game engine.

64
00:05:19,942 --> 00:05:25,183
In details, we use 8 transposed convolutional layers

65
00:05:25,183 --> 00:05:26,543
in the Imitator.

66
00:05:27,051 --> 00:05:30,813
with corresponding ReLU and BN layers,

67
00:05:30,813 --> 00:05:38,717
where the input and output of this network are the facial parameters and the facial images of a game character.

68
00:05:38,717 --> 00:05:47,581
So training this neural network, we randomly generate 20,000 facial parameters

69
00:05:47,581 --> 00:05:53,984
obeying uniform distribution and render them by the game engine of Justice.

70
00:05:54,825 --> 00:05:59,907
The following picture shows a part of our training dataset.

71
00:05:59,907 --> 00:06:06,489
Furthermore, we use L1-norm to define the distance between the training images and the network outputs.

72
00:06:06,489 --> 00:06:18,832
And then we use stochastic gradient descent method with momentum to optimize the wires in our imitator.

73
00:06:18,832 --> 00:06:20,893
Based on our experiments,

74
00:06:21,798 --> 00:06:31,380
It's quite easy to train such network because the texture of game characters is much simpler than our skin.

75
00:06:31,380 --> 00:06:38,721
Here are some pairs rendered by the imitator and the game engine with the same facial parameters.

76
00:06:38,721 --> 00:06:45,922
Can you guess which column is the ground truth?

77
00:06:45,922 --> 00:06:50,523
Let me reveal the answer.

78
00:06:50,523 --> 00:06:51,443
The right one is ground truth.

79
00:06:52,595 --> 00:06:56,376
Indeed, it's quite difficult to tell the differences.

80
00:06:56,376 --> 00:07:00,038
And this means that we can imitate the game engine very

81
00:07:00,038 --> 00:07:04,620
well and replace it, even if the training data is generated

82
00:07:04,620 --> 00:07:05,000
randomly.

83
00:07:05,000 --> 00:07:10,563
After we can generate differentiable game

84
00:07:10,563 --> 00:07:13,044
characters, the next problem is how

85
00:07:13,044 --> 00:07:16,346
to define the face similarity between game

86
00:07:16,346 --> 00:07:20,007
character and the human face.

87
00:07:20,007 --> 00:07:20,828
Yeah, we're fish.

88
00:07:21,153 --> 00:07:27,036
In our feature extractor, we use two discriminative neural networks to model human perception.

89
00:07:27,036 --> 00:07:34,119
The face recognition network is used to judge whether the persons in two images are the

90
00:07:34,119 --> 00:07:41,602
same one or not, and the face segmentation network is used to compare the detailed facial

91
00:07:41,602 --> 00:07:43,422
texture in two images.

92
00:07:43,422 --> 00:07:50,245
The face recognition network takes in the facial images and can generate...

93
00:07:50,692 --> 00:07:56,274
facial embeddings to represent identity information.

94
00:07:56,274 --> 00:08:00,735
Apparently, photos from the same person can always share similar embeddings.

95
00:08:00,735 --> 00:08:09,979
In our method, we adopt a famous pre-trained neural network, LACN, to extract global face

96
00:08:09,979 --> 00:08:16,601
information, which is identity information, for input photo and generated character.

97
00:08:16,601 --> 00:08:19,182
This network can take in

98
00:08:19,377 --> 00:08:28,306
128 plus 128 gray images and generate 256-dimensional facial embeddings with good speed and accuracy.

99
00:08:28,306 --> 00:08:38,677
Based on this model, we can define the identity loss function by computing the cosine distance

100
00:08:38,677 --> 00:08:41,900
between facial embeddings.

101
00:08:42,670 --> 00:08:52,753
Obviously, this loss function will achieve a small value when we generate a character that looks like the input photo.

102
00:08:52,753 --> 00:09:05,597
Generally speaking, game characters and real faces have different distributions, which may affect the performance of the face recognition network.

103
00:09:05,597 --> 00:09:09,898
Because this network is trained on the real face.

104
00:09:11,307 --> 00:09:19,532
So we further adopt a face semantic segmentation network in the extractor to solve such domain gap problem,

105
00:09:19,532 --> 00:09:25,555
because the local shape information like edges is domain irrelevant.

106
00:09:25,555 --> 00:09:31,819
Detailedly, we adopt ResNet-50 as backbone to design our segmentation network.

107
00:09:31,819 --> 00:09:37,062
We replace the last fully connected layer by 1 plus 1 convolution layer

108
00:09:37,062 --> 00:09:40,344
and increase the resolution of the segmentation network.

109
00:09:40,933 --> 00:09:52,963
this network from 132 to 18. As a fully convolutional network, it can generate a 2D semantic segmentation

110
00:09:52,963 --> 00:10:01,269
map. Each pixel in this map represents a class of human face, such as nose, eyes, and mouth,

111
00:10:01,269 --> 00:10:08,154
and its low-level features contain local details of the input image.

112
00:10:09,739 --> 00:10:18,466
We collect thousands of human photos with fine labels, which has 11 classes including eyes, nose, mouth, and so on.

113
00:10:18,466 --> 00:10:28,074
We also use data augmentation technique to further enhance the performance of the semantic segmentation network.

114
00:10:28,074 --> 00:10:35,100
We use normal piecewise cross entropy loss in the training procedure.

115
00:10:35,641 --> 00:10:42,684
and it can converge well as shown in the following figure.

116
00:10:42,684 --> 00:10:48,646
Because we expect that the created character share similar local information

117
00:10:48,646 --> 00:11:00,231
with input photo, we use pixelwise L1 norm to achieve this goal when we define the content loss.

118
00:11:00,231 --> 00:11:02,332
Considering that

119
00:11:02,857 --> 00:11:07,120
Five facial features are more important.

120
00:11:07,120 --> 00:11:13,483
We further use semantic probability maps to weight these parts to improve the shape similarity

121
00:11:13,483 --> 00:11:17,966
of the facial features.

122
00:11:17,966 --> 00:11:24,930
After we introduce two kinds of loss functions representing identity and shape information

123
00:11:24,930 --> 00:11:26,951
respectively.

124
00:11:26,951 --> 00:11:29,933
We combine these loss functions to construct an

125
00:11:30,546 --> 00:11:35,188
optimization problem related to the facial parameters.

126
00:11:35,188 --> 00:11:46,734
And then we can use gradient descent method to solve this problem and optimize the facial parameters.

127
00:11:46,734 --> 00:11:50,516
Since each submodule in our method is differentiable,

128
00:11:50,516 --> 00:11:52,998
we can easily use the chain rule to calculate

129
00:11:53,803 --> 00:12:00,585
derivative of the loss function Ls with respect to the facial parameters x and update x accordingly.

130
00:12:00,585 --> 00:12:14,071
Finally, the updated parameters can be read by the game and the corresponding character will be created.

131
00:12:14,071 --> 00:12:22,354
Geometrically, the imitator Gx is a low-dimensional manifold in the high-dimensional image space.

132
00:12:23,105 --> 00:12:31,811
so the gradient descent procedure of solving the facial parameters can be viewed as searching

133
00:12:31,811 --> 00:12:40,256
for the nearest point to the input image measured by defined facial similarity.

134
00:12:40,256 --> 00:12:45,859
In summary, we create a character through three steps.

135
00:12:45,859 --> 00:12:52,623
Firstly, we align the user uploaded photo based on the default game face.

136
00:12:53,109 --> 00:12:58,212
which will be helpful for comparing the features pixel by pixel.

137
00:12:58,212 --> 00:13:06,578
Secondly, we phrase all three networks in our method and solve the optimization problem that

138
00:13:06,578 --> 00:13:13,843
we defined previously. This will maximize the facial similarity between the input

139
00:13:13,843 --> 00:13:15,824
photo and the created character.

140
00:13:17,625 --> 00:13:23,127
The iterative process can be seen in the GIF on the right.

141
00:13:23,127 --> 00:13:31,029
Finally, we can write the optimized parameters in the game,

142
00:13:31,029 --> 00:13:37,210
and then we get a character that looks like the input photo.

143
00:13:37,210 --> 00:13:44,312
Most importantly, our method retains the original interactivity of the game.

144
00:13:44,969 --> 00:13:50,832
since it can generate the facial parameters that are open to players.

145
00:13:50,832 --> 00:13:54,955
Players now can further edit the characters we provided

146
00:13:54,955 --> 00:14:00,838
and even add darker makeup on them.

147
00:14:00,838 --> 00:14:06,742
In practical, some described variables such as hairstyle and eyebrows

148
00:14:06,742 --> 00:14:11,444
have a great impact on characters,

149
00:14:11,444 --> 00:14:14,066
but they are difficult to optimize directly.

150
00:14:14,442 --> 00:14:16,804
due to the one-hot coding.

151
00:14:16,804 --> 00:14:22,529
To solve this problem, we consider one-hot coding as a probability vector and use softmax function

152
00:14:22,529 --> 00:14:29,674
to convert an extra unnormalized vector xd into the probability vector x hat d

153
00:14:29,674 --> 00:14:35,319
before feeding the facial parameters into the imitator.

154
00:14:37,309 --> 00:14:45,097
Finally, we can construct a new optimization problem, which is LSHX, and it can be optimized smoothly.

155
00:14:45,097 --> 00:14:55,748
When the iteration stops and the optimized facial parameters are obtained,

156
00:14:56,590 --> 00:15:00,912
We only need to use argmax function to calculate the index

157
00:15:00,912 --> 00:15:06,535
corresponding to the maximum value in the vector of each described variable

158
00:15:06,535 --> 00:15:13,559
for game characters on the hairstyle and makeup configuration.

159
00:15:13,559 --> 00:15:16,741
This table shows the performance of our method,

160
00:15:16,741 --> 00:15:24,925
which has a short development period and high execution efficiency in the running time.

161
00:15:24,925 --> 00:15:26,346
Here we show some results.

162
00:15:27,937 --> 00:15:31,400
Our method has been verified on our game Justice.

163
00:15:31,400 --> 00:15:38,886
It can generate vivid characters based on a single photo.

164
00:15:38,886 --> 00:15:44,871
Our method is not limited to different classes

165
00:15:44,871 --> 00:15:49,795
and different genders.

166
00:15:49,795 --> 00:15:54,959
Our method can also be applied on mobile games.

167
00:15:54,959 --> 00:15:57,662
Here are some results on an upcoming game.

168
00:16:04,610 --> 00:16:08,293
Our method has three obvious advantages.

169
00:16:08,293 --> 00:16:16,220
Firstly, it does not require any labeled data since it is under a self-supervised learning pattern.

170
00:16:16,220 --> 00:16:20,884
Secondly, our proposed imitator can be easily imitated as a game engine

171
00:16:20,884 --> 00:16:24,007
regardless of the renderer type and 3D model structure.

172
00:16:24,007 --> 00:16:30,953
In the end, we found that deep learning-based features can represent human perception.

173
00:16:32,400 --> 00:16:36,661
very well on face similarity evaluation.

174
00:16:36,661 --> 00:16:41,742
Unfortunately, our method still has some shortcomings,

175
00:16:41,742 --> 00:16:45,763
such as being sensitive to pause and obstacle,

176
00:16:45,763 --> 00:16:48,524
solving described parameters not very well,

177
00:16:48,524 --> 00:16:53,886
and requiring human-like 3D model.

178
00:16:53,886 --> 00:16:57,547
Here, we show some robustness experimental results.

179
00:16:57,547 --> 00:17:00,948
Our method is robust to common image degradation.

180
00:17:01,937 --> 00:17:10,224
degradation but not robust enough to the pose because our imitator can only generate frontal

181
00:17:10,224 --> 00:17:19,252
images. Fortunately, we could also ask players to upload frontal images to meet our requirements

182
00:17:19,252 --> 00:17:24,597
as much as possible, and this will improve the created characters.

183
00:17:24,597 --> 00:17:29,661
That's all. Thanks very much for your watching.

