1
00:00:06,167 --> 00:00:08,748
Hello and welcome to Rocket League Scaling for Free-to-Play.

2
00:00:09,388 --> 00:00:12,509
My name is Matthew Sanders and I'm a Lead Online Services Engineer with Psyonix.

3
00:00:13,149 --> 00:00:16,471
I'm giving this talk because I was responsible for coordinating the backend

4
00:00:16,511 --> 00:00:19,032
preparation for transitioning Rocket League to free-to-play.

5
00:00:21,673 --> 00:00:24,754
Let's start by talking about my team and quickly covering the scope of the

6
00:00:24,794 --> 00:00:27,595
problem. What does the online services team do at

7
00:00:27,615 --> 00:00:30,957
Psyonix? We are responsible for all game related databases,

8
00:00:31,197 --> 00:00:32,818
web services, and some websites.

9
00:00:33,636 --> 00:00:36,717
This includes the software development and DevOps deployments for all of them.

10
00:00:38,098 --> 00:00:38,959
And what is free to play?

11
00:00:39,859 --> 00:00:42,880
Just like it sounds, the plan was for Rocket League to drop its price tag.

12
00:00:43,741 --> 00:00:47,022
This lower barrier to entry was expected to bring in lots of players that didn't

13
00:00:47,062 --> 00:00:48,823
want to or couldn't pay the sticker price.

14
00:00:49,964 --> 00:00:50,324
All right.

15
00:00:50,544 --> 00:00:53,125
So just scale up the backend to accommodate more players, right?

16
00:00:54,106 --> 00:00:57,848
Well, this change could ask our backend to handle an order of magnitude more traffic.

17
00:00:58,968 --> 00:01:02,710
We had gotten to know our backend very well since its original launch in 2015,

18
00:01:03,310 --> 00:01:06,571
and therefore we knew immediately that significant improvements would be needed.

19
00:01:07,611 --> 00:01:10,592
As we'll see in this talk, after nearly a year of preparation,

20
00:01:11,212 --> 00:01:13,273
we did see a huge increase in player traffic,

21
00:01:13,673 --> 00:01:16,974
but we were rather well prepared for it and had a successful free-to-play relaunch.

22
00:01:19,935 --> 00:01:21,755
To describe Rocket League's journey to free-to-play,

23
00:01:22,255 --> 00:01:23,496
we'll discuss it in three parts.

24
00:01:24,356 --> 00:01:26,077
Part one is about planning and preparation.

25
00:01:27,143 --> 00:01:30,984
We'll briefly cover setting up new partnerships along with some methods of analysis,

26
00:01:31,705 --> 00:01:34,285
and then load testing to figure out what improvements were needed the most.

27
00:01:35,786 --> 00:01:38,926
Part two is where we'll discuss our scaling improvements in more detail.

28
00:01:40,067 --> 00:01:45,148
Finally, in part three, we'll discuss the launch experience and the new normal that has developed since then.

29
00:01:47,829 --> 00:01:50,549
Let's dive right into part one, planning and preparation.

30
00:01:53,010 --> 00:01:54,190
Now, what shall we talk about?

31
00:01:54,895 --> 00:01:57,136
We went through a lot of different planning and preparation,

32
00:01:57,856 --> 00:02:01,898
and it spanned a lot of our calendar time, but here we'll cover just a few areas.

33
00:02:02,978 --> 00:02:07,700
First up, our analysis tasks mostly revolved around trying to figure out what to expect.

34
00:02:08,780 --> 00:02:12,761
We spent some time with load projections to figure out what player population levels we might see,

35
00:02:13,442 --> 00:02:17,383
and we also spent time analyzing our architecture to set up a backlog of potential issues.

36
00:02:18,503 --> 00:02:20,644
Next, our partnerships are definitely worth discussing.

37
00:02:21,397 --> 00:02:25,079
We lined up several support contracts ranging from simple ticket-based support

38
00:02:25,739 --> 00:02:29,241
to much tighter working relationships that directly helped us prepare for the launch.

39
00:02:30,982 --> 00:02:33,703
Finally, we'll talk about load testing and how it fit into this project.

40
00:02:36,645 --> 00:02:37,645
Let's start with load projections.

41
00:02:38,626 --> 00:02:41,067
How many new players will be brought in by going free to play?

42
00:02:42,228 --> 00:02:44,689
It might seem hard to divine, but there are techniques.

43
00:02:46,010 --> 00:02:49,672
One of the best ways is to find a comparable game release that's similar to yours.

44
00:02:50,603 --> 00:02:55,565
Unfortunately, Rocket League is a multi-console sort of sport sort of action game with rockets,

45
00:02:56,466 --> 00:02:58,907
so there were no incredibly close comparisons available.

46
00:03:00,167 --> 00:03:03,809
Ultimately, we settled on an increase of three to five times our current scale.

47
00:03:05,330 --> 00:03:08,151
We used the upper end of this projection as our load testing target.

48
00:03:08,691 --> 00:03:11,072
We'll cover much more on load on this load testing later.

49
00:03:14,254 --> 00:03:16,675
Next, let's talk about our architecture evaluation.

50
00:03:17,375 --> 00:03:19,196
What did we do to evaluate our architecture?

51
00:03:20,094 --> 00:03:21,455
We wrote up a thorough description

52
00:03:21,515 --> 00:03:24,295
of how each component works and called out any scalability

53
00:03:24,335 --> 00:03:25,316
risks in its design.

54
00:03:26,656 --> 00:03:28,617
Those risks went into the backlog destined

55
00:03:28,637 --> 00:03:29,977
for validation via load testing.

56
00:03:31,798 --> 00:03:33,338
As we worked on the first few, we

57
00:03:33,378 --> 00:03:35,859
worked out a template for consistency and reuse.

58
00:03:36,619 --> 00:03:39,100
This documentation actually made good reusable training

59
00:03:39,120 --> 00:03:39,500
material.

60
00:03:40,901 --> 00:03:43,041
We also added a peer review step for each write up.

61
00:03:43,962 --> 00:03:46,342
We have peer reviews for most steps of our SDLC,

62
00:03:46,522 --> 00:03:49,463
including documentation, so this was no exception to that.

63
00:03:50,719 --> 00:03:53,380
We also wrote up an audit of our source control repositories

64
00:03:53,560 --> 00:03:55,961
to help make sure we didn't miss evaluating any of our products.

65
00:03:57,142 --> 00:03:59,063
These evaluations were pretty straightforward,

66
00:03:59,303 --> 00:04:01,103
but the whole exercise took a while to get through,

67
00:04:01,163 --> 00:04:03,684
since we had a few dozen components that we wanted to evaluate.

68
00:04:06,986 --> 00:04:08,346
Now let's talk about our partnerships.

69
00:04:09,207 --> 00:04:10,127
From very early on,

70
00:04:10,527 --> 00:04:13,248
we knew we wanted to line up the best possible external support.

71
00:04:14,309 --> 00:04:15,889
We don't have a huge engineering staff,

72
00:04:16,550 --> 00:04:19,211
so we wanted to augment with dedicated experts wherever possible.

73
00:04:20,585 --> 00:04:22,766
The primary intent of signing support contracts

74
00:04:23,306 --> 00:04:25,307
was to be able to summon expert specialists

75
00:04:25,407 --> 00:04:26,168
at a moment's notice.

76
00:04:27,528 --> 00:04:30,290
We quickly noticed that having specialists on call like this

77
00:04:30,390 --> 00:04:31,550
is actually pretty expensive.

78
00:04:32,611 --> 00:04:33,611
But our thought process was,

79
00:04:34,312 --> 00:04:36,253
if the expert help reduces our downtime at all,

80
00:04:36,553 --> 00:04:37,693
it would quickly pay for itself

81
00:04:37,773 --> 00:04:39,394
by restoring our revenue stream sooner.

82
00:04:40,995 --> 00:04:42,916
The first company we have to talk about is Google.

83
00:04:43,836 --> 00:04:45,717
Our backend has always run on GCP,

84
00:04:46,158 --> 00:04:47,698
so we already had a working relationship

85
00:04:47,899 --> 00:04:48,779
and a support contract.

86
00:04:50,099 --> 00:04:52,781
That said, after discussing our upcoming release with them,

87
00:04:53,421 --> 00:04:54,942
we decided to take advantage of some

88
00:04:54,962 --> 00:04:58,385
of their additional programs, such as CRE and GTLA.

89
00:05:00,366 --> 00:05:03,629
Next, we wanted to augment our in-house DBA expertise.

90
00:05:04,889 --> 00:05:07,371
Percona are the MySQL experts, so it

91
00:05:07,411 --> 00:05:09,152
made sense to sign up for their support service,

92
00:05:10,113 --> 00:05:12,295
especially because we use Percona Server, which

93
00:05:12,355 --> 00:05:14,156
is Percona's own flavor of MySQL.

94
00:05:15,657 --> 00:05:17,298
Finally, Redis Labs was a new addition.

95
00:05:18,331 --> 00:05:20,171
We were running some open source Redis clusters,

96
00:05:20,352 --> 00:05:21,612
but that has no formal support.

97
00:05:22,632 --> 00:05:23,873
We got in touch with Redis Labs

98
00:05:24,073 --> 00:05:25,753
and started a migration to Redis Enterprise.

99
00:05:26,433 --> 00:05:27,314
This got us support,

100
00:05:27,854 --> 00:05:30,134
but it also offloaded a lot of DevOps work

101
00:05:30,194 --> 00:05:32,295
by using their new managed offering in GCP.

102
00:05:33,655 --> 00:05:35,756
Bringing on these companies was like buying insurance.

103
00:05:36,436 --> 00:05:37,657
It gave us a lot of confidence

104
00:05:37,697 --> 00:05:39,357
that we'd be able to deal with scaling issues

105
00:05:39,377 --> 00:05:39,997
during the release,

106
00:05:40,938 --> 00:05:42,898
but it also presented a golden opportunity

107
00:05:42,998 --> 00:05:44,138
to leverage their expertise

108
00:05:44,198 --> 00:05:46,019
during our planning and development phases too.

109
00:05:48,892 --> 00:05:51,053
We took advantage of our support partnerships immediately

110
00:05:51,533 --> 00:05:53,394
in the form of several different types of exercises.

111
00:05:54,795 --> 00:05:56,536
One handy exercise was called the premortem.

112
00:05:56,977 --> 00:05:58,417
This is similar to a postmortem,

113
00:05:58,517 --> 00:06:00,359
but predictive instead of retrospective.

114
00:06:01,339 --> 00:06:03,020
Basically, you just get your whole team together

115
00:06:03,120 --> 00:06:05,001
and brainstorm possible causes of outages

116
00:06:05,222 --> 00:06:06,843
based on what you know about your systems.

117
00:06:07,943 --> 00:06:10,165
Score each idea with severity and likelihood,

118
00:06:10,765 --> 00:06:12,806
then multiply those scores together for a stack rank.

119
00:06:13,887 --> 00:06:14,868
This stack rank helps guide

120
00:06:14,888 --> 00:06:15,948
which problems you should focus on.

121
00:06:16,778 --> 00:06:17,979
Items near the bottom of the list

122
00:06:18,579 --> 00:06:19,800
are most likely not worth your time

123
00:06:20,020 --> 00:06:22,441
since they'll be either low severity or unlikely to occur.

124
00:06:24,622 --> 00:06:27,003
Another exercise from Google was for resource planning.

125
00:06:28,003 --> 00:06:29,604
Since we were not a new game launch,

126
00:06:30,244 --> 00:06:32,385
we already had a firm baseline of resource usage,

127
00:06:32,705 --> 00:06:34,706
including CPU cores, memory, and disk

128
00:06:35,346 --> 00:06:37,807
across all different GCP services that we were using.

129
00:06:39,428 --> 00:06:40,988
Our load projection that we mentioned earlier

130
00:06:41,369 --> 00:06:44,030
gave us a times 5 multiplier, which then yielded

131
00:06:44,070 --> 00:06:45,230
our projected resource usage.

132
00:06:46,655 --> 00:06:49,256
This resource plan was directly useful in two ways.

133
00:06:49,916 --> 00:06:52,237
The first was to get our quotas increased preemptively,

134
00:06:53,077 --> 00:06:55,498
and the second was to give the affected GCP zones

135
00:06:55,759 --> 00:06:57,920
plenty of lead time to make sure they have enough hardware

136
00:06:57,960 --> 00:06:58,960
for our increased scale.

137
00:07:00,160 --> 00:07:02,221
Hardware usage can often be ignored or forgotten

138
00:07:02,321 --> 00:07:04,662
under all the abstraction in this age of cloud services,

139
00:07:05,363 --> 00:07:07,063
but if our services are suddenly scaling up

140
00:07:07,103 --> 00:07:07,904
by a factor of five,

141
00:07:08,524 --> 00:07:09,905
it's only courteous to give this heads up.

142
00:07:11,711 --> 00:07:14,232
Besides Google, we also talked to Redis Labs quite a lot.

143
00:07:14,933 --> 00:07:16,914
We had several technical Q&A sessions

144
00:07:17,114 --> 00:07:19,235
before even deciding to commit to Redis Enterprise,

145
00:07:19,916 --> 00:07:22,157
where we learned about the specific advantages it offers.

146
00:07:23,578 --> 00:07:25,779
They would also support us during our database migrations

147
00:07:25,819 --> 00:07:26,039
later.

148
00:07:29,361 --> 00:07:30,502
This brings us to load testing.

149
00:07:31,302 --> 00:07:33,604
This was the most important feature

150
00:07:33,884 --> 00:07:36,065
of our entire release cycle, and it's not even

151
00:07:36,105 --> 00:07:37,006
a player-facing feature.

152
00:07:38,630 --> 00:07:41,191
Our original intent was just to use it as a research tool

153
00:07:41,351 --> 00:07:42,471
to figure out what needs improvement,

154
00:07:43,391 --> 00:07:44,992
but it ended up with far more uses

155
00:07:45,272 --> 00:07:46,732
and was massively underestimated.

156
00:07:48,433 --> 00:07:50,354
Writing load tests was unique

157
00:07:50,454 --> 00:07:51,654
compared with the rest of our code base.

158
00:07:52,034 --> 00:07:53,555
They're totally different from writing services,

159
00:07:53,835 --> 00:07:55,955
so we got almost no reuse from our existing code base.

160
00:07:57,236 --> 00:07:58,716
Load testing was also expensive,

161
00:07:58,856 --> 00:08:00,517
both in engineering hours to write

162
00:08:01,237 --> 00:08:02,678
and just running our load test environment.

163
00:08:03,487 --> 00:08:11,073
This environment contained our full set of services and was necessarily scaled up beyond our production environment in order to handle our times five load projection.

164
00:08:12,895 --> 00:08:20,422
Lastly, for the load tests, they were hard to get right because it took both research and iteration to get the client requests simulated realistically.

165
00:08:21,983 --> 00:08:26,867
We wanted to get the request patterns to match which involved calling them in the right sequence and with the right timing.

166
00:08:27,989 --> 00:08:30,011
And we also wanted the request volumes to match,

167
00:08:30,191 --> 00:08:32,413
which involved tuning the weighting of how often

168
00:08:32,673 --> 00:08:33,594
each service was called.

169
00:08:35,735 --> 00:08:37,216
Once the scope of this work became clear,

170
00:08:37,897 --> 00:08:38,818
we realized we were going to have

171
00:08:38,838 --> 00:08:39,979
to level up our load testing.

172
00:08:40,419 --> 00:08:42,461
So we made load testing one of our primary focuses

173
00:08:42,681 --> 00:08:44,422
as we continued into major feature development.

174
00:08:47,224 --> 00:08:49,706
So we continued upgrading our load test code base,

175
00:08:49,906 --> 00:08:51,468
now as full-fledged feature development.

176
00:08:52,792 --> 00:08:55,033
We already had some engineering work invested in Locust,

177
00:08:55,574 --> 00:08:57,094
and after evaluating some other frameworks,

178
00:08:57,134 --> 00:08:58,055
we stayed committed to it.

179
00:08:59,236 --> 00:09:01,077
In particular, we wanted to make sure the framework

180
00:09:01,097 --> 00:09:02,457
we used supported WebSockets,

181
00:09:02,617 --> 00:09:04,318
which eliminated many other frameworks.

182
00:09:05,599 --> 00:09:07,920
Also, it ran in Kubernetes, which is another plus for us.

183
00:09:09,321 --> 00:09:11,902
The one relative downside was that it's written in Python,

184
00:09:12,503 --> 00:09:13,883
and we have almost no other Python code

185
00:09:13,903 --> 00:09:14,664
amongst our services.

186
00:09:16,185 --> 00:09:17,185
A quick note on terminology,

187
00:09:17,385 --> 00:09:19,846
Locust scales up by hatching Locusts,

188
00:09:20,387 --> 00:09:22,188
where one Locust simulates one client.

189
00:09:24,118 --> 00:09:25,519
As we started writing and running tests,

190
00:09:25,819 --> 00:09:27,920
we felt out several different practical aspects

191
00:09:27,960 --> 00:09:28,601
of load testing.

192
00:09:29,781 --> 00:09:31,922
The first was that our tests were growing in size,

193
00:09:32,062 --> 00:09:33,523
worthy of the term new code base.

194
00:09:34,564 --> 00:09:37,345
Orchestrating this many tests required its own debugging,

195
00:09:37,505 --> 00:09:38,906
sometimes even to make sure the framework

196
00:09:38,946 --> 00:09:39,706
was working properly.

197
00:09:41,727 --> 00:09:43,508
One notable gap in our original design

198
00:09:43,588 --> 00:09:45,749
was that our locusts did not coordinate.

199
00:09:46,149 --> 00:09:48,510
They simply generated randomized data for their test runs.

200
00:09:49,951 --> 00:09:51,112
Running certain test suites

201
00:09:51,372 --> 00:09:53,333
was generating far too many validation warnings.

202
00:09:54,224 --> 00:09:57,366
This was a big problem because while the call volume might be correct,

203
00:09:58,027 --> 00:10:00,949
the work done by each service call would be completely inaccurate.

204
00:10:01,990 --> 00:10:05,493
For example, most validation warnings cause our services to return an error

205
00:10:05,513 --> 00:10:06,654
without even hitting our database.

206
00:10:08,476 --> 00:10:12,720
Our solution was to set up an independent Redis database just for coordination of test data.

207
00:10:14,001 --> 00:10:16,843
Locusts would then register themselves and some of their actions,

208
00:10:17,404 --> 00:10:19,245
similar to how players coordinate in the real world.

209
00:10:21,055 --> 00:10:28,299
This let us query for active player IDs and other sensible inputs to reduce those validation warnings and thus exercise our services more accurately.

210
00:10:30,080 --> 00:10:39,146
Having to stand up this new Redis database and write the related queries and coordination code is just a classic example of how the scope of our load tests grew quite unexpectedly.

211
00:10:42,188 --> 00:10:46,290
Another big issue we found was that running load tests had multiple heavy time costs.

212
00:10:47,326 --> 00:10:51,127
The first was that scaling up a test took much longer than we were expecting,

213
00:10:51,467 --> 00:10:54,827
proportional to the scale of the test being run. For example,

214
00:10:54,867 --> 00:10:58,928
it was common for us to run a million locusts and our stable hatch rate was a

215
00:10:58,968 --> 00:11:00,108
thousand locusts per second.

216
00:11:01,049 --> 00:11:04,409
Some quick math shows that it's going to take a thousand seconds or nearly 17

217
00:11:04,929 --> 00:11:08,170
minutes. That's a hefty commitment for one test run.

218
00:11:08,530 --> 00:11:09,690
So what should we do about it?

219
00:11:10,370 --> 00:11:12,771
We could figure out how to stabilize a higher hatch rate,

220
00:11:13,471 --> 00:11:16,211
but that's another whole engineering problem and less more development time to

221
00:11:16,231 --> 00:11:16,432
spend.

222
00:11:17,890 --> 00:11:21,552
We were content to pay this time cost because we could just multitask while waiting,

223
00:11:22,152 --> 00:11:24,153
since our locus deployment runs entirely in the cloud.

224
00:11:25,914 --> 00:11:31,256
This hatch rate was actually also realistic since it roughly matched our target player authentication rates anyway.

225
00:11:32,577 --> 00:11:36,359
Scaling up more quickly than this could actually make each test less realistic.

226
00:11:38,040 --> 00:11:40,301
The next big time cost we found was the runtime.

227
00:11:41,061 --> 00:11:45,723
Once the test was running at full scale, was it just an immediate pass fail or should we let it run for a while?

228
00:11:47,097 --> 00:11:50,020
Given the ramp up time, the latter was definitely our preference.

229
00:11:51,141 --> 00:11:55,046
Longer runs helped reveal non-obvious issues, such as resource leaks,

230
00:11:55,826 --> 00:11:59,550
and running semi-randomized tests for longer helped test more input combinations.

231
00:12:01,152 --> 00:12:05,396
Now, the scale up time and the run time combined to form one test iteration.

232
00:12:06,280 --> 00:12:09,242
As multiple iterations are run, of course, this time stacks.

233
00:12:09,982 --> 00:12:12,744
We ran many experiments using this iterative testing

234
00:12:12,784 --> 00:12:15,026
with our load tests, and they were incredibly

235
00:12:15,106 --> 00:12:16,807
useful for experimental performance tuning,

236
00:12:17,408 --> 00:12:19,409
but they paid this heavy cost in time.

237
00:12:22,291 --> 00:12:24,913
Speaking of experiments, part of each iteration

238
00:12:24,973 --> 00:12:25,974
was gathering the results.

239
00:12:27,195 --> 00:12:29,616
As I mentioned, our load tests were not just pass-fail,

240
00:12:29,676 --> 00:12:30,557
so what did we look at?

241
00:12:32,018 --> 00:12:35,020
Locust reports HTTP response codes in aggregate,

242
00:12:35,620 --> 00:12:37,381
but we also reviewed our services logs

243
00:12:37,641 --> 00:12:39,362
and looked at our Stackdriver dashboards

244
00:12:39,462 --> 00:12:41,062
with many metrics plotted over time.

245
00:12:42,203 --> 00:12:44,244
The goal was to determine what errors

246
00:12:44,264 --> 00:12:45,324
the Locusts were receiving,

247
00:12:46,085 --> 00:12:48,506
if the services had unusual warnings or errors

248
00:12:48,526 --> 00:12:51,407
in their logs, and of course, what the resource usage

249
00:12:51,447 --> 00:12:53,148
looked like for our services and databases.

250
00:12:54,989 --> 00:12:56,389
Now that we were gathering test results,

251
00:12:56,769 --> 00:12:58,970
how sure were we that the testing was accurate?

252
00:12:59,652 --> 00:13:02,213
How accurately and completely did we simulate our clients?

253
00:13:04,253 --> 00:13:06,934
We started with plenty of knowledge about the global volumes of our service

254
00:13:06,954 --> 00:13:10,234
calls, and we definitely studied the call patterns of real clients.

255
00:13:10,995 --> 00:13:14,235
But in the end, our load tests were still just a simulation of real traffic.

256
00:13:15,836 --> 00:13:19,356
The biggest intentional difference is that our real clients call hundreds

257
00:13:19,376 --> 00:13:23,197
of different services, and it was prohibited for us to simulate them all.

258
00:13:24,357 --> 00:13:27,039
Instead, the research from our architecture evaluation

259
00:13:27,500 --> 00:13:29,862
helped inform us which features had the highest scaling risk,

260
00:13:30,482 --> 00:13:32,484
so we could make sure to focus load testing on those first.

261
00:13:33,865 --> 00:13:36,407
The bottom line, our load tests were as accurate

262
00:13:36,427 --> 00:13:37,087
as we could make them.

263
00:13:37,608 --> 00:13:39,269
We designed and adjusted the best we could.

264
00:13:39,869 --> 00:13:41,551
We couldn't know anymore until the actual launch.

265
00:13:42,632 --> 00:13:44,993
Still, these tests were definitely guiding us

266
00:13:45,033 --> 00:13:45,934
to many improvements.

267
00:13:46,695 --> 00:13:49,057
We ran into probably dozens of scaling issues

268
00:13:49,497 --> 00:13:51,579
and had to fix each one to scale up higher

269
00:13:51,659 --> 00:13:52,880
to uncover the next issue.

270
00:13:53,953 --> 00:13:57,175
To do this under pressure of live traffic would have been disastrous.

271
00:13:59,957 --> 00:14:02,458
So what did the future hold for our locust load tests?

272
00:14:03,219 --> 00:14:05,800
Unlike the cicada, we couldn't just take a break for 17 years.

273
00:14:06,601 --> 00:14:09,203
Given the amount of effort it was taking to develop and run our tests,

274
00:14:10,003 --> 00:14:12,505
we resolved to continue load testing up until the release.

275
00:14:13,665 --> 00:14:15,426
It's always better to know the limits of your services,

276
00:14:15,747 --> 00:14:17,188
even if it's too late to make improvements.

277
00:14:17,928 --> 00:14:19,309
You can have mitigation ready instead.

278
00:14:20,947 --> 00:14:26,630
Looking even beyond the release, we realized that load testing can and should be more fully integrated into our SDLC.

279
00:14:27,731 --> 00:14:32,473
Once running at a higher scale, we could expect all of our services to be more sensitive to performance issues.

280
00:14:33,934 --> 00:14:41,939
Changes will be more likely to have unexpected impacts, and continued load testing was going to be the easiest way to maintain confidence in the performance of our services.

281
00:14:43,620 --> 00:14:49,843
And as we begin thinking about what it will take to get to the rapidly approaching launch, this brings part one, planning and preparation, to a close.

282
00:14:52,365 --> 00:14:54,506
This next part is all about scaling improvements.

283
00:14:55,066 --> 00:14:56,626
This is where we'll discuss the feature work

284
00:14:56,646 --> 00:14:58,407
that comprised the bulk of our development time.

285
00:15:01,848 --> 00:15:03,669
The first part of our journey was all about planning

286
00:15:03,709 --> 00:15:06,030
and preparation to help decide what improvements we

287
00:15:06,070 --> 00:15:06,630
needed to make.

288
00:15:07,790 --> 00:15:09,431
And this part is all about the improvements we

289
00:15:09,471 --> 00:15:10,471
had time enough to complete.

290
00:15:11,852 --> 00:15:15,173
First, we moved our core set of services from Google App Engine

291
00:15:15,253 --> 00:15:15,833
to Kubernetes.

292
00:15:16,954 --> 00:15:18,815
Second, we overhauled our matchmaking service.

293
00:15:20,167 --> 00:15:21,908
Third, we migrated all of our Redis servers

294
00:15:21,948 --> 00:15:22,708
to Redis Enterprise.

295
00:15:23,968 --> 00:15:25,429
Fourth, we made a set of improvements

296
00:15:25,469 --> 00:15:26,629
to our MySQL databases.

297
00:15:27,610 --> 00:15:30,230
Fifth, we added a web service rate limiting system.

298
00:15:31,411 --> 00:15:33,131
Sixth, and finally, we'll spare a few moments

299
00:15:33,191 --> 00:15:35,252
to discuss this development cycle's postmortem.

300
00:15:38,553 --> 00:15:39,613
The first feature we'll talk about

301
00:15:40,073 --> 00:15:42,094
is the live migration of our core services

302
00:15:42,154 --> 00:15:44,335
from Google App Engine to Google Kubernetes Engine.

303
00:15:45,695 --> 00:15:51,378
GAE is Google's platform as a service, similar to Amazon's Elastic Beanstalk or Azure's App Services.

304
00:15:52,298 --> 00:15:55,700
Just upload your PHP code base and the service takes care of everything else.

305
00:15:57,160 --> 00:15:59,862
GKE is just Google's managed Kubernetes service.

306
00:16:01,743 --> 00:16:06,985
We had been using GAE since our original launch in 2015, but in the last few years it was showing

307
00:16:07,025 --> 00:16:11,768
its age, and we had actually started the migration to Kubernetes before free-to-play preparation had

308
00:16:11,808 --> 00:16:12,048
begun.

309
00:16:13,043 --> 00:16:16,844
Still, this effort was folded into our free to play release as a prerequisite.

310
00:16:18,364 --> 00:16:24,006
There were several reasons for us to move to GKE. We wanted increased control over our PHP runtime,

311
00:16:24,566 --> 00:16:27,827
since we were missing out on some significant performance improvements, among other things.

312
00:16:29,047 --> 00:16:31,248
We also needed more control over our resource scaling.

313
00:16:33,209 --> 00:16:37,650
Another goal was more deployment consistency by running more components of our architecture

314
00:16:37,770 --> 00:16:41,191
in Kubernetes. This also happened to make us more cloud agnostic.

315
00:16:42,835 --> 00:16:49,758
Another big part of our core services migration was introducing separate GCP projects to better separate our deployment environments.

316
00:16:50,899 --> 00:16:57,402
A project in GCP is simply a high level organizational structure to divide up your GCP resources.

317
00:16:58,582 --> 00:17:05,626
With this change, the paradigm is to have nearly identical project for each environment, with no sharing of resources between those environments.

318
00:17:07,216 --> 00:17:10,138
Having separate projects made it much easier to determine

319
00:17:10,158 --> 00:17:11,680
which environment owned each resource

320
00:17:12,280 --> 00:17:14,302
and enforced independence of our lower environments.

321
00:17:15,423 --> 00:17:16,824
Gone were the days of production affecting

322
00:17:16,864 --> 00:17:18,546
our test environment or vice versa.

323
00:17:20,227 --> 00:17:21,688
Our logging became simpler to browse

324
00:17:21,748 --> 00:17:23,169
because it was simply a matter of selecting

325
00:17:23,189 --> 00:17:24,871
the right project rather than setting up

326
00:17:24,891 --> 00:17:25,792
the right log filters.

327
00:17:27,133 --> 00:17:29,094
Finally, our security and access control

328
00:17:29,155 --> 00:17:31,537
was massively improved, although we had to rethink

329
00:17:31,597 --> 00:17:33,698
it completely due to the way everything was reorganized.

330
00:17:35,842 --> 00:17:38,083
When this migration was complete after many long months,

331
00:17:38,843 --> 00:17:40,924
incorporating load testing was indispensable.

332
00:17:42,025 --> 00:17:45,546
GKE was still fairly new to us, as our small DevOps team only

333
00:17:45,586 --> 00:17:47,507
had experience with it from deploying and migrating

334
00:17:47,547 --> 00:17:48,047
our services.

335
00:17:49,368 --> 00:17:51,909
This is where our support from the GCP teams came in handy,

336
00:17:53,089 --> 00:17:55,410
as they were able to assist us by monitoring our load

337
00:17:55,450 --> 00:17:58,731
tests from the platform side and suggesting adjustments

338
00:17:58,771 --> 00:18:00,632
based on their extensive Kubernetes experience.

339
00:18:02,110 --> 00:18:08,896
Completing this migration to Kubernetes was a fundamental part of getting our core services modernized and tuned for higher free-to-play scale.

340
00:18:11,778 --> 00:18:14,380
The second major feature we worked on was overhauling matchmaking.

341
00:18:15,601 --> 00:18:20,365
The architecture of our matchmaking system has some insurmountable scaling performance problems.

342
00:18:21,185 --> 00:18:25,629
These issues did not affect its correctness, but rather meant that it had a strict performance ceiling.

343
00:18:26,832 --> 00:18:29,673
To fix these problems, we knew we were going to need something new.

344
00:18:30,213 --> 00:18:34,115
And it was going to be difficult because Rocket League's matchmaking has a lot of complex rules.

345
00:18:35,735 --> 00:18:44,098
Our matchmaking service was a single threaded .NET application, pretty close to being maxed out and definitely not going to hold up anywhere near our load testing target.

346
00:18:44,958 --> 00:18:47,879
And as this cat could tell you, it contained a lot of spaghetti code.

347
00:18:50,540 --> 00:18:52,641
So with a blank slate, where do we go?

348
00:18:53,889 --> 00:18:59,011
We needed to fundamentally change the way it scales, yet preserve all of the original functional requirements.

349
00:19:00,071 --> 00:19:03,812
The most obvious direction was a horizontally scalable map-reduced solution.

350
00:19:05,233 --> 00:19:08,134
We considered building from scratch, but we also found OpenMatch.

351
00:19:09,254 --> 00:19:13,036
After much analysis and deliberation, we decided to go with OpenMatch.

352
00:19:14,076 --> 00:19:16,257
It seemed like it would let us skip some framework development,

353
00:19:16,837 --> 00:19:19,738
but still allow us the freedom to implement all of our functional requirements.

354
00:19:20,178 --> 00:19:21,058
It's also open source.

355
00:19:22,725 --> 00:19:24,286
In contract with our matchmaking service,

356
00:19:24,986 --> 00:19:27,267
OpenMatch was designed for scaling,

357
00:19:27,708 --> 00:19:29,388
which is where the MapReduce algorithm comes in.

358
00:19:30,769 --> 00:19:33,130
It runs as an orchestrated set of Docker containers.

359
00:19:34,210 --> 00:19:36,491
This orchestration also enforces a division

360
00:19:36,531 --> 00:19:38,172
of responsibilities for each container,

361
00:19:38,892 --> 00:19:40,753
which compartmentalizes the different areas

362
00:19:40,773 --> 00:19:41,674
of our matchmaking logic.

363
00:19:42,694 --> 00:19:45,215
This was a huge improvement over the original spaghetti code.

364
00:19:45,735 --> 00:19:46,296
Sorry, cat.

365
00:19:48,096 --> 00:19:50,417
Lastly, it was cutting edge, so cutting edge

366
00:19:50,457 --> 00:19:52,438
that it was not up to its 1.0 release yet.

367
00:19:53,953 --> 00:19:57,314
Integrating with a pre 1.0 product actually went as you might guess.

368
00:19:58,134 --> 00:20:01,875
We were held up by or found blockers that required us to wait for the next revision.

369
00:20:02,856 --> 00:20:06,476
This also forced us to fix all breaking changes when that next version was released.

370
00:20:07,757 --> 00:20:12,318
So it was not the smoothest development cycle, but we launched with Open Match 1.0 and it

371
00:20:12,338 --> 00:20:17,919
certainly follows better versioning practices now. For our last bullet, of course, we mentioned load

372
00:20:17,959 --> 00:20:23,000
testing again. Having load tests that incorporated the matchmaking path were essential.

373
00:20:24,468 --> 00:20:27,911
We were able to tune Open Match in GKE for running at higher load levels,

374
00:20:28,211 --> 00:20:29,413
just like with our core services.

375
00:20:30,554 --> 00:20:34,798
More importantly, we learned the dynamics of how even subtle configuration changes

376
00:20:35,018 --> 00:20:36,620
affect the matchmaking system as a whole.

377
00:20:37,801 --> 00:20:41,444
This allowed us to react more quickly and correctly during our free-to-play launch,

378
00:20:42,105 --> 00:20:43,886
as well as know where to go with further development.

379
00:20:45,388 --> 00:20:48,591
Overhauling our entire matchmaking system was another huge undertaking,

380
00:20:48,751 --> 00:20:49,632
lasting more than a year.

381
00:20:50,397 --> 00:20:54,280
But it was desperately needed and directly contributed to the success of our free to play release.

382
00:20:56,741 --> 00:20:59,223
Next, let's talk about migrating to Redis Enterprise.

383
00:21:00,924 --> 00:21:05,127
In our view, the best feature of Redis Enterprise is its fully automated re-sharding.

384
00:21:05,927 --> 00:21:10,210
This means we can increase our cluster size, that is, add shards to scale horizontally,

385
00:21:10,991 --> 00:21:17,115
with a few clicks, and the data in the database is automatically redistributed across the updated cluster with no downtime.

386
00:21:17,835 --> 00:21:20,016
And yes, we used this during the free to play release.

387
00:21:21,559 --> 00:21:24,742
Of course, there is a performance cost while your keys are being redistributed,

388
00:21:25,322 --> 00:21:28,185
so hopefully you don't need to do this because your database is already pegged.

389
00:21:29,106 --> 00:21:32,409
If it is, you won't see any performance improvement from the additional shards

390
00:21:32,489 --> 00:21:33,590
until the re-sharding is complete.

391
00:21:35,351 --> 00:21:37,753
As part of deciding to use Redis Enterprise in the first place,

392
00:21:38,434 --> 00:21:40,776
we also learned in detail about how it can be deployed,

393
00:21:41,236 --> 00:21:42,978
specifically with respect to its proxy.

394
00:21:45,560 --> 00:21:48,943
Pictured here are a few common Redis Enterprise proxy configurations.

395
00:21:50,317 --> 00:21:54,600
As pictured on the left, most deployments only need a single proxy because it has very high performance.

396
00:21:55,921 --> 00:22:02,845
However, for our largest cache database, we did end up needing a multi proxy configuration to guarantee consistent performance.

397
00:22:04,687 --> 00:22:14,033
We initially tried to enable support for open sources cluster mode pictured center, which deploys one proxy per shard and has all clients connect to all proxies.

398
00:22:14,915 --> 00:22:21,159
This was the simplest change, but unfortunately, we couldn't use it due to lackluster support in our web tiers Redis client library.

399
00:22:22,600 --> 00:22:28,063
Instead, we are using a multi proxy setup that uses DNS load balancing pictured on the right.

400
00:22:29,184 --> 00:22:42,012
This has the typical disadvantages versus a layer four load balancer. Specifically for us, our web tier had recently moved to Kubernetes, and we set up node local DNS caching to improve some of our network performance.

401
00:22:44,047 --> 00:22:47,111
Unfortunately, caching DNS results is incompatible

402
00:22:47,171 --> 00:22:48,232
with DNS load balancing.

403
00:22:49,033 --> 00:22:51,997
So all DNS queries on the same Kubernetes node

404
00:22:52,438 --> 00:22:54,800
would always resolve to the same Redis Enterprise proxy

405
00:22:55,001 --> 00:22:55,802
rather than rotating.

406
00:22:56,863 --> 00:22:59,086
That said, we run enough Kubernetes nodes

407
00:22:59,206 --> 00:23:01,689
that the connections still end up reasonably well distributed

408
00:23:01,789 --> 00:23:03,951
across the cluster, so it works just fine.

409
00:23:07,821 --> 00:23:14,026
Once using Redis Enterprise, we noticed that a few of our databases were performing as if under provisioned, despite our estimates.

410
00:23:15,527 --> 00:23:21,392
Why did we have to scale up these databases to get the performance we expected? It came down to improving observability.

411
00:23:22,713 --> 00:23:34,763
Measuring performance in Redis is somewhat difficult. Measuring CPU directly is too opaque, and measuring operations per second is also misleading if your commands have a mix of O1 and ON operations.

412
00:23:36,604 --> 00:23:37,005
Performance.

413
00:23:38,419 --> 00:23:41,801
So we did tune some of our commands to simplify from the application side.

414
00:23:44,502 --> 00:23:50,226
For example, we switched from using mget and mset to the single get and set sent with pipelining.

415
00:23:51,567 --> 00:24:00,153
Thanks entirely to the pipelining, there was only a minor difference in performance from the client's perspective, and the registrars were able to burn through the smaller commands more efficiently.

416
00:24:01,284 --> 00:24:05,506
Furthermore, this made the reported operations per second far more accurate,

417
00:24:06,186 --> 00:24:11,349
since mGet and mSet both counted as one operation, no matter how many keys they updated.

418
00:24:13,250 --> 00:24:16,871
This observability change gave us the right information to explain the performance issues

419
00:24:17,051 --> 00:24:22,814
and then scale the cluster appropriately. Tied closely with observability is how we monitor

420
00:24:22,834 --> 00:24:27,896
each database. Redis Labs provides a guide for setting up Prometheus and Grafana integration.

421
00:24:28,766 --> 00:24:34,712
including some fantastic pre-configured Grafana dashboards. Once we got this working, it was far

422
00:24:34,752 --> 00:24:39,197
better than anything else we hooked up for our open-source Redis deployments. The screenshot

423
00:24:39,237 --> 00:24:44,343
on this slide shows operations per second broken down into reads and writes, and it's just one of

424
00:24:44,363 --> 00:24:50,289
the many charts on these dashboards. Redis Enterprise also has a few other nice advantages

425
00:24:50,329 --> 00:24:50,890
that we leverage.

426
00:24:52,139 --> 00:24:56,542
Redis Enterprise handles Redis PubSub much better than open source in a few different ways,

427
00:24:57,142 --> 00:25:00,464
but primarily because it doesn't distribute all publications to all shards.

428
00:25:01,925 --> 00:25:08,830
Next, standing up a temporary replica for keyspace analysis or other inspection is just a point and click operation.

429
00:25:09,910 --> 00:25:15,794
This has already come in handy several times for looking at production data without interfering with the performance of the production clusters.

430
00:25:17,573 --> 00:25:22,516
Finally, none of the features of Redis Enterprise that we are using result in vendor lock-in with Redis Labs.

431
00:25:23,236 --> 00:25:24,637
The only risk is not wanting to leave.

432
00:25:27,699 --> 00:25:30,921
Besides Redis, our other primary database system is MySQL.

433
00:25:31,861 --> 00:25:35,644
It has been our most serious known scaling bottleneck for a long time.

434
00:25:36,544 --> 00:25:38,225
Specifically, it only scales vertically.

435
00:25:39,126 --> 00:25:43,708
Give the database server more CPUs and memory until no higher spec machine is available.

436
00:25:45,312 --> 00:25:46,953
It was easy enough for us to add replicas,

437
00:25:47,334 --> 00:25:49,355
but this could only offload the read traffic

438
00:25:49,395 --> 00:25:50,836
that could tolerate replication lag.

439
00:25:52,558 --> 00:25:54,699
We implemented countless optimizations over the years,

440
00:25:54,760 --> 00:25:55,800
which gave us more headroom,

441
00:25:56,561 --> 00:25:58,622
but eventually there's an upper limit on a single server,

442
00:25:59,023 --> 00:26:01,345
especially when anticipating a quintupling

443
00:26:01,365 --> 00:26:02,325
of the server's workload.

444
00:26:03,947 --> 00:26:06,529
Our best option over the years has been feature shards.

445
00:26:07,289 --> 00:26:09,711
This is where we move sets of tables to new servers.

446
00:26:10,932 --> 00:26:11,833
And it's relatively easy.

447
00:26:12,578 --> 00:26:16,160
Well, as long as you can identify a set of tables that can safely be separated out.

448
00:26:17,480 --> 00:26:20,802
Unfortunately, having all of our tables on the same server for many years

449
00:26:21,562 --> 00:26:24,763
has led to many tightly coupled features due to convenient table joins.

450
00:26:26,084 --> 00:26:30,126
We were left with a large core set of tables, including our largest overall tables,

451
00:26:30,666 --> 00:26:33,868
that could not be separated out without significant refactoring effort.

452
00:26:35,808 --> 00:26:38,570
Notably, this does not solve the vertical scaling problem,

453
00:26:38,910 --> 00:26:41,151
but does buy us a lot of headroom for each feature.

454
00:26:42,903 --> 00:26:50,809
Despite the scaling risk, we estimated that there was not sufficient time to safely incorporate a horizontal scaling solution to our databases that needed it the most.

455
00:26:52,370 --> 00:27:02,676
We thoroughly evaluated Vitesse. It would have let us spread single tables across multiple database servers. This would scale better, but to use it properly, we need to solve new types of problems.

456
00:27:03,917 --> 00:27:12,583
One big example that's both mundane and labor intensive is that Vitesse would have required us to eliminate stored procedures and move all of the SQL into our web tiers code base.

457
00:27:13,687 --> 00:27:16,628
We have over a thousand stored procedures, so ouch.

458
00:27:18,129 --> 00:27:20,750
In the end, we split out one last set of tables

459
00:27:20,790 --> 00:27:23,411
in this release cycle, and our load testing demonstrated

460
00:27:23,471 --> 00:27:25,232
that we were able to exceed our load target.

461
00:27:26,352 --> 00:27:28,433
But this was not our only MySQL improvement.

462
00:27:30,614 --> 00:27:32,215
We also incorporated ProxySQL

463
00:27:32,635 --> 00:27:34,096
to proxy the database connections

464
00:27:34,176 --> 00:27:36,777
between our core services and our MySQL databases.

465
00:27:38,558 --> 00:27:41,459
ProxySQL is deployed as a horizontally scalable set

466
00:27:41,499 --> 00:27:43,200
of identical proxies behind a load balancer.

467
00:27:44,295 --> 00:27:45,915
we can scale out as many of these as we need.

468
00:27:46,895 --> 00:27:49,276
Each proxy maintains a set of persistent connections

469
00:27:49,416 --> 00:27:50,556
to our MySQL databases

470
00:27:51,216 --> 00:27:53,077
and listens for connections coming from our web tier.

471
00:27:54,497 --> 00:27:56,938
The point of this setup is to offload the connection churn

472
00:27:57,018 --> 00:27:58,518
caused by our PHP web tier.

473
00:27:59,718 --> 00:28:02,499
Unfortunately, we can't use either connection pooling

474
00:28:02,679 --> 00:28:04,840
or persistent connections from PHP.

475
00:28:06,202 --> 00:28:08,724
The process model makes connection pooling impossible,

476
00:28:09,424 --> 00:28:11,305
and persistent connections are not feasible

477
00:28:11,526 --> 00:28:13,147
because it would establish one connection

478
00:28:13,247 --> 00:28:15,829
from each PHP worker thread to each database,

479
00:28:16,710 --> 00:28:18,431
and we run too many worker threads for that to work.

480
00:28:20,552 --> 00:28:23,655
Processing a login to MySQL is a surprising amount of work

481
00:28:23,695 --> 00:28:24,595
for the database server.

482
00:28:25,236 --> 00:28:28,198
Offloading this work cut the CPU usage on each server

483
00:28:28,238 --> 00:28:28,999
by about half.

484
00:28:30,100 --> 00:28:31,861
This gave us a lot more performance headroom, which

485
00:28:31,941 --> 00:28:33,382
is critical for our vertical scaling.

486
00:28:35,240 --> 00:28:36,981
We also gained instant failovers.

487
00:28:37,761 --> 00:28:39,722
Our previous technique for promoting a replica

488
00:28:39,842 --> 00:28:41,202
was to swap IP addresses.

489
00:28:42,083 --> 00:28:43,543
This would take about 50 seconds,

490
00:28:44,103 --> 00:28:46,684
which was a rather large unacceptable hiccup to deal with.

491
00:28:47,405 --> 00:28:49,225
So it was only reserved for emergencies

492
00:28:49,385 --> 00:28:50,206
or maintenance windows.

493
00:28:51,826 --> 00:28:54,747
Proxy SQL lets us promote a replica almost instantly.

494
00:28:55,948 --> 00:28:58,368
This allows us to upgrade our servers much more regularly

495
00:28:58,509 --> 00:28:59,529
and without a maintenance window.

496
00:29:01,550 --> 00:29:03,090
Proxy SQL has some other benefits too.

497
00:29:04,149 --> 00:29:06,771
It speaks to MySQL protocol, so it looks like a MySQL database

498
00:29:06,791 --> 00:29:07,451
to your application.

499
00:29:07,791 --> 00:29:08,912
Just point to a different IP.

500
00:29:10,352 --> 00:29:12,334
It also has dynamic configurability

501
00:29:12,514 --> 00:29:13,934
for routing queries to replicas.

502
00:29:14,775 --> 00:29:16,676
We had already done this manually in our web tier,

503
00:29:17,416 --> 00:29:19,577
but it's a handy tool for a scaling emergency

504
00:29:19,858 --> 00:29:21,218
without having to modify our services.

505
00:29:22,679 --> 00:29:24,960
There are also some cons from using ProxySQL.

506
00:29:26,021 --> 00:29:27,942
We had some downtime due to ProxySQL bugs.

507
00:29:28,747 --> 00:29:31,587
There were socket leaks in some error handling situations,

508
00:29:32,148 --> 00:29:33,848
which took us a while to detect and mitigate.

509
00:29:34,668 --> 00:29:36,069
In fact, we're still dealing with one

510
00:29:36,229 --> 00:29:38,269
which we can't reproduce for the maintainers to fix.

511
00:29:39,590 --> 00:29:41,490
Also, ProxySQL is a bit complicated

512
00:29:41,871 --> 00:29:44,251
and the learning curve plus our lack of experience with it

513
00:29:44,851 --> 00:29:46,812
resulted in a misconfiguration in production

514
00:29:46,932 --> 00:29:48,493
within a few days of our free-to-play launch.

515
00:29:49,353 --> 00:29:51,614
This is another good reason to release major features early.

516
00:29:52,974 --> 00:29:55,355
All in all, ProxySQL was a definite improvement.

517
00:29:55,915 --> 00:29:58,297
We only discovered these cons after using it for a while,

518
00:29:58,757 --> 00:30:04,080
and the pros still clearly outweigh them.

519
00:30:04,300 --> 00:30:06,181
The last feature we'll discuss is

520
00:30:06,221 --> 00:30:08,262
one of our handiest small features, rate limiting.

521
00:30:09,583 --> 00:30:10,483
Where did this feature come from?

522
00:30:10,984 --> 00:30:12,764
Originally, we designed a few options

523
00:30:12,804 --> 00:30:15,246
for introducing a log-in queue as a major feature,

524
00:30:16,246 --> 00:30:18,087
and the engineering effort for most of those designs

525
00:30:18,127 --> 00:30:18,768
was significant.

526
00:30:20,003 --> 00:30:22,104
Eventually, we thought up a much simpler solution

527
00:30:22,285 --> 00:30:23,745
by understanding the relationships

528
00:30:23,825 --> 00:30:25,306
between our player population levels,

529
00:30:26,146 --> 00:30:28,347
session creation rates, and player churn.

530
00:30:30,148 --> 00:30:32,890
We reasoned that a configurable limit on session creation

531
00:30:33,110 --> 00:30:35,111
would allow us to control population well enough

532
00:30:35,351 --> 00:30:36,391
in the event we had to use it.

533
00:30:37,732 --> 00:30:39,233
Of course, the hope was to never use it,

534
00:30:39,513 --> 00:30:41,074
so it was more of a load-shedding technique

535
00:30:41,114 --> 00:30:41,914
added to our tool belt.

536
00:30:43,055 --> 00:30:45,276
This is also why we didn't want to spend tons of time

537
00:30:45,336 --> 00:30:48,457
on a more complex login queue that would be rarely used.

538
00:30:50,199 --> 00:30:54,041
We went with a custom implementation instead of integrating an off-the-shelf product,

539
00:30:54,781 --> 00:30:57,182
because the logic is very simple to add to our web tier,

540
00:30:58,022 --> 00:31:01,084
and having our own implementation let us use our existing infrastructure,

541
00:31:01,724 --> 00:31:05,245
such as our feature flags, logs, metrics and dashboards.

542
00:31:06,466 --> 00:31:10,848
We also leveraged key expiration in Redis to reset each tally automatically,

543
00:31:11,328 --> 00:31:13,389
which keeps this logic flowchart really simple.

544
00:31:15,369 --> 00:31:18,651
Rate limiting turned out to be one of our most versatile and useful features.

545
00:31:20,000 --> 00:31:22,261
we could configure rate limiting for any set of services.

546
00:31:23,342 --> 00:31:25,522
This would let us limit or disable problematic features.

547
00:31:26,383 --> 00:31:28,083
To this day, we have rate limiting set up

548
00:31:28,124 --> 00:31:30,364
to protect our immensely popular tournaments feature.

549
00:31:32,165 --> 00:31:35,006
We could also limit service calls globally or per player.

550
00:31:35,546 --> 00:31:37,967
For example, we could limit overall session creation

551
00:31:38,488 --> 00:31:40,788
to 500 per second to control how quickly

552
00:31:40,828 --> 00:31:41,909
our player population grows,

553
00:31:42,909 --> 00:31:44,290
or we could limit friend invites

554
00:31:44,350 --> 00:31:45,930
to one per second per player,

555
00:31:46,531 --> 00:31:48,711
if we see a client bug or something like player abuse.

556
00:31:50,214 --> 00:31:54,838
This simple rate limiting feature definitely takes the cake for cost benefit in our development cycle.

557
00:31:55,379 --> 00:31:58,302
It was cheap to write and has demonstrated its benefits many times.

558
00:32:02,025 --> 00:32:07,210
As our feature development was drawing to a close, it started to seem like we hadn't accomplished as much as we wanted to.

559
00:32:07,230 --> 00:32:10,313
Weren't there many months available on the timeline?

560
00:32:11,574 --> 00:32:13,796
Don't be afraid of asking these kind of questions in a postmortem.

561
00:32:14,396 --> 00:32:15,638
Let's see if we can come up with some answers.

562
00:32:17,214 --> 00:32:19,235
The first thing that sprung to mind was that our team,

563
00:32:19,255 --> 00:32:22,016
like many others, transitioned to full-time work from home

564
00:32:22,056 --> 00:32:22,616
in March, 2020.

565
00:32:24,197 --> 00:32:27,078
As teams go, much of our work is in the cloud

566
00:32:27,138 --> 00:32:29,019
or at the very least remote desktop friendly.

567
00:32:30,500 --> 00:32:32,321
It only took most of us a matter of days

568
00:32:32,361 --> 00:32:33,681
to get set up in our home offices.

569
00:32:34,301 --> 00:32:36,642
But of course there are many other less tangible impacts

570
00:32:37,083 --> 00:32:38,843
that the industry as a whole is still sorting out.

571
00:32:40,044 --> 00:32:42,545
This makes it hard to put a finger on any specific impact

572
00:32:42,625 --> 00:32:43,245
or lack thereof.

573
00:32:45,287 --> 00:32:50,591
Next, major features will take a lot of effort and are very involved if you follow your department's SDLC.

574
00:32:51,932 --> 00:32:55,976
Changes to live services will always need live migration plans and rollback plans.

575
00:32:56,956 --> 00:32:59,619
Bigger changes need bigger plans, often with multiple phases.

576
00:33:01,260 --> 00:33:07,445
Our matchmaking and core services migrations were the epitome of this since they both fundamentally changed how they scale.

577
00:33:08,366 --> 00:33:10,488
That's quite different from your typical feature change.

578
00:33:12,109 --> 00:33:14,271
Something else to be aware of, don't overanalyze.

579
00:33:15,108 --> 00:33:18,651
A hefty backlog containing great ideas does no good if you have no time left to work on it.

580
00:33:19,872 --> 00:33:23,374
It's probably best to have some engineers start working on features while research is ongoing.

581
00:33:25,076 --> 00:33:28,859
Next, define intermediate deadlines, respect them, and coordinate them.

582
00:33:29,799 --> 00:33:34,363
Treat them as legitimate deadlines, escalate, and call out the risks if the work is falling behind.

583
00:33:35,644 --> 00:33:39,187
Make sure everyone on the team buys in. This can be hard to sell to the team

584
00:33:39,687 --> 00:33:42,730
if the real deadline, such as the launch day, is still much further away.

585
00:33:44,400 --> 00:33:48,162
most of our major features actually slipped far past our intermediate deadlines.

586
00:33:49,382 --> 00:33:53,884
We were almost in a situation where our major features were all trying to roll out at the same time,

587
00:33:54,285 --> 00:33:55,525
right before the free-to-play release.

588
00:33:56,466 --> 00:33:57,846
That was incredibly high risk.

589
00:33:59,567 --> 00:34:02,769
From our postmodem, we realized that our feature deadline slippage

590
00:34:03,289 --> 00:34:07,191
was also partially the result of not properly coordinating the teams.

591
00:34:08,565 --> 00:34:14,610
As I mentioned earlier, some of our features were already underway and simply folded in as prerequisites to our free-to-play release.

592
00:34:15,851 --> 00:34:22,056
This weak association allowed those features' deadlines to slip without considering the wider free-to-play release schedule.

593
00:34:23,277 --> 00:34:28,902
We should have brought our teams closer together, which would have clarified the overall schedule and the impact of any individual delays.

594
00:34:30,643 --> 00:34:34,867
So after reflecting on our development postmortem, we can now start thinking about the big release.

595
00:34:36,804 --> 00:34:38,824
This part is where we'll discuss the launch itself,

596
00:34:39,345 --> 00:34:40,845
followed by our new normal.

597
00:34:44,626 --> 00:34:45,686
First, we look at the schedule.

598
00:34:46,486 --> 00:34:48,407
Our final launch schedule is as you see here.

599
00:34:50,247 --> 00:34:52,107
Releasing the game update a week early

600
00:34:52,667 --> 00:34:54,508
was an important part of the launch logistics.

601
00:34:55,308 --> 00:34:56,728
This had a few nice impacts.

602
00:34:57,909 --> 00:34:59,709
Historically, every update has had

603
00:34:59,729 --> 00:35:01,389
a significant upswing of users

604
00:35:01,629 --> 00:35:03,130
who sign in to check out the new features.

605
00:35:04,378 --> 00:35:06,659
For this launch, releasing some of these new features early

606
00:35:07,259 --> 00:35:08,940
allowed some of this user bump to subside,

607
00:35:09,520 --> 00:35:11,561
which helped flatten the curve before going free to play.

608
00:35:13,742 --> 00:35:15,063
One of the features we released early

609
00:35:15,183 --> 00:35:17,244
was account linking with Epic Games accounts.

610
00:35:17,924 --> 00:35:19,585
This was a very complicated feature,

611
00:35:20,325 --> 00:35:22,926
and releasing it early let us start monitoring it

612
00:35:23,126 --> 00:35:24,247
and get in some bug fixes

613
00:35:24,327 --> 00:35:25,988
before the additional influx of players.

614
00:35:28,265 --> 00:35:31,407
Similarly, we were able to watch the newly patched clients

615
00:35:31,427 --> 00:35:32,688
for any abnormal call patterns,

616
00:35:34,329 --> 00:35:35,690
and had time to react to the hotfix

617
00:35:35,730 --> 00:35:36,731
before the free-to-play release.

618
00:35:41,374 --> 00:35:43,175
The free-to-play launch itself was on a Wednesday,

619
00:35:43,855 --> 00:35:46,237
and midweek is significantly less busy than the weekends,

620
00:35:46,817 --> 00:35:47,457
as we shall see.

621
00:35:50,579 --> 00:35:51,420
And here's the launch.

622
00:35:52,319 --> 00:35:57,963
This chart shows the game update, the hot fix, the free to play launch on Wednesday,

623
00:35:58,663 --> 00:36:00,745
and how the population climbed into the weekend.

624
00:36:02,226 --> 00:36:04,908
Look at that population increase compared with the previous week.

625
00:36:06,489 --> 00:36:09,171
We exceeded a million concurrent players for the first time ever.

626
00:36:10,172 --> 00:36:14,975
This was an incredible milestone, but achieving it actually felt like a foregone conclusion

627
00:36:15,255 --> 00:36:19,518
because our load testing had demonstrated that our performance ceiling was much higher than this.

628
00:36:21,307 --> 00:36:26,190
Sunday's highest population peak actually exceeded our five times high-end estimate,

629
00:36:27,130 --> 00:36:29,772
but it did not exceed our how high can you go load testing.

630
00:36:31,533 --> 00:36:35,196
This reinforces that you should continue with load testing and improvements up until the release.

631
00:36:36,597 --> 00:36:39,739
This also shows why releasing on a Wednesday was an excellent choice.

632
00:36:40,819 --> 00:36:44,702
The first boost on Wednesday was big, but then we had a few days to react to issues

633
00:36:44,842 --> 00:36:47,864
as the population climbed ever higher into that first weekend.

634
00:36:49,420 --> 00:36:53,484
But if you look closely at the charts, the release wasn't entirely sunshine and roses.

635
00:36:56,606 --> 00:37:00,549
As good as the launch looks, we did have some severe issues including two

636
00:37:00,810 --> 00:37:06,634
near total outage indicated by the red arrows. The first outage was on Wednesday, the 23rd,

637
00:37:06,794 --> 00:37:11,378
which was our free to play launch day. We had an unexpected scaling issue

638
00:37:11,678 --> 00:37:15,601
related to signing in with Epic Games accounts and our new account linking feature.

639
00:37:16,830 --> 00:37:20,551
This was an integration point between Psyonix and Epic Games Services

640
00:37:20,991 --> 00:37:22,772
that was not covered by our load testing.

641
00:37:24,112 --> 00:37:26,513
Also note that account linking had released a week earlier,

642
00:37:27,053 --> 00:37:29,554
so this problem only showed up at the higher free-to-play scale.

643
00:37:31,774 --> 00:37:33,775
The second outage was on Thursday the 24th.

644
00:37:34,535 --> 00:37:37,316
We saw climbing Redis traffic getting too close for comfort

645
00:37:37,956 --> 00:37:40,417
and made the call to double the number of Redis shards

646
00:37:40,497 --> 00:37:42,378
for the affected Redis Enterprise database.

647
00:37:43,712 --> 00:37:46,973
This was a preventative measure, but this database was already stressed,

648
00:37:47,674 --> 00:37:51,156
and adding the overhead of the re-sharding operation ended up starting a death

649
00:37:51,176 --> 00:37:52,456
spiral resulting in this outage.

650
00:37:54,597 --> 00:37:56,498
The third issue was on Friday the 25th.

651
00:37:57,299 --> 00:38:00,240
It's not visible on the chart because it did not affect player population.

652
00:38:01,241 --> 00:38:04,363
This was the proxy SQL configuration issue that I mentioned earlier.

653
00:38:05,823 --> 00:38:08,025
While preparing to promote a MySQL replica,

654
00:38:09,003 --> 00:38:11,304
We removed the read-only flag from its configuration,

655
00:38:11,824 --> 00:38:14,686
and ProxySQL picked up on this config change automatically,

656
00:38:14,846 --> 00:38:15,587
which was premature.

657
00:38:16,647 --> 00:38:19,509
That replica was added to ProxySQL's write group, which

658
00:38:19,589 --> 00:38:21,270
led to a split-brain situation.

659
00:38:22,510 --> 00:38:24,011
This wasn't an outage, but we had

660
00:38:24,031 --> 00:38:25,952
to spend quite a lot of time reconciling the data.

661
00:38:28,094 --> 00:38:30,895
Over the rest of the weekend, we had no load-related outages.

662
00:38:31,455 --> 00:38:33,577
This is despite far more load compared

663
00:38:33,597 --> 00:38:34,417
with the first few days.

664
00:38:36,627 --> 00:38:39,610
These were the only issues major enough to make it into our release postmortem.

665
00:38:40,290 --> 00:38:43,753
Also, a reminder to the viewer, don't forget to keep a record of your issues

666
00:38:43,873 --> 00:38:45,475
to write a complete and accurate postmortem.

667
00:38:46,416 --> 00:38:48,077
We've referred back to ours many times.

668
00:38:49,879 --> 00:38:52,301
This was a pretty incredible record for the first weekend.

669
00:38:53,141 --> 00:38:54,442
But how did the team handle it?

670
00:38:57,425 --> 00:38:58,986
Our team was tense, but confident.

671
00:38:59,647 --> 00:39:02,089
The rest of the studio just bombed us with memes.

672
00:39:03,513 --> 00:39:05,335
The peak load set a new record every day

673
00:39:05,375 --> 00:39:06,296
for the first five days,

674
00:39:06,796 --> 00:39:09,078
which meant every day was more stressful than the last.

675
00:39:10,219 --> 00:39:11,901
When was our all-time peak going to occur?

676
00:39:12,882 --> 00:39:14,603
Amazingly, our times five projection

677
00:39:15,003 --> 00:39:16,285
turned out to be extremely accurate.

678
00:39:17,846 --> 00:39:20,748
The entire team joined a war room Zoom call every day

679
00:39:21,049 --> 00:39:22,009
up through this first weekend.

680
00:39:23,130 --> 00:39:25,292
We were more prepared than any other release prior,

681
00:39:25,793 --> 00:39:27,134
but it was still tense to monitor.

682
00:39:28,135 --> 00:39:29,916
It was so difficult to try to load test

683
00:39:30,076 --> 00:39:31,117
everything we've ever written.

684
00:39:32,035 --> 00:39:33,676
Did we somehow miss something important?

685
00:39:35,557 --> 00:39:37,678
Lastly, it's worth saying that just because there

686
00:39:37,698 --> 00:39:39,600
were no other big outages doesn't

687
00:39:39,640 --> 00:39:42,241
mean that we weren't actively identifying and fixing issues

688
00:39:42,421 --> 00:39:43,622
before they became problems.

689
00:39:44,983 --> 00:39:47,144
We spun up a simple spreadsheet for lightweight tracking

690
00:39:47,164 --> 00:39:47,984
of potential issues.

691
00:39:49,025 --> 00:39:50,466
It ended up with over 50 entries.

692
00:39:51,146 --> 00:39:52,267
Many of these were fixed quickly,

693
00:39:52,587 --> 00:39:53,947
and the rest were put into the backlog

694
00:39:53,967 --> 00:39:55,408
to fix once things calmed down.

695
00:39:57,549 --> 00:39:59,751
After the population began to subside on Monday,

696
00:40:00,191 --> 00:40:01,352
the team was able to take a breath.

697
00:40:02,975 --> 00:40:03,955
and our new normal.

698
00:40:07,518 --> 00:40:07,999
Halfway through 2021,

699
00:40:08,059 --> 00:40:11,121
we're still seeing a sustained increase in users.

700
00:40:12,082 --> 00:40:14,324
It's holding at about three times our previous normal.

701
00:40:15,945 --> 00:40:16,766
With this new normal,

702
00:40:17,146 --> 00:40:19,648
the higher population makes our services more sensitive

703
00:40:19,668 --> 00:40:20,489
to performance issues.

704
00:40:21,249 --> 00:40:23,731
This applies to both code and configuration changes.

705
00:40:24,692 --> 00:40:27,314
The specific danger is that if the client call patterns

706
00:40:27,755 --> 00:40:29,496
deviate from our load test simulations,

707
00:40:30,097 --> 00:40:31,338
we end up in uncharted territory.

708
00:40:33,352 --> 00:40:35,192
One example of such a configuration change

709
00:40:35,412 --> 00:40:38,073
is when we enabled a limited time mode like Heat Seeker

710
00:40:38,114 --> 00:40:41,015
or Spike Rush, where up to a million players all

711
00:40:41,035 --> 00:40:43,556
switched over to that game mode nearly simultaneously.

712
00:40:43,576 --> 00:40:46,617
That's very out of the ordinary behavior,

713
00:40:47,137 --> 00:40:49,338
different from our load tests, and therefore not

714
00:40:49,358 --> 00:40:50,078
something we tested.

715
00:40:51,399 --> 00:40:53,840
Thankfully, we were able to react quickly to the situation

716
00:40:54,020 --> 00:40:55,600
by extrapolating from our previous load

717
00:40:55,640 --> 00:40:56,521
testing of matchmaking.

718
00:40:58,522 --> 00:41:01,603
A second example is when we set up an easy challenge that

719
00:41:01,643 --> 00:41:02,843
rewarded multiple items.

720
00:41:03,892 --> 00:41:07,113
This caused players to be rewarded items at a much higher rate than normal,

721
00:41:07,693 --> 00:41:09,974
and it ballooned our inventory system to its breaking point.

722
00:41:11,114 --> 00:41:13,955
This has left us watching our inventory system very closely

723
00:41:14,516 --> 00:41:17,797
until we can migrate it to a new database engine that supports horizontal scaling.

724
00:41:19,537 --> 00:41:22,718
On the upside, now that we've put all this work into our load testing system,

725
00:41:23,298 --> 00:41:25,519
it's readily available for testing any of these situations.

726
00:41:27,450 --> 00:41:32,151
Finally, we've renewed our diligence in following best practices for designing live migrations,

727
00:41:32,651 --> 00:41:34,952
promoting code, and always having a rollback plan.

728
00:41:36,692 --> 00:41:39,233
This concludes our look at the launch, so let's wrap up.

729
00:41:40,653 --> 00:41:43,774
Takeaways. What do I think you should take away from this talk?

730
00:41:45,955 --> 00:41:50,776
These are my picks for takeaways. From the planning phase, seek expert advice and support.

731
00:41:51,517 --> 00:41:55,718
Stay organized and get working on the long tentpoles first, but beware of overplanning.

732
00:41:56,383 --> 00:42:01,786
set a deadline for it. The next theme that permeated this entire talk was load testing.

733
00:42:02,626 --> 00:42:06,628
It's our most significant feature that came out of this release and we would have failed without it.

734
00:42:07,369 --> 00:42:11,651
It was far more work than we expected, but it was unquestionably worth the investment

735
00:42:11,771 --> 00:42:16,794
for the free-to-play release and beyond. Next, when thinking about major improvements,

736
00:42:17,714 --> 00:42:22,837
take care to carefully coordinate their deadlines and rollouts and respect those deadlines.

737
00:42:24,562 --> 00:42:26,763
Finally, you should always have some versatile

738
00:42:26,823 --> 00:42:28,304
load shedding controls like rate limiting.

739
00:42:29,084 --> 00:42:31,265
They're low effort and will get a lot of reuse.

740
00:42:32,426 --> 00:42:33,867
Even if your load testing demonstrates

741
00:42:33,907 --> 00:42:35,688
you can handle your load target all day long,

742
00:42:36,148 --> 00:42:37,709
what if you actually get 10 times

743
00:42:37,769 --> 00:42:38,849
or a hundred times that target?

744
00:42:39,629 --> 00:42:41,610
Rate limiting might just keep your services stable

745
00:42:41,730 --> 00:42:43,191
rather than have them go down entirely.

746
00:42:44,772 --> 00:42:47,653
So in closing, thanks to efforts described in this talk,

747
00:42:48,374 --> 00:42:50,315
Psyonix had a really successful free play launch

748
00:42:50,335 --> 00:42:50,975
for Rocket League,

749
00:42:51,175 --> 00:42:52,856
and I hope you will too in your future endeavors.

750
00:42:54,266 --> 00:42:55,310
Thanks very much for attending.

751
00:42:55,651 --> 00:42:57,177
Please ask questions if you have the time.

