1
00:00:05,350 --> 00:00:05,911
Hi, everyone.

2
00:00:05,931 --> 00:00:09,313
Thank you guys for joining us for our presentation.

3
00:00:09,333 --> 00:00:12,294
Obviously, we're very sad that we won't be able to meet everyone

4
00:00:12,334 --> 00:00:14,435
in person, but still very excited to deliver

5
00:00:14,475 --> 00:00:15,396
our presentation today.

6
00:00:16,317 --> 00:00:16,997
My name is Jeff.

7
00:00:17,297 --> 00:00:19,258
I'm a lead product manager here at AI Unity.

8
00:00:19,278 --> 00:00:22,540
I'm joined here with Dr. Deng, who's a researcher also

9
00:00:22,620 --> 00:00:25,302
at Unity, and Robin Lynn Nielsen, who's

10
00:00:25,322 --> 00:00:26,482
a co-founder of Keri Castle.

11
00:00:27,083 --> 00:00:29,244
Today, we're going to be talking about specifically using

12
00:00:29,264 --> 00:00:31,865
deep reinforcement learning and testing in MPC development.

13
00:00:33,744 --> 00:00:37,746
So you may be asking why Unity at the GDC ML Summit?

14
00:00:39,427 --> 00:00:41,709
Here at Unity, we see a variety of games,

15
00:00:42,089 --> 00:00:45,251
whether it's 2D puzzle games all the way to first-person shooters.

16
00:00:45,931 --> 00:00:47,612
So for us, we see a lot of studios

17
00:00:47,652 --> 00:00:51,295
trying to implement deep reinforcement learning using

18
00:00:51,435 --> 00:00:52,516
machine learning in their games.

19
00:00:53,196 --> 00:00:54,637
So one of the things we want to provide today,

20
00:00:54,677 --> 00:00:57,799
some of the aspects and learnings that we see at Unity.

21
00:01:00,116 --> 00:01:04,738
So first we want to start out, why are so many students trying deep reinforcement learning?

22
00:01:05,719 --> 00:01:11,962
Well, first they want to ship games faster, less code, make sense, spend less time on testing,

23
00:01:12,822 --> 00:01:16,564
and of course maintain the quality. So it's one of those situations where, you know, we want to have

24
00:01:16,604 --> 00:01:22,246
our free lunch, we have our cake, we eat it too, and then you sort of insert your favorite idiom.

25
00:01:23,127 --> 00:01:26,348
And this is really the core of why a lot of students are trying this approach.

26
00:01:28,132 --> 00:01:30,252
So there is a lot of poems applying deep reinforcement

27
00:01:30,272 --> 00:01:31,033
learning in gaming.

28
00:01:31,493 --> 00:01:33,174
It can actually solve quite a bit of challenges.

29
00:01:33,494 --> 00:01:35,735
But we should be mindful about some of the difficulties.

30
00:01:35,815 --> 00:01:37,775
And really, that's kind of the hardware

31
00:01:37,795 --> 00:01:38,996
we want to talk about today.

32
00:01:39,016 --> 00:01:40,897
A lot of the challenges that studios

33
00:01:40,957 --> 00:01:42,998
face when trying to implement deep reinforcement learning.

34
00:01:44,478 --> 00:01:46,839
So how are studios using deep reinforcement learning?

35
00:01:47,819 --> 00:01:49,420
Well, the first is from the player perspective.

36
00:01:49,460 --> 00:01:52,181
Now, this is creating some sort of player bot or virtual player.

37
00:01:52,201 --> 00:01:54,502
And this is usually for game testing, game balancing.

38
00:01:55,402 --> 00:01:57,203
The second is from the non-player perspective.

39
00:01:57,223 --> 00:01:57,583
So these are.

40
00:02:00,185 --> 00:02:02,066
And these are enemies, can be companions,

41
00:02:02,106 --> 00:02:03,406
can be passerby characters.

42
00:02:04,487 --> 00:02:06,668
And the last one we call the invisible scenario.

43
00:02:06,688 --> 00:02:08,069
Now these are the scene itself

44
00:02:08,169 --> 00:02:10,110
or other experience you've not seen by the player.

45
00:02:10,330 --> 00:02:12,631
So this is where the reinforcement learning

46
00:02:12,971 --> 00:02:14,492
is trying different things for the player.

47
00:02:15,072 --> 00:02:16,973
And they're in content generation,

48
00:02:17,013 --> 00:02:18,974
difficulty tuning and player engagement.

49
00:02:20,515 --> 00:02:22,816
So really today we're gonna focus on these two use cases.

50
00:02:22,836 --> 00:02:24,877
This is really predominantly where we see

51
00:02:25,537 --> 00:02:26,818
quite a bit of studios trying

52
00:02:27,498 --> 00:02:29,479
the game testing and creating enemies area.

53
00:02:31,293 --> 00:02:34,676
In game testing side, one of the things that we do constantly see is that

54
00:02:34,696 --> 00:02:38,758
students are trying to use deep reinforcement learning to test new levels. So this is just

55
00:02:38,799 --> 00:02:43,082
some sort of generalization of a player bot to test new levels or content.

56
00:02:44,362 --> 00:02:47,865
The second is around enemies. Now this is like natural looking enemies. Now these

57
00:02:48,325 --> 00:02:52,528
think about like making the enemies you look and feel real without having to write a lot of code.

58
00:02:54,714 --> 00:02:56,255
So really that's what we're gonna focus on today.

59
00:02:56,275 --> 00:02:59,017
You know, testing new levels, you know,

60
00:02:59,197 --> 00:03:00,257
in content using RL

61
00:03:00,297 --> 00:03:02,058
and then obviously natural looking enemies.

62
00:03:02,479 --> 00:03:05,941
There are obviously many more scenarios

63
00:03:05,981 --> 00:03:07,301
and a lot more use cases,

64
00:03:07,341 --> 00:03:09,643
but today we're gonna focus on these two.

65
00:03:09,683 --> 00:03:12,004
So with that, I'm gonna pass it over to Ervin.

66
00:03:13,325 --> 00:03:14,606
So we're talking about reinforcement learning.

67
00:03:15,366 --> 00:03:15,866
Thanks Jeff.

68
00:03:16,567 --> 00:03:16,747
So,

69
00:03:17,765 --> 00:03:20,407
To quick recap, what is reinforcement learning?

70
00:03:20,647 --> 00:03:23,089
So many of you are probably already familiar with this.

71
00:03:23,269 --> 00:03:26,030
But if you're not, think of traditional machine learning.

72
00:03:26,171 --> 00:03:30,473
It's taking an input, maybe it's an image of a cat or a dog,

73
00:03:30,893 --> 00:03:32,975
and learning how to map that to an output.

74
00:03:33,555 --> 00:03:35,156
And this output could be like a label, right?

75
00:03:35,196 --> 00:03:37,678
Like, is this picture a cat, or is this picture a dog?

76
00:03:39,239 --> 00:03:41,480
So reinforcement learning isn't too different.

77
00:03:42,280 --> 00:03:45,022
Let's imagine you want to train a behavior

78
00:03:45,082 --> 00:03:45,963
for an agent in a game.

79
00:03:46,676 --> 00:03:48,702
You collect a bunch of observations in your game.

80
00:03:48,762 --> 00:03:51,029
This can be things like the agent's location,

81
00:03:51,049 --> 00:03:53,376
its velocity, how close the enemies are.

82
00:03:55,075 --> 00:03:58,718
And then the agent basically learns a policy,

83
00:03:59,139 --> 00:04:02,182
what we call a policy, that takes those observations

84
00:04:02,582 --> 00:04:04,164
and maps them to actions.

85
00:04:04,744 --> 00:04:07,667
So just like the classifier would take an image

86
00:04:07,727 --> 00:04:10,790
and map it to a label, it maps observations to actions.

87
00:04:10,890 --> 00:04:12,612
And these actions could be, for instance,

88
00:04:13,192 --> 00:04:14,354
moving around in the game,

89
00:04:14,454 --> 00:04:17,316
shooting the enemies and doing other actions, right?

90
00:04:19,117 --> 00:04:22,218
But how does it learn how to map these observations to actions?

91
00:04:22,398 --> 00:04:23,898
And this is where reinforcement learning

92
00:04:23,938 --> 00:04:25,899
is a bit different than traditional machine learning.

93
00:04:27,139 --> 00:04:29,840
So in reinforcement learning, or RL,

94
00:04:30,340 --> 00:04:33,221
the environment, which in this case is your game,

95
00:04:34,302 --> 00:04:38,103
gives a reward to the agent when it does something desirable.

96
00:04:39,043 --> 00:04:42,625
So through the agent playing the game,

97
00:04:42,805 --> 00:04:44,585
exploring and trying different actions,

98
00:04:44,825 --> 00:04:47,726
it learns how to maximize this reward.

99
00:04:49,484 --> 00:04:51,145
it receives during a playthrough of the game.

100
00:04:51,745 --> 00:04:53,546
And we call this playthrough like an episode.

101
00:04:55,106 --> 00:04:57,607
So this is really good for creating behaviors in games

102
00:04:57,727 --> 00:04:59,928
because you don't have to explicitly specify

103
00:05:00,048 --> 00:05:02,990
how the agent achieves what you want.

104
00:05:03,390 --> 00:05:05,491
You just give it a reward when it does.

105
00:05:05,991 --> 00:05:08,312
And it kind of, through exploration,

106
00:05:08,452 --> 00:05:09,952
figures out a winning strategy.

107
00:05:12,113 --> 00:05:12,253
So,

108
00:05:14,129 --> 00:05:16,550
Let's say you wanted to use reinforcement learning

109
00:05:17,290 --> 00:05:20,751
to create a bot for playtesting, right?

110
00:05:22,472 --> 00:05:26,413
So you want a bot that you can use to test and balance

111
00:05:26,433 --> 00:05:26,793
your game.

112
00:05:27,173 --> 00:05:28,954
And reinforcement learning sounds great

113
00:05:29,014 --> 00:05:30,795
because you don't have to hard code the behavior.

114
00:05:33,029 --> 00:05:34,550
So what does this bot have to be able to do?

115
00:05:34,850 --> 00:05:36,931
It has to, first and foremost, obviously,

116
00:05:37,211 --> 00:05:39,392
be able to play the game and win the game,

117
00:05:39,972 --> 00:05:40,973
just like a human player.

118
00:05:41,653 --> 00:05:44,534
But the bot also needs to do a couple of other things.

119
00:05:44,975 --> 00:05:48,636
First, it needs to be able to deal with changes in your game.

120
00:05:49,057 --> 00:05:50,217
So if you're tweaking your level,

121
00:05:50,777 --> 00:05:52,858
it needs to be able to deal with these changes and adapt.

122
00:05:53,739 --> 00:05:56,620
It needs to be able to play new unseen levels

123
00:05:57,700 --> 00:06:00,362
to evaluate them, things it hasn't seen in training.

124
00:06:00,942 --> 00:06:02,543
Otherwise, why are you testing, right?

125
00:06:04,224 --> 00:06:07,649
And it also needs to solve the levels in a human-like way

126
00:06:07,809 --> 00:06:12,677
and not find some exploit to get the reward

127
00:06:12,777 --> 00:06:15,261
but not actually play the game the way you want it to.

128
00:06:15,281 --> 00:06:15,321
So.

129
00:06:19,070 --> 00:06:22,973
To give a concrete example, let's take the game Snoopy Pop from Jam City.

130
00:06:23,654 --> 00:06:25,756
So Snoopy Pop is a bubble shooter.

131
00:06:26,856 --> 00:06:29,979
You can see on the slide, there's a screenshot from the game

132
00:06:30,239 --> 00:06:32,201
and you see that Snoopy is holding a red bubble.

133
00:06:32,821 --> 00:06:35,143
And the objective of the game is to shoot the bubble

134
00:06:35,483 --> 00:06:37,225
at some angle up to the top of the screen.

135
00:06:37,985 --> 00:06:41,148
And if you get a match in color, it'll clear those bubbles off the screen.

136
00:06:42,268 --> 00:06:44,350
And you win the game by clearing enough bubbles

137
00:06:44,610 --> 00:06:48,533
and enough of the special bubbles with the yellow bird inside of it.

138
00:06:50,590 --> 00:06:53,932
So what GemCity needed from a testing bot

139
00:06:54,032 --> 00:06:56,833
was a bot that could play all of the existing levels

140
00:06:56,873 --> 00:07:01,736
of SnoopyPup with similar performance to human players

141
00:07:02,977 --> 00:07:05,578
and be able to play newly designed levels

142
00:07:05,879 --> 00:07:08,060
and determine A, if they're solvable

143
00:07:08,660 --> 00:07:11,902
and B, how challenging the level would be to a human player.

144
00:07:13,050 --> 00:07:14,991
Would the player need to use power-ups?

145
00:07:15,372 --> 00:07:16,872
Would they get frustrated and quit?

146
00:07:17,513 --> 00:07:19,354
These metrics would be really invaluable

147
00:07:19,454 --> 00:07:22,816
to the level designer while tweaking the level.

148
00:07:24,056 --> 00:07:25,637
So this seems simple enough,

149
00:07:27,318 --> 00:07:29,279
but what makes this game especially difficult

150
00:07:29,359 --> 00:07:30,940
to train a bot for?

151
00:07:31,761 --> 00:07:36,103
Well, first of all, there are over a thousand levels

152
00:07:36,684 --> 00:07:38,244
and they all have different layouts

153
00:07:38,525 --> 00:07:40,786
and different ways of solving them.

154
00:07:43,708 --> 00:07:49,732
Furthermore, each level is randomly generated, so the position of the bubbles and the color

155
00:07:49,752 --> 00:07:55,375
of the bubbles can change from playthrough to playthrough. So that means if you scripted a bot,

156
00:07:55,956 --> 00:08:01,619
or if your RL bot simply memorized the actions it took to solve a particular level,

157
00:08:02,220 --> 00:08:03,280
it would not be able to.

158
00:08:07,371 --> 00:08:10,553
And to make things even worse, every couple of levels,

159
00:08:10,573 --> 00:08:12,654
a new element is introduced.

160
00:08:12,974 --> 00:08:14,855
So these could be different obstacles,

161
00:08:14,955 --> 00:08:16,216
they could be different power-ups,

162
00:08:16,316 --> 00:08:18,878
they could be bubbles that explode in different ways.

163
00:08:19,978 --> 00:08:23,260
And your agent would have to learn how to deal

164
00:08:23,320 --> 00:08:24,261
with all of these things.

165
00:08:26,442 --> 00:08:29,204
So how are some ways that you might try

166
00:08:29,264 --> 00:08:33,206
to approach this problem?

167
00:08:33,810 --> 00:08:37,912
So we could take the naive approach and we could say, hey, each level is a separate game.

168
00:08:38,332 --> 00:08:47,436
We'll just train a different agent to play each level. And this will work, but because now you

169
00:08:47,456 --> 00:08:52,639
have thousands of levels, you have thousands of training runs and thousands of neural networks

170
00:08:52,659 --> 00:08:57,701
that you're creating. If each training run takes several hours to a day, this could get really

171
00:08:57,741 --> 00:09:00,983
expensive really fast. Not to mention,

172
00:09:02,100 --> 00:09:06,001
Having to retrain a bot for each new level that your level designers create

173
00:09:06,821 --> 00:09:11,082
makes them all separate and you can't really directly compare the performance

174
00:09:11,342 --> 00:09:14,063
of one bot to another because they're different bots.

175
00:09:17,004 --> 00:09:21,105
Secondly, we could also train the agent the way a human would play the game

176
00:09:21,185 --> 00:09:25,066
starting from level one, then once it masters level one, train on level two,

177
00:09:25,286 --> 00:09:27,547
then once it masters that, three, and so forth.

178
00:09:29,305 --> 00:09:34,949
If we do this though, the agent is likely to forget the things that it saw in the earlier levels.

179
00:09:36,190 --> 00:09:39,733
And in machine learning, we call this problem catastrophic forgetting.

180
00:09:40,874 --> 00:09:44,776
And basically neural networks have this issue where when it's learning something new,

181
00:09:44,836 --> 00:09:48,439
it tends to quickly forget things that it has seen in the past.

182
00:09:48,979 --> 00:09:53,723
Humans don't really have this problem, but we do have to worry about this in reinforcement learning.

183
00:09:55,030 --> 00:09:57,852
Finally, if you imagine that for each of the levels

184
00:09:57,872 --> 00:10:00,893
that you've created, you probably have at least a couple

185
00:10:01,553 --> 00:10:02,954
human playthroughs of the game.

186
00:10:04,295 --> 00:10:06,836
You could train a bot to directly imitate

187
00:10:07,136 --> 00:10:08,997
these human playthroughs and

188
00:10:10,140 --> 00:10:15,664
hopefully be able to solve the level. The problem is, as you remember, the levels were

189
00:10:17,046 --> 00:10:20,749
somewhat procedurally generated. There were some random bubble positions, there were some random

190
00:10:20,769 --> 00:10:27,114
bubble colors. And it's impossible to have human demonstrations for every single possible

191
00:10:27,194 --> 00:10:34,160
combination the agent could see. And so you can't just imitate what you see in the demonstrations

192
00:10:34,180 --> 00:10:36,502
because you won't have demonstrations for all scenarios.

193
00:10:37,707 --> 00:10:39,809
So it's clear that no one of these approaches

194
00:10:39,929 --> 00:10:43,852
would work by itself, but what could work?

195
00:10:44,112 --> 00:10:46,774
Now, we can't say we've solved all of the thousands

196
00:10:46,814 --> 00:10:48,695
of levels of SnoopyPup, but these approaches

197
00:10:48,755 --> 00:10:50,316
did prove effective in this game.

198
00:10:51,957 --> 00:10:55,660
So if we want one agent that can play all of the levels,

199
00:10:55,960 --> 00:10:58,702
and we don't want it to forget earlier game mechanics,

200
00:10:59,562 --> 00:11:01,804
we will have to rely on domain randomization

201
00:11:02,304 --> 00:11:05,006
to keep the agent robust to variations in the environment.

202
00:11:05,672 --> 00:11:07,953
In the SnoopyCloud case, this means

203
00:11:07,993 --> 00:11:10,213
being robust to new levels and different bubble

204
00:11:10,233 --> 00:11:11,014
configurations.

205
00:11:12,874 --> 00:11:15,215
Furthermore, while pure imitation learning might not

206
00:11:15,255 --> 00:11:18,655
be a good strategy, having those demonstrations,

207
00:11:18,675 --> 00:11:20,556
having those human demonstrations, can really

208
00:11:20,616 --> 00:11:23,957
help, especially if you combine it with reinforcement learning.

209
00:11:24,577 --> 00:11:26,677
It can guide the agent towards a good solution,

210
00:11:27,638 --> 00:11:29,898
but not prevent it from learning how

211
00:11:29,918 --> 00:11:33,179
to solve situations that were not in the demonstrations.

212
00:11:36,418 --> 00:11:39,899
Okay, so let's talk first talk a little bit about domain randomization.

213
00:11:40,679 --> 00:11:44,861
So it's a common technique used in reinforcement learning to make sure you're a trained agent

214
00:11:45,441 --> 00:11:50,582
can generalize how to solve a problem rather than just memorize very specific instances

215
00:11:50,742 --> 00:11:51,383
of your problem.

216
00:11:52,363 --> 00:11:54,884
So the idea is that you find aspects of your domain.

217
00:11:55,504 --> 00:11:58,465
So in this case, your game levels that can vary.

218
00:11:59,005 --> 00:12:04,607
For instance, lighting textures layout of the level type of enemy and so forth.

219
00:12:05,865 --> 00:12:08,708
And during training, you randomly vary these aspects

220
00:12:08,768 --> 00:12:10,690
so that the agent sees all of the variations.

221
00:12:12,131 --> 00:12:15,334
Then the agents will learn to cope with or ignore

222
00:12:15,354 --> 00:12:16,535
these variations.

223
00:12:17,445 --> 00:12:20,826
So I have three examples of domain randomization

224
00:12:20,866 --> 00:12:21,547
here on the slide.

225
00:12:22,047 --> 00:12:24,568
On the left is a classic example from OpenAI,

226
00:12:25,328 --> 00:12:28,530
where they had a virtual robot hand trying

227
00:12:28,610 --> 00:12:30,131
to manipulate a Rubik's cube.

228
00:12:30,871 --> 00:12:31,972
And it was learning how to do that.

229
00:12:32,452 --> 00:12:34,813
And during training, they varied the background color,

230
00:12:34,893 --> 00:12:37,734
as you can see, the lighting, and the physics

231
00:12:37,994 --> 00:12:39,955
of the actual robot manipulating the cube.

232
00:12:41,705 --> 00:12:44,286
And since the trained agent saw all of these variations

233
00:12:44,386 --> 00:12:46,926
and learned how to ignore the ones that weren't important

234
00:12:47,727 --> 00:12:49,627
and deal with the ones that mattered,

235
00:12:50,868 --> 00:12:53,868
it was able to eventually manipulate a cube in real life.

236
00:12:55,209 --> 00:12:58,850
So in a game, there's no real life test,

237
00:12:59,190 --> 00:13:02,911
but this real life test is now instead

238
00:13:03,692 --> 00:13:04,932
training on a new level, right?

239
00:13:05,772 --> 00:13:07,013
And it's kind of the same idea.

240
00:13:07,033 --> 00:13:09,233
All right, so.

241
00:13:10,189 --> 00:13:12,130
In a game, you can do domain randomization

242
00:13:12,210 --> 00:13:13,130
in a couple of ways.

243
00:13:13,911 --> 00:13:15,932
For instance, if you have thousands of levels

244
00:13:16,012 --> 00:13:19,273
like you did in SnoopyPOP, you could randomly

245
00:13:19,353 --> 00:13:21,415
sample the different levels and treat that

246
00:13:21,575 --> 00:13:23,396
as variations in your domain.

247
00:13:25,097 --> 00:13:28,418
Likewise, if you have levels that are procedurally

248
00:13:28,458 --> 00:13:33,301
generated, you can use procedural generation

249
00:13:33,361 --> 00:13:37,503
to create an infinite number of variations of that level.

250
00:13:41,678 --> 00:13:44,199
So if you want to read a bit more about how

251
00:13:44,219 --> 00:13:46,820
to use domain randomization to train robust agents,

252
00:13:48,500 --> 00:13:49,441
I'll leave these links here.

253
00:13:49,461 --> 00:13:51,681
And they'll also be available after the presentation.

254
00:13:54,262 --> 00:13:56,623
OK, so let's talk a bit more about what

255
00:13:56,663 --> 00:13:59,104
did I mean by we can combine demonstrations

256
00:13:59,204 --> 00:14:01,344
with reinforcement learning to improve performance.

257
00:14:02,325 --> 00:14:05,285
Well, so when you're training a reinforcement learning agent,

258
00:14:05,545 --> 00:14:07,246
the first thing you have to do in your game

259
00:14:07,366 --> 00:14:09,747
is define a reward function that describes

260
00:14:09,807 --> 00:14:11,227
what you want the agent to do.

261
00:14:12,495 --> 00:14:16,100
Now, the easiest types of rewards to define are sparse.

262
00:14:16,902 --> 00:14:18,684
For instance, you can reward the agent

263
00:14:18,965 --> 00:14:20,947
when it wins the game, right?

264
00:14:20,967 --> 00:14:23,451
And give it a negative reward when it loses the game.

265
00:14:24,372 --> 00:14:26,095
And this gives the agent the freedom

266
00:14:26,135 --> 00:14:27,777
to find the best solution towards that goal.

267
00:14:29,162 --> 00:14:31,384
But sparse rewards are much harder to solve,

268
00:14:31,744 --> 00:14:36,068
as the agent needs to, through luck, stumble upon them.

269
00:14:36,588 --> 00:14:38,410
So reinforcement learning is essentially

270
00:14:38,470 --> 00:14:39,731
guided random search.

271
00:14:39,771 --> 00:14:41,212
The agent tries different actions,

272
00:14:41,833 --> 00:14:43,454
and it sees what kind of rewards it gets.

273
00:14:44,175 --> 00:14:48,079
And it's very unlikely that the agent will randomly find a way

274
00:14:48,179 --> 00:14:49,900
to win a complex game.

275
00:14:50,901 --> 00:14:53,503
However, if you give small incremental rewards,

276
00:14:54,379 --> 00:14:56,780
you may spend a lot of time tweaking these rewards,

277
00:14:57,440 --> 00:15:00,742
and they may lead to unintended non-human-like behavior

278
00:15:00,782 --> 00:15:01,562
for your player bot.

279
00:15:03,223 --> 00:15:06,364
So what we can do to solve this issue,

280
00:15:06,404 --> 00:15:08,265
then, is to mix reinforcement learning

281
00:15:08,365 --> 00:15:10,866
with human demonstration data.

282
00:15:12,287 --> 00:15:14,708
So rather than purely using the reward functions

283
00:15:14,728 --> 00:15:16,289
to train our agent, we can use some

284
00:15:16,329 --> 00:15:18,770
of the demonstrations provided from a human player

285
00:15:19,310 --> 00:15:20,511
to guide the agent's learning.

286
00:15:21,146 --> 00:15:23,047
This can not only speed up exploration

287
00:15:23,148 --> 00:15:26,050
towards those sparse rewards, but also result

288
00:15:26,070 --> 00:15:29,332
in the agent solving a level in a more human-like manner

289
00:15:30,073 --> 00:15:32,755
as it chose to follow that path while exploring.

290
00:15:35,677 --> 00:15:37,098
So here's two examples.

291
00:15:37,759 --> 00:15:41,261
On the left, on the right of SnoopyPOP, we've seen that.

292
00:15:41,461 --> 00:15:43,983
On the left is an environment that we've

293
00:15:44,023 --> 00:15:45,524
created where the agents.

294
00:15:47,029 --> 00:15:50,171
needs to hit a switch and then grab a green block

295
00:15:51,011 --> 00:15:52,472
before it even sees any reward.

296
00:15:52,532 --> 00:15:55,753
And it gets plus one for touching the green block.

297
00:15:58,874 --> 00:16:01,616
Now, it's very unlikely, though possible,

298
00:16:01,756 --> 00:16:03,276
to find this by random search,

299
00:16:04,277 --> 00:16:07,078
but it's pretty obvious to human.

300
00:16:07,218 --> 00:16:10,460
And just providing a couple of demonstrations, around 10,

301
00:16:11,220 --> 00:16:13,601
we can cut the training time in more than half.

302
00:16:14,730 --> 00:16:17,452
And this is because it kind of pushes the agent

303
00:16:17,513 --> 00:16:20,775
to get the switch and to find the green blocker.

304
00:16:22,076 --> 00:16:23,617
So this is a sparse rewards.

305
00:16:24,138 --> 00:16:26,299
Even with dense rewards, like Snoopy Pop,

306
00:16:26,359 --> 00:16:28,921
where we do give a reward every time bubbles

307
00:16:28,981 --> 00:16:29,902
are cleared on the screen.

308
00:16:30,082 --> 00:16:30,622
It makes sense.

309
00:16:30,662 --> 00:16:31,483
Your score goes up.

310
00:16:34,665 --> 00:16:37,187
Demonstrations still can help you reduce your training time.

311
00:16:37,668 --> 00:16:39,809
So a similar number of demonstrations

312
00:16:40,650 --> 00:16:41,971
brought the training time down around 25%.

313
00:16:44,013 --> 00:16:45,294
So why would this be worth it?

314
00:16:45,834 --> 00:16:48,555
Well, in some cases, a few minutes of human playtime

315
00:16:49,436 --> 00:16:51,437
could save hours in training.

316
00:16:52,258 --> 00:16:54,119
And in many cases, you may already have this data.

317
00:16:54,159 --> 00:16:55,800
Maybe you as a designer have already

318
00:16:55,840 --> 00:16:57,241
played through the level a couple of times,

319
00:16:57,341 --> 00:16:59,622
or maybe there were human testers that did.

320
00:17:02,203 --> 00:17:04,325
So you can do a bit more with demonstrations.

321
00:17:04,365 --> 00:17:06,506
You can also solve problems that were not

322
00:17:06,566 --> 00:17:08,527
solvable using pure reinforcement learning.

323
00:17:09,551 --> 00:17:13,052
A good case study of this was during the Unity obstacle tower challenge,

324
00:17:13,252 --> 00:17:18,594
where the competitors had to create agents that could play the platformer game shown on the screen.

325
00:17:20,135 --> 00:17:24,937
Now, you can see that the agent basically has to solve this fairly complex block puzzle to

326
00:17:25,337 --> 00:17:30,339
pass this level, but push the purple block onto the switch and then go through the door.

327
00:17:32,883 --> 00:17:35,964
This is pretty much impossible to find through random luck.

328
00:17:36,385 --> 00:17:38,486
The agent will never figure out how to jump on that thing

329
00:17:38,606 --> 00:17:41,507
and then push the lock down and then go through the door.

330
00:17:42,408 --> 00:17:44,789
So furthermore, the level is procedurally generated.

331
00:17:44,829 --> 00:17:47,250
So every time you play, it's a slightly different layout

332
00:17:47,290 --> 00:17:48,991
and the blocks in a different place and so forth.

333
00:17:49,451 --> 00:17:53,033
So it can't just memorize the actions it took

334
00:17:53,073 --> 00:17:54,214
to be able to solve the level.

335
00:17:55,474 --> 00:17:57,696
Now, our winning competitor, Alex Nickel,

336
00:17:57,976 --> 00:17:59,477
used play throughs of the game

337
00:17:59,557 --> 00:18:00,617
to guide the agent's learning.

338
00:18:01,810 --> 00:18:03,911
leading the agent to discover the winning solution.

339
00:18:04,331 --> 00:18:05,771
So after enough iterations,

340
00:18:05,891 --> 00:18:07,532
the agent learned how to solve

341
00:18:07,772 --> 00:18:09,633
even the randomly generated levels

342
00:18:09,713 --> 00:18:11,073
that were not in the demonstrations.

343
00:18:12,094 --> 00:18:14,214
Now, a little caveat here,

344
00:18:14,254 --> 00:18:17,556
it took way more than 10 playthroughs to solve this level.

345
00:18:18,156 --> 00:18:22,478
In total, Alex used more than two days of human playtime

346
00:18:23,338 --> 00:18:26,379
to be able to solve the levels of Obstacle Tower that he did.

347
00:18:27,443 --> 00:18:29,424
You can read a lot more about his approach, which

348
00:18:29,464 --> 00:18:30,244
I think is really cool.

349
00:18:30,964 --> 00:18:33,965
And check out his code at the blog post link on the slide.

350
00:18:34,085 --> 00:18:35,526
And again, they will be available at the end

351
00:18:35,586 --> 00:18:36,386
of the presentation.

352
00:18:38,667 --> 00:18:40,527
So if you want to read more about how

353
00:18:40,567 --> 00:18:44,649
you can use demonstrations to help reinforcement learning

354
00:18:44,729 --> 00:18:47,890
agents learn in a timely manner, I'll

355
00:18:47,910 --> 00:18:49,850
leave these links here on the slide.

356
00:18:50,090 --> 00:18:51,371
And again, they'll be available.

357
00:18:53,692 --> 00:18:56,753
So something we've hinted on a little bit.

358
00:18:58,212 --> 00:18:59,953
is that RL takes a lot of time.

359
00:19:01,353 --> 00:19:04,114
Often it requires the agent to take millions of actions

360
00:19:04,714 --> 00:19:06,515
in a game to learn anything useful.

361
00:19:08,255 --> 00:19:10,836
And this would be OK if it wasn't for the fact

362
00:19:10,876 --> 00:19:12,857
that modern games are fairly complex

363
00:19:13,017 --> 00:19:14,837
and they have simulated physics, they

364
00:19:14,877 --> 00:19:16,598
may have complex 3D scenes.

365
00:19:17,538 --> 00:19:20,439
And running a million actions in a game isn't free.

366
00:19:23,071 --> 00:19:25,512
Techniques such as domain randomization

367
00:19:26,213 --> 00:19:29,114
would make the agent required to like even see

368
00:19:29,154 --> 00:19:32,295
even more samples because it now has to deal

369
00:19:32,335 --> 00:19:34,056
with all of the variations of the environment.

370
00:19:35,457 --> 00:19:37,238
So how are some ways we can train

371
00:19:37,678 --> 00:19:39,859
these playtesting bots more quickly?

372
00:19:40,479 --> 00:19:43,080
Well, any sort of speed up is gonna come

373
00:19:43,180 --> 00:19:44,121
in one of two forms.

374
00:19:44,221 --> 00:19:46,602
It's either gonna be through sample throughput, which is.

375
00:19:48,543 --> 00:19:54,686
the, you know, how many actions we can take in our game per unit time or sample efficiency, which is

376
00:19:56,327 --> 00:20:02,010
how many samples our algorithm takes to train, right? So some ways to increase sample throughput,

377
00:20:02,250 --> 00:20:07,433
you can speed up your game, you could increase the interval between physics calculations, you could

378
00:20:08,314 --> 00:20:14,037
increase frame skip, decrease rendering quality and so forth. It's nice because you can use the

379
00:20:14,057 --> 00:20:17,379
same hardware that you're running on before, but...

380
00:20:18,284 --> 00:20:18,904
It's limited.

381
00:20:19,285 --> 00:20:21,746
And if, for instance, if you speed the game to,

382
00:20:21,946 --> 00:20:24,348
if you compute physics too infrequently,

383
00:20:24,388 --> 00:20:26,389
you may have bugs that you didn't intend.

384
00:20:28,430 --> 00:20:30,311
Stuff may start tunneling through each other

385
00:20:30,411 --> 00:20:30,952
and so forth.

386
00:20:33,733 --> 00:20:35,694
So if you don't want to do that, then you

387
00:20:35,734 --> 00:20:37,896
can run many copies of your game.

388
00:20:38,836 --> 00:20:40,557
This doesn't require any modifications,

389
00:20:40,637 --> 00:20:42,598
doesn't change the dynamics of the game.

390
00:20:43,419 --> 00:20:46,321
But it does proportionally increase your computation cost.

391
00:20:47,172 --> 00:20:49,513
depending on how many games you're running.

392
00:20:49,953 --> 00:20:51,193
That's pretty obvious.

393
00:20:53,073 --> 00:20:55,594
Now, OK, so that's sample throughput.

394
00:20:55,814 --> 00:20:59,955
And for sample efficiency, we've already

395
00:20:59,995 --> 00:21:02,495
talked a bit about using demonstrations

396
00:21:02,775 --> 00:21:04,896
to reduce the number of samples needed.

397
00:21:05,976 --> 00:21:07,596
You do have to produce your demonstrations.

398
00:21:08,616 --> 00:21:10,997
But something else you can also do that we found effective,

399
00:21:11,117 --> 00:21:12,457
especially in the SnoopyPod case.

400
00:21:13,363 --> 00:21:18,610
is making sure that you're using a sample efficient deep reinforcement learning algorithms.

401
00:21:19,471 --> 00:21:23,556
There's kind of two main categories of deep reinforcement learning algorithms.

402
00:21:24,357 --> 00:21:27,661
I won't go too much into detail today, it's a bit out of scope, but...

403
00:21:29,420 --> 00:21:31,601
Basically we have on policy and off policy

404
00:21:32,441 --> 00:21:36,182
and off policy algorithms tend to require far fewer samples

405
00:21:36,222 --> 00:21:37,083
to learn behavior,

406
00:21:37,723 --> 00:21:40,824
but they tend to require larger neural network models

407
00:21:41,104 --> 00:21:43,925
and more computations spent on training.

408
00:21:44,666 --> 00:21:49,528
So this trade-off could be beneficial

409
00:21:49,608 --> 00:21:52,349
depending on how quickly you can take the actions

410
00:21:52,409 --> 00:21:52,829
in your game.

411
00:21:53,519 --> 00:21:56,780
And in the Snoopy Pop case, which was the game itself

412
00:21:56,840 --> 00:22:02,703
was limited to in speed up because we couldn't increase

413
00:22:02,723 --> 00:22:04,664
the physics steps too much.

414
00:22:04,684 --> 00:22:06,944
Otherwise, the bubbles would tunnel through each other.

415
00:22:08,405 --> 00:22:11,186
Using a sample efficient algorithm,

416
00:22:12,587 --> 00:22:15,008
off-policy algorithm, we saw more than five times

417
00:22:15,088 --> 00:22:17,849
training speed up versus an on-policy algorithm.

418
00:22:17,869 --> 00:22:22,651
All right, so to quickly recap.

419
00:22:24,988 --> 00:22:26,168
using RL for testing.

420
00:22:26,929 --> 00:22:30,911
So if you want agents that are robust across levels

421
00:22:31,211 --> 00:22:33,872
and could be able to play your new levels,

422
00:22:34,792 --> 00:22:37,414
you can use some form of domain randomization

423
00:22:37,674 --> 00:22:40,435
to make sure your bots can deal

424
00:22:40,475 --> 00:22:41,655
with all the different scenarios.

425
00:22:43,496 --> 00:22:47,018
Now, if the problem in your game is too hard to solve

426
00:22:47,458 --> 00:22:48,859
by pure random exploration,

427
00:22:49,866 --> 00:22:51,406
You may want to use demonstrations

428
00:22:51,507 --> 00:22:52,287
to guide your learning.

429
00:22:52,587 --> 00:22:55,648
This can reduce training time as well as produce

430
00:22:55,808 --> 00:22:57,769
more human-like behavior.

431
00:22:59,089 --> 00:23:02,430
And finally, we did talk a little bit

432
00:23:02,470 --> 00:23:05,231
about how we could reduce long training times.

433
00:23:05,291 --> 00:23:07,532
You could leverage parallel computing,

434
00:23:08,332 --> 00:23:10,353
you could use demonstrations, or you

435
00:23:10,373 --> 00:23:13,394
could try sample efficient reinforcement learning

436
00:23:13,474 --> 00:23:17,955
algorithms to reduce how long it takes to train your agents.

437
00:23:18,962 --> 00:23:23,323
So that's reinforcement learning for playtesting.

438
00:23:23,483 --> 00:23:27,143
And I'll pass it on to Robin to talk about reinforcement

439
00:23:27,183 --> 00:23:29,084
learning for non-player characters.

440
00:23:30,564 --> 00:23:31,284
Thank you very much.

441
00:23:31,824 --> 00:23:32,505
Hi, everyone.

442
00:23:33,485 --> 00:23:34,325
My name is Robin.

443
00:23:35,025 --> 00:23:39,246
I represent Karakassel, a small Swedish game company started

444
00:23:39,306 --> 00:23:41,747
together by me and Pal Fonando.

445
00:23:42,775 --> 00:23:49,001
We are a small team, but we love technology, and we use it to make great content for our games.

446
00:23:51,043 --> 00:23:56,849
Right now we are working on a game called Source of Madness.

447
00:23:57,469 --> 00:24:02,995
Source of Madness is an action roguelite. As an acolyte from the cult of knowledge,

448
00:24:03,215 --> 00:24:06,919
you have to close the box of Pandora and end the madness.

449
00:24:09,847 --> 00:24:15,391
dynamic world, but every time you die, you start over and the whole world changes.

450
00:24:17,313 --> 00:24:22,136
The world is full of horrible monsters from the void, and these monsters are different

451
00:24:22,217 --> 00:24:23,077
every time you play.

452
00:24:24,598 --> 00:24:30,963
So we asked ourselves, how can we make millions of monsters and keep them interesting, even

453
00:24:31,043 --> 00:24:32,825
after playing for many hours?

454
00:24:33,905 --> 00:24:35,246
We are only three people.

455
00:24:37,268 --> 00:24:39,530
So these were our challenges.

456
00:24:41,177 --> 00:24:47,801
We wanted to make the monsters move by controlling their muscle physics to look unique and super creepy.

457
00:24:48,821 --> 00:24:55,225
We have millions of different procedurally generated monsters and it needs to look natural.

458
00:24:56,326 --> 00:25:01,149
This would be almost impossible to code by hand, especially with only three people.

459
00:25:01,950 --> 00:25:05,752
So we turned to reinforcement learning as a solution.

460
00:25:08,784 --> 00:25:12,085
To train the monsters, we use the following setup in this slide.

461
00:25:12,985 --> 00:25:16,566
The AI gets input data about the environment and its body,

462
00:25:17,366 --> 00:25:19,627
and it learns to control its muscles.

463
00:25:20,767 --> 00:25:22,387
They can also grab the ground.

464
00:25:24,148 --> 00:25:27,209
Monsters are rewarded for moving towards the player,

465
00:25:28,269 --> 00:25:30,669
but we also use some additional rewards

466
00:25:31,109 --> 00:25:32,430
to get different behaviors.

467
00:25:33,210 --> 00:25:35,391
And I will show you an example of that now.

468
00:25:37,728 --> 00:25:46,831
In both of these videos, the main goal was to walk right towards the player, but we also gave them a smaller extra reward.

469
00:25:47,712 --> 00:25:54,974
So on the top, as you see, they got rewarded if they stayed close to the ground, so they got this tumbling movement.

470
00:25:54,994 --> 00:26:02,217
On the bottom, they got more reward by staying above the ground, and they learned to jump.

471
00:26:03,361 --> 00:26:08,584
So as you see in this way, we can get much more variation without spending too much time.

472
00:26:11,927 --> 00:26:15,689
The AI extends well to different body structures as well.

473
00:26:16,309 --> 00:26:19,512
The neural network will learn how to best work with its body.

474
00:26:20,132 --> 00:26:25,175
For example, a quad pad as you see on the left side or a spider on the right side.

475
00:26:28,680 --> 00:26:33,204
Many of you are also maybe using machine learning or want to try it out.

476
00:26:33,885 --> 00:26:38,729
So I will now share some things we've learned during this project and hopefully it will be

477
00:26:38,769 --> 00:26:44,815
of some help to you. So our main challenge with machine learning is to understand why

478
00:26:45,075 --> 00:26:50,920
when it doesn't work. So if your training doesn't learn there are many potential reasons.

479
00:26:51,421 --> 00:26:54,844
So when it doesn't work you want to find out what to change.

480
00:26:55,740 --> 00:27:00,302
and you want to avoid start guessing randomly because that takes a lot of time.

481
00:27:02,443 --> 00:27:08,266
So from our experience if you're working with machine learning in real time like for a game

482
00:27:09,067 --> 00:27:14,109
the biggest pitfalls are probably the reward function and the handling of actions

483
00:27:15,710 --> 00:27:20,493
because these are done by your own code and as we all know it's easy to make mistakes.

484
00:27:22,002 --> 00:27:26,083
Our key takeaway is to always make sure that these are working as intended.

485
00:27:26,823 --> 00:27:31,985
Whenever you change your code, just check them again and make sure that it still works as you want it to work.

486
00:27:35,327 --> 00:27:41,169
Otherwise, you'll maybe waste hours on a failed training and it takes time to find out what went

487
00:27:41,229 --> 00:27:49,772
wrong. Or even worse, the AI might work fairly well but not optimal. You might not even notice at first.

488
00:27:50,452 --> 00:27:52,753
That was the case with our monsters recently.

489
00:27:53,574 --> 00:27:55,935
We had not focused on the AI for a while,

490
00:27:55,955 --> 00:28:01,278
and we just started to notice that they didn't work as good as they used to.

491
00:28:02,999 --> 00:28:06,721
Finally, we found that a lot of the inputs were always zero,

492
00:28:07,402 --> 00:28:09,323
and it had been like that for two months.

493
00:28:10,284 --> 00:28:14,186
And once we fixed that, the monsters got a lot better again.

494
00:28:17,230 --> 00:28:22,454
To avoid this kind of mistake, it is important to make good visualizations of your data,

495
00:28:23,035 --> 00:28:28,819
so that you can easily spot if something is wrong. This is an in-game gizmo that we made

496
00:28:29,160 --> 00:28:35,424
early in the project. As we can see in the engine, it shows the reward in real time.

497
00:28:36,225 --> 00:28:41,869
So when this little monster is walking right towards the player, we should see that the bar

498
00:28:41,929 --> 00:28:42,890
goes more green.

499
00:28:43,677 --> 00:28:47,337
And if it didn't, then we know that something in our code

500
00:28:47,397 --> 00:28:48,017
must be wrong.

501
00:28:48,217 --> 00:28:50,478
Either the reward function or the training

502
00:28:50,538 --> 00:28:51,718
itself could be wrong then.

503
00:28:52,098 --> 00:28:53,778
So this will save you a lot of time

504
00:28:53,818 --> 00:28:56,679
if you can just see what is going on

505
00:28:56,839 --> 00:28:58,199
and not just guess randomly.

506
00:29:00,800 --> 00:29:03,340
Let's now consider these monsters again.

507
00:29:03,980 --> 00:29:08,321
As I said, we have a main reward and several other small rewards.

508
00:29:10,221 --> 00:29:11,262
We added them together.

509
00:29:12,547 --> 00:29:16,589
Here comes a problem if one reward is very big and the others are small,

510
00:29:17,149 --> 00:29:20,391
then the small rewards would have almost no effect.

511
00:29:22,051 --> 00:29:24,833
So we need to balance them properly and how do we do that?

512
00:29:25,313 --> 00:29:28,775
So to help with that we modified the reward gizmo.

513
00:29:29,135 --> 00:29:33,717
You see this on the right. This is the same gizmo as before for the reward,

514
00:29:33,897 --> 00:29:37,039
but it shows each kind of reward as a separate bar.

515
00:29:37,796 --> 00:29:40,937
So this monster is getting a reward for moving.

516
00:29:41,297 --> 00:29:43,177
That's green, the big bar on the left.

517
00:29:44,038 --> 00:29:46,818
And it's getting a punishment for being upside down.

518
00:29:46,978 --> 00:29:47,839
That's the red bar.

519
00:29:49,119 --> 00:29:51,760
And then if any bar is too big or too small,

520
00:29:52,080 --> 00:29:53,820
we can adjust the reward function

521
00:29:54,700 --> 00:29:58,181
and avoid training in a long time

522
00:29:58,301 --> 00:30:00,682
to realize that it didn't learn what we wanted to.

523
00:30:01,042 --> 00:30:02,062
As Erwin talked about,

524
00:30:02,122 --> 00:30:04,523
that it could get a non-human-like behavior

525
00:30:04,843 --> 00:30:06,243
if you're training a human agent.

526
00:30:07,036 --> 00:30:10,337
You can balance the reward function better if you see what's going on.

527
00:30:13,318 --> 00:30:19,559
So here is another gizmo that we made to see when a limb is grabbing. You see these little

528
00:30:19,699 --> 00:30:26,581
blue balls. With this gizmo we could easily see if an agent learned to use the grabbing or not.

529
00:30:27,521 --> 00:30:32,943
It also helped us to find out quickly when the agents were not grabbing at all due to a code mistake.

530
00:30:34,319 --> 00:30:38,340
that it was simply not able to grab, so it wouldn't help if we trained it longer.

531
00:30:39,200 --> 00:30:41,001
That we realized very fast in this way.

532
00:30:43,381 --> 00:30:48,703
So I hope this advice will be useful and inspiring. It's a really fun topic to work

533
00:30:48,723 --> 00:30:53,545
with machine learning and if you're interested to know more about this game that I'm working on,

534
00:30:53,685 --> 00:30:55,185
check out sourceofmadness.com.

535
00:30:57,680 --> 00:31:03,066
And finally, thank you all for watching. If you point your camera at this QR code,

536
00:31:03,106 --> 00:31:08,152
you can find more information related to this talk and also the links that are being talked about.

537
00:31:09,353 --> 00:31:13,898
So thank you all for watching and have a nice day. Thank you everyone.

