1
00:00:07,227 --> 00:00:09,730
Welcome to real-time rendering for feature film.

2
00:00:09,730 --> 00:00:18,222
We'll be presenting a case study of some work we at the Lucasfilm Advanced Development Group

3
00:00:18,222 --> 00:00:21,687
did last year in support of Rogue One, a Star Wars story.

4
00:00:27,872 --> 00:00:32,776
Today's session is going to be shared between the three of us, three speakers.

5
00:00:32,776 --> 00:00:38,561
John first is going to give an overview of the film itself

6
00:00:38,561 --> 00:00:41,483
and how this real-time work came to be involved.

7
00:00:41,483 --> 00:00:47,448
Then I will get into a breakdown of how ADG contributed to Rogue One.

8
00:00:47,448 --> 00:00:54,333
I'll speak to the details of the digital assets involved and the workflow for lighting the shots.

9
00:00:55,935 --> 00:01:02,658
Then I will hand over to Natty to discuss the technical details of rendering this content.

10
00:01:02,658 --> 00:01:10,902
We will also touch on some ongoing work in this area happening at ADG and at ILMxLAB.

11
00:01:10,902 --> 00:01:18,605
Then we'll close with some time for, oh, I haven't been doing my bullets.

12
00:01:18,605 --> 00:01:23,687
We'll close with some time for Q&A, but first I'd like to give some introductions.

13
00:01:26,840 --> 00:01:31,204
Since 1986, John has been at Industrial Light and Magic,

14
00:01:31,204 --> 00:01:35,568
where he is now chief creative officer and also a senior

15
00:01:35,568 --> 00:01:36,789
visual effects supervisor.

16
00:01:36,789 --> 00:01:40,452
As John is an avid computer graphics enthusiast

17
00:01:40,452 --> 00:01:45,916
and hobbyist himself, his shows have always

18
00:01:45,916 --> 00:01:49,679
been known to push the state of the art in production CG.

19
00:01:49,679 --> 00:01:50,860
John won an Academy Award.

20
00:01:51,528 --> 00:01:56,952
in 2007 for the visual effects work that he supervised in Pirates of the Caribbean, Dead

21
00:01:56,952 --> 00:01:57,672
Man's Chest.

22
00:01:57,672 --> 00:02:05,217
John is not only the visual effects supervisor for Rogue One, but he is also the creative

23
00:02:05,217 --> 00:02:07,719
mind behind the original idea for the film.

24
00:02:07,719 --> 00:02:14,824
Oh, and by the way, in 1987, John and his brother Thomas created something that you

25
00:02:14,824 --> 00:02:16,605
might be familiar with, Photoshop.

26
00:02:25,149 --> 00:02:27,270
John Mueller, everybody.

27
00:02:27,270 --> 00:02:30,611
Natty Hoffman is principal engineer and architect

28
00:02:30,611 --> 00:02:33,111
for rendering at the Lucasfilm Advanced Development Group.

29
00:02:33,111 --> 00:02:35,952
Natty has a long and storied career

30
00:02:35,952 --> 00:02:38,953
on the cutting edge of real-time computer graphics,

31
00:02:38,953 --> 00:02:43,554
from Westwood, Naughty Dog, Sony, Activision, 2K,

32
00:02:43,554 --> 00:02:45,815
and now Lucasfilm.

33
00:02:45,815 --> 00:02:50,076
Since around 2004, Natty has been spearheading

34
00:02:50,076 --> 00:02:51,757
the transition to practical.

35
00:02:53,780 --> 00:02:58,142
implementations of physically-based shading models

36
00:02:58,142 --> 00:03:00,424
across the games industry,

37
00:03:00,424 --> 00:03:04,586
which I'm sure many here are familiar with.

38
00:03:04,586 --> 00:03:09,149
Natty joined the Advanced Development Group at Lucasfilm

39
00:03:09,149 --> 00:03:12,191
one year ago in March of 2016.

40
00:03:12,191 --> 00:03:12,631
Uh... Natty, come on up.

41
00:03:12,631 --> 00:03:12,711
Uh...

42
00:03:12,711 --> 00:03:13,852
Me, I'm Roger Kordes,

43
00:03:13,852 --> 00:03:20,356
Digital Production Supervisor for the Advanced Development Group.

44
00:03:20,356 --> 00:03:22,077
I've been with Lucasfilm since 2010.

45
00:03:22,602 --> 00:03:25,463
I'm one of the founding members of the ADG.

46
00:03:25,463 --> 00:03:30,024
I was the lighting and look dev lead for Star Wars 1313.

47
00:03:30,024 --> 00:03:34,285
I am a real-time lighting and look dev specialist with a passion for generating high-fidelity

48
00:03:34,285 --> 00:03:34,685
images.

49
00:03:34,685 --> 00:03:38,266
One of my dreams is to match ILM quality in real-time.

50
00:03:38,266 --> 00:03:43,568
So this work that we're going to be talking about today was an absolute slam dunk for

51
00:03:43,568 --> 00:03:43,908
me.

52
00:03:43,908 --> 00:03:48,809
We should also introduce a couple of other concepts here.

53
00:03:48,809 --> 00:03:50,130
The concept of ILMxLAB.

54
00:03:51,107 --> 00:03:53,888
and of the Advanced Development Group.

55
00:03:53,888 --> 00:03:56,949
So in 2016, Kathy Kennedy firmly planted

56
00:03:56,949 --> 00:03:59,330
a Lucasfilm banner in the world of immersive entertainment

57
00:03:59,330 --> 00:04:00,931
with the launch of ILMxLAB.

58
00:04:00,931 --> 00:04:02,431
XLAB brings together the creative forces

59
00:04:02,431 --> 00:04:06,333
of the Lucasfilm Story Group, of Skywalker Sound,

60
00:04:06,333 --> 00:04:08,173
Industrial Light & Magic,

61
00:04:08,173 --> 00:04:11,875
and of the Lucasfilm Advanced Development Group

62
00:04:11,875 --> 00:04:13,115
with a mission to create new,

63
00:04:13,115 --> 00:04:17,737
premium immersive entertainment experiences

64
00:04:17,737 --> 00:04:19,357
that go beyond traditional film.

65
00:04:19,842 --> 00:04:20,803
and other linear media.

66
00:04:20,803 --> 00:04:28,549
As for the Advanced Development Group, we were founded in 2013 to be an innovation center

67
00:04:28,549 --> 00:04:32,473
within Lucasfilm focused on real-time rendering.

68
00:04:32,473 --> 00:04:38,558
Up next we'll show a few samples of the kinds of imagery that ADG Technology has helped

69
00:04:38,558 --> 00:04:39,378
to create.

70
00:04:39,775 --> 00:04:44,019
At its core, Lucasfilm is about story, so with that in mind, this first piece is a short

71
00:04:44,019 --> 00:04:49,865
cinematic that the Advanced Development Group put together in mid-2014 to show what we thought

72
00:04:49,865 --> 00:04:54,570
real-time rendering technology can bring to the creative world of storytelling.

73
00:05:17,832 --> 00:05:26,358
Report. According to our informants, a pair of rebel droids have attempted to make contact

74
00:05:26,358 --> 00:05:30,541
with the local underworld. They describe the droids as a golden protocol unit and a blue

75
00:05:30,541 --> 00:05:36,966
and white astromech. They may be attempting a rendezvous here at this outpost. Our ground

76
00:05:36,966 --> 00:05:42,009
forces have just engaged a transport that blasted its way out of Mos Eisley. Proceed

77
00:05:42,009 --> 00:05:44,711
but secure the perimeter first. Those droids must not escape.

78
00:05:52,761 --> 00:05:56,104
Wait for me!

79
00:06:03,800 --> 00:06:11,326
Hello, Captain Gator, Captain, um, Arto and I are ready for extraction and heading to the rendezvous point.

80
00:06:11,326 --> 00:06:12,908
I repeat, we are ready for extraction.

81
00:06:12,908 --> 00:06:13,889
Immediately!

82
00:06:13,889 --> 00:06:19,293
I've got Imperials all over me, but I'll do what I can. I've called in some backup.

83
00:06:19,293 --> 00:06:21,495
That doesn't sound very reassuring.

84
00:06:25,302 --> 00:06:30,124
They're everywhere, Arthur! Arthur! Oh, Arthur, leave me! I mean, Arthur, leave me!

85
00:06:30,124 --> 00:06:31,225
I'll save your soul!

86
00:06:31,225 --> 00:06:33,486
Isn't this how it all ends?

87
00:06:33,486 --> 00:06:43,651
Thanks, Earth-maker! We're saved!

88
00:06:43,651 --> 00:06:50,475
Stop right there!

89
00:06:53,154 --> 00:06:53,634
Oh no.

90
00:06:53,634 --> 00:06:54,335
Then, oh, thank you.

91
00:06:54,335 --> 00:07:10,108
So then last year, the Advanced Development Group developed Trials on Tatooine, a VR experience

92
00:07:10,108 --> 00:07:14,672
with which Lucasfilm and Industrial Light & Magic launched the ILMxLAB brand.

93
00:07:14,672 --> 00:07:16,573
And here we have the launch trailer.

94
00:08:04,874 --> 00:08:11,079
Then, this one, this is a high-fidelity rendering test that ADG did, which kind of leads us

95
00:08:11,079 --> 00:08:14,142
into what we'll be talking about today.

96
00:08:14,142 --> 00:08:19,186
So with those introductions out of the way, I'd like to turn the presentation over to

97
00:08:19,186 --> 00:08:25,591
John Knoll to talk to us about this specific real-time project and what it achieved for

98
00:08:25,591 --> 00:08:29,354
Rogue One, a Star Wars story.

99
00:08:32,932 --> 00:08:40,035
So my challenge to the advanced development group was to take this development and use it to create some finished work on a feature film

100
00:08:40,035 --> 00:08:45,958
The specific challenge was to take an ILM production asset, you know without completely rebuilding it

101
00:08:45,958 --> 00:08:53,022
Specifically for a game engine and rendering it using the real-time render at a quality level that I can put in the film

102
00:08:53,022 --> 00:09:00,866
Success story here is that the ADG shots are mixed right in with our render man renders and we have a short reel of k2so shots

103
00:09:01,199 --> 00:09:06,063
and it's a mixture of RenderMan renders and ADG renders.

104
00:09:06,063 --> 00:09:13,768
So, can you spot the difference?

105
00:09:13,768 --> 00:09:16,230
Our optimal route to the data vault

106
00:09:16,230 --> 00:09:20,113
places only 89 stormtroopers in our path.

107
00:09:20,113 --> 00:09:21,874
We will make it no more than 33% of the way

108
00:09:21,874 --> 00:09:23,475
before we are killed.

109
00:09:34,199 --> 00:09:37,161
We could transmit the plans to the rebel fleet.

110
00:09:37,161 --> 00:09:39,142
We'd have to get a signal out to tell them it's coming.

111
00:09:39,142 --> 00:09:42,564
It's the size of the data files, that's the problem.

112
00:09:42,564 --> 00:09:45,746
They'll never get through.

113
00:09:45,746 --> 00:09:54,632
Alright, so we'll play the reel again, but with ADG shots clearly labeled.

114
00:09:54,632 --> 00:09:57,613
Okay.

115
00:09:57,613 --> 00:10:01,596
Our optimal route to the data vault places only 89 Stormtroopers in our path.

116
00:10:01,957 --> 00:10:07,958
We will make it no more than 33% of the way before we are killed.

117
00:10:07,958 --> 00:10:18,982
We could transmit the plans to the rebel fleet.

118
00:10:18,982 --> 00:10:21,083
We'd have to get a signal up to tell them it's coming.

119
00:10:21,083 --> 00:10:23,263
It's the size of the data files.

120
00:10:23,263 --> 00:10:23,984
That's the problem.

121
00:10:23,984 --> 00:10:25,984
They'll never get through.

122
00:10:25,984 --> 00:10:26,504
All right.

123
00:10:26,504 --> 00:10:30,145
So Roger and Natty will present a technical breakdown

124
00:10:30,145 --> 00:10:31,326
on how this is all done.

125
00:10:31,326 --> 00:10:31,606
But...

126
00:10:31,872 --> 00:10:33,553
An important question is why.

127
00:10:33,553 --> 00:10:37,635
Why is real-time rendering important for Rogue One?

128
00:10:37,635 --> 00:10:42,198
And the answer is that in visual effects, the ability

129
00:10:42,198 --> 00:10:43,559
to iterate is crucial.

130
00:10:43,559 --> 00:10:45,920
Instant feedback and fast rendering mean more

131
00:10:45,920 --> 00:10:49,762
iterations, more creatively useful iterations per day.

132
00:10:49,762 --> 00:10:52,944
And a shorter iteration cycle gives us higher quality in

133
00:10:52,944 --> 00:10:55,125
fewer hours, and that's why we're chasing this.

134
00:10:55,125 --> 00:10:57,967
So about two years ago, and this is back during the

135
00:10:57,967 --> 00:11:01,249
production of Force Awakens, I'd seen some early real-time

136
00:11:01,822 --> 00:11:08,564
render tests out of the advanced development group that seemed extremely promising.

137
00:11:08,564 --> 00:11:14,426
In early 2015, ADG generated a version of the X-Wing shot that was in the original Thanksgiving

138
00:11:14,426 --> 00:11:16,667
trailer for Force Awakens.

139
00:11:16,667 --> 00:11:21,848
And ADG had rendered the X-Wing elements in seconds per frame, whereas the ILM software

140
00:11:21,848 --> 00:11:26,970
renders for the actual teaser were multiple hours per frame.

141
00:11:27,567 --> 00:11:33,828
X-Wings were then put through the exact same compositing step and the results were pretty

142
00:11:33,828 --> 00:11:34,108
good.

143
00:11:34,108 --> 00:11:37,069
The true versions don't look exactly the same.

144
00:11:37,069 --> 00:11:37,909
They're not identical.

145
00:11:37,909 --> 00:11:41,410
And there are a number of cheats in the ADG shot.

146
00:11:41,410 --> 00:11:47,351
For example, the aerial lights are crude approximations and the occlusion calculation is a little

147
00:11:47,351 --> 00:11:48,652
bit different.

148
00:11:48,652 --> 00:11:52,413
But generally, when I saw this, I felt like, all right, well, this is something we can

149
00:11:52,413 --> 00:11:52,873
work with.

150
00:11:54,547 --> 00:12:01,092
Then last year ADG hired Nettie Hoffman and trusted the future of Lucasfilm's real-time

151
00:12:01,092 --> 00:12:08,477
rendering to him and that gave me even further confidence that this is a direction to pursue.

152
00:12:08,477 --> 00:12:14,822
So we've established ILMxLAB and the ADG as the place within the company where ILM CG

153
00:12:14,822 --> 00:12:20,967
artists sit right alongside game industry talent to build these exciting new things.

154
00:12:20,967 --> 00:12:23,889
That's exactly what I needed for this project.

155
00:12:24,765 --> 00:12:28,748
So the challenge to ADG was to take this idea

156
00:12:28,748 --> 00:12:32,671
of rendering production quality images

157
00:12:32,671 --> 00:12:34,812
out of experiment and testing.

158
00:12:34,812 --> 00:12:37,954
I thought what I'd seen was promising enough that,

159
00:12:37,954 --> 00:12:41,796
all right, well can we, this looks like we've hit a bar

160
00:12:41,796 --> 00:12:43,638
that this could go into a feature film.

161
00:12:43,638 --> 00:12:46,159
Let's take it out of theory and into practice.

162
00:12:46,159 --> 00:12:46,960
Let's do a shot.

163
00:12:46,960 --> 00:12:48,681
So the,

164
00:12:51,748 --> 00:12:58,111
I was pretty upfront that we had to hit a level of quality that was as good as the RenderMan

165
00:12:58,111 --> 00:12:58,752
renders.

166
00:12:58,752 --> 00:13:03,654
If it didn't hit that bar, then we couldn't use it.

167
00:13:03,654 --> 00:13:10,157
But I was pretty happy with how that turned out.

168
00:13:10,157 --> 00:13:13,518
I think it succeeded pretty well.

169
00:13:13,518 --> 00:13:17,760
So talking to ADG about what asset we could try this with.

170
00:13:19,250 --> 00:13:26,893
You know, they were looking, their ideal case was a hard surface asset, opaque, pretty much

171
00:13:26,893 --> 00:13:32,455
like the X-Wings, and a perfect candidate for that was our K2SO character.

172
00:13:32,455 --> 00:13:39,217
He's going to be a hard surface asset, he's going to be CG all the way through, and some

173
00:13:39,217 --> 00:13:46,059
of the more difficult shading challenges aren't actually present in the character, so it would

174
00:13:46,059 --> 00:13:47,620
be a good test case to do.

175
00:13:48,533 --> 00:13:53,616
And with the unified asset standard having been designed

176
00:13:53,616 --> 00:13:55,717
and come into fruition between ILM and ADG

177
00:13:55,717 --> 00:13:57,518
during the production of Force Awakens,

178
00:13:57,518 --> 00:14:02,621
it seemed like everything was in place

179
00:14:02,621 --> 00:14:04,882
to give them a crack at this.

180
00:14:04,882 --> 00:14:08,304
And so the first step in this process

181
00:14:08,304 --> 00:14:10,925
was to mirror a shot that was being done

182
00:14:10,925 --> 00:14:12,126
through the traditional pipeline.

183
00:14:12,126 --> 00:14:14,167
So we had a shot that was being lit

184
00:14:14,167 --> 00:14:17,269
by an ILM lighting TD and render man.

185
00:14:17,579 --> 00:14:20,361
That kind of set the visual bar, the look.

186
00:14:20,361 --> 00:14:23,922
If we could hit this, then we could do another shot

187
00:14:23,922 --> 00:14:28,925
essentially without a safety net, without a backup plan.

188
00:14:28,925 --> 00:14:32,707
So we proceeded in parallel with the shot.

189
00:14:32,707 --> 00:14:34,948
And once we got to a place where I felt like

190
00:14:34,948 --> 00:14:36,889
we could final that element,

191
00:14:36,889 --> 00:14:40,131
then they proceeded with doing these shots

192
00:14:40,131 --> 00:14:41,912
with no backup plan.

193
00:14:41,912 --> 00:14:46,395
I'll also say, besides for final shot production,

194
00:14:47,077 --> 00:14:50,157
there's an important use of real-time rendering

195
00:14:50,157 --> 00:14:51,577
just working with actors.

196
00:14:51,577 --> 00:14:59,459
So in pre-production, we had this very sort of crude

197
00:14:59,459 --> 00:15:03,179
early version of K2SO that ran in real time

198
00:15:03,179 --> 00:15:04,880
on our motion capture stage.

199
00:15:04,880 --> 00:15:06,780
We had Alan come in for a day,

200
00:15:06,780 --> 00:15:08,640
and since the character's very tall,

201
00:15:08,640 --> 00:15:09,980
he was standing on stilts.

202
00:15:09,980 --> 00:15:13,781
And so he had had a little time to get used to the stilts,

203
00:15:13,781 --> 00:15:16,642
and then this was a chance for him to.

204
00:15:16,912 --> 00:15:19,393
get on the stage and sort of puppeteer the character.

205
00:15:19,393 --> 00:15:21,253
And he can watch himself in real time

206
00:15:21,253 --> 00:15:24,875
while he's seeing what feels right on the character.

207
00:15:24,875 --> 00:15:30,017
You know, is it, do you do more or less arm swinging?

208
00:15:30,017 --> 00:15:31,977
You know, how loose is your body?

209
00:15:31,977 --> 00:15:34,158
What feels right with the character?

210
00:15:34,158 --> 00:15:37,780
And while this is, you know, a very crude GL,

211
00:15:37,780 --> 00:15:42,401
because this was done, you know, way in pre-production

212
00:15:42,401 --> 00:15:45,082
before we had the final K2SO asset built,

213
00:15:48,063 --> 00:15:50,104
The same asset that we used for the film,

214
00:15:50,104 --> 00:15:51,726
we could have flipped the equation.

215
00:15:51,726 --> 00:15:54,347
You know, the constraint for the film was

216
00:15:54,347 --> 00:15:56,669
we had to have very high visual fidelity,

217
00:15:56,669 --> 00:15:57,390
can't have any noise,

218
00:15:57,390 --> 00:15:59,551
you have to have really smooth motion blur,

219
00:15:59,551 --> 00:16:01,853
can't be any visible artifacts,

220
00:16:01,853 --> 00:16:04,015
but I don't need it to run at 60 frames a second,

221
00:16:04,015 --> 00:16:06,957
so I can afford longer render times.

222
00:16:06,957 --> 00:16:09,279
But for this purpose, we could just flip that

223
00:16:09,279 --> 00:16:11,540
and impose it's gotta run at speed,

224
00:16:11,540 --> 00:16:17,104
and we could live with a little bit of noise in the renders.

225
00:16:17,104 --> 00:16:17,645
So we could.

226
00:16:18,225 --> 00:16:22,987
And in future, using some of this new ADG engine,

227
00:16:22,987 --> 00:16:26,449
I think it'll make this experience even better.

228
00:16:26,449 --> 00:16:34,092
And with that, I think I'll turn it back over to Roger.

229
00:16:34,092 --> 00:16:37,994
So what does it actually mean to work

230
00:16:37,994 --> 00:16:39,775
on a shot for an ILM production?

231
00:16:39,775 --> 00:16:45,298
Well, it means that we will consume assets

232
00:16:45,298 --> 00:16:46,258
from the ILM pipeline.

233
00:16:46,902 --> 00:16:52,226
It means that we will artistically light the shot.

234
00:16:52,226 --> 00:16:56,409
And then we will output renders

235
00:16:56,409 --> 00:16:59,652
that are consumable by the compositors.

236
00:16:59,652 --> 00:17:03,395
So let's break that down a little further.

237
00:17:03,395 --> 00:17:05,237
What do we mean by asset?

238
00:17:05,237 --> 00:17:06,918
That's kind of a loaded term too.

239
00:17:06,918 --> 00:17:11,262
There are many, many, many types of assets.

240
00:17:11,262 --> 00:17:14,644
And each type of asset is often represented

241
00:17:14,644 --> 00:17:16,586
by many, many, many individual files.

242
00:17:17,624 --> 00:17:23,708
So, for this Rogue One work specifically, the assets and file types that we're dealing

243
00:17:23,708 --> 00:17:34,134
with are animated geometry caches, paint and look dev data, cameras, background plates,

244
00:17:34,134 --> 00:17:41,639
virtual set geometry, and captured HDRI lighting spheres from set.

245
00:17:46,109 --> 00:17:52,972
As an ILM CG asset, K2SO himself comes to us in two primary components, the geometry

246
00:17:52,972 --> 00:17:59,175
stored in an Alembic container and the paint and lookdev data represented in a MaterialX

247
00:17:59,175 --> 00:17:59,935
description.

248
00:17:59,935 --> 00:18:09,339
Alembic is an extremely powerful and extensible open standard for transporting geometry between

249
00:18:09,339 --> 00:18:14,282
software packages, born out of a partnership between Industrial Light and Magic and Sony

250
00:18:14,282 --> 00:18:14,782
Imageworks.

251
00:18:16,495 --> 00:18:20,277
If you're curious about Alembic, you can learn more at alembic.io.

252
00:18:20,277 --> 00:18:25,639
MaterialX is a new open standard that has come out of the Advanced Development Group

253
00:18:25,639 --> 00:18:31,742
and Industrial Light & Magic, which allows material and texture information to be transported

254
00:18:31,742 --> 00:18:34,323
between different software packages and renderers.

255
00:18:34,323 --> 00:18:39,906
MaterialX is a key component of our Unified Assets standards.

256
00:18:39,906 --> 00:18:43,928
It allows us to transfer unified shading descriptions.

257
00:18:44,582 --> 00:18:49,027
between all of our packages, between paint packages like Mari,

258
00:18:49,027 --> 00:18:54,933
lookdev packages like Katana, other DCCs such as Maya or our own internal tool Xeno,

259
00:18:54,933 --> 00:19:00,459
and even other platforms entirely like game engines.

260
00:19:00,459 --> 00:19:04,183
If you're interested in MaterialX, please check out materialx.org.

261
00:19:04,183 --> 00:19:07,627
Adding support for MaterialX and Alembic.

262
00:19:08,120 --> 00:19:10,500
into the ADG version of the Unreal Engine

263
00:19:10,500 --> 00:19:12,981
was one of the major technical undertakings

264
00:19:12,981 --> 00:19:15,281
of this Rogue One project,

265
00:19:15,281 --> 00:19:17,142
which Natty will go into more detail there,

266
00:19:17,142 --> 00:19:19,502
but the key takeaway here is that

267
00:19:19,502 --> 00:19:23,303
because these standards are both so easily extendable,

268
00:19:23,303 --> 00:19:25,344
we were able to take assumptions

269
00:19:25,344 --> 00:19:27,884
that are baked into ILM's production implementations

270
00:19:27,884 --> 00:19:32,546
and reformat the asset data to be suitable

271
00:19:32,546 --> 00:19:34,226
for a real-time case like Unreal.

272
00:19:36,735 --> 00:19:41,478
So by the time ADG got our first K2SO turnover,

273
00:19:41,478 --> 00:19:44,060
the asset was basically complete at ILM.

274
00:19:44,060 --> 00:19:47,642
We had gone from these paintings

275
00:19:47,642 --> 00:19:50,264
to Landis had made this digital asset.

276
00:19:50,264 --> 00:19:54,047
Here we are looking at the control cage

277
00:19:54,047 --> 00:19:57,829
for a creased subdivision surface.

278
00:19:57,829 --> 00:20:00,491
There are 600,000 some odd vertexes

279
00:20:00,491 --> 00:20:03,013
in the control cage alone,

280
00:20:03,013 --> 00:20:04,934
with edge creases that go up to level four.

281
00:20:06,249 --> 00:20:10,150
So that means that for an extreme close-up on one of those creased edges,

282
00:20:10,150 --> 00:20:13,291
if you were to uniformly subdivide

283
00:20:13,291 --> 00:20:16,733
the entire cage, you'll end up with 600,000 times 4 to the 4th, or

284
00:20:16,733 --> 00:20:20,874
somewhere in the neighborhood of 154 million vertexes required in order to

285
00:20:20,874 --> 00:20:22,955
properly hold those creased edges.

286
00:20:22,955 --> 00:20:25,856
Fortunately, none of the shots that we were dealing with

287
00:20:25,856 --> 00:20:30,557
required this extreme case. We also have

288
00:20:30,557 --> 00:20:33,478
1,700 separate geometries in the animation hierarchy.

289
00:20:34,407 --> 00:20:39,010
and 63 UDIMs across 10 texture effects,

290
00:20:39,010 --> 00:20:45,595
with most of those textures stored at 4K resolution,

291
00:20:45,595 --> 00:20:49,597
all driving LookDev in the unified shading model

292
00:20:49,597 --> 00:20:51,178
shared between Lucasfilm and ILM,

293
00:20:51,178 --> 00:20:52,399
which we call Unified Surf.

294
00:20:52,399 --> 00:20:56,982
So each and every component of K2SO,

295
00:20:56,982 --> 00:20:59,504
every little piston and ring fitting,

296
00:20:59,504 --> 00:21:01,985
each of them can and does animate.

297
00:21:03,710 --> 00:21:07,552
This is the beauty for us of the Olympic cache.

298
00:21:07,552 --> 00:21:09,373
We do not have to be able to evaluate

299
00:21:09,373 --> 00:21:12,235
that animation control rig at runtime.

300
00:21:12,235 --> 00:21:14,777
When we are rendering in the game engine,

301
00:21:14,777 --> 00:21:19,059
we just consume the transforms from a baked geometry cache.

302
00:21:19,059 --> 00:21:22,241
Here is another view where you can see in his shoulders

303
00:21:22,241 --> 00:21:26,604
all the little pistons, how they move from his arms moving,

304
00:21:26,604 --> 00:21:30,567
and his knees and wrists and elbows,

305
00:21:30,567 --> 00:21:32,128
every little bit inside.

306
00:21:33,190 --> 00:21:34,490
has moving parts.

307
00:21:34,490 --> 00:21:36,950
As I mentioned a moment ago

308
00:21:36,950 --> 00:21:40,211
K2's MaterialX LookDev data

309
00:21:40,211 --> 00:21:47,872
has textures for driving tan effects in the unified shader.

310
00:21:47,872 --> 00:21:55,234
That includes texture data for a second specular lobe.

311
00:21:55,234 --> 00:21:56,014
And actually a third.

312
00:21:56,014 --> 00:21:59,695
But at the time of this Rogue One work

313
00:22:00,740 --> 00:22:03,481
ADG's real-time implementation of Unified Surf

314
00:22:03,481 --> 00:22:05,862
only supported a single specular lobe.

315
00:22:05,862 --> 00:22:08,764
So for the shots that ADG contributed to the film,

316
00:22:08,764 --> 00:22:11,506
the lighting and look dev TD, Justin Schubert,

317
00:22:11,506 --> 00:22:13,807
had to dial in a custom specular look 4K2

318
00:22:13,807 --> 00:22:16,768
that mixes the spec one and spec two effects.

319
00:22:16,768 --> 00:22:19,270
John wasn't exactly thrilled about this,

320
00:22:19,270 --> 00:22:21,631
but the results looked good enough,

321
00:22:21,631 --> 00:22:23,872
and so we powered through.

322
00:22:23,872 --> 00:22:25,753
Here we see a comparison between

323
00:22:25,753 --> 00:22:28,355
the two lobe RenderMan renders on the left.

324
00:22:28,735 --> 00:22:31,316
and the results of Justin's efforts on the ADG render

325
00:22:31,316 --> 00:22:31,776
on the right.

326
00:22:31,776 --> 00:22:34,817
I mean, obviously, it's not the same shot

327
00:22:34,817 --> 00:22:39,098
and not the same lighting setup, so we get away with it, right?

328
00:22:39,098 --> 00:22:46,359
So now that we have the K2 asset, the CG asset for K2

329
00:22:46,359 --> 00:22:50,360
himself, we've brought in the Alembic of the camera.

330
00:22:50,360 --> 00:22:53,541
We have the EXR textures of the background plates

331
00:22:53,541 --> 00:22:56,362
and of the lighting spheres imported into Unreal

332
00:22:56,362 --> 00:22:58,062
as HDRI lighting environments.

333
00:22:59,130 --> 00:23:03,112
Now we can get to work actually lighting the shot.

334
00:23:03,112 --> 00:23:08,356
So here we have some screen capture footage of a contrived version of an interactive lighting session,

335
00:23:08,356 --> 00:23:13,699
because we didn't actually capture any of this while we were doing the work.

336
00:23:13,699 --> 00:23:17,762
So you can see what the interactive lighting session would look like.

337
00:23:17,762 --> 00:23:21,464
I should mention here that for games industry veterans such as yourselves,

338
00:23:21,464 --> 00:23:26,748
real-time lighting is something that we kind of take for granted, right?

339
00:23:27,150 --> 00:23:32,273
But real-time light and shadow feedback on a full resolution, high fidelity Industrial

340
00:23:32,273 --> 00:23:38,376
Light and Magic asset with all 63 UDEMs of 4K textures and however many millions of vertexes

341
00:23:38,376 --> 00:23:45,140
slammed into the GPU, seeing this kind of real-time light and shadow feedback was revelatory

342
00:23:45,140 --> 00:23:46,521
for ILM lighting artists.

343
00:23:46,521 --> 00:23:54,805
I should also mention that at Industrial Light and Magic, you know, the concept of CG lighting,

344
00:23:54,805 --> 00:23:56,206
that means area lighting.

345
00:23:57,207 --> 00:24:03,688
And areolites means textured areolites as in a high dynamic range photograph of a

346
00:24:03,688 --> 00:24:10,750
Taken on location of a actual physical light source that was illuminating actors and props and sets

347
00:24:10,750 --> 00:24:15,411
At ADG we have some approximated areoliting shaders for surface shading

348
00:24:15,411 --> 00:24:21,713
We have basic rectangles and disks and also the standard unreal spheres and pills

349
00:24:21,713 --> 00:24:23,233
But we don't have textured areolites

350
00:24:23,792 --> 00:24:31,116
One important addition also is that area lights at ILM means area shadows, and that one is an even tougher pill to swallow.

351
00:24:31,116 --> 00:24:45,203
Once again, Justin was able to hack his way to the visual result that satisfied production for Rogue One by dialing in a tunable filter radius on standard PCF shadow maps.

352
00:24:45,203 --> 00:24:50,405
At this point, I'll pass it off to Natty to dive into some technical details of rendering.

353
00:24:57,935 --> 00:24:58,975
Thanks, Roger.

354
00:24:58,975 --> 00:25:02,457
So Unreal Engine now has some support for loading Alembic

355
00:25:02,457 --> 00:25:05,299
files, but we needed an implementation that could

356
00:25:05,299 --> 00:25:06,639
handle ILM Alembic files.

357
00:25:06,639 --> 00:25:09,981
As Roger touched on earlier, Alembic is a very flexible and

358
00:25:09,981 --> 00:25:12,863
extensible format, and ILM took full advantage of this

359
00:25:12,863 --> 00:25:15,604
extensibility and has extended it quite far.

360
00:25:15,604 --> 00:25:18,786
One of the ADG engineers, Ron Radetzky, did significant work

361
00:25:18,786 --> 00:25:22,288
to enable our pipeline to take in this highly extended Alembic

362
00:25:22,288 --> 00:25:25,510
format and turn it into vertex buffers that could be directly

363
00:25:25,510 --> 00:25:27,291
uploaded and consumed by the GPU.

364
00:25:30,392 --> 00:25:37,557
After animation happens in the ILM pipeline, a bake take is cached out in an Olympic file.

365
00:25:37,557 --> 00:25:40,619
This is a vertex cache of that final animated result.

366
00:25:40,619 --> 00:25:44,121
The bake take is a pure geometry cache.

367
00:25:44,121 --> 00:25:45,222
It's baked every frame.

368
00:25:45,222 --> 00:25:50,186
It has no information on the rig or deformers or anything like that.

369
00:25:50,186 --> 00:25:51,607
It's all baked out for that one frame.

370
00:26:00,204 --> 00:26:03,706
K2SO is mostly rigid.

371
00:26:03,706 --> 00:26:08,469
And so most of that cache is rigid transforms.

372
00:26:08,469 --> 00:26:13,892
There's a small amount of bendy bits, basically, as antenna.

373
00:26:13,892 --> 00:26:16,534
They are very small compared to the rest of them.

374
00:26:16,534 --> 00:26:19,076
And in these particular shots, I'm not even

375
00:26:19,076 --> 00:26:20,196
sure they animated at all.

376
00:26:20,196 --> 00:26:23,298
So you could basically think of it

377
00:26:23,298 --> 00:26:25,780
as a very complex rigid setup with a lot of rigid pieces.

378
00:26:28,728 --> 00:26:33,270
ILM geometry caches contain subdivision surfaces as well as traditional polygon meshes.

379
00:26:33,270 --> 00:26:35,170
Saron's implementation had to deal with both.

380
00:26:35,170 --> 00:26:39,031
For the subdivision surfaces, at the time of the Rogue One work, we determined that

381
00:26:39,031 --> 00:26:44,613
we only needed to render at subdivision level 2 in order to hit the visual quality target.

382
00:26:44,613 --> 00:26:49,934
We did some tests at level 3, but for these specific shots and cameras, level 2 was quite

383
00:26:49,934 --> 00:26:50,314
sufficient.

384
00:26:50,314 --> 00:26:54,855
This allowed us to pre-subdivide the entire mesh in main memory.

385
00:26:55,380 --> 00:26:56,981
And this was the most straightforward.

386
00:26:56,981 --> 00:26:59,022
It wasn't the most elegant or the most performant way,

387
00:26:59,022 --> 00:27:02,364
but it was definitely the most straightforward way

388
00:27:02,364 --> 00:27:04,525
to shim this data into Unreal,

389
00:27:04,525 --> 00:27:06,847
given the film schedule that we had to work with.

390
00:27:06,847 --> 00:27:08,868
Now, all of this subdivision surface rendering

391
00:27:08,868 --> 00:27:10,669
is built on a foundation of the open standard

392
00:27:10,669 --> 00:27:12,229
from Pixar, OpenSubdiv.

393
00:27:12,229 --> 00:27:14,911
For Rogue One, we did subdivision on the CPU.

394
00:27:14,911 --> 00:27:18,453
OpenSubdiv does have a GPU implementation

395
00:27:18,453 --> 00:27:19,233
and we took a look at it,

396
00:27:19,233 --> 00:27:21,575
but we found that it has some problem cases

397
00:27:21,575 --> 00:27:22,875
with UV texture coordinate data.

398
00:27:23,342 --> 00:27:26,523
and it doesn't work well in ILM assets like K2SO.

399
00:27:26,523 --> 00:27:28,024
In the Epic keynote this morning,

400
00:27:28,024 --> 00:27:30,444
Epic announced open subdiv support in Unreal Engine.

401
00:27:30,444 --> 00:27:32,405
This implementation, of course, didn't exist

402
00:27:32,405 --> 00:27:33,965
when this work was ongoing,

403
00:27:33,965 --> 00:27:35,846
but we look forward to evaluating it

404
00:27:35,846 --> 00:27:37,527
for possible use in future projects.

405
00:27:37,527 --> 00:27:40,087
Rendering K2SO uniformly subdivided level two

406
00:27:40,087 --> 00:27:44,869
was quite stressful for even our most powerful machines.

407
00:27:44,869 --> 00:27:46,529
As Roger pointed out earlier,

408
00:27:46,529 --> 00:27:48,850
every subdivision level is multiplying

409
00:27:48,850 --> 00:27:50,070
the polygon count by four.

410
00:27:53,415 --> 00:27:58,178
So Ron, who was working on the Alembic and subdivision part of

411
00:27:58,178 --> 00:28:00,139
the engine, thought there must be a smarter

412
00:28:00,139 --> 00:28:00,840
way to go about this.

413
00:28:00,840 --> 00:28:04,242
So in parallel to the Rogue One work, Ron developed a

414
00:28:04,242 --> 00:28:07,404
streaming Alembic solution that did not require

415
00:28:07,404 --> 00:28:09,785
subdividing everything up in memory ahead of time.

416
00:28:09,785 --> 00:28:13,267
And he also developed a curvature-based recursive

417
00:28:13,267 --> 00:28:15,668
reduction algorithm for removing redundant spans on

418
00:28:15,668 --> 00:28:16,829
subdivided surfaces.

419
00:28:16,829 --> 00:28:20,711
As you can see in the slide, the end result is quite a bit

420
00:28:20,711 --> 00:28:20,992
simpler.

421
00:28:21,612 --> 00:28:26,254
Using the redundant span-loop reduction method, Ron is able to render a version of K2 that

422
00:28:26,254 --> 00:28:31,297
is visually indistinguishable from the full level 3 tessellation, all while maintaining

423
00:28:31,297 --> 00:28:33,119
a constant 60 Hz frame rate.

424
00:28:33,119 --> 00:28:39,402
While we did not get to take advantage of this on the Rogue One work, we continue to

425
00:28:39,402 --> 00:28:40,643
enjoy this advancement for our current projects.

426
00:28:40,643 --> 00:28:46,226
In addition to subdivision surfaces, another facet of our Alembic geometry cache consumption

427
00:28:46,226 --> 00:28:49,288
is handling the motion blur time samples, which is a

428
00:28:50,232 --> 00:28:53,292
specific aspect of ILM's flavor of Alembic.

429
00:28:53,292 --> 00:28:56,113
ILM's geometry caches include motion blur samples

430
00:28:56,113 --> 00:28:57,834
at three times for each frame,

431
00:28:57,834 --> 00:28:59,334
when the camera shutter opens,

432
00:28:59,334 --> 00:29:01,114
when the camera shutter closes,

433
00:29:01,114 --> 00:29:02,315
and the time in the middle.

434
00:29:02,315 --> 00:29:04,936
This includes full data for all the geometry,

435
00:29:04,936 --> 00:29:06,836
any cameras that might be moving,

436
00:29:06,836 --> 00:29:09,377
for each of these three points in time for each frame.

437
00:29:09,377 --> 00:29:12,037
Ron's implementation enables us

438
00:29:12,037 --> 00:29:13,538
to do temporal supersampling

439
00:29:13,538 --> 00:29:15,778
by interpolating the camera and geometry positions

440
00:29:15,778 --> 00:29:18,279
in between those three subframe samples.

441
00:29:18,778 --> 00:29:21,020
For the Rogue One shots, we rendered out images at 64

442
00:29:21,020 --> 00:29:23,162
different times spread over each frame.

443
00:29:23,162 --> 00:29:26,965
This is, of course, computationally quite heavy,

444
00:29:26,965 --> 00:29:29,647
but we needed to do this if we wanted to get motion blur

445
00:29:29,647 --> 00:29:30,267
that was up to ILM standards.

446
00:29:30,267 --> 00:29:33,010
We get some additional use out of these renders by also

447
00:29:33,010 --> 00:29:36,593
jittering the camera at the same time using a Halton

448
00:29:36,593 --> 00:29:37,313
sequence.

449
00:29:37,313 --> 00:29:40,796
This also gets a spatial super sampling at the same cost.

450
00:29:40,796 --> 00:29:43,318
We have some ideas for applying these samples in

451
00:29:43,318 --> 00:29:45,380
another dimension, for example, aerial light sources

452
00:29:45,380 --> 00:29:46,161
as well in the future.

453
00:29:49,119 --> 00:29:52,700
Before we talk about rendering, we should talk about compositing, which is the process

454
00:29:52,700 --> 00:29:57,862
where the final image is actually put together by a specialized CG artist, a digital compositor.

455
00:29:57,862 --> 00:30:02,324
It bears some resemblances to the post-effects passes that we get in game renders.

456
00:30:02,324 --> 00:30:10,126
Various blurs, blings, blooms, flares, all these effects happen in comp, or in the composition

457
00:30:10,126 --> 00:30:11,006
phase, composite phase.

458
00:30:11,006 --> 00:30:15,108
And this would be no different for our shots, just like every other shot in the film.

459
00:30:17,037 --> 00:30:21,918
So we needed to be able to get our ADG renders output and hand it over in a way that is seamless

460
00:30:21,918 --> 00:30:23,198
and transparent to the compositor.

461
00:30:23,198 --> 00:30:27,859
At ILM, TD renders are handed off to the compositor in open EXR format.

462
00:30:27,859 --> 00:30:31,660
While Unreal does come off the shelf with EXR output capability, in that feature, the

463
00:30:31,660 --> 00:30:36,921
image is written before certain post-process effects happen, and those include, unfortunately,

464
00:30:36,921 --> 00:30:38,461
some lighting features.

465
00:30:38,461 --> 00:30:42,402
And we needed to output the full lid and shaded beauty render in an EXR container.

466
00:30:42,402 --> 00:30:46,063
That was a fairly trivial change to make in our version of Unreal Engine.

467
00:30:48,545 --> 00:30:53,006
So we render motion blur, but the depth-based defocus blur

468
00:30:53,006 --> 00:30:55,067
occurs in the comp phase.

469
00:30:55,067 --> 00:30:57,808
And this is true also for the ILM renders.

470
00:30:57,808 --> 00:31:00,728
We knew that we needed to have an extremely robust motion

471
00:31:00,728 --> 00:31:03,929
blur result in order to pass muster at John.

472
00:31:03,929 --> 00:31:07,410
We also needed to be able to output depth so that the

473
00:31:07,410 --> 00:31:09,270
compositor could have control over the depth of field in the

474
00:31:09,270 --> 00:31:09,790
final output.

475
00:31:09,790 --> 00:31:13,531
The concept of arbitrary output variables is a staple

476
00:31:13,531 --> 00:31:15,892
of offline rendering and compositing at ILM and at

477
00:31:15,892 --> 00:31:17,312
other feature film houses.

478
00:31:18,739 --> 00:31:24,343
These AOVs are output by the renderer, and they are used during the compositing phase.

479
00:31:24,343 --> 00:31:28,206
On the surface, these images are somewhat similar to G-buffers that you might have in

480
00:31:28,206 --> 00:31:30,868
a deferred shading engine.

481
00:31:30,868 --> 00:31:38,113
One key difference between G-buffers and the AOVs, the A stands for and really does mean

482
00:31:38,113 --> 00:31:38,894
arbitrary.

483
00:31:38,894 --> 00:31:41,616
These values can be stored from any point in the shading computation.

484
00:31:41,616 --> 00:31:46,079
It can be even some intermediate value that is only present during a particular sub-phase

485
00:31:46,079 --> 00:31:46,980
of the shading calculation.

486
00:31:47,824 --> 00:31:53,470
And to generate that kind of arbitrary output from any point of the shading math in a game

487
00:31:53,470 --> 00:31:57,554
engine render would require a lot of plumbing.

488
00:31:57,554 --> 00:32:00,657
So we identified a minimum set of AOVs that the compositor would need for our specific

489
00:32:00,657 --> 00:32:00,857
shots.

490
00:32:00,857 --> 00:32:06,162
Depth is one example of an arbitrary output variable that we were easily able to accommodate.

491
00:32:06,162 --> 00:32:10,947
Ideally, this exercise would result in less compositing work than traditional ILM shot.

492
00:32:11,500 --> 00:32:15,763
since something closer to the final result would be visible to the lighting artist interactively.

493
00:32:15,763 --> 00:32:19,365
So we were able to get away with three AOVs for most of our shots.

494
00:32:19,365 --> 00:32:24,128
The beauty RGBA, which is basically the final rendered color.

495
00:32:24,128 --> 00:32:27,150
We have the depth, which I mentioned earlier.

496
00:32:27,150 --> 00:32:29,792
And an object ID buffer.

497
00:32:29,792 --> 00:32:30,633
Some shots or AOV.

498
00:32:30,633 --> 00:32:35,396
A few shots required a fourth AOV, which was an emissive matte.

499
00:32:38,041 --> 00:32:40,422
Rogue One was an ACEScg show at ILM.

500
00:32:40,422 --> 00:32:45,264
At the time that this work was done with an ADG, our version of Unreal was still limited

501
00:32:45,264 --> 00:32:46,925
to the sRGB or REC709 color gamut.

502
00:32:46,925 --> 00:32:52,307
This was another area where ADG and ILM had some creative back and forth in order to ensure

503
00:32:52,307 --> 00:32:57,929
that the REC709 renders out of ADG fit correctly into the Rogue One ACEScg color pipeline.

504
00:32:57,929 --> 00:32:58,049
So

505
00:33:02,341 --> 00:33:05,742
Now that we are able to ingest the assets and do creative work interactively, and we

506
00:33:05,742 --> 00:33:09,844
have some baseline capability in place for all of these various technical features, we're

507
00:33:09,844 --> 00:33:10,544
ready to render.

508
00:33:10,544 --> 00:33:14,686
But what does rendering for feature film visual effects actually mean?

509
00:33:14,686 --> 00:33:19,268
To quote ILM visual effects supervisor Ben Snow, in computer graphics, a lot of the time

510
00:33:19,268 --> 00:33:22,950
we're trying to reproduce the reality that the viewer sees with their own eyes in the

511
00:33:22,950 --> 00:33:26,891
world around them, but with visual effects or film, we're really trying to reproduce

512
00:33:26,891 --> 00:33:28,532
filmed reality, which is a little bit different.

513
00:33:30,450 --> 00:33:35,732
So what does it mean to make things look like a photograph or like a frame from a film?

514
00:33:35,732 --> 00:33:40,114
Well, arguably one of the most significant components is to have absolutely no visible

515
00:33:40,114 --> 00:33:40,814
aliasing.

516
00:33:40,814 --> 00:33:46,316
The ILM render resolution for Rogue One was 4K, but for us to be able to kill all aliasing

517
00:33:46,316 --> 00:33:51,738
in our ADG renders, we had to use a combination of the jittered supersampling that I described

518
00:33:51,738 --> 00:33:56,140
earlier and uniform supersampling, effectively rendering the frame at a higher resolution.

519
00:33:56,651 --> 00:34:02,257
So for our final output for compositing into the movie, we

520
00:34:02,257 --> 00:34:05,600
ended up rendering at 9K in addition to the 64 jittered

521
00:34:05,600 --> 00:34:07,122
subpixel samples I mentioned.

522
00:34:07,122 --> 00:34:10,565
Here we see a portion of our beauty pass for this frame at

523
00:34:10,565 --> 00:34:12,747
a one to one pixel scale.

524
00:34:12,747 --> 00:34:17,152
Even with all of those samples, our shots were rendering at

525
00:34:17,152 --> 00:34:18,793
just about exactly one minute per frame.

526
00:34:19,460 --> 00:34:21,701
that render time is due to several factors.

527
00:34:21,701 --> 00:34:23,402
First of all, it was 9K resolution,

528
00:34:23,402 --> 00:34:26,243
and it was rendering effectively 64 times

529
00:34:26,243 --> 00:34:28,465
or 64 subsamples per pixel.

530
00:34:28,465 --> 00:34:31,786
And also, a significant portion of the frame time

531
00:34:31,786 --> 00:34:34,148
was due to massive file IOM and memory transfers,

532
00:34:34,148 --> 00:34:37,810
since we're talking about extremely large output files,

533
00:34:37,810 --> 00:34:41,292
and we're talking also about extremely large input files

534
00:34:41,292 --> 00:34:41,872
in the case of Alembic.

535
00:34:41,872 --> 00:34:46,154
This shot is one of the first tests that we generated

536
00:34:46,154 --> 00:34:47,935
in order to show John what the results

537
00:34:47,935 --> 00:34:49,316
of a real-time render could look like.

538
00:34:49,876 --> 00:34:53,298
These renders were pretty far from real time, of course, at about a minute per frame.

539
00:34:53,298 --> 00:34:56,800
But compared to multiple hours per frame, which the RenderMan renders were taking, that's

540
00:34:56,800 --> 00:34:58,341
still quite a win.

541
00:34:58,341 --> 00:35:01,864
This is an example of a shot that was also done at ILM.

542
00:35:01,864 --> 00:35:05,006
So our work in this case was to match the ILM result.

543
00:35:05,006 --> 00:35:07,527
The ILM lighting setup used only three light sources.

544
00:35:07,527 --> 00:35:11,610
There was a rectangular area light for the ceiling panel, another rectangle for the

545
00:35:11,610 --> 00:35:14,212
back wall panel, and an overall HDRI sphere.

546
00:35:14,212 --> 00:35:17,013
Roger, who lit this shot on the ADG side...

547
00:35:17,698 --> 00:35:22,502
managed to stay true to that using our approximated rectangular area lights and fake soft shadows.

548
00:35:22,502 --> 00:35:27,345
Screen space ambideclusion was our only shadowing factor for the HDRI sphere.

549
00:35:27,345 --> 00:35:31,067
K2's eyes are a complex set of refractive lenses.

550
00:35:31,067 --> 00:35:35,610
We didn't support the unified surf refraction model for transparent rendering in our real

551
00:35:35,610 --> 00:35:39,812
time version of the unified shader, so the eyes proved a bit problematic for us.

552
00:35:39,812 --> 00:35:44,795
We ended up offering a custom material and we did a custom pass and

553
00:35:46,383 --> 00:35:50,226
and ended up rendering it separately and we got results that were close enough.

554
00:35:50,226 --> 00:35:55,871
Lighting TD Justin Schubert took over lighting duties on the ADG side starting with this

555
00:35:55,871 --> 00:35:59,534
shot, an outdoor shot with K2 in the mid-background.

556
00:35:59,534 --> 00:36:04,959
We expected that with a single CG sun and an HDRI sphere and with K2's eyes not dominating

557
00:36:04,959 --> 00:36:07,981
the frame, this would be a solid candidate for the ADG test.

558
00:36:07,981 --> 00:36:13,346
There you can see clearly both the rendered motion blur as well as the defocus blur that

559
00:36:13,346 --> 00:36:14,127
was done in comp.

560
00:36:14,673 --> 00:36:19,555
The focus pull using an exact emulation of Rogue One's complex anamorphic lens model

561
00:36:19,555 --> 00:36:24,477
is done entirely in COMP using the depth mat, AOV output, along with ADG renders.

562
00:36:24,477 --> 00:36:28,099
These results were very promising and got a good response from John.

563
00:36:28,099 --> 00:36:31,840
We used CG geometry for the physical set to calculate occlusion and shadowing on K2 as

564
00:36:31,840 --> 00:36:33,681
he walked through the door.

565
00:36:33,681 --> 00:36:38,043
Additional darkening of K2 as he walked further inside was done in COMP as a final bit of

566
00:36:38,043 --> 00:36:38,463
sweetening.

567
00:36:41,487 --> 00:36:45,048
This was the last test shot that we did before moving on to actual production work.

568
00:36:45,048 --> 00:36:49,550
This shot was working so well, we actually stopped work on it, on the ADG version of

569
00:36:49,550 --> 00:36:54,331
it, before we had the chance to incorporate the final clean background plate paintwork.

570
00:36:54,331 --> 00:36:57,832
This shot presented two primary lighting challenges.

571
00:36:57,832 --> 00:37:02,133
First of all, on the ILM side, the canvas canopies above and around K2 were presented

572
00:37:02,133 --> 00:37:05,695
as complex textured area lights.

573
00:37:05,695 --> 00:37:10,176
ILM's lighting process extracts textured cards from the on-set captured HDRI sphere

574
00:37:10,744 --> 00:37:13,946
using an internal tool within Xeno called Lightcraft.

575
00:37:13,946 --> 00:37:16,307
Also, the choreographed animated effects lighting

576
00:37:16,307 --> 00:37:18,288
from the explosion needed to be addressed.

577
00:37:18,288 --> 00:37:19,629
Our approach to the textured area lights

578
00:37:19,629 --> 00:37:23,071
was to take the HDR images from the ILM Lightcraft solve,

579
00:37:23,071 --> 00:37:26,813
those textures can be seen here at the top of the screen,

580
00:37:26,813 --> 00:37:29,214
and place those textures on cards

581
00:37:29,214 --> 00:37:32,016
into the real scene for reference.

582
00:37:32,016 --> 00:37:34,557
Then, Justin built a lighting rig

583
00:37:34,557 --> 00:37:37,079
out of non-textured approximated area lights

584
00:37:37,079 --> 00:37:38,620
to match the layout, shape, color.

585
00:37:39,125 --> 00:37:44,929
and intensity of the hotspots that we should be getting from the original lighting textures.

586
00:37:44,929 --> 00:37:53,576
For the FX lighting animation, we didn't have matinee support for IDGLMBIC animation system

587
00:37:53,576 --> 00:37:55,638
and Sequencer did not yet exist.

588
00:37:55,638 --> 00:37:59,641
So we had to choreograph all of this together via Unreal Blueprint scripts and timelines

589
00:37:59,641 --> 00:38:01,823
which would be evaluated during the render process.

590
00:38:04,947 --> 00:38:07,708
Once again, for this shot as well, the results were very promising.

591
00:38:07,708 --> 00:38:11,730
We got this render of the K2SO element to the point that John said that he would have

592
00:38:11,730 --> 00:38:16,773
finaled it, leaving unsaid, if I didn't already have this perfectly good finaled shot from ILM.

593
00:38:16,773 --> 00:38:20,535
We did get a great quote from John on the shot, this is the future.

594
00:38:20,535 --> 00:38:25,537
It was at this point that ADG director Hilmar Koch asked John if he would be comfortable

595
00:38:25,537 --> 00:38:30,099
now switching over to a no safety net approach for at least one shot in Rogue One.

596
00:38:30,099 --> 00:38:30,500
John agreed.

597
00:38:31,002 --> 00:38:35,803
And Rogue One CG supervisor Vic Shutz identified not one but three shots from a sequence that

598
00:38:35,803 --> 00:38:39,064
was slated to be finished in San Francisco.

599
00:38:39,064 --> 00:38:42,285
These are the three shots that ADG contributed to the film.

600
00:38:42,285 --> 00:38:45,546
This was the first of our production shots.

601
00:38:45,546 --> 00:38:49,808
Everything we've talked about so far really had to come together in order for this to

602
00:38:49,808 --> 00:38:53,549
work under real production deadlines with no ILM safety net.

603
00:38:53,549 --> 00:38:57,270
We tried go-bulls or slide maps for the Imperial pill lights.

604
00:38:57,270 --> 00:39:00,872
At one point we had a very clear pill pattern projected and reflected in K2's arms.

605
00:39:01,353 --> 00:39:03,835
but creative direction ended up going for a softer look.

606
00:39:03,835 --> 00:39:07,417
Also, the lighting change when emerging from the hallway

607
00:39:07,417 --> 00:39:09,539
into the larger chamber was challenging.

608
00:39:09,539 --> 00:39:12,941
Due to self-shadow artifacts with depth-based shadow maps,

609
00:39:12,941 --> 00:39:14,962
the top lighting scenario in the larger chamber

610
00:39:14,962 --> 00:39:16,583
took some tweaking on Justin's part

611
00:39:16,583 --> 00:39:18,484
in order to resolve all banding issues.

612
00:39:18,484 --> 00:39:23,467
This one was a fairly straightforward shot.

613
00:39:23,467 --> 00:39:25,909
This was the first shot where we realized

614
00:39:25,909 --> 00:39:27,670
the significance of the object ID map,

615
00:39:27,670 --> 00:39:30,012
which is one of those three AOVs I mentioned earlier.

616
00:39:30,767 --> 00:39:34,289
This allowed the compositor Dan Elstrom to ensure that bright background elements came

617
00:39:34,289 --> 00:39:35,890
through the defocus blur correctly.

618
00:39:35,890 --> 00:39:37,991
This shot presented a challenge in that K2's self-bounce lighting is obviously missing

619
00:39:37,991 --> 00:39:38,952
from our direct dynamic lighting setup.

620
00:39:38,952 --> 00:39:45,275
K2's shoulders should be reflecting a lot of light up onto the back of his head and

621
00:39:45,275 --> 00:39:49,098
with the global illumination path tracer you can get that.

622
00:39:49,098 --> 00:39:56,882
In our case we ended up cheating with bounce lights, parented under certain bits of the

623
00:39:56,882 --> 00:39:58,703
animation hierarchy.

624
00:39:59,511 --> 00:40:01,813
and keyframe during the shot.

625
00:40:01,813 --> 00:40:04,915
We used a similar timing setup for this

626
00:40:04,915 --> 00:40:06,696
that was used for the explosion effect sliding

627
00:40:06,696 --> 00:40:08,197
in the last test shot.

628
00:40:08,197 --> 00:40:12,500
This sweeping camera really illustrates

629
00:40:12,500 --> 00:40:15,182
the weaknesses of our real-time implementation

630
00:40:15,182 --> 00:40:16,123
of the shading model.

631
00:40:16,123 --> 00:40:19,465
This was the one shot where our single specular lobe

632
00:40:19,465 --> 00:40:21,387
really started to limit us from reaching

633
00:40:21,387 --> 00:40:24,249
the approved or blessed K2 look.

634
00:40:24,249 --> 00:40:27,471
Justin had to do some shot-specific material tweaks.

635
00:40:27,848 --> 00:40:30,851
to make some of the more reflective bits and bobs sing properly.

636
00:40:30,851 --> 00:40:34,655
We are thrilled that we were able to contribute to a project like Rogue One.

637
00:40:34,655 --> 00:40:39,979
The work of innovation and real-time rendering doesn't stop here, though.

638
00:40:39,979 --> 00:40:44,444
Current ADG and XLAB projects in flight right now, such as the upcoming Vader VR project,

639
00:40:44,444 --> 00:40:49,428
demand continued advancement in both fidelity as well as performance, and we have a number

640
00:40:49,428 --> 00:40:51,430
of areas of active development.

641
00:40:51,430 --> 00:40:53,192
Aerial lights and shadows.

642
00:40:54,122 --> 00:40:56,804
Support for Pixar's universal scene description format,

643
00:40:56,804 --> 00:40:57,664
or USD format.

644
00:40:57,664 --> 00:41:00,246
We are excited by Epic's announcement of USD support

645
00:41:00,246 --> 00:41:02,168
this morning.

646
00:41:02,168 --> 00:41:04,469
Pushing our real-time subdivision surface rendering

647
00:41:04,469 --> 00:41:06,451
techniques further.

648
00:41:06,451 --> 00:41:07,792
Incorporating open color IO.

649
00:41:07,792 --> 00:41:14,036
Supporting true arbitrary output variables during rendering.

650
00:41:14,036 --> 00:41:16,959
And continuing to improve our real-time implementation

651
00:41:16,959 --> 00:41:17,479
of unified surf.

652
00:41:20,687 --> 00:41:24,248
In closing, I'd like to take this opportunity to give a huge shout out and thanks to the

653
00:41:24,248 --> 00:41:27,829
rest of the team who made this work possible over the past year.

654
00:41:27,829 --> 00:41:34,470
David Mennie, Nick Haynes, Indy Ray, Vic Schatz, Dan Enstrom, Justin Schubert, Ron Radetzky,

655
00:41:34,470 --> 00:41:35,251
and Hannah Gillis.

656
00:41:35,251 --> 00:41:48,013
And we have some time for Q&A.

657
00:41:50,025 --> 00:41:54,226
And by the way, ILM, ILMxLAB, and Lucasfilm Advanced Development Group are all hiring.

658
00:41:54,226 --> 00:41:56,607
And Sarah, our XLAB recruiter, is here in the room.

659
00:41:56,607 --> 00:41:57,787
Please stand up and wave, Sarah.

660
00:41:57,787 --> 00:41:58,447
There she is.

661
00:41:58,447 --> 00:41:58,848
Thank you.

662
00:41:58,848 --> 00:41:59,008
Yes?

663
00:41:59,008 --> 00:42:03,129
I'm kind of curious a little bit about the soft lighting and area light approximation

664
00:42:03,129 --> 00:42:05,849
that you guys come up with.

665
00:42:05,849 --> 00:42:17,172
You do stuff in general, you probably do that as well, basically using arrays of different

666
00:42:17,172 --> 00:42:19,493
lights in a pretty cool way.

667
00:42:22,718 --> 00:42:30,782
So, our area lighting shaders are, so we ended up using both techniques for some shots.

668
00:42:30,782 --> 00:42:36,145
Because render time did not have to be, you know, we weren't making a game to run at 60

669
00:42:36,145 --> 00:42:36,786
hertz.

670
00:42:36,786 --> 00:42:41,728
We had multiple seconds to render these images.

671
00:42:41,728 --> 00:42:45,450
I don't know if you want to talk about the lighting rigs that you built that were using

672
00:42:45,450 --> 00:42:46,811
many, many, many lights.

673
00:42:47,077 --> 00:42:50,078
Yeah, sure. We definitely used clusters of lights,

674
00:42:50,078 --> 00:42:52,258
basically old-school point light technique

675
00:42:52,258 --> 00:42:54,239
of casting a bunch of shadows,

676
00:42:54,239 --> 00:42:56,520
getting soft lighting from multiple shadow samples.

677
00:42:56,520 --> 00:43:01,502
Worked, but it was extremely expensive, way too expensive.

678
00:43:01,502 --> 00:43:04,302
So basically, I kind of went over to Roger.

679
00:43:04,302 --> 00:43:07,343
I was like, what do I do?

680
00:43:07,343 --> 00:43:08,724
I had basically a day to figure it out.

681
00:43:08,724 --> 00:43:11,865
What I ended up doing in the long run

682
00:43:11,865 --> 00:43:14,486
was taking the PCF shadows that exist in Unreal

683
00:43:15,005 --> 00:43:18,526
and putting a multiplier against its depth sampling.

684
00:43:18,526 --> 00:43:22,026
And then that in tandem, like that was then,

685
00:43:22,026 --> 00:43:25,367
I implemented a slider into the lights.

686
00:43:25,367 --> 00:43:26,867
And another guy that I work with,

687
00:43:26,867 --> 00:43:31,949
Brasher, Christian Maturi, helped me on that effort.

688
00:43:31,949 --> 00:43:33,169
Very good guy.

689
00:43:33,169 --> 00:43:37,210
Anyway, so in tandem with that, cranking the bias,

690
00:43:37,210 --> 00:43:39,470
I was able to actually shorten my depth

691
00:43:39,470 --> 00:43:42,051
and get softer shadow spread,

692
00:43:42,051 --> 00:43:42,291
which,

693
00:43:43,065 --> 00:43:46,307
ended up actually looking fairly nice.

694
00:43:46,307 --> 00:43:46,467
So.

695
00:43:46,467 --> 00:43:50,349
Is there something else you want to say about it?

696
00:43:50,349 --> 00:43:53,010
No, I think that pretty much covered it.

697
00:43:53,010 --> 00:43:55,072
The lights are pretty bright.

698
00:43:55,072 --> 00:43:55,892
I can't see.

699
00:43:55,892 --> 00:43:56,752
Oh, yes.

700
00:43:56,752 --> 00:44:01,075
You mentioned that you speed spaced any machine.

701
00:44:01,075 --> 00:44:04,797
Did you just simply darken the frame buffer?

702
00:44:04,797 --> 00:44:07,719
Or did you do something more to it?

703
00:44:07,719 --> 00:44:09,420
No, it was, uh.

704
00:44:10,809 --> 00:44:14,131
You would know the details, but I believe it was applied in this...

705
00:44:14,131 --> 00:44:16,692
It's being applied to specific lights, yeah.

706
00:44:16,692 --> 00:44:21,274
So the question was about screen space ambient inclusion and whether we are applying it to

707
00:44:21,274 --> 00:44:26,116
the final image, and the answer is no, we're using it as a shadowing factor, as an occlusion

708
00:44:26,116 --> 00:44:31,519
factor for specific light types which had no other shadow calculated.

709
00:44:31,519 --> 00:44:36,841
Mainly, like, really the only light that is shadowed by that is the environment sphere,

710
00:44:36,841 --> 00:44:38,962
because we have no other shadow term to pull from that.

711
00:44:42,127 --> 00:44:42,628
Yes.

712
00:44:42,628 --> 00:44:50,393
I noticed that you had certain visual effects elements in some of the shots as well.

713
00:44:50,393 --> 00:44:55,937
For example, sand kicking up from K2SO's feet, and in the X-Wing shots there was all the

714
00:44:55,937 --> 00:44:57,998
water vapor and everything.

715
00:44:57,998 --> 00:45:02,862
Did you also render those in UE4, and if so, how did you approach those?

716
00:45:02,862 --> 00:45:03,122
No, no, no.

717
00:45:03,122 --> 00:45:05,544
That's all in the composite.

718
00:45:05,544 --> 00:45:08,326
So the X-Wings are that specific element.

719
00:45:08,326 --> 00:45:11,528
That's what was rendered in the real-time game engine.

720
00:45:11,898 --> 00:45:14,720
And again, rendering those X-Wings fast,

721
00:45:14,720 --> 00:45:17,562
even just rendering the X-Wings takes multiple hours

722
00:45:17,562 --> 00:45:19,664
in RenderMan, so that was a win for us there.

723
00:45:19,664 --> 00:45:24,187
And on Rogue One, it was specifically the K2SO element

724
00:45:24,187 --> 00:45:25,808
that goes into composite.

725
00:45:25,808 --> 00:45:28,490
So the explosion, I mean, actually in that shot,

726
00:45:28,490 --> 00:45:30,892
that was a practical, that was photographed.

727
00:45:30,892 --> 00:45:33,834
And then the smoke and dust, those are 2D elements

728
00:45:33,834 --> 00:45:35,395
that are just done in compositing.

729
00:45:35,395 --> 00:45:36,676
And the same with the sand kicking up?

730
00:45:36,676 --> 00:45:39,638
Yeah, the sand kicking up, actually.

731
00:45:40,222 --> 00:45:47,526
was in the plate because Alan Tudyk was performing K2 in a motion capture suit so that he was

732
00:45:47,526 --> 00:45:50,789
actually kicking up sand and that was retained.

733
00:45:50,789 --> 00:45:51,089
Thank you.

734
00:45:51,089 --> 00:46:03,857
What kind of global illumination approaches are you thinking about using for the future?

735
00:46:03,857 --> 00:46:09,001
So there are various...

736
00:46:10,427 --> 00:46:14,010
mostly pre-computed or baked global illumination solutions

737
00:46:14,010 --> 00:46:15,771
that you can use within Unreal.

738
00:46:15,771 --> 00:46:20,675
You can bake light maps, you can bake diffuse indirect probes,

739
00:46:20,675 --> 00:46:25,880
you can bake IBLs from within Lightmass, which is Unreal's

740
00:46:25,880 --> 00:46:27,661
baked lighting tool.

741
00:46:27,661 --> 00:46:31,865
In terms of more dynamic GI, we intend

742
00:46:31,865 --> 00:46:35,307
to sort of address that as we run into the problem,

743
00:46:35,307 --> 00:46:37,609
because there is a lot of different ways

744
00:46:37,609 --> 00:46:39,291
you can skin that particular issue.

745
00:46:40,719 --> 00:46:49,463
I think the form in which we first encounter a need to solve it will kind of drive the approach that we take.

746
00:46:49,463 --> 00:46:57,226
Yeah, quick question. So in comparison to the approach that you've taken,

747
00:46:57,226 --> 00:47:05,650
what's your opinions on like GPU path tracers, like for example like

748
00:47:05,650 --> 00:47:09,032
popular ones like Octane, Render, or FStorm?

749
00:47:09,473 --> 00:47:11,874
these kind of up and coming GPU pass tracers

750
00:47:11,874 --> 00:47:14,635
that are basically using the GPU

751
00:47:14,635 --> 00:47:16,836
to render frames really, really fast.

752
00:47:16,836 --> 00:47:19,937
Like, have you experimented with that

753
00:47:19,937 --> 00:47:22,697
and what are the kind of comparisons with that approach?

754
00:47:22,697 --> 00:47:27,119
So, I certainly don't want to speak for John,

755
00:47:27,119 --> 00:47:31,260
but for me, the fact that the image is resolved fully

756
00:47:31,260 --> 00:47:37,162
and there is no convergence, progressive convergence,

757
00:47:37,162 --> 00:47:38,642
during interactive editing,

758
00:47:39,810 --> 00:47:45,574
that even though things are wrong, that's a win for rasterization in my case.

759
00:47:45,574 --> 00:47:51,197
For the lighting artist to be able to see an image that is very, very, very close to

760
00:47:51,197 --> 00:48:00,562
what the final image is going to be, completely interactively and in fluid real-time during

761
00:48:00,562 --> 00:48:04,865
working with no shift, no convergence required, that seems huge.

762
00:48:07,820 --> 00:48:13,684
I thought that movie companies were rendering their graphics with Maya and programs like

763
00:48:13,684 --> 00:48:13,924
that.

764
00:48:13,924 --> 00:48:15,425
Now you're doing it with Unreal Engine.

765
00:48:15,425 --> 00:48:16,526
Why is that?

766
00:48:16,526 --> 00:48:23,551
Well, most of the shots in the movie were rendered with other tools than Unreal Engine.

767
00:48:23,551 --> 00:48:26,233
And the full list is probably quite long.

768
00:48:26,233 --> 00:48:28,155
I guess there's the mainstream pipeline.

769
00:48:28,155 --> 00:48:34,920
Yeah, I mean, the majority of the renders on the show are RenderMan RISC renders.

770
00:48:34,920 --> 00:48:37,342
A lot of the environment work is rendered in V-Ray.

771
00:48:38,381 --> 00:48:42,442
Then a lot of effects elements, smoke and fire and that sort of thing.

772
00:48:42,442 --> 00:48:48,544
I think a lot of those are mantra renders, along with a fair number that were done.

773
00:48:48,544 --> 00:48:51,045
In Plume, an in-house package we've written.

774
00:48:51,045 --> 00:48:55,746
So it's a pretty big mixture of different techniques.

775
00:48:55,746 --> 00:49:04,309
But your question about why real-time, why GPU renders, was really about iteration time,

776
00:49:04,309 --> 00:49:07,010
looking at the future.

777
00:49:08,600 --> 00:49:13,842
Ideally we're letting artists really see what they're doing

778
00:49:13,842 --> 00:49:18,403
and be able to reduce that iteration cycle down to seconds.

779
00:49:18,403 --> 00:49:21,804
I think we get down to being able to hit feature film

780
00:49:21,804 --> 00:49:25,805
level quality with renders that are interactive

781
00:49:25,805 --> 00:49:28,286
so you can move lights around,

782
00:49:28,286 --> 00:49:31,267
you can make material adjustments pretty much in real time.

783
00:49:31,267 --> 00:49:35,148
That does shave quite a lot off of the amount of time

784
00:49:35,148 --> 00:49:37,428
that goes into developing an asset.

785
00:49:40,055 --> 00:49:44,037
Time is money.

786
00:49:44,037 --> 00:49:51,619
Do you see a tradeoff between the man hours that is required to prepare a shot for the

787
00:49:51,619 --> 00:49:55,101
real time versus just sending it off to the render farm?

788
00:49:55,101 --> 00:50:02,523
That was one of the requirements of this exercise was that there...

789
00:50:03,067 --> 00:50:05,928
we weren't allowing ourselves any custom preparation

790
00:50:05,928 --> 00:50:07,708
of the content that goes into these shots.

791
00:50:07,708 --> 00:50:09,929
We had to be able to consume the content directly

792
00:50:09,929 --> 00:50:12,850
from the fire hose of the ILM pipeline,

793
00:50:12,850 --> 00:50:14,510
and then that's what we executed.

794
00:50:14,510 --> 00:50:17,652
Yeah, I mean, certainly we could have made the job

795
00:50:17,652 --> 00:50:21,993
in some ways easier for ourselves

796
00:50:21,993 --> 00:50:27,175
by authoring something that's performant in the engine.

797
00:50:27,175 --> 00:50:31,836
But for me, a big part of the point of this exercise was,

798
00:50:31,836 --> 00:50:32,497
well, don't wanna.

799
00:50:33,053 --> 00:50:35,717
Do any special, we're going to rebuild it for Unreal.

800
00:50:35,717 --> 00:50:40,223
No, we're going to take one of our standard production assets

801
00:50:40,223 --> 00:50:42,627
kind of as they are normally built for RenderMan,

802
00:50:42,627 --> 00:50:44,409
and we're going to hand it off to ADG,

803
00:50:44,409 --> 00:50:47,593
and they're going to render it and make it look beautiful.

804
00:50:50,798 --> 00:50:54,081
Hi, thank you for your presentation.

805
00:50:54,081 --> 00:50:57,384
As these processes become more and more real time,

806
00:50:57,384 --> 00:51:00,127
I can't help wondering if you've spent any time

807
00:51:00,127 --> 00:51:03,410
anticipating what will become of the artist experience.

808
00:51:03,410 --> 00:51:04,571
I remember from my own time at ILM

809
00:51:04,571 --> 00:51:09,115
that a given asset might take 15, 20, 30 artists

810
00:51:09,115 --> 00:51:09,595
working on it.

811
00:51:09,595 --> 00:51:12,037
Do you see any of these processes,

812
00:51:12,037 --> 00:51:14,539
these improvements in render time and interactivity

813
00:51:14,539 --> 00:51:17,922
altering that experience for those individual artists?

814
00:51:17,922 --> 00:51:18,323
And if so, how?

815
00:51:19,308 --> 00:51:24,172
Well, I suppose some of the things I see happening are one artist being able to do more work.

816
00:51:24,172 --> 00:51:33,079
You know, I'm looking forward to the day when you can have a handful of artists kind of do whole sequences.

817
00:51:33,079 --> 00:51:38,123
It seems like you're getting closer to the city, you know, in real time, performing with multiple artists.

818
00:51:38,123 --> 00:51:39,905
Yeah, it's...

819
00:51:39,905 --> 00:51:45,069
You know, a lot of it comes down to just iteration, you know.

820
00:51:45,069 --> 00:51:46,790
It usually takes...

821
00:51:47,560 --> 00:51:54,566
I have 5-6 takes on lighting renders to really get things dialed in.

822
00:51:54,566 --> 00:51:59,830
But if you don't have to wait for an overnight render and look to see it in the theater,

823
00:51:59,830 --> 00:52:02,593
if you can make a tweak and just see what you're going to get,

824
00:52:02,593 --> 00:52:04,614
I think you can get there a lot quicker.

825
00:52:04,614 --> 00:52:06,656
Economics are huge in this business.

826
00:52:06,656 --> 00:52:11,480
Anything that allows you to do very high quality work and put fewer man hours into it.

827
00:52:15,080 --> 00:52:17,902
I had a question specifically about just the motion blur.

828
00:52:17,902 --> 00:52:20,224
I'm curious why you decided to do that in engine

829
00:52:20,224 --> 00:52:21,845
rather than kicking that to comp as well,

830
00:52:21,845 --> 00:52:23,386
where it seems like you might be able to get

831
00:52:23,386 --> 00:52:24,507
a little bit smoother result

832
00:52:24,507 --> 00:52:26,409
rather than just kicking it out 16 times in Unreal.

833
00:52:26,409 --> 00:52:29,451
That was specifically because that's the way

834
00:52:29,451 --> 00:52:30,712
it is rendered at ILM.

835
00:52:30,712 --> 00:52:32,594
That the compositing setup for these shots,

836
00:52:32,594 --> 00:52:34,536
like the compositor who's working on these shots

837
00:52:34,536 --> 00:52:36,837
is also compositing a ton of other shots

838
00:52:36,837 --> 00:52:38,199
that did not come from ADG.

839
00:52:38,199 --> 00:52:40,120
And so he's used to renders coming to him

840
00:52:40,120 --> 00:52:41,942
with motion blur in them. And so we were.

841
00:52:42,295 --> 00:52:43,236
We stuck with that.

842
00:52:43,236 --> 00:52:45,958
So rather than rendering a vector pass or something like that, you just figured that

843
00:52:45,958 --> 00:52:46,898
would be the way to do it?

844
00:52:46,898 --> 00:52:52,423
Well, also, that would require, like in order to have a motion blur technique that we could

845
00:52:52,423 --> 00:53:00,129
render out, we would have to invent one that is as high quality as what you can do with

846
00:53:00,129 --> 00:53:02,471
the temporal and spatial subsamples in the Olympic.

847
00:53:02,471 --> 00:53:05,513
I just figured for those three specific shots or something like that where the motion is

848
00:53:05,513 --> 00:53:08,696
rather linear and you're not dealing with anything, any crazy motions or anything like that.

849
00:53:09,014 --> 00:53:12,515
Technically, we probably could have rendered without blur

850
00:53:12,515 --> 00:53:15,236
and done a 2D vector thing.

851
00:53:15,236 --> 00:53:19,237
But I wasn't trying to give them an easy way out on this.

852
00:53:19,237 --> 00:53:25,939
And we have sometimes resorted to doing motion vector renders

853
00:53:25,939 --> 00:53:29,460
to save on render time or to reduce noise.

854
00:53:29,460 --> 00:53:34,141
And it can look good in plenty of cases,

855
00:53:34,141 --> 00:53:36,482
but it's sort of technically wrong.

856
00:53:36,963 --> 00:53:42,008
And there are always pathological cases where it just doesn't work at all, like spinning

857
00:53:42,008 --> 00:53:44,210
propellers and things like that.

858
00:53:44,210 --> 00:53:49,976
So I think in general that the preference, the culture at ILM is to try and get it more

859
00:53:49,976 --> 00:53:55,521
technically right and just let it grind a little bit longer on the farm.

860
00:53:58,003 --> 00:54:03,445
Hi. So obviously you had to pick and choose your battles here.

861
00:54:03,445 --> 00:54:08,147
And there are some compromises you had to be willing to make, um, to get, you

862
00:54:08,147 --> 00:54:12,609
know, good enough, but still run close enough to real time that it would meet

863
00:54:12,609 --> 00:54:13,689
your, your requirements.

864
00:54:13,689 --> 00:54:19,751
But, um, what do you, what do you see as like, uh, the prospects for being able

865
00:54:19,751 --> 00:54:26,354
to start handling more, more broad cases, like not just as restricted as you're

866
00:54:26,354 --> 00:54:27,074
kind of currently.

867
00:54:28,190 --> 00:54:29,510
having to be.

868
00:54:29,510 --> 00:54:36,893
So the work that Natty and his group are doing right now will lift a lot of the restrictions

869
00:54:36,893 --> 00:54:40,155
on the types of assets that we would even consider tackling at all.

870
00:54:40,155 --> 00:54:46,237
You know, having a full version of the unified shader implemented in real time, there will

871
00:54:46,237 --> 00:54:51,459
no longer be a reason to pick a certain asset over another.

872
00:54:51,459 --> 00:54:55,861
Most of the compromise that we made from the what was real time and then what was a minute

873
00:54:55,861 --> 00:54:57,462
of frame was all about aliasing.

874
00:54:58,633 --> 00:55:06,879
So for the lighting artist working in real time at his desk, it's an aliased image, but it's a real time image.

875
00:55:06,879 --> 00:55:13,623
And so if we can consume the geometry caches from ILM and we can shade them with the shading model that ILM uses,

876
00:55:13,623 --> 00:55:17,266
and it's a slightly aliased image, then I think we'll be in business.

877
00:55:17,266 --> 00:55:24,010
So you think you're on a roadmap to be able to handle non-hard surfaces even?

878
00:55:24,010 --> 00:55:27,513
Well, roadmap is a pretty abstract description of that.

879
00:55:28,028 --> 00:55:30,691
Yeah, more arbitrary hard surfaces is a good first one.

880
00:55:30,691 --> 00:55:37,276
Once you're talking about things with subsurface scattering, it's definitely something that

881
00:55:37,276 --> 00:55:41,719
we've been looking into, but that is obviously a trickier one to do at sort of ILM visual

882
00:55:41,719 --> 00:55:43,040
standards.

883
00:55:43,040 --> 00:55:43,221
Right.

884
00:55:43,221 --> 00:55:49,746
Yeah, I think just getting support for the second specular lobe and for the refraction

885
00:55:49,746 --> 00:55:53,709
and reflection model, I think, you know, opens things up pretty considerably.

886
00:55:53,972 --> 00:56:00,497
Yeah, the second speculo we already have in our latest work and refraction, we've definitely

887
00:56:00,497 --> 00:56:01,778
been investigating that as well.

888
00:56:01,778 --> 00:56:05,020
Really good looking soft area lights too.

889
00:56:05,020 --> 00:56:10,645
Yep, and we have some good ideas for that.

890
00:56:10,645 --> 00:56:16,530
Who benefits from this real time rendering?

891
00:56:16,530 --> 00:56:22,174
Is it just somebody who sees K2 and he can look at it?

892
00:56:22,467 --> 00:56:28,109
Compositor have everything composed it and edit it in real life and see how things change

893
00:56:28,109 --> 00:56:29,010
Well in terms of the pipeline

894
00:56:29,010 --> 00:56:29,150
Yeah

895
00:56:29,150 --> 00:56:33,732
somebody who's who's done a lot of

896
00:56:33,732 --> 00:56:37,253
sort of offline software rendering

897
00:56:37,253 --> 00:56:46,277
You know I've suffered through the the battle days where you would tweak a parameter

898
00:56:46,277 --> 00:56:51,759
And you dispatch a test and you you know wait 15 minutes to see a little postage stamp version

899
00:56:52,507 --> 00:56:57,148
See, it's, you know, when your iteration times

900
00:56:57,148 --> 00:56:58,628
are fairly long like that,

901
00:56:58,628 --> 00:57:02,149
you know, there's only so much finesse

902
00:57:02,149 --> 00:57:03,449
you can afford to put into a shot

903
00:57:03,449 --> 00:57:04,970
before you basically run out of time.

904
00:57:04,970 --> 00:57:09,851
And so the faster you can get the iteration loop,

905
00:57:09,851 --> 00:57:12,391
then the better you can make something look.

906
00:57:12,391 --> 00:57:14,252
You can kind of dial something to be just so.

907
00:57:14,252 --> 00:57:20,073
You can do much more high quality finished work

908
00:57:20,073 --> 00:57:20,633
on take one.

909
00:57:21,157 --> 00:57:27,302
So the goal here is to reduce the number of takes that are required and how long an artist

910
00:57:27,302 --> 00:57:30,385
has to work to get a satisfactory result.

911
00:57:30,385 --> 00:57:38,631
And letting an artist get more work done means there's in total fewer man hours that go into

912
00:57:38,631 --> 00:57:39,812
a given project.

913
00:57:39,812 --> 00:57:43,335
So that makes the work less expensive to do.

914
00:57:43,335 --> 00:57:47,278
There's less of a chance that it all goes to third world countries.

915
00:57:48,378 --> 00:57:50,479
To piggyback off of what John's saying too,

916
00:57:50,479 --> 00:57:52,900
the process was very WYSIWYG.

917
00:57:52,900 --> 00:57:54,501
Like, what you see is what you get.

918
00:57:54,501 --> 00:57:57,382
It's right there, it's fluid, you can light,

919
00:57:57,382 --> 00:57:59,503
what you see is what you get at the end.

920
00:57:59,503 --> 00:58:02,524
Like, there is no iteration in between.

921
00:58:02,524 --> 00:58:05,866
The only effect that you're not getting

922
00:58:05,866 --> 00:58:09,287
is like proper anti-aliasing, the motion blur,

923
00:58:09,287 --> 00:58:10,208
but the rest of the shot,

924
00:58:10,208 --> 00:58:12,949
if you have a back plate behind the thing,

925
00:58:12,949 --> 00:58:14,349
you can light to that back plate

926
00:58:14,349 --> 00:58:16,370
and get your final result in Viewport.

927
00:58:16,863 --> 00:58:22,445
So that's also the benefit, that's the primary benefit of this tool, right?

928
00:58:22,445 --> 00:58:27,548
I could have like Vic Schutz for instance at my desk saying I need another point light

929
00:58:27,548 --> 00:58:32,030
over here, I need some extra spec on his shoulder and I can just drop that in there and look

930
00:58:32,030 --> 00:58:37,753
at it and he can approve it and I can say okay now I send it off and render out the

931
00:58:37,753 --> 00:58:38,153
sample.

932
00:58:38,748 --> 00:58:41,749
So that benefit for the artist flows upward all the way.

933
00:58:41,749 --> 00:58:43,169
I mean, John benefits from this,

934
00:58:43,169 --> 00:58:45,930
getting more useful takes in dailies every day.

935
00:58:45,930 --> 00:58:48,911
I've shot a lot of live action too,

936
00:58:48,911 --> 00:58:50,831
both live action things and miniatures.

937
00:58:50,831 --> 00:58:54,532
And I'm also very used to a kind of different workflow.

938
00:58:54,532 --> 00:58:57,132
When you're on set and you're lighting something,

939
00:58:57,132 --> 00:58:59,573
you see how everything responds in real time

940
00:58:59,573 --> 00:59:02,173
and you'll have somebody kind of move in a light

941
00:59:02,173 --> 00:59:05,134
and you'll watch how the shadows fall on a character

942
00:59:05,134 --> 00:59:07,675
and you sort of slide things around until.

943
00:59:08,240 --> 00:59:10,721
oh yeah, that's feeling about right.

944
00:59:10,721 --> 00:59:14,542
And much slower software renders,

945
00:59:14,542 --> 00:59:16,723
it's a very different workflow.

946
00:59:16,723 --> 00:59:18,204
You kind of take a guess at it and you,

947
00:59:18,204 --> 00:59:22,186
and having a tool where you can drag things around

948
00:59:22,186 --> 00:59:24,167
in real time is sort of replicating

949
00:59:24,167 --> 00:59:25,927
a bit of that onset experience.

950
00:59:25,927 --> 00:59:28,308
I think that changes a little bit

951
00:59:28,308 --> 00:59:30,149
how you think about lighting.

952
00:59:30,149 --> 00:59:30,990
Thanks.

953
00:59:31,675 --> 00:59:34,597
So for GPUs, I'm curious, did you guys use the Consumer 980

954
00:59:34,597 --> 00:59:37,699
or Titan, or did you go the workstation Quadro type route,

955
00:59:37,699 --> 00:59:39,060
or something else altogether?

956
00:59:39,060 --> 00:59:41,642
So we were using, yeah, we used Quadros.

957
00:59:41,642 --> 00:59:50,009
So you have added UDIM support to Unreal?

958
00:59:50,009 --> 00:59:52,811
We added MaterialX support to Unreal, which

959
00:59:52,811 --> 00:59:54,752
kind of brings UDIMs with it, yeah.

960
00:59:54,752 --> 00:59:55,032
Cool.

961
01:00:00,563 --> 01:00:01,231
Thanks everybody.

962
01:00:01,231 --> 01:00:02,001
All right, thank you.

963
01:00:02,001 --> 01:00:02,325
Thank you.

