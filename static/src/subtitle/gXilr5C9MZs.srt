1
00:00:06,357 --> 00:00:15,200
Hello everyone. The title of my talk is Applying Reinforcement Learning to Develop Game AI in NetEase Games.

2
00:00:15,200 --> 00:00:21,842
My name is Tangjie Li. I am the senior engineer from Fuxi AI Lab in NetEase Games.

3
00:00:21,842 --> 00:00:27,484
First of all, I will introduce Fuxi AI Lab.

4
00:00:27,484 --> 00:00:30,945
Fuxi AI Lab was established in September 2017.

5
00:00:30,945 --> 00:00:35,747
It is the first game AI research lab in China.

6
00:00:36,391 --> 00:00:40,334
and our vision is enlighten games with AI.

7
00:00:40,334 --> 00:00:43,256
Combined with NetEase's powerful

8
00:00:43,256 --> 00:00:46,198
game development capabilities,

9
00:00:46,198 --> 00:00:50,542
Fushi AI Lab is committed to use AI technology

10
00:00:50,542 --> 00:00:53,744
to create a better game experience.

11
00:00:53,744 --> 00:00:57,947
Today, my talk will introduce some of our work

12
00:00:57,947 --> 00:01:03,332
on using reinforcement learning to develop game AI.

13
00:01:03,332 --> 00:01:05,733
So, what is reinforcement learning?

14
00:01:06,452 --> 00:01:11,913
Reinforcement learning is a kind of machine learning to solve MDP problems.

15
00:01:11,913 --> 00:01:15,293
Its process looks like the left picture.

16
00:01:15,293 --> 00:01:18,594
The dog is the AI we want to train.

17
00:01:18,594 --> 00:01:20,054
So how to train a dog?

18
00:01:20,054 --> 00:01:25,055
When we want to train a dog to learn the command down,

19
00:01:25,055 --> 00:01:27,935
we will give it some food if it does down action.

20
00:01:27,935 --> 00:01:32,536
From the dog's perspective, when it hears down,

21
00:01:32,536 --> 00:01:34,777
it will get some positive reward.

22
00:01:35,477 --> 00:01:44,842
else it will get nothing. The formal representation is shown on the right picture. The agent interacts

23
00:01:44,842 --> 00:01:50,765
with the environment to receive some state from the environment and does some action

24
00:01:50,765 --> 00:01:58,209
according to the state and then it will receive some reward. The objective of reinforcement

25
00:01:58,209 --> 00:02:05,032
learning algorithms is to get the optimal policy to maximize the cumulative rewards.

26
00:02:07,171 --> 00:02:13,695
As we know, there are many algorithms and concepts in reinforcement learning.

27
00:02:13,695 --> 00:02:18,798
So it's very difficult for game developers to learn and use it.

28
00:02:18,798 --> 00:02:27,684
To solve this problem, we developed a reinforcement learning framework, which we call RLEs.

29
00:02:27,684 --> 00:02:30,445
RLEs is a framework to make RL easy.

30
00:02:30,445 --> 00:02:34,548
It integrates common reinforcement learning algorithms.

31
00:02:35,056 --> 00:02:42,999
reduces the difficulty of RL applications and simplifies the game access process.

32
00:02:42,999 --> 00:02:49,702
It has many advantages, such as it contains common RL echo themes, it supports multi-agent,

33
00:02:49,702 --> 00:02:57,786
hierarchical RL, self-play, it has high performance, supports behavioral diversity,

34
00:02:57,786 --> 00:03:04,389
ERO or true skill evaluation. RLEs is a general reinforcement learning framework.

35
00:03:04,914 --> 00:03:07,776
that provides standard interfaces.

36
00:03:07,776 --> 00:03:10,337
And we only need to implement these interfaces

37
00:03:10,337 --> 00:03:14,540
or we want to add a new reinforcement learning

38
00:03:14,540 --> 00:03:16,721
for everything in the framework.

39
00:03:16,721 --> 00:03:18,962
There are some advantages.

40
00:03:18,962 --> 00:03:21,884
First, the framework is distributed

41
00:03:21,884 --> 00:03:25,526
and high performance.

42
00:03:25,526 --> 00:03:28,328
So the researchers don't need to care

43
00:03:28,328 --> 00:03:30,529
about the engineering work.

44
00:03:30,529 --> 00:03:31,710
Second.

45
00:03:32,255 --> 00:03:39,999
All algorithms are use features provided by the framework, such as self-play, AIO, or true skill evaluation.

46
00:03:39,999 --> 00:03:49,085
And the third, it is easy to test new algorithms with net-ease games, which has been integrated in the framework.

47
00:03:49,085 --> 00:04:01,973
Someone may know ILEAP. ILEAP is a well-known open-source reinforcement learning framework that supports a variety of higher algorithms with high performance.

48
00:04:02,283 --> 00:04:09,949
and the Node support, multi-agent, self-play, behavior diversity, or ELO evaluation, which

49
00:04:09,949 --> 00:04:13,572
are very important for game development.

50
00:04:13,572 --> 00:04:22,178
Compared with IRE, IRE not only maintains high performance, but also makes many optimizations

51
00:04:22,178 --> 00:04:29,043
for game development, and makes it easy to lend reinforcement learning to real games.

52
00:04:31,688 --> 00:04:39,730
As the table shows below, we can see that our yeast has faster speed in the same hardware

53
00:04:39,730 --> 00:04:44,331
and can get similar results on a tiny environment.

54
00:04:44,331 --> 00:04:55,994
Today, I will introduce some of our achievements in developing game AI using reinforcement learning.

55
00:04:55,994 --> 00:05:00,175
In the past few years, we have applied reinforcement learning

56
00:05:00,511 --> 00:05:01,811
to a number of games.

57
00:05:01,811 --> 00:05:07,233
And today, I will focus two of them.

58
00:05:07,233 --> 00:05:10,274
The first one is Justice Online.

59
00:05:10,274 --> 00:05:14,856
Justice Online is an MMORPG developed by Netis Games.

60
00:05:14,856 --> 00:05:17,677
It's a very successful game in China

61
00:05:17,677 --> 00:05:20,038
since it launched on June 29, 2018.

62
00:05:20,038 --> 00:05:25,680
This is a promotional video for Justice Online.

63
00:05:48,665 --> 00:05:52,909
Thanks for watching!

64
00:06:30,255 --> 00:06:36,763
This is another video showing the Liu Pai Jing Wu gameplay of Justice Online, which

65
00:06:36,763 --> 00:06:40,206
we will introduce and the two classes are both long in.

66
00:07:05,360 --> 00:07:15,883
Okay, in this gameplay, human players challenge hierarchy players who may not be online, so

67
00:07:15,883 --> 00:07:19,044
ask AI to substitute for him.

68
00:07:19,044 --> 00:07:23,325
This is a 1v1 scene, and the game designers

69
00:07:23,602 --> 00:07:28,264
want us to meet these demands.

70
00:07:28,264 --> 00:07:32,787
First, the highest level AI should be strong enough.

71
00:07:32,787 --> 00:07:37,150
Second, we should provide a different levels

72
00:07:37,150 --> 00:07:39,911
AI for different players.

73
00:07:39,911 --> 00:07:43,274
And third, the AI should have behavior diversity.

74
00:07:43,274 --> 00:07:50,278
Of course, there are traditional rule-based AI in Justice Online.

75
00:07:50,874 --> 00:07:55,998
But it's very hard to write a high-level AI with multiple behavioral styles.

76
00:07:55,998 --> 00:08:07,529
So we try reinforcement learning to solve the problem and use the Impala algorithm in our ease.

77
00:08:07,529 --> 00:08:11,613
However, there are many difficulties in this gameplay.

78
00:08:11,613 --> 00:08:16,198
The first one is the AI's potential action space is very huge.

79
00:08:17,352 --> 00:08:24,433
Human player can choose 5 weapon skills from 8 and 5 class skills from 12 to use.

80
00:08:24,433 --> 00:08:30,935
There are about 40,000 combinations, and AI should cover all skill sets.

81
00:08:30,935 --> 00:08:35,176
The second, the gameplay's map is very big.

82
00:08:35,176 --> 00:08:44,018
The attacker and defender are both on either side of the map, and the distance of them is about 67 meters.

83
00:08:44,658 --> 00:08:48,100
The agent can only move one meter at one step.

84
00:08:48,100 --> 00:08:50,121
It's about 10 steps per second.

85
00:08:50,121 --> 00:08:55,463
So it's difficult for AI to explore the environment.

86
00:08:55,463 --> 00:09:01,486
Third, we should provide different levels of AI,

87
00:09:01,486 --> 00:09:02,727
including high level.

88
00:09:02,727 --> 00:09:07,329
Especially, the highest level AI should

89
00:09:07,329 --> 00:09:11,191
defeat professional or semi-professional players.

90
00:09:11,191 --> 00:09:14,332
Fourth, the AI should have behavioral diversity.

91
00:09:14,727 --> 00:09:18,668
to enrich players' experience.

92
00:09:18,668 --> 00:09:22,250
Finally, the AI should cover cross-class battle.

93
00:09:22,250 --> 00:09:24,971
That means that the model can be used

94
00:09:24,971 --> 00:09:26,812
against different classes.

95
00:09:26,812 --> 00:09:31,234
When facing with a real game,

96
00:09:31,234 --> 00:09:34,155
if we want to use reinforcement learning

97
00:09:34,155 --> 00:09:35,135
to develop a game AI,

98
00:09:35,135 --> 00:09:40,898
the first important thing is to build a MDP model.

99
00:09:40,898 --> 00:09:44,119
In response to the difficulties mentioned just now,

100
00:09:44,725 --> 00:09:49,508
we have proposed some solutions in terms of modeling.

101
00:09:49,508 --> 00:09:54,352
The first one is how to deal with the huge action space.

102
00:09:54,352 --> 00:09:59,996
We use a zero-one vector state to express equipped skills.

103
00:09:59,996 --> 00:10:05,220
For example, the vector of the picture on the right side, I show on the below.

104
00:10:05,220 --> 00:10:11,945
And then, equipped skills are random initialized, proportional to human data.

105
00:10:13,525 --> 00:10:24,234
As we know, state, action, and reward are three important elements in reinforcement learning.

106
00:10:24,234 --> 00:10:36,664
The state space of this gameplay contains position, status, equippable skills, legal actions, and other important information.

107
00:10:36,664 --> 00:10:39,286
And the action space contains move, jump, and cast skills.

108
00:10:39,286 --> 00:10:41,668
And the reward contains the action space.

109
00:10:42,676 --> 00:10:45,316
The reward has two components.

110
00:10:45,316 --> 00:10:48,157
And the first one is the final result,

111
00:10:48,157 --> 00:10:51,197
to mean win or lose.

112
00:10:51,197 --> 00:10:54,258
It's essential, but it's very sparse

113
00:10:54,258 --> 00:10:57,939
and only can be received at the end of the game.

114
00:10:57,939 --> 00:11:02,359
As we know, this is very difficult.

115
00:11:02,359 --> 00:11:06,900
It's very difficult for reinforcement learning algorithm.

116
00:11:06,900 --> 00:11:09,841
If the problems, reward is very sparse.

117
00:11:09,841 --> 00:11:12,181
So we introduce a dense reward.

118
00:11:12,605 --> 00:11:16,367
which we call HPDiff between game frames.

119
00:11:16,367 --> 00:11:19,908
It's dense, effective, and make RL easy to learn.

120
00:11:19,908 --> 00:11:27,272
There are many combinations of actions.

121
00:11:27,272 --> 00:11:31,114
So we use two ways to improve exploration efficiency.

122
00:11:31,114 --> 00:11:33,876
The first one is use legal action

123
00:11:33,876 --> 00:11:37,377
as a mask layer like the right-hand side network.

124
00:11:37,377 --> 00:11:41,840
The second, we introduce an action via reward.

125
00:11:42,470 --> 00:11:49,211
which is a negative reward when the agent uses an illegal or unavailable action.

126
00:11:49,211 --> 00:11:55,853
These two methods accelerate training and improve the success rate of cut skills.

127
00:11:55,853 --> 00:12:01,894
Another problem is the representation of special states.

128
00:12:01,894 --> 00:12:10,775
In this gameplay, there may be more than one combat unit, such as battle pets, illusions.

129
00:12:10,775 --> 00:12:11,315
Besides,

130
00:12:12,018 --> 00:12:16,920
There are some creation skills, such as traps,

131
00:12:16,920 --> 00:12:19,721
terrain creations, or air swords.

132
00:12:19,721 --> 00:12:26,103
The common solution to solve the problem

133
00:12:26,103 --> 00:12:29,284
is to use the image at the import state.

134
00:12:29,284 --> 00:12:35,126
Of course, it's a good academic solution.

135
00:12:35,126 --> 00:12:39,968
However, if we use the image at the import state,

136
00:12:39,968 --> 00:12:40,808
we shouldn't need.

137
00:12:41,238 --> 00:12:49,143
add convolution or ResNet technology, which will need a lot of GPU and computations.

138
00:12:49,143 --> 00:12:59,669
That will make game AI development very expensive. In this case, we don't use image representation.

139
00:12:59,669 --> 00:13:08,454
Instead, we use the LiDAR representation like the red picture. So we can use small neural network

140
00:13:08,454 --> 00:13:09,935
and drop convolution layers.

141
00:13:10,791 --> 00:13:19,276
that makes faster convergence and less computing resources.

142
00:13:19,276 --> 00:13:23,139
Now we have the model of this game play,

143
00:13:23,139 --> 00:13:25,240
but how to train the AI?

144
00:13:25,240 --> 00:13:28,923
The first idea is to train with the role-based AI.

145
00:13:28,923 --> 00:13:33,866
We found that by training with the role-based AI

146
00:13:33,866 --> 00:13:35,607
built in Justice Online,

147
00:13:35,607 --> 00:13:38,929
the agent got a 100% win rate very quickly.

148
00:13:40,402 --> 00:13:48,969
However, we found that if we use this AI combined with a human player, the agent got confused and acted bad.

149
00:13:48,969 --> 00:14:01,860
The reason is that this agent has never seen other opponents in the training process except the robust AI.

150
00:14:01,860 --> 00:14:04,823
So we use self-play.

151
00:14:05,512 --> 00:14:11,657
Self-play is a good way to solve the problem of overfitting to a particular opponent.

152
00:14:11,657 --> 00:14:18,162
But a new problem arises. It's difficult to make sure whether training is converging

153
00:14:18,162 --> 00:14:29,111
when using self-play. So to solve the problem, RIS provides a module that supports self-play

154
00:14:29,111 --> 00:14:35,075
automated testing. RIS can evaluate with fixed models in the training history.

155
00:14:35,760 --> 00:14:44,263
and compare with these models and plot the result on the TensorBoard just like the red picture shows,

156
00:14:44,263 --> 00:14:52,245
we can easily recognize that the ability of our agent is increasing.

157
00:14:52,245 --> 00:14:58,707
When we use reinforcement learning to develop game AI, we encountered more problems. First,

158
00:14:58,707 --> 00:15:05,090
the big map and original fixed ball positions will cost a long time to learn drawing near.

159
00:15:05,470 --> 00:15:08,431
and poor model generalization.

160
00:15:08,431 --> 00:15:12,452
Second, too many timeout samples at the early stage

161
00:15:12,452 --> 00:15:16,453
lead to low training efficiency and increased training time.

162
00:15:16,453 --> 00:15:20,274
Third, chasing and fleeing is more difficult to learn

163
00:15:20,274 --> 00:15:23,915
than fight because of sparse rewards.

164
00:15:23,915 --> 00:15:26,655
Fourth, due to the large action space

165
00:15:26,655 --> 00:15:30,156
and the settings of random equipped skills,

166
00:15:30,156 --> 00:15:33,897
some common combos become hard to learn.

167
00:15:33,897 --> 00:15:35,417
To solve the problem.

168
00:15:35,838 --> 00:15:38,138
we use curricular learning.

169
00:15:38,138 --> 00:15:46,320
The first curricular learning is to learn chess and three.

170
00:15:46,320 --> 00:15:51,001
The method is we set agent birth distance.

171
00:15:51,001 --> 00:15:55,542
The first, we set from near to far.

172
00:15:55,542 --> 00:16:01,243
First, learn to fight, and then chess and three.

173
00:16:01,243 --> 00:16:04,304
Second, we will randomly set birth positions.

174
00:16:04,725 --> 00:16:08,888
to improve the model generalization.

175
00:16:08,888 --> 00:16:11,990
Another curriculum learning is to learn equipped skills,

176
00:16:11,990 --> 00:16:14,993
which start from fixed to random,

177
00:16:14,993 --> 00:16:17,475
which accelerate learning process

178
00:16:17,475 --> 00:16:19,776
by starting from simple tasks.

179
00:16:19,776 --> 00:16:24,640
The third curriculum learning is fine-tune models.

180
00:16:24,640 --> 00:16:28,363
Fine-tune models with more samples of common skill sets

181
00:16:28,363 --> 00:16:31,666
to achieve better combat performance,

182
00:16:31,666 --> 00:16:34,328
such as skill combos, dodged enemies.

183
00:16:34,917 --> 00:16:39,061
high damage skills, and remove negative effects.

184
00:16:39,061 --> 00:16:47,227
There are seven classes in Justice Online, and we need to cover all class pairs.

185
00:16:47,227 --> 00:16:57,255
If we use one model for one pair, we need to train 14 two models, which will take a long time to train.

186
00:16:57,255 --> 00:17:04,301
So we use cross-class training to solve the problem. That means that one model for one class.

187
00:17:04,873 --> 00:17:08,075
So we only need to train 7 models.

188
00:17:08,075 --> 00:17:13,017
The details are shown on this page.

189
00:17:13,017 --> 00:17:18,539
Each team randomly selects one out of 7 classes to run a game.

190
00:17:18,539 --> 00:17:29,444
Although we can train a high-level AI, but an AI with a fixed behavior style tends to

191
00:17:29,444 --> 00:17:33,786
bore players and make players easy to find a way to cope.

192
00:17:34,635 --> 00:17:38,796
So we need to generate AI with different behavior styles.

193
00:17:38,796 --> 00:17:47,039
Self-play using multiple seeds with different reward shaping

194
00:17:47,039 --> 00:17:48,500
is a way to solve the problem.

195
00:17:48,500 --> 00:17:55,502
The detail I show on the right picture.

196
00:17:55,502 --> 00:17:58,023
We start a train.

197
00:17:58,023 --> 00:18:00,844
We start a train with n groups.

198
00:18:00,844 --> 00:18:04,305
Each group has its own seed and a reward shaping function.

199
00:18:05,131 --> 00:18:12,113
During training, they put models into the same model buffer and select openers from it.

200
00:18:12,113 --> 00:18:22,256
Using this method, we found that each group can train a high-level AI with different behavioral

201
00:18:22,256 --> 00:18:32,619
styles. This page shows three behavioral styles of Shenxiang, including aggressive,

202
00:18:32,619 --> 00:18:34,380
balanced, and defensive.

203
00:18:39,050 --> 00:18:45,935
As we know, people have different levels, and game designers don't need an AI to defeat

204
00:18:45,935 --> 00:18:48,517
all the players.

205
00:18:48,517 --> 00:18:52,019
So we should provide different levels of AI.

206
00:18:52,019 --> 00:19:01,966
The method is that we first train the highest level AI, and then use different difficulty

207
00:19:01,966 --> 00:19:04,788
parameters to imitate human actions.

208
00:19:04,788 --> 00:19:04,989
We use

209
00:19:06,657 --> 00:19:16,205
three dimensions to control AI's level. The first is agile, which means the frequency of cut skills.

210
00:19:16,205 --> 00:19:23,090
The second is accuracy, which means the action, random action rate. The third one is perception,

211
00:19:23,090 --> 00:19:30,316
which means that there is some noise in the input states.

212
00:19:30,316 --> 00:19:36,021
Now we have three dimensions, but how to compute them?

213
00:19:36,567 --> 00:19:42,790
the parameters of different levels of AI, we use an EIO rating system.

214
00:19:42,790 --> 00:19:48,913
At the first step, generate all AI players with different parameters.

215
00:19:48,913 --> 00:19:53,155
The second, match opponents for each player according to the rank.

216
00:19:53,155 --> 00:19:58,857
The third, compute EIO changes according to the combined results.

217
00:19:58,857 --> 00:20:05,400
Fourth, update the leaderboard. If not run finish, go step one.

218
00:20:05,400 --> 00:20:05,780
Finally,

219
00:20:06,766 --> 00:20:12,547
output the leaderboard after reaching the max round.

220
00:20:12,547 --> 00:20:15,088
A table on the right shows we can

221
00:20:15,088 --> 00:20:19,188
see that with the increase of the difficulty control

222
00:20:19,188 --> 00:20:24,209
parameters, the ability of the AI decreases continuously.

223
00:20:24,209 --> 00:20:32,291
In addition to previous solutions mentioned just now,

224
00:20:32,291 --> 00:20:34,892
we met a lot of problems when we use.

225
00:20:35,410 --> 00:20:38,472
when we use reinforcement learning to develop game AI,

226
00:20:38,472 --> 00:20:43,995
the most common problem is that the poor model performance,

227
00:20:43,995 --> 00:20:48,677
including no convergent, low level, unusual behavior.

228
00:20:48,677 --> 00:20:54,380
We summarize some debug methods and the list here.

229
00:20:54,380 --> 00:20:58,862
First, check the correctness of the game interface.

230
00:20:58,862 --> 00:21:01,724
Second, check the correctness of the training code.

231
00:21:01,724 --> 00:21:05,185
Third, check whether there are missing states.

232
00:21:06,341 --> 00:21:11,786
Fourth, build a simple task for faster validation.

233
00:21:11,786 --> 00:21:16,310
This video shows the reinforcement learning AI

234
00:21:16,310 --> 00:21:18,772
combats against a top human player.

235
00:21:18,772 --> 00:21:24,077
As you can see from the video, our AI has learned a lot

236
00:21:24,077 --> 00:21:25,198
of tricks.

237
00:21:25,198 --> 00:21:30,242
And by properly using custom skills and moving,

238
00:21:30,242 --> 00:21:33,725
the AI defeated the human player.

239
00:21:34,756 --> 00:21:36,390
and the red one is the AI.

240
00:22:06,612 --> 00:22:09,954
Since we deployed the AI service online,

241
00:22:09,954 --> 00:22:11,796
the peak TPS exceeds 8,000,

242
00:22:11,796 --> 00:22:15,578
and the daily request about 16 million.

243
00:22:15,578 --> 00:22:18,760
It can be seen from the table that,

244
00:22:18,760 --> 00:22:21,002
compared with the road-based AI,

245
00:22:21,002 --> 00:22:26,285
the high-level RAI has significantly improved

246
00:22:26,285 --> 00:22:31,188
in terms of win-win and battle duration.

247
00:22:31,188 --> 00:22:34,811
In addition to these indicators,

248
00:22:34,811 --> 00:22:35,892
the AI, the new AI has generated

249
00:22:36,120 --> 00:22:44,986
a lot of discussion in player forum, and most players are interested in the new AI.

250
00:22:44,986 --> 00:22:50,329
Some players said, oh, I thought it was a real person, not an AI.

251
00:22:50,329 --> 00:22:54,772
And someone said, the Su-1 AI is completely reasonable.

252
00:22:54,772 --> 00:22:59,414
It suggested that human players should learn from the AI.

253
00:22:59,414 --> 00:23:04,177
In addition to developer game AI, we found that

254
00:23:04,518 --> 00:23:08,299
reinforcement learning can do more things, such as balance tests.

255
00:23:08,299 --> 00:23:15,600
Before launching Longin Class, we used reinforcement learning to test the class balance.

256
00:23:15,600 --> 00:23:24,202
Game designers made several iterations of skills and values according to the test result.

257
00:23:24,202 --> 00:23:34,164
On the first few pages, I introduce our achievements in single-agent MMORPG environment.

258
00:23:35,068 --> 00:23:40,150
And now I will introduce our work on the multi-agent sports

259
00:23:40,150 --> 00:23:42,131
game, FIWARE Basketball.

260
00:23:42,131 --> 00:23:44,593
The gameplay is a 3D scene.

261
00:23:44,593 --> 00:23:48,475
And the designers want us to develop a human-like AI.

262
00:23:48,475 --> 00:23:52,417
Of course, they need different levels

263
00:23:52,417 --> 00:23:54,498
AI for different players.

264
00:23:54,498 --> 00:24:01,461
The biggest difference between Liu Pai Jing and FIWARE

265
00:24:01,461 --> 00:24:03,442
Basketball is that.

266
00:24:03,749 --> 00:24:06,811
It's a marting agent problem.

267
00:24:06,811 --> 00:24:12,755
We use two different methods for offense and defense.

268
00:24:12,755 --> 00:24:17,118
For offense, we use a strategic model.

269
00:24:17,118 --> 00:24:19,619
Why use a strategic model?

270
00:24:19,619 --> 00:24:24,743
Some primitive actions are important for team offense,

271
00:24:24,743 --> 00:24:28,005
such as passports and pick and roll.

272
00:24:28,005 --> 00:24:30,947
Introduce a coach role to perform team actions in order

273
00:24:30,947 --> 00:24:32,268
to improve team rewards.

274
00:24:32,774 --> 00:24:36,875
which simplifies learning advanced strategies, such as pick and roll,

275
00:24:36,875 --> 00:24:52,678
reduce the action space of the original model, since some actions can only be executed by the coach, which is controlled by the strategic model,

276
00:24:52,678 --> 00:24:56,839
and then avoid passing the buck among teammates.

277
00:24:56,839 --> 00:25:02,220
Just as the red gift shows, the AIs pass the buck when the offensive time is up.

278
00:25:03,253 --> 00:25:05,896
when there is no coach.

279
00:25:05,896 --> 00:25:14,663
This page shows the structure of the original model and the strategic model.

280
00:25:14,663 --> 00:25:18,446
On each tick, the coach outputs the strategic action.

281
00:25:18,446 --> 00:25:27,133
Then the strategic action will be sent to the corresponding player if the action is not known.

282
00:25:27,133 --> 00:25:32,978
Finally, each player executes the strategic action if he receives it from the coach.

283
00:25:33,561 --> 00:25:40,246
and else execute the action from the original model.

284
00:25:40,246 --> 00:25:46,231
We found that defense and offense are a little different.

285
00:25:46,231 --> 00:25:50,614
When you play on the defense team, the most important thing

286
00:25:50,614 --> 00:25:53,697
is to decide who to defend.

287
00:25:53,697 --> 00:25:56,479
However, it's difficult to design rewards

288
00:25:56,479 --> 00:26:00,022
to achieve defense or coordination.

289
00:26:00,022 --> 00:26:01,523
Besides, it's difficult for a model

290
00:26:01,523 --> 00:26:03,204
to learn coordination behaviors.

291
00:26:03,672 --> 00:26:05,353
by primitive defense actions.

292
00:26:05,353 --> 00:26:11,558
To solve the problem, we use a hierarchical multi-agent

293
00:26:11,558 --> 00:26:15,481
reinforcement learning for defense.

294
00:26:15,481 --> 00:26:21,125
There are two level policies on defense.

295
00:26:21,125 --> 00:26:25,328
The high-level policies are learned

296
00:26:25,328 --> 00:26:28,771
by multi-agent algorithms, such as ComNet, QMIS.

297
00:26:28,771 --> 00:26:31,373
And the low-level policies are learned

298
00:26:31,373 --> 00:26:32,273
by single-agent algorithms.

299
00:26:34,362 --> 00:26:39,426
The result of our experiments on the right picture

300
00:26:39,426 --> 00:26:43,309
show that the policy learned by the Hierarchical Multi-Agent

301
00:26:43,309 --> 00:26:46,592
Reinforcement Learning are significantly

302
00:26:46,592 --> 00:26:51,016
better than that learned by the Common Reinforcement Learning

303
00:26:51,016 --> 00:26:54,259
algorithms.

304
00:26:54,259 --> 00:26:58,262
This page introduced the details of the Hierarchical Multi-Agent

305
00:26:58,262 --> 00:27:00,404
Reinforcement Learning for Defense.

306
00:27:01,561 --> 00:27:06,544
High-level policy decides who and how to defend, which we call the goal.

307
00:27:06,544 --> 00:27:10,527
A low-level policy outputs primitive action.

308
00:27:10,527 --> 00:27:16,512
An agent's low-level observation only contains the information about itself and the target

309
00:27:16,512 --> 00:27:21,935
offensive player who is chosen by the high-level policy.

310
00:27:21,935 --> 00:27:24,317
Each goal is executed for up to 15 steps.

311
00:27:24,317 --> 00:27:31,362
High-level policy gets a sparse reward of minus one or plus one at the end of the episode.

312
00:27:32,556 --> 00:27:36,058
easy to shape rewards for low-level policy with a goal.

313
00:27:36,058 --> 00:27:47,646
The left video shows the one match that the RL AI combats with human players.

314
00:27:47,646 --> 00:27:55,991
Like Justice Online, the RL AI also generates a lot of discussions, and I selected a few

315
00:27:55,991 --> 00:27:58,933
of them and put them on the right.

316
00:28:02,010 --> 00:28:04,592
Besides the Justice Online and the Fiber Basketball,

317
00:28:04,592 --> 00:28:07,194
the reinforcement learning technology

318
00:28:07,194 --> 00:28:09,976
has been applied to a lot of games.

319
00:28:09,976 --> 00:28:15,859
This page shows the AI we trained in other four games.

320
00:28:15,859 --> 00:28:20,703
Our reinforcement learning team also published

321
00:28:20,703 --> 00:28:22,784
many high quality academic papers

322
00:28:22,784 --> 00:28:26,727
based on the result of game AI development.

323
00:28:26,727 --> 00:28:31,130
In the future, we'll do more work.

324
00:28:32,264 --> 00:28:38,247
We may explore multi-agent scenes in MMORPGs.

325
00:28:38,247 --> 00:28:44,129
We want to make human-like AI in FPS games.

326
00:28:44,129 --> 00:28:51,793
We want to explore model-based algorithms in card games and board games, such as Alpha-Beta Search, MCTS.

327
00:28:51,793 --> 00:28:57,456
Also, we want to make smarter AI for auto-test in games.

328
00:28:57,456 --> 00:29:00,898
As we know, this is very important for game development.

329
00:29:04,127 --> 00:29:10,328
I will appreciate my colleagues.

330
00:29:10,328 --> 00:29:10,629
Thank you.

