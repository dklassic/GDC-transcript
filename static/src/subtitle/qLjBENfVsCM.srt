1
00:00:06,208 --> 00:00:07,749
Thank you for coming out to this panel.

2
00:00:07,769 --> 00:00:09,051
This is something that a lot of us

3
00:00:09,311 --> 00:00:11,673
are really excited about getting to discuss,

4
00:00:11,734 --> 00:00:13,195
especially here at GDC.

5
00:00:14,757 --> 00:00:16,979
So a few months back, when I had pitched

6
00:00:17,019 --> 00:00:20,563
the idea of doing a panel on ethics and AI here at GDC,

7
00:00:20,583 --> 00:00:23,946
I actually got a lot of feedback from game devs saying,

8
00:00:24,126 --> 00:00:25,328
ethics and AI?

9
00:00:25,488 --> 00:00:27,670
But we're at a games conference.

10
00:00:28,569 --> 00:00:33,692
If this was a social media conference or a conference that was maybe focused on machine learning,

11
00:00:34,032 --> 00:00:38,054
then sure, but why would we talk about ethics at a games conference?

12
00:00:38,935 --> 00:00:44,498
And hearing this feedback from game devs was what made me realize just how important having a panel

13
00:00:44,538 --> 00:00:50,401
like this should be, especially as we're starting to see the spectrum of game AI grow into so many

14
00:00:50,441 --> 00:00:56,225
different areas over the past few years. When social media platforms were being built

15
00:00:56,977 --> 00:01:00,059
ethical and privacy concerns were not their focus.

16
00:01:00,559 --> 00:01:01,659
And look at where we are today.

17
00:01:02,800 --> 00:01:05,881
When machine learning was brought into the education system,

18
00:01:06,342 --> 00:01:08,182
ethical considerations were not made.

19
00:01:08,663 --> 00:01:11,384
And it caused major issues because of biases

20
00:01:11,444 --> 00:01:13,485
that had been programmed into their systems.

21
00:01:14,585 --> 00:01:17,146
With big data comes big responsibility.

22
00:01:19,668 --> 00:01:21,829
When our panelists actually got together the other day,

23
00:01:21,849 --> 00:01:24,050
an interesting statement was made.

24
00:01:24,611 --> 00:01:26,912
And it was that discussing ethics

25
00:01:27,152 --> 00:01:28,853
had always seemed very taboo.

26
00:01:29,233 --> 00:01:30,113
And we wondered why.

27
00:01:31,054 --> 00:01:33,135
Is it because companies and individuals

28
00:01:33,195 --> 00:01:35,877
are scared to admit that they aren't necessarily

29
00:01:35,917 --> 00:01:37,598
the experts in a given field?

30
00:01:38,178 --> 00:01:39,559
That they actually need help?

31
00:01:40,339 --> 00:01:44,321
That maybe a system, game, or tool that was built by a company

32
00:01:45,763 --> 00:01:50,386
was coded with biases that could be offensive or even dangerous to some people.

33
00:01:51,367 --> 00:01:57,691
But where one would rather ship a product than actually have their product be scrutinized and redesigned for improvements.

34
00:01:59,392 --> 00:02:02,413
Through AI we are able to do amazing things.

35
00:02:02,994 --> 00:02:09,298
But without proper considerations around biases and ethical implications, things can easily go south.

36
00:02:10,991 --> 00:02:14,216
So today, I'm joined by some amazing developers

37
00:02:14,516 --> 00:02:16,379
from different areas of expertise

38
00:02:16,719 --> 00:02:19,122
who work with and use AI across a number

39
00:02:19,162 --> 00:02:21,606
of very different areas, and who have all

40
00:02:21,646 --> 00:02:23,849
been advocates in their respective fields

41
00:02:23,969 --> 00:02:25,591
around ethics and games and AI.

42
00:02:26,932 --> 00:02:30,273
We're going to cover quite a wide range of topics today,

43
00:02:30,353 --> 00:02:34,254
from biases in game AI to data and privacy concerns,

44
00:02:34,874 --> 00:02:38,775
and areas of AI that transcend from traditional video games

45
00:02:38,815 --> 00:02:40,615
like XR and digital assistants.

46
00:02:41,396 --> 00:02:43,176
And then we'd like to spend some time

47
00:02:43,196 --> 00:02:44,697
where we open it up to the audience,

48
00:02:44,717 --> 00:02:47,117
because we really want to see what do you all think

49
00:02:47,537 --> 00:02:49,478
are areas that we should all be addressing

50
00:02:49,518 --> 00:02:50,838
when it comes to ethics in AI.

51
00:02:52,479 --> 00:02:55,620
So, hello, my name is Alicia Leidecker.

52
00:02:56,621 --> 00:02:59,061
I'm one of the advisors here at the AI Summit,

53
00:02:59,762 --> 00:03:02,623
and during my day job, I work on XR

54
00:03:02,863 --> 00:03:05,344
as Director of Developer Experience at Magic Leap.

55
00:03:06,104 --> 00:03:07,944
Prior to that, I was lead AI

56
00:03:08,224 --> 00:03:09,945
on many of the Assassin's Creed titles.

57
00:03:10,765 --> 00:03:14,307
So we're going to start off by having our panelists, if you'd like to start, Emily,

58
00:03:14,867 --> 00:03:17,509
talk a little bit about yourselves and some of the work that you do.

59
00:03:17,849 --> 00:03:18,069
Sure.

60
00:03:18,889 --> 00:03:19,710
I'm Emily Short.

61
00:03:19,910 --> 00:03:25,953
I'm Chief Product Officer at Spirit AI, and what we do at Spirit is middleware for games.

62
00:03:26,833 --> 00:03:28,074
So that includes...

63
00:03:28,814 --> 00:03:34,400
a product called Character Engine, which does dialogue for NPCs and how they can respond

64
00:03:34,561 --> 00:03:38,285
to natural language input or other kinds of input from players.

65
00:03:39,046 --> 00:03:44,432
And the second product, which probably has the greater application in this area, is Ally,

66
00:03:44,572 --> 00:03:49,778
which is a community moderation tool that looks at toxic behavior within communities.

67
00:03:50,238 --> 00:03:53,399
and helps give community managers an opportunity to see

68
00:03:53,439 --> 00:03:57,359
in a triage dashboard who is causing the most trouble

69
00:03:57,760 --> 00:04:00,320
in this space and what are the biggest concerns

70
00:04:00,360 --> 00:04:01,820
that we should be looking at moderating.

71
00:04:01,861 --> 00:04:04,521
So we're not just waiting for things to be reported

72
00:04:04,561 --> 00:04:07,262
by players, but we're actually able to surface issues

73
00:04:07,722 --> 00:04:08,462
in the community.

74
00:04:08,842 --> 00:04:10,663
And obviously both of those products,

75
00:04:10,703 --> 00:04:13,363
and especially Ally, raise a lot of questions

76
00:04:13,443 --> 00:04:14,723
and things that we need to think about

77
00:04:14,823 --> 00:04:17,244
about how do we train to look for these things?

78
00:04:17,684 --> 00:04:18,866
What data are we using?

79
00:04:18,966 --> 00:04:20,027
How are we tagging it?

80
00:04:20,107 --> 00:04:23,130
How do we decide what's offensive, what's racist,

81
00:04:23,170 --> 00:04:24,011
what's appropriate?

82
00:04:24,452 --> 00:04:26,534
And then how do we make use of that information

83
00:04:26,574 --> 00:04:27,235
when we have it?

84
00:04:27,375 --> 00:04:30,299
How do we protect the privacy of the clients

85
00:04:30,379 --> 00:04:32,922
and of the players that are making use of the system?

86
00:04:33,262 --> 00:04:34,844
And then when we're applying characters

87
00:04:34,884 --> 00:04:36,025
that can respond to risk.

88
00:04:37,166 --> 00:04:40,570
input in interactions, how do we make sure that players

89
00:04:40,610 --> 00:04:42,933
who are interacting with those characters understand

90
00:04:42,973 --> 00:04:45,256
that they're interacting with an AI and they form

91
00:04:45,276 --> 00:04:47,419
an appropriate rather than an inappropriate kind

92
00:04:47,439 --> 00:04:48,680
of connection with that character?

93
00:04:48,700 --> 00:04:50,643
Thank you.

94
00:04:51,495 --> 00:04:51,836
Celia.

95
00:04:52,236 --> 00:04:52,497
Hey.

96
00:04:52,677 --> 00:04:54,319
So my name is Celia Hulandt, and I

97
00:04:54,459 --> 00:04:58,243
am the least knowledgeable person about AI on this panel.

98
00:04:58,844 --> 00:05:01,047
My background is in psychology, actually.

99
00:05:01,067 --> 00:05:02,308
I have a PhD in psychology.

100
00:05:03,690 --> 00:05:06,213
I specialize in child development

101
00:05:06,333 --> 00:05:08,836
and development of cognitive.

102
00:05:09,557 --> 00:05:10,217
psychology.

103
00:05:11,519 --> 00:05:13,260
I've been working in the game industry for the past 10 years.

104
00:05:13,681 --> 00:05:18,646
I started at Ubisoft in France, I'm French, and then moved to Ubisoft Montreal.

105
00:05:19,647 --> 00:05:23,991
I worked at the Playtest Lab there and also worked with the Rainbow Six franchise.

106
00:05:24,711 --> 00:05:29,796
I moved to LucasArts, working on Star Wars games like 1313 that never saw the light of

107
00:05:29,836 --> 00:05:30,317
day, sadly.

108
00:05:31,097 --> 00:05:37,180
And then moved to Epic Games in 2013 to be Director of User Experience at Epic,

109
00:05:37,380 --> 00:05:39,241
because now I'm specialized in game UX.

110
00:05:39,281 --> 00:05:42,603
This is how my background meets game development.

111
00:05:43,123 --> 00:05:46,224
And so I worked on all the different products at Epic,

112
00:05:46,244 --> 00:05:48,365
so Unreal Engine and of course Fortnite.

113
00:05:49,246 --> 00:05:53,848
I left Epic late 2017 and now I'm a consultant in freelance

114
00:05:54,168 --> 00:05:56,329
and also wrote a book called The Gamer's Brain.

115
00:05:57,369 --> 00:06:06,093
And so I'm very interested into understanding how we can use psychology in developing products for good or evil.

116
00:06:07,333 --> 00:06:09,374
So all these questions are really, really interesting to me.

117
00:06:11,153 --> 00:06:12,094
Awesome, thank you.

118
00:06:13,054 --> 00:06:13,414
Timony.

119
00:06:13,434 --> 00:06:15,536
Hi, I'm Timony West.

120
00:06:15,776 --> 00:06:18,738
I'm the Director of Augmented and Virtual Reality Research

121
00:06:18,778 --> 00:06:19,499
at Unity Labs.

122
00:06:21,140 --> 00:06:23,382
My background was originally in product design

123
00:06:23,882 --> 00:06:25,283
in social media and others,

124
00:06:25,623 --> 00:06:28,725
and I've spent my entire career trying to figure out

125
00:06:28,825 --> 00:06:33,149
how to get data from people, largely personal information,

126
00:06:33,589 --> 00:06:35,790
and then give it back to them in a way that makes sense

127
00:06:35,890 --> 00:06:36,891
and is useful for them.

128
00:06:37,532 --> 00:06:39,114
And when it comes to spatial computing,

129
00:06:39,154 --> 00:06:41,376
that has been an even bigger conversation,

130
00:06:41,456 --> 00:06:44,140
because we are literally creating tools that

131
00:06:44,180 --> 00:06:46,222
let you record everything about your house,

132
00:06:46,663 --> 00:06:47,944
everything about the way you move,

133
00:06:48,024 --> 00:06:49,186
the way you're moving your device,

134
00:06:49,686 --> 00:06:52,109
and then try to put that both back into the engine

135
00:06:52,209 --> 00:06:54,332
and then into your game or to your experience

136
00:06:54,392 --> 00:06:55,594
in a way where it makes sense.

137
00:06:56,634 --> 00:06:58,215
and actually adds additional value.

138
00:06:58,455 --> 00:07:01,175
So I fundamentally believe that if we take in information

139
00:07:01,676 --> 00:07:03,536
from these devices and from our users,

140
00:07:03,616 --> 00:07:05,857
we have an ethical consideration to give back

141
00:07:06,337 --> 00:07:08,017
more than we've got, and that is,

142
00:07:08,357 --> 00:07:10,518
especially when it comes to having devices

143
00:07:10,558 --> 00:07:13,699
that have 10 different cameras watching your every move

144
00:07:13,719 --> 00:07:14,979
and listening to your every breath,

145
00:07:15,920 --> 00:07:17,320
something we really need to take seriously.

146
00:07:17,520 --> 00:07:18,600
So I'm glad to be here talking about it.

147
00:07:21,204 --> 00:07:24,505
So I'm Luke Dickin, I'm director for Central and Strategic Analytics.

148
00:07:24,565 --> 00:07:26,105
You might have heard me talk here yesterday.

149
00:07:26,125 --> 00:07:28,005
I will try not to shout at you so much today.

150
00:07:29,826 --> 00:07:35,607
So I'm at Zynga and we are kind of well known as a data driven games company.

151
00:07:35,647 --> 00:07:39,368
We've kind of come to prominence in maybe 2009 kind of era.

152
00:07:40,688 --> 00:07:42,548
You know, there are quotes flying around that,

153
00:07:42,648 --> 00:07:43,748
oh, we're not a games company,

154
00:07:43,768 --> 00:07:46,449
we're an analytics company masquerading as a game studio.

155
00:07:47,710 --> 00:07:55,135
So we've been kind of collecting a lot of data over the years, and I think that we do a pretty good job with it.

156
00:07:55,315 --> 00:07:59,138
So I'm really excited to kind of come here and talk to you about the way that we do that.

157
00:08:00,692 --> 00:08:02,113
Awesome, thank you.

158
00:08:02,493 --> 00:08:04,415
So, sorry Luke, we're actually gonna start off

159
00:08:04,455 --> 00:08:06,936
by talking about NPCs, I know yesterday you ran into it.

160
00:08:06,956 --> 00:08:07,797
And I'm gonna start shouting again.

161
00:08:07,937 --> 00:08:08,378
About this.

162
00:08:09,899 --> 00:08:11,740
Yeah, that's fine, let's shout.

163
00:08:13,522 --> 00:08:16,424
So my first question to the panelist is,

164
00:08:17,324 --> 00:08:20,307
what are some examples that you've seen in video games

165
00:08:20,947 --> 00:08:23,389
where you've seen biases that might have been programmed

166
00:08:23,429 --> 00:08:25,531
around characters and systems AI?

167
00:08:28,043 --> 00:08:28,823
Anyone want to start?

168
00:08:28,843 --> 00:08:33,884
Yeah, I remember when I was playing Watch Dogs,

169
00:08:34,444 --> 00:08:36,484
there was a lot that the AI was always,

170
00:08:37,525 --> 00:08:39,705
you would see like women always getting

171
00:08:41,165 --> 00:08:43,146
violated and in trouble.

172
00:08:43,346 --> 00:08:50,087
And as a player, so you're incarnating a male character

173
00:08:50,187 --> 00:08:52,867
and always coming here to help out the women.

174
00:08:53,367 --> 00:08:55,128
And the women always need.

175
00:08:55,788 --> 00:08:57,630
someone to get out of that violence.

176
00:08:57,690 --> 00:09:00,293
And so the women are, in that game,

177
00:09:00,513 --> 00:09:04,237
are massively portrayed as getting beat up

178
00:09:04,437 --> 00:09:05,678
and needing some help.

179
00:09:10,865 --> 00:09:13,569
I always like to diss Navi from Ocarina of Time.

180
00:09:13,829 --> 00:09:14,711
How many people play that?

181
00:09:15,912 --> 00:09:16,593
It's not even fair.

182
00:09:16,654 --> 00:09:17,395
She's just clippy.

183
00:09:17,515 --> 00:09:17,855
I get it.

184
00:09:18,276 --> 00:09:22,843
But it was an early attempt to have the user sort of guided

185
00:09:23,223 --> 00:09:25,186
and given context as they go through experience.

186
00:09:25,727 --> 00:09:27,308
But she was extraordinarily annoying.

187
00:09:27,368 --> 00:09:28,749
She just wanted you to stay on task.

188
00:09:28,809 --> 00:09:30,570
And the great thing about Ocarina

189
00:09:30,610 --> 00:09:32,491
was the first time you could really run around in 3D

190
00:09:32,531 --> 00:09:33,452
in Hyrule, right?

191
00:09:33,512 --> 00:09:34,853
So that's what you're going to do.

192
00:09:35,393 --> 00:09:37,174
And she was just there being annoying,

193
00:09:37,494 --> 00:09:38,855
telling you to go to the next temple.

194
00:09:38,875 --> 00:09:39,796
And you're like, no, Navi.

195
00:09:39,816 --> 00:09:41,097
I just want to ride my horse.

196
00:09:41,197 --> 00:09:41,737
Leave me alone.

197
00:09:46,821 --> 00:09:47,301
Anybody else?

198
00:09:48,190 --> 00:09:53,416
I mean one that jumps out to me I think is, you know, if you look at sort of systems like

199
00:09:53,456 --> 00:09:59,463
in RimWorld, RimWorld has a whole kind of deep simulation system but a lot of it kind

200
00:09:59,503 --> 00:10:06,090
of gets represented as very, it's an example of a way that's very easy to get NPCs doing

201
00:10:06,130 --> 00:10:06,811
things that are very...

202
00:10:08,635 --> 00:10:12,798
kind of stereotypical because it's leaning into things like,

203
00:10:13,018 --> 00:10:14,679
oh, this person has a misogynist trait,

204
00:10:14,739 --> 00:10:16,240
this person has a misandrist trait.

205
00:10:16,781 --> 00:10:19,883
And it's kind of, it raises some interesting questions

206
00:10:19,923 --> 00:10:22,285
about like, does that actually need to be part

207
00:10:22,325 --> 00:10:24,266
of the AI system and what we're actually

208
00:10:24,806 --> 00:10:26,007
kind of representing?

209
00:10:26,908 --> 00:10:28,109
Or is there something kind of,

210
00:10:29,790 --> 00:10:31,651
I guess it comes back to the question of taboo, right?

211
00:10:32,312 --> 00:10:34,273
Like, if it's taboo for us to talk about it here,

212
00:10:34,313 --> 00:10:35,394
is it taboo for us to like?

213
00:10:35,814 --> 00:10:36,295
Express it.

214
00:10:36,475 --> 00:10:36,835
Exactly.

215
00:10:37,203 --> 00:10:41,544
So it was interesting for RimWorld because that was one programmer, one AI programmer,

216
00:10:41,624 --> 00:10:46,285
who decided to program what they thought the rules were for how men and women should behave

217
00:10:46,305 --> 00:10:50,386
in the game and how sexuality should behave in that game.

218
00:10:51,547 --> 00:10:55,788
What ended up happening is the community ended up going on Reddit and basically demanding

219
00:10:55,828 --> 00:10:58,228
the programmer to remove some of those biases.

220
00:10:58,708 --> 00:11:03,410
And the only reason that that person made changes was because the community demanded

221
00:11:03,490 --> 00:11:03,750
for it.

222
00:11:04,255 --> 00:11:09,924
So what do we think that we could actually do to help put in place processes?

223
00:11:10,105 --> 00:11:15,314
Or how can developers be more mindful about coding systems without biases that have been

224
00:11:15,694 --> 00:11:16,195
put in place?

225
00:11:17,695 --> 00:11:21,938
Well, I think some of the approaches that we see brought

226
00:11:22,078 --> 00:11:26,502
into narrative contexts around having sensitivity feedback,

227
00:11:26,542 --> 00:11:30,365
having people comment who belong to particular groups that

228
00:11:30,405 --> 00:11:33,167
might have a little bit of more understanding of what does

229
00:11:33,207 --> 00:11:34,408
this portrayal mean for us?

230
00:11:34,428 --> 00:11:36,790
What is this kind of representation likely

231
00:11:36,850 --> 00:11:37,871
to convey about us?

232
00:11:38,371 --> 00:11:40,333
Have a look at the work and give some feedback.

233
00:11:40,353 --> 00:11:44,296
And intentionally seeking out sensitivity players has been

234
00:11:44,336 --> 00:11:45,637
really useful for story games.

235
00:11:45,657 --> 00:11:46,138
And I think it's

236
00:11:46,558 --> 00:11:48,779
it's entirely possible to bring that kind of approach

237
00:11:48,879 --> 00:11:51,761
also into looking at systemic representations.

238
00:11:52,542 --> 00:11:54,843
And I would add to that, it's not necessarily the case

239
00:11:54,923 --> 00:11:58,666
that systemically representing negative attitudes

240
00:11:58,706 --> 00:12:02,088
on the parts of characters, like having a racist character

241
00:12:02,148 --> 00:12:04,029
is not necessarily inherently evil.

242
00:12:04,069 --> 00:12:06,911
But I think we need to be careful about how

243
00:12:06,931 --> 00:12:08,292
we represent and unpack that.

244
00:12:09,253 --> 00:12:11,974
And so having people be able to intentionally

245
00:12:12,054 --> 00:12:14,356
seek that kind of feedback about their system is quite important.

246
00:12:15,082 --> 00:12:18,925
Actually, that is the curious thing about being the creator of a system.

247
00:12:19,005 --> 00:12:22,167
If you're trying to mimic a real world system, you may or may not agree with.

248
00:12:22,547 --> 00:12:25,429
Because just inherently in creating and defining the system,

249
00:12:25,469 --> 00:12:28,571
it sort of feels like you're giving it the thumbs up, even if you're not.

250
00:12:28,992 --> 00:12:32,394
You know, like, because I am replicating this, I feel like the sort of

251
00:12:32,954 --> 00:12:36,497
implicit acknowledgement that the system is here,

252
00:12:36,957 --> 00:12:38,658
even if I completely disagree with it.

253
00:12:38,958 --> 00:12:40,099
So just an interesting tension.

254
00:12:40,339 --> 00:12:41,220
There's no real solution there.

255
00:12:44,694 --> 00:12:49,117
I know when we were on our call the other day, we brought up some interesting discussions

256
00:12:49,178 --> 00:12:54,922
around just what the role of a programmer for systems and AI means, and actually about

257
00:12:54,962 --> 00:12:57,043
the education of programmers.

258
00:12:57,523 --> 00:13:02,627
I think a few of us brought up how, who here, actually who's a programmer, has done some

259
00:13:02,667 --> 00:13:05,329
type of ethical class when you were in university?

260
00:13:06,753 --> 00:13:11,676
a lot of us, right? Or quite a few of us. I remember personally for myself that it was

261
00:13:11,716 --> 00:13:17,879
kind of that class that you didn't take seriously. And it was actually very focused on business

262
00:13:18,259 --> 00:13:24,782
rather than actual ethical areas that we should consider as programmers. So what do we think

263
00:13:24,802 --> 00:13:28,124
that we can do on the education side for programmers and designers?

264
00:13:29,039 --> 00:13:30,379
Maybe don't even call it ethics.

265
00:13:30,439 --> 00:13:31,820
It doesn't really matter what the ethics are.

266
00:13:31,840 --> 00:13:33,500
That's like a social thing.

267
00:13:33,520 --> 00:13:35,061
You know, it changes from country to country

268
00:13:35,101 --> 00:13:36,001
or even region to region.

269
00:13:36,041 --> 00:13:38,342
But just being able to create or have a class

270
00:13:38,362 --> 00:13:41,463
that teaches you the underlying socialization systems

271
00:13:41,943 --> 00:13:44,123
so that you can confirm your own bias

272
00:13:44,203 --> 00:13:46,004
and know that it's a bias, right?

273
00:13:46,124 --> 00:13:48,945
Recognize which part is the socialization aspect

274
00:13:48,985 --> 00:13:50,865
and how it differs across different cultures.

275
00:13:51,485 --> 00:13:53,506
And then it takes out the is this good or bad?

276
00:13:53,606 --> 00:13:56,527
And it's just like, this is a tendency that you have.

277
00:13:57,107 --> 00:13:58,887
Acknowledge it and then see if you wanna build it

278
00:13:58,927 --> 00:14:00,427
into the system you're designing or not.

279
00:14:03,628 --> 00:14:07,489
Yeah, maybe we should call it unconscious bias

280
00:14:08,069 --> 00:14:10,650
more than ethics because ethics is a set of values

281
00:14:10,670 --> 00:14:13,410
that we decide and who is deciding how we decide.

282
00:14:13,510 --> 00:14:16,191
It's complicated, but if we talk about unconscious bias

283
00:14:16,251 --> 00:14:19,332
and we explain that as humans, we are fallible

284
00:14:19,492 --> 00:14:20,732
and we all have biases.

285
00:14:21,429 --> 00:14:23,790
And it's okay, we need to admit it and accept it.

286
00:14:24,951 --> 00:14:26,812
But then we need to work around it,

287
00:14:26,872 --> 00:14:29,174
because we can't, even if we know our biases,

288
00:14:29,254 --> 00:14:31,416
we still can't get away from them.

289
00:14:32,116 --> 00:14:35,058
So we need to design the environment to go around them.

290
00:14:35,959 --> 00:14:36,640
The example of...

291
00:14:37,800 --> 00:14:39,081
I'm going to try to do that fast.

292
00:14:39,101 --> 00:14:41,721
But the example of if you want to hire more women,

293
00:14:43,462 --> 00:14:46,023
we are discriminated against women, even women

294
00:14:46,163 --> 00:14:46,883
against other women.

295
00:14:47,243 --> 00:14:49,724
And so if you take the example of the orchestra,

296
00:14:49,744 --> 00:14:52,304
I think it was in Boston, they wanted to hire more women

297
00:14:52,344 --> 00:14:54,045
because they figured, how come we

298
00:14:54,085 --> 00:14:59,746
don't have that many musicians in our group that are women?

299
00:15:00,487 --> 00:15:04,088
And so to avoid being discriminated against women,

300
00:15:04,448 --> 00:15:06,930
they started to do blind auditions.

301
00:15:07,090 --> 00:15:09,891
So they wear a curtain, and they couldn't see.

302
00:15:10,251 --> 00:15:13,613
And also, later on, they added carpets,

303
00:15:13,753 --> 00:15:16,414
because they could still hear the high heels of the women.

304
00:15:16,954 --> 00:15:20,876
But after they added the carpet and the curtain,

305
00:15:21,416 --> 00:15:24,958
then they hired, I think it was, plus 30% to 50% more women

306
00:15:25,018 --> 00:15:25,678
in the orchestra.

307
00:15:30,220 --> 00:15:31,461
So we are biased.

308
00:15:32,001 --> 00:15:32,661
It's happening.

309
00:15:33,461 --> 00:15:35,162
We have to acknowledge it, and we

310
00:15:35,182 --> 00:15:38,463
have to understand how it works and how we perpetuate them.

311
00:15:38,643 --> 00:15:41,424
And so in AI or in any system, if we

312
00:15:41,524 --> 00:15:43,865
don't take that into account, we are

313
00:15:43,905 --> 00:15:47,186
going to perpetuate all these discriminations.

314
00:15:47,646 --> 00:15:49,226
Just going to give one more example from AI.

315
00:15:50,854 --> 00:15:54,076
I love that example because it's pretty clear.

316
00:15:54,096 --> 00:15:58,520
In Turkish, there is no gender when you talk.

317
00:16:00,201 --> 00:16:03,223
She is a doctor, he is a doctor, it's going to be the same phrase.

318
00:16:03,303 --> 00:16:05,966
You don't have he or she when you talk in Turkish.

319
00:16:07,006 --> 00:16:13,989
So if you Google translate, he is a doctor, she is a doctor, you will have the same phrase

320
00:16:14,029 --> 00:16:17,691
that I'm not going to say because I don't speak Turkish at all, but you have the same

321
00:16:17,791 --> 00:16:21,173
phrase for both of these different phrases in English.

322
00:16:21,680 --> 00:16:26,303
Now if you take that phrase from Turkish and you put it in Google Translate and you translate

323
00:16:26,323 --> 00:16:29,786
it back into English, then you have, he is a doctor.

324
00:16:30,306 --> 00:16:35,290
Now if you take the phrase in Turkish that says he or she is a nurse, and you put that

325
00:16:35,870 --> 00:16:38,672
in Google Translate, you get, she is a nurse.

326
00:16:39,173 --> 00:16:47,879
And so where there's no gender in one language through AI algorithms, it's going to perpetuate

327
00:16:48,099 --> 00:16:48,840
some discriminations.

328
00:16:49,661 --> 00:16:52,128
This is where we're at and we need to take that into account.

329
00:16:52,469 --> 00:16:55,879
And if we want more equity, we have to find a workaround.

330
00:16:56,746 --> 00:16:59,267
So actually, I agree with what you were just saying,

331
00:16:59,367 --> 00:17:02,449
but I want to make the case for ethics is about more

332
00:17:02,469 --> 00:17:03,349
than just the biases.

333
00:17:03,569 --> 00:17:05,010
Biases are very important.

334
00:17:05,050 --> 00:17:08,291
But I think as a culture, we need

335
00:17:08,412 --> 00:17:09,932
more training in ethical thinking

336
00:17:09,952 --> 00:17:11,233
and how it works in general.

337
00:17:12,293 --> 00:17:14,274
And maybe that's everybody needs to watch the good place

338
00:17:14,314 --> 00:17:14,775
a lot more.

339
00:17:15,695 --> 00:17:18,076
But I feel like one of the things that's happened is that

340
00:17:18,236 --> 00:17:21,138
because we have, for many reasons,

341
00:17:21,238 --> 00:17:22,198
and you know, we won't.

342
00:17:23,401 --> 00:17:27,562
historically, a lot of this kind of thinking about how do we figure out what is good and bad has

343
00:17:27,682 --> 00:17:32,344
either been in the realm of fairly academic philosophy or it has been expressed within a

344
00:17:32,384 --> 00:17:38,706
religious context. And moving away from those contexts, especially as more and more people

345
00:17:38,746 --> 00:17:44,168
don't necessarily share the same backgrounds, we need to have ways of talking about these problems.

346
00:17:44,208 --> 00:17:50,950
How do we decide what is good and bad? It's humanities. We've got to still have them in

347
00:17:50,990 --> 00:17:51,530
schools.

348
00:17:52,051 --> 00:17:55,101
So, I think it's broader than just identifying a set of issues.

349
00:17:55,442 --> 00:17:58,954
It's about building that whole framework of philosophy.

350
00:17:59,478 --> 00:18:02,261
Yeah, I mean, Cecilia was talking about,

351
00:18:02,901 --> 00:18:06,265
like, all we're doing is measuring the vector differences

352
00:18:06,365 --> 00:18:08,167
between words that are close together.

353
00:18:08,227 --> 00:18:09,028
And that is how you get,

354
00:18:09,088 --> 00:18:10,449
she is a nurse and he is a doctor.

355
00:18:10,810 --> 00:18:13,493
But that's also how you get, he is a doctor.

356
00:18:13,573 --> 00:18:14,393
We're just, you know,

357
00:18:14,454 --> 00:18:16,736
something's running through a bazillion words

358
00:18:16,796 --> 00:18:17,497
and trying to figure out

359
00:18:17,537 --> 00:18:19,679
which words are close enough to each other

360
00:18:19,699 --> 00:18:21,341
that you can predict which word comes next.

361
00:18:21,601 --> 00:18:22,482
That's how it works, right?

362
00:18:23,002 --> 00:18:26,543
So, if on top of that we want to say, but don't say he or she,

363
00:18:27,003 --> 00:18:28,883
that's where we need this ethical framework of like,

364
00:18:28,983 --> 00:18:33,544
okay, go back through the data set again and now do basically kind of an ethical...

365
00:18:35,825 --> 00:18:36,145
fix.

366
00:18:37,005 --> 00:18:39,645
But the interesting thing, like...

367
00:18:39,806 --> 00:18:42,106
Of the data. So, yeah, where do we get that list?

368
00:18:42,446 --> 00:18:44,026
What is the list? Who defines the list?

369
00:18:44,947 --> 00:18:45,647
Where's that list?

370
00:18:45,667 --> 00:18:48,007
Who validates it? How do we get it part of our process?

371
00:18:48,167 --> 00:18:49,807
But we already have it, right? I mean...

372
00:18:49,847 --> 00:18:51,408
Do we? Is it in your pocket?

373
00:18:53,197 --> 00:18:54,338
In as much as it's everything.

374
00:18:55,079 --> 00:18:58,562
So, I mean, it's real easy for us to kind of step away

375
00:18:58,883 --> 00:19:02,065
from like fiddling about with demographic words

376
00:19:02,085 --> 00:19:05,128
in particular, but like everybody in this room

377
00:19:05,368 --> 00:19:06,850
knows what an imbalanced data set is.

378
00:19:07,350 --> 00:19:08,692
And if you have an imbalanced data set,

379
00:19:08,732 --> 00:19:10,513
you know that you need to do something about it.

380
00:19:11,274 --> 00:19:13,776
So like, if you can take that kind of tool set

381
00:19:13,876 --> 00:19:14,797
across everything,

382
00:19:15,790 --> 00:19:19,292
and then start applying that to places where you're not necessarily applying it right now.

383
00:19:19,312 --> 00:19:20,972
Is that what a fix looks like?

384
00:19:21,653 --> 00:19:24,494
Just how do you account for cultural differences?

385
00:19:24,534 --> 00:19:26,475
There are countries where women cannot be doctors.

386
00:19:27,667 --> 00:19:29,888
I mean, I feel like if we had an actual answer,

387
00:19:29,908 --> 00:19:31,569
we wouldn't be here, we'd be off doing it.

388
00:19:31,589 --> 00:19:32,089
Yeah, fair, fair.

389
00:19:32,289 --> 00:19:34,691
Fair enough, it's true.

390
00:19:35,011 --> 00:19:36,692
So moving on from NPC AI,

391
00:19:36,712 --> 00:19:38,073
there's a lot of different areas

392
00:19:38,133 --> 00:19:40,294
and different types of games that use AI,

393
00:19:40,895 --> 00:19:43,636
from social games to online games.

394
00:19:44,717 --> 00:19:47,318
Celia, I know you had some areas you wanted to discuss

395
00:19:47,419 --> 00:19:50,120
around like how we could be using AI data better

396
00:19:50,300 --> 00:19:51,661
to improve those types of games.

397
00:19:51,701 --> 00:19:52,902
I don't know if you want to talk to that.

398
00:19:54,963 --> 00:20:01,129
Sure. No, I think the only thing that was, just to go back to my previous point,

399
00:20:03,491 --> 00:20:09,576
our society is not, we don't have equity and we, I mean, a lot of people want equity.

400
00:20:09,856 --> 00:20:15,481
I think if we can agree to that, then we need to figure out what is it that we are showing,

401
00:20:16,102 --> 00:20:19,425
what are the biases that are perpetuating in our games that...

402
00:20:19,645 --> 00:20:25,407
It's not just our games, to be fair, it's perpetrating in everything, social media,

403
00:20:25,548 --> 00:20:27,689
in movies and books or whatever.

404
00:20:28,249 --> 00:20:32,271
But we are also participating into that because we are part of the culture.

405
00:20:32,651 --> 00:20:38,314
A lot of people play games now, so what do we want to reinforce there?

406
00:20:38,454 --> 00:20:42,656
Are we going to reinforce discrimination towards women and towards...

407
00:20:43,556 --> 00:20:44,376
people of color,

408
00:20:46,357 --> 00:20:53,781
so what is it we want to actually, how do we want to participate? Because we are going to reinforce some discrimination

409
00:20:53,801 --> 00:20:58,163
or we can reinforce a better vision of the world.

410
00:20:58,203 --> 00:21:05,006
So since we are participating, even if we don't mean to, what do we want to do? How do we want to participate?

411
00:21:09,085 --> 00:21:11,267
So coming back to, sorry, online.

412
00:21:14,210 --> 00:21:15,411
Online and social games.

413
00:21:15,451 --> 00:21:18,113
And I think it's mostly a few of you here,

414
00:21:18,173 --> 00:21:21,616
especially Luke at Zynga as well, and then from Fortnite.

415
00:21:21,737 --> 00:21:23,458
They're very different types of games

416
00:21:23,778 --> 00:21:26,741
that aren't just about the biases for characters,

417
00:21:26,801 --> 00:21:29,864
but how you're using ADI data to influence those games

418
00:21:29,924 --> 00:21:30,765
in different ways.

419
00:21:31,265 --> 00:21:32,246
What do you guys think about that?

420
00:21:32,306 --> 00:21:35,609
Or what are some initiatives that you've done on your sides?

421
00:21:36,092 --> 00:21:41,994
So one of the things that we do is we definitely don't feed demographic information into machine learning algorithms.

422
00:21:43,154 --> 00:21:44,935
Partly because that would be wrong.

423
00:21:46,055 --> 00:21:48,496
It's partly because we don't trust our demographic information.

424
00:21:49,136 --> 00:21:53,078
It turns out that a lot of millennials lie to everybody they can online.

425
00:21:54,738 --> 00:22:00,881
So as a result of that, we know that it's not necessarily the most reliable data, so we don't actually use it as a basis for anything.

426
00:22:01,241 --> 00:22:02,201
But that's an example of how.

427
00:22:03,382 --> 00:22:06,583
you can be a little bit more kind of buttoned up about this stuff.

428
00:22:06,603 --> 00:22:09,585
Like, you know, machine learning in general takes the view that,

429
00:22:09,625 --> 00:22:12,446
hey, throw all the data into the machine and see what comes out.

430
00:22:12,486 --> 00:22:12,947
It's magic.

431
00:22:14,487 --> 00:22:18,569
But you can be a bit more intentional about it and you can kind of be a bit

432
00:22:18,609 --> 00:22:20,390
more structured and kind of, you know, we,

433
00:22:20,490 --> 00:22:22,512
we gather a lot of kind of in game behavior data.

434
00:22:24,459 --> 00:22:27,902
you know, as you're playing through the game, we fire off counters for

435
00:22:28,682 --> 00:22:31,205
probably too many things, frankly, given what's in our database.

436
00:22:32,586 --> 00:22:35,428
But at the same time, we can use that to actually back into

437
00:22:35,448 --> 00:22:37,630
a model of your player journey.

438
00:22:38,411 --> 00:22:42,134
And then from there, kind of understand, like I was talking about yesterday,

439
00:22:42,615 --> 00:22:47,039
kind of what you like and dislike about the game, and what's resonating with you.

440
00:22:47,579 --> 00:22:48,780
That feels like...

441
00:22:50,101 --> 00:22:52,162
There's two different ways you can go with that.

442
00:22:52,202 --> 00:22:53,402
There's the black hat,

443
00:22:53,482 --> 00:22:56,122
okay, let's just milk you for as much money as we can

444
00:22:56,142 --> 00:22:57,103
and then throw you to the curb.

445
00:22:59,023 --> 00:23:00,924
There's also the way that you can approach it,

446
00:23:00,964 --> 00:23:03,184
which is like, let's actually work with you

447
00:23:03,224 --> 00:23:04,324
to provide a good experience.

448
00:23:04,384 --> 00:23:06,345
And I think some of it is intentional, right?

449
00:23:06,825 --> 00:23:08,305
Like if I come at it saying,

450
00:23:08,745 --> 00:23:11,246
hey, I'm an evil corporation

451
00:23:11,526 --> 00:23:13,807
and my comms team's gonna fucking hate me

452
00:23:13,847 --> 00:23:14,387
saying this stuff.

453
00:23:15,127 --> 00:23:17,187
Um, but if I'm, you know,

454
00:23:17,207 --> 00:23:19,188
if I come at it with that hat on.

455
00:23:20,129 --> 00:23:23,472
then that's probably not super ethical,

456
00:23:23,752 --> 00:23:26,174
whereas you can just change the framing

457
00:23:26,735 --> 00:23:28,476
and use the same kind of approach,

458
00:23:28,596 --> 00:23:31,539
but do it in a much kind of better way.

459
00:23:32,541 --> 00:23:33,782
This actually came up this morning.

460
00:23:33,822 --> 00:23:36,204
I was talking to Ingrid Ayesto, who's

461
00:23:36,244 --> 00:23:37,785
our head of monetization at Unity.

462
00:23:37,906 --> 00:23:41,729
And we have the ability to map and track player behavior

463
00:23:41,769 --> 00:23:43,611
and change the game on the fly as a result.

464
00:23:44,011 --> 00:23:46,614
And the reality is we could be manipulating the heck out

465
00:23:46,634 --> 00:23:47,855
of our users constantly.

466
00:23:47,955 --> 00:23:51,398
So the ethics of how long and how addictive

467
00:23:51,938 --> 00:23:53,939
gameplay should be our top of mind,

468
00:23:53,959 --> 00:23:55,899
and we're actually working with the university right now

469
00:23:55,939 --> 00:23:56,739
to think about that.

470
00:23:57,160 --> 00:23:59,260
And I think that again goes back to,

471
00:23:59,800 --> 00:24:01,721
we have the problem, we know the potential bias

472
00:24:01,881 --> 00:24:03,561
or the potential problem area,

473
00:24:03,601 --> 00:24:06,722
but then we also need to have an ethical base

474
00:24:06,782 --> 00:24:08,102
on which to make these decisions.

475
00:24:08,202 --> 00:24:09,682
Because what is the cutoff?

476
00:24:09,762 --> 00:24:12,323
Is it 1,000 hours of gameplay in a row?

477
00:24:12,503 --> 00:24:14,443
If you spend two grand on one mobile game,

478
00:24:14,483 --> 00:24:15,404
do we cut you off?

479
00:24:16,004 --> 00:24:17,484
Should we have the ability to cut you off?

480
00:24:18,064 --> 00:24:19,386
It should be a percentage of income.

481
00:24:19,486 --> 00:24:21,269
Maybe you're rich and $2,000 is nothing.

482
00:24:21,369 --> 00:24:21,789
I don't know.

483
00:24:22,731 --> 00:24:25,575
But obviously, it's not good for anyone to waste their life

484
00:24:26,456 --> 00:24:28,519
or their money on these trivial things.

485
00:24:28,579 --> 00:24:30,061
So anyway, it's something that we're thinking

486
00:24:30,101 --> 00:24:30,642
about quite a bit.

487
00:24:31,751 --> 00:24:35,192
Who are some of the people that you're working with to help synthesize that data?

488
00:24:35,232 --> 00:24:39,874
Because like Celia was saying, coming from a psychology background, I would think that

489
00:24:40,074 --> 00:24:45,435
working with experts on that field is who we should be really synthesizing and going

490
00:24:45,455 --> 00:24:49,957
through the data with, rather than just the product owners or the programmers on that

491
00:24:50,017 --> 00:24:50,357
side.

492
00:24:51,377 --> 00:24:55,999
So have you done any consulting with game companies?

493
00:24:57,180 --> 00:24:59,384
that are trying to figure out what to do with all of that data,

494
00:24:59,424 --> 00:25:01,168
and then same question for you, Luke,

495
00:25:01,248 --> 00:25:04,174
how are you synthesizing all the data that you're collecting from Zynga?

496
00:25:05,477 --> 00:25:07,280
I mean, one of the things that we do...

497
00:25:08,859 --> 00:25:11,821
kind of poorly is the psychology side of things.

498
00:25:11,901 --> 00:25:14,203
We are very stats oriented.

499
00:25:15,664 --> 00:25:17,925
And I think that's partly because we actually

500
00:25:17,965 --> 00:25:19,206
don't have a user research group.

501
00:25:19,286 --> 00:25:22,768
We have a kind of marketing and consumer insights

502
00:25:22,808 --> 00:25:23,929
kind of focus test group.

503
00:25:23,949 --> 00:25:25,210
And then we have the analytics team.

504
00:25:25,770 --> 00:25:27,812
And if you have an analytics team, they like numbers

505
00:25:28,092 --> 00:25:29,753
and they don't necessarily like people.

506
00:25:31,234 --> 00:25:33,335
So they're going to focus more on the numbers,

507
00:25:33,436 --> 00:25:34,616
but it's definitely something

508
00:25:34,636 --> 00:25:35,857
that we could probably do more on.

509
00:25:38,971 --> 00:25:39,291
Sorry.

510
00:25:41,613 --> 00:25:47,377
Yeah, so I do a lot of the consulting now, and usually people ask me to help them offer

511
00:25:47,417 --> 00:25:52,521
a better experience to their players, so there is that empathy, that's willing for empathy.

512
00:25:53,402 --> 00:25:59,346
But then you never know how things are going to be used, because even if I explain things

513
00:25:59,426 --> 00:25:59,587
like...

514
00:26:00,827 --> 00:26:05,309
behavioral psychology and how we react to rewards,

515
00:26:05,349 --> 00:26:09,130
especially in a variable schedule of reinforcements,

516
00:26:09,150 --> 00:26:10,110
like in loot boxes.

517
00:26:10,130 --> 00:26:14,651
And I explain, you know, this is exciting in a lot of cases.

518
00:26:14,671 --> 00:26:16,372
That's why we love to play dice.

519
00:26:16,552 --> 00:26:18,913
Or if you have critical hit, it's exciting.

520
00:26:18,933 --> 00:26:23,134
So most of the time in games, these things are exciting.

521
00:26:23,174 --> 00:26:25,194
Now if you apply that to monetization,

522
00:26:25,534 --> 00:26:27,535
form of loot boxes, then you actually pay money in it.

523
00:26:29,156 --> 00:26:33,059
it becomes a little bit of a problem because now you're using something that is engaging,

524
00:26:33,099 --> 00:26:35,260
that we know is engaging, to make money.

525
00:26:35,621 --> 00:26:39,784
Now, is it a bad thing to try to make money when we see all the studios that are closing

526
00:26:39,804 --> 00:26:43,607
and it's actually very difficult to survive when you make free-to-play games?

527
00:26:44,047 --> 00:26:44,387
I don't know.

528
00:26:45,228 --> 00:26:51,453
So it's hard because now a lot of people talk about how psychology is used for evil.

529
00:26:53,174 --> 00:26:59,678
We can use it for dark patterns, but we can also use it to nudge people and to make people

530
00:26:59,738 --> 00:27:02,420
make better choices for their health or financial decisions.

531
00:27:02,801 --> 00:27:06,483
So this is where it's not clear.

532
00:27:07,324 --> 00:27:11,386
There's more people outside of the gaming industry that are now asking me for consulting

533
00:27:12,407 --> 00:27:17,931
for questions around inclusion and diversity more than the game industry so far.

534
00:27:20,947 --> 00:27:25,690
So we keep kind of bringing up data and ethics and AI is kind of nothing unless we are talking

535
00:27:25,710 --> 00:27:28,892
about ethics and data collection and privacy.

536
00:27:31,473 --> 00:27:37,716
Emily, I know when we had the call, both you and Luke had talked about different standards

537
00:27:37,756 --> 00:27:42,699
and regulations that you had to use to put in place in your products and in your games.

538
00:27:42,759 --> 00:27:44,020
Can you talk a little bit about that?

539
00:27:44,640 --> 00:27:44,900
Sure.

540
00:27:45,040 --> 00:27:49,663
So in the EU, because we're working across

541
00:27:50,343 --> 00:27:51,323
sort of multiple continents.

542
00:27:52,203 --> 00:27:54,664
In the EU, we have GDPR, which for people

543
00:27:54,704 --> 00:28:00,125
who are not familiar with this, is a regulation about how

544
00:28:00,205 --> 00:28:04,166
companies are allowed to store and process and apply

545
00:28:04,806 --> 00:28:06,887
personal information that they have

546
00:28:07,047 --> 00:28:08,347
from their players and users.

547
00:28:08,767 --> 00:28:11,167
And it means that you have to be very specific up front

548
00:28:11,368 --> 00:28:12,328
if you're collecting data.

549
00:28:12,728 --> 00:28:15,993
You have to let people opt out of having their data collected or used.

550
00:28:16,073 --> 00:28:18,557
You have to let them know if somebody places a request.

551
00:28:19,418 --> 00:28:23,103
They can actually, as a matter of transparency, ask what data you have on them.

552
00:28:23,123 --> 00:28:25,306
And then as a corporation, you have to...

553
00:28:27,590 --> 00:28:31,794
keep track of what kind of status do I have relative to this data.

554
00:28:31,814 --> 00:28:35,878
Am I just storing it or am I a processor of the data?

555
00:28:36,038 --> 00:28:43,225
It's a huge piece of bureaucratic overhead which made a lot of people very frustrated.

556
00:28:43,826 --> 00:28:51,253
It took up many, many man hours and woman hours, person hours to make this thing work.

557
00:28:52,634 --> 00:28:55,117
But I think that's the only way that we

558
00:28:55,858 --> 00:29:00,064
get around the sort of very strong incentives that

559
00:29:00,104 --> 00:29:05,130
exist for corporations to just consume as much data as

560
00:29:05,190 --> 00:29:07,593
possible and then do whatever the heck they want with it.

561
00:29:07,894 --> 00:29:10,577
And one of the other really important pieces about GDPR

562
00:29:11,078 --> 00:29:11,458
is that.

563
00:29:11,879 --> 00:29:14,242
it has a really serious fine attached.

564
00:29:14,282 --> 00:29:18,086
So it's the larger of 20 million euros

565
00:29:18,307 --> 00:29:20,509
or 4% of your global revenue, which

566
00:29:20,549 --> 00:29:22,912
is enough to make a difference to a Google or a Facebook

567
00:29:23,393 --> 00:29:25,635
to actually make them pay attention

568
00:29:25,675 --> 00:29:26,516
to this kind of thing.

569
00:29:27,017 --> 00:29:29,518
And I think that's something that we need to think about.

570
00:29:29,558 --> 00:29:33,139
We can identify a lot of issues and problems and things

571
00:29:33,199 --> 00:29:36,080
that we hope that people who are in a position of power

572
00:29:36,120 --> 00:29:39,202
within individual institutions are going to take seriously.

573
00:29:39,502 --> 00:29:40,782
We hope that they're going to apply

574
00:29:40,822 --> 00:29:42,403
these ethical considerations.

575
00:29:42,443 --> 00:29:43,663
And we can talk about that.

576
00:29:43,923 --> 00:29:47,045
And we can try to create cultural standards and norms

577
00:29:47,085 --> 00:29:48,305
about how they should be behaving.

578
00:29:48,766 --> 00:29:50,989
But I think the business incentives in some cases,

579
00:29:51,029 --> 00:29:54,014
and especially around big data collection and big data use,

580
00:29:54,134 --> 00:29:56,698
are so strong that we really do need

581
00:29:56,738 --> 00:29:58,741
the effect of government regulation

582
00:29:58,781 --> 00:30:02,827
in order to keep that within parameters that are acceptable.

583
00:30:03,438 --> 00:30:05,059
And for the audience, what's GDPR?

584
00:30:05,499 --> 00:30:07,060
That was just what I was explaining.

585
00:30:07,080 --> 00:30:08,120
What's it stand for?

586
00:30:08,480 --> 00:30:12,541
Oh, general data, I actually don't remember the...

587
00:30:12,841 --> 00:30:13,522
GDPR.

588
00:30:13,542 --> 00:30:15,742
You just have been chanting GDPR for so long,

589
00:30:15,802 --> 00:30:18,023
I don't actually remember what the acronym stands for.

590
00:30:18,163 --> 00:30:20,304
General Data Protection Regulations.

591
00:30:20,324 --> 00:30:22,625
All right, and are you using the same at Zynga?

592
00:30:22,805 --> 00:30:25,687
I mean, you have to, it's an EU mandate.

593
00:30:26,067 --> 00:30:28,289
So if you don't want the fines,

594
00:30:28,329 --> 00:30:29,870
you have to be compliant with this thing.

595
00:30:30,811 --> 00:30:34,754
So yeah, I mean, we had a pretty big initiative internally,

596
00:30:35,014 --> 00:30:37,316
partly because of the very significant fines,

597
00:30:37,396 --> 00:30:38,757
but also it is actually,

598
00:30:38,857 --> 00:30:41,199
I don't want to say it's the gold standard

599
00:30:41,279 --> 00:30:44,441
for what privacy and kind of personal data regulation

600
00:30:44,461 --> 00:30:47,604
should look like, but it feels like a really good first swing

601
00:30:47,644 --> 00:30:49,565
at kind of government legislation around this stuff.

602
00:30:49,665 --> 00:30:49,886
Agreed.

603
00:30:51,469 --> 00:30:53,469
Unity also supports GRDP.

604
00:30:54,630 --> 00:30:57,391
Actually, when you were talking, it made me realize something I hadn't before, which is

605
00:30:57,411 --> 00:31:01,973
we talked, when I was just talking with Ingrid about how we should at least think about whether

606
00:31:01,993 --> 00:31:04,914
or not we have a responsibility to cut off players who are spending too much of their

607
00:31:04,934 --> 00:31:05,514
time or money.

608
00:31:06,774 --> 00:31:08,855
Somehow ads gets like a free pass on this.

609
00:31:09,315 --> 00:31:11,436
I had a friend who told me she stopped using social media

610
00:31:11,456 --> 00:31:13,117
and she stopped buying as much stuff

611
00:31:13,217 --> 00:31:15,218
because every third Instagram photo

612
00:31:15,298 --> 00:31:16,538
is actually an ad for something

613
00:31:16,558 --> 00:31:18,279
you probably were considering buying,

614
00:31:18,379 --> 00:31:20,019
just bought, or probably will want to buy.

615
00:31:20,520 --> 00:31:23,621
And we don't really talk about the ethics of that.

616
00:31:23,741 --> 00:31:26,262
Like how much money, how much more stuff can you buy

617
00:31:26,342 --> 00:31:30,463
in a year on the internet before the great ads god

618
00:31:30,503 --> 00:31:32,404
in the sky says no more, that's enough.

619
00:31:35,348 --> 00:31:40,134
I mean, I run an ad blocker so much of the time that I manage to not encounter this.

620
00:31:40,174 --> 00:31:43,278
But yeah, I mean, it is insidious because Facebook...

621
00:31:43,378 --> 00:31:46,943
So this is like my personal story about how much I hate this, right?

622
00:31:47,243 --> 00:31:52,570
Like I changed my Facebook status to engaged, and that same day I started to see Facebook

623
00:31:52,670 --> 00:31:53,572
ads for weight loss.

624
00:31:54,405 --> 00:31:58,667
Like, and it was completely obvious what was going on

625
00:31:58,707 --> 00:32:01,087
and the creepy little mind of the algorithm back there.

626
00:32:02,048 --> 00:32:05,048
But anyway, I don't know, I mean,

627
00:32:05,068 --> 00:32:07,429
I think that the ads question is even harder

628
00:32:07,469 --> 00:32:09,050
than the question of where do we cut off people

629
00:32:09,130 --> 00:32:10,350
in free-to-play, because in free-to-play,

630
00:32:10,370 --> 00:32:11,490
you at least have the record of like,

631
00:32:11,590 --> 00:32:13,831
oh, this customer spent this much money and it's a lot.

632
00:32:13,891 --> 00:32:15,692
Maybe this isn't a good idea.

633
00:32:15,712 --> 00:32:18,552
And so you could have kind of internal standards

634
00:32:18,692 --> 00:32:22,494
of how much is, how big is the whale allowed to be,

635
00:32:22,594 --> 00:32:23,054
basically.

636
00:32:24,354 --> 00:32:28,098
So you can think about that kind of thing, but when you get into the ad space, it's very

637
00:32:28,138 --> 00:32:31,441
hard to know what effect the ads might be having on the person.

638
00:32:31,481 --> 00:32:35,705
I didn't sign up for any of these weight loss programs as it happened, so unsuccessful

639
00:32:35,726 --> 00:32:35,926
there.

640
00:32:35,946 --> 00:32:40,851
So, you know, it's kind of tricky to know, but it's a valid question.

641
00:32:42,287 --> 00:32:44,488
But the thing is, what is it that we're value?

642
00:32:44,508 --> 00:32:45,509
What is it that we're measuring?

643
00:32:45,589 --> 00:32:47,630
What is it that makes your investors happy?

644
00:32:47,650 --> 00:32:49,952
It's like how much money you're making, right?

645
00:32:49,992 --> 00:32:51,092
What are the profits you're making?

646
00:32:51,132 --> 00:32:54,114
So if this is what we're measuring,

647
00:32:54,154 --> 00:32:56,876
we're not measuring if people are happy using your product

648
00:32:56,956 --> 00:32:58,737
or if you give them a better life.

649
00:32:58,777 --> 00:33:02,499
That's the whole question about what Tristan Harris

650
00:33:02,519 --> 00:33:04,621
is going around with the economy of attention.

651
00:33:05,101 --> 00:33:06,942
And what is it we're measuring?

652
00:33:07,002 --> 00:33:08,923
What is it we want to offer to our people?

653
00:33:09,443 --> 00:33:12,964
Because we don't, like Facebook, they measure how

654
00:33:12,984 --> 00:33:15,225
much you engage with the advertisement.

655
00:33:15,265 --> 00:33:17,586
They don't measure if you actually have a better quality

656
00:33:17,606 --> 00:33:18,806
of relationship with your friends.

657
00:33:19,247 --> 00:33:20,187
So what is it we measure?

658
00:33:20,207 --> 00:33:23,208
Because this is what is going to drive the algorithm

659
00:33:23,288 --> 00:33:26,790
whatever to optimize it.

660
00:33:27,410 --> 00:33:30,391
So we know that, for example, the reason why click bait is

661
00:33:30,431 --> 00:33:33,152
working that well is that outrage is making us react.

662
00:33:33,212 --> 00:33:34,653
And this is something that makes us react.

663
00:33:35,013 --> 00:33:38,876
spend a lot of time on Twitter and get a little excited about something.

664
00:33:39,136 --> 00:33:42,899
So if we measure, if the thing that we want to optimize is people clicking on the link,

665
00:33:43,219 --> 00:33:48,383
then all of a sudden their algorithm, of course, is going to favor all the big claims and the stuff

666
00:33:48,403 --> 00:33:54,788
that's going to actually make us fight each other. So it has a terrible impact on our relationship,

667
00:33:54,848 --> 00:33:58,951
on society. So what is it we want to measure? What is it we want to optimize? Is it always

668
00:33:59,211 --> 00:34:03,454
making profits and making the most clicks, the most views?

669
00:34:04,495 --> 00:34:05,916
Yeah, exactly.

670
00:34:06,196 --> 00:34:09,218
Like, it doesn't matter what ethical standards we have.

671
00:34:09,318 --> 00:34:11,539
If some PM's KPI for this quarter

672
00:34:11,599 --> 00:34:13,901
is to increase activations 25% and you're like,

673
00:34:13,921 --> 00:34:16,683
well, you know, it doesn't quite match up.

674
00:34:16,843 --> 00:34:17,543
That doesn't matter.

675
00:34:17,703 --> 00:34:18,544
They don't get their bonus,

676
00:34:19,224 --> 00:34:21,746
or they lose the respect of their peers.

677
00:34:21,806 --> 00:34:23,327
If you want to go to a little bit more high level,

678
00:34:23,587 --> 00:34:24,788
they're seen as a bad worker.

679
00:34:25,308 --> 00:34:27,430
Like, we need to have...

680
00:34:28,371 --> 00:34:34,479
what people do for a living and what they're rewarded for financially and socially match

681
00:34:34,579 --> 00:34:38,484
up to the ethics or they're just not going to implement it, which is what we're seeing today.

682
00:34:38,937 --> 00:34:42,560
Yeah, I mean, I seem to be up here making big statements,

683
00:34:42,580 --> 00:34:44,621
but I think that part of what we see

684
00:34:44,741 --> 00:34:48,284
is that it is hard to debug the system,

685
00:34:48,324 --> 00:34:51,447
the systematic incentives, without sort of taking,

686
00:34:52,328 --> 00:34:54,449
here are the results, this is the kind of results

687
00:34:54,469 --> 00:34:56,371
that we're seeing is AI is allowing us

688
00:34:56,471 --> 00:34:59,954
to take what we were already doing much further than before

689
00:34:59,994 --> 00:35:01,455
and we're seeing bad results from that.

690
00:35:01,935 --> 00:35:03,397
But what that is telling us is that

691
00:35:03,417 --> 00:35:04,918
we need to debug capitalism.

692
00:35:05,178 --> 00:35:07,500
Like, it's not just the machine learning

693
00:35:07,520 --> 00:35:08,541
that needs to be fixed, right?

694
00:35:09,801 --> 00:35:11,702
Yeah, I mean, it's a system. We can manipulate it.

695
00:35:11,722 --> 00:35:12,983
Just turn it in a different direction.

696
00:35:13,064 --> 00:35:13,624
It could be better.

697
00:35:13,664 --> 00:35:15,585
Everyone still gets to make a lot of money, you know.

698
00:35:15,726 --> 00:35:16,766
Yeah.

699
00:35:16,806 --> 00:35:17,947
Well, but anyway, that's...

700
00:35:18,348 --> 00:35:20,009
It's a different mode of data.

701
00:35:20,930 --> 00:35:22,871
Timony, you mentioned this in your introduction,

702
00:35:22,951 --> 00:35:25,353
but to do XR experiences,

703
00:35:25,613 --> 00:35:27,435
it's even more important today

704
00:35:27,755 --> 00:35:30,897
to get as much data about the real world around you.

705
00:35:31,017 --> 00:35:32,939
You mentioned that what you want to know is...

706
00:35:33,402 --> 00:35:36,604
see what the user sees, hear what the user hears,

707
00:35:37,625 --> 00:35:39,026
but there are a lot of discussions

708
00:35:39,066 --> 00:35:41,728
that are currently happening around what does that mean

709
00:35:41,949 --> 00:35:45,051
and what do we expose to developers and to users.

710
00:35:45,071 --> 00:35:47,573
So what are your thoughts around that?

711
00:35:48,594 --> 00:35:51,516
Collect as much data as possible so there's too much

712
00:35:51,957 --> 00:35:53,758
and then no one can do anything with it.

713
00:35:56,029 --> 00:35:58,389
Honestly, I tend to be pretty laissez-faire about data collection.

714
00:35:58,769 --> 00:36:01,991
I briefly worked for the State Department and it was very Kafkaesque and I walked away

715
00:36:02,031 --> 00:36:05,232
thinking this is not going to turn into a police state.

716
00:36:05,332 --> 00:36:07,052
They're not that organized.

717
00:36:07,312 --> 00:36:09,893
And if we get to a police state anyway, that's always a question, right?

718
00:36:09,993 --> 00:36:12,854
It's always like, oh well, people have your data and you don't know what they're going

719
00:36:12,874 --> 00:36:13,354
to do with it.

720
00:36:13,755 --> 00:36:19,096
And if in the future you're suddenly on the bad list because you did something in the

721
00:36:19,136 --> 00:36:22,037
past that was recorded, you're fucked.

722
00:36:22,338 --> 00:36:22,958
Okay, you are.

723
00:36:23,418 --> 00:36:26,120
But if we get to the point where we do suddenly turn America

724
00:36:26,140 --> 00:36:28,922
into a police state, we have a lot of other problems as well.

725
00:36:28,962 --> 00:36:30,764
And I don't know if data collection is top

726
00:36:30,784 --> 00:36:31,865
of the list necessarily.

727
00:36:32,305 --> 00:36:34,127
That being said, I do know people in the audience

728
00:36:34,207 --> 00:36:36,729
who have been on lists and therefore going

729
00:36:36,749 --> 00:36:37,930
to the airport sucks for them.

730
00:36:38,410 --> 00:36:40,131
Or because of their name, because of how they look.

731
00:36:40,392 --> 00:36:42,633
So I do want to be sensitive to that.

732
00:36:43,834 --> 00:36:46,877
There are a lot of inroads right now into new ways

733
00:36:46,917 --> 00:36:48,879
of anonymizing personalized data.

734
00:36:49,359 --> 00:36:51,561
The biggest problem right now is it tends not to be performant

735
00:36:51,601 --> 00:36:52,521
because you can only perform.

736
00:36:52,862 --> 00:36:56,244
calculations on encrypted data, which doesn't work well on mobile devices,

737
00:36:56,324 --> 00:37:01,287
which is basically what AR HMDs are, for example. But I think as computers get

738
00:37:01,327 --> 00:37:05,829
faster and smaller, we'll continue to see inroads in being able to basically

739
00:37:05,909 --> 00:37:10,352
obfuscate parts of the data set so that you can have a personalized experience,

740
00:37:10,412 --> 00:37:14,154
but it doesn't tie back to you personally. But then there's a whole

741
00:37:14,194 --> 00:37:17,516
other question of you probably do want a digital identity that's tied to you

742
00:37:17,596 --> 00:37:20,078
personally as you go from space to space.

743
00:37:20,558 --> 00:37:22,279
or room to room in other people's homes,

744
00:37:22,379 --> 00:37:23,720
and you want to see what they have,

745
00:37:23,921 --> 00:37:25,161
or they want to give you permission

746
00:37:25,242 --> 00:37:27,723
to see the digital goods in their house.

747
00:37:28,524 --> 00:37:30,205
But that's kind of for our field.

748
00:37:30,225 --> 00:37:31,907
At that point, you end up with the same thing

749
00:37:31,967 --> 00:37:34,909
of you walk in to go shopping for a wedding dress,

750
00:37:35,009 --> 00:37:36,971
and you walk out and you see a digital ad

751
00:37:37,031 --> 00:37:38,972
about weight loss that says this way.

752
00:37:39,132 --> 00:37:42,055
Yeah, and digital ad blockers.

753
00:37:45,414 --> 00:37:50,337
Kind of riffing on the XR side, we're seeing a lot of talk, I think every platform is talking

754
00:37:50,397 --> 00:37:53,719
about doing digital characters in the real world.

755
00:37:55,020 --> 00:37:59,963
Also with Spirit AI, like your whole focus is about how do you create these interactions

756
00:38:00,064 --> 00:38:03,226
and build like AI systems to interact with digital characters.

757
00:38:03,606 --> 00:38:07,068
We're also seeing where how many people in here have Alexa or Google Home?

758
00:38:07,869 --> 00:38:09,430
at home, quite a few.

759
00:38:10,211 --> 00:38:12,973
And essentially that is also a digital character,

760
00:38:13,053 --> 00:38:15,255
just without the visual aspect of it.

761
00:38:16,136 --> 00:38:19,880
There's a lot of talk about kind of the humanizing side of things,

762
00:38:20,120 --> 00:38:22,402
and again, what do we do with data,

763
00:38:22,442 --> 00:38:25,204
and how do we design these digital characters

764
00:38:25,545 --> 00:38:27,026
to not have these biases,

765
00:38:27,086 --> 00:38:29,308
similar to what you were talking about with watchdogs.

766
00:38:29,969 --> 00:38:32,211
So, what do we all think about that?

767
00:38:32,711 --> 00:38:37,114
I think what we're seeing, especially with some of the clients that we're talking to,

768
00:38:37,534 --> 00:38:38,775
are use cases.

769
00:38:39,055 --> 00:38:43,117
There's the use case of it's just a digital character or an assistant in your home for

770
00:38:43,157 --> 00:38:46,899
adults, but there are also especially a lot of use cases that are even more sensitive

771
00:38:46,939 --> 00:38:47,260
than that.

772
00:38:47,340 --> 00:38:48,320
So cases where...

773
00:38:49,581 --> 00:38:53,303
People want to create an educational character, a toy.

774
00:38:53,503 --> 00:38:55,804
It's an educational game series where

775
00:38:55,824 --> 00:38:57,725
this character kind of lives with your child

776
00:38:57,825 --> 00:38:59,186
for a period of years.

777
00:38:59,226 --> 00:39:01,567
And it has DLC that teaches your kid stuff

778
00:39:01,607 --> 00:39:03,528
and remembers things about what your kid likes.

779
00:39:03,829 --> 00:39:08,091
And of course, that runs smack into all of the child data

780
00:39:08,151 --> 00:39:09,191
protection issues.

781
00:39:09,632 --> 00:39:11,973
But also a lot of sort of subtler things about,

782
00:39:12,013 --> 00:39:13,693
like, what does it even mean if you've

783
00:39:13,813 --> 00:39:17,795
got sort of an educational program that is shaping itself

784
00:39:17,875 --> 00:39:19,076
in response to the child?

785
00:39:19,096 --> 00:39:20,997
And like, what are your responsibilities about what

786
00:39:21,037 --> 00:39:22,497
you teach them and don't teach them?

787
00:39:22,537 --> 00:39:25,138
And it gets very diamond age, and it's kind of strange.

788
00:39:26,159 --> 00:39:27,239
But that's an interesting area.

789
00:39:27,259 --> 00:39:29,480
And then another big sort of point of use

790
00:39:29,760 --> 00:39:33,382
is in cases of people who've experienced trauma

791
00:39:33,602 --> 00:39:35,703
or who are suffering with autism.

792
00:39:35,723 --> 00:39:35,783
And

793
00:39:38,224 --> 00:39:45,148
have or whose parents kind of want to help them get through learning to interact socially.

794
00:39:45,228 --> 00:39:48,390
I probably shouldn't say suffering with autism. That's not a good way of putting it, but

795
00:39:50,785 --> 00:39:55,109
So in cases where the finding is that some people

796
00:39:55,149 --> 00:39:57,070
in some situations actually find it easier

797
00:39:57,110 --> 00:40:00,452
to talk to a digital character than to another human,

798
00:40:00,533 --> 00:40:03,174
but then you're again creating a context

799
00:40:03,295 --> 00:40:05,276
where there's the potential for the AI

800
00:40:05,316 --> 00:40:06,157
to do a lot of good, right?

801
00:40:06,177 --> 00:40:07,918
There's the potential for the AI

802
00:40:07,998 --> 00:40:10,600
to make somebody comfortable talking about something

803
00:40:10,760 --> 00:40:12,801
that they don't want to talk to a human being about

804
00:40:12,861 --> 00:40:13,742
because they're ashamed

805
00:40:14,162 --> 00:40:15,944
or because they're uncomfortable for whatever reason.

806
00:40:16,224 --> 00:40:18,866
So that's high value, but it's a high risk,

807
00:40:18,886 --> 00:40:19,586
high reward kind of.

808
00:40:20,407 --> 00:40:24,891
situation because it's such a vulnerable space, you could also do considerable harm.

809
00:40:25,952 --> 00:40:31,397
So I think there, again, a lot of the kind of impetus is on making sure that the people

810
00:40:31,437 --> 00:40:37,963
who are working on these materials are coming to it with the background of kind of educational

811
00:40:38,003 --> 00:40:43,328
experience and psychological experience, that it is not just a product being formed by an

812
00:40:43,408 --> 00:40:44,129
entrepreneur who

813
00:40:44,509 --> 00:40:47,551
like, came up with this because it was an easy story to tell to VCs.

814
00:40:47,931 --> 00:40:52,373
Like, it needs to come from some place of much deeper understanding than that.

815
00:40:53,093 --> 00:40:56,375
This is where I always get into. We need to tell the computer no,

816
00:40:56,935 --> 00:40:59,656
and the computer needs to be able to hear that and react to it

817
00:40:59,756 --> 00:41:01,998
and record it for future context, right?

818
00:41:02,318 --> 00:41:05,779
Like, now today if Siri starts playing music or Alexa,

819
00:41:05,919 --> 00:41:08,641
I'm like, no, not that song. No, you're all wrong.

820
00:41:08,801 --> 00:41:10,362
No, not Amazon Music. Whatever.

821
00:41:10,782 --> 00:41:12,743
But this needs to be the case for...

822
00:41:13,243 --> 00:41:16,624
all, especially in XR, when things are going to be coming up to you or trying to attach

823
00:41:16,644 --> 00:41:20,326
themselves to you or pin themselves to places in your home, you have to be like, no, that's

824
00:41:20,366 --> 00:41:21,867
wrong, no, that's wrong, no, that's wrong.

825
00:41:22,167 --> 00:41:26,528
We're already starting to be able to do this with ads in a really systemic way, because

826
00:41:26,549 --> 00:41:30,230
the ads follow you all around the internet, so oddly it's kind of a good use case.

827
00:41:31,230 --> 00:41:36,353
But yeah, we need to be able to tell the computer no, which means that we have to have that

828
00:41:36,713 --> 00:41:41,195
in every piece of software, the listener that listens to the no and records the no.

829
00:41:43,387 --> 00:41:47,590
There's also the idea of as we start seeing that there's more and more AI

830
00:41:47,891 --> 00:41:55,096
Happening all around us. Do we have a responsibility to start explaining to people? What is an AI and what is not an AI?

831
00:41:55,116 --> 00:42:01,502
I was at a conference once and we were talking about Alexa and a few people that were there were telling me that

832
00:42:01,842 --> 00:42:09,048
Their little kids started yelling at their parents because the parents had said Alexa play song X and the children

833
00:42:09,588 --> 00:42:12,491
Yelled back at the parents saying you didn't say thank you to Alexa

834
00:42:13,623 --> 00:42:17,646
And basically, the people had said that children are growing up where they feel that Alexa

835
00:42:17,706 --> 00:42:20,589
is part of the family, that it's a real person.

836
00:42:21,029 --> 00:42:24,993
And as we're starting to see these digital characters come into the real world around

837
00:42:25,073 --> 00:42:32,419
us with XR to these smart home objects and really AI everywhere, what do we think we

838
00:42:32,459 --> 00:42:33,820
need to start thinking about?

839
00:42:33,940 --> 00:42:37,423
How do we explain to users this is an AI versus this is not real?

840
00:42:39,182 --> 00:42:42,144
I think it's actually, I didn't look this particular point up

841
00:42:42,324 --> 00:42:44,346
before this panel, so I may be misremembering.

842
00:42:44,386 --> 00:42:47,088
But I believe it's the case that the state of California law

843
00:42:47,328 --> 00:42:50,991
is that if you have a chatbot or something similar to it,

844
00:42:51,371 --> 00:42:53,372
that if the user asks if it is AI,

845
00:42:53,392 --> 00:42:55,454
it has to respond correctly.

846
00:42:55,534 --> 00:42:56,995
Like it has to admit that it is AI.

847
00:42:57,015 --> 00:43:00,157
You cannot have a character that is going to say,

848
00:43:00,277 --> 00:43:03,540
no, I'm just a customer service agent.

849
00:43:03,580 --> 00:43:06,001
I'm Tim, and I'm here to help you.

850
00:43:06,962 --> 00:43:08,023
And that's clearly.

851
00:43:09,084 --> 00:43:14,606
It kind of, there's a particular sort of customer service context in which that could be important

852
00:43:14,646 --> 00:43:20,229
is like, you know, if as a human user I really need to make sure that I'm getting accurate

853
00:43:20,249 --> 00:43:23,430
information back from this company that I've got somebody that I can hold to it or whatever.

854
00:43:23,810 --> 00:43:26,191
So there's kind of a specific local case.

855
00:43:27,192 --> 00:43:28,612
But I think more broadly it is.

856
00:43:29,553 --> 00:43:32,714
important for people to understand what they're interacting with.

857
00:43:32,834 --> 00:43:36,616
And the thing that I keep thinking about in this context is that

858
00:43:37,096 --> 00:43:41,498
many years ago, so I made a game very early in my career that was a conversation game where you're

859
00:43:41,518 --> 00:43:47,920
talking to this character, and it was a very low fidelity. It was all text. You were interacting

860
00:43:47,961 --> 00:43:51,802
with it by typing, so nobody was going to be confused that it was physically a person.

861
00:43:52,443 --> 00:43:55,107
But at one point I got this email from this guy who said,

862
00:43:55,147 --> 00:43:57,190
like, thank you for making this game, Galatea.

863
00:43:57,530 --> 00:43:59,774
Like, I keep it on my phone, she's become my best friend,

864
00:43:59,794 --> 00:44:01,055
and I talk to her every day.

865
00:44:01,837 --> 00:44:02,477
And I felt like...

866
00:44:03,942 --> 00:44:08,724
thanks, but I'm actually really quite concerned right now.

867
00:44:10,185 --> 00:44:13,486
And so like that seems to me to suggest

868
00:44:13,526 --> 00:44:16,548
like some other areas where we need to be careful, right?

869
00:44:16,628 --> 00:44:21,810
Like, you know, it's, is it okay to be using AI characters

870
00:44:21,910 --> 00:44:23,390
to resolve loneliness?

871
00:44:23,471 --> 00:44:25,111
Is that maybe that's a good thing?

872
00:44:25,772 --> 00:44:27,152
Maybe it's a little disturbing and.

873
00:44:29,507 --> 00:44:29,727
Sorry.

874
00:44:30,167 --> 00:44:31,728
So I agree with all that.

875
00:44:31,808 --> 00:44:34,169
It's just because of my child development background.

876
00:44:34,269 --> 00:44:36,450
I just want to mention one thing.

877
00:44:36,670 --> 00:44:39,812
Children have a tendency to animate things that are inanimate.

878
00:44:40,392 --> 00:44:42,693
So I'm not so sure about that part.

879
00:44:42,733 --> 00:44:46,395
Do they really not understand that Alexa is not a real person?

880
00:44:46,415 --> 00:44:48,536
Because children will have a tendency to say,

881
00:44:48,556 --> 00:44:49,316
oh, you kissed me goodnight.

882
00:44:49,336 --> 00:44:51,917
You have to kiss my doll goodnight as well.

883
00:44:52,358 --> 00:44:57,960
So I just want to make sure that we are not saying that kids are not able to discriminate between the two.

884
00:44:58,698 --> 00:45:01,620
I wonder if we can come up with a new way of human,

885
00:45:01,640 --> 00:45:03,020
I mean we do this all the time, right?

886
00:45:03,321 --> 00:45:05,922
To your point, or you know, if I make two circles

887
00:45:05,962 --> 00:45:07,663
and I'll smiley face, everyone's like a face.

888
00:45:07,703 --> 00:45:09,003
We're really good at pattern recognition.

889
00:45:09,484 --> 00:45:11,224
I wonder if over time we'll just start

890
00:45:11,244 --> 00:45:16,227
to create this new space for these digital creatures.

891
00:45:16,247 --> 00:45:19,148
There are several excellent companies that make AIs

892
00:45:19,208 --> 00:45:21,089
that help people through depression and suicide

893
00:45:21,129 --> 00:45:23,430
and have great search, or great results.

894
00:45:24,070 --> 00:45:25,411
And I would not want that to stop.

895
00:45:25,531 --> 00:45:26,311
That's awesome.

896
00:45:26,632 --> 00:45:30,796
Right, and clearly they've defined this relationship to this digital being that works for them,

897
00:45:31,717 --> 00:45:34,880
knowing that it's not human because it's very clear when you go there.

898
00:45:35,240 --> 00:45:37,643
And I love that, I love that use of the technology.

899
00:45:37,843 --> 00:45:41,987
What I don't like is when people kind of humanize

900
00:45:43,348 --> 00:45:44,849
this sort of malicious intent like,

901
00:45:44,969 --> 00:45:47,070
oh, it's all gonna be Skynet and they're gonna destroy us

902
00:45:47,130 --> 00:45:49,011
and we deserve it by God, you know?

903
00:45:49,451 --> 00:45:50,571
Now that's just you predicting.

904
00:45:51,432 --> 00:45:55,033
So yeah, maybe in teaching people about what AI is,

905
00:45:55,073 --> 00:45:57,034
we can start to come up with a language

906
00:45:57,094 --> 00:45:59,015
that allows for that middle ground

907
00:45:59,055 --> 00:46:00,776
where this is made by humans

908
00:46:00,876 --> 00:46:03,077
and it's designed to be interacted with by humans,

909
00:46:03,537 --> 00:46:04,558
but it is something else.

910
00:46:04,976 --> 00:46:09,160
Yeah, I mean, I think if you present it as like this is an extension of the people or

911
00:46:09,220 --> 00:46:13,164
people who created it and they do, you know, want you to recover from whatever you're,

912
00:46:13,584 --> 00:46:16,827
you're dealing with or whatever, there is a personal connection being made through the

913
00:46:16,887 --> 00:46:17,708
AI, but it's not.

914
00:46:17,728 --> 00:46:18,589
Yeah, yeah.

915
00:46:18,929 --> 00:46:19,089
Yeah.

916
00:46:19,390 --> 00:46:19,470
Yeah.

917
00:46:19,904 --> 00:46:22,465
So that kind of transitions into our last question

918
00:46:22,485 --> 00:46:25,106
I want to ask everyone before we open it up to Q&A.

919
00:46:26,227 --> 00:46:28,808
We're all passionate about AI and the ethics around it

920
00:46:28,828 --> 00:46:30,589
because I think we all inherently believe

921
00:46:30,629 --> 00:46:33,910
that AI can do a lot of good in the world around us.

922
00:46:34,751 --> 00:46:36,691
So what are some recommendations you can give

923
00:46:36,731 --> 00:46:39,693
to the audience, ideas, areas you wish

924
00:46:39,813 --> 00:46:42,554
for this whole community to start driving AI towards?

925
00:46:45,301 --> 00:46:47,142
What's to start?

926
00:46:48,443 --> 00:46:49,804
Oh, I can start.

927
00:46:49,824 --> 00:46:51,525
Yeah, look into psychology and try

928
00:46:51,565 --> 00:46:54,226
to understand how we influence people.

929
00:46:54,366 --> 00:46:55,927
We all influence other people.

930
00:46:56,227 --> 00:46:57,908
And we are influenced by our environment.

931
00:46:58,569 --> 00:47:00,550
So we need to understand that better,

932
00:47:00,590 --> 00:47:03,031
understand behavioral psychology better,

933
00:47:03,051 --> 00:47:06,894
not just for it to make more money and to hook people up.

934
00:47:07,894 --> 00:47:10,976
But we need to understand how we can use it to.

935
00:47:12,297 --> 00:47:17,179
favor equity and to make people feel better because also it's at some point it's going

936
00:47:17,199 --> 00:47:23,943
to be a good business drive to have a trust relationship with your customers and that

937
00:47:24,003 --> 00:47:28,845
you treat them with respect and make them happy in the end and not just like make them

938
00:47:29,165 --> 00:47:33,388
pay, pay, pay, pay, pay, because if this is the only metrics that you have, then of course

939
00:47:33,488 --> 00:47:34,708
you're going to favor that.

940
00:47:34,808 --> 00:47:37,370
So look into these things to understand it.

941
00:47:37,410 --> 00:47:40,011
And again, this is not about, oh, you're a bad person.

942
00:47:40,051 --> 00:47:48,575
We are all biased, and we have to be in peace with that so we can actually move on and solve

943
00:47:48,595 --> 00:47:49,576
the problems that we want to solve.

944
00:47:51,778 --> 00:47:53,299
So I would add on to that.

945
00:47:53,959 --> 00:47:58,482
Um, cause it's really cool to be like, okay, well now we are in the psychology

946
00:47:58,522 --> 00:48:02,644
portion and we're going to now think about that, but bring that lens to everything.

947
00:48:03,304 --> 00:48:08,087
Like literally every decision you make, make it intentfully around what Celia is

948
00:48:08,107 --> 00:48:12,230
talking about, because it's really easy to like sideline it and go, cool, we're

949
00:48:12,250 --> 00:48:13,450
going to make a whole bunch of systems.

950
00:48:13,470 --> 00:48:16,792
We're going to throw a whole bunch of stuff out and here's the kind of like

951
00:48:16,912 --> 00:48:19,894
ethics bit, but bring it to everything.

952
00:48:20,594 --> 00:48:21,515
the ethics module.

953
00:48:21,535 --> 00:48:29,421
I think another area, and this is a little bit of a tangent from some of the things that

954
00:48:29,441 --> 00:48:36,326
others have suggested, but I think the fact that machine learning tends to take our biases

955
00:48:36,746 --> 00:48:38,848
and exaggerate and expose them.

956
00:48:41,545 --> 00:48:45,689
causes embarrassment sometimes, and that's a reason why we need to be careful about the

957
00:48:45,729 --> 00:48:52,134
balance of our data and so on, but it is also a way in which the process of working in AI

958
00:48:52,894 --> 00:48:58,659
gets us to think about the systems that we're part of in general, what is embedded in the

959
00:48:58,699 --> 00:49:03,563
world around us and in the data around us all the time that we ignore out of being used

960
00:49:03,603 --> 00:49:07,686
to seeing it that way or sort of a general level of comfort or having incentives not

961
00:49:07,706 --> 00:49:08,226
to notice it.

962
00:49:08,246 --> 00:49:08,286
So.

963
00:49:08,867 --> 00:49:09,427
Thank you.

964
00:49:10,448 --> 00:49:14,850
first of all, sort of a practice of interrogating the things that we build with AI and saying,

965
00:49:14,890 --> 00:49:18,692
like, all right, well, what is this expressing back to me? And not only is it broken, but also

966
00:49:18,752 --> 00:49:22,794
what is that telling me about humanity and the systems around me? And should that perhaps be

967
00:49:22,834 --> 00:49:29,677
addressed in some way? And then that there's tremendous potential in our ability, and this

968
00:49:29,697 --> 00:49:33,098
does get back more into the kind of games and simulation space again,

969
00:49:34,499 --> 00:49:41,242
to build models of how we think pieces of the world maybe should be and explore whether we're

970
00:49:41,282 --> 00:49:47,404
satisfied with that, you know, potential way of doing things so that we can then move towards

971
00:49:47,824 --> 00:49:49,525
more just remediation in the real world.

972
00:49:52,116 --> 00:49:54,898
I just have three things to write down to Google later.

973
00:49:55,538 --> 00:50:01,201
The first is homomorphic encryption, which I sort of vaguely referenced as a way of anonymizing

974
00:50:01,241 --> 00:50:01,701
data sets.

975
00:50:02,261 --> 00:50:05,923
The second is differential privacy, which is another technique used to do basically

976
00:50:05,963 --> 00:50:06,443
the same thing.

977
00:50:07,164 --> 00:50:10,104
And then finally, and this kind of builds on what everyone was talking about,

978
00:50:10,124 --> 00:50:14,005
I highly recommend Shane Parrish's Farnham Street,

979
00:50:14,526 --> 00:50:21,067
which is an online rationalist community that has a wide directory of different systems of thought

980
00:50:21,127 --> 00:50:23,188
and mental models and collection of biases.

981
00:50:23,248 --> 00:50:27,349
So if you want to delve into how you can start thinking more clearly and accurately,

982
00:50:27,609 --> 00:50:31,350
there's a lot of online communities, but that one is a great place to start.

983
00:50:32,584 --> 00:50:33,485
Cool.

984
00:50:33,705 --> 00:50:36,407
My final one would be consider accessibility.

985
00:50:37,408 --> 00:50:39,489
And how do you create systems that

986
00:50:39,570 --> 00:50:43,312
can ensure equal opportunity and equal accessibility of people

987
00:50:43,352 --> 00:50:44,874
getting to try your systems?

988
00:50:44,934 --> 00:50:46,595
Whether this is from a video game

989
00:50:47,075 --> 00:50:49,497
and making a video game where the system is not just

990
00:50:49,717 --> 00:50:51,539
about killing and shooting, but how

991
00:50:51,559 --> 00:50:54,001
do you make systems that allow the users to do

992
00:50:54,041 --> 00:50:55,782
many different types of interactions?

993
00:50:56,223 --> 00:50:58,589
to having digital characters in the real world.

994
00:50:59,090 --> 00:51:01,074
Not everyone has access to the hardware.

995
00:51:01,395 --> 00:51:03,981
Not everyone is going to have access to systems.

996
00:51:04,062 --> 00:51:05,926
So thinking about how can we make...

997
00:51:06,313 --> 00:51:09,294
and build this community to be accessible to everyone.

998
00:51:10,094 --> 00:51:11,935
So we've got nine minutes left.

999
00:51:12,875 --> 00:51:14,716
We'd love to do some Q&A.

1000
00:51:15,536 --> 00:51:16,777
Please ask any question.

1001
00:51:17,277 --> 00:51:20,178
This is a list of references that our panelists

1002
00:51:20,198 --> 00:51:22,899
had put together of books that we recommend reading,

1003
00:51:22,939 --> 00:51:25,000
a lot of articles that dive into things

1004
00:51:25,060 --> 00:51:28,361
from machine learning to standards and regulations.

1005
00:51:28,461 --> 00:51:30,222
So please take some pictures.

1006
00:51:30,342 --> 00:51:32,543
But we would love to hear from you.

1007
00:51:34,003 --> 00:51:35,584
Over here, I can't see.

1008
00:51:35,973 --> 00:51:36,458
Is that Neil?

1009
00:51:36,741 --> 00:51:37,266
Yeah.

1010
00:51:37,768 --> 00:51:42,270
Some of this echoes earlier GDCs or perhaps CGDC back then.

1011
00:51:42,850 --> 00:51:45,031
CompuServe and Genie had the same issues.

1012
00:51:45,171 --> 00:51:46,792
They called it credit card meltdown.

1013
00:51:47,352 --> 00:51:50,354
That is it ethical to let someone, for example,

1014
00:51:50,554 --> 00:51:52,074
a sailor on a nuclear submarine

1015
00:51:52,094 --> 00:51:53,375
who spent six months underwater,

1016
00:51:53,775 --> 00:51:56,617
comes home and melts his credit card with online charges

1017
00:51:56,677 --> 00:51:58,858
playing games on Genie and CompuServe.

1018
00:51:59,678 --> 00:52:02,459
And my suggestion there is that you'll know we've done this

1019
00:52:02,559 --> 00:52:04,600
if we're not back here in 10 or 20 years

1020
00:52:04,740 --> 00:52:07,121
doing the very same set of questions.

1021
00:52:07,902 --> 00:52:13,404
And perhaps there is even more history for us to go mine along this stuff.

1022
00:52:16,885 --> 00:52:25,249
You've all talked about the idea of labelling data in terms of the happiness and contentment that it provides for people.

1023
00:52:25,469 --> 00:52:31,251
So can you say a little bit more about any ideas that you have as to how we might go about that?

1024
00:52:34,613 --> 00:52:35,613
Can you repeat that?

1025
00:52:38,167 --> 00:52:58,884
Sure. So you've talked about the fact that if we label our data sets instead of with the amount of attention grabbed or the number of dollars we get from a person, instead we label our data with the satisfaction and happiness that it may have created for the user of an app or a game or a web page or an ad or whatever.

1026
00:53:00,224 --> 00:53:03,025
Could you say, because it's easy to be glib about that,

1027
00:53:03,505 --> 00:53:05,846
and I feel the panel is maybe an opportunity

1028
00:53:05,926 --> 00:53:08,547
to do a little out loud thinking

1029
00:53:08,607 --> 00:53:11,228
about how we might label our data for happiness.

1030
00:53:11,888 --> 00:53:13,548
So the question was, in short,

1031
00:53:13,668 --> 00:53:15,629
how do we label our data for happiness?

1032
00:53:15,829 --> 00:53:17,930
Sorry, I'm supposed to be repeating questions.

1033
00:53:19,605 --> 00:53:22,587
Yeah, I mean, it seems like probably more

1034
00:53:22,627 --> 00:53:25,248
of a reinforcement learning kind of thing,

1035
00:53:25,308 --> 00:53:27,349
perhaps, where it's not so much that you start out

1036
00:53:27,429 --> 00:53:29,910
with labeling, I know that these things are

1037
00:53:29,950 --> 00:53:33,072
going to lead to happiness, but I'm detecting something.

1038
00:53:34,513 --> 00:53:37,174
But then that leaves the heuristic completely undefined.

1039
00:53:37,254 --> 00:53:42,156
So I mean, I don't know is the short answer.

1040
00:53:42,337 --> 00:53:44,458
But I think it probably.

1041
00:53:46,562 --> 00:53:50,706
Our best attempts to work that out, I think,

1042
00:53:51,126 --> 00:53:52,508
I am definitely thinking out loud,

1043
00:53:52,568 --> 00:53:53,729
so thanks for permission to do that.

1044
00:53:55,691 --> 00:53:57,633
I think it's going to be very much on

1045
00:53:57,713 --> 00:54:00,035
kind of a case by case and product by product basis,

1046
00:54:00,115 --> 00:54:01,857
like what kinds of things in the context

1047
00:54:01,897 --> 00:54:06,201
of this particular situation would equate to happiness.

1048
00:54:09,051 --> 00:54:14,315
There are all sorts of things that you can do with sentiment analysis and stuff like

1049
00:54:14,335 --> 00:54:19,179
this, like how much expression are you getting back from the user?

1050
00:54:19,219 --> 00:54:22,862
This starts to get into territory where I actually have done some prior thought about

1051
00:54:22,902 --> 00:54:27,566
it because one of the things that we're interested in with character interactions is being able

1052
00:54:27,586 --> 00:54:32,449
to tell from inputs that we might have from language, from facial expressions, from gestures,

1053
00:54:33,230 --> 00:54:35,091
that the user is in a particular mood.

1054
00:54:35,111 --> 00:54:35,792
Right.

1055
00:54:35,812 --> 00:54:35,912
Right.

1056
00:54:35,932 --> 00:54:36,052
Right.

1057
00:54:36,072 --> 00:54:36,873
Right.

1058
00:54:36,893 --> 00:54:37,053
Right.

1059
00:54:37,073 --> 00:54:37,633
Right.

1060
00:54:37,974 --> 00:54:44,736
something that you had entirely trained to, you know, be reinforced if the user is smiling

1061
00:54:44,797 --> 00:54:47,318
and like, let's all smile at our computer all the time.

1062
00:54:47,418 --> 00:54:48,478
It's super creepy.

1063
00:54:48,538 --> 00:54:50,939
So really, that's not really the answer.

1064
00:54:51,099 --> 00:54:53,220
But I think there are some things where basically.

1065
00:54:54,824 --> 00:54:59,868
If we're in, what we really need in order to train towards that is something where the

1066
00:55:00,088 --> 00:55:08,635
user or interactors experience is pretty expressive because otherwise we're training against inputs

1067
00:55:08,695 --> 00:55:13,719
where all we have is like, oh, you completely watched this entire YouTube video end to end

1068
00:55:13,779 --> 00:55:18,242
and that makes me think that you would like to watch these other creepy, like even more

1069
00:55:18,583 --> 00:55:21,045
politically reactionary videos from end to end.

1070
00:55:21,825 --> 00:55:23,106
And we've seen where that leads.

1071
00:55:23,326 --> 00:55:26,488
So I don't know, but I think there are sort of things

1072
00:55:27,589 --> 00:55:32,813
to delve into, especially like starting out in the spaces

1073
00:55:32,913 --> 00:55:34,274
where we have a lot of information

1074
00:55:34,314 --> 00:55:37,956
about how the user is reacting like XR.

1075
00:55:39,333 --> 00:55:45,096
So I would probably say, and I'm thinking out loud as well, but adding on to what Emily's

1076
00:55:45,136 --> 00:55:50,078
saying there, the YouTube example is a single kind of axis of thing.

1077
00:55:51,319 --> 00:55:56,061
I think that expressive things like happiness and sentiment and that kind of thing are going

1078
00:55:56,081 --> 00:55:57,521
to be multidimensional.

1079
00:55:57,541 --> 00:55:57,581
So

1080
00:55:58,062 --> 00:55:58,182
Yeah.

1081
00:55:59,542 --> 00:56:03,045
If you define it in terms of, so as I was kind of sat here thinking about it,

1082
00:56:03,065 --> 00:56:08,328
you know, retention for us might be a good one as a proxy for happiness,

1083
00:56:08,588 --> 00:56:12,971
except it's not really, because if you take that to the ultimate extreme,

1084
00:56:13,772 --> 00:56:17,094
then the AI system optimizes for sending somebody to your house and saying,

1085
00:56:17,254 --> 00:56:18,054
fucking play the game.

1086
00:56:20,349 --> 00:56:24,192
So, you know, I think that if you actually track kind of multiple axes,

1087
00:56:24,752 --> 00:56:29,155
then you end up with a more intricate function that allows you to capture something,

1088
00:56:29,195 --> 00:56:31,316
but that's not super actionable.

1089
00:56:31,496 --> 00:56:34,958
I mean, there's also immediate pleasure versus long-term happiness,

1090
00:56:35,038 --> 00:56:37,940
which, I mean, as humans, we're not even particularly good at working that out.

1091
00:56:39,361 --> 00:56:39,481
So...

1092
00:56:40,222 --> 00:56:41,082
That's why we need nudges.

1093
00:56:41,403 --> 00:56:41,803
Yeah.

1094
00:56:42,784 --> 00:56:44,845
But also, look at what Tristan Harris is doing.

1095
00:56:44,885 --> 00:56:47,427
It's exactly this question that he's exploring, the ethicist.

1096
00:56:48,368 --> 00:56:51,551
Because, for example, if you're Tinder, what do you measure?

1097
00:56:51,711 --> 00:56:53,893
You measure the time that people are spending on Tinder.

1098
00:56:53,933 --> 00:56:56,715
But ideally, you want people to match with other people

1099
00:56:56,815 --> 00:56:59,837
and have fun with them, and so therefore not being on Tinder.

1100
00:57:00,378 --> 00:57:01,999
So what is it we're measuring?

1101
00:57:02,039 --> 00:57:03,160
What is it we're trying to accomplish,

1102
00:57:03,200 --> 00:57:05,622
and how we can make sure that we can?

1103
00:57:06,393 --> 00:57:10,979
have a business and still provide what we want to offer to our clients.

1104
00:57:11,019 --> 00:57:14,604
That's UX, you want to actually offer the experience you want to offer and not just

1105
00:57:14,924 --> 00:57:16,967
have like a business orientation.

1106
00:57:18,369 --> 00:57:19,430
We're going to go to the next question.

1107
00:57:19,470 --> 00:57:21,473
We have I think room for two, maybe three more.

1108
00:57:22,004 --> 00:57:24,647
Hello, sorry if I stutter, I'm a little nervous.

1109
00:57:24,767 --> 00:57:25,007
Oh no.

1110
00:57:26,329 --> 00:57:30,834
Being realistic, it will be probably a long time

1111
00:57:30,874 --> 00:57:35,819
before we have global regulation in AI ethics.

1112
00:57:37,281 --> 00:57:39,603
So how can we go about justifying?

1113
00:57:41,171 --> 00:57:45,374
like for producers and managers, that we have to care about these things.

1114
00:57:46,515 --> 00:57:49,997
Like, for example, in the Facebook ad about weight loss,

1115
00:57:51,458 --> 00:57:56,721
how could we justify that we shouldn't show that ad,

1116
00:57:57,662 --> 00:58:03,366
even though if we show, we know that it can perform well maybe?

1117
00:58:05,387 --> 00:58:09,410
And in a different realm...

1118
00:58:10,258 --> 00:58:19,624
like uh in the realm of diversity for example um there is a lot of research about uh why

1119
00:58:19,664 --> 00:58:27,729
it's important to have like diverse teams and like uh so we can use this information to

1120
00:58:27,849 --> 00:58:33,152
justify to the managers why it's important uh so are there uh any questions?

1121
00:58:34,789 --> 00:58:45,601
Is there any studies being done like, oh, we, if we are more ethical, we can also perform better?

1122
00:58:48,087 --> 00:58:54,553
So that's a great question. We're asking, the platforms for ethics and AI just doesn't exist today.

1123
00:58:54,953 --> 00:58:58,376
And this is why we're talking about it and trying to see what people can do.

1124
00:58:58,516 --> 00:59:06,202
So two questions there. One, what can we do to help inform producers, people on teams to think about ethics?

1125
00:59:06,683 --> 00:59:10,646
And then what are some of the trainings that already exist that we can share with managers?

1126
00:59:14,797 --> 00:59:21,589
I'd say there's more studies on how diversity improves monetization and KPIs than ethics,

1127
00:59:21,810 --> 00:59:26,318
but I think you kind of get one from the other, or at least a broadening of that.

1128
00:59:28,999 --> 00:59:33,765
Yeah, I mean, there's a lot of horror stories out there right now about like machine learning gone bad.

1129
00:59:33,785 --> 00:59:36,448
Right. Usually as a result of a lack of diversity.

1130
00:59:36,908 --> 00:59:37,289
Exactly.

1131
00:59:37,870 --> 00:59:43,596
And I think, you know, finding those kind of articles is a really good start for this kind of work.

1132
00:59:44,537 --> 00:59:45,158
We should do it more.

1133
00:59:46,330 --> 00:59:51,833
Yeah, also about having a great diversity in your team is going to help try to figure

1134
00:59:51,913 --> 00:59:57,076
out how this system is going to work and to try to come up with all the different ways

1135
00:59:57,116 --> 00:59:57,737
it can go bad.

1136
00:59:58,697 --> 01:00:04,761
If you only have people that look like you on the team, it's going to be very hard to

1137
01:00:04,801 --> 01:00:06,042
come up with all these examples.

1138
01:00:07,122 --> 01:00:09,264
So that's a way also to talk about.

1139
01:00:09,284 --> 01:00:13,546
And I would say for us, it's talking about this, not just that advocacy.

1140
01:00:13,927 --> 01:00:16,488
conferences, but talking about this with the people actually

1141
01:00:16,528 --> 01:00:18,589
working in the field, so they start being more aware.

1142
01:00:20,270 --> 01:00:22,071
Last question, Nicole, I think?

1143
01:00:22,431 --> 01:00:23,091
Yes, thank you.

1144
01:00:23,171 --> 01:00:23,932
Great panel.

1145
01:00:23,952 --> 01:00:25,993
Really awesome that it's here at the AI Summit.

1146
01:00:26,833 --> 01:00:30,835
And my question is just an extension of the idea of like,

1147
01:00:31,635 --> 01:00:33,436
well, in the context of Facebook,

1148
01:00:33,676 --> 01:00:34,997
I actually have a gender blocker.

1149
01:00:35,497 --> 01:00:37,918
I declare myself as non-declared.

1150
01:00:37,938 --> 01:00:40,220
And what I found is that I got less.

1151
01:00:40,640 --> 01:00:43,060
wrinkle cream and dresses, and you know,

1152
01:00:43,080 --> 01:00:45,321
I mean you're going to get Zooli ads

1153
01:00:45,381 --> 01:00:46,702
no matter which gender you choose.

1154
01:00:47,882 --> 01:00:49,842
But I thought that was a way of me as a user saying,

1155
01:00:49,883 --> 01:00:52,303
look, that's not what I want to see

1156
01:00:52,343 --> 01:00:54,084
and I will go out intentionally on the net

1157
01:00:54,504 --> 01:00:57,385
and like go to some very specific sites

1158
01:00:57,425 --> 01:01:00,846
in order to kind of crack the code that might be,

1159
01:01:01,226 --> 01:01:05,007
that's the preferences that the AI is assuming about me.

1160
01:01:05,607 --> 01:01:09,269
So my question is, is there a lot of talk about giving users

1161
01:01:09,329 --> 01:01:12,130
control over the platform?

1162
01:01:12,610 --> 01:01:14,151
Can we have a reset button?

1163
01:01:14,271 --> 01:01:16,112
We've talked a little bit, I've heard a little bit last

1164
01:01:16,132 --> 01:01:18,453
year about having a reset button, not just for ads in

1165
01:01:18,493 --> 01:01:21,414
general, but like my social media feeds.

1166
01:01:22,114 --> 01:01:26,896
I want to be able to hit a reset button and just start from scratch.

1167
01:01:27,197 --> 01:01:31,599
I may go down a really dark path, and am I going to be able to like my way out of that

1168
01:01:31,679 --> 01:01:32,239
dark path?

1169
01:01:32,979 --> 01:01:39,902
And I've got multiple personalities, not as a medical condition, but just simply, I run

1170
01:01:39,942 --> 01:01:42,304
a studio, I'm a designer, I'm also an engineer.

1171
01:01:42,324 --> 01:01:45,605
I want to be able to, and I'm in a lively environment, so I want to be able to switch.

1172
01:01:46,125 --> 01:01:48,009
And I don't have those kind of controls either.

1173
01:01:48,891 --> 01:01:51,916
So there's this efficacy of just being like one thing for

1174
01:01:51,996 --> 01:01:55,443
everybody, and that's just like a fire hose, without

1175
01:01:55,483 --> 01:01:56,385
that flexibility.

1176
01:01:56,445 --> 01:01:58,869
So are people talking about the ethics of user control?

1177
01:01:59,561 --> 01:02:00,242
Certainly they are.

1178
01:02:01,102 --> 01:02:03,344
The idea of especially the right to be forgotten

1179
01:02:03,464 --> 01:02:07,227
is like how people talk about like your permission

1180
01:02:07,308 --> 01:02:10,090
to have information about you removed from the system.

1181
01:02:10,110 --> 01:02:12,872
And there are specific applications of that under GDPR,

1182
01:02:12,952 --> 01:02:16,275
but that could apply in a lot of other cases as well.

1183
01:02:17,896 --> 01:02:19,218
I know we don't have a lot of time to answer so.

1184
01:02:19,693 --> 01:02:21,793
Yeah, especially in Europe, there's a lot of people

1185
01:02:21,873 --> 01:02:22,513
thinking about that.

1186
01:02:22,533 --> 01:02:23,654
And there's a lot of committees.

1187
01:02:24,374 --> 01:02:27,714
In France, you have the CNIL, C-N-I-L. They're looking at

1188
01:02:27,734 --> 01:02:28,374
all these questions.

1189
01:02:28,434 --> 01:02:31,015
But yeah, in Europe, you can ask for your data to be

1190
01:02:31,215 --> 01:02:32,215
removed, so I'm not sure.

1191
01:02:32,235 --> 01:02:34,315
Yeah, I mean, it's not necessarily easy.

1192
01:02:34,335 --> 01:02:35,476
It kind of depends on.

1193
01:02:35,676 --> 01:02:37,136
But I think it's possible to.

1194
01:02:37,376 --> 01:02:40,536
Yeah, I mean, but as people become conscious of that,

1195
01:02:41,097 --> 01:02:43,837
building products towards a point where it becomes easier

1196
01:02:43,897 --> 01:02:45,137
to opt out of things.

1197
01:02:46,297 --> 01:02:47,358
Or say, you know what?

1198
01:02:48,998 --> 01:02:53,444
I would like to change my gender presentation now, and that's across the board, so let's

1199
01:02:53,625 --> 01:02:55,808
acknowledge that, like all of those kinds of applications.

1200
01:02:56,690 --> 01:03:01,012
Yeah, I mean, there's also, there is a flip side to that though,

1201
01:03:01,052 --> 01:03:04,373
that is the ethical consideration of people using that kind of tool

1202
01:03:05,014 --> 01:03:07,455
to remove things that people should know about.

1203
01:03:08,995 --> 01:03:12,457
You know, we, there are a lot of kind of right to be forgotten cases

1204
01:03:13,077 --> 01:03:17,038
where it's a politician who doesn't want some image to be circulating,

1205
01:03:18,039 --> 01:03:20,480
generally of them with some high profile donor or something.

1206
01:03:22,951 --> 01:03:25,514
you know, there's a counterpoint to the ethics,

1207
01:03:25,574 --> 01:03:27,397
and I think there's use cases on both sides.

1208
01:03:27,577 --> 01:03:30,340
Yeah, but I think it's also not just the right to be forgotten

1209
01:03:30,360 --> 01:03:32,603
in terms of what I publish, but as a consumer,

1210
01:03:32,643 --> 01:03:35,266
I've got a right to, you know, adjust, you know,

1211
01:03:35,286 --> 01:03:36,588
the difference, you know, I want to be able to go

1212
01:03:36,648 --> 01:03:39,031
to a horror movie or a romantic comedy

1213
01:03:39,191 --> 01:03:39,812
or something like that.

1214
01:03:39,852 --> 01:03:42,055
I don't have that flexibility in social media right now.

1215
01:03:42,716 --> 01:03:44,697
I don't have a wide variety of things.

1216
01:03:44,777 --> 01:03:45,979
It's just one thing.

1217
01:03:46,039 --> 01:03:51,564
And again, you can navigate yourself into some very specific feed content, and it's

1218
01:03:51,584 --> 01:03:53,225
very hard to get out, especially if you're not a developer.

1219
01:03:53,245 --> 01:03:55,247
I think we're going to have to continue the discussion out there.

1220
01:03:55,267 --> 01:03:56,268
We are already over, but thank you so much.

1221
01:03:56,288 --> 01:03:56,988
Thank you to our panelists.

1222
01:03:57,008 --> 01:03:57,189
Thank you.

1223
01:03:57,209 --> 01:03:57,429
Thank you.

1224
01:03:57,449 --> 01:03:57,649
Thank you.

1225
01:03:57,669 --> 01:03:57,849
Thank you.

1226
01:03:57,869 --> 01:03:58,169
Thank you.

1227
01:03:58,210 --> 01:03:58,490
Thank you.

1228
01:03:58,510 --> 01:03:58,830
Thank you.

1229
01:03:58,850 --> 01:03:59,170
Thank you.

1230
01:03:59,190 --> 01:03:59,471
Thank you.

1231
01:03:59,491 --> 01:03:59,691
Thank you.

1232
01:03:59,711 --> 01:03:59,971
Thank you.

1233
01:03:59,991 --> 01:04:00,171
Thank you.

