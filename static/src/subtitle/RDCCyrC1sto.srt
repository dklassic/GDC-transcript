1
00:00:08,398 --> 00:00:08,999
Hey, everyone.

2
00:00:09,039 --> 00:00:09,760
My name's Ben.

3
00:00:11,681 --> 00:00:13,703
I'm the lead programmer of Havoc AI.

4
00:00:15,024 --> 00:00:18,247
So I've been talking here on and off for like five years

5
00:00:18,327 --> 00:00:20,749
about how game AI needs to be more ambitious.

6
00:00:20,970 --> 00:00:23,372
And I don't think that's a particularly controversial

7
00:00:23,412 --> 00:00:23,872
viewpoint.

8
00:00:24,112 --> 00:00:25,634
Of course, we should try to make things better.

9
00:00:26,194 --> 00:00:28,496
So the question is, what holds us back?

10
00:00:28,736 --> 00:00:30,038
What limits game AI?

11
00:00:30,418 --> 00:00:32,380
What limits the set of tools we can use?

12
00:00:33,300 --> 00:00:35,821
And the number one answer is computation time.

13
00:00:36,161 --> 00:00:38,762
There might be some technique that's absolutely amazing

14
00:00:38,882 --> 00:00:41,603
for, say, action selection for companion AI,

15
00:00:41,943 --> 00:00:43,703
but if it takes 20 milliseconds to run,

16
00:00:43,723 --> 00:00:44,724
then it's off the table.

17
00:00:44,884 --> 00:00:46,764
We have 60 frames a second to push out,

18
00:00:46,804 --> 00:00:48,305
so that's just not practical.

19
00:00:48,685 --> 00:00:51,446
We have to do the best we can with the time we have.

20
00:00:52,386 --> 00:00:55,367
And I don't want to imply that AI is alone in that tragedy.

21
00:00:55,687 --> 00:00:56,807
Everybody wants to do more.

22
00:00:57,287 --> 00:00:59,788
The physics people want to do real-time fluid dynamics.

23
00:01:00,108 --> 00:01:03,389
The graphics people want individual beard hairs swaying in the wind.

24
00:01:03,909 --> 00:01:04,949
The sound people...

25
00:01:04,970 --> 00:01:07,910
I'm not really 100% on what the sound people want,

26
00:01:08,551 --> 00:01:10,431
but I'll bet that if they had more computation time,

27
00:01:10,471 --> 00:01:12,232
stuff would sound just extra awesome.

28
00:01:14,484 --> 00:01:17,688
Well, I don't have any answers for them, but I've got an answer for us.

29
00:01:18,208 --> 00:01:21,332
Because in AI, we have special powers.

30
00:01:21,952 --> 00:01:22,753
We have an ability.

31
00:01:22,813 --> 00:01:25,636
We can do something that none of those guys can do.

32
00:01:28,479 --> 00:01:29,060
We can wait.

33
00:01:30,667 --> 00:01:34,712
Let me explain. The other disciplines I mentioned, physics, graphics, sound, are

34
00:01:34,812 --> 00:01:35,693
presentational.

35
00:01:36,033 --> 00:01:39,617
They do their thing sixty times a second, or whatever your target frame rate is,

36
00:01:40,077 --> 00:01:41,799
and it has to happen that frame.

37
00:01:42,240 --> 00:01:43,621
If you're tossing crates around,

38
00:01:43,941 --> 00:01:47,085
they can't pause for a few frames and decide which way to bounce.

39
00:01:47,405 --> 00:01:48,306
Players would notice that.

40
00:01:48,727 --> 00:01:51,950
These systems have a responsibility to run in real time.

41
00:01:53,461 --> 00:01:54,803
Some of AI is real time.

42
00:01:55,264 --> 00:01:56,626
Things like collision avoidance,

43
00:01:56,706 --> 00:01:59,591
whatever we need to feed to animation, that stuff.

44
00:01:59,952 --> 00:02:01,975
But those things are already fast enough.

45
00:02:02,496 --> 00:02:04,700
What's left is stuff like action selection,

46
00:02:04,820 --> 00:02:07,124
navigation, rebuilding occupancy grids.

47
00:02:09,044 --> 00:02:14,628
What's left is deliberation, basically, and by deliberation, I mean large-scale computation,

48
00:02:14,888 --> 00:02:18,230
which takes in a lot of information and produces a complicated result.

49
00:02:18,650 --> 00:02:22,793
Things like action selection, where you have to think about everyone around you and what

50
00:02:22,833 --> 00:02:28,157
your health is and how many arrows you have left, or pathfinding, where you consider thousands

51
00:02:28,177 --> 00:02:32,580
of potential routes through the world and produce a path potentially involving many,

52
00:02:32,640 --> 00:02:33,300
many steps.

53
00:02:35,501 --> 00:02:38,163
These things are slow, they have to be slow to be good,

54
00:02:38,604 --> 00:02:39,664
but they're infrequent.

55
00:02:39,744 --> 00:02:43,327
We don't need to peg the CPU at 100% just on pathfinding,

56
00:02:43,608 --> 00:02:46,270
because we only need new paths once in a while.

57
00:02:47,030 --> 00:02:49,713
Say it costs 20 milliseconds to find a path.

58
00:02:49,833 --> 00:02:52,115
Well, that's way too much to spend on a single frame.

59
00:02:52,535 --> 00:02:54,877
But it's also crazy fast by human standards.

60
00:02:55,457 --> 00:02:57,939
If we were to spread that out over 20 frames,

61
00:02:58,320 --> 00:03:00,822
that's a third of a second, one millisecond per frame,

62
00:03:01,542 --> 00:03:05,753
A third of a second is an entirely reasonable amount of time for a character to spend

63
00:03:05,913 --> 00:03:07,337
figuring out which way to go.

64
00:03:07,778 --> 00:03:09,402
That's normal human reaction time.

65
00:03:11,370 --> 00:03:15,313
So our special power is we can amortize these expensive but

66
00:03:15,433 --> 00:03:19,376
infrequent processes and hide the latency behind normal

67
00:03:19,436 --> 00:03:20,617
human reaction time.

68
00:03:21,057 --> 00:03:24,120
We can harness a huge amount of potential computation time

69
00:03:24,460 --> 00:03:27,042
as long as we don't do it too often and as long as we're

70
00:03:27,082 --> 00:03:28,804
willing to wait a little while for the results.

71
00:03:29,464 --> 00:03:32,206
If a particular planning problem takes an unexpectedly

72
00:03:32,246 --> 00:03:34,448
long time, that doesn't blow the frame rate.

73
00:03:34,788 --> 00:03:37,651
It just means the guy spends an extra fraction of a second

74
00:03:37,711 --> 00:03:38,311
cogitating.

75
00:03:39,571 --> 00:03:44,262
Slicing up long computations, I think, is absolutely essential for really next-level

76
00:03:44,443 --> 00:03:44,744
AI.

77
00:03:45,044 --> 00:03:46,428
We will never get there without it.

78
00:03:48,590 --> 00:03:49,991
So how to slice up a task?

79
00:03:50,731 --> 00:03:51,832
I see three approaches.

80
00:03:52,132 --> 00:03:54,654
The most obvious way is split it up manually.

81
00:03:55,034 --> 00:03:58,396
Like do n iterations of A star on this frame and n

82
00:03:58,456 --> 00:03:59,797
iterations on the next frame.

83
00:04:00,457 --> 00:04:02,979
Or if you're building a nav mesh, maybe you'll do the face

84
00:04:03,019 --> 00:04:05,881
clipping on one frame and edge connection on the next frame

85
00:04:05,901 --> 00:04:07,322
and so on.

86
00:04:08,362 --> 00:04:11,284
The second approach is put the computation on a background

87
00:04:11,324 --> 00:04:14,426
thread or multiple background threads and let it chug along

88
00:04:14,466 --> 00:04:15,167
with your workload.

89
00:04:15,407 --> 00:04:17,988
And then every frame, you check whether it's finished.

90
00:04:19,606 --> 00:04:22,531
And the third approach is kind of a combination of the two,

91
00:04:23,031 --> 00:04:25,636
and it's what I'm going to spend most of my time talking about.

92
00:04:26,457 --> 00:04:28,440
But first, I want to concentrate on those first two

93
00:04:28,821 --> 00:04:30,804
and why they're good, but not quite good enough.

94
00:04:33,582 --> 00:04:37,845
The big problem with manual slicing is that it really, really sucks to write.

95
00:04:38,286 --> 00:04:41,728
When I'm programming an algorithm, I do it with four loops and function calls

96
00:04:41,769 --> 00:04:43,710
and all the normal programming stuff.

97
00:04:44,491 --> 00:04:46,633
One millisecond after an algorithm starts running,

98
00:04:46,693 --> 00:04:48,634
maybe it's five function calls deep.

99
00:04:49,155 --> 00:04:51,597
It's got a ton of local state at each of the five levels.

100
00:04:51,957 --> 00:04:52,998
It's midway through a loop.

101
00:04:53,659 --> 00:04:56,621
The state it is in is remarkably complicated.

102
00:04:57,462 --> 00:05:01,183
And writing the algorithm in such a way that it can save that state

103
00:05:01,483 --> 00:05:04,264
and then stop and then come back to that state later

104
00:05:04,605 --> 00:05:05,765
is truly painful.

105
00:05:08,766 --> 00:05:10,227
Here's an easy example. Quicksort.

106
00:05:11,007 --> 00:05:12,188
Here's regular old Quicksort.

107
00:05:13,359 --> 00:05:16,520
And here's what I need to do to spread quicksort over multiple frames.

108
00:05:17,060 --> 00:05:20,441
I need external state, I need to flatten subroutine calls in general,

109
00:05:20,642 --> 00:05:24,363
I need way more complicated ways of expressing the same thing.

110
00:05:24,923 --> 00:05:27,784
Simply put, this is not how these programming languages

111
00:05:28,064 --> 00:05:29,304
were meant to be used.

112
00:05:30,545 --> 00:05:32,526
Manual slicing is painful enough, in fact,

113
00:05:33,106 --> 00:05:35,907
if you're doing it, you're tempted to scale back your ambitions

114
00:05:36,227 --> 00:05:39,148
just because every layer of detail doubles the difficulty.

115
00:05:40,680 --> 00:05:43,681
Also, it forces you to guess where you need to split,

116
00:05:44,121 --> 00:05:45,801
and usually your guesses are wrong.

117
00:05:46,162 --> 00:05:48,402
And then changing those splits is even more painful,

118
00:05:48,662 --> 00:05:51,383
because you have to completely rework which state you save

119
00:05:51,723 --> 00:05:53,204
and how you resume the algorithm.

120
00:05:53,704 --> 00:05:56,405
For example, that manually sliced quicksort

121
00:05:56,785 --> 00:05:59,646
assumes I'll never want to slice in the middle of a left recursion.

122
00:06:00,166 --> 00:06:03,107
That's why it's so short and simple, relatively speaking.

123
00:06:03,647 --> 00:06:06,168
Feel like rewriting it when that assumption turns out to be wrong?

124
00:06:06,188 --> 00:06:07,088
Because it gets a lot longer.

125
00:06:09,334 --> 00:06:12,357
Manual slicing is a monumentally inefficient solution

126
00:06:12,478 --> 00:06:14,100
in terms of implementation burden.

127
00:06:14,560 --> 00:06:18,085
But at the same time, we should recognize for a moment

128
00:06:18,225 --> 00:06:20,288
how promising it is as an ideal.

129
00:06:20,909 --> 00:06:22,250
If we could really do this,

130
00:06:22,631 --> 00:06:25,014
we'd have as much control over timing as we wanted.

131
00:06:26,418 --> 00:06:30,360
I'll mention in passing that some languages support a coding

132
00:06:30,400 --> 00:06:34,461
style known as continuation passing or another related

133
00:06:34,521 --> 00:06:37,383
construct known as coroutines, which make this all

134
00:06:37,443 --> 00:06:38,563
considerably simpler.

135
00:06:38,943 --> 00:06:41,964
So if your game is written in C Sharp or Lua, it's worth

136
00:06:42,004 --> 00:06:44,305
looking into, though continuation passing and

137
00:06:44,345 --> 00:06:47,566
coroutines can work even better with the approach I'll

138
00:06:47,606 --> 00:06:48,307
talk about later.

139
00:06:51,278 --> 00:06:54,581
The next approach is background threads, where you rely on the

140
00:06:54,661 --> 00:06:57,804
operating system scheduler to do the time slicing for you.

141
00:06:58,444 --> 00:07:01,286
You create a background thread, or maybe multiple threads

142
00:07:01,527 --> 00:07:04,709
organized into a pool, and you execute your long-running

143
00:07:04,769 --> 00:07:07,351
tasks on them instead of on the main thread.

144
00:07:08,052 --> 00:07:11,555
The idea is to use up spare CPU cycles during the rest of

145
00:07:11,595 --> 00:07:14,637
the frame, and during the AI step, just process what's done.

146
00:07:16,290 --> 00:07:19,973
Since these threads have their own stacks, there's no need for manual slicing.

147
00:07:20,373 --> 00:07:22,755
The operating system takes care of saving state for you,

148
00:07:23,156 --> 00:07:24,897
and you can program in the normal fashion.

149
00:07:26,458 --> 00:07:30,441
I've used background threads a lot, and I think they can work very well.

150
00:07:31,021 --> 00:07:33,803
But there's some problems you need to constantly keep in mind

151
00:07:33,823 --> 00:07:34,444
if you're using these.

152
00:07:38,251 --> 00:07:43,574
The immediate problem is your background threads can end up taking resources away from your main thread.

153
00:07:43,954 --> 00:07:46,595
You've got enough background tasks to fill up your cores,

154
00:07:47,116 --> 00:07:48,857
and then you've also got the main thread, of course.

155
00:07:49,437 --> 00:07:51,578
And all these threads have stuff to do,

156
00:07:51,918 --> 00:07:55,900
so if the scheduler decides that it's going to run your background task threads for a while

157
00:07:56,221 --> 00:07:59,762
and pause your main thread, then there goes your frame rate.

158
00:08:00,623 --> 00:08:02,367
But wait, you say, thread priorities.

159
00:08:02,687 --> 00:08:05,212
I'll just set my background threads to have low priority

160
00:08:05,433 --> 00:08:07,156
and my main thread to have a high priority

161
00:08:07,216 --> 00:08:08,979
so the background threads only execute

162
00:08:09,000 --> 00:08:10,322
when there's nothing else going on.

163
00:08:10,883 --> 00:08:12,366
Good idea, except...

164
00:08:13,932 --> 00:08:16,334
Here's a bunch of threads, and here's your CPU cores.

165
00:08:17,094 --> 00:08:19,476
At any given time, some of the threads are running,

166
00:08:19,736 --> 00:08:22,358
some are ready to run, and some are waiting for I O,

167
00:08:22,478 --> 00:08:25,060
or waiting for more tasks, or waiting for a mutex.

168
00:08:25,660 --> 00:08:27,922
And the usual assumption is the cores

169
00:08:27,962 --> 00:08:30,143
are filled with the highest priority threads, which

170
00:08:30,183 --> 00:08:31,484
are currently ready to run.

171
00:08:32,005 --> 00:08:34,286
So if a high priority thread becomes ready,

172
00:08:34,867 --> 00:08:36,748
it kicks out a lower priority thread.

173
00:08:37,469 --> 00:08:39,370
and when a thread starts waiting on something

174
00:08:39,850 --> 00:08:42,472
it gets replaced with the ready thread

175
00:08:42,672 --> 00:08:44,133
which has the highest priority.

176
00:08:48,156 --> 00:08:51,158
But on some platforms, which shall remain nameless

177
00:08:51,579 --> 00:08:53,440
but which were possibly developed by a company

178
00:08:53,620 --> 00:08:54,561
I maybe now work for,

179
00:08:55,081 --> 00:08:56,682
there's this weird effect where

180
00:08:56,942 --> 00:08:59,064
once a thread is assigned to a core

181
00:08:59,364 --> 00:09:00,925
it really doesn't want to move.

182
00:09:01,926 --> 00:09:03,927
Each core has its own set of threads

183
00:09:04,368 --> 00:09:05,609
and only looks there

184
00:09:06,565 --> 00:09:08,247
when it needs to find a thread to run.

185
00:09:09,148 --> 00:09:13,392
The only time a thread is moved is when a core has absolutely nothing to do.

186
00:09:14,153 --> 00:09:16,715
Only then does it steal a thread from another core.

187
00:09:18,016 --> 00:09:21,057
So if this is your main thread and this is your graphics thread,

188
00:09:21,657 --> 00:09:23,557
they can get stuck on the same core.

189
00:09:23,898 --> 00:09:26,798
And you've got all these other threads on the other cores humming along,

190
00:09:26,858 --> 00:09:28,439
since, of course, they've got plenty to do,

191
00:09:28,959 --> 00:09:31,840
with the effect that your higher priority threads have

192
00:09:31,860 --> 00:09:33,780
to compete with each other for resources,

193
00:09:34,100 --> 00:09:36,481
while your low priority threads get all the time they want.

194
00:09:39,168 --> 00:09:43,672
The solution is thread affinities, where you control which threads can be on which cores.

195
00:09:44,352 --> 00:09:47,835
Thread affinities are powerful, but they're really difficult to get right.

196
00:09:48,395 --> 00:09:51,218
If you make them too stringent, you end up wasting cores.

197
00:09:51,958 --> 00:09:55,682
A decent rule of thumb is give all your high-priority threads

198
00:09:56,082 --> 00:10:00,025
different thread affinities, and don't give thread affinities to lower-priority threads.

199
00:10:00,485 --> 00:10:04,129
But even that isn't enough if you have multi-threaded foreground tasks

200
00:10:04,229 --> 00:10:06,130
to the point where you have oversubscribed cores.

201
00:10:09,411 --> 00:10:13,074
Another problem is priority inversion, which I'll just rush through.

202
00:10:15,436 --> 00:10:20,279
If this low priority thread holds a lock, and this high priority thread wants that lock,

203
00:10:20,780 --> 00:10:24,202
then this medium priority thread ends up getting all the core time.

204
00:10:25,925 --> 00:10:28,066
Priority inversion is a complicated subject,

205
00:10:28,146 --> 00:10:29,787
and I don't want to go too far into it,

206
00:10:30,147 --> 00:10:33,048
but the main takeaway is that if your background thread

207
00:10:33,128 --> 00:10:34,829
and foreground thread share a lock,

208
00:10:35,209 --> 00:10:37,010
which is pretty much inevitable if you want them

209
00:10:37,030 --> 00:10:40,151
to communicate, then you can get into situations

210
00:10:40,511 --> 00:10:42,812
where a background thread prevents your main thread

211
00:10:42,852 --> 00:10:43,292
from running.

212
00:10:46,713 --> 00:10:49,675
The final problem with background threads is more subtle.

213
00:10:50,805 --> 00:10:54,366
The whole point of background threads is to use up spare CPU.

214
00:10:54,846 --> 00:10:58,027
But that means that the latency of your background tasks

215
00:10:58,427 --> 00:11:01,088
is at the mercy of everything else in your game.

216
00:11:01,708 --> 00:11:05,369
So in some area of the game, say, that have a lot of physics bodies,

217
00:11:05,769 --> 00:11:08,830
suddenly your AI latency goes way up in that area,

218
00:11:09,110 --> 00:11:11,370
because physics tasks are taking up more time,

219
00:11:11,630 --> 00:11:13,691
which means less time for the background tasks.

220
00:11:14,711 --> 00:11:17,332
With background threads, you have a ton of potential CPU,

221
00:11:17,752 --> 00:11:19,232
but no guaranteed CPU.

222
00:11:20,559 --> 00:11:23,580
manual slicing doesn't have that problem because your code runs in the foreground.

223
00:11:26,342 --> 00:11:28,382
So, well, what do we like about these systems?

224
00:11:28,843 --> 00:11:32,704
With manual slicing, the primary advantage is the level of control.

225
00:11:33,044 --> 00:11:34,865
If we were to do enough work on it,

226
00:11:35,385 --> 00:11:38,167
we can do AI for exactly as long as we want to do AI.

227
00:11:40,817 --> 00:11:41,977
But it's a lot of work.

228
00:11:42,457 --> 00:11:44,138
Background threads are a lot easier.

229
00:11:44,798 --> 00:11:47,279
You don't usually hear somebody say how easy multi-threading is,

230
00:11:47,319 --> 00:11:47,959
but there you go.

231
00:11:48,380 --> 00:11:50,360
But you have no control over what they're doing.

232
00:11:52,541 --> 00:11:55,102
Here's a third option, and it's how we've been approaching

233
00:11:55,182 --> 00:11:56,763
asynchrony in Havoc AI.

234
00:11:58,977 --> 00:12:02,980
The basic idea is you have a pool of threads like with the last approach,

235
00:12:03,461 --> 00:12:06,263
but instead of running in the background,

236
00:12:06,623 --> 00:12:07,764
they run in the foreground.

237
00:12:08,184 --> 00:12:11,527
And every frame, the main thread waits for them.

238
00:12:12,207 --> 00:12:13,788
But it doesn't wait forever.

239
00:12:14,349 --> 00:12:17,271
After a specific period of time, say three milliseconds,

240
00:12:17,671 --> 00:12:19,032
those threads get suspended.

241
00:12:19,693 --> 00:12:21,695
And then the next frame, they get woken up again.

242
00:12:23,351 --> 00:12:25,172
So this is the best of both worlds in a way.

243
00:12:25,532 --> 00:12:28,474
For manual slicing, you get precise control

244
00:12:28,554 --> 00:12:31,236
over how much CPU goes to AI, and you

245
00:12:31,256 --> 00:12:33,477
get a guaranteed amount of execution time,

246
00:12:33,817 --> 00:12:35,719
regardless of what else is going on in the game.

247
00:12:36,259 --> 00:12:38,580
But unlike manual slicing, you don't

248
00:12:38,620 --> 00:12:40,261
have to code in an unpleasant style.

249
00:12:40,341 --> 00:12:42,483
You just implement as usual, and you

250
00:12:42,503 --> 00:12:45,084
let the OS take care of saving your state

251
00:12:45,184 --> 00:12:46,625
and suspending and resuming things.

252
00:12:48,485 --> 00:12:50,810
The problem then becomes how do you do this?

253
00:12:50,970 --> 00:12:54,156
How do you get the threads to run for precisely three milliseconds

254
00:12:54,197 --> 00:12:56,241
during the AI step and then stop?

255
00:12:57,986 --> 00:13:01,149
Well, my first idea was to have the OS schedule things.

256
00:13:01,630 --> 00:13:04,433
So the main thread has the absolute highest priority,

257
00:13:04,973 --> 00:13:08,697
and the asynchronous threads are just below that, but suspended.

258
00:13:09,558 --> 00:13:11,760
The main thread wakes up the async threads,

259
00:13:12,101 --> 00:13:15,064
and then sets a timer for itself for three milliseconds,

260
00:13:15,444 --> 00:13:18,127
and then sleeps on that timer while the async threads do

261
00:13:18,147 --> 00:13:18,587
their thing.

262
00:13:19,308 --> 00:13:20,369
When the timer goes off.

263
00:13:21,276 --> 00:13:23,040
It suspends the async threads,

264
00:13:23,401 --> 00:13:26,508
processes any results that they've produced during that frame,

265
00:13:26,929 --> 00:13:28,373
and then the AI tick is over.

266
00:13:31,655 --> 00:13:33,316
There's a couple of problems with this, though.

267
00:13:33,936 --> 00:13:36,357
First, most of the platforms you're going to be working on

268
00:13:36,437 --> 00:13:38,578
are not set up for this level of precision.

269
00:13:39,098 --> 00:13:42,580
A timer which goes off won't actually resume the thread

270
00:13:42,660 --> 00:13:46,002
holding the timer until after the next natural scheduler

271
00:13:46,062 --> 00:13:49,783
tick, which on most platforms is around 16 milliseconds.

272
00:13:50,404 --> 00:13:52,265
On some platforms, you can adjust this.

273
00:13:52,605 --> 00:13:55,526
But even at the minimum tick, you don't have very fine

274
00:13:55,566 --> 00:13:56,306
grained control.

275
00:13:57,969 --> 00:14:00,971
the more serious problem is the possibility of deadlock.

276
00:14:01,631 --> 00:14:04,353
Let's say that when an async thread's three milliseconds were up,

277
00:14:04,853 --> 00:14:06,314
it had just finished processing,

278
00:14:06,814 --> 00:14:08,675
it had grabbed a lock to write its output,

279
00:14:09,035 --> 00:14:11,497
and then just then the main thread suspended it.

280
00:14:11,897 --> 00:14:15,379
And then, of course, the main thread tried to grab that lock itself,

281
00:14:15,739 --> 00:14:17,120
just to see if there was output ready.

282
00:14:17,620 --> 00:14:18,821
And bam, deadlock.

283
00:14:18,861 --> 00:14:19,821
Your game has crashed.

284
00:14:21,014 --> 00:14:24,016
the main thread is waiting for a lock but the only thread that can give it that

285
00:14:24,076 --> 00:14:27,859
lock is asleep and the only thread that can wake up that thread is waiting for a

286
00:14:27,899 --> 00:14:28,139
lock.

287
00:14:29,119 --> 00:14:31,521
So, do not be tempted to go this route.

288
00:14:32,201 --> 00:14:36,804
It might work 999 times out of a thousand and then the stars align and

289
00:14:36,824 --> 00:14:39,786
you get a deadlock, probably while you're demoing your game at E3.

290
00:14:40,247 --> 00:14:41,027
That's how fate works.

291
00:14:42,594 --> 00:14:44,315
Deadlock happens because at some points,

292
00:14:44,575 --> 00:14:47,096
it is just not safe to suspend async threads.

293
00:14:47,736 --> 00:14:50,237
My initial thought was that the async threads should just

294
00:14:50,337 --> 00:14:53,858
guard those locations, setting some flag

295
00:14:53,918 --> 00:14:55,919
to tell the main thread that it has

296
00:14:55,939 --> 00:14:58,160
to wait a little longer before it gets to suspend them.

297
00:14:59,540 --> 00:15:01,801
But there's a simpler and more elegant solution.

298
00:15:02,441 --> 00:15:05,502
Let the threads themselves take care of going to sleep.

299
00:15:06,853 --> 00:15:09,674
Instead of relying on the scheduler, the async threads

300
00:15:10,034 --> 00:15:11,514
regularly check a timer.

301
00:15:12,055 --> 00:15:15,356
And when their time is up, they voluntarily grab a

302
00:15:16,236 --> 00:15:19,217
semaphore, which won't be given to them until the next

303
00:15:19,257 --> 00:15:20,417
frame, causing them to sleep.

304
00:15:21,418 --> 00:15:24,059
They also take care of notifying the main thread when

305
00:15:24,079 --> 00:15:27,060
the last async thread has grabbed the semaphore and gone

306
00:15:27,100 --> 00:15:27,420
to sleep.

307
00:15:31,932 --> 00:15:34,533
So the main thread wakes up the async threads

308
00:15:34,633 --> 00:15:36,294
by releasing the SEMA4 they're waiting on.

309
00:15:36,914 --> 00:15:39,915
And then it waits for the everyone's gone back to sleep

310
00:15:39,995 --> 00:15:40,315
signal.

311
00:15:41,376 --> 00:15:42,976
When the async threads are woken up,

312
00:15:43,296 --> 00:15:46,057
they mark down the current time, add three milliseconds,

313
00:15:46,318 --> 00:15:47,238
and then they go to work.

314
00:15:48,438 --> 00:15:50,319
Every now and then, they check on the time.

315
00:15:50,579 --> 00:15:52,200
And when their three milliseconds are up,

316
00:15:52,560 --> 00:15:53,940
they wait on the SEMA4 again.

317
00:15:54,240 --> 00:15:57,242
And the last one to go to sleep notifies the main thread

318
00:15:57,442 --> 00:15:58,842
that the async step is over.

319
00:16:00,015 --> 00:16:03,456
And importantly, the async threads never go to sleep

320
00:16:03,777 --> 00:16:06,097
if they're holding an externally visible lock.

321
00:16:06,417 --> 00:16:08,678
They only go to sleep when it's safe to sleep.

322
00:16:10,939 --> 00:16:13,860
Essentially, we're replacing the OS scheduler

323
00:16:14,220 --> 00:16:15,360
with our own scheduler.

324
00:16:16,461 --> 00:16:18,901
The replacement is more precise, and it also

325
00:16:18,941 --> 00:16:22,122
has more information about when it's safe to switch over.

326
00:16:23,723 --> 00:16:26,244
It does require that we do these periodic timer checks,

327
00:16:27,184 --> 00:16:28,904
a procedure we call tend to thread pool.

328
00:16:29,847 --> 00:16:31,390
instead of using hardware interrupts.

329
00:16:31,791 --> 00:16:33,595
But that's really not a major burden

330
00:16:33,695 --> 00:16:35,018
compared to manual slicing.

331
00:16:37,870 --> 00:16:41,793
There's a certain number of annoying but important details I'm going to go into,

332
00:16:42,173 --> 00:16:44,775
but before I go into those, I want to note in passing

333
00:16:45,115 --> 00:16:48,517
that our implementation of an asynchronous thread pool and task queue

334
00:16:48,878 --> 00:16:51,179
will be available in the next release of Havoc AI,

335
00:16:52,060 --> 00:16:53,080
or actually just Havoc.

336
00:16:53,120 --> 00:16:55,482
If you're licensed of any Havoc product,

337
00:16:55,722 --> 00:16:57,523
you'll have access to the system for your own code,

338
00:16:57,603 --> 00:16:59,084
so if you license Havoc anything,

339
00:16:59,405 --> 00:17:00,846
you can probably just take a nap right now.

340
00:17:02,146 --> 00:17:04,047
I also want to give credit where credit is due.

341
00:17:04,448 --> 00:17:07,589
Havoc's implementation and the majority of details I'll go into later

342
00:17:07,909 --> 00:17:11,111
are not my work, but the work of my teammate, Leven van der Heide.

343
00:17:11,972 --> 00:17:14,833
So if you want to thank someone for this being an actual thing

344
00:17:14,993 --> 00:17:16,414
as opposed to a clever idea,

345
00:17:16,754 --> 00:17:18,535
he's the one who actually made this stuff work.

346
00:17:21,526 --> 00:17:24,187
The first detail is how the locks in the CMU4s work,

347
00:17:24,567 --> 00:17:27,407
the machinery of resuming the async threads

348
00:17:27,667 --> 00:17:28,868
and pausing the main thread,

349
00:17:29,228 --> 00:17:32,929
and then resuming the main thread when the async threads are back to being paused.

350
00:17:34,069 --> 00:17:36,229
Obviously, this is the most important part of the system,

351
00:17:36,609 --> 00:17:38,650
but I don't want to spend a ton of time on it,

352
00:17:38,850 --> 00:17:41,010
simply because working through correctness proofs

353
00:17:41,350 --> 00:17:42,831
is nobody's idea of a good time.

354
00:17:43,551 --> 00:17:46,852
But in short, each async thread has its own semaphore

355
00:17:47,132 --> 00:17:49,093
which it uses to suspend between threads,

356
00:17:49,533 --> 00:17:50,453
or between frames.

357
00:17:50,933 --> 00:17:52,954
And the main thread has a semaphore

358
00:17:53,014 --> 00:17:55,334
which the async threads use to wake it back up.

359
00:17:56,655 --> 00:17:59,035
Suffice it to say that it's not that difficult.

360
00:17:59,135 --> 00:18:01,196
If you can implement a task queuing system

361
00:18:01,496 --> 00:18:02,736
with dependencies and whatnot,

362
00:18:03,016 --> 00:18:05,297
you can implement a simple async thread pool.

363
00:18:06,497 --> 00:18:08,498
And just Google the sleeping barber problem.

364
00:18:08,678 --> 00:18:09,858
Everything you need to know is there.

365
00:18:12,630 --> 00:18:14,831
I do, though, want to talk about thread affinities

366
00:18:15,131 --> 00:18:16,592
and how they relate to the locking.

367
00:18:16,892 --> 00:18:19,354
It's very important that you set thread affinities

368
00:18:19,414 --> 00:18:20,855
for all your async threads.

369
00:18:21,275 --> 00:18:23,916
Otherwise, you can run into the problem I mentioned earlier,

370
00:18:24,397 --> 00:18:27,519
where threads get distributed to the wrong cores

371
00:18:27,659 --> 00:18:28,899
and end up waiting for each other

372
00:18:29,039 --> 00:18:30,620
while low-priority threads run.

373
00:18:30,961 --> 00:18:32,602
Without thread affinities,

374
00:18:32,642 --> 00:18:34,343
this is almost guaranteed to happen.

375
00:18:34,903 --> 00:18:38,125
So make sure to give each async thread its own core.

376
00:18:38,245 --> 00:18:40,986
And of course, don't oversubscribe your async threads.

377
00:18:43,103 --> 00:18:45,885
But having one async thread per core

378
00:18:46,246 --> 00:18:49,769
sets up a hilarious problem when you wake up all the async threads.

379
00:18:50,270 --> 00:18:52,752
Halfway through this wake up, maybe the main thread

380
00:18:52,872 --> 00:18:56,676
wakes up the async thread that it shares its thread affinity with,

381
00:18:56,716 --> 00:18:58,177
that's on the same core as it is.

382
00:18:58,437 --> 00:18:59,118
And what happens?

383
00:18:59,158 --> 00:19:01,720
Well, the async thread takes off, particularly

384
00:19:02,521 --> 00:19:06,865
if it's benefiting from priority boosting.

385
00:19:08,066 --> 00:19:10,228
and then the main thread is no longer running

386
00:19:10,348 --> 00:19:13,130
and never has a chance to wake up the rest of the async threads.

387
00:19:13,811 --> 00:19:17,294
You can solve this with careful use of affinities and priorities

388
00:19:17,414 --> 00:19:19,175
and waking the threads up in the right order.

389
00:19:19,575 --> 00:19:22,538
But a much simpler solution is just to chain the wakeups.

390
00:19:23,138 --> 00:19:26,041
Instead of the main thread waking up all the async threads,

391
00:19:26,341 --> 00:19:28,182
it just wakes up the first async thread.

392
00:19:28,282 --> 00:19:30,044
And when that async thread wakes up,

393
00:19:30,084 --> 00:19:32,586
the first thing it does is wake up the second thread,

394
00:19:32,626 --> 00:19:35,248
which wakes up the third thread, and so on.

395
00:19:37,011 --> 00:19:39,913
So as long as you've set correct affinities

396
00:19:39,994 --> 00:19:41,355
for all the async threads,

397
00:19:41,735 --> 00:19:44,517
that should wake them up basically simultaneously

398
00:19:44,838 --> 00:19:46,379
without any worries about scheduling.

399
00:19:49,341 --> 00:19:51,083
The next detail is timing.

400
00:19:51,743 --> 00:19:54,966
An x64 processor has a number of timers available to it,

401
00:19:55,286 --> 00:19:57,928
but the only one that's really precise enough for our needs

402
00:19:58,129 --> 00:19:59,450
is the timestamp counter,

403
00:19:59,750 --> 00:20:02,572
which is accessed through the rdtsc instruction.

404
00:20:03,694 --> 00:20:06,335
RDTSC gives you very high precision,

405
00:20:06,715 --> 00:20:07,856
but it's not perfect.

406
00:20:08,096 --> 00:20:11,918
In particular, it's a remarkably slow instruction.

407
00:20:12,418 --> 00:20:15,419
On some x64 chips, it's one of the slowest instructions

408
00:20:15,459 --> 00:20:16,180
you can execute.

409
00:20:17,100 --> 00:20:18,901
So if you've got an inner loop like this,

410
00:20:19,481 --> 00:20:21,662
don't tend to thread pool every time through.

411
00:20:22,443 --> 00:20:24,724
Instead, you can do a tend to thread pool

412
00:20:24,904 --> 00:20:26,585
on certain counter multiples.

413
00:20:27,685 --> 00:20:30,867
Or you can separate an inner and an outer loop like this.

414
00:20:32,936 --> 00:20:35,839
By the way, once upon a time, the timestamp counter

415
00:20:35,899 --> 00:20:38,301
was at the mercy of the CPU clock frequency

416
00:20:38,661 --> 00:20:40,543
and could be thrown off by power saving.

417
00:20:41,084 --> 00:20:44,066
That's actually no longer the case on modern x64 chips.

418
00:20:44,467 --> 00:20:46,449
But if you have a very low min spec,

419
00:20:46,649 --> 00:20:47,590
it's worth thinking about.

420
00:20:48,370 --> 00:20:50,813
Likewise, once upon a time, RDTSC

421
00:20:50,893 --> 00:20:53,295
wasn't necessarily synchronized between cores.

422
00:20:53,735 --> 00:20:55,537
But on the platforms we've tried this on,

423
00:20:55,617 --> 00:20:57,619
synchronization hasn't actually been a problem.

424
00:21:00,972 --> 00:21:03,213
It's important that your tend-to-threadpool function

425
00:21:03,374 --> 00:21:05,695
be as efficient as possible, because it's

426
00:21:05,756 --> 00:21:06,937
going to be called a lot.

427
00:21:07,577 --> 00:21:10,319
Make sure you don't grab any locks or semaphores,

428
00:21:10,760 --> 00:21:13,762
or atomically change any global values before it's

429
00:21:13,802 --> 00:21:15,243
time for you to go to sleep.

430
00:21:15,864 --> 00:21:17,885
And make sure that any per-thread state

431
00:21:18,146 --> 00:21:20,908
is on a different cache line to avoid false sharing

432
00:21:20,948 --> 00:21:22,109
between the async threads.

433
00:21:27,653 --> 00:21:30,416
You should also make it not suck to call tend-to-threadpool.

434
00:21:31,381 --> 00:21:35,638
In particular, don't force your programmers to pass a thread pool pointer everywhere.

435
00:21:37,216 --> 00:21:40,937
You could use a singleton, but even better is to use a thread local variable

436
00:21:41,257 --> 00:21:43,658
in order to store a pointer to the per-thread state.

437
00:21:44,698 --> 00:21:47,659
That gives you a useful side benefit.

438
00:21:48,079 --> 00:21:50,760
You can make tendToThreadPool behave like a NOP

439
00:21:50,980 --> 00:21:54,261
if it's called outside any async thread pooling.

440
00:21:54,861 --> 00:21:58,302
That reduces the need to make two versions of your algorithms

441
00:21:58,662 --> 00:22:01,083
and makes it easier to retrofit asynchrony

442
00:22:01,463 --> 00:22:04,043
onto existing code that you're already using elsewhere.

443
00:22:07,074 --> 00:22:11,738
The trickiest part of all this stuff is calling tend to thread pool at the right frequency.

444
00:22:12,338 --> 00:22:16,461
If you call it too seldom, you'll get time spikes, and if you call it too often, you'll

445
00:22:16,481 --> 00:22:19,083
waste a lot of CPU time on RDTSC.

446
00:22:20,790 --> 00:22:24,494
Now, we've spent a lot of time getting timings just right

447
00:22:24,554 --> 00:22:27,437
because we've set a target of 100 microseconds

448
00:22:27,738 --> 00:22:28,899
for our maximum variance.

449
00:22:29,279 --> 00:22:33,063
That is, a thread never stays awake more than 100 microseconds

450
00:22:33,163 --> 00:22:35,086
longer than it was supposed to, which

451
00:22:35,146 --> 00:22:37,728
means it has to call this at least once every 100

452
00:22:37,888 --> 00:22:38,189
microseconds.

453
00:22:39,751 --> 00:22:43,656
For that level of variance, you need to retrofit a lot of code for async use.

454
00:22:43,996 --> 00:22:46,039
For instance, we actually have a special version

455
00:22:46,860 --> 00:22:48,382
for asynchrony of memcpy,

456
00:22:48,763 --> 00:22:52,188
just because copying a one megabyte array takes about half a millisecond.

457
00:22:53,673 --> 00:22:56,616
For smaller data sizes, for higher acceptable variance,

458
00:22:56,917 --> 00:22:58,478
you don't have to go this far.

459
00:22:59,219 --> 00:23:02,622
But you should have some way of finding places in your code

460
00:23:02,902 --> 00:23:04,664
where you're calling tend to thread pool

461
00:23:04,764 --> 00:23:06,546
either too seldom or too often.

462
00:23:07,026 --> 00:23:08,688
A profiler can really help with this,

463
00:23:08,768 --> 00:23:11,791
particularly if you examine where tend to thread pool

464
00:23:11,871 --> 00:23:13,252
is most being called from.

465
00:23:14,774 --> 00:23:17,135
And you can also manually instrument your code.

466
00:23:17,616 --> 00:23:19,938
If your tend to thread pool function,

467
00:23:19,998 --> 00:23:23,020
if you pass in a unique ID from each place,

468
00:23:23,360 --> 00:23:25,682
then you can record when that happened

469
00:23:26,083 --> 00:23:30,106
and look for unusually high or low intervals

470
00:23:30,166 --> 00:23:30,966
between the calls.

471
00:23:34,622 --> 00:23:38,724
A major pain point with asynchronous computation in games is working with world state.

472
00:23:39,244 --> 00:23:43,826
Actually in this system, threading notwithstanding, since everything is happening in the foreground

473
00:23:43,906 --> 00:23:48,748
during a dedicated AI tick, it's actually a lot easier than with general multithreading.

474
00:23:49,149 --> 00:23:51,450
But there are still a couple of things you need to keep in mind.

475
00:23:52,250 --> 00:23:57,673
First of course is don't hold external locks when you're checking the sleep timer.

476
00:23:58,810 --> 00:24:02,294
Second is, if you read world state and then call tend to thread pool

477
00:24:02,374 --> 00:24:03,855
and then read more world state,

478
00:24:04,236 --> 00:24:06,518
consider whether the first batch of world state

479
00:24:06,838 --> 00:24:08,120
might now have changed.

480
00:24:09,021 --> 00:24:11,443
For things like influence maps, maybe that doesn't matter.

481
00:24:11,824 --> 00:24:13,986
For things like planning problems, it probably does.

482
00:24:14,587 --> 00:24:16,929
You can also have your tend to thread pool function return

483
00:24:17,009 --> 00:24:18,070
whether it slept or not,

484
00:24:18,571 --> 00:24:20,973
and roll back and recheck things if it did.

485
00:24:23,171 --> 00:24:27,534
So far I've talked about async threads as either running in the foreground or suspended,

486
00:24:27,774 --> 00:24:33,117
but you can also set it up so async threads switch between foreground and background mode

487
00:24:33,457 --> 00:24:36,199
by adjusting the thread priorities during the tick.

488
00:24:36,779 --> 00:24:41,722
That gives you a guaranteed time slice during the AI step, but it still allows you to soak

489
00:24:41,822 --> 00:24:45,664
up those idle CPU resources during the rest of the frame.

490
00:24:46,585 --> 00:24:51,052
One important point about this, the main thread should increase async priorities

491
00:24:51,433 --> 00:24:54,117
and async threads should decrease their own priorities.

492
00:24:54,417 --> 00:24:59,104
Make sure to do it that way, otherwise things will happen at the wrong time and threads will get stuck.

493
00:25:00,385 --> 00:25:03,547
There's a big drawback to the background mode, though.

494
00:25:03,907 --> 00:25:05,808
When async threads are in the foreground,

495
00:25:05,948 --> 00:25:08,489
they have full access to the external world data

496
00:25:08,609 --> 00:25:10,250
as long as they don't try to sleep

497
00:25:10,290 --> 00:25:11,551
in the middle of a transaction.

498
00:25:11,871 --> 00:25:13,052
But if they're in the background,

499
00:25:13,472 --> 00:25:16,054
then it's just like any other multi-threaded system.

500
00:25:16,074 --> 00:25:18,135
You have to worry about concurrent access.

501
00:25:18,695 --> 00:25:21,357
So if your system needs to read a lot of world state,

502
00:25:21,857 --> 00:25:23,118
making it work in the background

503
00:25:23,258 --> 00:25:24,639
may be more trouble than it's worth.

504
00:25:25,642 --> 00:25:29,807
I should also mention that allowing both background threads and suspended frames

505
00:25:30,167 --> 00:25:32,650
makes an implementation considerably more complex.

506
00:25:36,168 --> 00:25:39,349
One bit of future work we've been looking at is internal locks.

507
00:25:39,509 --> 00:25:42,611
And by internal, I mean locks that are shared by asynchronous threads

508
00:25:42,651 --> 00:25:44,692
but are not visible outside the thread pool.

509
00:25:45,272 --> 00:25:48,254
You see these pretty often when you've got multiple threads working

510
00:25:48,294 --> 00:25:50,035
in parallel to produce a set of output.

511
00:25:51,215 --> 00:25:52,876
Internal locks, of course, aren't a problem

512
00:25:53,076 --> 00:25:55,917
as long as you don't hold them across a tend to thread pool call.

513
00:25:56,258 --> 00:25:57,618
But it would be nice to have a lock.

514
00:25:58,299 --> 00:25:59,620
which had that flexibility.

515
00:26:00,081 --> 00:26:02,284
The idea is that that special kind of lock

516
00:26:02,765 --> 00:26:04,567
wouldn't interfere with 10-to-thread pool.

517
00:26:04,828 --> 00:26:06,730
And the way it would do that is,

518
00:26:07,031 --> 00:26:09,074
if a thread is waiting for an internal lock,

519
00:26:09,394 --> 00:26:12,118
it should be treated as effectively already suspended.

520
00:26:13,888 --> 00:26:17,789
That means it needs to be implemented on top of the existing per-thread semaphores

521
00:26:18,669 --> 00:26:22,190
and there's a lot of machinery for making sure that when a lock is released

522
00:26:22,530 --> 00:26:26,090
it goes to the right thread, depending on whether it's suspended for the right or wrong reasons.

523
00:26:26,531 --> 00:26:29,251
It's not going to be easy to implement, but I think

524
00:26:30,291 --> 00:26:33,332
a successful implementation of this would open up a lot of flexibility

525
00:26:33,712 --> 00:26:36,773
in what sorts of processing people could do in an async thread pool.

526
00:26:40,139 --> 00:26:44,062
I know that asynchronous processing isn't the most natural way to write code.

527
00:26:44,502 --> 00:26:48,685
Instinctively, we want to be able to ask a question and get the answer back immediately.

528
00:26:49,906 --> 00:26:54,449
But asynchronous AI opens up possibilities which nothing else opens up for us.

529
00:26:54,849 --> 00:26:56,990
Computers have basically stopped getting faster.

530
00:26:57,551 --> 00:27:01,654
If you can't afford a technique today, you won't be able to afford it in five years either.

531
00:27:02,629 --> 00:27:03,770
unless you change the rules.

532
00:27:04,550 --> 00:27:06,171
And asynchrony changes the rules.

533
00:27:06,552 --> 00:27:09,974
It uses game AI's natural tolerance for latency

534
00:27:10,394 --> 00:27:13,216
to open up more processing power than we've ever had before.

535
00:27:14,197 --> 00:27:15,838
I swear, asynchrony is important.

536
00:27:15,998 --> 00:27:17,779
It's probably the most important thing

537
00:27:17,919 --> 00:27:19,020
I've ever talked about here.

538
00:27:19,460 --> 00:27:22,242
This is something you need to do if you want to move forward.

539
00:27:22,262 --> 00:27:24,003
Thank you.

540
00:27:24,023 --> 00:27:24,263
Thank you.

541
00:27:24,283 --> 00:27:28,706
Thank you.

542
00:27:28,726 --> 00:27:28,946
Thank you. 2

543
00:27:28,986 --> 00:27:29,166
Thank you.

544
00:27:29,186 --> 00:27:29,406
Thank you.

545
00:27:29,426 --> 00:27:29,606
Thank you.

546
00:27:29,627 --> 00:27:30,407
Got a couple minutes for questions.

547
00:27:37,739 --> 00:27:38,299
Hi.

548
00:27:38,639 --> 00:27:39,219
Thanks for the talk.

549
00:27:39,920 --> 00:27:45,142
I wanted to ask if you guys experimented into diving into, like, implementation with fibers.

550
00:27:47,043 --> 00:27:47,803
Implementation with what?

551
00:27:48,103 --> 00:27:48,523
Fibers.

552
00:27:49,484 --> 00:27:49,784
Yeah.

553
00:27:50,104 --> 00:27:51,245
I would love to use fibers.

554
00:27:51,325 --> 00:27:54,826
For that matter, I would love to use user mode scheduling and all sorts of things that

555
00:27:54,866 --> 00:27:56,007
are just perfect for this.

556
00:27:56,687 --> 00:27:58,988
We had to worry about cross-platform compatibility.

557
00:27:59,088 --> 00:28:03,350
I didn't want something that I'd have to reimplement and have different vagaries on each platform.

558
00:28:03,410 --> 00:28:06,872
So that meant, you know, doing it the vanilla way with locks and such like.

559
00:28:07,349 --> 00:28:08,109
Okay, great, thank you.

560
00:28:12,191 --> 00:28:12,831
Thanks for the talk.

561
00:28:13,352 --> 00:28:16,033
So given how easy it is to call tend to thread pool

562
00:28:16,413 --> 00:28:18,434
and how widespread the calls are,

563
00:28:18,914 --> 00:28:21,315
it seems like it's very easy to actually call it

564
00:28:21,375 --> 00:28:23,416
from a scope that's holding a lock.

565
00:28:24,756 --> 00:28:26,477
Do you often run into these problems?

566
00:28:26,497 --> 00:28:28,018
Do you have some like instrumentation

567
00:28:28,058 --> 00:28:29,218
or some tools to deal with that?

568
00:28:29,597 --> 00:28:32,538
I mean, we ran into it a few times when we were first using this,

569
00:28:32,578 --> 00:28:34,419
when we were retrofitting existing code,

570
00:28:34,759 --> 00:28:37,460
but, like, we're already very careful with locks.

571
00:28:37,740 --> 00:28:39,481
Because when you're holding a lock,

572
00:28:40,081 --> 00:28:41,722
you're doing something unsafe, of course.

573
00:28:42,282 --> 00:28:44,363
And because you're doing something unsafe,

574
00:28:44,683 --> 00:28:46,604
you're restricting what can happen in the world.

575
00:28:47,044 --> 00:28:49,265
So our use of locks is already pretty tight.

576
00:28:50,225 --> 00:28:51,686
We did see a few uses of this,

577
00:28:51,846 --> 00:28:55,608
but I would say that none of them were very difficult to resolve,

578
00:28:55,868 --> 00:28:58,349
with the exception of the stuff happening entirely internally.

579
00:28:58,709 --> 00:29:01,370
It's a funny thing that it was the external locks which were harder to fix

580
00:29:01,891 --> 00:29:02,631
than the internal ones.

581
00:29:07,013 --> 00:29:09,815
Last question. So since when you do time slicing,

582
00:29:09,835 --> 00:29:12,116
you're essentially trading CPU time for memory.

583
00:29:13,156 --> 00:29:15,797
Do you guys handle anything with memory spikes essentially now

584
00:29:15,817 --> 00:29:18,479
that if different asynchronous tasks in the background

585
00:29:18,519 --> 00:29:21,680
are getting paused but consuming quite a bit of memory?

586
00:29:22,741 --> 00:29:25,062
So by trading CPU time for memory,

587
00:29:25,082 --> 00:29:27,883
you mean because there's all these threads which with their own working set.

588
00:29:29,320 --> 00:29:31,362
It hasn't tended to be a big problem.

589
00:29:31,442 --> 00:29:33,503
I mean, yes, you have a stack per thread.

590
00:29:34,604 --> 00:29:36,565
But if you've got more threads going,

591
00:29:37,045 --> 00:29:40,947
then they generally have smaller heap sizes,

592
00:29:40,987 --> 00:29:43,689
just because they're operating on a smaller area of the problem.

593
00:29:44,790 --> 00:29:50,173
It's definitely possible to limit the amount of memory

594
00:29:50,213 --> 00:29:51,794
being used per thread.

595
00:29:51,814 --> 00:29:54,295
But this just isn't something we've seen a need to do yet.

596
00:29:54,315 --> 00:29:54,876
Yeah.

597
00:29:59,982 --> 00:30:00,403
time.

598
00:30:00,423 --> 00:30:00,544
Thanks.

