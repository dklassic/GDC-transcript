1
00:00:06,343 --> 00:00:08,924
Hello, I'm Ed Tombush, and today I present

2
00:00:09,124 --> 00:00:11,505
A Recipe for Mixed Reality in Mario Kart Live.

3
00:00:12,505 --> 00:00:14,966
Now, I've been making video games for around 15 years now,

4
00:00:15,447 --> 00:00:17,327
and I currently work for Valen Studios,

5
00:00:17,448 --> 00:00:19,608
a newer startup located in upstate New York.

6
00:00:20,549 --> 00:00:22,770
Mario Kart Live was the first game released by Valen,

7
00:00:23,210 --> 00:00:24,550
and this was a game that I worked on

8
00:00:24,711 --> 00:00:26,791
along with a small team for three odd years.

9
00:00:28,192 --> 00:00:30,013
So what is Mario Kart Live Home Circuit?

10
00:00:31,573 --> 00:00:34,375
Well, first off, it's a game for the Nintendo Switch.

11
00:00:35,371 --> 00:00:36,672
And when you purchase the game,

12
00:00:36,872 --> 00:00:38,473
it comes with a remote controlled cart

13
00:00:38,693 --> 00:00:41,274
and you get to choose between Mario and Luigi.

14
00:00:42,675 --> 00:00:43,876
When you power up the cart,

15
00:00:44,096 --> 00:00:46,657
it connects to the game software running on the Switch.

16
00:00:47,678 --> 00:00:49,979
And the game allows the player to control the cart

17
00:00:50,199 --> 00:00:51,420
using the Switch controllers.

18
00:00:52,780 --> 00:00:54,501
The carts have an onboard camera

19
00:00:54,702 --> 00:00:57,303
where a live video feed is sent back to the Switch

20
00:00:57,483 --> 00:00:58,464
and shown on screen,

21
00:00:59,024 --> 00:01:01,565
giving the player a first person view of driving.

22
00:01:04,014 --> 00:01:05,634
The game also comes with four gates.

23
00:01:06,795 --> 00:01:08,155
And the gates are important as they

24
00:01:08,195 --> 00:01:09,835
help to define the play space.

25
00:01:10,836 --> 00:01:13,296
After a player has connected the cart for the first time,

26
00:01:13,876 --> 00:01:16,197
the game guides them to place the gates around the room

27
00:01:16,277 --> 00:01:17,157
to form a circuit.

28
00:01:18,097 --> 00:01:19,997
And this lets the player create any sort of track

29
00:01:20,037 --> 00:01:22,878
that they would like inside their house using the space

30
00:01:22,898 --> 00:01:23,778
that they have available.

31
00:01:26,339 --> 00:01:28,779
And even at its core, the driving experience

32
00:01:28,979 --> 00:01:29,760
is super fun.

33
00:01:30,500 --> 00:01:33,360
In fact, this is how we started experimenting in this space.

34
00:01:34,362 --> 00:01:38,045
In the beginning, we used drone parts like cameras and video transmitters

35
00:01:38,565 --> 00:01:40,848
and cobbled them together on some RC cars.

36
00:01:42,189 --> 00:01:45,632
And we found early on that it's really fun to drive around your house

37
00:01:46,052 --> 00:01:47,774
from the point of view of a tiny cart.

38
00:01:49,035 --> 00:01:53,079
And the sense of speed that you get from a toy cart zipping around your living room

39
00:01:53,559 --> 00:01:54,300
is pretty amazing.

40
00:02:01,278 --> 00:02:03,139
But of course, we're making a Mario Kart game,

41
00:02:03,599 --> 00:02:06,081
and we need more than just that first person driving.

42
00:02:06,981 --> 00:02:09,102
And this brings us to the mixed reality experience

43
00:02:09,122 --> 00:02:12,323
that we created, where we combine the digital content

44
00:02:12,403 --> 00:02:13,904
and interactions of the video game

45
00:02:14,464 --> 00:02:16,785
with the physical play space of the player's home,

46
00:02:17,186 --> 00:02:19,146
and hopefully seamlessly integrate the two.

47
00:02:20,487 --> 00:02:22,168
It's really the digital content

48
00:02:22,188 --> 00:02:23,688
that we place in the player's home

49
00:02:24,109 --> 00:02:26,410
that turns the experience into Mario Kart Live.

50
00:02:27,650 --> 00:02:30,451
Content like item boxes that you collect at the gates.

51
00:02:31,089 --> 00:02:33,410
the bananas that you drop on the track for your enemies,

52
00:02:34,250 --> 00:02:36,411
coins that you collect, the red shells that

53
00:02:36,451 --> 00:02:39,292
chase after other racers, the AI competitors.

54
00:02:40,332 --> 00:02:42,833
And it's extremely important that all of this content

55
00:02:43,033 --> 00:02:44,174
feels like it belongs.

56
00:02:44,794 --> 00:02:47,055
It needs to feel as if it's in the same room

57
00:02:47,115 --> 00:02:50,056
you're racing in, like it's part of the video feed

58
00:02:50,096 --> 00:02:50,996
that you see on screen.

59
00:02:52,236 --> 00:02:53,437
And here's the interesting bit.

60
00:02:54,577 --> 00:02:56,778
When you get it right, nobody notices.

61
00:02:57,278 --> 00:02:58,338
It feels like it belongs.

62
00:02:59,031 --> 00:03:01,072
But it's when you get it wrong that it stands out.

63
00:03:02,493 --> 00:03:03,774
And that's the point of this talk,

64
00:03:04,034 --> 00:03:05,915
to explain the tools and techniques we

65
00:03:06,035 --> 00:03:07,496
use to get everything just right,

66
00:03:08,416 --> 00:03:09,777
to make the user feel like they're

67
00:03:09,797 --> 00:03:11,978
playing in the world of Mario Kart inside their home.

68
00:03:14,100 --> 00:03:17,321
And unlike a lot of augmented or mixed reality experiences

69
00:03:17,361 --> 00:03:19,763
that you see, this is not a tech demo.

70
00:03:19,863 --> 00:03:22,764
We're making a game that needs to have replayability.

71
00:03:23,385 --> 00:03:25,726
And it needs to be robust to normal gameplay patterns.

72
00:03:26,367 --> 00:03:28,608
It needs to just work all of the time.

73
00:03:30,478 --> 00:03:34,119
As we started this project, we did look for existing tech that could help us,

74
00:03:34,979 --> 00:03:36,840
but nothing quite matched our use case.

75
00:03:38,140 --> 00:03:41,561
So we ended up rolling our own solutions to the problems that we needed to solve.

76
00:03:42,181 --> 00:03:43,641
And that had some advantages.

77
00:03:43,881 --> 00:03:46,062
We have control over the entire pipeline.

78
00:03:46,682 --> 00:03:47,942
There are no black boxes.

79
00:03:48,142 --> 00:03:49,362
We understand all the bits.

80
00:03:50,523 --> 00:03:53,623
And it allowed us to build technology tailored to gameplay.

81
00:03:54,683 --> 00:03:57,984
And conversely, gameplay tailored to what our technology is good at.

82
00:04:01,968 --> 00:04:05,351
Now I'm going to talk about the ingredients, all the tools

83
00:04:05,411 --> 00:04:07,673
and techniques that let us execute on the content

84
00:04:07,693 --> 00:04:08,333
that we just saw.

85
00:04:10,955 --> 00:04:13,277
And we start with a model of how we represent

86
00:04:13,317 --> 00:04:14,938
the physical world in our game.

87
00:04:16,079 --> 00:04:17,840
Now everything is measured exactly.

88
00:04:18,661 --> 00:04:21,563
We have a rig in game that matches the exact dimensions

89
00:04:21,603 --> 00:04:22,024
of the cart.

90
00:04:23,184 --> 00:04:25,446
We assume its pivot is dead center on the floor.

91
00:04:25,466 --> 00:04:28,468
It has a camera with an offset and a rotation

92
00:04:28,509 --> 00:04:29,149
from that pivot.

93
00:04:30,037 --> 00:04:33,319
Everything is modeled in the game in understandable real units.

94
00:04:34,220 --> 00:04:38,083
Distances are measured in centimeters, velocities centimeters per second,

95
00:04:38,763 --> 00:04:41,025
acceleration in centimeters per second squared.

96
00:04:42,106 --> 00:04:45,868
And this is going to be a reoccurring theme. Everything in game is modeled to

97
00:04:45,908 --> 00:04:48,430
match the real world as precisely as we can.

98
00:04:51,232 --> 00:04:55,656
And the same is true for the play space. I'm going to show you how we generate a

99
00:04:55,816 --> 00:04:57,437
map of the user's course.

100
00:04:58,394 --> 00:05:01,755
And everything in this map is modeled in real-world units.

101
00:05:03,015 --> 00:05:05,216
The gates positions are estimated in centimeters,

102
00:05:05,416 --> 00:05:06,657
the same as the carts position.

103
00:05:07,577 --> 00:05:09,357
And all the content that we put in the world

104
00:05:09,557 --> 00:05:11,778
is modeled at that same centimeter scale.

105
00:05:13,339 --> 00:05:15,979
So when all this comes together, and the physical world,

106
00:05:16,079 --> 00:05:19,060
our digital representation of the world, and the content

107
00:05:19,080 --> 00:05:22,441
that we put in it, when it all agrees on the scale and units,

108
00:05:23,042 --> 00:05:24,222
well, that's when things start to feel

109
00:05:24,262 --> 00:05:25,122
as if they fit together.

110
00:05:33,663 --> 00:05:36,045
Just as we strive to model the physical world in-game,

111
00:05:36,605 --> 00:05:38,647
we also need to model our in-game camera

112
00:05:38,867 --> 00:05:40,689
to match the physical camera on the cart.

113
00:05:42,330 --> 00:05:44,192
Now we have a video feed coming to the game

114
00:05:44,312 --> 00:05:47,114
and we wanna draw digital content over that video feed.

115
00:05:48,435 --> 00:05:50,957
We need that content to feel as if it's in the same space.

116
00:05:51,198 --> 00:05:53,700
Those item boxes need to feel like they belong

117
00:05:53,740 --> 00:05:54,420
in front of the gate.

118
00:05:55,661 --> 00:05:57,963
And in order to do this, our cameras need to match.

119
00:05:58,784 --> 00:06:00,826
We need to make the digital camera in-game

120
00:06:01,066 --> 00:06:02,747
match the physical camera on the cart.

121
00:06:05,601 --> 00:06:08,063
There's two main parts to matching the cameras.

122
00:06:09,584 --> 00:06:12,426
A physical lens will often add barrel distortion.

123
00:06:13,346 --> 00:06:16,308
And this can be observed as straight lines that appear to be curved,

124
00:06:16,889 --> 00:06:19,210
especially when you move towards the edges of the frame.

125
00:06:20,351 --> 00:06:23,393
Of course, this distortion isn't present in the game camera.

126
00:06:23,893 --> 00:06:25,795
So we need to remove this from the video image.

127
00:06:26,335 --> 00:06:27,776
And we call this undistortion.

128
00:06:29,253 --> 00:06:31,414
This will make the image rectilinear,

129
00:06:31,735 --> 00:06:33,135
where straight lines remain straight,

130
00:06:33,655 --> 00:06:36,197
and we can very closely align it with our game camera.

131
00:06:37,497 --> 00:06:38,538
But we need one other thing.

132
00:06:39,638 --> 00:06:42,219
We also need to calculate the field of view

133
00:06:42,319 --> 00:06:43,520
of the physical camera,

134
00:06:43,900 --> 00:06:45,741
so that we can apply the same field of view

135
00:06:46,001 --> 00:06:46,881
to the game camera.

136
00:06:48,762 --> 00:06:50,343
Now these screenshots show an image

137
00:06:50,403 --> 00:06:51,464
from the physical camera.

138
00:06:52,644 --> 00:06:54,685
The first image on the left is the raw view

139
00:06:54,925 --> 00:06:56,666
with the barrel distortion still present.

140
00:06:58,133 --> 00:07:00,475
The image on the right shows an undistorted view.

141
00:07:01,456 --> 00:07:03,577
Now, after we undistort, we're actually

142
00:07:03,637 --> 00:07:05,619
left with some extra pixel data that we end up

143
00:07:05,719 --> 00:07:06,640
cropping from the view.

144
00:07:07,200 --> 00:07:09,262
And that's kind of highlighted by that blue rectangle

145
00:07:09,302 --> 00:07:10,023
in the right image.

146
00:07:10,463 --> 00:07:12,745
We only end up drawing what's inside that blue box.

147
00:07:13,105 --> 00:07:16,068
But this example, it helps to show what the undistortion is

148
00:07:16,128 --> 00:07:16,708
actually doing.

149
00:07:18,590 --> 00:07:21,072
In order to remove distortion and calculate

150
00:07:21,112 --> 00:07:23,474
the field of view, we need to calibrate the camera.

151
00:07:25,912 --> 00:07:29,035
So we capture images, a lot of images, of a chessboard.

152
00:07:30,036 --> 00:07:32,618
And you need at least 20 images, probably more,

153
00:07:32,958 --> 00:07:36,441
with the chessboard near, far, different parts of the frame,

154
00:07:36,741 --> 00:07:40,745
different angles, before you start to get some good results.

155
00:07:41,926 --> 00:07:44,208
And we use OpenCV for the calibration.

156
00:07:46,569 --> 00:07:50,052
First, we use OpenCV to find the chessboard corners

157
00:07:50,273 --> 00:07:51,113
in each of the images.

158
00:07:52,535 --> 00:07:55,375
With a set of images in corresponding chessboard

159
00:07:55,415 --> 00:07:58,056
corners, we can use OpenCV to calculate

160
00:07:58,096 --> 00:08:00,316
the intrinsic parameters of the camera.

161
00:08:01,356 --> 00:08:04,397
Now, these are the parameters that numerically describe

162
00:08:04,437 --> 00:08:06,958
the camera, such as the distortion coefficients,

163
00:08:07,458 --> 00:08:10,598
projection matrix, and in turn, the field of view.

164
00:08:15,719 --> 00:08:17,320
Now, with the distortion coefficients,

165
00:08:17,540 --> 00:08:20,620
we can now undistort the video image from the camera.

166
00:08:21,629 --> 00:08:24,471
And in addition, we could set the field of view

167
00:08:24,691 --> 00:08:28,173
of the in-game camera to match that of the video image.

168
00:08:29,154 --> 00:08:31,535
And so here's an example of a scene switching

169
00:08:31,575 --> 00:08:34,998
between a raw distorted image here

170
00:08:37,239 --> 00:08:39,160
to an undistorted final image here.

171
00:08:39,180 --> 00:08:42,062
Again, this is the raw image.

172
00:08:44,143 --> 00:08:45,284
This is the undistorted image.

173
00:08:49,808 --> 00:08:53,051
There are a lot of resources online that describe camera calibration,

174
00:08:53,211 --> 00:08:55,592
and I've given some useful search terms to help find them.

175
00:08:57,153 --> 00:09:03,657
And it's worth noting that we often use Python tools that are Python for tools that are external

176
00:09:03,717 --> 00:09:09,581
to the game. And this is one great example. So we have a simple tool for extracting good

177
00:09:09,661 --> 00:09:14,744
chessboard images from a video. Another tool for finding chessboard corners and running the

178
00:09:14,764 --> 00:09:15,345
calibration.

179
00:09:16,212 --> 00:09:18,333
And we have scripts for undistorting still images

180
00:09:18,393 --> 00:09:20,694
that allow us to preview and evaluate the results.

181
00:09:21,914 --> 00:09:24,195
And it's very easy to get started in experimenting

182
00:09:24,235 --> 00:09:25,596
with these concepts in Python.

183
00:09:29,878 --> 00:09:31,299
The next thing to understand

184
00:09:31,499 --> 00:09:33,800
is what kind of data we exchange with the cart.

185
00:09:35,060 --> 00:09:36,621
And it's pretty straightforward overall.

186
00:09:37,642 --> 00:09:38,902
From the game to the cart,

187
00:09:38,982 --> 00:09:40,743
we send steering and throttle information.

188
00:09:41,704 --> 00:09:45,185
We sample and condition the user input from the controllers.

189
00:09:45,870 --> 00:09:48,211
and send that as commands to the cart for driving.

190
00:09:49,512 --> 00:09:52,453
Now, from the cart to the game, we get a stream of video data.

191
00:09:53,333 --> 00:09:56,595
And in addition, we get a stream of data from the IMU.

192
00:09:59,696 --> 00:10:00,556
What is the IMU?

193
00:10:01,096 --> 00:10:04,498
Well, the IMU stands for inertial measurement unit.

194
00:10:05,018 --> 00:10:07,479
And it's a sensor that can measure the movement of the car

195
00:10:07,559 --> 00:10:08,379
in 3D space.

196
00:10:09,860 --> 00:10:12,141
Our IMU is what is called a six-axis sensor.

197
00:10:14,980 --> 00:10:17,781
which is the combination of a three-axis accelerometer

198
00:10:18,221 --> 00:10:19,681
and a three-axis gyro.

199
00:10:21,362 --> 00:10:24,043
The accelerometer measures the linear acceleration

200
00:10:24,303 --> 00:10:25,903
on the three axes, x, y, and z.

201
00:10:27,464 --> 00:10:30,005
And the gyro measures the angular velocity,

202
00:10:30,165 --> 00:10:32,726
or rotation of the cart on those same axes.

203
00:10:34,286 --> 00:10:37,387
So it's important to note that our IMU samples are time-stamped

204
00:10:38,127 --> 00:10:39,908
and that the IMU data is sampled at 160 Hertz.

205
00:10:46,249 --> 00:10:49,451
And of course, we receive a stream of video data as well,

206
00:10:49,631 --> 00:10:51,932
which, as you might expect, consists

207
00:10:51,992 --> 00:10:53,093
of a series of images.

208
00:10:54,294 --> 00:10:56,695
And maybe the most interesting thing about the video frames

209
00:10:56,755 --> 00:10:58,976
is that they're timestamped, just like the IMU.

210
00:10:59,517 --> 00:11:02,799
And importantly, they're timestamped from the same clock

211
00:11:02,879 --> 00:11:03,879
as the IMU samples.

212
00:11:05,820 --> 00:11:09,062
Unlike the IMU data, which is sampled at 160 Hertz,

213
00:11:09,723 --> 00:11:11,784
the video data comes in at 30 Hertz or 30 FPS.

214
00:11:14,343 --> 00:11:16,404
Now, because of this difference in frequency,

215
00:11:16,824 --> 00:11:19,566
we end up receiving about five to six IMU samples

216
00:11:19,726 --> 00:11:20,566
per video frame.

217
00:11:22,507 --> 00:11:25,249
But with a timestamp video in the IMU stream,

218
00:11:25,989 --> 00:11:31,092
we can correlate the IMU samples that belong to a video frame.

219
00:11:32,673 --> 00:11:34,414
Now, for each frame that we process,

220
00:11:34,434 --> 00:11:37,715
we want to know what IMU samples occurred in that same time

221
00:11:37,755 --> 00:11:39,917
slice that the video frame captured.

222
00:11:41,217 --> 00:11:43,338
And this is basically our unit of work.

223
00:11:43,762 --> 00:11:45,123
in our mixed reality pipeline.

224
00:11:48,826 --> 00:11:51,708
Now, it's worth pointing out that sometimes this data stream

225
00:11:51,768 --> 00:11:53,929
can be imperfect, and we can lose data.

226
00:11:54,830 --> 00:11:57,452
After all, we're using Wi-Fi for communicating

227
00:11:57,492 --> 00:12:00,874
between the cart and the switch, and we can often drop data,

228
00:12:01,334 --> 00:12:03,476
especially as the cart moves far from the switch

229
00:12:03,596 --> 00:12:05,978
or even if the user is in a noisy network environment.

230
00:12:07,479 --> 00:12:09,460
Most commonly, we lose video frames.

231
00:12:10,586 --> 00:12:13,247
In that case, we still want to process the IMU samples that

232
00:12:13,287 --> 00:12:14,887
correlated to the missing frames.

233
00:12:16,828 --> 00:12:18,628
Now, the IMU samples are very important.

234
00:12:19,368 --> 00:12:20,729
We don't want to miss any of those.

235
00:12:21,509 --> 00:12:22,389
Now, can we miss some?

236
00:12:22,809 --> 00:12:23,470
Yes, we can.

237
00:12:23,690 --> 00:12:26,590
But because of their smaller size relative to the video

238
00:12:26,630 --> 00:12:28,091
data, it's less likely.

239
00:12:29,611 --> 00:12:31,292
And we also have mechanisms in place

240
00:12:31,332 --> 00:12:33,672
for reconstructing missing IMU data,

241
00:12:33,912 --> 00:12:35,133
should we lose a small amount.

242
00:12:36,753 --> 00:12:38,574
And we have larger fail-safes for when we miss

243
00:12:38,614 --> 00:12:39,934
significant portions of data.

244
00:12:41,725 --> 00:12:43,025
So what do we get from this data?

245
00:12:45,986 --> 00:12:47,866
Well, first, we'll talk about banner detection,

246
00:12:48,146 --> 00:12:49,426
our computer vision pass.

247
00:12:51,367 --> 00:12:53,988
Any time we have a gate banner in the video frame,

248
00:12:54,688 --> 00:12:55,748
we need to try to find it.

249
00:12:56,808 --> 00:12:57,728
Why do we want to find it?

250
00:12:57,888 --> 00:12:59,289
Well, there's two good reasons.

251
00:13:01,449 --> 00:13:03,950
The first is so we can place content around the gate.

252
00:13:04,810 --> 00:13:06,930
Now, the gate is a great place to put content.

253
00:13:07,431 --> 00:13:09,771
We could have high confidence in their location

254
00:13:09,851 --> 00:13:11,552
and they're fairly stable once we detect them.

255
00:13:13,065 --> 00:13:17,048
They also provide great information about the world around the cart,

256
00:13:17,709 --> 00:13:20,752
and they actually help us to build our digital model of the play space.

257
00:13:20,772 --> 00:13:27,398
Now I'm going to give a high-level overview of how we find and identify the banners.

258
00:13:28,419 --> 00:13:33,123
And our goal is to find the eight points that define the inner and outer rectangles of the banner.

259
00:13:35,825 --> 00:13:39,989
And to find those eight points, we're going to try to identify the black border around the marker.

260
00:13:44,453 --> 00:13:46,234
And we're going to start by trying to find

261
00:13:46,354 --> 00:13:47,655
the sides of the banner.

262
00:13:48,635 --> 00:13:50,816
So let's keep that in mind as we go through this process.

263
00:13:51,556 --> 00:13:53,217
The first few steps of the algorithm

264
00:13:53,317 --> 00:13:56,378
are going to be searching for the vertical-ish sides

265
00:13:56,458 --> 00:13:56,958
of the banner.

266
00:13:57,878 --> 00:13:58,819
So how do we find those?

267
00:14:01,900 --> 00:14:04,821
Well, we progressively scan the image from left to right,

268
00:14:04,961 --> 00:14:06,302
looking pixel by pixel.

269
00:14:07,482 --> 00:14:09,743
We're looking for high contrast transitions,

270
00:14:10,323 --> 00:14:11,824
and those transitions are represented

271
00:14:11,844 --> 00:14:13,064
by circles in this image here.

272
00:14:14,098 --> 00:14:15,419
And we don't scan every row.

273
00:14:15,539 --> 00:14:17,779
We scan at a fixed interval, so we don't

274
00:14:17,799 --> 00:14:18,839
have to look at every pixel.

275
00:14:20,360 --> 00:14:22,060
And in this result, the green circles

276
00:14:22,340 --> 00:14:24,420
represent a transition from light to dark,

277
00:14:25,181 --> 00:14:28,341
and the red circles represent a transition from dark to light.

278
00:14:32,122 --> 00:14:33,822
And once we have found these transitions,

279
00:14:34,062 --> 00:14:36,783
we walk the lines that define them.

280
00:14:38,083 --> 00:14:39,523
So when walking these transitions,

281
00:14:39,563 --> 00:14:42,764
we want the results to be mostly straight and mostly vertical.

282
00:14:43,528 --> 00:14:45,949
Remember, we're searching for the sides of the banner.

283
00:14:55,051 --> 00:14:56,471
Now we have a large set of straight lines.

284
00:14:57,071 --> 00:14:59,211
And we know that they define a transition in contrast

285
00:14:59,251 --> 00:15:01,432
from light to dark or dark to light.

286
00:15:02,652 --> 00:15:06,453
We search these lines and look for pairs, line pairs

287
00:15:06,513 --> 00:15:09,833
that could possibly form either the left or the right side

288
00:15:09,853 --> 00:15:10,313
of the banner.

289
00:15:14,615 --> 00:15:17,897
As we can see, that removes quite a few potential transitions.

290
00:15:18,978 --> 00:15:21,839
We're left with possible edges, pairs

291
00:15:22,399 --> 00:15:24,601
that could form the left or the right of a banner.

292
00:15:25,781 --> 00:15:28,263
And we know that a banner is going to have a left and a right edge.

293
00:15:29,203 --> 00:15:33,125
So we look for pairs of edges that could potentially form a banner.

294
00:15:37,688 --> 00:15:39,589
And these edge pairs form quads.

295
00:15:40,949 --> 00:15:43,411
We can see that there are only two possible quads remaining

296
00:15:43,431 --> 00:15:43,951
in this image.

297
00:15:44,948 --> 00:15:47,349
And we can refine these results a bit more.

298
00:15:50,490 --> 00:15:53,711
We could walk what should be the top and the bottom borders.

299
00:15:54,651 --> 00:15:58,213
We can confirm that there are straight, horizontal-ish

300
00:15:58,273 --> 00:16:01,414
transitions where we expect the top and the bottom

301
00:16:01,454 --> 00:16:02,154
of the banner to be.

302
00:16:03,214 --> 00:16:06,175
Now, you can see this process by the blue and red hash marks

303
00:16:06,395 --> 00:16:07,816
on the top and bottom of the quads.

304
00:16:09,256 --> 00:16:12,738
The red hash marks in that top quad indicate a miss.

305
00:16:13,920 --> 00:16:16,001
That top quad is probably not a banner.

306
00:16:16,201 --> 00:16:18,142
It couldn't find those top and bottom edges.

307
00:16:21,824 --> 00:16:24,004
And that leaves us with one result in this image.

308
00:16:29,487 --> 00:16:30,907
So we think we have found a banner.

309
00:16:31,668 --> 00:16:33,188
Now we have to try and identify it.

310
00:16:33,968 --> 00:16:35,889
We want to know what gate it is we're looking at

311
00:16:36,009 --> 00:16:38,390
and whether or not it's the front or the back of the gate.

312
00:16:39,391 --> 00:16:41,612
Well, we start by unwarping the found banner.

313
00:16:42,065 --> 00:16:44,526
That's where we take it from a skewed perspective shape

314
00:16:44,986 --> 00:16:46,467
to a flat rectangular shape.

315
00:16:48,868 --> 00:16:50,189
And then we threshold the image.

316
00:16:50,969 --> 00:16:53,030
We take all the pixels that are closer to white,

317
00:16:53,210 --> 00:16:55,592
bring them to full white, and all of the pixels

318
00:16:55,652 --> 00:16:57,553
closer to black and bring them to full black.

319
00:17:00,994 --> 00:17:02,875
And we could do a fuzzy comparison

320
00:17:03,115 --> 00:17:04,896
against a set of reference images.

321
00:17:05,337 --> 00:17:06,417
And these are shown on the right.

322
00:17:07,818 --> 00:17:10,359
This lets us determine what gate it is we are looking at.

323
00:17:17,874 --> 00:17:23,319
Now we know what banner we're looking at, we need to estimate what we call the pose of the banner,

324
00:17:23,819 --> 00:17:30,466
or the camera relative translation and rotation. We know the eight points in screen space that

325
00:17:30,526 --> 00:17:36,792
represent the banner. We also know, because we can measure it with a ruler, the physical

326
00:17:36,852 --> 00:17:42,678
dimensions of the banner and how the points in screen space relate to the physical cardboard gate.

327
00:17:44,884 --> 00:17:48,146
Now this association of 2D screen space points

328
00:17:48,847 --> 00:17:51,829
to 3D object points from the physical banner,

329
00:17:53,030 --> 00:17:57,113
we can use OpenCV's solve PNP or perspective in point

330
00:17:58,013 --> 00:18:02,336
to calculate the 3D translation and rotation of the banner

331
00:18:02,957 --> 00:18:04,258
with respect to the camera.

332
00:18:07,400 --> 00:18:09,021
So that's what we do for every frame

333
00:18:09,061 --> 00:18:09,962
that comes through the pipeline.

334
00:18:10,726 --> 00:18:18,649
We search the image for banners, and for each banner that's found, we identify it and calculate the camera-relative translation and rotation.

335
00:18:19,649 --> 00:18:22,951
And we call this CVResult, and every frame gets its own set.

336
00:18:34,115 --> 00:18:39,437
Now, while our system works well most of the time, it's worth showing off one of our extreme failure cases.

337
00:18:40,365 --> 00:18:41,566
the dreaded dog crate.

338
00:18:43,548 --> 00:18:46,751
Now the dog crate has a lot of contrasty transitions,

339
00:18:46,851 --> 00:18:49,513
so our algorithm finds a ton of edges.

340
00:18:51,475 --> 00:18:52,736
And in turn, some quads.

341
00:18:53,517 --> 00:18:53,937
Here's one.

342
00:18:55,638 --> 00:18:56,439
And a few more.

343
00:18:58,221 --> 00:18:59,021
And more.

344
00:18:59,042 --> 00:19:01,884
Okay, that's a lot of quads.

345
00:19:03,826 --> 00:19:05,928
And that's over 600 potential quads.

346
00:19:07,314 --> 00:19:11,337
And iterated over this many quads can have a noticeable impact on the processing time.

347
00:19:12,218 --> 00:19:15,760
And it was situations like this that forced us to come up with some scoring

348
00:19:16,101 --> 00:19:21,144
and early out techniques to avoid the processing overhead that these kind of situations can present.

349
00:19:25,527 --> 00:19:28,890
And while we use a custom implementation for finding the banners,

350
00:19:29,530 --> 00:19:31,232
there are open source solutions available.

351
00:19:31,909 --> 00:19:35,011
In fact, our first prototype, we used what are called

352
00:19:35,031 --> 00:19:36,032
Eruko markers.

353
00:19:36,532 --> 00:19:38,874
These are large pixelated markers in this image here.

354
00:19:40,815 --> 00:19:44,458
OpenCV has an implementation for detecting the Eruko markers

355
00:19:44,518 --> 00:19:46,200
in an image and returning their corners.

356
00:19:47,861 --> 00:19:49,903
And the pixels in the Eruko markers

357
00:19:50,463 --> 00:19:52,765
allow OpenCV to uniquely identify them.

358
00:19:54,146 --> 00:19:55,847
Once we know the corners of those markers,

359
00:19:55,947 --> 00:19:59,570
it's the same call to solve PNP to estimate their poses.

360
00:20:04,387 --> 00:20:06,847
Now, we know about the gates that are present in any frame.

361
00:20:07,168 --> 00:20:09,188
We know their ID, and we know their pose

362
00:20:09,248 --> 00:20:10,229
relative to the camera.

363
00:20:11,549 --> 00:20:14,090
And those banners, they provide really good information.

364
00:20:15,790 --> 00:20:17,911
But we want to interact with content at the gates

365
00:20:18,331 --> 00:20:21,372
even when we can't see it, like when we're driving past them.

366
00:20:22,473 --> 00:20:24,473
And we want to interact with other things in the world

367
00:20:24,674 --> 00:20:26,074
even when we don't see any gates.

368
00:20:27,294 --> 00:20:28,895
So in addition to gates, we really

369
00:20:29,135 --> 00:20:31,716
want an estimation of the world around the cart.

370
00:20:32,557 --> 00:20:34,759
and the cart's position within that world.

371
00:20:37,161 --> 00:20:38,062
We need SLAM.

372
00:20:39,964 --> 00:20:40,804
So what's SLAM?

373
00:20:42,286 --> 00:20:46,089
SLAM stands for Simultaneous Localization and Mapping.

374
00:20:46,830 --> 00:20:48,671
And it's a very common field of robotics.

375
00:20:49,832 --> 00:20:52,715
So I was looking for a nice and concise definition for SLAM,

376
00:20:52,855 --> 00:20:54,416
and Wikipedia had a pretty good one.

377
00:20:55,137 --> 00:20:58,480
They say that SLAM is the computational problem.

378
00:20:59,094 --> 00:21:03,697
of constructing or updating a map of an unknown environment

379
00:21:04,097 --> 00:21:06,118
while simultaneously keeping track

380
00:21:06,158 --> 00:21:08,359
of an agent's location within it.

381
00:21:12,902 --> 00:21:16,244
So basically, SLAM is building a map of the world

382
00:21:17,285 --> 00:21:19,446
and figuring out where you are in that world

383
00:21:20,066 --> 00:21:20,786
at the same time.

384
00:21:22,527 --> 00:21:23,548
So this is what we're gonna do.

385
00:21:24,068 --> 00:21:26,250
We're gonna build a map of a user's play space.

386
00:21:29,892 --> 00:21:31,694
And there are many approaches to the SLAM problem.

387
00:21:32,294 --> 00:21:34,396
This chart shows many SLAM techniques

388
00:21:34,476 --> 00:21:36,017
as they've evolved over the years.

389
00:21:37,319 --> 00:21:39,200
And if I had to place our method on this timeline,

390
00:21:39,260 --> 00:21:40,441
it would fall close to the 2007

391
00:21:40,822 --> 00:21:43,083
or even pre-history timeframe.

392
00:21:44,224 --> 00:21:46,406
Now, there are a few reasons to use older methods.

393
00:21:47,287 --> 00:21:50,390
Often newer methods are struggling to run real-time,

394
00:21:51,010 --> 00:21:53,933
let alone alongside a game that has its own processing needs.

395
00:21:55,454 --> 00:21:57,456
And while creating our own solution was a big lift,

396
00:21:58,474 --> 00:22:02,338
It allowed for a custom implementation that exactly fit our use case.

397
00:22:06,361 --> 00:22:09,864
And I couldn't talk about our SLAM solution without mentioning Klaus Brenner's

398
00:22:10,365 --> 00:22:16,450
amazing YouTube tutorial on SLAM. In the beginning, his tutorial gave us the baseline

399
00:22:16,490 --> 00:22:21,855
understanding to even go down this path. We actually started by following the tutorial,

400
00:22:22,075 --> 00:22:24,677
modifying it to match some of our differing assumptions.

401
00:22:25,546 --> 00:22:27,507
And while our solution is greatly evolved

402
00:22:27,567 --> 00:22:28,607
as development went on,

403
00:22:29,107 --> 00:22:31,108
there's still plenty of inspiration in there

404
00:22:31,148 --> 00:22:32,188
from Klaus's tutorial.

405
00:22:33,108 --> 00:22:35,569
It's a great introduction to the SLAM problem.

406
00:22:35,989 --> 00:22:38,190
He uses Python and provides code samples

407
00:22:38,230 --> 00:22:39,510
for the entire implementation.

408
00:22:40,170 --> 00:22:42,811
So if this is something you're interested in learning about,

409
00:22:42,831 --> 00:22:44,512
this is a great place to start.

410
00:22:47,393 --> 00:22:49,853
So we ended up with our own SLAM solution

411
00:22:49,933 --> 00:22:51,514
heavily inspired by Klaus.

412
00:22:52,687 --> 00:22:56,329
We don't really have a name for it, maybe valen-SLAM, b-SLAM.

413
00:22:57,589 --> 00:22:59,890
Now, if we wanted to get cute with the acronym,

414
00:22:59,990 --> 00:23:01,691
like many other implementations do,

415
00:23:01,771 --> 00:23:05,132
we might call it S-L-I-E-K-F-SLAM.

416
00:23:06,053 --> 00:23:07,013
Well, what would that stand for?

417
00:23:07,033 --> 00:23:11,895
It would stand for Sparse Landmark, Inertial,

418
00:23:12,395 --> 00:23:15,597
Extended Common Filter, Simultaneous Localization

419
00:23:15,657 --> 00:23:16,097
and Mapping.

420
00:23:17,497 --> 00:23:18,238
So let's break that down.

421
00:23:19,118 --> 00:23:19,938
Sparse Landmark.

422
00:23:21,125 --> 00:23:24,648
We use the four gates and only the four gates as our landmarks.

423
00:23:25,068 --> 00:23:28,491
And that's very sparse as far as SLAM implementations go.

424
00:23:29,271 --> 00:23:31,433
Most methods track thousands of world features,

425
00:23:31,493 --> 00:23:33,375
while we only consider the four gates.

426
00:23:34,896 --> 00:23:35,536
Inertial.

427
00:23:36,577 --> 00:23:40,500
We use the IMU to help make predictions about our motion.

428
00:23:41,581 --> 00:23:44,644
In fact, this is what allows us to use such sparse landmarks.

429
00:23:44,884 --> 00:23:47,506
Our motion estimation alone is very good.

430
00:23:48,277 --> 00:23:50,198
And it allows us to keep a consistent map,

431
00:23:50,359 --> 00:23:51,580
even with only the four gates.

432
00:23:53,261 --> 00:23:54,382
Extended common filter.

433
00:23:55,123 --> 00:23:56,564
Now at the very highest level,

434
00:23:56,725 --> 00:23:58,386
the extended common filter gives us

435
00:23:58,546 --> 00:24:01,909
a probabilistic estimation of the state of the world

436
00:24:02,230 --> 00:24:03,591
and the cart's position within it.

437
00:24:04,572 --> 00:24:07,234
And this is a huge topic, could span its own talk.

438
00:24:07,275 --> 00:24:08,956
So I'm not gonna go very deep into this,

439
00:24:09,336 --> 00:24:10,838
but I will give a high-level overview

440
00:24:10,878 --> 00:24:11,959
and provide some resources.

441
00:24:13,549 --> 00:24:16,310
And finally, SLAM, Simultaneous Localization and Mapping.

442
00:24:16,350 --> 00:24:17,110
We talked about that.

443
00:24:17,190 --> 00:24:19,471
It's building a map of the world and figuring out

444
00:24:19,511 --> 00:24:20,091
where we are in it.

445
00:24:24,792 --> 00:24:27,272
Now, the one thing I will talk about with a common filter

446
00:24:27,372 --> 00:24:28,393
is its update loop.

447
00:24:29,773 --> 00:24:32,954
A common filter contains a set of state variables

448
00:24:33,194 --> 00:24:35,054
that it's continuously estimating.

449
00:24:35,715 --> 00:24:39,315
In our case, things like the position, orientation,

450
00:24:39,836 --> 00:24:41,136
and velocities of the car.

451
00:24:42,648 --> 00:24:44,548
And anytime you're working with a common filter,

452
00:24:44,568 --> 00:24:48,930
you would expect to see it update in two phases, predict

453
00:24:50,010 --> 00:24:50,590
and measure.

454
00:24:52,191 --> 00:24:53,811
Now, during the prediction phase,

455
00:24:53,871 --> 00:24:56,712
the filter is updated given the current state

456
00:24:57,512 --> 00:24:58,713
and optionally some input.

457
00:25:00,333 --> 00:25:02,374
After the prediction is made, the filter

458
00:25:02,414 --> 00:25:05,835
has an updated probabilistic view of the state variables

459
00:25:05,875 --> 00:25:06,675
that it's estimating.

460
00:25:08,795 --> 00:25:10,436
The second phase is measurement.

461
00:25:11,649 --> 00:25:16,831
Any observations that can be made about the state can be applied or measured.

462
00:25:17,671 --> 00:25:21,272
Now these measurements to the state help the state variables become more accurate

463
00:25:21,732 --> 00:25:24,173
and hopefully converge towards reality.

464
00:25:26,894 --> 00:25:31,796
When we update our filter, we predict the new state using the IMU data stream.

465
00:25:33,817 --> 00:25:38,059
We make measurements against the state using some assumptions about the vehicle motion,

466
00:25:38,339 --> 00:25:40,280
the way that we expect the cart to move.

467
00:25:41,538 --> 00:25:43,740
And additionally, we make measurements against the state

468
00:25:43,840 --> 00:25:47,081
given our gate observations, the CV results

469
00:25:47,101 --> 00:25:47,861
that we talked about.

470
00:25:51,023 --> 00:25:52,484
So I'm going to show some example

471
00:25:52,564 --> 00:25:55,565
output from our SLAM system that will demonstrate how it works.

472
00:25:56,105 --> 00:25:58,086
And I'm going to be using real data from the game.

473
00:25:59,367 --> 00:26:02,668
As the game is running, we could save off the stream of IMU data

474
00:26:03,109 --> 00:26:05,550
along with the CV results for the banners that we found.

475
00:26:06,992 --> 00:26:08,573
Outside of the game, we have a set

476
00:26:08,593 --> 00:26:11,013
of tools that let us run our SLAM simulation

477
00:26:11,074 --> 00:26:12,074
in a Python environment.

478
00:26:12,714 --> 00:26:14,695
And we're going to be looking at the output for those tools.

479
00:26:16,215 --> 00:26:18,296
Now I've captured data from a real session

480
00:26:18,376 --> 00:26:20,977
while driving a five-lap circuit around my living room.

481
00:26:23,017 --> 00:26:24,698
This video shows the path that I followed

482
00:26:24,738 --> 00:26:25,758
while collecting the data.

483
00:26:26,058 --> 00:26:28,579
And you can see the mini-map shows the trajectory

484
00:26:28,619 --> 00:26:30,680
that we'll hopefully see in the simulated output.

485
00:26:34,813 --> 00:26:36,474
So let's start by thinking about how

486
00:26:36,514 --> 00:26:39,516
we predict the motion of the cart given the IMU data.

487
00:26:41,117 --> 00:26:43,799
Naively, we can observe our stream of IMU samples

488
00:26:44,540 --> 00:26:46,581
and integrate our accelerometer data

489
00:26:46,921 --> 00:26:49,663
and hope for a good estimation of our position over time.

490
00:26:51,305 --> 00:26:53,586
The accelerometer has an inherent bias.

491
00:26:54,527 --> 00:26:57,709
And a bias is a consistent offset in the output

492
00:26:58,129 --> 00:26:59,290
from the true value.

493
00:27:01,093 --> 00:27:03,494
If we knew the bias, we could subtract it

494
00:27:03,554 --> 00:27:05,435
from our IMU readings to calculate

495
00:27:05,475 --> 00:27:07,576
the actual acceleration along an axis.

496
00:27:08,796 --> 00:27:10,957
With standard motion equations, we

497
00:27:10,997 --> 00:27:14,358
could estimate the velocity and, in turn, the position

498
00:27:14,558 --> 00:27:15,519
of the car over time.

499
00:27:17,519 --> 00:27:19,980
There's some pretty significant problems that begin to show up.

500
00:27:21,061 --> 00:27:23,001
Now, in general, data from an IMU

501
00:27:23,121 --> 00:27:24,622
is very noisy to begin with.

502
00:27:25,422 --> 00:27:27,183
And now we have that IMU strapped

503
00:27:27,203 --> 00:27:28,763
to a moving, vibrating vehicle.

504
00:27:30,169 --> 00:27:31,910
And as I mentioned, the data is biased.

505
00:27:32,451 --> 00:27:34,272
And I talked about how we might remove it.

506
00:27:34,772 --> 00:27:37,714
But in order to remove it, you need a good estimation

507
00:27:37,774 --> 00:27:40,196
of the bias, which may even change over time.

508
00:27:41,537 --> 00:27:43,718
And because we're integrating from acceleration

509
00:27:43,838 --> 00:27:47,501
to a velocity, and then from a velocity to a position,

510
00:27:48,782 --> 00:27:51,243
any error that is present grows exponentially.

511
00:27:51,864 --> 00:27:54,065
Integration alone will quickly drift away

512
00:27:54,125 --> 00:27:55,406
from the actual trajectory.

513
00:28:00,521 --> 00:28:03,302
We can observe this drift in our Python simulation.

514
00:28:04,663 --> 00:28:09,045
As I mentioned, this output represents a simulation

515
00:28:09,165 --> 00:28:12,447
driven by data from a five-lap circuit in my living room.

516
00:28:13,808 --> 00:28:15,989
And in this example, we only show the integration

517
00:28:16,029 --> 00:28:17,669
of the IMU data from our data set.

518
00:28:18,690 --> 00:28:20,351
Now, I must admit, I cheated a bit here.

519
00:28:20,371 --> 00:28:23,092
I actually had to fudge things for this example

520
00:28:23,272 --> 00:28:25,973
to get a trajectory that looked even close to something

521
00:28:25,994 --> 00:28:27,054
that was recognizable.

522
00:28:28,040 --> 00:28:34,021
Anything less and I had an estimated position that quickly drifted from the origin without any recognizable motion.

523
00:28:35,462 --> 00:28:38,703
And that's kind of the point. With IMU integration alone,

524
00:28:39,963 --> 00:28:44,644
the estimated position quickly diverges from the origin and the results aren't very useful.

525
00:28:45,325 --> 00:28:49,706
So integration alone just isn't enough. But we have some tricks up our sleeve.

526
00:28:50,766 --> 00:28:53,647
Now while the IMU can tell us when the cart is moving,

527
00:28:54,627 --> 00:28:56,808
it can also tell us when it's not moving.

528
00:28:57,876 --> 00:29:00,177
We can look at a sliding window of IMU data

529
00:29:00,317 --> 00:29:01,898
and determine when the car is at rest.

530
00:29:03,158 --> 00:29:05,319
Now, our SLAM simulation is continuously

531
00:29:05,439 --> 00:29:07,560
estimating the velocities on each axis.

532
00:29:08,721 --> 00:29:12,062
If we know we aren't moving, we can make measurements

533
00:29:12,142 --> 00:29:15,323
on the SLAM filter that both the linear velocities

534
00:29:15,503 --> 00:29:17,524
and angular velocities should be 0.

535
00:29:19,305 --> 00:29:21,586
We're basically asserting that the cart shouldn't

536
00:29:21,646 --> 00:29:24,467
be moving or rotating when we know it's at rest.

537
00:29:29,807 --> 00:29:31,667
We also know that the vehicle is a cart,

538
00:29:32,588 --> 00:29:34,448
and that a cart drives in a certain way.

539
00:29:35,809 --> 00:29:38,469
We can assume that the velocity on the up axis

540
00:29:38,650 --> 00:29:40,650
is going to be close to 0 most of the time.

541
00:29:42,151 --> 00:29:45,792
We can also assume that the lateral left-right axis

542
00:29:46,132 --> 00:29:47,813
is going to be 0 most of the time,

543
00:29:48,713 --> 00:29:50,613
definitely when moving in a straight line,

544
00:29:51,414 --> 00:29:54,395
and even when the cart is turning if it isn't sliding.

545
00:29:55,990 --> 00:29:57,471
Now, we can use these assumptions

546
00:29:57,571 --> 00:30:02,055
to measure that the velocity is 0 on the up and lateral axis,

547
00:30:02,895 --> 00:30:04,096
even when the car is moving.

548
00:30:05,377 --> 00:30:07,198
Now, these constraints are a bit more fragile,

549
00:30:07,358 --> 00:30:09,560
since they aren't always guaranteed to be true.

550
00:30:10,040 --> 00:30:12,382
We have to be careful about how competently we

551
00:30:12,422 --> 00:30:13,203
make these measurements.

552
00:30:14,724 --> 00:30:16,025
These measurements certainly won't

553
00:30:16,065 --> 00:30:17,906
be true if the car has been picked up,

554
00:30:18,246 --> 00:30:19,587
if it's not driving on the floor.

555
00:30:20,674 --> 00:30:22,855
Now, in order to combat this, we have a state machine

556
00:30:23,355 --> 00:30:25,976
that is evaluating if we think the cart is driving

557
00:30:26,557 --> 00:30:27,677
or if it's been picked up.

558
00:30:29,278 --> 00:30:31,739
Now, this is where the magic of common filters comes in.

559
00:30:32,919 --> 00:30:35,580
When we make these measurements that put some constraints

560
00:30:35,780 --> 00:30:38,841
on the velocities, the filter is actually

561
00:30:38,902 --> 00:30:42,403
converging on better estimates about its entire state.

562
00:30:43,523 --> 00:30:43,964
Like what?

563
00:30:44,664 --> 00:30:46,805
Well, like the estimated IMU biases,

564
00:30:47,685 --> 00:30:49,766
like the estimated position within the world.

565
00:30:50,787 --> 00:30:51,687
Let's see what that looks like.

566
00:30:54,769 --> 00:30:57,450
This example shows the same IMU integration

567
00:30:57,710 --> 00:31:00,032
on the same five-lap data set that we saw before.

568
00:31:01,032 --> 00:31:03,714
The only addition here are the previously mentioned

569
00:31:03,874 --> 00:31:05,695
measurements, the motion constraints

570
00:31:05,735 --> 00:31:06,695
that we just talked about.

571
00:31:07,936 --> 00:31:09,897
And this is a pretty significant improvement.

572
00:31:10,897 --> 00:31:12,838
Once again, there's no additional information

573
00:31:12,878 --> 00:31:15,900
provided here, just IMU data with measurements

574
00:31:15,940 --> 00:31:17,221
that constrain the velocities.

575
00:31:18,501 --> 00:31:19,742
But we have more data available.

576
00:31:21,432 --> 00:31:23,933
Now, if you recall, our computer vision pass

577
00:31:24,373 --> 00:31:26,695
provides us with information about the gates.

578
00:31:27,775 --> 00:31:30,257
We know about gates that we currently see by ID,

579
00:31:30,877 --> 00:31:34,279
and we know the camera relative translation and rotation

580
00:31:34,359 --> 00:31:34,779
of the gate.

581
00:31:36,120 --> 00:31:39,242
We can combine this data with our estimated cart position

582
00:31:39,322 --> 00:31:42,124
to calculate where a gate is positioned in world space.

583
00:31:43,284 --> 00:31:45,025
And we can make measurements on our filter

584
00:31:45,185 --> 00:31:46,246
about a gate's position.

585
00:31:47,307 --> 00:31:48,267
So let's see what that looks like.

586
00:31:56,028 --> 00:31:58,050
Now, once again, this is the same data as before,

587
00:31:58,510 --> 00:31:59,930
five laps around my living room.

588
00:32:00,331 --> 00:32:02,392
But this time, we make measurements about the gates

589
00:32:02,432 --> 00:32:03,032
when we see them.

590
00:32:04,253 --> 00:32:06,374
Imagine we see a gate on a particular frame.

591
00:32:07,234 --> 00:32:08,575
If we haven't seen the gate before,

592
00:32:08,855 --> 00:32:11,096
we add its estimated position to the state.

593
00:32:12,377 --> 00:32:14,798
If we have seen the gate before, we

594
00:32:14,918 --> 00:32:18,880
use its prior estimate and current observation

595
00:32:19,180 --> 00:32:23,643
to refine both its position, the cart's position,

596
00:32:24,705 --> 00:32:26,766
and to further refine all the other variables

597
00:32:26,806 --> 00:32:27,506
we were estimating.

598
00:32:28,446 --> 00:32:32,027
And now we're doing SLAM, Simultaneous Localization

599
00:32:32,227 --> 00:32:32,707
and Mapping.

600
00:32:34,968 --> 00:32:36,348
So what do we get out of all this?

601
00:32:36,488 --> 00:32:39,489
Well, we get an estimation of the cart's position

602
00:32:39,689 --> 00:32:41,530
and orientation within the world.

603
00:32:42,690 --> 00:32:45,050
We get an estimation of the cart's velocities

604
00:32:45,310 --> 00:32:47,091
along all axes, x, y, and z.

605
00:32:48,271 --> 00:32:50,352
We get an estimation of the angular velocities,

606
00:32:50,452 --> 00:32:52,232
or turn rates, along all three axes.

607
00:32:53,833 --> 00:32:56,954
We also get an estimation of the accelerometer biases

608
00:32:57,034 --> 00:32:57,954
on each of those axes.

609
00:33:00,335 --> 00:33:03,256
Additionally, we get an estimate of the physical camera's

610
00:33:03,776 --> 00:33:06,577
rotation relative to the cart.

611
00:33:07,878 --> 00:33:10,519
Now, this is really interesting, because while the camera

612
00:33:10,579 --> 00:33:13,379
is fixed to the physical cart, there

613
00:33:13,420 --> 00:33:15,780
can be some variation to the camera angle,

614
00:33:16,260 --> 00:33:19,342
both because of manufacturing tolerances and the fact

615
00:33:19,362 --> 00:33:21,262
that it's in a mount that's not completely rigid.

616
00:33:22,617 --> 00:33:24,959
So this estimation of the camera's angle

617
00:33:25,639 --> 00:33:28,200
helps to improve the overall accuracy of the SLAM model.

618
00:33:29,561 --> 00:33:32,623
And we even use this estimate to adjust our in-game camera rig

619
00:33:33,063 --> 00:33:36,125
to better align the digital world to the physical world

620
00:33:36,165 --> 00:33:37,426
as we see it through the camera.

621
00:33:38,927 --> 00:33:41,948
And finally, we get an estimated positions of all

622
00:33:41,968 --> 00:33:43,089
of the gates in the world.

623
00:33:44,370 --> 00:33:47,011
So put a different way, we get an estimated view

624
00:33:47,572 --> 00:33:49,273
of a user-generated play space.

625
00:33:50,165 --> 00:33:53,205
and the location and orientation of the cart

626
00:33:53,585 --> 00:33:54,405
within that world.

627
00:33:57,066 --> 00:33:59,787
Now, SLAM is a large field with multiple solutions

628
00:33:59,887 --> 00:34:02,507
using different techniques and tons of resources online.

629
00:34:03,147 --> 00:34:04,627
So if you want to dive into some of this stuff,

630
00:34:04,667 --> 00:34:05,888
there's plenty of places to look.

631
00:34:06,808 --> 00:34:07,468
And as I mentioned,

632
00:34:07,568 --> 00:34:11,449
Klaus Brenner's SLAM lecture playlist on YouTube is amazing,

633
00:34:11,489 --> 00:34:13,029
and it heavily inspired our approach.

634
00:34:13,949 --> 00:34:16,190
Additionally, Udacity has a course titled

635
00:34:16,370 --> 00:34:19,870
Artificial Intelligence for Robots, for Robotics.

636
00:34:20,219 --> 00:34:21,520
which has some great resources,

637
00:34:21,861 --> 00:34:24,563
specifically a great introduction to the common filter.

638
00:34:27,666 --> 00:34:30,008
So now we're able to understand the world around us.

639
00:34:30,289 --> 00:34:31,630
We have a pretty good estimation

640
00:34:31,670 --> 00:34:33,391
of where the card is in that world,

641
00:34:34,352 --> 00:34:36,074
but there's one more piece to this puzzle,

642
00:34:36,494 --> 00:34:39,217
and that is information about the shape of the track

643
00:34:39,697 --> 00:34:41,239
that the user intends to race on.

644
00:34:42,260 --> 00:34:44,142
After all, we intend to host a race

645
00:34:44,362 --> 00:34:46,123
on a course that the player has constructed.

646
00:34:46,871 --> 00:34:49,672
So we need some information about the shape of the course

647
00:34:49,832 --> 00:34:52,572
and the path that it takes through the user's play space.

648
00:34:53,572 --> 00:34:55,093
And we call that track space.

649
00:34:57,053 --> 00:35:00,494
This video shows the user-facing track building experience.

650
00:35:01,814 --> 00:35:04,335
The in-game fantasy is that Lakitu

651
00:35:04,495 --> 00:35:06,635
pours paint on your tires and instructs

652
00:35:06,655 --> 00:35:10,816
you to drive a pace lap, leaving a trail of paint as you go.

653
00:35:11,837 --> 00:35:15,137
So basically, we're looking for one good continuous run

654
00:35:15,538 --> 00:35:16,578
around the whole track.

655
00:35:17,114 --> 00:35:19,635
that will help define the shape of the user's course.

656
00:35:33,520 --> 00:35:35,121
Now, as the user drives the PaceLab,

657
00:35:35,481 --> 00:35:37,461
we sample their position over time.

658
00:35:38,422 --> 00:35:41,403
The green dots here represent what we call breadcrumbs.

659
00:35:42,303 --> 00:35:44,784
We add the breadcrumbs based on how you're driving.

660
00:35:45,832 --> 00:35:48,494
Long straightaways only need a few breadcrumbs to define them.

661
00:35:49,274 --> 00:35:50,955
When turning, we add more breadcrumbs

662
00:35:50,995 --> 00:35:52,816
to help further define any curves.

663
00:35:57,118 --> 00:36:00,060
The breadcrumbs are defined as a sequence of 2D points.

664
00:36:03,662 --> 00:36:05,963
And we can traverse this sequence of breadcrumbs

665
00:36:06,183 --> 00:36:07,484
to get line segments.

666
00:36:11,866 --> 00:36:14,608
And from the line segments, we can measure their distances.

667
00:36:18,492 --> 00:36:20,314
And by summing all of the distances,

668
00:36:20,414 --> 00:36:22,616
we can determine the length of the entire track.

669
00:36:27,140 --> 00:36:29,843
So now we can think about a user-generated course

670
00:36:30,043 --> 00:36:32,426
in terms of a distance around the track.

671
00:36:34,248 --> 00:36:36,830
So let's imagine that this course is 800 centimeters long.

672
00:36:37,751 --> 00:36:40,094
As you traverse the course, you start at a distance of 0

673
00:36:40,694 --> 00:36:41,855
when you pass through the start gate.

674
00:36:43,184 --> 00:36:44,445
And your distance along the course

675
00:36:44,465 --> 00:36:46,506
will increase as you eventually reach

676
00:36:46,546 --> 00:36:50,128
a distance of 800 centimeters as you cross the start gate again

677
00:36:50,228 --> 00:36:50,948
to finish a lap.

678
00:36:53,849 --> 00:36:56,351
Now, all of the gates can be represented in terms

679
00:36:56,391 --> 00:36:57,491
of a track space distance.

680
00:37:01,713 --> 00:37:05,055
In fact, any arbitrary point along the course

681
00:37:05,375 --> 00:37:07,777
can be described in terms of a track distance.

682
00:37:12,952 --> 00:37:15,914
And we can easily convert any distance along the track

683
00:37:16,514 --> 00:37:17,655
to a world position.

684
00:37:18,356 --> 00:37:21,258
We have an API, convert distance to slam position,

685
00:37:21,318 --> 00:37:22,038
to do just that.

686
00:37:25,281 --> 00:37:27,863
And this track distance is actually two-dimensional.

687
00:37:28,703 --> 00:37:30,365
We can provide an offset parameter

688
00:37:30,425 --> 00:37:33,487
to get a position to the left or right of the center

689
00:37:33,507 --> 00:37:34,128
line of the track.

690
00:37:38,971 --> 00:37:40,132
And we can do the opposite.

691
00:37:40,997 --> 00:37:43,098
If we know the world position of an entity,

692
00:37:44,640 --> 00:37:46,401
we can calculate the track distance

693
00:37:46,541 --> 00:37:48,062
and offset of that entity.

694
00:37:49,283 --> 00:37:51,405
We'll often do this in the context of a race.

695
00:37:52,266 --> 00:37:54,468
We can compare the relative race positions

696
00:37:54,628 --> 00:37:58,231
of different competitors by comparing their track distances

697
00:37:58,491 --> 00:38:00,953
to determine who's in first, second, third, and so on.

698
00:38:02,414 --> 00:38:04,096
And getting this in was a big win for us.

699
00:38:04,436 --> 00:38:08,179
We went for quite some time with only considering the gate

700
00:38:08,239 --> 00:38:09,700
crosses for racer position.

701
00:38:10,339 --> 00:38:13,441
as in the person in the lead was the first person

702
00:38:13,481 --> 00:38:14,722
to pass a particular gate.

703
00:38:16,183 --> 00:38:17,984
But with this conversion to track space,

704
00:38:18,424 --> 00:38:22,567
we can continuously estimate the race position of all racers

705
00:38:23,267 --> 00:38:24,228
even between the gates.

706
00:38:27,590 --> 00:38:29,511
We can think about the track in other ways as well.

707
00:38:30,532 --> 00:38:33,073
We can think about it in terms of a normalized distance,

708
00:38:33,294 --> 00:38:36,316
and we call this track progress, where

709
00:38:36,376 --> 00:38:38,857
0 is the beginning of the track and 1 is the end.

710
00:38:42,696 --> 00:38:45,277
And we can easily convert between track progress

711
00:38:46,017 --> 00:38:47,038
and our track distance.

712
00:38:54,581 --> 00:38:56,402
Additionally, we could think about the track

713
00:38:56,482 --> 00:38:58,163
in terms of gate progress.

714
00:38:59,263 --> 00:39:01,504
Now, gate progress lets us imagine

715
00:39:01,684 --> 00:39:02,525
that the whole numbers, 0, 1, 2, 3, 4,

716
00:39:05,586 --> 00:39:09,148
represent gate positions with normalized distances

717
00:39:09,348 --> 00:39:10,608
as the decimal between them.

718
00:39:11,721 --> 00:39:15,183
Now gate 1, the start gate, is represented as 0.0.

719
00:39:17,384 --> 00:39:18,084
Gate 2 as 1.0.

720
00:39:19,204 --> 00:39:23,907
Gate 3 as 2.0, and so on, until we return to the start gate

721
00:39:23,987 --> 00:39:25,788
again for a gate progress of 4.0.

722
00:39:31,330 --> 00:39:33,371
And we primarily use gate progress

723
00:39:33,451 --> 00:39:35,552
to think about the space in between the gates.

724
00:39:36,851 --> 00:39:39,312
For example, we can get a point on the course directly

725
00:39:39,352 --> 00:39:42,493
between gates 2 and 3 with a gate progress of 1.5.

726
00:39:42,533 --> 00:39:49,556
And like track progress, we can easily

727
00:39:49,596 --> 00:39:52,377
convert between gate progress and track distance.

728
00:39:59,360 --> 00:40:01,600
Now, up until now, we've only been

729
00:40:01,640 --> 00:40:03,481
dealing with our estimation of the world

730
00:40:03,541 --> 00:40:06,082
and how we rationalize a user-defined course.

731
00:40:06,916 --> 00:40:09,757
Well, now we're going to look at how we use these results for gameplay.

732
00:40:12,057 --> 00:40:15,238
Before we dive in, let's recap all the data that we have access to.

733
00:40:16,118 --> 00:40:20,600
We have video and IMU data streamed from the cart associated with timestamps.

734
00:40:22,040 --> 00:40:27,461
We have CV results that contain gates with ID and camera relative positional information

735
00:40:28,002 --> 00:40:30,562
for all the banners that are on screen at any frame.

736
00:40:31,874 --> 00:40:34,215
And we have SLAM results that contain an estimation

737
00:40:34,275 --> 00:40:36,695
about the cart's motion along with its whirl position,

738
00:40:37,135 --> 00:40:38,155
estimation of the gates,

739
00:40:38,235 --> 00:40:40,176
and additionally, the track space information.

740
00:40:42,496 --> 00:40:43,756
So how do we use all this?

741
00:40:47,277 --> 00:40:49,218
Well, first we move the cart.

742
00:40:50,858 --> 00:40:52,558
So in the game, as you might imagine,

743
00:40:53,058 --> 00:40:55,199
there is an entity that represents the player

744
00:40:55,339 --> 00:40:56,959
in their digital version of the cart.

745
00:40:58,482 --> 00:41:03,466
And the important thing to know is that we're actually moving that entity through the game world.

746
00:41:05,447 --> 00:41:10,512
We use the estimated velocities and turn rate from SLAM to update its position and orientation in-game.

747
00:41:12,093 --> 00:41:17,217
And you'll note we're not directly consuming SLAM's estimated world position or orientation here.

748
00:41:18,898 --> 00:41:23,442
And that's because the SLAM simulation and game simulation don't really run in lockstep.

749
00:41:24,572 --> 00:41:27,553
We might drop video frames, have late or delayed frames,

750
00:41:27,633 --> 00:41:29,273
or have other hitches in the video feed.

751
00:41:30,293 --> 00:41:31,994
Our game simulation, on the other hand,

752
00:41:32,134 --> 00:41:33,655
is continuously updating.

753
00:41:33,955 --> 00:41:35,735
And we want our in-game representation

754
00:41:35,755 --> 00:41:38,296
of the cart's motion to update smoothly and consistently.

755
00:41:39,897 --> 00:41:42,117
In addition, SLAM is continuously

756
00:41:42,257 --> 00:41:44,578
updating its estimate of the cart's position

757
00:41:45,258 --> 00:41:47,099
based on gate observations.

758
00:41:48,079 --> 00:41:51,280
And there can be cases where a measurement from a gate

759
00:41:51,360 --> 00:41:53,521
causes a large correction to that position.

760
00:41:54,422 --> 00:41:56,323
And we want to hide that from the game simulation.

761
00:41:58,584 --> 00:42:01,366
So we do consider SLAM's estimate of the cart's position

762
00:42:01,406 --> 00:42:02,086
and orientation.

763
00:42:03,427 --> 00:42:06,208
Whenever the game simulation receives updated SLAM

764
00:42:06,248 --> 00:42:09,109
information, we interpolate towards that estimated world

765
00:42:09,149 --> 00:42:10,150
position and rotation.

766
00:42:11,751 --> 00:42:14,872
So this two-step process allows for smooth updates

767
00:42:14,892 --> 00:42:18,234
to the game entity while still maintaining

768
00:42:18,274 --> 00:42:21,515
consistency between the game and SLAM's view of the world.

769
00:42:23,188 --> 00:42:27,911
As we'll see, this concept that the cart is moving through the game world is very important,

770
00:42:28,271 --> 00:42:31,273
and it's basically the backbone of our entire mixed reality system.

771
00:42:35,015 --> 00:42:37,977
So let's talk about the content that we put at the gates.

772
00:42:40,538 --> 00:42:43,340
We want our gate content to look as if it's part of the world.

773
00:42:44,281 --> 00:42:47,123
It needs to be locked and aligned with the video stream each frame.

774
00:42:47,683 --> 00:42:50,425
We don't want it to jitter, drift, or pop in and out of existence.

775
00:42:51,612 --> 00:42:54,154
And it needs to be robust to imperfect information.

776
00:42:54,674 --> 00:42:56,576
There are cases where we don't find the marker.

777
00:42:56,776 --> 00:42:58,978
Maybe it was occluded by a chair leg or a cat.

778
00:43:00,239 --> 00:43:01,740
And by design, the user is encouraged

779
00:43:01,760 --> 00:43:03,661
to drive through the gate and underneath the banner.

780
00:43:04,582 --> 00:43:05,803
While you're driving through the gate,

781
00:43:05,843 --> 00:43:07,845
we can no longer see the banner, but we still

782
00:43:07,885 --> 00:43:11,127
want that content that is being drawn inside the gate opening

783
00:43:11,267 --> 00:43:13,909
to remain stable and convincing in these situations.

784
00:43:15,250 --> 00:43:16,932
We can see an example in this video,

785
00:43:17,092 --> 00:43:19,554
interacting with the item boxes as we pass through the gates.

786
00:43:20,305 --> 00:43:21,626
even when there's no Vanna in the frame.

787
00:43:22,867 --> 00:43:24,168
So let's take a look at how this works.

788
00:43:26,689 --> 00:43:28,290
Remember that for every gate that's

789
00:43:28,330 --> 00:43:30,952
detected in a video frame, we have a CV result

790
00:43:30,992 --> 00:43:34,194
that represents gate, its ID, and its camera

791
00:43:34,234 --> 00:43:35,935
relative translation and rotation.

792
00:43:37,196 --> 00:43:39,517
We also know the world position of the cart.

793
00:43:40,798 --> 00:43:43,480
And from our cart rig, we know the relative offset

794
00:43:43,560 --> 00:43:45,401
and rotation of the camera on the cart.

795
00:43:48,997 --> 00:43:54,261
We can concatenate these transforms to arrive at a world position and orientation for the gate

796
00:43:54,761 --> 00:43:58,143
that perfectly matches the banner that we detect in the video frame.

797
00:43:59,904 --> 00:44:03,407
And we can place digital content at this location in our game simulation.

798
00:44:04,027 --> 00:44:08,110
It has a world space in position just like our in-game cart does.

799
00:44:11,998 --> 00:44:14,159
Now the next frame, we move the cart again.

800
00:44:14,419 --> 00:44:17,080
We update the position and rotation of the cart

801
00:44:17,160 --> 00:44:19,701
based on our SLAM estimation, just as we did before.

802
00:44:21,622 --> 00:44:24,263
And if we happen to see grade 3 again, great.

803
00:44:25,023 --> 00:44:27,404
We can use the CV results to update its position

804
00:44:27,484 --> 00:44:28,845
to match our new estimate.

805
00:44:29,865 --> 00:44:32,706
And in fact, we add a bit of interpolation to this update

806
00:44:33,187 --> 00:44:34,467
to keep things looking smooth.

807
00:44:34,967 --> 00:44:37,248
Sometimes the CV results can be a bit noisy,

808
00:44:37,708 --> 00:44:39,249
and the interpolation helps to hide some of that.

809
00:44:42,097 --> 00:44:44,398
But what if we didn't see gate 3 again this frame?

810
00:44:45,679 --> 00:44:46,359
Well, that's OK.

811
00:44:46,819 --> 00:44:48,400
The cart has moved in the world.

812
00:44:49,100 --> 00:44:52,162
The digital gate has moved relative to the cart.

813
00:44:53,563 --> 00:44:56,164
And because we're estimating the motion of the physical cart,

814
00:44:56,364 --> 00:44:58,846
this is hopefully very similar to how

815
00:44:58,866 --> 00:45:01,407
the gate has moved relative to the gate in the real world.

816
00:45:04,709 --> 00:45:06,010
And once again, here's the result.

817
00:45:20,220 --> 00:45:22,021
So next we're gonna talk about world items.

818
00:45:22,722 --> 00:45:24,382
Items that exist around the track,

819
00:45:24,623 --> 00:45:26,023
in the space between the gates.

820
00:45:28,325 --> 00:45:30,245
And there's plenty of content that appears

821
00:45:30,346 --> 00:45:31,926
simply through the course of playing the game.

822
00:45:32,367 --> 00:45:33,907
There are bananas thrown on the course,

823
00:45:34,027 --> 00:45:35,348
Bob-ombs dropped on the track,

824
00:45:35,908 --> 00:45:37,369
coins that you drop when you get hit.

825
00:45:38,210 --> 00:45:40,151
But let's focus on a thrown banana peel.

826
00:45:41,531 --> 00:45:43,292
How do we make it look as if it's sitting

827
00:45:43,432 --> 00:45:44,333
on the floor of your room?

828
00:45:45,493 --> 00:45:47,234
How do we make it appear as if it's stuck

829
00:45:47,294 --> 00:45:49,355
in one spot on the ground as you drive past it?

830
00:45:50,450 --> 00:45:52,772
How do we make it look as if it's in the same spot

831
00:45:52,832 --> 00:45:54,754
when you see it again the next lap?

832
00:45:56,296 --> 00:45:57,878
Well, it really took all of the tools

833
00:45:57,918 --> 00:45:59,600
that we've talked about to make this possible.

834
00:46:00,481 --> 00:46:01,962
So let's look at how it all comes together.

835
00:46:04,725 --> 00:46:06,887
Well, we know the cart's position in the whirl.

836
00:46:08,389 --> 00:46:10,211
When we throw a banana peel, we can

837
00:46:10,291 --> 00:46:13,094
calculate its whirl position as an offset from the cart.

838
00:46:21,075 --> 00:46:24,557
And of course, in game, the cart is moving through the world

839
00:46:24,777 --> 00:46:26,278
by updating its world position.

840
00:46:27,699 --> 00:46:29,300
And this world position is constantly

841
00:46:29,400 --> 00:46:31,882
updated to match SLAM's estimated world

842
00:46:31,922 --> 00:46:33,103
position in your house.

843
00:46:35,464 --> 00:46:38,606
And because our game camera matches our physical camera,

844
00:46:39,587 --> 00:46:42,069
the banana looks as if it's anchored in the world

845
00:46:42,209 --> 00:46:43,029
as we drive past it.

846
00:46:44,830 --> 00:46:47,312
And it appears to be in the same spot where we left it,

847
00:46:47,812 --> 00:46:49,473
when we see it again in the future.

848
00:46:55,675 --> 00:46:57,516
So next we'll look at tracks-based items,

849
00:46:57,696 --> 00:47:00,697
or content that we want spawned relative to the track.

850
00:47:02,558 --> 00:47:03,719
And typically these are items

851
00:47:03,739 --> 00:47:05,500
that we want to think about at design time.

852
00:47:06,260 --> 00:47:08,662
We need to be able to put things like hazards and coins

853
00:47:08,862 --> 00:47:10,543
around the course in a meaningful way,

854
00:47:11,063 --> 00:47:13,324
and depending on what kind of race the user's engaged with.

855
00:47:14,205 --> 00:47:14,885
But here's the catch.

856
00:47:15,345 --> 00:47:17,326
We don't know about the course, we didn't define it,

857
00:47:17,446 --> 00:47:19,147
the user did when they placed their gates.

858
00:47:20,148 --> 00:47:22,049
So we need a method to think about the course

859
00:47:22,089 --> 00:47:24,110
and how we're going to place objects and hazards

860
00:47:24,170 --> 00:47:25,150
along it at design time.

861
00:47:25,797 --> 00:47:27,538
absent any knowledge of what it might look like.

862
00:47:28,859 --> 00:47:31,060
And this video shows a great example with track coins.

863
00:47:31,461 --> 00:47:34,163
For certain races, we want to place a cluster of coins

864
00:47:34,323 --> 00:47:36,424
on the course between each of the gates.

865
00:47:39,727 --> 00:47:42,129
So let's focus on that cluster that we want to spawn

866
00:47:42,369 --> 00:47:43,790
between gates 2 and 3.

867
00:47:43,890 --> 00:47:47,753
Well, we can use our track space API.

868
00:47:49,634 --> 00:47:51,535
For a spawn position between gates 2 and 3,

869
00:47:52,841 --> 00:47:55,383
we first use our convert gate to distance function with 1.5

870
00:47:56,864 --> 00:47:58,145
as the gate progress parameter.

871
00:47:59,426 --> 00:48:01,367
That call to convert gate to distance

872
00:48:01,427 --> 00:48:04,189
will return the actual track distance in centimeters

873
00:48:04,549 --> 00:48:06,851
to that point on the track halfway between the gates.

874
00:48:08,832 --> 00:48:11,694
And we use that track distance to determine a world position

875
00:48:11,894 --> 00:48:14,436
using the convert distance to slam position call.

876
00:48:15,697 --> 00:48:16,177
And that's it.

877
00:48:16,578 --> 00:48:18,319
That's the bridge we needed to cross.

878
00:48:19,320 --> 00:48:22,182
Now our newly spawned coin cluster has a world position.

879
00:48:23,067 --> 00:48:25,728
And everything from the banana example applies here as well.

880
00:48:26,488 --> 00:48:28,068
We can drive the cart through the world,

881
00:48:28,528 --> 00:48:30,609
and the coins appear to be anchored within that world

882
00:48:30,789 --> 00:48:31,669
relative to the cart.

883
00:48:33,909 --> 00:48:36,030
Using Trackspace, we can think about the track

884
00:48:36,130 --> 00:48:38,010
in meaningful ways at design time.

885
00:48:38,670 --> 00:48:40,350
And we can place content anywhere we'd like

886
00:48:40,450 --> 00:48:41,871
on a user-generated course.

887
00:48:42,891 --> 00:48:45,611
Now, Dan Doptis, the game director on Mario Kart Live,

888
00:48:46,312 --> 00:48:47,772
did a talk on this as well titled,

889
00:48:48,292 --> 00:48:51,673
Mixed Reality Racing Fuses Deeper AR Experiences

890
00:48:51,733 --> 00:48:52,793
with Physical Gameplay.

891
00:48:53,679 --> 00:48:55,019
And he talks a lot about this, how

892
00:48:55,059 --> 00:48:59,142
we think about user-generated content or user-generated

893
00:48:59,182 --> 00:49:00,582
courses at design time.

894
00:49:06,466 --> 00:49:08,307
Finally, we'll talk about dynamic items,

895
00:49:08,747 --> 00:49:11,689
items that need to interact with the course itself.

896
00:49:13,750 --> 00:49:17,492
And two good examples of dynamic items are AI and red shells.

897
00:49:18,493 --> 00:49:21,134
Now, these things need to move and behave intelligently

898
00:49:21,354 --> 00:49:23,035
on the course that the user defines.

899
00:49:26,032 --> 00:49:27,653
We'll look at a red shell for this example.

900
00:49:28,734 --> 00:49:30,535
As a red shell traverses the course,

901
00:49:30,735 --> 00:49:33,217
it tends to operate primarily in track space.

902
00:49:33,957 --> 00:49:36,358
It holds a track space distance that represents

903
00:49:36,399 --> 00:49:37,499
a point along the course.

904
00:49:39,961 --> 00:49:42,342
And to move the red shell, we can increase that distance

905
00:49:42,422 --> 00:49:44,283
by some amount given the shell speed.

906
00:49:46,184 --> 00:49:49,046
Our track space API allows us to convert that distance

907
00:49:49,086 --> 00:49:51,748
to a slam position, which gives us a new world

908
00:49:51,768 --> 00:49:52,588
position of the shell.

909
00:49:58,304 --> 00:50:01,406
We can even have the shell or an AI racer move left or right

910
00:50:01,506 --> 00:50:03,808
on the course with our offset parameter.

911
00:50:10,753 --> 00:50:11,474
And that's really it.

912
00:50:12,195 --> 00:50:15,597
Using our TrackSpace API, we can make dynamic items,

913
00:50:15,737 --> 00:50:18,880
like AI racers and red shells, move intelligently

914
00:50:18,980 --> 00:50:20,661
on a user-defined course, a course

915
00:50:20,681 --> 00:50:21,882
that we had no prior knowledge of.

916
00:50:27,233 --> 00:50:30,455
The final thing we need to do is to get everything on screen.

917
00:50:33,457 --> 00:50:35,118
We have a lot of data that we have accumulated.

918
00:50:35,138 --> 00:50:39,900
The video frame and IMU data, our CB results,

919
00:50:40,280 --> 00:50:41,921
the SLAM results with track information,

920
00:50:41,961 --> 00:50:43,922
and of course, the output from our game simulation.

921
00:50:44,583 --> 00:50:45,623
And now we need to render it.

922
00:50:46,784 --> 00:50:49,805
Functionally, we're rendering the undistorted video image.

923
00:50:50,832 --> 00:50:52,793
And we're drawing all of the game content

924
00:50:52,973 --> 00:50:56,314
on top of that video image using a game camera that

925
00:50:56,454 --> 00:50:58,214
matches the physical camera.

926
00:51:01,375 --> 00:51:03,676
And one thing I haven't really talked about is latency.

927
00:51:04,016 --> 00:51:05,237
And that could be a talk of its own.

928
00:51:06,577 --> 00:51:09,278
We measure our latency in terms of the amount of time

929
00:51:09,318 --> 00:51:11,458
it takes to get a video frame that's

930
00:51:11,498 --> 00:51:14,840
captured on the cart under the screen of the Switch.

931
00:51:16,500 --> 00:51:18,841
And as you can imagine, the processing that we do

932
00:51:18,901 --> 00:51:19,441
takes time.

933
00:51:20,434 --> 00:51:22,915
from the computer vision algorithm to the SLAM update

934
00:51:23,255 --> 00:51:24,696
to the update of the game simulation.

935
00:51:26,976 --> 00:51:28,917
Often, we even get a new video frame

936
00:51:29,137 --> 00:51:31,298
before we're able to get the last frame on screen.

937
00:51:33,359 --> 00:51:34,779
You might be tempted, as we were,

938
00:51:34,919 --> 00:51:37,160
to try and render that frame as quickly as possible,

939
00:51:37,680 --> 00:51:40,341
maybe with the game simulation results from the prior image.

940
00:51:41,101 --> 00:51:43,182
And we've tried this many different ways,

941
00:51:43,302 --> 00:51:44,643
and we failed many times.

942
00:51:45,537 --> 00:51:52,818
The simple fact is that any discrepancy between the video frame and the results from the game simulation will be seen.

943
00:51:53,258 --> 00:51:55,559
Things will be out of sync and it will break the illusion.

944
00:51:58,380 --> 00:52:02,600
This is the really important bit. Everything needs to be rendered together as one unit.

945
00:52:03,861 --> 00:52:06,541
And yes, that means that there's latency built into this approach.

946
00:52:07,402 --> 00:52:12,763
And for that reason, we spend a lot of time on optimization and techniques for further reducing the latency in other ways.

947
00:52:15,818 --> 00:52:18,239
So that's it, an overview of our pipeline

948
00:52:18,639 --> 00:52:19,999
and a lot of the tools and techniques

949
00:52:20,039 --> 00:52:21,580
that went into making Mario Kart Live.

950
00:52:22,560 --> 00:52:23,880
I'll leave you with a closing thought,

951
00:52:24,120 --> 00:52:26,401
a thing I've said hundreds of times during development,

952
00:52:27,021 --> 00:52:29,262
and that's we need to lean into our strengths

953
00:52:29,662 --> 00:52:30,922
and avoid our weaknesses.

954
00:52:32,703 --> 00:52:34,463
Making a mixed reality game is hard,

955
00:52:35,164 --> 00:52:36,764
and there are a lot of things that we tried

956
00:52:36,924 --> 00:52:39,665
and content that just doesn't work well in mixed reality.

957
00:52:41,005 --> 00:52:42,506
And that was a big part of development,

958
00:52:42,646 --> 00:52:45,346
finding the things that work well and leaning into those.

959
00:52:46,638 --> 00:52:48,599
while avoiding the things that don't work well,

960
00:52:49,380 --> 00:52:51,561
content that emphasizes the flaws in the system.

961
00:52:55,383 --> 00:52:57,285
Be sure to check out Valen Studios online.

962
00:52:57,785 --> 00:52:59,586
You can learn more about what we are up to

963
00:52:59,666 --> 00:53:00,887
and explore our job listings.

964
00:53:01,627 --> 00:53:02,348
And that's it for now.

965
00:53:02,608 --> 00:53:03,188
Thanks for your time.

