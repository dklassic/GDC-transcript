1
00:00:12,188 --> 00:00:16,671
Hi, I'm Edgar Handy from Square Enix Japan.

2
00:00:16,671 --> 00:00:21,714
So today I would like to share you about our research on application of styles transfer

3
00:00:21,714 --> 00:00:24,216
for expressing character AI emotion.

4
00:00:25,573 --> 00:00:28,233
So for today's topic, for today's talk,

5
00:00:28,233 --> 00:00:30,874
we will divide it into four sections.

6
00:00:30,874 --> 00:00:33,475
In the beginning, I will talk about the overview

7
00:00:33,475 --> 00:00:36,295
about using painting to express AI emotion.

8
00:00:36,295 --> 00:00:39,816
Then we will jump into the rule-based emotional mechanics

9
00:00:39,816 --> 00:00:44,617
that ultimately control how the neural style transfer behave.

10
00:00:44,617 --> 00:00:47,637
Then finally, we will talk about the machine learning system

11
00:00:47,637 --> 00:00:48,898
and how it is being implemented.

12
00:00:48,898 --> 00:00:51,398
And this is will be the main beef of this,

13
00:00:51,398 --> 00:00:52,518
the main dish of this talk.

14
00:00:53,751 --> 00:00:56,734
So before jumping into the main section,

15
00:00:56,734 --> 00:00:58,915
I would like to give a little bit introduction.

16
00:00:58,915 --> 00:01:00,036
I'm a machine learning engineer

17
00:01:00,036 --> 00:01:02,459
working in AI division, Square Enix Japan.

18
00:01:02,459 --> 00:01:03,960
Before I joined Square Enix,

19
00:01:03,960 --> 00:01:06,482
I was involved in AI ML researchers

20
00:01:06,482 --> 00:01:08,604
on cognitive developmental science

21
00:01:08,604 --> 00:01:10,145
and reinforcement learning on games.

22
00:01:11,990 --> 00:01:14,772
So we just started a new division called AI Division

23
00:01:14,772 --> 00:01:15,693
back in January.

24
00:01:15,693 --> 00:01:19,756
And we are responsible for all researchers in Square Enix

25
00:01:19,756 --> 00:01:23,119
and within the scope of AI, such as machine learning,

26
00:01:23,119 --> 00:01:25,882
improving workflow for game development

27
00:01:25,882 --> 00:01:27,263
and then reaching game content.

28
00:01:27,263 --> 00:01:31,566
And today's presentation and this whole project

29
00:01:31,566 --> 00:01:33,708
would not have been possible without the help

30
00:01:33,708 --> 00:01:35,069
of all people that are listed here.

31
00:01:35,069 --> 00:01:36,651
So I would like to say thank you beforehand.

32
00:01:38,467 --> 00:01:41,428
And now let's jump into the overview.

33
00:01:41,428 --> 00:01:43,148
So first, I would like to summarize

34
00:01:43,148 --> 00:01:45,769
the whole presentation in under one video.

35
00:01:45,769 --> 00:01:47,750
This is a very short video.

36
00:01:47,750 --> 00:01:49,951
And here, I would like to introduce you

37
00:01:49,951 --> 00:01:53,172
to a character AI called Pinoc that you see in the middle.

38
00:01:53,172 --> 00:01:58,114
This is our experimental testbed for this experiment.

39
00:01:58,114 --> 00:02:01,195
Pinoc is a character AI equipped with emotional system

40
00:02:01,195 --> 00:02:03,136
that you can see on the panel on the left side.

41
00:02:03,816 --> 00:02:07,558
Pinot learned how to interact with his surroundings

42
00:02:07,558 --> 00:02:09,799
by interaction with the player.

43
00:02:09,799 --> 00:02:12,400
You as a player control the vary that you

44
00:02:12,400 --> 00:02:15,581
see near Pinot's legs to encourage Pinot making

45
00:02:15,581 --> 00:02:16,582
interaction with the world.

46
00:02:16,582 --> 00:02:17,682
And you can give feedback.

47
00:02:17,682 --> 00:02:19,083
And through these interactions, you

48
00:02:19,083 --> 00:02:21,104
will see the emotion and mood system

49
00:02:21,104 --> 00:02:23,365
fluctuates all over the place.

50
00:02:23,365 --> 00:02:25,886
And finally, Pinot will show you what

51
00:02:25,886 --> 00:02:29,247
it feels by painting its surroundings.

52
00:02:29,247 --> 00:02:31,508
And this will be done with neural cell transfer.

53
00:02:31,508 --> 00:02:33,529
So let's take a look at how this works.

54
00:02:34,782 --> 00:02:38,446
So you can see here, you can move around the fairy and P-Nom interacts with the world.

55
00:02:38,446 --> 00:02:44,713
You can give feedback through the fairies and this will affect the emotion and mood system.

56
00:02:44,713 --> 00:02:47,476
As you see, everything is glowing over there.

57
00:02:47,476 --> 00:02:50,960
P-Nom in the idle time can decide,

58
00:02:50,960 --> 00:02:52,502
Hey, I want to show you what I feel.

59
00:02:53,122 --> 00:02:56,703
But rather than giving you those weird panels,

60
00:02:56,703 --> 00:02:58,204
Pino will occasionally,

61
00:02:58,204 --> 00:03:00,905
okay, I would like to paint something that I see,

62
00:03:00,905 --> 00:03:02,345
we will put it on a canvas

63
00:03:02,345 --> 00:03:04,025
and we will start a machine learning system

64
00:03:04,025 --> 00:03:06,646
in the background to do style transfer

65
00:03:06,646 --> 00:03:07,807
and it will be the painting.

66
00:03:07,807 --> 00:03:09,547
And this is the painting of the rubber ball

67
00:03:09,547 --> 00:03:12,028
that you see at the, on the floor.

68
00:03:12,028 --> 00:03:14,348
So we will get into more detail about this

69
00:03:14,348 --> 00:03:16,369
and the full version of the video later on.

70
00:03:16,369 --> 00:03:17,649
So this is summarizes everything

71
00:03:17,649 --> 00:03:19,230
about what we are going to do.

72
00:03:19,230 --> 00:03:22,591
So now let's jump into the talk about paintings.

73
00:03:24,283 --> 00:03:28,666
So generally, when we want to express character AI emotion

74
00:03:28,666 --> 00:03:33,209
in games, it has been limited to this triad facial expression,

75
00:03:33,209 --> 00:03:35,730
vocal information, and body gesture.

76
00:03:35,730 --> 00:03:37,551
And we are trying, and in Square Enix,

77
00:03:37,551 --> 00:03:41,354
we are trying to explore a new way

78
00:03:41,354 --> 00:03:44,095
to let our AI express what they feel

79
00:03:44,095 --> 00:03:46,537
in many variations possible.

80
00:03:46,537 --> 00:03:49,479
And we see painting as one of the options.

81
00:03:50,304 --> 00:03:55,227
So paintings have been a natural medium for us humans to express what we feel,

82
00:03:55,227 --> 00:04:01,572
what we interpret, about our surroundings, and we would like to apply the same concept

83
00:04:01,572 --> 00:04:07,076
to the AI to give a new way for AI to interact with the player.

84
00:04:07,076 --> 00:04:13,100
And all this system is implemented using neural style transfer,

85
00:04:13,100 --> 00:04:15,322
which we will jump into the detail later on.

86
00:04:16,532 --> 00:04:19,894
So before we talk about the machine learning system,

87
00:04:19,894 --> 00:04:24,157
I will have to explain to you about the rule-based system

88
00:04:24,157 --> 00:04:27,699
of the AI called emotion mechanics.

89
00:04:27,699 --> 00:04:31,021
As I mentioned before, Pinot has an emotion

90
00:04:31,021 --> 00:04:33,863
that controls the style transfer.

91
00:04:33,863 --> 00:04:36,465
This emotion is defined inside this AI,

92
00:04:36,465 --> 00:04:39,968
is defined by emotions and mood, which

93
00:04:39,968 --> 00:04:42,009
is a short-term feeling and a long-term feeling.

94
00:04:42,679 --> 00:04:45,381
The talk in depth about this topic

95
00:04:45,381 --> 00:04:49,824
has been done by my colleague, Goh Chebweda, back in GDC 2019.

96
00:04:49,824 --> 00:04:51,905
If you have any interest in this,

97
00:04:51,905 --> 00:04:53,146
please refer to his talk.

98
00:04:53,146 --> 00:04:58,390
So we want to put everything together, but we need a model.

99
00:04:58,390 --> 00:05:02,392
First, we need a model for the emotion,

100
00:05:02,392 --> 00:05:04,634
and we apply a modified OCC model.

101
00:05:04,634 --> 00:05:08,796
As you can see, it's on the diagram on the right side,

102
00:05:08,796 --> 00:05:09,617
OCC model.

103
00:05:09,617 --> 00:05:11,358
This is a modified version of this.

104
00:05:11,979 --> 00:05:16,321
And what the AI feels is affected by three aspects.

105
00:05:16,321 --> 00:05:19,863
First is the consequence of an event

106
00:05:19,863 --> 00:05:24,206
that the AI observe, whether they are pleased or displeased

107
00:05:24,206 --> 00:05:27,527
towards that, whether the AI is approving or disapproving

108
00:05:27,527 --> 00:05:31,790
towards an action of the other agent or another AI,

109
00:05:31,790 --> 00:05:34,712
or whether the AI likes or dislikes an object.

110
00:05:34,932 --> 00:05:40,681
And all the emotion that is implemented in this game is within the red box that you see at the

111
00:05:40,681 --> 00:05:46,229
bottom of the diagram. Multiple of these emotions can be activated at the same time. We will take

112
00:05:46,229 --> 00:05:47,371
a look at this mechanics later.

113
00:05:49,347 --> 00:05:51,209
Now we have to model the mode system.

114
00:05:51,209 --> 00:05:54,451
For this, we also use a modified system

115
00:05:54,451 --> 00:05:57,493
called modified path model that you can see

116
00:05:57,493 --> 00:05:59,094
in the diagram in the middle.

117
00:05:59,094 --> 00:06:01,916
It consists of two axes, and we can divide

118
00:06:01,916 --> 00:06:04,958
this two-dimensional space into four quadrants.

119
00:06:04,958 --> 00:06:09,001
The first is exuberant, docile, afraid, and hostile,

120
00:06:09,001 --> 00:06:13,104
where exuberant corresponds to all positive axes,

121
00:06:13,104 --> 00:06:15,426
and afraid corresponds to all negative axes.

122
00:06:16,712 --> 00:06:22,178
Now we have the model for each individual system for emotion and mood.

123
00:06:22,178 --> 00:06:25,861
To get the final system, we have to put everything together,

124
00:06:25,861 --> 00:06:28,103
as you can see on the diagram on the right side.

125
00:06:28,103 --> 00:06:33,008
We put all the implemented emotions inside the two-dimensional mood space.

126
00:06:33,400 --> 00:06:35,942
And you can see a red dot in the middle.

127
00:06:35,942 --> 00:06:38,764
That is the current mode value of the AI.

128
00:06:38,764 --> 00:06:42,086
So the current mode value of the AI is dynamic.

129
00:06:42,086 --> 00:06:43,587
It changes over time.

130
00:06:43,587 --> 00:06:46,569
And it changes according to the activated emotion.

131
00:06:46,569 --> 00:06:50,952
For example, if the hope emotion at the top left corner

132
00:06:50,952 --> 00:06:54,434
of the space is activated, then the red dot

133
00:06:54,434 --> 00:06:56,435
will gradually move towards hope.

134
00:06:56,435 --> 00:06:58,697
And if multiple of them activated at the same time,

135
00:06:58,697 --> 00:07:01,398
then we will have to compute the resultant movement.

136
00:07:02,858 --> 00:07:07,040
So this mode value will be sent to the machine learning system

137
00:07:07,040 --> 00:07:09,582
later to define its final results.

138
00:07:09,582 --> 00:07:11,263
Now, we have the emotion system done.

139
00:07:11,263 --> 00:07:14,425
Let's take a look at the neural style transfer.

140
00:07:14,425 --> 00:07:16,666
Now, this is the machine learning topic

141
00:07:16,666 --> 00:07:19,047
which will be the main dish of today's talk.

142
00:07:19,047 --> 00:07:24,130
I think most people here already know what neural style transfer

143
00:07:24,130 --> 00:07:25,551
is because it's very popular lately.

144
00:07:25,551 --> 00:07:27,793
It's been going for years.

145
00:07:27,793 --> 00:07:30,714
But please let me give a short introduction to it

146
00:07:30,714 --> 00:07:31,875
just in case.

147
00:07:32,518 --> 00:07:35,139
Neural style transfer is a machine learning technique

148
00:07:35,139 --> 00:07:38,581
for you to do stylization on a given arbitrary content image.

149
00:07:38,581 --> 00:07:40,543
As you can see on the left side, it's my dog.

150
00:07:40,543 --> 00:07:42,123
You put them inside the neural network

151
00:07:42,123 --> 00:07:45,125
and you set the reference style that you want to use.

152
00:07:45,125 --> 00:07:47,026
For example, an abstract painting

153
00:07:47,026 --> 00:07:49,288
that you see at the bottom.

154
00:07:49,288 --> 00:07:51,709
And the result is on the right side.

155
00:07:53,355 --> 00:07:58,063
For our use cases, there are four types of implementations

156
00:07:58,063 --> 00:07:59,445
that we do, that we did.

157
00:07:59,445 --> 00:08:01,749
The first one is basic style transfer,

158
00:08:01,749 --> 00:08:04,273
which we only transfer one style.

159
00:08:04,273 --> 00:08:06,837
You already saw that in the previous slide.

160
00:08:07,422 --> 00:08:09,805
And then the next one is blending style transfer,

161
00:08:09,805 --> 00:08:13,489
mask style transfer, and color control style transfer.

162
00:08:13,489 --> 00:08:15,511
We will jump into it one by one.

163
00:08:15,511 --> 00:08:17,754
However, due to the limited amount of time

164
00:08:17,754 --> 00:08:20,738
that we have today, we have to skip the technical detail

165
00:08:20,738 --> 00:08:22,319
on the color control style transfer.

166
00:08:22,319 --> 00:08:24,042
But I will get into the overview just a little bit.

167
00:08:26,133 --> 00:08:28,194
So blending style transfer, as the name suggests,

168
00:08:28,194 --> 00:08:31,415
it's a technique for you to blend multiple styles,

169
00:08:31,415 --> 00:08:33,696
two or more, into one content image,

170
00:08:33,696 --> 00:08:35,437
as you can see in the gray box.

171
00:08:35,437 --> 00:08:37,458
For ease of comparison,

172
00:08:37,458 --> 00:08:41,520
I also put the basic style transfer inside the blue box,

173
00:08:41,520 --> 00:08:43,481
so you can see what the difference,

174
00:08:43,481 --> 00:08:46,082
what a difference when you do only one style transfer

175
00:08:46,082 --> 00:08:47,562
and a blending of the two styles.

176
00:08:47,562 --> 00:08:49,983
But, okay, the example is only two styles,

177
00:08:49,983 --> 00:08:51,144
but you can actually do more.

178
00:08:53,395 --> 00:08:57,297
For mass style transfer, it's similar to the blending style

179
00:08:57,297 --> 00:08:57,577
transfer.

180
00:08:57,577 --> 00:09:00,439
However, we use a soft mass to define,

181
00:09:00,439 --> 00:09:02,220
to localize two style regions.

182
00:09:02,220 --> 00:09:05,621
In each region, you can also do multiple blend of styles.

183
00:09:05,621 --> 00:09:07,963
For example, in our use case, we only

184
00:09:07,963 --> 00:09:10,784
have two regions, the bright region and the dark region.

185
00:09:10,784 --> 00:09:13,946
And you can see that the bright region corresponds

186
00:09:13,946 --> 00:09:14,866
to the blue style.

187
00:09:14,866 --> 00:09:17,367
And the background, the dark regions,

188
00:09:17,367 --> 00:09:21,329
corresponds to the blend of two styles.

189
00:09:21,329 --> 00:09:21,569
So.

190
00:09:22,485 --> 00:09:30,067
Finally, this is color control style transfer, which is the one that we will skip the detail for today.

191
00:09:30,067 --> 00:09:34,087
This is a technique where you do style transfer however you can control the degree

192
00:09:34,087 --> 00:09:41,409
between the content image or the style image color. As you can see at the bottom, 100%

193
00:09:41,409 --> 00:09:48,531
corresponds to basic style transfer, 0% corresponds to style transfer however you would like to

194
00:09:48,531 --> 00:09:49,891
you maintain the

195
00:09:51,512 --> 00:09:57,881
the content image color. But if you notice, you still retain the style image touch.

196
00:09:57,881 --> 00:10:03,008
So it looks like a painting of a dog. Also, you can also do blending with this.

197
00:10:03,008 --> 00:10:07,995
However, it will be the outside of today's topic.

198
00:10:10,228 --> 00:10:14,695
To implement the style transfer, we use an architecture

199
00:10:14,695 --> 00:10:17,399
based on the previous work called a learn representation

200
00:10:17,399 --> 00:10:18,019
for R6 style.

201
00:10:18,019 --> 00:10:21,044
This architecture is extremely simple.

202
00:10:21,044 --> 00:10:23,888
It consists of only a bunch of convolutions,

203
00:10:23,888 --> 00:10:25,570
upsampling, and residual blocks.

204
00:10:27,039 --> 00:10:29,581
To get into a little bit more detail inside,

205
00:10:29,581 --> 00:10:32,043
we can see feature extraction layer,

206
00:10:32,043 --> 00:10:34,105
downsampling layer of the feature maps.

207
00:10:34,105 --> 00:10:37,088
Then we do bottleneck computations

208
00:10:37,088 --> 00:10:38,069
with residual blocks.

209
00:10:38,069 --> 00:10:40,911
After that, finally, we upsample the feature maps back

210
00:10:40,911 --> 00:10:42,933
into the pixel space, and you get the final results

211
00:10:42,933 --> 00:10:44,154
of the style transfer.

212
00:10:44,154 --> 00:10:47,317
The stylization computation is mostly

213
00:10:47,317 --> 00:10:50,840
done inside the orange block called conditional instance

214
00:10:50,840 --> 00:10:52,941
normalization, or I will call it SIN.

215
00:10:54,337 --> 00:10:59,721
Zooming in the scene, we see two operations, normalization and f-in transformation of the

216
00:10:59,721 --> 00:11:07,187
given feature maps. And this is done individually for each channel. The f-in transform here is

217
00:11:07,187 --> 00:11:13,792
basically just a scaling and shifting operations with a learnable f-in parameters. And all these

218
00:11:13,792 --> 00:11:17,835
learnable f-in parameters are unique for each style, so we have to learn them one by one.

219
00:11:20,452 --> 00:11:22,833
Now in order for you to blend the styles,

220
00:11:22,833 --> 00:11:25,935
assume that we have two styles that corresponds

221
00:11:25,935 --> 00:11:31,078
to two affine parameters, gamma 1, beta 1, gamma 2, and beta 2.

222
00:11:31,078 --> 00:11:34,620
If you want to have 70%, 30% of the blend of the styles,

223
00:11:34,620 --> 00:11:38,482
then what we do is just compute the complex combination of them

224
00:11:38,482 --> 00:11:41,684
by weighting each.

225
00:11:43,334 --> 00:11:47,178
its parameters, and then sum them over.

226
00:11:47,178 --> 00:11:50,342
And you will get the new gamma and beta,

227
00:11:50,342 --> 00:11:55,067
which corresponds to the blend of the 70% and 30%

228
00:11:55,067 --> 00:11:55,367
of the styles.

229
00:11:55,367 --> 00:11:58,751
So in order for you to do the mask style transfer,

230
00:12:00,095 --> 00:12:07,683
we have to augment the SYN module into something called mask conditional instance normalization.

231
00:12:07,683 --> 00:12:13,529
Now this is almost exactly the same as before, however we take an additional input which is a soft mask

232
00:12:13,529 --> 00:12:17,493
to define the style localizations area.

233
00:12:17,493 --> 00:12:19,375
Zooming in to get into the detail.

234
00:12:20,736 --> 00:12:25,580
Mask conditional instance normalization consists again of two operations,

235
00:12:25,580 --> 00:12:29,064
normalize and FN transform, but this time with the mask inside there.

236
00:12:29,064 --> 00:12:33,388
We give the soft region mask, as you can see on the right diagram.

237
00:12:33,388 --> 00:12:38,432
What we do here first, we divide the feature maps on each channel into two regions.

238
00:12:39,207 --> 00:12:44,169
So we define the region 1 as all feature maps pixels that

239
00:12:44,169 --> 00:12:47,430
correspond to the mask value greater than 0.5,

240
00:12:47,430 --> 00:12:50,991
and the rest of them into the region 2.

241
00:12:50,991 --> 00:12:55,633
After that, we do normalization for each region.

242
00:12:55,633 --> 00:12:56,353
We do not mix them.

243
00:12:57,213 --> 00:13:00,337
And finally, we do a theme transformation

244
00:13:00,337 --> 00:13:04,081
with new parameters called mask shift and mask scale

245
00:13:04,081 --> 00:13:05,283
parameters.

246
00:13:05,283 --> 00:13:10,108
And I will have to remind you that because each region has

247
00:13:10,108 --> 00:13:12,551
its own style, we have a theme parameter that

248
00:13:12,551 --> 00:13:14,153
is unique for each region.

249
00:13:16,650 --> 00:13:18,914
Now, there's a little bit more detail.

250
00:13:18,914 --> 00:13:20,577
If you're an artist, you would like to,

251
00:13:20,577 --> 00:13:22,581
okay, I don't want to look at this.

252
00:13:22,581 --> 00:13:24,184
Feel free to go to sleep a little bit.

253
00:13:24,184 --> 00:13:25,607
And I will go a little bit

254
00:13:25,607 --> 00:13:28,513
into the more engineering aspect of this one.

255
00:13:29,335 --> 00:13:32,798
In order for we to do F-in transformation on each feature

256
00:13:32,798 --> 00:13:34,999
map because it's a 2D tensors, we

257
00:13:34,999 --> 00:13:39,242
can imagine that each parameter, gamma 1 and gamma 2,

258
00:13:39,242 --> 00:13:40,583
is also a tensor.

259
00:13:40,583 --> 00:13:45,186
Gamma 1 is an F-in parameter correspond to region 1.

260
00:13:45,186 --> 00:13:50,429
Gamma 2 is an F-in parameter correspond to the region 2.

261
00:13:50,429 --> 00:13:53,191
Now in order to make, in order to blend this

262
00:13:53,191 --> 00:13:55,952
into a localization, into a stylized localization,

263
00:13:56,671 --> 00:13:58,832
So it's a localized stylization.

264
00:13:58,832 --> 00:14:01,933
We first use the original mask to do

265
00:14:01,933 --> 00:14:07,135
element-wise multiplication into the tensor of the scale

266
00:14:07,135 --> 00:14:07,815
parameter.

267
00:14:07,815 --> 00:14:10,276
And then we create an inverted mask.

268
00:14:10,276 --> 00:14:12,737
And then again, we do element-wise multiplication.

269
00:14:13,325 --> 00:14:19,508
to the fn parameter of the second region and then we sum them up. The final result is the

270
00:14:19,508 --> 00:14:25,790
mask scale parameter that we can apply to the whole feature maps to get the mask stylization.

271
00:14:25,790 --> 00:14:31,692
So if you realize this is basically a complex combination as we did before for spiral blending,

272
00:14:31,692 --> 00:14:39,455
but this time we are doing a blending but for special blending. It is the style localization.

273
00:14:41,315 --> 00:14:44,316
And finally, we also do the same thing for shift parameters.

274
00:14:44,316 --> 00:14:46,677
Basically, we define again original mask

275
00:14:46,677 --> 00:14:49,478
and inverted mask, so element-wise multiplication.

276
00:14:49,478 --> 00:14:52,299
And we sum them together to get the final mask shift

277
00:14:52,299 --> 00:14:52,820
parameters.

278
00:14:55,523 --> 00:15:00,404
So we follow the same training setup as the previous paper

279
00:15:00,404 --> 00:15:04,045
to model, to train the model.

280
00:15:04,045 --> 00:15:06,805
So if you would like to know the detail how to train this model,

281
00:15:06,805 --> 00:15:08,726
please refer to the original paper

282
00:15:08,726 --> 00:15:11,546
because it will give you all the awesome stuff to do this.

283
00:15:11,546 --> 00:15:14,327
It's extremely simple.

284
00:15:14,327 --> 00:15:18,228
Finally, the model is optimizing FIGG-16 perceptual loss,

285
00:15:18,228 --> 00:15:21,848
which is a very common optimization

286
00:15:21,848 --> 00:15:23,368
strategy for style transfer.

287
00:15:23,486 --> 00:15:25,579
and we train all of this inside Cancer Flow.

288
00:15:28,180 --> 00:15:31,261
Now we already know the rule-based emotion module.

289
00:15:31,261 --> 00:15:33,482
We have the neural style transfer.

290
00:15:33,482 --> 00:15:35,243
If we combine all of this, you can still

291
00:15:35,243 --> 00:15:37,104
get the diagram in the middle.

292
00:15:37,104 --> 00:15:39,505
You have emotion module that outputs the mode value,

293
00:15:39,505 --> 00:15:41,386
where we can compute the style which,

294
00:15:41,386 --> 00:15:43,907
or what we call the style of impedimenters,

295
00:15:43,907 --> 00:15:47,249
to compute either the style localization

296
00:15:47,249 --> 00:15:48,649
or the blend of those multiple styles.

297
00:15:48,649 --> 00:15:52,291
And then we send all of those into the style transfer module,

298
00:15:52,291 --> 00:15:54,372
and you get the final results on the right side.

299
00:15:57,182 --> 00:16:03,967
Now, we have explained the motion module and the slice

300
00:16:03,967 --> 00:16:04,647
transfer module.

301
00:16:04,647 --> 00:16:05,488
We get everything.

302
00:16:05,488 --> 00:16:09,391
So we would like to take a look at it again at the video,

303
00:16:09,391 --> 00:16:11,813
the full version of the video of the previous video

304
00:16:11,813 --> 00:16:14,395
to look at the more detail on what's happening.

305
00:16:14,395 --> 00:16:18,618
As I said before, Pinot is trying

306
00:16:18,618 --> 00:16:21,080
to learn how to interact with the world.

307
00:16:21,080 --> 00:16:22,821
You as a fairy who control the fairy

308
00:16:22,821 --> 00:16:24,802
can encourage whatever it can do.

309
00:16:24,802 --> 00:16:27,044
So as you can see here, Pinot is eating the ball.

310
00:16:27,670 --> 00:16:31,492
and it is a bad thing to do. We can give a feedback to Finn Pino, that's a bad thing.

311
00:16:31,492 --> 00:16:35,714
This will in the background affect Pino's behavior towards that ball later,

312
00:16:35,714 --> 00:16:41,776
but that's out of the scope of today's topic. And according to what Pino is doing and the

313
00:16:41,776 --> 00:16:47,239
user interaction that you give through the fairy, it will affect what Pino feels.

314
00:16:47,239 --> 00:16:52,661
And you see the load system down there, it's fluctuating over time.

315
00:16:58,002 --> 00:17:04,485
Now, in the idle time, Kino would like to share you what it feels.

316
00:17:04,485 --> 00:17:09,568
Rather than showing you the panel that you saw before,

317
00:17:09,568 --> 00:17:12,189
Kino will show it by painting what it sees.

318
00:17:12,189 --> 00:17:15,151
For example, the very rubber ball that they just ate before.

319
00:17:15,925 --> 00:17:20,187
and we do and it is implemented using the style transfer that we saw before

320
00:17:20,187 --> 00:17:25,049
where the Pino's emotional system affecting the final results

321
00:17:25,049 --> 00:17:28,551
of the appropriate style being applied to the canvas.

322
00:17:31,715 --> 00:17:33,496
So just like any other experiments,

323
00:17:33,496 --> 00:17:36,377
there are something that went bad and went good

324
00:17:36,377 --> 00:17:37,858
for this research.

325
00:17:37,858 --> 00:17:41,639
However, there are a little bit of difficulty on this one

326
00:17:41,639 --> 00:17:44,400
because art is a subjective matter.

327
00:17:44,400 --> 00:17:47,301
It's really hard to say which style is actually

328
00:17:47,301 --> 00:17:48,602
being represented properly.

329
00:17:49,322 --> 00:17:55,965
inside our neural network. For example, for me, I think the top pictures are good and the bottom

330
00:17:55,965 --> 00:18:03,228
pictures are bad, but that's not really the case for other people. So in the end, we have to choose

331
00:18:03,228 --> 00:18:09,771
the appropriate style for each emotion, test it inside the neural network, see if it's bad, then

332
00:18:09,771 --> 00:18:14,853
we will throw the style. If it's good, then we will adopt the style. So it's really hard to find

333
00:18:16,174 --> 00:18:22,962
like a set of hyper parameters that can grant you the best representation for all the style

334
00:18:22,962 --> 00:18:30,232
that you want to use. Now we have to explain the machine learning system.

335
00:18:30,232 --> 00:18:34,898
We would like to see how it's actually being implemented inside the game.

336
00:18:36,644 --> 00:18:40,307
The game that you saw before is implemented inside UE4.

337
00:18:40,307 --> 00:18:44,811
And since it's UE4, we have to implement the style transfer

338
00:18:44,811 --> 00:18:45,771
using C++.

339
00:18:45,771 --> 00:18:48,233
First, we have to construct the style transfer pipeline.

340
00:18:48,233 --> 00:18:49,214
As you can see in the diagram here,

341
00:18:49,214 --> 00:18:52,157
they start from the camera view, which corresponds

342
00:18:52,157 --> 00:18:53,738
to what the AI sees.

343
00:18:53,738 --> 00:18:56,300
From the camera view, you get the texture

344
00:18:56,300 --> 00:18:59,943
that will be convertible to pixels.

345
00:18:59,943 --> 00:19:02,805
And you can also do object detection

346
00:19:02,805 --> 00:19:03,766
to generate the soft mask.

347
00:19:04,554 --> 00:19:06,817
From emotion module, as we have seen before,

348
00:19:06,817 --> 00:19:08,419
we can compute the style weights,

349
00:19:08,419 --> 00:19:10,942
get all of these three things, send all of them

350
00:19:10,942 --> 00:19:13,745
into style transfer module, and you get the final stylization

351
00:19:13,745 --> 00:19:14,566
results.

352
00:19:14,566 --> 00:19:17,970
Finally, we apply back the results into texture,

353
00:19:17,970 --> 00:19:21,474
and you see what was drawn on the canvas.

354
00:19:24,117 --> 00:19:27,060
To execute this system, we have two choices, obviously.

355
00:19:27,060 --> 00:19:29,063
Whether we want to use CPU or GPU.

356
00:19:29,063 --> 00:19:34,350
So we have to wait which one is better for this problem.

357
00:19:34,350 --> 00:19:35,711
First, let's take a look at GPU.

358
00:19:35,711 --> 00:19:39,436
When we are trying to implement this,

359
00:19:39,436 --> 00:19:41,879
V4 doesn't have GPU interference support

360
00:19:41,879 --> 00:19:43,061
for TensorFlow model.

361
00:19:43,626 --> 00:19:49,508
So logically thinking, we can try to implement TensorFlow GPU inside UE4.

362
00:19:49,508 --> 00:19:57,030
However, due to the complexities of the GPU library of TensorFlow, it is way too expensive to do.

363
00:19:57,030 --> 00:20:04,993
And we also try ONNX Runtime. That one is easier to be implemented for GPU inference.

364
00:20:04,993 --> 00:20:08,654
However, it is not portable to consoles due to the lack of CUDA support.

365
00:20:09,635 --> 00:20:14,137
We can also try to write a GPGP core share code for a console,

366
00:20:14,137 --> 00:20:17,138
but we find that it's way too expensive for such a small experiment.

367
00:20:17,138 --> 00:20:23,202
And finally, even though we can do everything that we want with the GPU and consoles,

368
00:20:23,202 --> 00:20:27,544
for our use case, the GPU budget will be prioritized for graphics,

369
00:20:27,544 --> 00:20:33,707
and it's not really a good idea for us to use the GPU into the table.

370
00:20:35,727 --> 00:20:37,947
And when we are looking at the CPU,

371
00:20:37,947 --> 00:20:41,528
we try to implement the TensorFlow Lite.

372
00:20:41,528 --> 00:20:44,809
So TensorFlow Lite is a sublibrary of TensorFlow

373
00:20:44,809 --> 00:20:46,809
that you can do CPU inference.

374
00:20:46,809 --> 00:20:51,310
It was actually dedicated for microcontrollers

375
00:20:51,310 --> 00:20:52,171
and ARM architecture.

376
00:20:52,171 --> 00:20:57,192
But yeah, we can try to adjust the source codes inside UE4

377
00:20:57,192 --> 00:20:57,752
to do that.

378
00:20:57,752 --> 00:20:59,992
And TensorFlow Lite is also very small.

379
00:20:59,992 --> 00:21:01,573
So there are only fewer source codes

380
00:21:01,573 --> 00:21:03,933
to be adjusted such that it can compile nicely.

381
00:21:05,163 --> 00:21:08,365
CPU, even though people say it's slow,

382
00:21:08,365 --> 00:21:10,167
it's fast enough for our use case

383
00:21:10,167 --> 00:21:15,251
because we can put animations to buffer the computation time.

384
00:21:15,251 --> 00:21:18,213
And then by using CPU, there's no resource conflict

385
00:21:18,213 --> 00:21:20,295
with the GPU to do graphics rendering.

386
00:21:20,295 --> 00:21:23,357
And finally, we can also port them

387
00:21:23,357 --> 00:21:26,700
into console for x86, x86-4, and ARM architecture, which is

388
00:21:26,700 --> 00:21:29,442
something that we really want to do.

389
00:21:31,205 --> 00:21:35,182
So finally, we cross out the GPU and we go with the CPU options.

390
00:21:37,064 --> 00:21:40,247
Now to fit everything inside UE4,

391
00:21:40,247 --> 00:21:43,129
we have to take necessary source codes from TensorFlow Lite

392
00:21:43,129 --> 00:21:46,992
library, put them inside UE4 as a plug-ins.

393
00:21:46,992 --> 00:21:49,194
However, this is not going to compile well.

394
00:21:49,194 --> 00:21:52,156
So what we have to do is first adjust a lot of source codes,

395
00:21:52,156 --> 00:21:54,378
trial and error, trying to make sure

396
00:21:54,378 --> 00:21:58,261
that UE4 and the TensorFlow Lite would compile together nicely.

397
00:21:58,261 --> 00:22:01,383
After days of experiments on trying out this,

398
00:22:01,383 --> 00:22:02,844
we finally get everything running well.

399
00:22:03,485 --> 00:22:08,135
And finally, we have a nice plugin where we can do the API call from the game codes

400
00:22:08,135 --> 00:22:13,427
to read the TensorFlow format model and do the CPU inference.

401
00:22:15,955 --> 00:22:19,256
And after all those things for, I think, around a week,

402
00:22:19,256 --> 00:22:20,077
we finally get this.

403
00:22:20,077 --> 00:22:22,658
This is the in-game UI of the style transfer.

404
00:22:22,658 --> 00:22:25,319
On the left side, you can see the camera view of the AI.

405
00:22:25,319 --> 00:22:27,540
That's what Pino is looking at.

406
00:22:27,540 --> 00:22:31,783
And the corresponding mask after we do the object detection.

407
00:22:31,783 --> 00:22:33,904
The final result of the style transfer

408
00:22:33,904 --> 00:22:36,205
is on the far right side.

409
00:22:36,205 --> 00:22:38,926
And the blend is shown in the middle,

410
00:22:38,926 --> 00:22:41,427
where you can see the top panel corresponds

411
00:22:41,427 --> 00:22:44,849
to the style implemented at the background.

412
00:22:45,340 --> 00:22:47,785
It only consists of 100% of one style.

413
00:22:47,785 --> 00:22:51,171
And the bottom panel corresponds to the styles,

414
00:22:51,171 --> 00:22:55,620
the blend of styles that is applied to the box.

415
00:22:55,620 --> 00:22:57,724
And for this experiment.

416
00:22:58,742 --> 00:23:02,624
this style corresponds to the emotional,

417
00:23:02,624 --> 00:23:06,426
to the mood value that is shown in the middle diagram.

418
00:23:06,426 --> 00:23:08,528
So I would like to emphasize here,

419
00:23:08,528 --> 00:23:09,328
is that this is a test bed

420
00:23:09,328 --> 00:23:11,289
where we can try multiple styles.

421
00:23:11,289 --> 00:23:13,670
So for the one that we are looking,

422
00:23:13,670 --> 00:23:15,551
where we are showing you right now,

423
00:23:15,551 --> 00:23:17,192
is just like, it's just an example

424
00:23:17,192 --> 00:23:20,274
of how we can try to blend multiple styles

425
00:23:20,274 --> 00:23:22,635
by playing around inside this mood space.

426
00:23:22,815 --> 00:23:29,599
So at the end of the day, we still have to choose which style is better to express the

427
00:23:29,599 --> 00:23:31,861
appropriate emotions of the AI.

428
00:23:34,089 --> 00:23:38,190
So to summarize today's talk, we explore paintings

429
00:23:38,190 --> 00:23:41,692
with style transfer as a new way to express AI emotions.

430
00:23:41,692 --> 00:23:46,313
The AI emotions itself is modeled using modified OCC

431
00:23:46,313 --> 00:23:47,213
and PET model.

432
00:23:47,213 --> 00:23:50,915
The appropriate styles of the style transfer

433
00:23:50,915 --> 00:23:53,736
is selected according to the mood value of the AI.

434
00:23:53,736 --> 00:23:57,737
And we also introduce four use cases

435
00:23:57,737 --> 00:23:59,358
that we did in this experiment, which

436
00:23:59,358 --> 00:24:02,359
is basic style transfer, blending, mask, and color

437
00:24:02,359 --> 00:24:03,459
control of style transfer.

438
00:24:04,179 --> 00:24:12,387
Finally, we implemented the whole system inside UE4 using C++ interface from TF Lite for CPU inference.

439
00:24:12,387 --> 00:24:17,372
So before we close the presentation for today, I would like to share with you

440
00:24:17,372 --> 00:24:20,295
one weird experiment that we did.

441
00:24:20,295 --> 00:24:25,280
We tried something called G-buffer style transfer, and this is inside UE4.

442
00:24:25,872 --> 00:24:30,055
This style transfer leverages the game engine GBuffer.

443
00:24:30,055 --> 00:24:33,497
And we do style transfer into each component of the GBuffer

444
00:24:33,497 --> 00:24:35,759
before the final image rendering.

445
00:24:35,759 --> 00:24:37,921
So let's take a look at the basic style transfer

446
00:24:37,921 --> 00:24:38,821
at the top panel.

447
00:24:38,821 --> 00:24:41,203
You see in the middle, that's the UE4 scene.

448
00:24:41,203 --> 00:24:43,164
And if we take a picture of that,

449
00:24:43,164 --> 00:24:45,466
and we do basic style transfer as usual,

450
00:24:45,466 --> 00:24:48,388
treating it as an image, you get the results on the right side.

451
00:24:49,425 --> 00:24:53,230
But if you look at the bottom where we do the gbuffer style transfer,

452
00:24:53,230 --> 00:24:55,012
that's the same uniform scene.

453
00:24:55,012 --> 00:24:58,276
However, what we do is that we access the gbuffer components.

454
00:24:58,276 --> 00:25:03,863
We apply that corresponding yellow styles into some of the gbuffer components.

455
00:25:03,863 --> 00:25:08,750
And what you get is the result on the bottom right corner, which looks really fascinating.

456
00:25:10,915 --> 00:25:14,617
So thank you very much for coming to my sessions.

457
00:25:14,617 --> 00:25:17,178
If you have any extra questions later,

458
00:25:17,178 --> 00:25:20,120
please feel free to contact me by either email, Twitter,

459
00:25:20,120 --> 00:25:21,080
or LinkedIn.

460
00:25:21,080 --> 00:25:23,081
I also tweet a lot about machine learning

461
00:25:23,081 --> 00:25:26,283
that will be useful for game development.

462
00:25:26,283 --> 00:25:27,623
If you are interested, please check it out.

463
00:25:27,623 --> 00:25:31,806
And thank you very much for coming to the session.

