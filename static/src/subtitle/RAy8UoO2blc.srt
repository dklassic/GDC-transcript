1
00:00:06,102 --> 00:00:10,185
So, hello. My name is Jalal. I'm a technical architect at Ubisoft Montreal.

2
00:00:10,786 --> 00:00:15,710
I'll be talking today about some of the architecture optimization we did on Rainbow Six Siege.

3
00:00:16,731 --> 00:00:19,913
So, of course, the work I'm presenting today is the work of the whole 3D team.

4
00:00:20,473 --> 00:00:21,714
So I'd like to thank them in this slide.

5
00:00:23,176 --> 00:00:30,061
So I will start by giving an overview of some of the rendering passes we're doing on Rainbow.

6
00:00:31,131 --> 00:00:34,438
Then I'm going to talk about two of my babies on this production,

7
00:00:34,558 --> 00:00:38,606
which is the material-based draw call system and the checkerboard rendering.

8
00:00:40,820 --> 00:00:42,001
So the rendering overview.

9
00:00:42,902 --> 00:00:45,103
So where is Rainbow Six Siege first?

10
00:00:45,303 --> 00:00:47,825
So it's the rebirth of a very loud franchise.

11
00:00:47,905 --> 00:00:51,888
So that meant that a lot of hardcore players

12
00:00:51,968 --> 00:00:52,808
were waiting for the game.

13
00:00:52,828 --> 00:00:58,612
And for the hardcore players, 60 FPS on competitive gameplay

14
00:00:58,792 --> 00:00:59,553
is really important.

15
00:00:59,593 --> 00:01:01,574
So it was really important for us to hit that target.

16
00:01:02,355 --> 00:01:05,157
Also, we had to give more importance to the PC build,

17
00:01:05,177 --> 00:01:08,359
since a lot of hardcore players actually play on PCs.

18
00:01:09,654 --> 00:01:11,495
Also, we have a gameplay-driven game.

19
00:01:11,535 --> 00:01:14,617
So if you have to do a trade-off between graphic

20
00:01:14,958 --> 00:01:18,400
beautifulness or animation beauty, we're pretty much

21
00:01:18,420 --> 00:01:23,024
going to always go for gameplay and something that's

22
00:01:25,726 --> 00:01:28,168
going to improve the gameplay and the user

23
00:01:28,208 --> 00:01:29,189
experience of the player.

24
00:01:31,163 --> 00:01:33,986
I also have the destruction as a core gameplay mechanic.

25
00:01:34,647 --> 00:01:37,691
So, and destruction needs to be pretty much consistent

26
00:01:37,731 --> 00:01:42,436
between all platforms, so since in competitive mode

27
00:01:42,497 --> 00:01:45,440
you don't want a low-end PC to have different outcome

28
00:01:45,460 --> 00:01:49,205
than a high-end PC, so most of the destruction

29
00:01:49,505 --> 00:01:50,706
needs to be equivalent.

30
00:01:53,255 --> 00:01:56,376
Also was the first renderer that ships...

31
00:01:58,417 --> 00:02:00,298
was the first Rainbow Six that ships with the engine

32
00:02:00,378 --> 00:02:02,399
and we had a lot of legacy code, so Rainbow Six

33
00:02:03,199 --> 00:02:05,780
had a lot of development through the years,

34
00:02:06,500 --> 00:02:10,161
a lot of PS3-specific code that needs to be cleaned up, so

35
00:02:11,162 --> 00:02:13,803
we had to survive a lot of legacy code.

36
00:02:15,383 --> 00:02:18,044
A little video in reverse that I just pulled off the internet,

37
00:02:18,465 --> 00:02:19,405
showing what we achieved.

38
00:02:21,226 --> 00:02:22,306
In reverse, because it's cooler.

39
00:02:37,313 --> 00:02:37,473
Yeah.

40
00:02:40,734 --> 00:02:46,355
So on the tech side, so we were targeting 60 FPS.

41
00:02:46,415 --> 00:02:50,756
That meant that we need to run the GPU at 14 milliseconds

42
00:02:50,816 --> 00:02:52,577
on average on non-combat situations.

43
00:02:53,097 --> 00:02:55,337
And we also negotiated with the engine team

44
00:02:55,357 --> 00:02:57,398
a 38 millisecond linear on CPU.

45
00:02:58,998 --> 00:03:02,499
And our critical path was around 10 millisecond on average.

46
00:03:04,589 --> 00:03:18,619
Also needed to provide scalable destruction, which is important since you want when a level starts to start at 60 FPS and also when it ends when there has been a lot of combat and a lot of stuff happening on the level to still be at 60 FPS.

47
00:03:19,260 --> 00:03:27,445
We also wanted to ship at a resolution bigger than 720p on all platforms and we wanted to commit 4K on PC.

48
00:03:31,103 --> 00:03:34,884
So, also to support a wide range of PCs,

49
00:03:34,964 --> 00:03:39,185
so we need to support one gig video graphic card.

50
00:03:39,225 --> 00:03:41,146
And it turns out to be quite difficult

51
00:03:41,286 --> 00:03:42,747
when you target current gen,

52
00:03:43,207 --> 00:03:45,748
since if there's like one thing that we have,

53
00:03:46,048 --> 00:03:48,889
I think plenty of on the current gen is actually memory.

54
00:03:49,749 --> 00:03:52,590
So, yeah, that needed a lot of like work

55
00:03:52,610 --> 00:03:57,351
to pretty much get back all the default calls of the engine

56
00:03:58,092 --> 00:03:59,932
to be able to run on one gig graphic cards.

57
00:04:01,897 --> 00:04:05,337
So one more thing, that Siege is a live game,

58
00:04:05,357 --> 00:04:07,578
so we're still doing updates, we're still working

59
00:04:07,758 --> 00:04:10,078
on some graphic features, we're still doing development.

60
00:04:11,059 --> 00:04:14,899
One of the stuff we're looking at is auto exposure.

61
00:04:14,919 --> 00:04:17,020
A lot of players are complaining of visibility

62
00:04:17,040 --> 00:04:19,280
from inside to outside, and it's stuff

63
00:04:19,300 --> 00:04:21,781
that we're still looking for, and we have to be careful

64
00:04:21,841 --> 00:04:24,602
not to break things, like just a couple of weeks ago,

65
00:04:24,662 --> 00:04:26,902
released an update where some shaders were missing,

66
00:04:26,942 --> 00:04:29,443
and we ended up with this bug on screen.

67
00:04:32,857 --> 00:04:36,940
Here's a hierarchical view of a GPU frame on Rainbow Six Siege.

68
00:04:37,261 --> 00:04:40,604
So we spend on average 5 milliseconds on geometry rendering.

69
00:04:41,245 --> 00:04:44,408
So to achieve that we rely heavily on Kullin.

70
00:04:44,708 --> 00:04:48,672
It's going to be like the draw call based material system that I'm going to be talking about in the second part.

71
00:04:49,633 --> 00:04:51,415
And we also do a lot of shadow caching.

72
00:04:52,882 --> 00:04:55,422
We spend also average 5 millisecond on lighting.

73
00:04:56,202 --> 00:04:58,143
We do screen space reflection and

74
00:04:59,783 --> 00:05:03,684
this actually also includes screen space reflection pass, not the ray tracing

75
00:05:03,744 --> 00:05:07,265
because it's done on a sink in parallel of shadows.

76
00:05:09,185 --> 00:05:14,486
And here is where the checkerboard rendering technique, which is going to be the second part of the talk, helps the most.

77
00:05:16,047 --> 00:05:18,227
And we also spend an average like 4 millisecond on...

78
00:05:19,174 --> 00:05:24,370
various post-processing, like downscaling hierarchical Z-buffers and stuff like that.

79
00:05:29,089 --> 00:05:33,631
On CPU, our critical path was around 10 milliseconds.

80
00:05:34,471 --> 00:05:39,973
And we implemented the Eskidor, where actually most of the rendering passes

81
00:05:40,474 --> 00:05:44,695
can be automatically independent on the number of draw calls that we're going to do.

82
00:05:45,796 --> 00:05:49,637
It's going to be split up in smaller pieces, so even the OPAC, pretty much,

83
00:05:49,677 --> 00:05:51,858
if we know that it's going to take around 4 milliseconds,

84
00:05:51,878 --> 00:05:54,479
we're going to automatically split it in different pieces.

85
00:05:55,139 --> 00:05:57,280
And we're going to try to maximize the number of...

86
00:05:57,760 --> 00:06:01,163
command buffer execution by having the graphic thread,

87
00:06:01,864 --> 00:06:03,265
which is our immediate context thread,

88
00:06:04,165 --> 00:06:05,627
steal tasks in order.

89
00:06:06,367 --> 00:06:10,931
And so pretty much visibility in all other passes

90
00:06:11,531 --> 00:06:14,153
gets stolen, and we don't have any overhead on that.

91
00:06:15,935 --> 00:06:19,638
And we're able to self-squeeze the most out

92
00:06:19,758 --> 00:06:22,960
of our main graphic thread.

93
00:06:23,821 --> 00:06:25,522
Also, shadow caching helps here a lot.

94
00:06:26,529 --> 00:06:28,050
I'm gonna be talking more about shadow caching.

95
00:06:29,732 --> 00:06:32,934
We, like, I was saying 4ms, actually, like,

96
00:06:33,594 --> 00:06:38,478
currently, the 4ms is spent on the opaque pass,

97
00:06:38,498 --> 00:06:41,360
and we rarely, like, go above 4ms,

98
00:06:42,101 --> 00:06:43,602
even, like, with the number of door calls we're doing,

99
00:06:44,303 --> 00:06:45,043
virtual door calls.

100
00:06:47,305 --> 00:06:51,048
So, for example, like, one of our passes...

101
00:06:51,805 --> 00:06:55,567
So I'm going to be repeating Colin and Hy-Z a lot in this talk,

102
00:06:55,607 --> 00:06:59,129
because pretty much we do Hy-Z, Colin,

103
00:06:59,149 --> 00:07:00,730
in every single pass on our render.

104
00:07:02,230 --> 00:07:05,152
So for first-person rendering, we pretty much

105
00:07:05,192 --> 00:07:08,514
start by rendering the first-person character

106
00:07:08,534 --> 00:07:09,954
to the gbuffer and wbuffer.

107
00:07:11,615 --> 00:07:15,477
From that, we just render the first 100 best occluders

108
00:07:15,517 --> 00:07:19,980
that we have on our scene, which is usually just the 400 closest

109
00:07:20,020 --> 00:07:20,740
objects to the camera.

110
00:07:22,088 --> 00:07:22,808
to the depth buffer.

111
00:07:23,549 --> 00:07:28,034
We downscaled that depth buffer to generate a high Z buffer,

112
00:07:28,455 --> 00:07:28,935
starting from 512, 256.

113
00:07:32,099 --> 00:07:33,961
And we used that hierarchical Z buffer

114
00:07:34,001 --> 00:07:39,607
to actually perform calling on the subsequent passes

115
00:07:39,647 --> 00:07:43,010
that uses the main view, and most notably the back pass

116
00:07:43,050 --> 00:07:43,451
by itself.

117
00:07:47,515 --> 00:07:52,319
So for shadows, all shadows pretty much start off with a cache.

118
00:07:53,140 --> 00:07:59,965
And we always, when doing caching, we generate a high-level Z-buffer from that cache to perform calling.

119
00:08:01,326 --> 00:08:04,069
For the resolution part of the shadows...

120
00:08:05,751 --> 00:08:09,772
We do it on a separate pass, pretty much we had it in the beginning merged with our lighting result,

121
00:08:09,792 --> 00:08:15,474
but the VGPR pressure was too big, so we just moved it from the lighting result.

122
00:08:16,234 --> 00:08:26,358
And also used the IEqualZ buffer of the shadow map to pretty much determine if a whole block is shadowed or not and skip work.

123
00:08:27,759 --> 00:08:30,520
For our local lights, we just resolved them in QuarterRez.

124
00:08:30,760 --> 00:08:32,400
Turns out to be a good quality.

125
00:08:33,277 --> 00:08:34,818
versus performance trade-off.

126
00:08:37,481 --> 00:08:40,524
So, for shadow maps,

127
00:08:41,164 --> 00:08:43,146
we always start when loading a level

128
00:08:43,546 --> 00:08:45,188
by shooting a static shadow map,

129
00:08:45,228 --> 00:08:46,950
which is 6K by 6K, 16-bit.

130
00:08:48,551 --> 00:08:49,832
We do it at load time, not at...

131
00:08:51,555 --> 00:08:57,560
map generation time because we have a lot of game plays that

132
00:08:57,960 --> 00:09:01,904
where actually level designer place object that are randomly

133
00:09:01,924 --> 00:09:05,467
going to appear and block some passages, block some windows.

134
00:09:06,007 --> 00:09:09,250
And those actually are very good occluders that we want to

135
00:09:09,290 --> 00:09:10,531
get into the static shadow map.

136
00:09:11,011 --> 00:09:14,914
And most of the time, like 99% of the time, they are static

137
00:09:14,954 --> 00:09:19,178
objects, so it's actually better to have them on the

138
00:09:19,198 --> 00:09:19,718
static shadow map.

139
00:09:20,629 --> 00:09:26,853
So from that we generate a 512 by 512 ESM that's going to be used for many large particle shadowing.

140
00:09:27,454 --> 00:09:29,155
And also generate the hierarchical Z-buffer.

141
00:09:33,038 --> 00:09:41,064
So to scale our shadow rendering, we blend between the static shadow map that we generated at map load and dynamic cascades.

142
00:09:42,225 --> 00:09:45,948
So, and all the dynamic cascades are going to actually use the static shadow map.

143
00:09:46,687 --> 00:09:48,888
to do calling so

144
00:09:50,369 --> 00:09:51,529
like on consoles.

145
00:09:52,890 --> 00:09:53,871
It's what's one for example,

146
00:09:54,831 --> 00:09:56,432
the first cascade is fully dynamic.

147
00:09:57,913 --> 00:09:59,634
Because we don't have enough resolution with 6K

148
00:10:00,054 --> 00:10:01,675
to actually provide good enough quality.

149
00:10:02,835 --> 00:10:05,097
The second and third cascades are

150
00:10:06,337 --> 00:10:07,638
content just the dynamic object

151
00:10:08,518 --> 00:10:10,139
and when we render those dynamic object,

152
00:10:10,159 --> 00:10:11,060
we use the static shadow,

153
00:10:11,320 --> 00:10:13,521
the static high Z buffer to actually do calling

154
00:10:13,821 --> 00:10:15,122
and reduce the amount of draw calls

155
00:10:15,502 --> 00:10:16,343
and the cost.

156
00:10:17,051 --> 00:10:18,512
of doing the rendering of dynamic parts.

157
00:10:19,613 --> 00:10:22,936
And we just blend between the static and the dynamic shadow map.

158
00:10:22,956 --> 00:10:27,079
The fourth cascade is pretty much substituted by the static shadow map.

159
00:10:27,180 --> 00:10:31,904
And we have the ability pretty much to scale that dynamically

160
00:10:32,104 --> 00:10:34,826
or like on low end PC,

161
00:10:35,046 --> 00:10:38,409
we just generate one dynamic cascade

162
00:10:38,929 --> 00:10:40,531
and everything else is the static shadow map.

163
00:10:44,144 --> 00:10:45,946
Because we have a lot of locking lights,

164
00:10:47,427 --> 00:10:52,852
like usually a level has more than 80 shadow spotlights.

165
00:10:54,213 --> 00:10:57,456
We can't have them all in cache at any given point.

166
00:10:57,476 --> 00:10:58,817
It would cost too much in memory.

167
00:10:59,464 --> 00:11:05,126
So, what we do is, when we encounter a new light,

168
00:11:05,606 --> 00:11:10,528
and we actually only support a maximum of eight shadowed

169
00:11:10,868 --> 00:11:12,069
lights on the frustum.

170
00:11:12,669 --> 00:11:15,710
So, when we encounter a new light,

171
00:11:15,930 --> 00:11:19,031
we always shoot the static part of the shadow map.

172
00:11:20,152 --> 00:11:21,352
When we shoot the static part,

173
00:11:21,372 --> 00:11:23,013
we generate a high-frequency buffer from it.

174
00:11:24,213 --> 00:11:27,478
uh... then we're rendering the lights with just instead of clearing the depth

175
00:11:27,518 --> 00:11:30,041
buffer we just copy the static part into the dynamic part

176
00:11:30,842 --> 00:11:33,706
we render the dynamic objects on top and of course

177
00:11:34,246 --> 00:11:36,809
we do culling since we already have a hierarchical z-buffer

178
00:11:37,971 --> 00:11:42,316
and then the result resides in an array that can be accessed in further passes

179
00:11:45,197 --> 00:11:48,179
So for the lighting, our lighting is based on

180
00:11:48,359 --> 00:11:49,460
a clustered structure.

181
00:11:49,480 --> 00:11:53,663
Each frog cell is 32 by 32 pixel based

182
00:11:53,783 --> 00:11:55,664
and has a z exponential distribution.

183
00:11:57,085 --> 00:11:59,727
And we also perform high-tech culling on the light volume

184
00:12:00,708 --> 00:12:03,450
using the high z, the view high z buffer and other

185
00:12:05,131 --> 00:12:05,271
like.

186
00:12:05,872 --> 00:12:08,033
just going through the structure hierarchically.

187
00:12:08,594 --> 00:12:11,015
And we fill each cell with the list of lights

188
00:12:11,656 --> 00:12:13,958
that we're going to render on that cell.

189
00:12:15,439 --> 00:12:20,802
And we also see local queue maps as lights.

190
00:12:20,843 --> 00:12:24,165
So they are gathered within the light list volume.

191
00:12:25,046 --> 00:12:27,607
And just like when we render them,

192
00:12:28,008 --> 00:12:29,329
they're pulled off the same way.

193
00:12:30,470 --> 00:12:32,311
We just maintain an index, and they actually

194
00:12:32,351 --> 00:12:34,072
reside in a queue.

195
00:12:35,379 --> 00:12:36,119
in a cubemap array.

196
00:12:37,039 --> 00:12:39,540
The same thing we do for shadows and gobos.

197
00:12:39,960 --> 00:12:41,481
So pretty much everything is in arrays,

198
00:12:41,981 --> 00:12:44,722
and we only maintain indices in that array,

199
00:12:45,102 --> 00:12:47,643
and during the rendering, we just need to fetch

200
00:12:48,763 --> 00:12:53,765
the shadow or the cubemap for a given cell.

201
00:12:54,305 --> 00:12:57,466
So that allows us to pretty much handle the same features

202
00:12:57,546 --> 00:13:00,067
between the forward and the deferred paths.

203
00:13:03,129 --> 00:13:06,856
So I'm going to be switching to the second part of the talk,

204
00:13:06,876 --> 00:13:08,318
which is the material-based vehicle system.

205
00:13:10,001 --> 00:13:13,608
So Rainbow Six Destruction, we had a main guideline

206
00:13:14,029 --> 00:13:14,971
from the art direction.

207
00:13:16,131 --> 00:13:19,894
which was, when destruction happens, it really needs to be visible.

208
00:13:19,934 --> 00:13:23,256
So when you go into a room, you didn't see destruction happening,

209
00:13:23,436 --> 00:13:24,657
you just see the aftermath of it.

210
00:13:25,258 --> 00:13:27,860
You need to know that there has been a big change.

211
00:13:27,920 --> 00:13:32,624
So we need to pretty much have better persistency of objects

212
00:13:33,244 --> 00:13:37,488
and have this feeling of a lot of volume getting generated from destruction.

213
00:13:38,849 --> 00:13:41,211
Fortunately, we're dealing with a close quarter game, so...

214
00:13:42,067 --> 00:13:45,810
We have walls and floors mainly to destroy procedurally.

215
00:13:46,770 --> 00:13:48,672
They generate a unique geometry.

216
00:13:49,352 --> 00:13:50,714
Problem that we have with that is that

217
00:13:50,974 --> 00:13:52,555
they're usually very good occluders,

218
00:13:52,635 --> 00:13:54,657
but not in our case when you start poking holes

219
00:13:55,497 --> 00:13:58,320
into walls and floors, you lose occlusion efficiency.

220
00:14:00,819 --> 00:14:02,880
We also have a lot of, to deal with a lot of

221
00:14:03,980 --> 00:14:05,421
debris and problems.

222
00:14:05,441 --> 00:14:07,061
So those are usually smaller meshes,

223
00:14:07,622 --> 00:14:09,522
but they are in greater numbers.

224
00:14:10,202 --> 00:14:12,263
Can also be procedurally generated,

225
00:14:12,323 --> 00:14:14,664
so they have unique geometry or instances.

226
00:14:16,385 --> 00:14:18,485
So here's an example of a destruction.

227
00:14:40,035 --> 00:14:41,256
So I'm going to decompose that.

228
00:14:42,397 --> 00:14:47,121
In our game, it's really important to have the player

229
00:14:47,441 --> 00:14:49,843
model the destruction the way he wants.

230
00:14:49,963 --> 00:14:54,927
So if he wants to shoot a hole that's

231
00:14:55,027 --> 00:14:59,111
like just two centimeters from the door frame, he can do so.

232
00:15:00,062 --> 00:15:01,984
So we generate a lot of unique geometry for that.

233
00:15:03,345 --> 00:15:08,849
And to pretty much have this volume that's

234
00:15:08,910 --> 00:15:12,392
getting conserved from the hole in the first place,

235
00:15:13,133 --> 00:15:15,395
we generate a lot of debris to fill up

236
00:15:15,435 --> 00:15:16,816
the space beyond the player.

237
00:15:16,916 --> 00:15:19,578
In this situation, we have more than 100 debris

238
00:15:19,618 --> 00:15:21,700
that got generated from the destruction.

239
00:15:22,020 --> 00:15:24,442
Some of them are going to disappear, but most of them

240
00:15:24,462 --> 00:15:27,244
are going to stay persistent during the hole, actually,

241
00:15:27,544 --> 00:15:27,825
around.

242
00:15:28,460 --> 00:15:31,843
which can last up to three minutes or even more on PvE games.

243
00:15:34,865 --> 00:15:38,008
For the occlusion efficiency, so I took a wall

244
00:15:38,448 --> 00:15:40,570
and shot a couple of bullets into it.

245
00:15:41,590 --> 00:15:44,252
So you can see how pretty much those holes are going to propagate

246
00:15:44,392 --> 00:15:46,034
along the hierarchical Z-buffer,

247
00:15:47,054 --> 00:15:52,158
and thus making like an object that's...

248
00:15:53,768 --> 00:15:59,472
Uh, small enough on screen, but we're going to pretty much select this, uh, this

249
00:15:59,592 --> 00:16:06,497
uh, high Z, uh, level, the high Z texture level to do the ABE test.

250
00:16:06,577 --> 00:16:10,259
And we're going to actually pass it as visible, even though it was tested,

251
00:16:10,860 --> 00:16:14,302
uh, like on a, on a bigger level, it will have failed the test.

252
00:16:15,720 --> 00:16:19,582
So pretty much easily reduce our occlusion efficiency

253
00:16:19,662 --> 00:16:21,904
just by poking a couple of holes in any given wall.

254
00:16:22,284 --> 00:16:23,865
And that happens a lot in gameplay actually.

255
00:16:25,740 --> 00:16:31,542
So, the early prototypes were largely graphic, CPU and GPU bound.

256
00:16:32,982 --> 00:16:35,963
Like, once we go through computing the destruction by itself,

257
00:16:36,363 --> 00:16:39,264
we have to render it, so there's all the debris that get generated,

258
00:16:40,144 --> 00:16:43,025
all the unique geometry that gets added into the world.

259
00:16:43,785 --> 00:16:46,746
So, we started thinking about ways on improving that.

260
00:16:46,766 --> 00:16:47,146
And of course...

261
00:16:47,947 --> 00:16:53,569
Our first implementation only had a PC, the i6-11, deferred context style of render,

262
00:16:54,010 --> 00:16:55,950
which is not great at scaling.

263
00:16:57,071 --> 00:17:02,374
So, we thought of what we have pretty much on our scenes,

264
00:17:02,934 --> 00:17:05,155
and you can see that when you do a destruction,

265
00:17:06,235 --> 00:17:08,576
usually a lot of stuff share the same material.

266
00:17:08,636 --> 00:17:12,538
So, most of the debris that are going to be generated share the same material.

267
00:17:13,320 --> 00:17:15,902
Walls usually also share the same material.

268
00:17:15,942 --> 00:17:18,003
It also goes from this art direction

269
00:17:18,043 --> 00:17:21,305
where destruction have to be visualized by the player.

270
00:17:21,345 --> 00:17:25,548
So if we're using a red wall, it means that can be destroyed.

271
00:17:25,608 --> 00:17:28,230
So it's a lot of red walls here and there.

272
00:17:28,270 --> 00:17:31,312
So we can actually batch a lot of stuff by material,

273
00:17:31,792 --> 00:17:32,272
by design.

274
00:17:33,233 --> 00:17:35,614
And as I said, all the debris share the same material,

275
00:17:35,975 --> 00:17:36,355
which helps.

276
00:17:36,855 --> 00:17:38,356
There has been a presentation, because we're

277
00:17:38,376 --> 00:17:40,958
sharing a lot of work with the Assassin's Creed Unity.

278
00:17:42,076 --> 00:17:43,737
done by Eric Har last year.

279
00:17:43,757 --> 00:17:44,697
That's interesting.

280
00:17:44,717 --> 00:17:46,238
Like, there's stuff that I'm not covering,

281
00:17:46,278 --> 00:17:47,418
but actually they are covering.

282
00:17:47,918 --> 00:17:49,319
And I invite you to go see it.

283
00:17:52,400 --> 00:17:54,861
Also, we wanted more granularity in culling,

284
00:17:55,261 --> 00:18:01,644
since when we decrease, like, the efficiency of our culling,

285
00:18:01,684 --> 00:18:04,505
we want smaller units to actually be able to select

286
00:18:04,545 --> 00:18:08,026
a higher-level MIP in our high-Z buffer to do the culling.

287
00:18:09,647 --> 00:18:11,127
So to implement that...

288
00:18:12,249 --> 00:18:17,554
We started off by unifying a lot of buffers that we have on our renderer.

289
00:18:17,614 --> 00:18:23,440
So in Rainbow Six, a lot of resources will reside in some unified buffer of some sort.

290
00:18:23,880 --> 00:18:27,483
So we have the unified vertex buffer, which is like one buffer that contains the geometry

291
00:18:27,524 --> 00:18:29,525
for the whole map.

292
00:18:29,926 --> 00:18:31,567
Unified index buffer, same thing.

293
00:18:31,627 --> 00:18:34,831
Unified constant buffer, it contains all the constants for the whole rarcode.

294
00:18:37,979 --> 00:18:43,741
So we pretty much simulate structured buffers on top of that.

295
00:18:43,781 --> 00:18:45,622
They're implemented as byte address buffers.

296
00:18:46,162 --> 00:18:52,304
But we generate some C++ code to help access it from the CPU

297
00:18:52,404 --> 00:18:55,945
and some HLSL code to help access it from the GPU.

298
00:18:56,325 --> 00:18:59,806
We also add some metadata to pretty much explain

299
00:19:00,266 --> 00:19:02,927
how we're going to access this data to do it more efficiently.

300
00:19:04,200 --> 00:19:07,481
So here's an example of how we generate that code.

301
00:19:07,501 --> 00:19:14,302
So we go from the scripter that is in C++ that pretty much just

302
00:19:15,223 --> 00:19:18,844
goes to the different values that we're

303
00:19:18,864 --> 00:19:21,264
going to pass onto the GPU like we declared them.

304
00:19:21,885 --> 00:19:24,265
We generate an equivalent HLSL code

305
00:19:24,325 --> 00:19:28,967
that's going to help us load that chunk of data

306
00:19:29,527 --> 00:19:31,387
from a unified constant buffer.

307
00:19:31,407 --> 00:19:33,428
So all the door calls are going to be used.

308
00:19:34,159 --> 00:19:35,660
just in bind, just this buffer.

309
00:19:36,440 --> 00:19:39,702
And we're going to use indices that we pass along with our pipeline

310
00:19:40,282 --> 00:19:42,624
to be able to fetch those values.

311
00:19:45,225 --> 00:19:47,306
So, what's cool about that is that

312
00:19:47,826 --> 00:19:50,988
we have complete control over data layout.

313
00:19:51,008 --> 00:19:53,589
So we can easily, like, we tested a lot of stuff,

314
00:19:53,789 --> 00:19:55,710
like structure of Q32 arrays,

315
00:19:56,151 --> 00:19:58,072
and just a couple of lines actually in our code

316
00:19:58,092 --> 00:20:00,013
to change from one method of access.

317
00:20:00,981 --> 00:20:03,662
to another like AOS, SOA.

318
00:20:04,582 --> 00:20:08,223
It can also support in constant buffers other type of data

319
00:20:08,303 --> 00:20:10,623
and be pretty much going to be hidden

320
00:20:11,124 --> 00:20:13,664
from the high level programmer.

321
00:20:14,744 --> 00:20:19,246
We can have like RGBA8 or other type of formats.

322
00:20:19,766 --> 00:20:21,126
And it's just going to be transparent

323
00:20:21,166 --> 00:20:24,387
because we generate all the code that does that.

324
00:20:25,087 --> 00:20:27,648
And to some certain extent, especially on GCN.

325
00:20:28,648 --> 00:20:32,589
Anyways, those are instructions that do the same thing, so better just do it ourselves.

326
00:20:33,849 --> 00:20:40,850
So we also support some high-level APIs to be able to broadcast values into different

327
00:20:43,651 --> 00:20:44,091
constants.

328
00:20:44,571 --> 00:20:51,732
So since all the copies and all the management of those buffers, we handle ourselves using

329
00:20:51,772 --> 00:20:52,412
compute shaders.

330
00:20:52,432 --> 00:20:56,493
We can pretty much do broadcasting values quite easily.

331
00:20:57,789 --> 00:21:01,171
And code generation is always nice on iteration and

332
00:21:02,452 --> 00:21:07,015
to migrate access pattern, to do tests and to

333
00:21:07,695 --> 00:21:10,497
pretty much do whatever we want with our data and give us more control

334
00:21:12,098 --> 00:21:12,739
on our renderer.

335
00:21:14,981 --> 00:21:16,301
So, as I said,

336
00:21:17,182 --> 00:21:21,465
so the geometry, which is the unified vertex buffer, unified index buffer

337
00:21:22,246 --> 00:21:24,367
and constant are unified in our renderer.

338
00:21:25,464 --> 00:21:28,547
So a draw call is going to be defined by the shader it uses,

339
00:21:28,968 --> 00:21:31,551
the non-unified resources like textures,

340
00:21:32,872 --> 00:21:35,495
and the render state, like sampler state, raster state,

341
00:21:35,555 --> 00:21:35,936
et cetera.

342
00:21:37,057 --> 00:21:40,021
So elements that are going to share all of the above

343
00:21:40,241 --> 00:21:41,342
are going to be batched together.

344
00:21:42,925 --> 00:21:49,670
And all the paths are actually just going to use a subset of the different non-unified resources,

345
00:21:49,710 --> 00:21:53,232
like in a shadow path, which is going to, if you don't have alpha testing,

346
00:21:53,252 --> 00:21:56,995
we're just going to skip fetching the textures.

347
00:21:57,215 --> 00:22:02,859
We just remove them into our batching computation.

348
00:22:06,905 --> 00:22:10,589
When we initialize our renderer, or when we load our map,

349
00:22:10,890 --> 00:22:13,552
so each submesh instance is going

350
00:22:13,592 --> 00:22:16,215
to be mapped to three types of batches that

351
00:22:16,636 --> 00:22:18,678
are going to be used to alter the rendering, which

352
00:22:18,698 --> 00:22:20,840
is the normal, shadow, and visibility.

353
00:22:22,349 --> 00:22:29,632
So those kind of like types just are a helper to mask different type of data

354
00:22:30,212 --> 00:22:37,134
and try to remove some of the stuff that is not used in any given path

355
00:22:37,154 --> 00:22:38,815
to increase the amount of batching.

356
00:22:40,916 --> 00:22:43,336
And each one of these batches in the end when doing the rendering

357
00:22:43,356 --> 00:22:47,158
is going to correspond to a multi-draw index indirect command.

358
00:22:48,645 --> 00:22:51,626
So here I have an example of three submesh instances.

359
00:22:51,946 --> 00:22:55,427
So each one of them is going to map to a different normal batch.

360
00:22:56,647 --> 00:22:58,988
Two of them are going to share the same visibility batch.

361
00:22:59,008 --> 00:23:00,989
And they're going to be drawn with the same wichidraw

362
00:23:01,269 --> 00:23:03,349
indexed indirect command.

363
00:23:04,129 --> 00:23:07,571
And the instance two is going to have its own visibility batch.

364
00:23:07,671 --> 00:23:11,572
And then the three of them are going to share the shadow batch,

365
00:23:12,672 --> 00:23:14,033
which is usually the case.

366
00:23:14,093 --> 00:23:16,833
Shadows are more batchable by design.

367
00:23:18,159 --> 00:23:18,562
So, uh...

368
00:23:20,190 --> 00:23:25,151
And then on the GPU to pretty much access the data for each of those submesh instances,

369
00:23:26,052 --> 00:23:29,192
we maintain a globally unique index all through our pipeline

370
00:23:29,392 --> 00:23:36,054
that is maintained by the GPU and help us fetch any kind of data for our submesh instance.

371
00:23:36,554 --> 00:23:41,815
So from that index, we can go through multiple levels of indirection

372
00:23:42,235 --> 00:23:48,037
to fetch the vertices that are going to be used when running the vertex shader,

373
00:23:48,857 --> 00:23:49,897
the transform matrix.

374
00:23:50,878 --> 00:23:55,201
So just like depending on the data that we want to fetch

375
00:23:55,261 --> 00:23:57,983
and its frequency of access, it's going to have different index

376
00:23:58,463 --> 00:24:00,324
and it enables us to fetch whatever we want.

377
00:24:03,527 --> 00:24:05,808
What's important to note is that sometimes we take

378
00:24:06,629 --> 00:24:10,632
one level of indirection off just to speed up the access

379
00:24:10,652 --> 00:24:13,494
because we don't want to do too much indirect access

380
00:24:13,514 --> 00:24:16,356
to some of the further laid out data.

381
00:24:19,020 --> 00:24:22,801
So for each pass that we do, like the opaque pass,

382
00:24:23,601 --> 00:24:25,762
what we start is gathering dynamic buffer,

383
00:24:26,342 --> 00:24:28,443
all those globally unique indices.

384
00:24:30,964 --> 00:24:34,325
So each pass is gonna map only to one batch type,

385
00:24:34,665 --> 00:24:36,245
like normal, shadows, or visibility.

386
00:24:37,530 --> 00:24:46,222
And it takes us around 1.5 milliseconds to gather all of those indices for all the passes.

387
00:24:46,442 --> 00:24:48,845
So we just like spawn jobs at the beginning of the frame.

388
00:24:48,865 --> 00:24:54,432
We gather all those indices and then we start rendering using the dynamic buffers.

389
00:24:56,439 --> 00:24:59,841
We also append some extra data that can change between frames.

390
00:25:01,702 --> 00:25:07,705
And we also write our offset into the dynamic index buffer that we're going to be building

391
00:25:07,725 --> 00:25:11,287
when we're going to do the advanced culling per triangle.

392
00:25:15,029 --> 00:25:19,531
So from this path buffer that contains the list of all the submeshes for a given path,

393
00:25:20,151 --> 00:25:24,633
we start by doing submesh instance culling, which is like culling.

394
00:25:25,717 --> 00:25:26,558
for that frequency.

395
00:25:27,919 --> 00:25:29,381
It's either we're gonna draw the object,

396
00:25:29,401 --> 00:25:31,963
it's gonna go either directly to the draw path

397
00:25:32,183 --> 00:25:33,564
or it's gonna be further culled

398
00:25:33,705 --> 00:25:35,546
or just gonna discard it from the start.

399
00:25:35,566 --> 00:25:38,169
So if it goes further down the culling path,

400
00:25:38,569 --> 00:25:40,150
so all of our measures are split

401
00:25:40,291 --> 00:25:41,952
in chunks of 64 triangles.

402
00:25:43,053 --> 00:25:43,654
We do culling.

403
00:25:45,935 --> 00:25:48,616
for those 64 triangles.

404
00:25:49,496 --> 00:25:54,979
And if the chunk passes this phase,

405
00:25:55,759 --> 00:25:57,500
we do backface culling per triangle.

406
00:25:58,701 --> 00:26:01,322
And in the end, we only draw pretty much the triangles

407
00:26:01,402 --> 00:26:04,843
that passed all of those culling levels.

408
00:26:06,744 --> 00:26:07,925
So on the first level of culling,

409
00:26:07,945 --> 00:26:09,565
which is like per submesh instance,

410
00:26:09,826 --> 00:26:11,886
we start by screen space size culling.

411
00:26:13,708 --> 00:26:16,509
Then we do distance culling, frustum culling, occlusion culling.

412
00:26:16,629 --> 00:26:23,072
So our renderer actually starts by doing a very coarse culling phase, which gathers a

413
00:26:23,672 --> 00:26:28,094
bulk of objects, and then we just go through them and further cull them.

414
00:26:28,114 --> 00:26:31,836
We can actually discard a lot of stuff on the frustum culling path too.

415
00:26:33,595 --> 00:26:38,380
The second level of culling, which is by 64 triangle chunk

416
00:26:38,561 --> 00:26:40,443
of each one of those measures that pass

417
00:26:40,483 --> 00:26:41,484
the first level of culling.

418
00:26:41,504 --> 00:26:43,547
We're gonna pretty much go through the same thing.

419
00:26:44,588 --> 00:26:47,331
We have, we substitute the distance culling

420
00:26:47,912 --> 00:26:52,337
by orientation culling, so in a baking phase,

421
00:26:53,598 --> 00:26:53,979
we just.

422
00:26:55,033 --> 00:26:57,974
try to compute a normal spread for each chunk

423
00:26:58,675 --> 00:27:00,636
and we do orientation culling at that stage

424
00:27:01,036 --> 00:27:02,237
and then we do occlusion culling.

425
00:27:02,677 --> 00:27:04,698
So occlusion culling on that stage enables us

426
00:27:04,778 --> 00:27:07,879
to improve our culling efficiency since we reduced

427
00:27:08,239 --> 00:27:12,762
the size of any given object we're gonna cull

428
00:27:12,802 --> 00:27:15,843
to chunks of 64 triangles so we have better chances

429
00:27:15,903 --> 00:27:21,926
to fetch the Z-samples from higher level MIP

430
00:27:21,966 --> 00:27:22,707
on the high Z-buffer.

431
00:27:23,795 --> 00:27:27,716
Then if everything passes, we do just triangle normal culling.

432
00:27:27,756 --> 00:27:29,596
We maintain a buffer, actually, for each triangle.

433
00:27:29,996 --> 00:27:31,256
And we don't fetch the vertices.

434
00:27:32,897 --> 00:27:35,277
And we have a buffer with per triangle normals

435
00:27:35,317 --> 00:27:37,738
that we fetch and do the triangle normal culling.

436
00:27:39,458 --> 00:27:40,758
So we go from this pass buffer.

437
00:27:40,818 --> 00:27:42,719
We do all this type of culling.

438
00:27:43,439 --> 00:27:47,600
Then we're going to fill up a multi-draw parameter buffer.

439
00:27:48,400 --> 00:27:51,641
And for each batch, it's going to be a multi-draw indirect

440
00:27:51,701 --> 00:27:52,281
with all.

441
00:27:52,735 --> 00:27:53,756
The parameter is laid out.

442
00:28:02,637 --> 00:28:06,298
And then on the draw call itself,

443
00:28:07,899 --> 00:28:13,100
what we do is we start from the start instance

444
00:28:14,060 --> 00:28:15,540
that we fill out during the calling.

445
00:28:16,730 --> 00:28:21,452
to fetch the submesh instance index, which actually the sort instance is directly the

446
00:28:21,472 --> 00:28:29,293
submesh instance index. And that is used to fetch the unified constant buffer for that

447
00:28:29,393 --> 00:28:35,015
instance, which is going to help us through an indirection get the mesh descriptor. And

448
00:28:35,115 --> 00:28:41,957
using the sort index, which is SV vertex ID, and the dynamic index buffer and the unified

449
00:28:42,317 --> 00:28:44,277
vertex buffer, we're going to fetch the vertex data.

450
00:28:46,195 --> 00:28:51,560
So in the end, we run our vertex shader and we fetch all of the data ourselves.

451
00:28:53,081 --> 00:28:53,221
Yeah.

452
00:28:54,522 --> 00:29:01,228
So, vertex shader is going to pretty much consist of this fixed input,

453
00:29:02,409 --> 00:29:08,714
which is the per instance primitive info that we're going to just fill out with the global instance index of each submesh

454
00:29:09,295 --> 00:29:12,297
and then the hardware generated vertex ID.

455
00:29:14,799 --> 00:29:20,269
And then we already generated the code to actually fetch from the global buffer

456
00:29:22,292 --> 00:29:26,319
using those indices, the vertex info by itself.

457
00:29:30,833 --> 00:29:34,777
Then, to pass on the data from the vertex shader to the pixel shader,

458
00:29:35,317 --> 00:29:38,920
since we don't want per pixel to actually fetch all the indirections,

459
00:29:39,361 --> 00:29:44,185
we pack up all the resultant indices that are going to help us

460
00:29:45,166 --> 00:29:49,490
get any kind of info used in the vertex shader in a non-interpolated

461
00:29:53,014 --> 00:29:55,436
attribute passed on from the vertex shader to the pixel shader.

462
00:29:56,712 --> 00:29:58,352
And what's important to know,

463
00:29:58,392 --> 00:29:59,472
this is a good optimization,

464
00:29:59,532 --> 00:30:02,173
is actually use read first lane on that value,

465
00:30:02,593 --> 00:30:04,253
since you want everything in there

466
00:30:04,313 --> 00:30:06,294
to be in the scalar registers.

467
00:30:07,614 --> 00:30:12,295
And we got pretty good wins on Xbox One and PS4

468
00:30:12,315 --> 00:30:12,895
by using that.

469
00:30:17,316 --> 00:30:20,597
So here are an example of, like, doing destruction

470
00:30:20,837 --> 00:30:23,458
and amount of draw calls added to the scene.

471
00:30:23,498 --> 00:30:25,338
So just took a...

472
00:30:26,111 --> 00:30:33,255
walls that are similar in material, shot a couple of explosives into them, and we just added five

473
00:30:33,295 --> 00:30:40,059
draw calls compared to the original scene that we were drawing. So batch visualization pretty much

474
00:30:40,099 --> 00:30:45,722
that shows you the color coding of the batch. You can see all the debris sharing the same batch.

475
00:30:46,263 --> 00:30:50,525
Also the decals that are drawn on top of the debris are also using the same pipeline.

476
00:30:51,388 --> 00:30:53,508
We have some tricks to actually do sorting

477
00:30:54,748 --> 00:30:59,029
per batch level on them, and we pretty much impose some

478
00:30:59,109 --> 00:31:02,830
constraints on the artists, on the number of levels

479
00:31:02,850 --> 00:31:06,411
that they can have on their decals.

480
00:31:07,631 --> 00:31:11,752
And actually, the sorting of the decals is kind of fixed,

481
00:31:11,772 --> 00:31:14,452
so they have to choose from the beginning

482
00:31:14,532 --> 00:31:17,433
that you're always going to have decal type 1

483
00:31:18,413 --> 00:31:19,413
on top of decal type 2.

484
00:31:20,165 --> 00:31:21,365
or whatever happens.

485
00:31:24,586 --> 00:31:30,888
So for this scene, for example, we go from 10,500 draw calls.

486
00:31:31,608 --> 00:31:36,109
Like, those are draw calls that we would do if we

487
00:31:36,129 --> 00:31:37,069
didn't have the whole system.

488
00:31:40,058 --> 00:31:44,501
they correspond actually to the submesh instances

489
00:31:44,541 --> 00:31:45,982
that we're gonna draw on the scene in the end.

490
00:31:48,224 --> 00:31:53,608
From that, we end up with 412 batch draw calls

491
00:31:53,688 --> 00:31:55,850
for the opac, decals, and visibility pass.

492
00:31:57,230 --> 00:31:59,291
VCTPath is usually on our renderer.

493
00:32:00,012 --> 00:32:03,975
Just six, I think, to eight draw calls on average.

494
00:32:04,135 --> 00:32:05,576
Like, something really ridiculous.

495
00:32:06,877 --> 00:32:10,040
And 64 draw calls for the shadows.

496
00:32:11,521 --> 00:32:15,444
And from beginning to start, we managed to drop off 73%

497
00:32:16,144 --> 00:32:18,286
of all the triangles that were originally on the scene,

498
00:32:18,646 --> 00:32:23,170
knowing that we already do core sculling on CPU to start up.

499
00:32:25,371 --> 00:32:26,412
So one problem that

500
00:32:27,587 --> 00:32:32,189
we shipped with and we're working actually to improve in future updates

501
00:32:32,229 --> 00:32:34,950
is that we're pushing a lot of empty draw calls. Empty draw calls have a cost.

502
00:32:35,470 --> 00:32:40,311
So just like the command buffer running on those empty commands

503
00:32:40,792 --> 00:32:45,193
is going to take time and it's time that the GPU is doing nothing.

504
00:32:45,213 --> 00:32:49,414
We try to hide it with async jobs, but it's not enough.

505
00:32:49,474 --> 00:32:52,075
It's actually if the GPU can manage to work, it's better to do work.

506
00:32:53,236 --> 00:33:01,922
So, the next thing is pretty much just be able to specify the final number of draw calls on the GPU,

507
00:33:02,242 --> 00:33:08,947
and when we know that we called on the GPU like 10 out of 20 draw calls,

508
00:33:08,967 --> 00:33:15,431
we're just going to do an issue, those 10 draw calls, instead of issuing the whole 20 draw calls,

509
00:33:15,491 --> 00:33:17,152
and have some of them with zero indices.

510
00:33:20,634 --> 00:33:25,780
We can pretty much move to buying these resources to improve the batching.

511
00:33:27,762 --> 00:33:35,651
We still don't see the benefits right now for it, but it can help us reduce even further

512
00:33:35,691 --> 00:33:37,092
more the load on the CPU.

513
00:33:38,690 --> 00:33:43,895
And we want to move all the traversal of our scene graph

514
00:33:45,036 --> 00:33:47,338
to the LOD, pretty much have all the scene

515
00:33:47,378 --> 00:33:48,639
being descripted on GPU.

516
00:33:48,659 --> 00:33:50,641
The CPU doesn't know pretty much anything about it.

517
00:33:51,722 --> 00:33:53,864
Right now we have very complex CPU code

518
00:33:53,924 --> 00:33:55,125
that does LOD selection.

519
00:33:55,405 --> 00:33:56,627
We're moving it to the GPU,

520
00:33:56,987 --> 00:33:58,829
and then we're gonna do LOD selection on the GPU,

521
00:33:58,909 --> 00:34:02,472
and everything's gonna go without any CPU intervention.

522
00:34:06,840 --> 00:34:09,983
So the last part of the talk, which is checkerboard rendering.

523
00:34:13,265 --> 00:34:20,511
So for us, as I said, 60 FPS was important for adversarial PvP games.

524
00:34:20,911 --> 00:34:22,833
So we wanted to target early on in production.

525
00:34:24,194 --> 00:34:24,414
So our...

526
00:34:25,167 --> 00:34:29,008
we're working hard like pretty much to reach it like in our first label we

527
00:34:29,088 --> 00:34:29,789
managed to have

528
00:34:30,269 --> 00:34:35,051
on 50 FPS a couple of weeks later we ran 60 FPS and afterwards

529
00:34:35,071 --> 00:34:35,592
we're just like

530
00:34:36,052 --> 00:34:38,633
pushed to maintain that frame rate so anything that goes in

531
00:34:39,093 --> 00:34:42,495
in the engine or in the art have to have to be performance improved

532
00:34:43,495 --> 00:34:43,815
approved

533
00:34:46,217 --> 00:34:51,259
and to be able to have like a quick jump on the frame rate

534
00:34:52,329 --> 00:34:57,493
We started thinking out of the box, like Killzone came out when we were pretty much working

535
00:34:58,653 --> 00:35:03,457
on those techniques, and one interesting thing that they were doing is pretty much rendering

536
00:35:03,617 --> 00:35:04,698
half the pixels on screen.

537
00:35:05,378 --> 00:35:11,063
On their adversarial games, they keep the same cost per pixel as a 30 FPS game.

538
00:35:12,156 --> 00:35:13,957
But their technique was AQA based.

539
00:35:14,657 --> 00:35:17,658
And we wanted our stuff to run on PC too,

540
00:35:18,058 --> 00:35:21,139
since we wanted really to have the best performance possible

541
00:35:21,179 --> 00:35:24,160
on low-end PC and have decent frame rates

542
00:35:24,220 --> 00:35:25,681
with decent GPU on 4K.

543
00:35:29,402 --> 00:35:30,822
Was a big quick win for us.

544
00:35:32,263 --> 00:35:33,543
We just enabled it like.

545
00:35:34,199 --> 00:35:35,660
without telling anyone on the floor that

546
00:35:36,621 --> 00:35:38,482
what has been enabled just to see if people will notice

547
00:35:38,862 --> 00:35:40,624
like any quality drop.

548
00:35:41,824 --> 00:35:46,047
Luckily, people noticed like just the increase in frame rate, which is good.

549
00:35:48,209 --> 00:35:51,271
So, what we implemented was pretty much

550
00:35:51,931 --> 00:35:54,333
instead of rendering to 1080p

551
00:35:55,414 --> 00:35:56,835
render target, we

552
00:35:58,696 --> 00:36:00,137
reduced the resolution on the

553
00:36:00,698 --> 00:36:01,478
wide axis to 960x1080.

554
00:36:04,740 --> 00:36:07,361
We maintain a velocity vector per pixel, which is 3D.

555
00:36:07,701 --> 00:36:11,802
And we have our R12, G12, B8 format.

556
00:36:13,063 --> 00:36:15,663
That's going to have pretty much the implementation of it

557
00:36:15,803 --> 00:36:17,924
on the bonus lab, if you're interested.

558
00:36:18,564 --> 00:36:21,105
Then each frame, we just offset the projection matrix

559
00:36:21,285 --> 00:36:26,967
to render either the even or the odd lines.

560
00:36:28,836 --> 00:36:33,098
And one important thing to note is that to get correct texture gradients,

561
00:36:33,738 --> 00:36:35,219
you need to fix them up.

562
00:36:35,760 --> 00:36:38,801
Since the scaling is not uniform between the two axes,

563
00:36:39,762 --> 00:36:43,443
we need to divide the ddx by two when doing the sampling,

564
00:36:43,463 --> 00:36:46,445
so we need to go through all our shaders and fix them up

565
00:36:46,925 --> 00:36:48,266
depending on what they're doing on screen.

566
00:36:52,323 --> 00:36:57,549
So, of course, things that are not represented by motion need to be dealt with.

567
00:36:57,629 --> 00:37:01,533
So, lighting changes, shadow changes, transparency.

568
00:37:02,574 --> 00:37:05,057
We started doing a lot of work.

569
00:37:06,899 --> 00:37:09,700
to maintain a history buffer just of lights and shadows

570
00:37:10,640 --> 00:37:13,702
and try to determine if for a given missing pixel

571
00:37:13,922 --> 00:37:19,565
there has been like a change just on either of those

572
00:37:19,665 --> 00:37:23,066
to better fill that missing pixel.

573
00:37:24,089 --> 00:37:26,831
It was taking too much GPU time and too much memory.

574
00:37:27,191 --> 00:37:29,333
Additional memory that we have to maintain between frames.

575
00:37:29,773 --> 00:37:32,776
We end up pretty much just relying on color clamping

576
00:37:33,276 --> 00:37:36,839
most of the time, as explained in the Unreal TAA paper.

577
00:37:40,882 --> 00:37:42,824
Also, a lot of data needed to be tweaked.

578
00:37:43,404 --> 00:37:47,647
Artists like to do very sudden effects,

579
00:37:48,007 --> 00:37:49,869
and usually those effects are the ones

580
00:37:49,909 --> 00:37:51,390
that are not represented in the...

581
00:37:53,231 --> 00:37:54,912
with velocities, like they're gonna do

582
00:37:54,972 --> 00:37:58,732
like one frame flashes, particles that are gonna change

583
00:37:58,932 --> 00:38:03,393
per frame, so you can have that if you want

584
00:38:03,433 --> 00:38:07,134
to have decent quality, so a lot of stuff

585
00:38:07,174 --> 00:38:09,194
needed to pretty much be tweaked to have

586
00:38:09,774 --> 00:38:12,475
like a less harsh transition, and all our

587
00:38:12,855 --> 00:38:15,596
like engine oscillators, we just hack them

588
00:38:15,656 --> 00:38:17,476
to never have a zero to one transition.

589
00:38:17,916 --> 00:38:20,157
It always go to a smoother curve.

590
00:38:22,038 --> 00:38:24,419
Still had a lot of issue with aliasing on vertical lines.

591
00:38:24,939 --> 00:38:30,062
And fixing that wasn't easy after all,

592
00:38:30,202 --> 00:38:34,044
and took us, I think, a couple of months

593
00:38:34,144 --> 00:38:37,786
to get to something working properly.

594
00:38:38,446 --> 00:38:41,328
But we were not satisfied with the aliasing properties

595
00:38:41,468 --> 00:38:42,448
of the technique we chose.

596
00:38:43,749 --> 00:38:46,470
So this is where checkerboard rendering comes.

597
00:38:47,728 --> 00:38:50,871
So the base idea was to actually solve the aliasing issues.

598
00:38:51,672 --> 00:38:53,513
We experimented on a series of images

599
00:38:54,835 --> 00:38:57,777
and compared the signal to noise ratio

600
00:38:58,458 --> 00:39:04,283
and was usually better if we were using checkerboard pattern

601
00:39:04,323 --> 00:39:06,906
to interpolate between the missing pixels.

602
00:39:09,328 --> 00:39:13,152
So the idea of using MSA2X was bouncing around.

603
00:39:14,972 --> 00:39:19,375
since the beginning, and we made that push for it for E3 2015, and in two weeks we switched

604
00:39:19,975 --> 00:39:21,797
to the technique that I'm going to describe right now.

605
00:39:23,017 --> 00:39:24,498
So here is a couple of examples.

606
00:39:24,538 --> 00:39:32,504
So we took some images, took off the vertical lines, the odd vertical lines, and we reconstruct

607
00:39:32,824 --> 00:39:37,046
the image from the neighbors, the direct neighbors of each pixel.

608
00:39:37,705 --> 00:39:40,586
So you can see line versus checkerboard.

609
00:39:41,227 --> 00:39:43,668
Like, there's less visible aliasing on the image.

610
00:39:44,328 --> 00:39:45,448
Sorry about the moire.

611
00:39:47,069 --> 00:39:50,170
The resolution of those screens are not high enough

612
00:39:50,750 --> 00:39:51,851
to display the image properly.

613
00:39:53,311 --> 00:39:55,572
So, other example.

614
00:39:56,272 --> 00:39:58,533
Checkerboard versus line. Less aliasing.

615
00:40:00,474 --> 00:40:03,135
Like, on a computer-generated image, less aliasing.

616
00:40:05,276 --> 00:40:07,177
Sorry. This actually shows the difference.

617
00:40:11,654 --> 00:40:14,235
So, how do we manage that?

618
00:40:14,315 --> 00:40:17,656
So, we start by rendering to a Quartrez render target.

619
00:40:18,117 --> 00:40:20,998
This Quartrez render target is gonna be MSAA to X.

620
00:40:21,398 --> 00:40:25,320
So, Quartrez multiplied by two, half the pixels.

621
00:40:25,880 --> 00:40:28,421
Pretty much ends up being the same amount of pixels

622
00:40:29,202 --> 00:40:32,743
as the previous technique.

623
00:40:35,222 --> 00:40:40,904
So we just used a standard MSAA pattern, which has two color

624
00:40:40,924 --> 00:40:44,786
and z samples, which is perfect.

625
00:40:44,926 --> 00:40:47,887
We want this thing to run on PC, and that works on PC.

626
00:40:49,848 --> 00:40:53,289
And we just need to run our pixel shaders per sample

627
00:40:53,329 --> 00:40:57,551
instead of per pixel, because we still want the frequency of

628
00:40:57,571 --> 00:41:01,833
shading to happen on each of those samples we are rendering.

629
00:41:03,251 --> 00:41:05,035
So in the end, each of those samples

630
00:41:05,075 --> 00:41:07,762
is going to line up exactly with one pixel

631
00:41:07,942 --> 00:41:09,526
on the final render target.

632
00:41:11,250 --> 00:41:13,054
So checkerboard rendering.

633
00:41:15,465 --> 00:41:18,227
The good thing that we get with that is that on our particles,

634
00:41:18,607 --> 00:41:21,228
we can just, instead of evaluating the pixel shader

635
00:41:21,268 --> 00:41:23,129
per sample, can do it per pixel.

636
00:41:23,550 --> 00:41:25,231
We do that for smoke and other effects.

637
00:41:25,671 --> 00:41:28,933
So we don't have to deal with sorting between small particle

638
00:41:28,973 --> 00:41:31,634
buffers and normal particles.

639
00:41:32,274 --> 00:41:36,176
Everything that needs to go full quarter res,

640
00:41:36,717 --> 00:41:40,859
it's just going to be evaluated per pixel instead of per sample.

641
00:41:41,519 --> 00:41:43,360
Also, we can fit a lot more stuff on the ESRAM.

642
00:41:44,443 --> 00:41:48,605
So all of our render targets are actually half the size, which is good.

643
00:41:49,385 --> 00:41:51,986
And we don't need to mix up the gradients anymore.

644
00:41:52,507 --> 00:41:52,687
Why?

645
00:41:53,927 --> 00:41:57,069
Because right now, the scaling becomes uniform.

646
00:41:57,629 --> 00:42:01,511
So instead of dividing the dx, dy, which actually costs some AU,

647
00:42:02,771 --> 00:42:08,594
we just bias the MIPS by one, and we have the same result.

648
00:42:09,435 --> 00:42:14,499
So instead of being some cost that we do on the CPU, on the GPU,

649
00:42:15,600 --> 00:42:17,381
it just, like all of our samplers,

650
00:42:19,883 --> 00:42:23,086
change the MIB bias when we switch on this technique.

651
00:42:23,686 --> 00:42:25,748
Actually, this technique can be switched on and off on PC.

652
00:42:29,571 --> 00:42:31,853
So, consoles give you the control.

653
00:42:32,671 --> 00:42:42,083
of choosing your sample locations. It's not possible on all GPUs on PCs. I think it's only on Maxwell on NVIDIA.

654
00:42:44,005 --> 00:42:47,149
But it has been there for a long time on AMD, actually.

655
00:42:49,237 --> 00:42:55,438
To unify our implementation, instead of switching the sample location, we just offset

656
00:42:57,779 --> 00:43:01,339
the same way we were doing before, between two frames.

657
00:43:01,840 --> 00:43:07,321
So it's going to yield the same result, but we're going to end up with one missing line

658
00:43:07,361 --> 00:43:12,442
that we have to do proper clamping when we actually fetch the values.

659
00:43:16,867 --> 00:43:20,808
So to reconstruct the missing pixels,

660
00:43:21,328 --> 00:43:23,569
which are represented here by P and Q,

661
00:43:25,489 --> 00:43:28,710
we sample the current frame direct neighbor

662
00:43:29,550 --> 00:43:32,211
are in linear Z.

663
00:43:32,271 --> 00:43:38,712
Yeah, we sample the linear Z direct neighbors.

664
00:43:38,912 --> 00:43:42,053
We also sample the direct neighbors colors

665
00:43:42,153 --> 00:43:43,153
for each missing pixel.

666
00:43:44,227 --> 00:43:46,731
and also we compute a history color,

667
00:43:46,872 --> 00:43:48,955
there's a seamless thing, in Z.

668
00:43:54,885 --> 00:43:57,931
So, to compute this history color...

669
00:43:59,621 --> 00:44:05,626
We're going to select one of the direct neighbors' velocity

670
00:44:05,686 --> 00:44:06,587
to do reprojection.

671
00:44:07,127 --> 00:44:08,328
So we choose.

672
00:44:08,668 --> 00:44:09,869
We tested a couple of stuff.

673
00:44:09,889 --> 00:44:13,732
We ended up just choosing the closest one to the camera,

674
00:44:13,752 --> 00:44:17,295
since we always want to preserve the first person object

675
00:44:17,875 --> 00:44:18,316
on screen.

676
00:44:18,876 --> 00:44:22,559
And you don't want a silhouette to be mixed up with other stuff.

677
00:44:22,599 --> 00:44:24,981
So we chose the closest pixel.

678
00:44:28,427 --> 00:44:30,067
currently rendered pixel to the camera.

679
00:44:32,748 --> 00:44:34,509
And with that motion velocity,

680
00:44:34,709 --> 00:44:38,111
we sampled the previously resolved color,

681
00:44:38,131 --> 00:44:40,872
so the full resolution render target.

682
00:44:41,352 --> 00:44:45,634
So, we use binary filtering.

683
00:44:46,194 --> 00:44:47,955
Actually, I think we need to try

684
00:44:48,055 --> 00:44:50,816
to use catMirrorROM interpolation.

685
00:44:51,536 --> 00:44:52,957
I heard it's gonna yield better results.

686
00:44:54,408 --> 00:44:56,790
But that can introduce accumulation color.

687
00:44:56,810 --> 00:44:59,533
We didn't see that much effect on our rendering,

688
00:44:59,973 --> 00:45:04,377
but it's a possibility that we didn't actually run into.

689
00:45:06,198 --> 00:45:08,881
And then we clamped the reprojected color,

690
00:45:08,901 --> 00:45:12,023
that's history color we computed to the neighbors,

691
00:45:12,684 --> 00:45:14,786
direct neighbors, like for Q it's A, B, E, F.

692
00:45:17,675 --> 00:45:20,157
And then using the previous step,

693
00:45:20,317 --> 00:45:22,298
because we maintain a 3D velocity vector,

694
00:45:22,638 --> 00:45:26,920
we determine if the pixel was hidden next frame.

695
00:45:27,400 --> 00:45:32,643
And we blend back between the unclamped and clamped value,

696
00:45:33,023 --> 00:45:37,185
depending on how sure are we of that pixel being hidden

697
00:45:37,546 --> 00:45:39,447
or included in the previous frame.

698
00:45:43,359 --> 00:45:47,983
So now having this history color and the interpolated direct neighbors,

699
00:45:51,966 --> 00:45:53,548
the interpolated color from direct neighbors,

700
00:45:54,409 --> 00:45:58,672
we define a weight that we're going to use to blend between the two.

701
00:45:59,353 --> 00:46:03,656
Since, like usually, when you have transparency, when you have lighting changes,

702
00:46:05,038 --> 00:46:07,580
it's always, or...

703
00:46:08,972 --> 00:46:13,199
Most of the time, it's better to just use the interpolated color

704
00:46:13,540 --> 00:46:14,882
instead of the reprojected color.

705
00:46:14,902 --> 00:46:17,646
It's going to yield better results on the image.

706
00:46:19,340 --> 00:46:24,203
So depending on the minimum difference that we have between those neighbors,

707
00:46:25,944 --> 00:46:29,907
we're going to define a weight, and we combine it with different weights

708
00:46:29,947 --> 00:46:31,848
depending on the magnitude of the velocity.

709
00:46:31,888 --> 00:46:37,552
So the bigger the velocity, the more we're going to tend toward the interpolated color

710
00:46:37,612 --> 00:46:39,853
instead of the clamped color, historic color.

711
00:46:42,111 --> 00:46:49,699
And we, using those two weights, we just write out the final color.

712
00:46:52,408 --> 00:46:54,749
So here, I'm not going to explain this diagram.

713
00:46:54,769 --> 00:46:58,391
I'd just like to show you the complete pipeline that we go

714
00:46:58,411 --> 00:47:02,172
through to determine this missing color.

715
00:47:02,232 --> 00:47:06,154
It's actually a lot of passes and a lot of determinations.

716
00:47:06,674 --> 00:47:13,976
And we have a lot of knobs to either have more ghosting or

717
00:47:14,056 --> 00:47:17,558
more flickering, depending on the situation on the map.

718
00:47:19,645 --> 00:47:23,087
So the result is quite context, and we did a lot of tweaks

719
00:47:23,407 --> 00:47:26,289
to get it right for our rendering.

720
00:47:27,810 --> 00:47:32,693
It runs in full res, and it costs about 1.4 milliseconds

721
00:47:32,773 --> 00:47:33,393
on Xbox One.

722
00:47:34,574 --> 00:47:39,237
And we have a net win of 8 to 10 milliseconds each frame.

723
00:47:41,659 --> 00:47:44,901
So on top of that, we run TAA.

724
00:47:46,286 --> 00:47:49,329
So it integrates well with checkerboard renderings.

725
00:47:49,630 --> 00:47:55,276
We're already doing the whole clamping and velocity vector reprojection.

726
00:47:56,016 --> 00:47:58,019
And actually it can run on the same shader.

727
00:47:59,520 --> 00:48:02,203
But we separate it just also for VGPR usage.

728
00:48:04,626 --> 00:48:04,786
So...

729
00:48:06,557 --> 00:48:08,318
It's kind of like, imagine that our pixel

730
00:48:08,359 --> 00:48:10,140
is actually one of our samples.

731
00:48:10,180 --> 00:48:13,502
So per sub-sample level, we have this flipboard,

732
00:48:13,582 --> 00:48:17,585
MSAA4X style jitter that we're gonna just

733
00:48:17,885 --> 00:48:19,066
flip through each frame.

734
00:48:19,526 --> 00:48:22,308
So we have a total cycle of eight frames

735
00:48:23,129 --> 00:48:25,911
to determine the final color of one pixel.

736
00:48:28,693 --> 00:48:32,615
And the reprojected color uses similar weighting logic

737
00:48:32,655 --> 00:48:35,017
that we use for determining the missing pixels.

738
00:48:36,973 --> 00:48:44,617
And on top of that, we run this unteaten algorithm

739
00:48:45,637 --> 00:48:48,419
to smooth out some of the imperfections

740
00:48:49,559 --> 00:48:51,520
that we have when we're doing the resolve.

741
00:48:52,040 --> 00:48:53,381
So because during the resolve, we're

742
00:48:53,401 --> 00:48:55,622
going to interleave perfect pixels that

743
00:48:55,682 --> 00:48:57,343
were rendered this frame we're sure about

744
00:48:57,723 --> 00:48:59,464
to imperfect pixels that were projected

745
00:48:59,524 --> 00:49:02,385
and that went through a lot of history.

746
00:49:05,086 --> 00:49:08,008
So we apply a filter to detect patterns

747
00:49:08,348 --> 00:49:10,729
that we might introduce when doing the resolve.

748
00:49:11,729 --> 00:49:15,051
So that filter, the detector filter,

749
00:49:15,171 --> 00:49:19,513
is gonna run on five horizontal and vertical adjacent pixels.

750
00:49:20,774 --> 00:49:24,636
We determine a threshold, and depending on that threshold,

751
00:49:24,816 --> 00:49:27,137
and if each of the five pixels

752
00:49:27,257 --> 00:49:29,078
either are above or under that threshold.

753
00:49:29,820 --> 00:49:35,643
we generate a mask of either 01010 or 100101, anyway.

754
00:49:37,704 --> 00:49:41,566
And we detect that pretty much we have this

755
00:49:41,606 --> 00:49:45,908
teething pattern on screen.

756
00:49:46,109 --> 00:49:48,530
And if that's the case, we just blend more between

757
00:49:48,910 --> 00:49:50,631
the different samples to try to remove it.

758
00:49:51,211 --> 00:49:54,033
So it's more blurriness, but it's for better

759
00:49:54,733 --> 00:49:55,673
image quality in the end.

760
00:49:56,785 --> 00:50:00,971
So the overall technique was very good for us.

761
00:50:01,011 --> 00:50:05,698
We managed to get a lot of GPU time out of it.

762
00:50:07,621 --> 00:50:10,866
Similar quality, like it's close enough to that.

763
00:50:11,928 --> 00:50:17,454
For us 3D programmers, of course, we're going to see that there is something being done on the image,

764
00:50:17,494 --> 00:50:23,180
but for the average user, and it just means better frame rates, which is what we were targeting.

765
00:50:23,340 --> 00:50:30,667
And it's really interesting when you want to target 4K, because instead of having this 4x resolution scaling, it's just 2x.

766
00:50:33,497 --> 00:50:35,277
The implementation wasn't very scientific.

767
00:50:35,297 --> 00:50:37,898
It was pretty much trial and error, just testing stuff,

768
00:50:37,918 --> 00:50:39,259
seeing if it looks better on our game.

769
00:50:39,739 --> 00:50:41,919
So maybe we need to define a better framework

770
00:50:41,979 --> 00:50:46,201
for what we did and have more scientific approach

771
00:50:46,361 --> 00:50:49,462
of whatever weights we choose to have in the technique.

772
00:50:51,122 --> 00:50:56,064
And pretty much we're still gonna like run with it.

773
00:50:57,116 --> 00:50:59,437
even if we have better GPUs or whatever,

774
00:50:59,798 --> 00:51:01,419
just because it allows us to actually push

775
00:51:01,439 --> 00:51:04,660
the quality per pixel, which is interesting.

776
00:51:07,182 --> 00:51:10,504
So, special thanks to all those guys

777
00:51:10,524 --> 00:51:12,765
that helped on the presentation,

778
00:51:13,666 --> 00:51:16,167
and also for the insights on the techniques.

779
00:51:17,568 --> 00:51:18,108
Thank you guys.

780
00:51:19,569 --> 00:51:19,889
Questions?

781
00:51:35,146 --> 00:51:36,487
Hey, great talk.

782
00:51:37,868 --> 00:51:40,749
From playing Rainbow Six, I noticed there's no motion blur

783
00:51:40,769 --> 00:51:41,190
in the game.

784
00:51:41,250 --> 00:51:43,171
Is that because of the checkerboard pattern?

785
00:51:45,252 --> 00:51:47,874
It was actually a choice, the art direction choice, not to

786
00:51:47,934 --> 00:51:50,755
have motion blur.

787
00:51:51,596 --> 00:51:57,500
And the technique actually, and TAA by itself, introduces.

788
00:51:58,646 --> 00:52:01,428
uh... this kind of like a brand and on the image that's

789
00:52:02,469 --> 00:52:05,731
uh... i think was enough of loren for uh... for

790
00:52:06,551 --> 00:52:07,191
for what we have

791
00:52:07,672 --> 00:52:07,872
so

792
00:52:08,492 --> 00:52:11,034
was uh... idea can kind of like an art

793
00:52:11,394 --> 00:52:13,916
direction choice not to have motion blur

794
00:52:14,256 --> 00:52:17,318
actually makes sense to have motion blur since it's going to help smooth out

795
00:52:17,998 --> 00:52:19,479
uh... the results and lessen

796
00:52:19,979 --> 00:52:21,761
any artifact you might have from

797
00:52:22,141 --> 00:52:23,041
implementing the technique

798
00:52:23,922 --> 00:52:24,322
good thanks

799
00:52:26,827 --> 00:52:27,828
Thank you, great talk.

800
00:52:29,330 --> 00:52:32,914
So the checkerboard rendering introduces inter-frame dependency.

801
00:52:33,915 --> 00:52:36,077
How do you deal with multi-GPU rendering?

802
00:52:36,918 --> 00:52:42,744
So it's one RGBA8 buffer that we have to sync between frames.

803
00:52:43,385 --> 00:52:47,529
So on Rainbow Six, we disable all...

804
00:52:48,570 --> 00:52:50,111
all driver syncing.

805
00:52:51,051 --> 00:52:53,832
And it's like the driver is not aware of any of the buffers,

806
00:52:53,872 --> 00:52:55,593
so we just disable, like, hardware handling,

807
00:52:56,213 --> 00:52:57,614
driver handling from our resources,

808
00:52:57,694 --> 00:52:59,055
and we do our syncing ourselves.

809
00:53:00,215 --> 00:53:02,756
It's gonna cost, but we pretty much managed

810
00:53:02,796 --> 00:53:05,318
to have the scaling on other parts of our rendering,

811
00:53:06,858 --> 00:53:09,239
and we still end up syncing that buffer,

812
00:53:10,260 --> 00:53:10,520
which is...

813
00:53:11,401 --> 00:53:13,743
one, let's say, of the few buffers we are thinking.

814
00:53:13,763 --> 00:53:16,185
Uh, the technique we use for other stuff

815
00:53:16,225 --> 00:53:19,327
is pretty much redoing the work each frame on each GPU

816
00:53:19,367 --> 00:53:21,889
rather than having the between-frame dependency.

817
00:53:21,909 --> 00:53:23,891
man 2 in audience Thank you.

818
00:53:23,911 --> 00:53:29,255
No problem.

819
00:53:29,275 --> 00:53:29,756
Other questions?

820
00:53:29,776 --> 00:53:34,900
I guess that's it.

821
00:53:35,620 --> 00:53:36,081
Cool, thank you.

822
00:53:36,101 --> 00:53:36,681
I'll...

823
00:53:40,396 --> 00:53:48,399
I'll be at the Ubisoft lounge tomorrow at 11 if anyone have additional questions or just want to talk with me, I'll be like on the wrap-up room right now. Thank you guys.

