1
00:00:06,137 --> 00:00:08,718
Welcome to our presentation on using AI

2
00:00:08,758 --> 00:00:11,499
to create digital avatars for AR and VR.

3
00:00:15,380 --> 00:00:16,360
My name is Kevin He.

4
00:00:17,081 --> 00:00:19,901
I started my career in gaming 10 years ago

5
00:00:20,402 --> 00:00:22,422
at Blizzard Entertainment as an engine developer

6
00:00:22,442 --> 00:00:23,162
for World of Warcraft.

7
00:00:23,803 --> 00:00:28,544
After that, I took technical roles in Roblox at Disney

8
00:00:29,264 --> 00:00:32,285
ever since I enjoyed working on entertainment technology.

9
00:00:33,443 --> 00:00:37,305
So five years ago, we started this company called Deep Motion.

10
00:00:37,725 --> 00:00:40,226
We focus on working AI for animation.

11
00:00:41,807 --> 00:00:46,430
Today, I'm going to introduce to you the technologies we

12
00:00:46,470 --> 00:00:52,273
have been working on and what we can apply AI and simulation

13
00:00:52,713 --> 00:00:54,534
to interactive avatars.

14
00:00:55,512 --> 00:00:58,375
So my talk will be divided into four modules.

15
00:00:58,956 --> 00:01:00,938
The first one on simulated motion.

16
00:01:01,379 --> 00:01:03,261
That's our first phase engine

17
00:01:03,581 --> 00:01:06,004
to use physics simulation to create the characters.

18
00:01:06,865 --> 00:01:09,247
The second one is on motion intelligence.

19
00:01:10,068 --> 00:01:12,591
That is an extension of our simulated motion

20
00:01:12,951 --> 00:01:16,295
to provide you more diversity in the character movement.

21
00:01:17,710 --> 00:01:20,371
The third one is about motion perception.

22
00:01:20,991 --> 00:01:24,113
That will add additional ability to the AI

23
00:01:24,433 --> 00:01:27,834
to understand how people, how animals move around us

24
00:01:28,374 --> 00:01:31,275
and apply that knowledge to enhance our motion brain.

25
00:01:32,115 --> 00:01:34,356
And in the last, I'll give a quick overview

26
00:01:34,516 --> 00:01:37,337
of the pipeline we're working on to help you

27
00:01:37,777 --> 00:01:41,439
to create interactive content for AR and VR.

28
00:01:42,639 --> 00:01:45,100
So let's start on the first module,

29
00:01:45,320 --> 00:01:46,400
which is simulated motion.

30
00:01:50,716 --> 00:01:52,179
First, I want to talk about what's

31
00:01:52,199 --> 00:01:55,784
the difference between keyframe animation and simulated motion.

32
00:01:56,887 --> 00:01:59,588
As we all know, keyframe is the standard tag

33
00:02:00,008 --> 00:02:02,209
for film and for game entertainment.

34
00:02:02,729 --> 00:02:06,351
But in general, keyframe animation, they are static.

35
00:02:06,871 --> 00:02:09,633
Every time you play, they behave the same,

36
00:02:09,773 --> 00:02:11,093
so that's repetitive.

37
00:02:11,694 --> 00:02:14,195
And keyframe take a lot of hand crafting to create.

38
00:02:14,815 --> 00:02:19,297
It's also not reactive to user input and the user influence,

39
00:02:19,837 --> 00:02:22,379
which is the quality you are looking for

40
00:02:22,559 --> 00:02:23,939
in AR and VR applications.

41
00:02:24,860 --> 00:02:27,341
On the right hand side, the simulated motion,

42
00:02:27,702 --> 00:02:30,203
we are looking at physics simulated movement.

43
00:02:30,624 --> 00:02:31,504
That's procedural.

44
00:02:31,984 --> 00:02:33,645
Every time you run a simulation,

45
00:02:33,766 --> 00:02:35,787
it generates a unique experience.

46
00:02:36,807 --> 00:02:38,949
It's interactive, it's reactive.

47
00:02:39,509 --> 00:02:42,511
When you engage the AR or VR characters

48
00:02:42,971 --> 00:02:44,212
created off simulation,

49
00:02:44,993 --> 00:02:46,374
you can see the immediate feedback,

50
00:02:46,794 --> 00:02:50,716
how the animals, how the character react to your input.

51
00:02:51,417 --> 00:02:53,558
So that's something we find very interesting.

52
00:02:54,299 --> 00:02:57,981
For that reason, we did a bunch of experiments

53
00:02:58,242 --> 00:02:59,262
in simulated motion.

54
00:03:01,544 --> 00:03:04,146
This is the following slides.

55
00:03:04,206 --> 00:03:06,448
We're going through the path we went through

56
00:03:06,628 --> 00:03:10,191
to achieve simulated motion for digital avatars.

57
00:03:11,952 --> 00:03:14,795
In order to have a fully physics-similar character,

58
00:03:14,935 --> 00:03:18,978
we first work on building a very robust physics engine for joint.

59
00:03:19,700 --> 00:03:20,240
for motors.

60
00:03:20,840 --> 00:03:23,301
As you can see in these slides, most of these demos,

61
00:03:23,701 --> 00:03:26,521
they are cars and vehicles, machines, right?

62
00:03:26,782 --> 00:03:27,822
So we started from here.

63
00:03:28,222 --> 00:03:31,543
We make a really robust, multi-joint physics simulation

64
00:03:31,723 --> 00:03:32,323
of machines.

65
00:03:33,623 --> 00:03:36,524
So that will lead us to the later stage,

66
00:03:36,684 --> 00:03:39,325
where we can add more intelligence on top of machine.

67
00:03:41,045 --> 00:03:43,246
Then the step two we went through

68
00:03:43,946 --> 00:03:45,246
is to apply.

69
00:03:47,516 --> 00:03:49,218
Apply the physics simulation on human.

70
00:03:49,578 --> 00:03:52,180
So first we need to create a physics-based character model.

71
00:03:52,741 --> 00:03:54,783
So this model is basically created

72
00:03:54,983 --> 00:03:58,306
with multiple rigid body representation.

73
00:03:58,827 --> 00:04:03,171
And all the rigid bodies are connected with the joints,

74
00:04:03,451 --> 00:04:06,474
just to simulate the biomechanical model of human.

75
00:04:07,395 --> 00:04:09,156
With such a physical model of human,

76
00:04:13,565 --> 00:04:15,747
The next step is to apply our physics simulation

77
00:04:15,907 --> 00:04:20,531
on this skeletal model, then want to see what you get.

78
00:04:21,052 --> 00:04:23,074
So the result is a ragdoll.

79
00:04:23,594 --> 00:04:25,556
As we know very well in gaming, it's

80
00:04:25,676 --> 00:04:29,279
a passive animation effect to simulate a character falling

81
00:04:29,320 --> 00:04:29,820
to the ground.

82
00:04:30,300 --> 00:04:31,922
So we do have some physics simulation.

83
00:04:31,962 --> 00:04:33,123
We have a ragdoll character.

84
00:04:33,984 --> 00:04:36,867
It's modeled with physics, but not so interesting yet.

85
00:04:37,587 --> 00:04:41,928
Then, step four, we need to apply muscle torque.

86
00:04:42,288 --> 00:04:44,089
We need to give the ragdoll some power

87
00:04:44,549 --> 00:04:47,570
to activate the ragdoll so it can stand up,

88
00:04:47,970 --> 00:04:50,070
so it can maintain its own balance.

89
00:04:50,870 --> 00:04:58,052
So, in this step, we basically apply a very simple muscle model

90
00:04:58,752 --> 00:05:00,193
at the joints of the skeleton.

91
00:05:01,071 --> 00:05:04,053
For those of you who are interested in the details,

92
00:05:04,434 --> 00:05:08,517
we apply a PD controller on every joint to drive.

93
00:05:09,057 --> 00:05:12,880
Imagine each joint is like a motor in a crane, right?

94
00:05:13,300 --> 00:05:15,982
So the PD controller will drive every joint

95
00:05:16,102 --> 00:05:20,966
of the virtual human being to match the target trajectory

96
00:05:21,426 --> 00:05:23,308
you want the robot to achieve.

97
00:05:24,148 --> 00:05:28,292
So with this very simple PD-based muscular model,

98
00:05:28,992 --> 00:05:29,893
we now can.

99
00:05:30,335 --> 00:05:31,536
can power the ragdoll.

100
00:05:31,956 --> 00:05:33,978
So the ragdoll will tighten up his muscle,

101
00:05:34,658 --> 00:05:37,000
and we do physics simulation.

102
00:05:37,680 --> 00:05:40,602
So you'll get someone who can hold his posture,

103
00:05:41,042 --> 00:05:42,383
but still not standing there.

104
00:05:43,043 --> 00:05:46,225
The remaining problem is balancing.

105
00:05:47,806 --> 00:05:50,668
So in the fifth step of our simulated motion,

106
00:05:51,609 --> 00:05:54,310
we want to add a balancing to the digital avatar.

107
00:05:54,951 --> 00:05:56,292
And it's not actually.

108
00:05:57,757 --> 00:05:58,818
It's not very hard.

109
00:05:59,198 --> 00:06:01,420
If we study the human balance theory,

110
00:06:02,541 --> 00:06:06,984
when our center of mass is under gravity,

111
00:06:07,064 --> 00:06:11,527
when the gravity pulls us forward, we will flip forward.

112
00:06:12,287 --> 00:06:14,549
The problem is the projection center of mass

113
00:06:14,849 --> 00:06:17,711
is go outside of your support polygon,

114
00:06:17,851 --> 00:06:20,893
as shown by the red square on the ground.

115
00:06:21,354 --> 00:06:23,435
So all you need to do is just tighten the muscle.

116
00:06:24,693 --> 00:06:27,035
across your skeleton properly.

117
00:06:27,315 --> 00:06:29,216
So you can generate a ground reaction force,

118
00:06:29,697 --> 00:06:31,178
as shown by the green arrows.

119
00:06:31,958 --> 00:06:34,781
The aggregate effect of the ground reaction force

120
00:06:35,161 --> 00:06:38,944
will push your body back to a balanced posture.

121
00:06:40,045 --> 00:06:43,967
So after implementing this basic robotic balancing theory,

122
00:06:43,988 --> 00:06:45,589
we can get this.

123
00:06:46,029 --> 00:06:49,632
We can get a digital robot standing in place.

124
00:06:50,472 --> 00:06:54,555
performing some simple actions, such as rotating his waist,

125
00:06:55,176 --> 00:07:00,541
do a kick, or just do squatting without falling to the ground.

126
00:07:01,802 --> 00:07:05,405
So you can think of this like a virtual world

127
00:07:05,525 --> 00:07:08,588
analogy of Boston Dynamics robot in the physical world.

128
00:07:09,048 --> 00:07:13,272
Basically, this phase, we employed robotics balancing

129
00:07:13,372 --> 00:07:15,053
algorithm into our digital agent.

130
00:07:15,394 --> 00:07:17,756
We can get a standing in place balanced agent.

131
00:07:19,024 --> 00:07:21,405
And the final step, we want to add locomotion

132
00:07:21,605 --> 00:07:22,645
to this digital agent.

133
00:07:23,145 --> 00:07:24,706
So, it's not that hard.

134
00:07:25,346 --> 00:07:26,886
Basically, we can balance in space.

135
00:07:27,287 --> 00:07:29,187
We just need to lift one foot at a time

136
00:07:29,587 --> 00:07:31,248
and alternate between the two feet.

137
00:07:32,808 --> 00:07:36,549
And then you want to give it some forward push,

138
00:07:36,969 --> 00:07:40,290
you know, implement a foot planting logic

139
00:07:40,811 --> 00:07:42,971
to place your foot strategically

140
00:07:43,311 --> 00:07:44,352
as you move forward.

141
00:07:44,372 --> 00:07:45,832
Thanks very much.

142
00:07:46,741 --> 00:07:51,585
we implemented a very simple locomotion controller

143
00:07:52,045 --> 00:07:54,807
that can power the digital character physically

144
00:07:55,528 --> 00:07:58,910
and walk around while not falling to the ground.

145
00:08:01,032 --> 00:08:05,115
So, so far we implemented a physically simulated character

146
00:08:05,175 --> 00:08:06,736
that can do self-balancing and walking.

147
00:08:07,408 --> 00:08:11,449
Now, the question is, what can we do with this digital agent?

148
00:08:11,989 --> 00:08:14,010
So I want to show you a few applications

149
00:08:14,190 --> 00:08:16,971
in VR, which is our main interest area of today.

150
00:08:17,811 --> 00:08:20,252
The first application of simulated motion to VR

151
00:08:20,872 --> 00:08:22,412
is full-body motion tracking.

152
00:08:24,273 --> 00:08:27,394
Before going to the demos, I want to talk a little bit,

153
00:08:27,754 --> 00:08:30,015
why do we need full-body avatar in VR?

154
00:08:31,055 --> 00:08:33,036
We all saw that Facebook released this.

155
00:08:33,992 --> 00:08:38,314
awesome Facebook Horizon platform for social VR.

156
00:08:38,794 --> 00:08:42,235
But as you can see, in social VR scenario,

157
00:08:43,576 --> 00:08:48,537
if you only see the half body, the upper torso of your friends

158
00:08:49,097 --> 00:08:53,379
or fellow, that's not as believable

159
00:08:53,559 --> 00:08:55,040
as you hope it to be.

160
00:08:56,115 --> 00:08:58,476
So in multiplayer or social VR setup,

161
00:08:58,736 --> 00:09:02,117
you do want to see the full-body presence of your friends,

162
00:09:02,477 --> 00:09:06,638
you know, of your teammates in the virtual space.

163
00:09:07,358 --> 00:09:09,479
However, commercial VR device

164
00:09:10,079 --> 00:09:11,980
only offer you three point of tracking.

165
00:09:12,780 --> 00:09:16,981
Basically, you have one 6DOF tracker in your HMD device.

166
00:09:17,581 --> 00:09:20,922
You typically have two other 6DOF or 3DOF trackers.

167
00:09:22,407 --> 00:09:24,088
on the hand controllers you are holding.

168
00:09:24,688 --> 00:09:28,350
So in short, you usually have three-point tracking.

169
00:09:28,771 --> 00:09:31,712
You can utilize that to model the upper body movement.

170
00:09:32,333 --> 00:09:34,874
And then the key question is, how do you generate

171
00:09:35,295 --> 00:09:37,536
or synthesize the lower body movement?

172
00:09:38,016 --> 00:09:40,338
So together, you get a full body representation

173
00:09:40,498 --> 00:09:41,198
in virtual space.

174
00:09:41,598 --> 00:09:43,439
And that's the first application

175
00:09:43,620 --> 00:09:45,641
we are trying to apply simulation to.

176
00:09:48,701 --> 00:09:52,604
And we did receive questions from users,

177
00:09:52,984 --> 00:09:55,205
why don't we just use full-body mocap equipment?

178
00:09:55,406 --> 00:09:56,927
We know we can do that, right?

179
00:09:57,547 --> 00:10:03,551
We can apply the traditional optical-based motion capture

180
00:10:03,751 --> 00:10:05,152
or inertia-based motion capture.

181
00:10:05,192 --> 00:10:06,193
There are a lot of options.

182
00:10:06,993 --> 00:10:12,075
But in general, to apply a professional motion capture

183
00:10:12,135 --> 00:10:15,216
equipment to achieve full-body tracking, that's expensive.

184
00:10:15,616 --> 00:10:18,757
You have to set up a studio to do that.

185
00:10:18,877 --> 00:10:22,438
You need to purchase expensive hardware and software

186
00:10:22,558 --> 00:10:23,019
to do that.

187
00:10:23,439 --> 00:10:26,520
The post-processing is quite complicated.

188
00:10:26,880 --> 00:10:27,900
It takes a lot of time.

189
00:10:28,540 --> 00:10:32,101
And also, there's no single standard for motion capture.

190
00:10:32,261 --> 00:10:34,702
So it's very tailored from project to project.

191
00:10:37,320 --> 00:10:42,964
In short, you cannot really achieve it with consumer-level VR device, right?

192
00:10:43,525 --> 00:10:47,808
It doesn't provide enough sensors all over your body to do full-body motion capture.

193
00:10:48,489 --> 00:10:54,754
So, let's come back to how can we solve the problem in the context of consumer-level VR hardware.

194
00:10:55,635 --> 00:10:57,256
let's say you only have three tracking points,

195
00:10:57,537 --> 00:10:58,357
or four, or two.

196
00:10:58,598 --> 00:10:59,899
It's not so much different.

197
00:11:00,599 --> 00:11:03,222
The traditional technique to solve VR 3-point tracking

198
00:11:03,762 --> 00:11:04,543
is use IK.

199
00:11:05,184 --> 00:11:09,187
So IK is a shorthand for inverse kinematic.

200
00:11:10,088 --> 00:11:11,850
It's basically a mathematical solver

201
00:11:12,250 --> 00:11:15,113
to solve partial body or full body motion

202
00:11:15,213 --> 00:11:18,616
based on where your hands are, where your head is.

203
00:11:19,472 --> 00:11:20,653
It's an inverse algorithm.

204
00:11:21,273 --> 00:11:25,016
But IK is only looking at geometric constraint

205
00:11:25,236 --> 00:11:25,877
of human body.

206
00:11:26,457 --> 00:11:28,739
It doesn't really look at physics, dynamics,

207
00:11:28,939 --> 00:11:31,061
like mass properties, Newton laws.

208
00:11:31,702 --> 00:11:35,685
So with only geometric relationship considered,

209
00:11:36,325 --> 00:11:38,367
IK often have ambiguous solution.

210
00:11:38,887 --> 00:11:41,610
You have to tune IK to make it work right.

211
00:11:41,710 --> 00:11:43,451
We often have flip elbow,

212
00:11:43,891 --> 00:11:46,173
a lot of complex problem using IK.

213
00:11:47,234 --> 00:11:49,156
And also, IK only solves the upper body.

214
00:11:49,657 --> 00:11:52,119
It still cannot predict where the lower body are.

215
00:11:53,040 --> 00:11:56,864
So a standard technique to add lower body is use a keyframe.

216
00:11:58,486 --> 00:12:01,049
You can just keyframe a few fixed locomotion pattern

217
00:12:01,609 --> 00:12:05,413
and slap together with upper body IK driven.

218
00:12:06,332 --> 00:12:10,354
And then it works in some scenario,

219
00:12:10,634 --> 00:12:14,076
but it's hard to match the lower body and the upper body

220
00:12:14,496 --> 00:12:18,298
because keyframe-based lower body are just fixed animation, right?

221
00:12:18,658 --> 00:12:22,060
You often have foot gliding problem, self-penetration issue.

222
00:12:22,580 --> 00:12:26,602
It's sometimes really weird because it's just keyframe.

223
00:12:26,682 --> 00:12:28,323
It's just fixed solution.

224
00:12:28,823 --> 00:12:32,064
So although the traditional technique of IK plus keyframe...

225
00:12:33,054 --> 00:12:37,239
is a solution for full-body avatar tracking,

226
00:12:37,659 --> 00:12:39,861
but we think there can be a better way to do it.

227
00:12:40,418 --> 00:12:42,719
using inverse dynamic, using physics simulation.

228
00:12:43,279 --> 00:12:46,821
So in this example, we are experimenting,

229
00:12:46,921 --> 00:12:48,522
how can we use a simulated character

230
00:12:48,782 --> 00:12:49,842
to achieve three-point tracking?

231
00:12:50,263 --> 00:12:53,524
Basically, all you need to do is to tell the physics system

232
00:12:54,005 --> 00:12:56,546
where the hands are, where is the head,

233
00:12:57,106 --> 00:12:59,587
and let the physics to figure out, you know,

234
00:13:00,208 --> 00:13:04,230
the body's motion state, the lower body's posture

235
00:13:04,350 --> 00:13:05,930
in order to maintain a full body balance.

236
00:13:06,731 --> 00:13:07,991
So we want to maximize.

237
00:13:08,860 --> 00:13:10,957
maximize the knowledge.

238
00:13:12,376 --> 00:13:16,518
in physics equation to give us as much prior as possible

239
00:13:16,838 --> 00:13:18,819
to drive a full body movement.

240
00:13:19,519 --> 00:13:22,421
So with such simulation-driven character,

241
00:13:23,021 --> 00:13:26,303
we can generate full body motion more naturally.

242
00:13:26,903 --> 00:13:28,924
And also, because it's simulation,

243
00:13:28,944 --> 00:13:32,085
so it can automatically interact with the environment.

244
00:13:32,686 --> 00:13:36,387
If you push the character, it will react to it.

245
00:13:36,848 --> 00:13:39,429
If the character run into a wall,

246
00:13:40,169 --> 00:13:40,929
it will react to it.

247
00:13:41,190 --> 00:13:43,030
So it's more interactive to the environment.

248
00:13:44,291 --> 00:13:48,212
So that's how we are doing simulated motion for 3-point.

249
00:13:48,893 --> 00:13:50,293
So in these slides, we are showing

250
00:13:50,353 --> 00:13:53,295
two flavors of that 3-point tracking using simulation.

251
00:13:53,755 --> 00:13:57,496
The left-hand side uses Oculus Quest as the testing hardware.

252
00:13:57,997 --> 00:13:59,977
The right-hand side uses HTC Vive.

253
00:14:00,778 --> 00:14:02,819
There are no essential differences,

254
00:14:02,979 --> 00:14:04,779
but it's just different settings.

255
00:14:05,380 --> 00:14:05,820
In general,

256
00:14:07,062 --> 00:14:08,543
We found a simulated motion.

257
00:14:09,203 --> 00:14:14,568
We can run on mobile or tethered environment pretty smoothly.

258
00:14:16,249 --> 00:14:19,712
As you can see, even with three tracking points,

259
00:14:20,233 --> 00:14:24,796
we can synthesize some reasonable lower body movement.

260
00:14:25,537 --> 00:14:27,679
As the character is leaning, we're

261
00:14:27,699 --> 00:14:30,801
trying to predict the torso leaning from the head angle

262
00:14:30,961 --> 00:14:32,883
and hands positions.

263
00:14:33,956 --> 00:14:44,543
So with such technology, we can create a reasonable full-body representation of yourself in virtual space.

264
00:14:45,344 --> 00:14:51,528
So with such a virtual body, you can engage multiplayer experience better.

265
00:14:51,688 --> 00:14:54,730
You can socialize with friends in VR more intuitively.

266
00:14:58,260 --> 00:15:02,062
Yeah, another benefit of that is you can also interact

267
00:15:02,082 --> 00:15:04,262
with the environment more easily, right?

268
00:15:04,322 --> 00:15:10,364
Like, we can see the right-hand side, the Vive demo.

269
00:15:11,245 --> 00:15:20,088
The actor is moving around, and she can sit on the ground,

270
00:15:20,508 --> 00:15:22,548
right, because there's collision with the ground,

271
00:15:22,868 --> 00:15:24,729
so the sitting motion generates automatically.

272
00:15:26,310 --> 00:15:29,552
In this case, she's sitting on a virtual box.

273
00:15:30,633 --> 00:15:34,956
And the collision will generate the leg movement.

274
00:15:35,337 --> 00:15:37,518
You have the leg need to rest on the box.

275
00:15:41,942 --> 00:15:44,824
So that's three-point tracking.

276
00:15:48,334 --> 00:15:50,214
The next demo, I'm going to show a different flavor

277
00:15:50,495 --> 00:15:51,375
of multi-point tracking.

278
00:15:51,775 --> 00:15:52,956
This is four-point tracking.

279
00:15:53,556 --> 00:15:55,617
There is additional fourth tracker

280
00:15:56,657 --> 00:15:59,198
on the back of the actor.

281
00:15:59,839 --> 00:16:05,241
So the tracker on the back will track rotation only.

282
00:16:05,821 --> 00:16:09,043
In other words, you could use a cheap inertia sensor

283
00:16:09,463 --> 00:16:12,044
for the fourth tracker and just attach it

284
00:16:12,665 --> 00:16:13,925
on the performer's back.

285
00:16:14,605 --> 00:16:15,326
It should still work.

286
00:16:16,538 --> 00:16:18,240
Why do we do the four-point tracking?

287
00:16:18,340 --> 00:16:20,302
It's because, you know, even three-point tracking

288
00:16:20,343 --> 00:16:21,724
can track some basic leaning,

289
00:16:22,365 --> 00:16:24,567
but the fidelity of the torso curvature

290
00:16:24,607 --> 00:16:26,109
is still not ideal.

291
00:16:26,610 --> 00:16:28,252
With additional points, as you can see,

292
00:16:28,452 --> 00:16:29,613
the character can do the leaning,

293
00:16:30,134 --> 00:16:32,997
you know, in a much more realistic fashion.

294
00:16:33,810 --> 00:16:36,673
For some shooter games, for some location-based VR

295
00:16:36,753 --> 00:16:41,599
that need, like the shooting demo on the right-hand side,

296
00:16:42,139 --> 00:16:45,143
the shooter really need to hide behind an obstacle

297
00:16:45,724 --> 00:16:50,349
and leaning out his body in order to do the action.

298
00:16:52,845 --> 00:16:54,765
Now we can do it with four-point tracking.

299
00:16:55,726 --> 00:16:58,507
So still, the four-point are pretty lightweight.

300
00:16:58,687 --> 00:17:02,588
It only need upper body standard VR plus additional sensor.

301
00:17:03,148 --> 00:17:06,910
And with this setup, we can get a pretty good

302
00:17:07,350 --> 00:17:08,310
full-body representation.

303
00:17:08,690 --> 00:17:11,791
Like the left-hand side, the performer is doing

304
00:17:11,851 --> 00:17:13,752
some dancing kind of movement.

305
00:17:14,072 --> 00:17:19,174
It can track the hip and the torso reasonably.

306
00:17:19,928 --> 00:17:21,289
And if we look at the lower body,

307
00:17:21,929 --> 00:17:25,410
the lower body's leg movement is not one-to-one mapped

308
00:17:25,771 --> 00:17:27,011
to the real leg movement.

309
00:17:27,471 --> 00:17:31,473
But because we employed the robotics balancing theory,

310
00:17:31,673 --> 00:17:36,055
as I illustrated early, when the center of mass of the actor

311
00:17:36,516 --> 00:17:39,577
move outside the supporting area of your feet,

312
00:17:40,758 --> 00:17:43,019
the AI knows automatically swing the foot

313
00:17:43,459 --> 00:17:44,579
to the right direction.

314
00:17:44,959 --> 00:17:45,860
So it roughly match.

315
00:17:46,380 --> 00:17:49,382
It's most of the time reasonable.

316
00:17:57,324 --> 00:17:59,425
And in a summary, here I just want to recap

317
00:17:59,565 --> 00:18:01,325
what's the difference between three-point tracking

318
00:18:01,365 --> 00:18:02,226
and four-point tracking.

319
00:18:02,786 --> 00:18:05,086
As we can see, three-point tracking on the left,

320
00:18:05,366 --> 00:18:08,667
it track a basic torso leaning,

321
00:18:10,108 --> 00:18:13,109
but you need to lean very clearly for the VR to pick up.

322
00:18:13,909 --> 00:18:16,890
The right-hand side with the fourth tracker on the back,

323
00:18:17,370 --> 00:18:20,471
you know, we can track more details in the torso.

324
00:18:20,511 --> 00:18:22,171
You can do a little bit curvature movement.

325
00:18:22,890 --> 00:18:25,173
you know, in the back, we can still track it.

326
00:18:25,733 --> 00:18:28,597
So there's such quality improvement

327
00:18:28,737 --> 00:18:30,960
as you put more tracker into the system.

328
00:18:31,480 --> 00:18:32,862
But I also want to stress,

329
00:18:33,483 --> 00:18:36,947
this shows the flexibility of a physics-based tracking system.

330
00:18:37,821 --> 00:18:40,484
You can just add, you know, if you only have one tracker,

331
00:18:40,664 --> 00:18:41,645
you can put it on the head.

332
00:18:42,185 --> 00:18:44,488
We'll figure out the hands and everything else.

333
00:18:44,848 --> 00:18:46,290
If it has two tracker or three tracker,

334
00:18:46,670 --> 00:18:49,853
you put it on more, you put them on more body parts,

335
00:18:50,314 --> 00:18:51,415
and it will track more detail.

336
00:18:51,795 --> 00:18:54,858
And we can incrementally increase the tracking quality

337
00:18:55,799 --> 00:18:57,261
by adding more sensors.

338
00:18:57,721 --> 00:18:59,983
But the system can synthesize everything else.

339
00:19:01,739 --> 00:19:04,140
And the last demo, I just want to show the same technology

340
00:19:04,220 --> 00:19:06,401
you can apply in a two-player environment.

341
00:19:06,921 --> 00:19:09,543
So because both avatars are physically simulated,

342
00:19:09,863 --> 00:19:11,143
they can collide with each other.

343
00:19:12,884 --> 00:19:15,565
You can sort of touch each other in virtual space.

344
00:19:16,346 --> 00:19:19,567
We can do some dancing or fighting or people hugging.

345
00:19:20,007 --> 00:19:22,388
So that sort of very intimate social experience

346
00:19:22,748 --> 00:19:25,710
will be enabled by this physically simulated avatar

347
00:19:25,810 --> 00:19:26,630
tracking technology.

348
00:19:27,900 --> 00:19:31,021
We've seen a lot of demos about tracking the avatar, which

349
00:19:31,081 --> 00:19:31,862
is your own motion.

350
00:19:32,262 --> 00:19:35,604
But what about NPC, non-player characters?

351
00:19:36,064 --> 00:19:38,846
In this case, we also use simulated motion

352
00:19:39,266 --> 00:19:41,127
to simulate the dog movement.

353
00:19:41,487 --> 00:19:42,708
The dog is a very simple one.

354
00:19:42,728 --> 00:19:44,509
It's just trying to do a balancing,

355
00:19:44,829 --> 00:19:47,410
just trying to stand on his feet and remain balanced.

356
00:19:48,021 --> 00:19:52,182
But here, the player is using his virtual hand

357
00:19:52,342 --> 00:19:55,723
being tracked by leap motion to engage with the dog.

358
00:19:56,203 --> 00:19:58,923
Even though we didn't add any keyframe animation to the dog,

359
00:19:59,323 --> 00:20:03,044
but just because the AI dog trying to balance in place,

360
00:20:03,624 --> 00:20:07,565
you'll see a lot of richer experience being rendered,

361
00:20:07,665 --> 00:20:11,106
being generated in real time as the player trying

362
00:20:11,126 --> 00:20:12,826
to touch the dog, trying to pinch,

363
00:20:12,986 --> 00:20:15,887
or doing some random movement.

364
00:20:16,873 --> 00:20:20,057
So this shows the potential of simulated motion

365
00:20:21,618 --> 00:20:24,121
in creating non-player character as well.

366
00:20:25,122 --> 00:20:28,146
I think by combining avatar, which

367
00:20:28,166 --> 00:20:31,730
is a player character, and a simulated non-player character,

368
00:20:32,270 --> 00:20:36,375
we can build a full procedural world that's

369
00:20:36,415 --> 00:20:37,616
a lot more interactive.

370
00:20:38,424 --> 00:20:43,029
because the motion itself now is generated by simulation and AI

371
00:20:43,429 --> 00:20:47,033
instead of just key frame can experience.

372
00:20:50,997 --> 00:20:52,959
So we talk about simulated motion

373
00:20:53,260 --> 00:20:55,542
as the first phase of the technology.

374
00:20:56,203 --> 00:20:58,966
And the second phase, we experimented to

375
00:21:00,171 --> 00:21:04,176
upgrade simulated motion to the motion intelligence phase.

376
00:21:04,877 --> 00:21:06,239
And what is motion intelligence?

377
00:21:06,859 --> 00:21:07,640
I like to quote.

378
00:21:09,466 --> 00:21:11,267
a definition from DeepMind blog.

379
00:21:11,828 --> 00:21:14,770
So the agility and flexibility of a monkey

380
00:21:14,990 --> 00:21:16,111
swing through the trees,

381
00:21:16,611 --> 00:21:18,713
where a football player dodging opponents

382
00:21:19,193 --> 00:21:21,495
and scoring goal can be breathtaking.

383
00:21:22,296 --> 00:21:23,897
So how to master the AI

384
00:21:24,037 --> 00:21:26,599
to coordinate your full body muscle structure

385
00:21:27,079 --> 00:21:29,861
to achieve this kind of motor maneuver?

386
00:21:30,422 --> 00:21:33,224
That's the hallmark of motion intelligence.

387
00:21:33,384 --> 00:21:35,326
That's very exciting area in AI.

388
00:21:35,886 --> 00:21:38,267
And that's the kind of technology

389
00:21:38,447 --> 00:21:39,768
we are moving towards as well.

390
00:21:40,408 --> 00:21:43,849
To its core, we just want to build a digital cerebellum

391
00:21:44,490 --> 00:21:47,571
to do balancing, to do motor skills,

392
00:21:47,671 --> 00:21:50,752
like a little baby learn from observing.

393
00:21:52,833 --> 00:21:55,654
For those of you who are familiar with reinforced learning,

394
00:21:56,075 --> 00:21:58,436
the setup fundamentally is a reinforced learning.

395
00:21:58,916 --> 00:22:00,176
You have a digital agent.

396
00:22:00,957 --> 00:22:01,357
It will...

397
00:22:02,457 --> 00:22:05,018
It will take observations from the virtual environment.

398
00:22:05,198 --> 00:22:07,599
In this case, that's the VR or AR environment.

399
00:22:08,660 --> 00:22:14,083
It will send instructions to the agent and generate a reward.

400
00:22:14,763 --> 00:22:19,025
By collecting reward or penalty from thousands,

401
00:22:19,105 --> 00:22:21,106
thousands of iterations of experiments,

402
00:22:21,626 --> 00:22:22,647
you learn from your failures.

403
00:22:23,027 --> 00:22:24,087
You learn from your success.

404
00:22:24,528 --> 00:22:28,149
And actually, that's very familiar to us

405
00:22:28,250 --> 00:22:31,391
because that's how humans learn our experience as well.

406
00:22:31,891 --> 00:22:33,612
Applying that to a digital world,

407
00:22:34,132 --> 00:22:37,074
build a deep reinforced learning pipeline,

408
00:22:37,454 --> 00:22:40,095
you can actually teach a digital agent to learn to walk,

409
00:22:40,115 --> 00:22:41,036
learn to stand as well.

410
00:22:43,897 --> 00:22:47,239
So this example, this is the input

411
00:22:47,319 --> 00:22:48,860
to the machine learning algorithm.

412
00:22:49,361 --> 00:22:51,602
It's just running, right?

413
00:22:51,682 --> 00:22:53,063
One second of running animation.

414
00:22:54,804 --> 00:22:57,486
So this is the reinforcement learning result

415
00:22:57,646 --> 00:22:59,127
after 100 iterations.

416
00:22:59,527 --> 00:23:01,488
Basically, you do 100 times experiments.

417
00:23:01,908 --> 00:23:06,051
Every time you try to tighten your muscle, in a way,

418
00:23:06,271 --> 00:23:08,092
you can mimic the input motion.

419
00:23:08,493 --> 00:23:11,655
Of course, you will fall. You will run into the wall.

420
00:23:11,695 --> 00:23:12,755
A lot of bizarre.

421
00:23:13,210 --> 00:23:19,478
scenarios but when you collect the feedback, use the feedback to do gradient descent to correct your

422
00:23:20,099 --> 00:23:24,524
you know your motion, you can eventually get something that's kind of working. But if you

423
00:23:24,544 --> 00:23:28,949
keep doing this after 1000 iterations, you will generate AI that can do running,

424
00:23:30,030 --> 00:23:32,333
running while keeping balance pretty smoothly.

425
00:23:33,825 --> 00:23:35,467
And because of the simulation, it

426
00:23:35,507 --> 00:23:37,368
can also endure your disturbance.

427
00:23:37,949 --> 00:23:39,630
As the digital agent running, you

428
00:23:39,670 --> 00:23:41,712
can use a mouse tracker to track him around.

429
00:23:42,032 --> 00:23:43,233
You can apply a virtual force.

430
00:23:43,574 --> 00:23:44,955
He will just react, right?

431
00:23:44,975 --> 00:23:47,177
So that's the beauty of simulated motion.

432
00:23:48,598 --> 00:23:50,700
And then in the nest, we can naturally

433
00:23:50,800 --> 00:23:54,063
will teach multiple behavior to that digital brain.

434
00:23:54,483 --> 00:23:57,025
In this case, there are three behavior, like running,

435
00:23:57,546 --> 00:23:59,267
sharp turning, and get up.

436
00:24:00,204 --> 00:24:02,225
by combining three behavior to the guy,

437
00:24:02,686 --> 00:24:06,348
now you have an agent a little bit more smart.

438
00:24:06,969 --> 00:24:08,590
It can turn now.

439
00:24:09,871 --> 00:24:12,092
And if you pull him to the ground,

440
00:24:12,773 --> 00:24:14,094
he can pick up himself.

441
00:24:14,574 --> 00:24:19,037
This whole thing will be generated by a simulation.

442
00:24:19,657 --> 00:24:20,478
It's not keyframe.

443
00:24:20,638 --> 00:24:24,020
So every time you play it in VR, it could be different.

444
00:24:25,466 --> 00:24:27,548
It is then, you can keep building on,

445
00:24:27,688 --> 00:24:29,209
you can keep expanding the motion brain

446
00:24:29,849 --> 00:24:31,410
to have six behavior, eight behavior.

447
00:24:31,811 --> 00:24:34,012
In this case, that's a six behavior agent.

448
00:24:34,513 --> 00:24:37,195
It can do basic indoor mobility maneuver,

449
00:24:37,575 --> 00:24:39,016
such as climbing up stairs,

450
00:24:39,537 --> 00:24:42,479
such as sitting in a sofa, yada, yada.

451
00:24:42,919 --> 00:24:44,660
As you can see, we can keep building that.

452
00:24:45,621 --> 00:24:49,704
And this one, we want to add a more upper body variance

453
00:24:49,844 --> 00:24:52,306
to the MPC, so we add 20 behavior

454
00:24:52,727 --> 00:24:54,908
to a pedestrian motion brain.

455
00:24:55,402 --> 00:25:00,605
And in this case, the guys are walking while making a call,

456
00:25:00,765 --> 00:25:01,466
reading notes.

457
00:25:01,986 --> 00:25:05,208
So just add a lot of diversity to the scenario.

458
00:25:05,668 --> 00:25:10,391
So we could use this to simulate a crowd, crowd simulation,

459
00:25:10,411 --> 00:25:11,451
or paddy string case.

460
00:25:12,512 --> 00:25:14,913
We can actually use this kind of motion brain

461
00:25:15,453 --> 00:25:18,515
in non-gaming environment, such as if you're

462
00:25:18,935 --> 00:25:19,996
making autopilot car.

463
00:25:20,884 --> 00:25:23,425
If you make a training program involving

464
00:25:23,785 --> 00:25:28,587
full-body physical intelligence, we

465
00:25:28,607 --> 00:25:30,888
can use simulation and AI to help.

466
00:25:33,609 --> 00:25:36,110
This example shows a more sophisticated motion brain.

467
00:25:37,056 --> 00:25:38,576
modeled of basketball player.

468
00:25:39,037 --> 00:25:42,878
So each demonstration in this slide

469
00:25:42,938 --> 00:25:45,979
had three behaviors, three styles of passing.

470
00:25:46,499 --> 00:25:50,080
As you can see, the AI can stitch them together.

471
00:25:50,340 --> 00:25:52,541
And based on high-level commands,

472
00:25:52,621 --> 00:25:57,122
like where to move towards, AI will automatically

473
00:25:57,222 --> 00:26:01,983
pick the right behavior to transition into and blend them

474
00:26:02,003 --> 00:26:02,323
together.

475
00:26:04,380 --> 00:26:07,641
And who says the technology can only apply to humans?

476
00:26:08,022 --> 00:26:10,203
Actually, it's the exact same methodology.

477
00:26:11,243 --> 00:26:14,785
Physic-simulated character and driven by AI.

478
00:26:14,805 --> 00:26:16,846
This you can use on animals as well.

479
00:26:17,346 --> 00:26:21,188
Actually, quadruped is easier to do than human

480
00:26:21,228 --> 00:26:22,829
because quadruped has four legs.

481
00:26:23,449 --> 00:26:24,810
It hardly can fall to the ground.

482
00:26:24,970 --> 00:26:27,211
So you can apply to an animal,

483
00:26:27,551 --> 00:26:29,412
different form factors of creatures.

484
00:26:31,330 --> 00:26:33,214
And we talk about motion intelligence.

485
00:26:33,715 --> 00:26:36,099
So now I want to just show a few example

486
00:26:36,480 --> 00:26:38,403
how we can apply motion intelligence

487
00:26:38,524 --> 00:26:40,066
to create an AR character.

488
00:26:41,637 --> 00:26:46,819
So the left-hand side are just a boy character

489
00:26:47,179 --> 00:26:48,919
spawned on a kitchen top in AR.

490
00:26:49,520 --> 00:26:52,101
And you can mess around with the boy

491
00:26:52,181 --> 00:26:56,923
and see it's dancing, it's being pushed off.

492
00:26:57,543 --> 00:27:00,084
The right-hand side is an AR dog.

493
00:27:00,904 --> 00:27:02,605
You can spawn the dog on a carpet

494
00:27:03,225 --> 00:27:04,966
and throw different toys at the dog

495
00:27:05,026 --> 00:27:07,427
and do some kind of engagement interaction.

496
00:27:10,869 --> 00:27:13,393
This is the, you're probably familiar with this video

497
00:27:13,413 --> 00:27:16,638
already, because we've shown similar six behavior

498
00:27:16,838 --> 00:27:19,161
characters in the earlier slide.

499
00:27:19,542 --> 00:27:23,147
Now if we apply the exact same brain in the AR application,

500
00:27:23,528 --> 00:27:24,489
you can create a little.

501
00:27:25,337 --> 00:27:27,838
indoor mobility demo with a house, right?

502
00:27:27,858 --> 00:27:31,140
This is like a two-story house with a staircase.

503
00:27:32,201 --> 00:27:34,962
You can dynamically change the digital environment,

504
00:27:35,383 --> 00:27:36,964
and that character will respond to it.

505
00:27:37,584 --> 00:27:38,745
It will try to climb up.

506
00:27:39,785 --> 00:27:41,506
If you change where the staircase is,

507
00:27:41,586 --> 00:27:43,307
the character may fall to the ground,

508
00:27:43,648 --> 00:27:45,329
may succeed, just like in real world.

509
00:27:46,429 --> 00:27:50,531
So basically everything we did in simulated motion

510
00:27:50,831 --> 00:27:55,834
and in motion intelligence can be automatically applied to AR

511
00:27:56,294 --> 00:28:00,156
so that you can create a highly interactive character.

512
00:28:00,556 --> 00:28:02,457
You can create a highly interactive environment.

513
00:28:03,017 --> 00:28:04,578
We think this level of interaction

514
00:28:04,858 --> 00:28:08,599
is necessary to bring the AR world to life.

515
00:28:09,260 --> 00:28:10,240
You can experience it.

516
00:28:10,860 --> 00:28:15,883
You can really feel the presence of this digital being.

517
00:28:18,196 --> 00:28:23,003
And then the next module, I want to talk about motion perception.

518
00:28:23,504 --> 00:28:27,349
We talk about motion intelligence, very cool, to synthesize simulated motion.

519
00:28:27,850 --> 00:28:31,555
But what about the vision?

520
00:28:31,715 --> 00:28:32,937
What about the input part?

521
00:28:33,581 --> 00:28:35,902
for our motion brain, right?

522
00:28:36,302 --> 00:28:39,864
As a human, we can understand the external world very well

523
00:28:39,944 --> 00:28:42,325
by observing how people move around.

524
00:28:43,506 --> 00:28:47,348
So this last piece, we are extending

525
00:28:47,768 --> 00:28:50,189
the motion intelligence to cover the perception path.

526
00:28:50,910 --> 00:28:55,852
Basically, we can do the following thing.

527
00:28:56,968 --> 00:29:00,249
five steps in order to achieve some basic motion perception.

528
00:29:00,810 --> 00:29:04,411
The first, as we know, you can apply some post-estimation

529
00:29:04,632 --> 00:29:08,133
algorithm, such as the OpenPose from CMU.

530
00:29:08,613 --> 00:29:09,514
Welcome to check it out.

531
00:29:09,534 --> 00:29:11,615
It's an open source project, very popular.

532
00:29:12,055 --> 00:29:13,916
You can use an algorithm like OpenPose

533
00:29:14,296 --> 00:29:15,997
to extract your joint position.

534
00:29:17,342 --> 00:29:21,266
of human from just an image, from just a camera feed, camera input, right?

535
00:29:21,887 --> 00:29:28,433
And the second step, you can retarget this skeletal joint position, retarget that posture

536
00:29:28,933 --> 00:29:31,736
to a digital avatar, to a digital model.

537
00:29:33,037 --> 00:29:34,879
And the third step, you can apply...

538
00:29:36,419 --> 00:29:40,544
physics simulation on that character to give it physicality,

539
00:29:40,984 --> 00:29:43,487
give it collision, give it a weight and mass,

540
00:29:43,547 --> 00:29:44,488
all these nice things.

541
00:29:45,029 --> 00:29:46,771
And then your avatar become more alive.

542
00:29:46,931 --> 00:29:48,854
It's not only tracking the input motion,

543
00:29:49,294 --> 00:29:51,357
it can also collide with the environment.

544
00:29:52,229 --> 00:29:55,570
And also by applying this simulation technique,

545
00:29:56,110 --> 00:29:58,111
you can make the tracking quality higher

546
00:29:58,611 --> 00:30:01,092
because we can automatically fix some artifact,

547
00:30:01,512 --> 00:30:02,733
like a self-penetration,

548
00:30:03,473 --> 00:30:06,294
like a joint limits violation, things like that.

549
00:30:07,627 --> 00:30:11,128
So by adding this motion perception to your pipeline,

550
00:30:11,608 --> 00:30:15,529
you can further improve the pipeline's capability

551
00:30:15,889 --> 00:30:18,690
to model human motion, to capture human motion,

552
00:30:19,190 --> 00:30:21,590
and convert them to a digital character

553
00:30:21,730 --> 00:30:23,251
in real time or offline.

554
00:30:24,511 --> 00:30:27,512
So this last piece also demonstrate

555
00:30:28,092 --> 00:30:29,692
just from single RGB camera,

556
00:30:30,513 --> 00:30:35,634
what you can track from a 2D video or from a camera input.

557
00:30:42,730 --> 00:30:46,775
And in the last slide, I just want to quickly talk about the pipeline.

558
00:30:47,196 --> 00:30:52,663
So we talk about a lot how to create simulated character in AR and VR,

559
00:30:52,824 --> 00:30:58,271
but exactly what's the workflow for artists and content creators to achieve this effect.

560
00:30:58,893 --> 00:31:02,236
So here I want to summarize this workflow at a high level.

561
00:31:03,097 --> 00:31:09,583
The first, you can upload or ingest your own FBX file,

562
00:31:09,783 --> 00:31:15,087
your 3D Max or Maya digital assets into this AI system,

563
00:31:15,588 --> 00:31:16,229
whatever it is.

564
00:31:17,269 --> 00:31:20,432
And the second, you want to add a physics layer

565
00:31:20,532 --> 00:31:22,895
on top of the artistic model.

566
00:31:23,675 --> 00:31:25,697
By that, I mean you will add the.

567
00:31:26,910 --> 00:31:30,492
the collider, the mass, the weight, the joint,

568
00:31:30,512 --> 00:31:32,553
the muscle strength, all those physics

569
00:31:32,714 --> 00:31:36,196
and the controller parameters to the artistic model.

570
00:31:36,856 --> 00:31:39,038
After this second step and the third step,

571
00:31:39,078 --> 00:31:41,779
you need to decide what are the set of behavior

572
00:31:41,799 --> 00:31:45,162
I want my motion brain to master.

573
00:31:45,762 --> 00:31:48,504
And you will pick those subsets of behaviors.

574
00:31:50,125 --> 00:31:52,687
If you can have reference motion provided to the AI,

575
00:31:52,747 --> 00:31:53,107
that's good.

576
00:31:53,827 --> 00:31:55,288
If you do not have reference motion,

577
00:31:56,069 --> 00:31:59,392
you just need to describe it procedurally or mathematically

578
00:32:00,312 --> 00:32:00,993
in some fashion.

579
00:32:01,753 --> 00:32:04,235
After you select your behavior, you

580
00:32:04,275 --> 00:32:06,077
can start a training process.

581
00:32:06,937 --> 00:32:11,060
The training process, for very simple behavior,

582
00:32:11,080 --> 00:32:12,281
you don't need to run machine learning.

583
00:32:12,662 --> 00:32:15,224
For example, as I showed in earlier slides,

584
00:32:15,704 --> 00:32:18,026
for the standing in balancing behavior,

585
00:32:18,931 --> 00:32:20,312
That's a very simple heuristic.

586
00:32:20,752 --> 00:32:24,634
If center of mass is moved outside your supporting area

587
00:32:24,674 --> 00:32:29,276
of your feet, then try to tighten the muscle

588
00:32:29,376 --> 00:32:33,037
to move the center back.

589
00:32:34,918 --> 00:32:37,559
That kind of simple heuristic, you

590
00:32:37,579 --> 00:32:40,781
can just write it from a few lines of script.

591
00:32:41,721 --> 00:32:41,941
So.

592
00:32:43,098 --> 00:32:45,419
However, if the behavior is quite complicated,

593
00:32:45,859 --> 00:32:48,620
like some of our more advanced demo for the basketball player,

594
00:32:49,300 --> 00:32:51,801
dribbling the basketball without losing it,

595
00:32:52,001 --> 00:32:54,922
without losing balance, that's kind of hard

596
00:32:55,442 --> 00:32:58,383
to manually generate a few lines of script

597
00:32:58,503 --> 00:33:00,043
to control the muscles, things like that.

598
00:33:00,403 --> 00:33:02,184
For those, you can run through a neural network,

599
00:33:02,524 --> 00:33:06,045
just like machine learning, to do thousands of tries

600
00:33:06,065 --> 00:33:07,126
and errors to figure it out.

601
00:33:08,526 --> 00:33:11,707
So this way or another, you will train a control logic.

602
00:33:12,850 --> 00:33:15,572
and then you download this control logic as an assets file.

603
00:33:15,952 --> 00:33:20,016
This assets file imported to your game engine,

604
00:33:20,096 --> 00:33:23,458
your own environment, and then finally,

605
00:33:23,478 --> 00:33:26,281
during the runtime, you need the physics engine

606
00:33:26,341 --> 00:33:29,123
to be paired up to the controller assets,

607
00:33:29,644 --> 00:33:33,226
and you will get your controlled simulated character in place.

608
00:33:34,708 --> 00:33:37,650
So that is an overview of how this pipeline

609
00:33:37,710 --> 00:33:38,611
could be constructed.

610
00:33:40,853 --> 00:33:43,297
I mean, at D-Motion, we provide solutions

611
00:33:43,678 --> 00:33:45,360
for various motion intelligence.

612
00:33:47,744 --> 00:33:51,831
Tech, as I described, we create AR, VR, body tracking solution.

613
00:33:52,308 --> 00:33:55,968
We also help our partners to train interactive characters

614
00:33:56,729 --> 00:33:58,809
and provide the real-time motion perception

615
00:33:59,469 --> 00:34:01,149
and customize motion brain, et cetera.

616
00:34:01,649 --> 00:34:05,250
So if you have any questions for the slides,

617
00:34:05,870 --> 00:34:08,871
or if you need additional help on motion brain training

618
00:34:09,231 --> 00:34:13,651
or digital avatar creation, feel free to reach out to us.

619
00:34:14,832 --> 00:34:18,532
So, yeah, that concludes my presentation,

620
00:34:19,072 --> 00:34:21,213
and I'm going to take questions if you have.

621
00:34:22,595 --> 00:34:22,875
Thank you.

622
00:34:38,402 --> 00:34:40,402
OK, go ahead.

623
00:34:40,803 --> 00:34:40,983
Hi.

624
00:34:41,443 --> 00:34:43,364
So it's really impressive.

625
00:34:44,364 --> 00:34:45,104
It's very impressive.

626
00:34:45,985 --> 00:34:50,887
So the thing, though, that strikes me is, do you think

627
00:34:51,522 --> 00:34:57,605
There is a way that you can also include the next level, which is personality.

628
00:34:58,726 --> 00:35:05,529
Is there any way at all that even if you train 10 people, 10 characters to walk, is there

629
00:35:05,609 --> 00:35:11,553
any way you can think of to inject, for the artist to inject different kinds of personalities

630
00:35:11,733 --> 00:35:14,954
in the different walks, procedural as they may be?

631
00:35:16,298 --> 00:35:21,725
Thank you. So the question is, is there any way to inject personality in the trained AI,

632
00:35:21,865 --> 00:35:27,953
such as 10 styles of walking? How can you represent emotion or personality?

633
00:35:28,213 --> 00:35:31,137
Very great question. So the answer is yes.

634
00:35:32,779 --> 00:35:33,620
Although the motor brain...

635
00:35:34,281 --> 00:35:37,783
I showed in the demo are for primary motor skill,

636
00:35:37,863 --> 00:35:39,664
mostly like walking and dancing.

637
00:35:40,164 --> 00:35:45,347
But if you train 10 behavior that

638
00:35:45,387 --> 00:35:47,148
share the same pattern, like walking,

639
00:35:47,328 --> 00:35:50,670
with 10 different walking or running, what you can do,

640
00:35:51,210 --> 00:35:55,573
you can easily include action label in that training.

641
00:35:55,613 --> 00:35:57,153
So you can label that action.

642
00:35:57,494 --> 00:35:59,275
You can label the mood as happy.

643
00:36:00,149 --> 00:36:05,072
tired, angry, with some reference motion to the AI.

644
00:36:05,412 --> 00:36:07,313
So as you are training, you're basically creating

645
00:36:07,353 --> 00:36:10,154
a latent space to tell the AI, you know,

646
00:36:10,194 --> 00:36:11,955
what kind of motion you consider happy,

647
00:36:12,035 --> 00:36:14,716
what kind of sad, and the AI will create

648
00:36:15,196 --> 00:36:17,917
an interplay, the intermediate result

649
00:36:18,118 --> 00:36:19,658
representing the level of happiness.

650
00:36:21,039 --> 00:36:23,760
So in short answer, yeah, you can define more parameters

651
00:36:24,621 --> 00:36:28,022
to train the AI to hopefully demonstrate some personality.

652
00:36:29,867 --> 00:36:30,847
Thank you.

653
00:36:31,147 --> 00:36:33,169
I thought the tech looked amazing.

654
00:36:33,229 --> 00:36:36,451
It reminded me very much of Natural Motion's

655
00:36:36,571 --> 00:36:38,913
Euphoria SDK back in the day,

656
00:36:39,673 --> 00:36:41,655
but my understanding around that SDK

657
00:36:41,695 --> 00:36:45,237
was it was incredibly complex to add new behaviors

658
00:36:45,718 --> 00:36:49,400
and how easy is it to kind of train within,

659
00:36:49,480 --> 00:36:51,862
you know, how are developers finding using your tech,

660
00:36:51,902 --> 00:36:53,963
I guess, because I think that was almost the reason

661
00:36:54,003 --> 00:36:56,385
why Euphoria didn't become bigger

662
00:36:56,425 --> 00:36:59,187
was it became too problematic for developers to use.

663
00:36:59,904 --> 00:37:00,965
How are you finding that?

664
00:37:01,845 --> 00:37:02,606
Great, thank you.

665
00:37:02,666 --> 00:37:03,326
The question is,

666
00:37:04,747 --> 00:37:09,611
natural motion euphoria produced very impressive physics-based animation toolkit in the past,

667
00:37:10,072 --> 00:37:11,553
but it's hard to use,

668
00:37:11,913 --> 00:37:16,637
and how the motion or how the new solution can help in that space.

669
00:37:17,237 --> 00:37:17,377
So...

670
00:37:18,698 --> 00:37:20,999
We were also inspired by natural motion.

671
00:37:21,079 --> 00:37:22,520
So it's great technology.

672
00:37:23,260 --> 00:37:25,181
But really, the difficulty in deployment

673
00:37:25,301 --> 00:37:28,382
is how much time to create a new behavior.

674
00:37:28,782 --> 00:37:31,623
For example, using the last generation tech

675
00:37:32,024 --> 00:37:35,065
without deep learning, without this new simulation, AI.

676
00:37:36,546 --> 00:37:38,907
it probably take a month to add a new,

677
00:37:39,128 --> 00:37:42,410
let's say, dancing movement to your physics-based character,

678
00:37:42,891 --> 00:37:44,632
because it's not just dancing, right?

679
00:37:44,672 --> 00:37:47,334
You need to think about the muscle, pulling sequence,

680
00:37:47,394 --> 00:37:50,236
the balancing requirement, all the physical constraint.

681
00:37:50,617 --> 00:37:52,999
The mathematics behind that can be daunting.

682
00:37:54,180 --> 00:37:56,661
So I think the new tech is exactly trying

683
00:37:56,681 --> 00:37:57,742
to address that concern,

684
00:37:58,163 --> 00:38:00,625
because the beauty of deep learning and AI is that

685
00:38:02,384 --> 00:38:03,665
you can learn from data.

686
00:38:04,125 --> 00:38:06,667
So we just need to throw more reference motions

687
00:38:06,967 --> 00:38:09,409
from motion capture, from high quality key frame

688
00:38:10,009 --> 00:38:10,769
to the trainer.

689
00:38:11,470 --> 00:38:14,912
And AI will do that thousands and thousands of iteration

690
00:38:15,412 --> 00:38:17,394
and figure out some, you know,

691
00:38:17,614 --> 00:38:19,875
some pretty complex neural nets

692
00:38:20,235 --> 00:38:22,817
that represent some implicit math.

693
00:38:23,782 --> 00:38:28,845
know, to control that. So, I'll give you an example now, in order to add a new running

694
00:38:29,405 --> 00:38:34,928
or dancing sequence to our motion brain, it probably takes an hour for a simple case.

695
00:38:35,929 --> 00:38:40,731
For a very complicated case, yeah, it could take days, but still it's like machine time days. It's

696
00:38:40,771 --> 00:38:47,855
not like our mechanical engineers spend months to tune every motor control. Thank you. Sure, thank you.

697
00:38:53,314 --> 00:38:54,035
Any more questions?

698
00:38:57,518 --> 00:38:58,098
Okay, thank you.

699
00:38:58,178 --> 00:38:59,119
So if you like the talk,

700
00:38:59,239 --> 00:39:02,702
welcome to give us a good rating in the review.

701
00:39:03,083 --> 00:39:03,303
Thanks.

