1
00:00:06,378 --> 00:00:08,280
Thanks for coming, it's an amazing turnout.

2
00:00:08,821 --> 00:00:11,605
My name's Alex, I'm a co-founder at Creative AI,

3
00:00:11,625 --> 00:00:13,889
and in general we apply lots of AI techniques

4
00:00:14,049 --> 00:00:15,291
into creative industries.

5
00:00:16,012 --> 00:00:17,013
One of those is deep learning,

6
00:00:17,033 --> 00:00:19,136
but that's more of a hobby of mine.

7
00:00:21,419 --> 00:00:27,381
I apply algorithms on recent papers that I've read and put them onto GitHub, including this

8
00:00:27,461 --> 00:00:29,501
one that I did recently called Neural Enhance.

9
00:00:29,641 --> 00:00:33,102
You can find the code at github.com.

10
00:00:34,042 --> 00:00:35,402
That's all the neural network.

11
00:00:36,063 --> 00:00:42,864
And another project I did, this one's published last year at the ICCC called Neural Doodle

12
00:00:42,904 --> 00:00:46,845
where you can just sketch cool things and the neural network will paint that out in

13
00:00:46,865 --> 00:00:47,885
the style of an artist.

14
00:00:48,406 --> 00:00:52,049
So in this case, it's turning my doodles into a Renoir painting.

15
00:00:53,331 --> 00:00:54,292
So that's also on GitHub.

16
00:00:54,552 --> 00:00:58,035
And my journey with neural networks

17
00:00:58,116 --> 00:01:00,178
actually started a long time ago.

18
00:01:00,218 --> 00:01:02,240
And 15 years ago, I published an article called,

19
00:01:02,440 --> 00:01:05,243
it was a book chapter called The Dark Art of Neural Networks.

20
00:01:05,743 --> 00:01:08,486
And luckily, a lot of things have changed since then.

21
00:01:09,287 --> 00:01:10,648
And now they actually work.

22
00:01:13,830 --> 00:01:15,493
And we understand them much, much better.

23
00:01:15,673 --> 00:01:18,097
The entire academic community is kind of converged around

24
00:01:18,778 --> 00:01:21,904
deep learning and adding ideas or taking the best ideas from

25
00:01:21,964 --> 00:01:24,929
the field, and we've got a much better theoretical

26
00:01:24,989 --> 00:01:26,071
understanding of how they work.

27
00:01:28,015 --> 00:01:30,316
The downside, however, is that deep learning is still,

28
00:01:30,416 --> 00:01:32,916
and it may always be a dark art.

29
00:01:33,136 --> 00:01:34,837
That part is not going to change.

30
00:01:35,557 --> 00:01:37,637
You will never understand a deep learning model

31
00:01:37,677 --> 00:01:39,417
as well as we understand a rule-based system

32
00:01:39,697 --> 00:01:40,758
or a finite state machine.

33
00:01:41,218 --> 00:01:44,738
And the reason for that is that modern deep learning models

34
00:01:44,798 --> 00:01:47,919
can be made up of up to a billion parameters.

35
00:01:48,219 --> 00:01:49,999
That's a billion floating point numbers

36
00:01:50,039 --> 00:01:52,320
that somehow combine together, multiply, divide.

37
00:01:52,620 --> 00:01:55,100
You have to somehow understand how this works.

38
00:01:55,688 --> 00:01:57,529
And you will get a very different sense of understanding

39
00:01:57,549 --> 00:02:00,590
of how such a model works compared to a finite state

40
00:02:00,630 --> 00:02:03,632
machine with a few states.

41
00:02:03,792 --> 00:02:05,172
So what I want to give you in this talk

42
00:02:05,212 --> 00:02:06,693
is an overview of all the different tools

43
00:02:06,733 --> 00:02:09,014
that you can use to understand deep learning.

44
00:02:09,694 --> 00:02:11,795
And starting from the data side of things

45
00:02:11,855 --> 00:02:13,996
and understanding properties of the data sets

46
00:02:14,256 --> 00:02:16,697
and how they can help us understand

47
00:02:17,377 --> 00:02:19,538
how that data flows through these different models.

48
00:02:19,918 --> 00:02:22,059
So covering the models and the computation.

49
00:02:22,119 --> 00:02:24,440
And then finally, the more the learning side of things

50
00:02:24,480 --> 00:02:25,340
and the optimization.

51
00:02:26,961 --> 00:02:28,843
And there are lots of different fields of maths

52
00:02:28,903 --> 00:02:31,405
that help understand deep learning.

53
00:02:31,465 --> 00:02:33,246
And I'll give you sufficient references

54
00:02:33,266 --> 00:02:35,028
so you can dig into it at home.

55
00:02:35,188 --> 00:02:37,951
Not too many equations, but a lot of homework.

56
00:02:39,672 --> 00:02:40,733
So what is deep learning?

57
00:02:41,484 --> 00:02:42,825
Well, ultimately, the simple answer

58
00:02:42,885 --> 00:02:46,006
is that you compute an output y based on an input x.

59
00:02:46,406 --> 00:02:50,689
And deep learning is basically making a really heavy, memory

60
00:02:50,729 --> 00:02:54,190
intensive, overweight, bloated function f that

61
00:02:54,230 --> 00:02:56,772
will compute y from x.

62
00:02:57,032 --> 00:02:59,993
But it can do things that no other machine learning

63
00:03:00,033 --> 00:03:00,713
algorithm can do.

64
00:03:02,504 --> 00:03:04,685
But I like to think of deep learning as basically taking

65
00:03:04,705 --> 00:03:08,107
all the ideas of neural network that work well in practice

66
00:03:08,768 --> 00:03:12,550
and moving more towards the side of software engineering.

67
00:03:13,030 --> 00:03:15,572
So it's a field that is typically

68
00:03:15,612 --> 00:03:17,033
called differentiable computing.

69
00:03:17,073 --> 00:03:19,094
So not just solving problems that you'd

70
00:03:19,154 --> 00:03:21,736
expect with neural networks, but generally building

71
00:03:21,796 --> 00:03:25,518
large programs that just happen to be differentiable,

72
00:03:25,558 --> 00:03:28,960
which means we can train them using gradient descents

73
00:03:29,040 --> 00:03:30,441
or other optimization algorithms.

74
00:03:31,262 --> 00:03:33,083
And this is particularly important because these two

75
00:03:33,183 --> 00:03:34,363
disciplines are going to merge.

76
00:03:34,403 --> 00:03:36,724
Like software engineering and deep learning

77
00:03:36,784 --> 00:03:38,605
are becoming closer and closer thanks

78
00:03:38,645 --> 00:03:39,965
to libraries like TensorFlow.

79
00:03:40,825 --> 00:03:43,606
And so it's very useful to understand

80
00:03:43,766 --> 00:03:45,387
how deep learning works a bit more formally

81
00:03:45,447 --> 00:03:47,827
because it's going to become such an important part of

82
00:03:47,887 --> 00:03:50,208
solving difficult problems in the future.

83
00:03:52,949 --> 00:03:54,149
So when should you use deep learning?

84
00:03:54,209 --> 00:03:55,590
And well, the simple answer to this

85
00:03:55,730 --> 00:03:57,851
is that it's the same as machine learning in general.

86
00:03:58,291 --> 00:04:00,011
Whether you're doing a classification problem,

87
00:04:00,031 --> 00:04:00,752
you're predicting.

88
00:04:01,217 --> 00:04:04,878
Outputs, classes, this is a cat, a dog, that's your output y.

89
00:04:05,238 --> 00:04:07,119
Based on input x could be an image.

90
00:04:07,639 --> 00:04:09,479
Or a regression problem, which is basically

91
00:04:10,599 --> 00:04:13,780
predicting continuous values y from the same input x.

92
00:04:15,361 --> 00:04:17,241
Then as kind of a special case, generation

93
00:04:17,281 --> 00:04:19,742
is sort of predicting very large images

94
00:04:19,802 --> 00:04:21,722
based on very small inputs.

95
00:04:21,882 --> 00:04:23,782
So that's kind of, deep learning is also

96
00:04:23,842 --> 00:04:24,923
shining in that department.

97
00:04:24,983 --> 00:04:27,883
And it's one of my favorite applications of deep learning.

98
00:04:31,006 --> 00:04:35,688
So what really justifies using deep learning is actually the data set itself.

99
00:04:36,028 --> 00:04:38,430
So you need to have certain properties in your data set.

100
00:04:38,450 --> 00:04:42,792
In fact, I think there are three big properties of your data that justifies using deep learning.

101
00:04:43,112 --> 00:04:46,413
Otherwise, you might as well just use regular machine learning and linear classifiers.

102
00:04:47,786 --> 00:04:50,968
These three properties are a very high dimensional input.

103
00:04:51,388 --> 00:04:54,290
So if you're working with large images or videos or even

104
00:04:54,310 --> 00:04:58,113
a highly sampled audio signal, then deep learning

105
00:04:58,173 --> 00:04:59,654
is ideally suited to this.

106
00:04:59,694 --> 00:05:01,975
And that's why all the progress has been made on image

107
00:05:02,015 --> 00:05:04,697
recognition and voice recognition and translation,

108
00:05:04,757 --> 00:05:06,358
because there's really a lot of data there.

109
00:05:06,678 --> 00:05:08,700
And deep learning does it exceptionally well with it.

110
00:05:10,076 --> 00:05:13,018
But it also requires you to have a sufficiently interesting

111
00:05:13,058 --> 00:05:14,059
problem.

112
00:05:14,299 --> 00:05:17,060
So think of deep learning as trying

113
00:05:17,080 --> 00:05:20,042
to approximate a manifold in n-dimensional space.

114
00:05:20,142 --> 00:05:22,464
So it's like a decision surface that

115
00:05:22,504 --> 00:05:25,846
separates true from false in your input space.

116
00:05:26,647 --> 00:05:28,148
And neural networks are very good,

117
00:05:28,288 --> 00:05:29,889
or deep learning in general, is very

118
00:05:29,929 --> 00:05:31,990
good at finding these complex decision surfaces.

119
00:05:32,951 --> 00:05:35,793
And if you just had a very simple decision surface,

120
00:05:35,813 --> 00:05:37,654
you wouldn't need a deep learning model for that.

121
00:05:40,224 --> 00:05:42,527
The third thing is having a lot of examples.

122
00:05:42,787 --> 00:05:45,730
And this is almost a cliche of deep learning,

123
00:05:45,750 --> 00:05:46,931
that you need a lot of data for it.

124
00:05:47,292 --> 00:05:49,594
But Google's internal data set that they

125
00:05:50,155 --> 00:05:51,737
wrote about in some recent papers

126
00:05:51,817 --> 00:05:55,000
have over 100 million images, which is completely crazy.

127
00:05:55,040 --> 00:05:56,341
And it allows you to be very lazy

128
00:05:56,381 --> 00:05:57,423
about how to approach things.

129
00:05:57,783 --> 00:05:59,084
You just grab that much data, and you

130
00:05:59,124 --> 00:06:00,166
don't have to worry so much.

131
00:06:01,927 --> 00:06:06,370
However, it does help if you have a statistical understanding of how the data is structured.

132
00:06:06,790 --> 00:06:11,092
So if you have these images, it's good to have some sense of how the data is distributed.

133
00:06:11,172 --> 00:06:12,693
So are there more cats than dogs?

134
00:06:12,773 --> 00:06:14,294
And if so, we need to re-bias things.

135
00:06:14,354 --> 00:06:18,016
Or if there are very few chairs, we may need to train the neural network a bit more on

136
00:06:18,056 --> 00:06:19,236
those variations.

137
00:06:19,617 --> 00:06:24,224
to get it to be just as accurate at recognizing chairs than it is animals, for example.

138
00:06:25,086 --> 00:06:30,294
So this is called data de-biasing and it's a, I would say, a relatively standard practice

139
00:06:30,334 --> 00:06:31,756
in machine learning and data science.

140
00:06:32,949 --> 00:06:35,130
But what deep learning is sort of increasingly doing

141
00:06:35,170 --> 00:06:37,852
is also doing the same thing, but in the input space.

142
00:06:37,892 --> 00:06:39,593
So it's a bit harder to do, because typically,

143
00:06:39,633 --> 00:06:40,954
these input spaces are very big.

144
00:06:41,514 --> 00:06:43,835
Here we see a data set that's very imbalanced.

145
00:06:43,855 --> 00:06:45,496
There's lots of samples on the right side,

146
00:06:45,516 --> 00:06:47,157
but we're missing information on the left side.

147
00:06:47,957 --> 00:06:49,858
So we could gather more examples from the left

148
00:06:50,199 --> 00:06:52,120
and have a much better representative picture

149
00:06:52,400 --> 00:06:53,220
of this data set.

150
00:06:54,082 --> 00:06:59,750
And this is more valuable than having a lot of examples that are condensed on the right

151
00:06:59,790 --> 00:06:59,990
side.

152
00:07:00,030 --> 00:07:03,955
It's better to have a nice distribution of your input data across all the dimensions

153
00:07:03,975 --> 00:07:04,696
that you care about.

154
00:07:05,966 --> 00:07:10,228
And this is a very active and important area of research.

155
00:07:10,268 --> 00:07:16,092
And in fact, Google's data set of 100 million images is

156
00:07:16,132 --> 00:07:18,253
actually partly automatically labeled as well.

157
00:07:18,313 --> 00:07:20,314
So they didn't go through all those images and manually

158
00:07:20,334 --> 00:07:21,355
assigning labels to them.

159
00:07:21,775 --> 00:07:24,457
They just understand that based on proximity or the

160
00:07:24,537 --> 00:07:27,759
structure of the images that they have a certain tag.

161
00:07:27,779 --> 00:07:28,899
So they don't have to go through that

162
00:07:28,919 --> 00:07:29,640
whole thing manually.

163
00:07:30,293 --> 00:07:34,977
And that is kind of the essence of this field of interactive learning, being able to suggest

164
00:07:35,738 --> 00:07:37,879
questions to humans, say, what is this?

165
00:07:38,060 --> 00:07:42,803
And then popping that up and getting a really valuable piece of data is better than having

166
00:07:43,123 --> 00:07:45,265
10 times more data that is a very low value.

167
00:07:47,067 --> 00:07:49,548
Here are some references if you want to go into more detail.

168
00:07:49,949 --> 00:07:51,270
The slides will be available later.

169
00:07:51,350 --> 00:07:52,511
You don't have to scribble it down.

170
00:07:57,145 --> 00:08:03,667
So now we have this data, we can basically feed it into neural models or deep learning models.

171
00:08:05,387 --> 00:08:08,228
I like to think of these models as computational graphs.

172
00:08:08,288 --> 00:08:13,210
So I'm not going to use the words biologically inspired, or I might occasionally use the word neuron,

173
00:08:13,550 --> 00:08:18,832
but deep learning has progressed way beyond the biological inspiration.

174
00:08:19,244 --> 00:08:20,884
that it originally started with.

175
00:08:21,365 --> 00:08:23,406
Now, it's easier to think of these models

176
00:08:23,506 --> 00:08:27,048
as very large computation graphs that basically take some input

177
00:08:27,108 --> 00:08:29,690
and then produce some outputs with the steps

178
00:08:29,730 --> 00:08:30,770
of operations in between.

179
00:08:30,790 --> 00:08:33,832
So on the left side, we have our input x.

180
00:08:33,932 --> 00:08:35,693
And that is typically a tensor.

181
00:08:35,753 --> 00:08:38,755
So that means it's a 2D matrix, for example.

182
00:08:39,075 --> 00:08:41,397
If it's an image, if it's a batch of images,

183
00:08:41,477 --> 00:08:42,677
it might be a 3D matrix.

184
00:08:43,018 --> 00:08:45,379
If it's a batch of videos, it might be a 4D matrix.

185
00:08:45,894 --> 00:08:47,637
And then on the output, we have similarly,

186
00:08:47,938 --> 00:08:49,901
it's going to be also a tensor, which

187
00:08:49,961 --> 00:08:51,384
is going to be another matrix of,

188
00:08:51,625 --> 00:08:55,172
it could be Boolean values, that this is a cat, a dog, a chair,

189
00:08:55,753 --> 00:08:58,178
or probabilities of different classes.

190
00:09:01,928 --> 00:09:04,932
So within this computation graph, we have even more tensors.

191
00:09:05,112 --> 00:09:08,196
These are potentially smaller matrices of different sizes

192
00:09:08,276 --> 00:09:09,337
scattered throughout the graph.

193
00:09:09,778 --> 00:09:11,821
And these make up the model, what

194
00:09:11,841 --> 00:09:14,184
we're going to be doing our computation with

195
00:09:14,244 --> 00:09:15,105
and what will be trained.

196
00:09:15,145 --> 00:09:16,727
It becomes this function f.

197
00:09:17,288 --> 00:09:19,911
And we have lots of little tensors scattered throughout.

198
00:09:21,425 --> 00:09:23,247
Some of them will be relatively small.

199
00:09:23,327 --> 00:09:24,268
Others will be much bigger.

200
00:09:24,628 --> 00:09:26,190
And effectively, you can think of it

201
00:09:26,270 --> 00:09:29,593
as a way of expressing all these different matrices that

202
00:09:29,773 --> 00:09:32,756
build up one bigger matrix multiplication with lots

203
00:09:32,796 --> 00:09:34,458
of custom operations in there instead.

204
00:09:37,726 --> 00:09:43,090
Another way to think of this is that when you specify these computation graphs, for example in TensorFlow,

205
00:09:43,810 --> 00:09:49,174
you'll specify what happens to your input x and the library might not immediately know what it is, so there's some abstraction there.

206
00:09:49,494 --> 00:09:53,497
And then it will sort of reason about these different operations in this abstract way,

207
00:09:53,557 --> 00:09:58,561
like multiply this input times this other matrix and then perform a nonlinear function on that,

208
00:09:58,701 --> 00:10:00,322
add the results, split it into two.

209
00:10:00,903 --> 00:10:04,791
And all these different steps are specified on abstract concepts.

210
00:10:04,811 --> 00:10:08,157
And then TensorFlow, or Theano, or other deep learning libraries

211
00:10:08,197 --> 00:10:11,343
will compile that and run it on your GPU as fast as it can.

212
00:10:16,330 --> 00:10:19,372
So there's two things to note about these computation graphs.

213
00:10:20,993 --> 00:10:23,695
The first is that it's important to have nonlinear functions

214
00:10:24,096 --> 00:10:25,797
at regular intervals throughout the graph.

215
00:10:26,197 --> 00:10:27,639
If you just have linear functions

216
00:10:27,659 --> 00:10:29,620
throughout your deep model, you'll

217
00:10:29,660 --> 00:10:31,642
end up with just another linear function.

218
00:10:31,682 --> 00:10:34,144
So it's going to be a really expensive linear function,

219
00:10:34,164 --> 00:10:35,305
which is a complete waste of time.

220
00:10:35,785 --> 00:10:38,927
So you want to put nonlinear functions into your graph

221
00:10:39,348 --> 00:10:41,429
to really give it the benefits of the depth.

222
00:10:41,489 --> 00:10:43,731
And the deeper you go, the more benefits

223
00:10:43,771 --> 00:10:45,813
you get, as long as you add nonlinear functions.

224
00:10:46,972 --> 00:10:49,293
So from left to right, these are some of the most common,

225
00:10:49,313 --> 00:10:50,134
the most popular ones.

226
00:10:50,494 --> 00:10:51,955
On the left is the tanh function.

227
00:10:51,995 --> 00:10:54,557
It's like a smooth step from minus 1 to plus 1.

228
00:10:55,598 --> 00:10:58,500
Sigmoid looks a bit similar, except it's between 0 and 1.

229
00:10:59,281 --> 00:11:01,423
And they basically use this very often

230
00:11:01,883 --> 00:11:05,205
for machine translation and sequence-to-sequence learning.

231
00:11:06,366 --> 00:11:07,627
In the middle, we've got the ReLU.

232
00:11:07,827 --> 00:11:09,769
It's called the Rectified Linear Unit.

233
00:11:10,435 --> 00:11:13,638
which is basically defined as 0 on the negative infinity side.

234
00:11:13,658 --> 00:11:17,020
Then it's the identity function on the positive infinity side.

235
00:11:17,661 --> 00:11:20,243
And this is used extremely often in image recognition,

236
00:11:20,263 --> 00:11:21,023
image processing.

237
00:11:22,284 --> 00:11:24,326
And then on the right side, we have the LU,

238
00:11:24,686 --> 00:11:26,087
exponential linear function, which

239
00:11:26,107 --> 00:11:27,548
is kind of a mix between the two.

240
00:11:27,568 --> 00:11:30,050
It goes from minus 1 smoothly transitioning

241
00:11:30,170 --> 00:11:31,631
into a linear function.

242
00:11:32,112 --> 00:11:34,914
And this is used more often in image segmentation.

243
00:11:35,494 --> 00:11:36,835
And I like this one.

244
00:11:37,095 --> 00:11:38,516
It's a good one for generative models.

245
00:11:42,510 --> 00:11:45,353
So the second thing to note about the computation graphs,

246
00:11:45,413 --> 00:11:48,696
here we have lots of nonlinear functions throughout the graph,

247
00:11:48,756 --> 00:11:51,339
but it's also good to have these residual connections.

248
00:11:51,719 --> 00:11:54,562
So this is a concept that's borrowed from signal processing,

249
00:11:55,042 --> 00:11:57,364
where you have a base signal, and then you add a bit more

250
00:11:57,384 --> 00:11:58,285
detail on top of that.

251
00:11:59,026 --> 00:12:01,248
So in this case, we're going to have a separate branch that

252
00:12:01,368 --> 00:12:03,750
branches off, and then another computation is done,

253
00:12:03,770 --> 00:12:05,492
and then we add the result back in later.

254
00:12:06,212 --> 00:12:09,575
And this is important because it helps the data flow a bit more smoothly.

255
00:12:10,055 --> 00:12:13,678
If you can imagine a graph like this that has thousands of layers like this,

256
00:12:13,698 --> 00:12:16,600
this is the biggest depth that Microsoft has trained.

257
00:12:16,620 --> 00:12:18,742
It went up to 1,500 deep models.

258
00:12:22,464 --> 00:12:26,805
the data flows much more smoothly if you have these regular skip connections throughout the graph.

259
00:12:27,505 --> 00:12:30,426
So that means that when you're going forwards and you're computing these functions,

260
00:12:30,726 --> 00:12:36,687
it means you can remove parts and it still kind of works, but also when you're going backwards, it means that the

261
00:12:37,108 --> 00:12:41,469
training procedure has an easier time updating all the values in the neural networks.

262
00:12:42,309 --> 00:12:45,209
I think this is one of the biggest insights of the past couple years actually.

263
00:12:45,850 --> 00:12:48,590
It made a big difference in all the competitions.

264
00:12:51,954 --> 00:12:55,598
So the tricks we've seen so far are on the general side.

265
00:12:55,638 --> 00:12:57,460
They're reusable concepts, you can apply them

266
00:12:57,901 --> 00:13:00,804
into whatever function you're trying to approximate,

267
00:13:00,844 --> 00:13:01,445
you can use these.

268
00:13:02,166 --> 00:13:03,868
But they're also very specific tricks

269
00:13:03,928 --> 00:13:06,871
that apply to, for example, image processing.

270
00:13:08,147 --> 00:13:11,850
And this can be thought of as a node inside the computation graph.

271
00:13:11,890 --> 00:13:13,391
It's called a convolution filter.

272
00:13:13,551 --> 00:13:17,334
You might have heard the word convolutional neural networks, and this is what's going

273
00:13:17,414 --> 00:13:18,215
on under the hood.

274
00:13:18,775 --> 00:13:24,280
It's like, it's taking a really small kernel or a 3x3 matrix and scanning it over the input

275
00:13:24,360 --> 00:13:27,062
image and then producing another output as a result.

276
00:13:27,522 --> 00:13:32,668
So you can think of this as like edge detection or pattern detection on the 3x3 scale

277
00:13:33,088 --> 00:13:38,394
and then producing the sort of the output grayscale image that shows how strong those

278
00:13:38,434 --> 00:13:44,220
edge detector were activated. So you can imagine stacking these detectors together,

279
00:13:44,240 --> 00:13:48,885
we have more 3x3 matrices and just ending up with more and more output channels and

280
00:13:48,905 --> 00:13:50,586
they're all detecting different kinds of patterns.

281
00:13:51,058 --> 00:13:53,001
And those patterns then get turned into more patterns.

282
00:13:53,062 --> 00:13:54,985
And as you make the neural network deeper,

283
00:13:55,005 --> 00:13:57,970
it learns patterns of patterns and building

284
00:13:58,031 --> 00:13:59,133
higher level abstractions.

285
00:14:01,307 --> 00:14:03,588
So on the right, you see there are gaps between the blue cells.

286
00:14:03,628 --> 00:14:05,409
That means we're sort of padding them with zero.

287
00:14:05,429 --> 00:14:08,069
That means you can do things like up-sampling operations.

288
00:14:08,329 --> 00:14:10,330
You can also skip certain cells as well.

289
00:14:10,410 --> 00:14:11,891
You can do down-sampling operations.

290
00:14:12,471 --> 00:14:14,051
And you can change the kernel size

291
00:14:14,071 --> 00:14:15,792
and make it a 5 by 5 matrix.

292
00:14:16,752 --> 00:14:20,193
And so in practice, most image recognition neural networks

293
00:14:20,353 --> 00:14:23,214
use layers and layers of these operations just stacked

294
00:14:23,254 --> 00:14:27,916
together to basically form these deep computational graphs that

295
00:14:28,036 --> 00:14:28,816
recognize images.

296
00:14:34,925 --> 00:14:37,868
So another common pattern that's used often in image,

297
00:14:38,289 --> 00:14:42,713
not image recognition, translation, is the LSTM cell.

298
00:14:43,474 --> 00:14:45,295
So it's the long short term memory.

299
00:14:46,768 --> 00:14:50,871
And you can think of it as like an electric circuit that's

300
00:14:50,891 --> 00:14:53,332
very specifically crafted to be assembled together.

301
00:14:53,392 --> 00:14:56,334
So they put together thousands of these, and then they stack

302
00:14:56,374 --> 00:14:57,915
another layer of them on top of each other.

303
00:14:58,215 --> 00:15:00,436
And they can learn sequence to sequence translations.

304
00:15:00,676 --> 00:15:04,799
So they're very good at translating.

305
00:15:05,359 --> 00:15:06,640
And that's what Google uses everywhere.

306
00:15:08,481 --> 00:15:10,602
So here are more details about the models.

307
00:15:11,663 --> 00:15:12,223
But so far.

308
00:15:14,973 --> 00:15:18,614
In general, deep learning is difficult to get right when you're trying to figure out

309
00:15:18,654 --> 00:15:19,695
what you need to plug together.

310
00:15:19,715 --> 00:15:25,597
It's a very intuition-driven process, and this is the part where the dark art comes

311
00:15:25,657 --> 00:15:25,757
in.

312
00:15:26,098 --> 00:15:29,819
You need to have some sense of what's going to work, and you can do that by reading a

313
00:15:29,859 --> 00:15:31,420
lot of the papers in the field.

314
00:15:31,460 --> 00:15:38,123
So every morning I try to read some PDFs from archive.org, which is a preprint server for

315
00:15:38,543 --> 00:15:39,643
the machine learning community.

316
00:15:40,894 --> 00:15:42,095
sort of converged on.

317
00:15:42,395 --> 00:15:43,916
So you can get papers that are related

318
00:15:43,936 --> 00:15:44,836
to what you're trying to solve,

319
00:15:44,896 --> 00:15:46,958
and then get a sense of what architecture they're using,

320
00:15:47,298 --> 00:15:47,798
and try it out.

321
00:15:50,232 --> 00:15:53,414
But so in practice, if you have hundreds of thousands

322
00:15:53,454 --> 00:15:55,394
of GPUs available to you, if you're

323
00:15:55,414 --> 00:15:57,635
willing to spend the money on the cloud compute,

324
00:15:58,055 --> 00:16:00,476
then you can use more experimental testing

325
00:16:00,517 --> 00:16:02,517
approaches, trying out all the different variations

326
00:16:02,557 --> 00:16:04,618
of your architecture and seeing what comes out.

327
00:16:04,638 --> 00:16:06,399
It can be a bit expensive, but that

328
00:16:06,439 --> 00:16:10,000
makes it more of a methodical and experimental science

329
00:16:10,060 --> 00:16:13,962
as opposed to we don't yet have a full analytical understanding

330
00:16:13,982 --> 00:16:15,503
of how these architectures work.

331
00:16:18,318 --> 00:16:20,059
So in practice, what this means is

332
00:16:20,079 --> 00:16:23,781
that it puts more emphasis on the computation side of things

333
00:16:23,821 --> 00:16:26,083
and understanding how the data flows

334
00:16:26,463 --> 00:16:27,463
through your neural networks.

335
00:16:27,523 --> 00:16:31,726
And that's important to understand

336
00:16:31,766 --> 00:16:33,467
the statistics of the data as it's

337
00:16:33,847 --> 00:16:34,947
progressing through the network.

338
00:16:36,408 --> 00:16:38,089
So in traditional machine learning,

339
00:16:38,249 --> 00:16:40,250
you have to, well, it's recommended

340
00:16:40,270 --> 00:16:43,532
that you normalize your input data, which

341
00:16:43,572 --> 00:16:45,933
means you want the mean to be around zero,

342
00:16:45,973 --> 00:16:47,274
and you want the standard deviation.

343
00:16:47,688 --> 00:16:48,529
to be around 1.

344
00:16:48,689 --> 00:16:51,510
And so that has a lot of benefits.

345
00:16:51,850 --> 00:16:53,970
All these nonlinear functions that we talked about,

346
00:16:54,390 --> 00:16:56,951
if you feed in a nice normal distribution

347
00:16:56,991 --> 00:17:00,872
into these functions, you get a mix of plus 1's and minus 1's,

348
00:17:00,892 --> 00:17:06,154
or a positive or a 0 in the case of the middle function.

349
00:17:07,351 --> 00:17:11,014
So this means that the neural network or the deep learning model is actually making decisions.

350
00:17:11,175 --> 00:17:15,299
It's actually deciding whether something should be true or false or a plus one and minus one.

351
00:17:15,739 --> 00:17:20,424
Whereas if you weren't normalizing your data, it might end up in the case of this tanh function,

352
00:17:20,484 --> 00:17:26,069
all the values might end up being 1.0, in which case it's giving you no extra information about your input data.

353
00:17:26,090 --> 00:17:27,511
You've just lost a lot of information.

354
00:17:28,752 --> 00:17:35,460
And these were fundamentally the problems that the machine learning community wrestled with for about 20 years until they figured out what was happening

355
00:17:36,402 --> 00:17:41,147
So it sounds simple, but it took us a while to figure out how how things were

356
00:17:42,249 --> 00:17:43,650
Things were going on under the hood

357
00:17:45,525 --> 00:17:47,847
So as you take your data and you filter it

358
00:17:47,948 --> 00:17:50,670
through this computational graph, the deeper it gets,

359
00:17:50,850 --> 00:17:56,155
the harder it is to predict what shape the data's distribution

360
00:17:56,175 --> 00:17:58,958
is going to have, if it's going to drift off towards infinity

361
00:17:59,358 --> 00:18:00,879
or if it's going to converge towards zero

362
00:18:00,919 --> 00:18:02,621
and end up just vanishing completely.

363
00:18:03,502 --> 00:18:06,665
So that will depend entirely on the kinds of operations

364
00:18:06,685 --> 00:18:07,265
that you're using.

365
00:18:07,285 --> 00:18:09,207
It will depend on how you've initialized your data,

366
00:18:09,547 --> 00:18:11,289
how you've initialized the parameters.

367
00:18:12,129 --> 00:18:15,710
And so it's generally a very difficult thing to predict these distributions.

368
00:18:18,671 --> 00:18:23,633
So the trick, and this is what brought back the deep learning craze.

369
00:18:26,328 --> 00:18:28,670
is correctly initializing the parameters.

370
00:18:28,750 --> 00:18:30,291
This is one of the approaches such

371
00:18:30,351 --> 00:18:33,513
that your output distribution is basically also

372
00:18:33,533 --> 00:18:35,254
a normalized distribution.

373
00:18:35,334 --> 00:18:38,356
So if you set the certain weights and biases,

374
00:18:38,396 --> 00:18:41,458
you can shift your data's distribution around and then

375
00:18:41,499 --> 00:18:43,780
make sure that it is a normal distribution again.

376
00:18:44,861 --> 00:18:46,282
So there's some great papers on this.

377
00:18:46,302 --> 00:18:48,503
This is a recent paper with a new approach

378
00:18:48,543 --> 00:18:50,245
to doing that called weight normalization.

379
00:18:50,665 --> 00:18:53,587
It's a well-written paper and quite accessible.

380
00:18:56,169 --> 00:18:58,511
The other approach, which is used almost everywhere,

381
00:18:58,531 --> 00:19:00,032
is called batch normalization.

382
00:19:00,392 --> 00:19:02,554
So basically keeping track of the mean

383
00:19:02,834 --> 00:19:05,196
and the standard deviation of your data set

384
00:19:05,276 --> 00:19:06,857
as you're filtering it through the graph.

385
00:19:07,197 --> 00:19:10,540
And then effectively, you can adjust it on the fly.

386
00:19:10,720 --> 00:19:12,161
The more data you filter through it,

387
00:19:12,201 --> 00:19:13,622
then the better it will understand

388
00:19:13,662 --> 00:19:16,224
how to move the data set around.

389
00:19:17,356 --> 00:19:24,044
This is a technique that allowed us to really get much better results in image recognition

390
00:19:24,144 --> 00:19:26,367
and it's made a significant difference.

391
00:19:26,407 --> 00:19:30,011
This is one of the most important techniques of the last five years.

392
00:19:31,835 --> 00:19:34,937
So as a result of applying these techniques, things like batch normalization,

393
00:19:35,477 --> 00:19:38,419
you get a nice distribution of positive and negative results

394
00:19:38,539 --> 00:19:41,721
by each of the channels inside your deep model,

395
00:19:42,001 --> 00:19:43,922
making decisions positive and negative,

396
00:19:44,223 --> 00:19:46,004
and you end up with a nice sparse result,

397
00:19:46,024 --> 00:19:48,065
as opposed to just having zeros everywhere.

398
00:19:51,126 --> 00:19:56,608
So, and this is what made things like neural style possible, because each of these different

399
00:19:56,748 --> 00:20:00,569
photos, as you filter it through the network, will have a different fingerprint or a different

400
00:20:01,729 --> 00:20:02,530
unique identifier.

401
00:20:02,570 --> 00:20:03,490
And it looks a bit like this.

402
00:20:04,670 --> 00:20:08,291
So, you can train for this and try and encourage these results to emerge.

403
00:20:08,551 --> 00:20:11,312
And I'm going to skip a bit quicker through that.

404
00:20:12,793 --> 00:20:15,734
What this means is that you have certain channels

405
00:20:15,794 --> 00:20:18,935
of certain neurons that have certain activation patterns,

406
00:20:19,235 --> 00:20:21,896
and you can remove them, and the network will still mostly work.

407
00:20:22,236 --> 00:20:26,177
So we don't want to put all of our eggs in one basket.

408
00:20:26,217 --> 00:20:28,178
There's a certain amount of redundancy there,

409
00:20:28,218 --> 00:20:30,858
and the systems are relatively fault tolerant,

410
00:20:30,939 --> 00:20:32,779
thanks to these representations.

411
00:20:35,618 --> 00:20:39,201
And yeah, you can train to encourage these representations

412
00:20:39,261 --> 00:20:39,781
to emerge.

413
00:20:40,542 --> 00:20:43,524
And I'm going to skip this, because I'm

414
00:20:43,544 --> 00:20:44,465
running short on time.

415
00:20:46,126 --> 00:20:49,289
But that leads into the section on training and gradient

416
00:20:49,309 --> 00:20:49,609
descent.

417
00:20:50,867 --> 00:20:55,190
So this is where we take all these models that we've talked about and we actually train

418
00:20:55,210 --> 00:20:57,951
them to approximate the function that we're interested in learning.

419
00:20:58,812 --> 00:21:07,098
And in general, this is also from the machine learning perspective, there are two functions

420
00:21:07,138 --> 00:21:08,458
that you can use to train a model.

421
00:21:08,759 --> 00:21:13,102
The first is mean error, which is you calculate the difference between what you have and what

422
00:21:13,142 --> 00:21:18,165
you want, and then you use that as a signal to basically train the whole network.

423
00:21:19,246 --> 00:21:22,249
If you have non-continuous values, if you have Booleans,

424
00:21:22,289 --> 00:21:24,011
like this is a cat, a dog, then you'll

425
00:21:24,091 --> 00:21:25,813
use another error function.

426
00:21:26,093 --> 00:21:28,436
It's called categorical cross-entropy.

427
00:21:29,317 --> 00:21:31,479
And so these are two, they're not

428
00:21:31,639 --> 00:21:32,961
specific to deep learning there.

429
00:21:33,081 --> 00:21:35,904
You'll use them in regular machine learning as well.

430
00:21:38,245 --> 00:21:40,186
And the way deep learning takes those signals

431
00:21:40,226 --> 00:21:42,626
and then trains the model is using an algorithm

432
00:21:42,666 --> 00:21:43,867
called backpropagation.

433
00:21:44,527 --> 00:21:46,527
And backpropagation combines two things.

434
00:21:46,587 --> 00:21:49,048
It combines the chain rule with the update rule.

435
00:21:50,088 --> 00:21:52,229
The chain rule is there to take the error, which

436
00:21:52,329 --> 00:21:54,129
is calculated on the right side.

437
00:21:54,149 --> 00:21:56,630
We calculate how far off we are from our target.

438
00:21:56,910 --> 00:21:59,690
And then we sort of propagate that through the graph again.

439
00:22:00,330 --> 00:22:02,851
And the goal is to calculate the gradient of the error.

440
00:22:03,071 --> 00:22:04,831
So we want to know what direction

441
00:22:04,871 --> 00:22:07,972
we need to adjust the weights in to get a better result.

442
00:22:09,752 --> 00:22:13,195
So there are derivations of how you can do this.

443
00:22:13,255 --> 00:22:15,217
There are some great tutorials that links to this.

444
00:22:17,299 --> 00:22:18,460
I'm not going to derive that now.

445
00:22:19,041 --> 00:22:21,984
But as a result of applying the chain rule and going back,

446
00:22:22,024 --> 00:22:25,808
we get a sense of what the error function looks like locally.

447
00:22:26,188 --> 00:22:29,711
And so we can step in the right direction to minimize that error.

448
00:22:30,893 --> 00:22:32,274
That is done using an update rule.

449
00:22:32,833 --> 00:22:34,894
And there's a collection of different rules

450
00:22:34,914 --> 00:22:35,514
that you can use.

451
00:22:35,534 --> 00:22:39,697
There's the standard SGD, which is stochastic gradient descent.

452
00:22:40,117 --> 00:22:41,558
And that's the classic.

453
00:22:41,778 --> 00:22:43,119
It's been around for decades.

454
00:22:43,519 --> 00:22:45,641
And there are others that are more recent, more modern.

455
00:22:45,741 --> 00:22:47,101
And they have a lot of different.

456
00:22:49,044 --> 00:22:53,487
benefits but all the bottom ones use a lot of extra memory to sort of maintain

457
00:22:53,787 --> 00:22:59,591
they maintain extra per parameter weights that tell it how quickly to step

458
00:23:00,132 --> 00:23:04,435
so you basically get a lot of acceleration for free but it costs you

459
00:23:05,396 --> 00:23:05,656
memory

460
00:23:09,278 --> 00:23:11,339
So of all these different algorithms,

461
00:23:12,239 --> 00:23:13,619
the ones that are used the most often

462
00:23:13,719 --> 00:23:15,820
are the plain old stochastic gradient descenders.

463
00:23:15,840 --> 00:23:17,620
So the simplest thing you could possibly do,

464
00:23:17,920 --> 00:23:20,221
like just take one step in the direction of the gradient

465
00:23:21,121 --> 00:23:22,201
based on random data.

466
00:23:22,902 --> 00:23:25,382
And the second most popular is probably

467
00:23:25,522 --> 00:23:27,282
Adam at the bottom, which is used very often.

468
00:23:27,863 --> 00:23:29,583
Eve is the up and coming.

469
00:23:30,643 --> 00:23:32,004
These academics and their names.

470
00:23:33,884 --> 00:23:36,905
And RMSProp is also, it'll come back.

471
00:23:37,985 --> 00:23:40,147
Most of the time, as a practitioner, you don't need to know about these.

472
00:23:40,167 --> 00:23:41,748
They're completely encapsulated.

473
00:23:41,788 --> 00:23:43,810
You just call this update rule, and it just works.

474
00:23:43,870 --> 00:23:45,791
And you don't have to worry about the details.

475
00:23:46,632 --> 00:23:49,654
One thing you might have to worry about is that deep optimization

476
00:23:49,674 --> 00:23:56,079
or optimization of these deep graphs is a non-convex problem.

477
00:23:56,119 --> 00:24:00,022
So you have no guarantees that you will reach a global minima.

478
00:24:01,552 --> 00:24:05,714
But as it turns out, and this is very recent research on the topic, it doesn't really matter

479
00:24:06,214 --> 00:24:11,375
that you can't prove that the global minima can't be reached because it's such a huge

480
00:24:11,415 --> 00:24:15,356
parameter space with over a billion different parameters, there are minimas everywhere and

481
00:24:15,376 --> 00:24:21,598
there's many symmetric versions of the solution and as it turns out, these algorithms, the

482
00:24:21,738 --> 00:24:27,320
update rules, will converge often to results that are, if not the global minima, they're

483
00:24:27,340 --> 00:24:29,340
pretty close and so you won't notice in practice.

484
00:24:31,306 --> 00:24:35,427
So there are also some proofs that if you simplify deep learning down to some very basic

485
00:24:35,467 --> 00:24:37,368
primitives then it actually is convex.

486
00:24:37,848 --> 00:24:41,369
So if you want what we can mathematically prove, it's not quite the state of the art,

487
00:24:41,429 --> 00:24:44,469
but the state of the art stuff, you don't have to worry about it.

488
00:24:44,910 --> 00:24:45,910
It mostly just works.

489
00:24:46,710 --> 00:24:48,591
The hard part is figuring out the architecture.

490
00:24:49,831 --> 00:24:54,672
So here are some more great tutorials that go into more detail about the backpropagation

491
00:24:54,712 --> 00:24:57,973
process and I highly recommend those.

492
00:24:58,093 --> 00:24:59,873
And again I'll post the slides online afterwards.

493
00:25:02,675 --> 00:25:05,379
So to summarize, data is important,

494
00:25:05,519 --> 00:25:07,903
but it's more important to understand the statistics

495
00:25:07,943 --> 00:25:09,625
of your data, make sure it's well distributed

496
00:25:10,767 --> 00:25:12,429
across all your output categories

497
00:25:12,569 --> 00:25:13,671
and all your input space.

498
00:25:15,963 --> 00:25:19,326
The models in deep learning have progressed way beyond neural networks and biologically

499
00:25:19,366 --> 00:25:20,127
inspired models.

500
00:25:20,167 --> 00:25:24,090
They're general computation graphs that you could integrate into your software to solve

501
00:25:24,130 --> 00:25:25,652
problems based on data.

502
00:25:28,054 --> 00:25:33,938
Making architectures and models for deep learning is difficult, but you can sort of get away

503
00:25:33,978 --> 00:25:36,881
with doing lots of simulations and seeing which ones work best.

504
00:25:36,981 --> 00:25:41,144
It's a bit computationally expensive, but it's quite a reliable way forward.

505
00:25:42,546 --> 00:25:47,431
And from the optimization perspective, you can't guarantee for sure that it will converge,

506
00:25:47,611 --> 00:25:48,873
but in practice it always does.

507
00:25:49,373 --> 00:25:53,838
And modern deep learning libraries are very reliable, so you get very good results from

508
00:25:53,858 --> 00:25:53,979
them.

509
00:25:56,942 --> 00:25:57,903
Great, that's it for me.

510
00:25:58,163 --> 00:25:58,684
Thank you very much.

